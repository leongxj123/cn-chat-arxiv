# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era](https://arxiv.org/abs/2403.08946) | 在大型语言模型时代，为了适应其复杂性和先进能力，我们引入了可用的XAI概念，通过积极增强LLMs在实际环境中的生产力和适用性，实现XAI方法论的重大转变。 |
| [^2] | [Increasing Fairness via Combination with Learning Guarantees.](http://arxiv.org/abs/2301.10813) | 该论文提出了一种公平质量度量方法，名为判别风险，旨在反映个体和群体公平性。此外，研究者还讨论了公平性是否可以在理论上得到保证。 |

# 详细

[^1]: 可用的XAI：在LLM时代利用可解释性的10个策略

    Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era

    [https://arxiv.org/abs/2403.08946](https://arxiv.org/abs/2403.08946)

    在大型语言模型时代，为了适应其复杂性和先进能力，我们引入了可用的XAI概念，通过积极增强LLMs在实际环境中的生产力和适用性，实现XAI方法论的重大转变。

    

    可解释人工智能（XAI）指的是提供人类可理解的洞见，揭示人工智能模型的运作方式的技术。最近，XAI的重点正被扩展到常常因为不透明而备受批评的大型语言模型（LLMs）。这一拓展需要对XAI方法论进行显著转变，因为有两个原因。首先，许多现有的XAI方法无法直接应用于LLMs，因为它们的复杂性和先进能力。其次，随着LLMs越来越广泛地应用于不同行业应用中，XAI的角色从仅仅打开“黑匣子”转变为积极增强LLMs在实际环境中的生产力和适用性。与此同时，不同于传统机器学习模型仅作为XAI洞见的被动接受者，LLMs的独特能力能够相互增强XAI。因此，在本文中，我们通过分析（1）...

    arXiv:2403.08946v1 Announce Type: cross  Abstract: Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the "black box" to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1)
    
[^2]: 通过学习保证提高公平性

    Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10813](http://arxiv.org/abs/2301.10813)

    该论文提出了一种公平质量度量方法，名为判别风险，旨在反映个体和群体公平性。此外，研究者还讨论了公平性是否可以在理论上得到保证。

    

    随着机器学习系统在越来越多的现实场景中得到广泛应用，对于隐藏在机器学习模型中的潜在歧视的担忧正在增加。许多技术已经被开发出来以增强公平性，包括常用的群体公平性度量和几种结合集成学习的公平感知方法。然而，现有的公平度量只能关注其中之一，即群体公平性或个体公平性，它们之间的硬性兼容性暗示了即使其中之一得到满足，仍可能存在偏见。此外，现有的提升公平性的机制通常只提供经验结果来证明其有效性，但很少有论文讨论公平性是否可以在理论上得到保证。为了解决这些问题，本文提出了一种公平质量度量方法——判别风险，以反映个体和群体公平性两个方面。此外，我们还研究了p...

    The concern about underlying discrimination hidden in ML models is increasing, as ML systems have been widely applied in more and more real-world scenarios and any discrimination hidden in them will directly affect human life. Many techniques have been developed to enhance fairness including commonly-used group fairness measures and several fairness-aware methods combining ensemble learning. However, existing fairness measures can only focus on one aspect -- either group or individual fairness, and the hard compatibility among them indicates a possibility of remaining biases even if one of them is satisfied. Moreover, existing mechanisms to boost fairness usually present empirical results to show validity, yet few of them discuss whether fairness can be boosted with certain theoretical guarantees. To address these issues, we propose a fairness quality measure named discriminative risk in this paper to reflect both individual and group fairness aspects. Furthermore, we investigate the p
    

