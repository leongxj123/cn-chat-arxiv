# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models](https://arxiv.org/abs/2402.15481) | 提出了Prejudice-Caprice Framework（PCF）来全面衡量LLMs中的歧视，考虑了它们在不同上下文中的一贯偏见偏好和偏好变化。 |
| [^2] | [Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways](https://arxiv.org/abs/2402.04420) | 通过调查研究和实验研究，本文研究了机器学习错误对人们的影响，发现强化刻板印象的错误引起更多主观上的伤害体验，而违反刻板印象的错误对男性产生更大的主观上的伤害，有助于我们理解机器学习在引发刻板印象和伤害方面的作用。 |
| [^3] | [A Comprehensive Survey of Artificial Intelligence Techniques for Talent Analytics.](http://arxiv.org/abs/2307.03195) | 这篇论文综述了人工智能技术在人才分析中的应用。通过使用大数据和人工智能技术，组织可以从数据科学的角度理解组织行为并实时做出决策，为有效的人才管理提供智能支持。 |

# 详细

[^1]: 偏见和反复无常：衡量大型语言模型社会歧视的统计框架

    Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models

    [https://arxiv.org/abs/2402.15481](https://arxiv.org/abs/2402.15481)

    提出了Prejudice-Caprice Framework（PCF）来全面衡量LLMs中的歧视，考虑了它们在不同上下文中的一贯偏见偏好和偏好变化。

    

    arXiv:2402.15481v1 公告类型: 新的 摘要: 大型语言模型（LLMs）在社会运营中的日益融合加剧了它们对经济、法律、教育和医疗等重要领域决策的影响，引发了公众对这些模型涉及歧视安全和可靠性的担忧。然而，先前的歧视测量框架仅评估LLMs的平均歧视行为，往往由于忽视了一个额外的导致歧视的因素，即LLMs在不同上下文中的预测变化而变得不足。在这项工作中，我们提出了Prejudice-Caprice Framework（PCF），通过考虑LLMs的一贯偏见偏好和在多样上

    arXiv:2402.15481v1 Announce Type: new  Abstract: The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemmin
    
[^2]: 测量机器学习中的刻板印象伤害：需要了解谁正在受到哪些错误以及以何种方式受到伤害

    Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways

    [https://arxiv.org/abs/2402.04420](https://arxiv.org/abs/2402.04420)

    通过调查研究和实验研究，本文研究了机器学习错误对人们的影响，发现强化刻板印象的错误引起更多主观上的伤害体验，而违反刻板印象的错误对男性产生更大的主观上的伤害，有助于我们理解机器学习在引发刻板印象和伤害方面的作用。

    

    随着机器学习应用的普及，我们需要了解它们可能造成的伤害。然而，当前的公平性指标很少基于人类对伤害的心理体验。借鉴刻板印象的社会心理学，我们以图像搜索中的性别刻板印象为案例研究，研究了人们对机器学习错误的反应。首先，我们使用调查研究表明，并非所有的机器学习错误都反映了刻板印象，也没有同样的伤害程度。然后，在实验研究中，我们随机使参与者接触到强化、违反和中性的机器学习错误。我们发现，强化刻板印象的错误引起更多主观上的伤害体验，但对认知信念、态度或行为的改变很小。这种体验上的伤害对女性影响更大。然而，某些违反刻板印象的错误对男性产生更大的主观上的伤害，可能是由于对男性阳刚性的威胁感知。

    As machine learning applications proliferate, we need an understanding of their potential for harm. However, current fairness metrics are rarely grounded in human psychological experiences of harm. Drawing on the social psychology of stereotypes, we use a case study of gender stereotypes in image search to examine how people react to machine learning errors. First, we use survey studies to show that not all machine learning errors reflect stereotypes nor are equally harmful. Then, in experimental studies we randomly expose participants to stereotype-reinforcing, -violating, and -neutral machine learning errors. We find stereotype-reinforcing errors induce more experientially (i.e., subjectively) harmful experiences, while having minimal changes to cognitive beliefs, attitudes, or behaviors. This experiential harm impacts women more than men. However, certain stereotype-violating errors are more experientially harmful for men, potentially due to perceived threats to masculinity. We conc
    
[^3]: 人才分析的人工智能技术综述

    A Comprehensive Survey of Artificial Intelligence Techniques for Talent Analytics. (arXiv:2307.03195v1 [cs.CY])

    [http://arxiv.org/abs/2307.03195](http://arxiv.org/abs/2307.03195)

    这篇论文综述了人工智能技术在人才分析中的应用。通过使用大数据和人工智能技术，组织可以从数据科学的角度理解组织行为并实时做出决策，为有效的人才管理提供智能支持。

    

    在当今竞争激烈且快速发展的商业环境下，组织需要重新思考以量化方式做出人才相关决策的重要性。事实上，大数据和人工智能技术的最新发展已经彻底改变了人力资源管理。大规模人才和管理相关数据的可用性为企业领导者提供了从数据科学角度理解组织行为和获取实际知识的无与伦比机会，进而为实时决策和有效的人才管理提供智能支持。在过去的十年中，人才分析已经成为应用数据科学在人力资源管理方面的有希望的领域，引起了人工智能社区的广泛关注并激发了许多研究工作。为此，我们提供了一个最新、全面的关于人工智能技术在人才分析中的应用的综述。

    In today's competitive and fast-evolving business environment, it is a critical time for organizations to rethink how to make talent-related decisions in a quantitative manner. Indeed, the recent development of Big Data and Artificial Intelligence (AI) techniques have revolutionized human resource management. The availability of large-scale talent and management-related data provides unparalleled opportunities for business leaders to comprehend organizational behaviors and gain tangible knowledge from a data science perspective, which in turn delivers intelligence for real-time decision-making and effective talent management at work for their organizations. In the last decade, talent analytics has emerged as a promising field in applied data science for human resource management, garnering significant attention from AI communities and inspiring numerous research efforts. To this end, we present an up-to-date and comprehensive survey on AI technologies used for talent analytics in the f
    

