# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models.](http://arxiv.org/abs/2401.01301) | 大型语言模型存在法律幻觉，不一致法律事实，幻觉普遍存在高达69%至88%的情况，无法纠正用户错误法律假设。 |

# 详细

[^1]: 大型法律虚构：揭示大型语言模型中的法律幻觉

    Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. (arXiv:2401.01301v1 [cs.CL])

    [http://arxiv.org/abs/2401.01301](http://arxiv.org/abs/2401.01301)

    大型语言模型存在法律幻觉，不一致法律事实，幻觉普遍存在高达69%至88%的情况，无法纠正用户错误法律假设。

    

    大型语言模型（LLMs）有可能改变法律实践，但其潜力受到法律幻觉的威胁，即这些模型产生与法律事实不一致的回答。我们使用一套原创的法律查询来调查这些幻觉的程度，将LLMs的回答与结构化的法律元数据进行对比，并检查其一致性。我们的工作有四个关键贡献：（1）我们建立了法律幻觉的分类体系，为今后在这一领域进行的研究提供了概念框架。（2）我们发现，法律幻觉的普遍性令人担忧，在对随机联邦法院案例进行具体、可验证的问题时，ChatGPT 3.5产生的幻觉发生率为69％，而Llama 2为88％。（3）我们展示了LLMs在逆向问题设置中往往无法纠正用户的错误法律假设。（4）我们提供了证据表明LLMs并不总能预测或并不总知道...

    Large language models (LLMs) have the potential to transform the practice of law, but this potential is threatened by the presence of legal hallucinations -- responses from these models that are not consistent with legal facts. We investigate the extent of these hallucinations using an original suite of legal queries, comparing LLMs' responses to structured legal metadata and examining their consistency. Our work makes four key contributions: (1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area. (2) We find that legal hallucinations are alarmingly prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases. (3) We illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual question setup. (4) We provide evidence that LLMs cannot always predict, or do not always know, wh
    

