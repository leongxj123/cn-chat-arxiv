<rss version="2.0"><channel><title>Chat Arxiv cs.DL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DL</description><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#23558;OCR&#24314;&#27169;&#20026;&#23383;&#31526;&#32423;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#24050;&#26377;&#26550;&#26500;&#26356;&#20855;&#26679;&#26412;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20174;&#32780;&#20351;&#25968;&#23383;&#21382;&#21490;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#25991;&#29486;&#21490;&#26009;&#24471;&#20197;&#26356;&#22909;&#22320;&#21442;&#19982;&#31038;&#21306;&#12290;</title><link>http://arxiv.org/abs/2304.02737</link><description>&lt;p&gt;
&#24314;&#35774;&#22810;&#26679;&#21270;&#25968;&#23383;&#21382;&#21490;&#30340;&#39640;&#25928;OCR
&lt;/p&gt;
&lt;p&gt;
Efficient OCR for Building a Diverse Digital History. (arXiv:2304.02737v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#23558;OCR&#24314;&#27169;&#20026;&#23383;&#31526;&#32423;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#24050;&#26377;&#26550;&#26500;&#26356;&#20855;&#26679;&#26412;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20174;&#32780;&#20351;&#25968;&#23383;&#21382;&#21490;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#25991;&#29486;&#21490;&#26009;&#24471;&#20197;&#26356;&#22909;&#22320;&#21442;&#19982;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#26377;&#25104;&#21315;&#19978;&#19975;&#30340;&#29992;&#25143;&#26597;&#38405;&#25968;&#23383;&#26723;&#26696;&#65292;&#20294;&#20182;&#20204;&#21487;&#20197;&#20351;&#29992;&#30340;&#20449;&#24687;&#24182;&#19981;&#33021;&#20195;&#34920;&#21508;&#31181;&#25991;&#29486;&#21490;&#26009;&#30340;&#22810;&#26679;&#24615;&#12290;&#20856;&#22411;&#29992;&#20110;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26550;&#26500;&#8212;&#8212;&#32852;&#21512;&#23398;&#20064;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;&#22312;&#20302;&#36164;&#28304;&#25991;&#29486;&#38598;&#21512;&#20013;&#24456;&#38590;&#25193;&#23637;&#65292;&#22240;&#20026;&#23398;&#20064;&#35821;&#35328;-&#35270;&#35273;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#24207;&#21015;&#21644;&#35745;&#31639;&#12290;&#26412;&#30740;&#31350;&#23558;OCR&#24314;&#27169;&#20026;&#23383;&#31526;&#32423;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#65292;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12290;&#22240;&#20026;&#35813;&#27169;&#22411;&#21482;&#23398;&#20064;&#23383;&#31526;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#23427;&#27604;&#29616;&#26377;&#26550;&#26500;&#26356;&#20855;&#26679;&#26412;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#33021;&#22815;&#22312;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22833;&#36133;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;OCR&#12290;&#20851;&#38190;&#26159;&#65292;&#35813;&#27169;&#22411;&#20026;&#31038;&#21306;&#21442;&#19982;&#22312;&#20351;&#25968;&#23383;&#21382;&#21490;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#25991;&#29486;&#21490;&#26009;&#26041;&#38754;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thousands of users consult digital archives daily, but the information they can access is unrepresentative of the diversity of documentary history. The sequence-to-sequence architecture typically used for optical character recognition (OCR) - which jointly learns a vision and language model - is poorly extensible to low-resource document collections, as learning a language-vision model requires extensive labeled sequences and compute. This study models OCR as a character level image retrieval problem, using a contrastively trained vision encoder. Because the model only learns characters' visual features, it is more sample efficient and extensible than existing architectures, enabling accurate OCR in settings where existing solutions fail. Crucially, the model opens new avenues for community engagement in making digital history more representative of documentary history.
&lt;/p&gt;</description></item></channel></rss>