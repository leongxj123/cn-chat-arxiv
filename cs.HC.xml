<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoAT&#30340;Chain-of-Action-Thought&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#20808;&#21069;&#21160;&#20316;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#24773;&#20917;&#20197;&#21450;&#26410;&#26469;&#21160;&#20316;&#24605;&#32771;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#26426;GUI&#20195;&#29702;&#30340;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02713</link><description>&lt;p&gt;
Android&#22312;&#21160;&#29289;&#22253;&#20013;: GUI&#20195;&#29702;&#30340;&#21160;&#20316;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Android in the Zoo: Chain-of-Action-Thought for GUI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoAT&#30340;Chain-of-Action-Thought&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#20808;&#21069;&#21160;&#20316;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#24773;&#20917;&#20197;&#21450;&#26410;&#26469;&#21160;&#20316;&#24605;&#32771;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#26426;GUI&#20195;&#29702;&#30340;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23548;&#33268;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#22823;&#37327;&#33258;&#20027;GUI&#20195;&#29702;&#28608;&#22686;&#65292;&#36825;&#20123;&#20195;&#29702;&#36890;&#36807;&#39044;&#27979;API&#30340;&#19968;&#31995;&#21015;&#21160;&#20316;&#26469;&#23436;&#25104;&#30001;&#33258;&#28982;&#35821;&#35328;&#35302;&#21457;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#35813;&#20219;&#21153;&#39640;&#24230;&#20381;&#36182;&#20110;&#36807;&#21435;&#30340;&#21160;&#20316;&#21644;&#35270;&#35273;&#35266;&#23519;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#24456;&#23569;&#32771;&#34385;&#20013;&#38388;&#25130;&#22270;&#21644;&#23631;&#24149;&#25805;&#20316;&#20256;&#36882;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#20316;&#24605;&#32500;&#38142;&#65288;CoAT&#65289;&#65292;&#23427;&#32771;&#34385;&#20102;&#20808;&#21069;&#21160;&#20316;&#30340;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#20998;&#26512;&#24212;&#24403;&#25191;&#34892;&#30340;&#21160;&#20316;&#20197;&#21450;&#36873;&#25321;&#30340;&#21160;&#20316;&#24102;&#26469;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#29616;&#25104;LLM&#36827;&#34892;&#38646;&#27425;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;CoAT&#30456;&#27604;&#20110;&#26631;&#20934;&#19978;&#19979;&#25991;&#24314;&#27169;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#26631;&#30340;&#23436;&#25104;&#24773;&#20917;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Android-In-The-Zoo&#65288;AitZ&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;18,643&#20010;&#23631;&#24149;&#21160;&#20316;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02713v1 Announce Type: new  Abstract: Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18659</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#28216;&#25103;&#65306;&#35843;&#30740;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Games: A Survey and Roadmap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#24613;&#21095;&#22686;&#21152;&#65292;&#24182;&#20276;&#38543;&#30528;&#20844;&#20247;&#23545;&#35813;&#20027;&#39064;&#30340;&#21442;&#19982;&#12290;&#23613;&#31649;&#36215;&#21021;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;LLMs&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#21253;&#25324;&#28216;&#25103;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21450;&#20026;&#28216;&#25103;&#25552;&#20379;&#25903;&#25345;&#30340;&#21508;&#31181;&#24212;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#26126;&#30830;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21487;&#20197;&#25198;&#28436;&#30340;&#19981;&#21516;&#35282;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23578;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#21644;LLMs&#22312;&#28216;&#25103;&#20013;&#26410;&#26469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;LLMs&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#20316;&#20026;LLMs&#21644;&#28216;&#25103;&#20132;&#21449;&#39046;&#22495;&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#21644;&#36335;&#32447;&#22270;&#65292;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#25104;&#20026;&#36825;&#19968;&#28608;&#21160;&#20154;&#24515;&#30340;&#26032;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#21644;&#21019;&#26032;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18659v1 Announce Type: cross  Abstract: Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.
&lt;/p&gt;</description></item></channel></rss>