<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#35843;&#26597;&#20102;&#22810;&#26234;&#33021;&#20307;&#12289;&#20154;&#26234;&#33021;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#22312;&#31038;&#20250;&#22256;&#22659;&#21512;&#20316;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#35752;&#35770;&#20102;&#21512;&#20316;&#30340;&#21160;&#26426;&#12289;&#31574;&#30053;&#12289;&#20154;&#31867;&#20559;&#35265;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.17270</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#12289;&#20154;&#26234;&#33021;&#20307;&#21450;&#20854;&#36827;&#23637;&#65306;&#21512;&#20316;&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17270
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#20102;&#22810;&#26234;&#33021;&#20307;&#12289;&#20154;&#26234;&#33021;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#22312;&#31038;&#20250;&#22256;&#22659;&#21512;&#20316;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#35752;&#35770;&#20102;&#21512;&#20316;&#30340;&#21160;&#26426;&#12289;&#31574;&#30053;&#12289;&#20154;&#31867;&#20559;&#35265;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#30740;&#31350;&#21512;&#20316;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#21508;&#31181;&#23398;&#31185;&#30340;&#22522;&#26412;&#35838;&#39064;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12290;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#37325;&#22609;&#20102;&#36825;&#19968;&#39046;&#22495;&#65292;&#20026;&#29702;&#35299;&#21644;&#22686;&#24378;&#21512;&#20316;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#26412;&#35843;&#26597;&#32771;&#23519;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#31038;&#20250;&#22256;&#22659;&#21512;&#20316;&#20132;&#27719;&#22788;&#30340;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#12290;&#39318;&#20808;&#65292;&#30528;&#37325;&#20110;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#25903;&#25345;&#29702;&#24615;&#26234;&#33021;&#20307;&#20043;&#38388;&#21512;&#20316;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#21160;&#26426;&#65292;&#20197;&#21450;&#29992;&#20110;&#21046;&#23450;&#26377;&#25928;&#31574;&#30053;&#23545;&#25239;&#19981;&#21516;&#23545;&#25163;&#30340;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25506;&#35752;&#20102;&#20154;&#26234;&#33021;&#20307;&#21512;&#20316;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#29992;&#20110;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#30340;&#20559;&#35265;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#22686;&#24378;&#20154;&#31867;&#21512;&#20316;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20363;&#22914; u
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17270v1 Announce Type: new  Abstract: The study of cooperation within social dilemmas has long been a fundamental topic across various disciplines, including computer science and social science. Recent advancements in Artificial Intelligence (AI) have significantly reshaped this field, offering fresh insights into understanding and enhancing cooperation. This survey examines three key areas at the intersection of AI and cooperation in social dilemmas. First, focusing on multi-agent cooperation, we review the intrinsic and external motivations that support cooperation among rational agents, and the methods employed to develop effective strategies against diverse opponents. Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with humans and the human biases towards AI agents. Third, we review the emergent field of leveraging AI agents to enhance cooperation among humans. We conclude by discussing future research avenues, such as u
&lt;/p&gt;</description></item><item><title>GigaPevt&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#19994;&#21307;&#30103;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;&#65292;&#22312;&#23545;&#35805;&#36136;&#37327;&#21644;&#24230;&#37327;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;1.18\%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16654</link><description>&lt;p&gt;
GigaPevt&#65306;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
GigaPevt: Multimodal Medical Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16654
&lt;/p&gt;
&lt;p&gt;
GigaPevt&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#19994;&#21307;&#30103;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;&#65292;&#22312;&#23545;&#35805;&#36136;&#37327;&#21644;&#24230;&#37327;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;1.18\%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#19968;&#20010;&#26234;&#33021;&#39640;&#25928;&#30340;&#21307;&#30103;&#21161;&#25163;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#12290;&#20027;&#35201;&#38480;&#21046;&#26469;&#33258;&#25968;&#25454;&#27169;&#24577;&#30340;&#31232;&#32570;&#24615;&#65292;&#38477;&#20302;&#20102;&#20840;&#38754;&#30340;&#24739;&#32773;&#24863;&#30693;&#12290;&#26412;&#28436;&#31034;&#35770;&#25991;&#20171;&#32461;&#20102;GigaPevt&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#21151;&#33021;&#21644;&#19987;&#19994;&#21307;&#30103;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23545;&#35805;&#36136;&#37327;&#21644;&#24230;&#37327;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#20351;&#24471;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.18\%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16654v1 Announce Type: cross  Abstract: Building an intelligent and efficient medical assistant is still a challenging AI problem. The major limitation comes from the data modality scarceness, which reduces comprehensive patient perception. This demo paper presents the GigaPevt, the first multimodal medical assistant that combines the dialog capabilities of large language models with specialized medical models. Such an approach shows immediate advantages in dialog quality and metric performance, with a 1.18\% accuracy improvement in the question-answering task.
&lt;/p&gt;</description></item><item><title>ScreenQA&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;86K&#20010;&#38382;&#31572;&#23545;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;&#65292;&#26088;&#22312;&#35780;&#20272;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2209.08199</link><description>&lt;p&gt;
ScreenQA: &#31227;&#21160;&#24212;&#29992;&#25130;&#22270;&#19978;&#30340;&#22823;&#35268;&#27169;&#38382;&#31572;&#23545;
&lt;/p&gt;
&lt;p&gt;
ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08199
&lt;/p&gt;
&lt;p&gt;
ScreenQA&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;86K&#20010;&#38382;&#31572;&#23545;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;&#65292;&#26088;&#22312;&#35780;&#20272;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;ScreenQA&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#26469;&#29702;&#35299;&#23631;&#24149;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#23631;&#24149;&#25968;&#25454;&#38598;&#35201;&#20040;&#20391;&#37325;&#20110;&#32467;&#26500;&#21644;&#32452;&#20214;&#32423;&#21035;&#30340;&#29702;&#35299;&#65292;&#35201;&#20040;&#20391;&#37325;&#20110;&#20687;&#23548;&#33322;&#21644;&#20219;&#21153;&#23436;&#25104;&#20043;&#31867;&#30340;&#26356;&#39640;&#32423;&#21035;&#30340;&#32452;&#21512;&#20219;&#21153;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;86K&#20010;&#38382;&#31572;&#23545;&#26469;&#24357;&#21512;&#36825;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24076;&#26395;&#33021;&#22815;&#22522;&#20934;&#21270;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.08199v2 Announce Type: replace  Abstract: We present a new task and dataset, ScreenQA, for screen content understanding via question answering. The existing screen datasets are focused either on structure and component-level understanding, or on a much higher-level composite task such as navigation and task completion. We attempt to bridge the gap between these two by annotating 86K question-answer pairs over the RICO dataset in hope to benchmark the screen reading comprehension capacity.
&lt;/p&gt;</description></item></channel></rss>