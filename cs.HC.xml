<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>VeML&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#29256;&#26412;&#31649;&#29702;&#31995;&#32479;&#65292;&#22312;&#35299;&#20915;&#29983;&#21629;&#21608;&#26399;&#39640;&#25104;&#26412;&#38382;&#39064;&#12289;&#25968;&#25454;&#30456;&#20284;&#24615;&#35745;&#31639;&#21644;&#25968;&#25454;&#27169;&#24335;&#20998;&#26512;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.13037</link><description>&lt;p&gt;
VeML&#65306;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;
&lt;/p&gt;
&lt;p&gt;
VeML: An End-to-End Machine Learning Lifecycle for Large-scale and High-dimensional Data. (arXiv:2304.13037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13037
&lt;/p&gt;
&lt;p&gt;
VeML&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#29256;&#26412;&#31649;&#29702;&#31995;&#32479;&#65292;&#22312;&#35299;&#20915;&#29983;&#21629;&#21608;&#26399;&#39640;&#25104;&#26412;&#38382;&#39064;&#12289;&#25968;&#25454;&#30456;&#20284;&#24615;&#35745;&#31639;&#21644;&#25968;&#25454;&#27169;&#24335;&#20998;&#26512;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21253;&#21547;&#35768;&#22810;&#36845;&#20195;&#36807;&#31243;&#65292;&#20174;&#25968;&#25454;&#20934;&#22791;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#21040;&#27169;&#22411;&#35757;&#32451;&#65292;&#20877;&#21040;&#37096;&#32626;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#29992;&#20110;&#25512;&#29702;&#12290;&#24403;&#26500;&#24314;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#31471;&#21040;&#31471;&#29983;&#21629;&#21608;&#26399;&#26102;&#65292;&#24517;&#39035;&#35774;&#35745;&#21644;&#25191;&#34892;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#36825;&#20250;&#20135;&#29983;&#22823;&#37327;&#30340;&#29983;&#21629;&#21608;&#26399;&#29256;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;VeML&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#29256;&#26412;&#31649;&#29702;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#35299;&#20915;&#20102;&#20854;&#20182;&#31995;&#32479;&#27809;&#26377;&#35299;&#20915;&#30340;&#20960;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#39640;&#25104;&#26412;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#35758;&#23558;&#22312;&#25105;&#20204;&#31995;&#32479;&#20013;&#31649;&#29702;&#30340;&#31867;&#20284;&#25968;&#25454;&#38598;&#30340;&#29983;&#21629;&#21608;&#26399;&#36716;&#31227;&#21040;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#12290;&#21478;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24046;&#24322;&#32780;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#27169;&#24335;&#20998;&#26512;&#26041;&#27861;&#26469;&#26816;&#27979;&#20808;&#21069;&#20351;&#29992;&#30340;&#25968;&#25454;&#21644;&#26032;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#25143;&#21487;&#20197;&#33258;&#23450;&#20041;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#24037;&#20316;&#27969;&#65292;&#24182;&#23558;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#19982;&#20854;API&#36830;&#25509;&#36215;&#26469;&#65292;&#20316;&#20026;&#29992;&#25143;&#36816;&#34892;&#33258;&#23450;&#20041;&#20195;&#30721;&#30340;&#26725;&#26753;&#12290; VeML&#24050;&#24212;&#29992;&#20110;&#22788;&#29702;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An end-to-end machine learning (ML) lifecycle consists of many iterative processes, from data preparation and ML model design to model training and then deploying the trained model for inference. When building an end-to-end lifecycle for an ML problem, many ML pipelines must be designed and executed that produce a huge number of lifecycle versions. Therefore, this paper introduces VeML, a Version management system dedicated to end-to-end ML Lifecycle. Our system tackles several crucial problems that other systems have not solved. First, we address the high cost of building an ML lifecycle, especially for large-scale and high-dimensional dataset. We solve this problem by proposing to transfer the lifecycle of similar datasets managed in our system to the new training data. We design an algorithm based on the core set to compute similarity for large-scale, high-dimensional data efficiently. Another critical issue is the model accuracy degradation by the difference between training data a
&lt;/p&gt;</description></item></channel></rss>