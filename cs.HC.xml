<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#36890;&#36807;&#26816;&#27979;&#28508;&#22312;&#24187;&#35273;&#24182;&#24314;&#35758;&#26367;&#20195;&#26041;&#26696;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#25104;&#21151;&#20943;&#23569;&#20154;&#31867;&#23548;&#33322;&#38169;&#35823;&#39640;&#36798;29%&#32780;&#19981;&#22686;&#21152;&#35748;&#30693;&#36127;&#25285;</title><link>https://arxiv.org/abs/2402.16973</link><description>&lt;p&gt;
&#36890;&#36807;&#31361;&#20986;&#28508;&#22312;&#38169;&#35823;&#24182;&#24314;&#35758;&#32416;&#27491;&#25104;&#21151;&#24341;&#23548;&#20154;&#31867;&#20570;&#20986;&#20915;&#31574;&#30340;&#19981;&#23436;&#32654;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16973
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26816;&#27979;&#28508;&#22312;&#24187;&#35273;&#24182;&#24314;&#35758;&#26367;&#20195;&#26041;&#26696;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#25104;&#21151;&#20943;&#23569;&#20154;&#31867;&#23548;&#33322;&#38169;&#35823;&#39640;&#36798;29%&#32780;&#19981;&#22686;&#21152;&#35748;&#30693;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#21033;&#29992;&#19981;&#23436;&#32654;&#35821;&#35328;&#27169;&#22411;&#26469;&#22312;&#22522;&#20110;&#23450;&#20301;&#23548;&#33322;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#24341;&#23548;&#20154;&#31867;&#20915;&#31574;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#23436;&#32654;&#30340;&#35828;&#26126;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26377;&#25928;&#30340;&#36890;&#20449;&#26426;&#21046;&#26469;&#26356;&#25104;&#21151;&#22320;&#24341;&#23548;&#20154;&#31867;&#12290;&#25105;&#20204;&#26500;&#24314;&#30340;&#36890;&#20449;&#26426;&#21046;&#21253;&#25324;&#21487;&#20197;&#26816;&#27979;&#35828;&#26126;&#20013;&#28508;&#22312;&#24187;&#35273;&#24182;&#24314;&#35758;&#23454;&#38469;&#26367;&#20195;&#26041;&#26696;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#30452;&#35266;&#30340;&#30028;&#38754;&#23558;&#35813;&#20449;&#24687;&#21576;&#29616;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#20154;&#31867;&#23548;&#33322;&#38169;&#35823;&#38477;&#20302;&#39640;&#36798;29%&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35748;&#30693;&#36127;&#25285;&#12290;&#36825;&#19968;&#32467;&#26524;&#31361;&#26174;&#20102;&#23558;&#22810;&#26679;&#21270;&#30340;&#36890;&#20449;&#28192;&#36947;&#25972;&#21512;&#21040;AI&#31995;&#32479;&#20013;&#26469;&#24357;&#34917;&#20854;&#32570;&#38519;&#24182;&#22686;&#24378;&#20854;&#23545;&#20154;&#31867;&#30340;&#23454;&#29992;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16973v1 Announce Type: new  Abstract: This paper addresses the challenge of leveraging imperfect language models to guide human decision-making in the context of a grounded navigation task. We show that an imperfect instruction generation model can be complemented with an effective communication mechanism to become more successful at guiding humans. The communication mechanism we build comprises models that can detect potential hallucinations in instructions and suggest practical alternatives, and an intuitive interface to present that information to users. We show that this approach reduces the human navigation error by up to 29% with no additional cognitive burden. This result underscores the potential of integrating diverse communication channels into AI systems to compensate for their imperfections and enhance their utility for humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#35270;&#39057;&#25968;&#25454;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#35821;&#35328;&#32534;&#30721;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.05822</link><description>&lt;p&gt;
&#32534;&#30721;-&#23384;&#20648;-&#26816;&#32034;&#65306;&#36890;&#36807;&#35821;&#35328;&#32534;&#30721;&#30340;&#33258;&#25105;&#20013;&#24515;&#24863;&#30693;&#22686;&#24378;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception. (arXiv:2308.05822v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#35270;&#39057;&#25968;&#25454;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#35821;&#35328;&#32534;&#30721;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20381;&#36182;&#20110;&#33258;&#24049;&#30340;&#35760;&#24518;&#26469;&#32534;&#30721;&#12289;&#23384;&#20648;&#21644;&#26816;&#32034;&#25105;&#20204;&#30340;&#32463;&#21382;&#12290;&#28982;&#32780;&#65292;&#35760;&#24518;&#38388;&#38548;&#26377;&#26102;&#20250;&#21457;&#29983;&#12290;&#23454;&#29616;&#35760;&#24518;&#22686;&#24378;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#22836;&#25140;&#24335;&#26174;&#31034;&#35774;&#22791;&#26469;&#25429;&#25417;&#21644;&#20445;&#30041;&#33258;&#25105;&#20013;&#24515;&#30340;&#35270;&#39057;&#65292;&#36825;&#31181;&#20570;&#27861;&#36890;&#24120;&#34987;&#31216;&#20026;&#29983;&#27963;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#25216;&#26415;&#32570;&#20047;&#39640;&#25928;&#32534;&#30721;&#21644;&#23384;&#20648;&#22914;&#27492;&#22823;&#37327;&#30340;&#35270;&#39057;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20174;&#24222;&#22823;&#30340;&#35270;&#39057;&#23384;&#26723;&#20013;&#26816;&#32034;&#29305;&#23450;&#20449;&#24687;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#22797;&#26434;&#20102;&#24555;&#36895;&#35775;&#38382;&#25152;&#38656;&#20869;&#23481;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as life logging. However, a significant challenge arises from the sheer volume of video data generated through life logging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation system that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Add
&lt;/p&gt;</description></item></channel></rss>