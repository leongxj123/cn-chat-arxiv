<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>SynthoGestures&#26159;&#19968;&#31181;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;&#21512;&#25104;&#36924;&#30495;&#25163;&#21183;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21160;&#24577;&#20154;&#26426;&#30028;&#38754;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#22810;&#31181;&#21464;&#20307;&#21644;&#27169;&#25311;&#19981;&#21516;&#25668;&#20687;&#26426;&#31867;&#22411;&#65292;&#25552;&#39640;&#20102;&#25163;&#21183;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#24182;&#33410;&#30465;&#20102;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04421</link><description>&lt;p&gt;
SynthoGestures&#65306;&#19968;&#31181;&#29992;&#20110;&#39550;&#39542;&#22330;&#26223;&#30340;&#21512;&#25104;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios. (arXiv:2309.04421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04421
&lt;/p&gt;
&lt;p&gt;
SynthoGestures&#26159;&#19968;&#31181;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;&#21512;&#25104;&#36924;&#30495;&#25163;&#21183;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21160;&#24577;&#20154;&#26426;&#30028;&#38754;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#22810;&#31181;&#21464;&#20307;&#21644;&#27169;&#25311;&#19981;&#21516;&#25668;&#20687;&#26426;&#31867;&#22411;&#65292;&#25552;&#39640;&#20102;&#25163;&#21183;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#24182;&#33410;&#30465;&#20102;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27773;&#36710;&#39046;&#22495;&#20013;&#65292;&#20026;&#21160;&#24577;&#20154;&#26426;&#30028;&#38754;&#21019;&#24314;&#22810;&#26679;&#21270;&#21644;&#20840;&#38754;&#30340;&#25163;&#21183;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#34394;&#25311;3D&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25163;&#21183;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#34394;&#24187;&#24341;&#25806;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;&#21183;&#65292;&#25552;&#20379;&#23450;&#21046;&#36873;&#39033;&#24182;&#38477;&#20302;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;&#29983;&#25104;&#22810;&#31181;&#21464;&#20307;&#65292;&#21253;&#25324;&#25163;&#21183;&#36895;&#24230;&#12289;&#24615;&#33021;&#21644;&#25163;&#24418;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27169;&#25311;&#19981;&#21516;&#30340;&#25668;&#20687;&#26426;&#20301;&#32622;&#21644;&#31867;&#22411;&#65292;&#22914;RGB&#12289;&#32418;&#22806;&#21644;&#28145;&#24230;&#25668;&#20687;&#26426;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#26102;&#38388;&#21644;&#36153;&#29992;&#33719;&#21462;&#36825;&#20123;&#25668;&#20687;&#26426;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#26694;&#26550;SynthoGestures&#25552;&#39640;&#20102;&#25163;&#21183;&#35782;&#21035;&#20934;&#30830;&#29575;&#65292;&#21487;&#20197;&#26367;&#20195;&#25110;&#22686;&#24378;&#30495;&#25163;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33410;&#30465;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#20419;&#36827;&#20102;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating a diverse and comprehensive dataset of hand gestures for dynamic human-machine interfaces in the automotive domain can be challenging and time-consuming. To overcome this challenge, we propose using synthetic gesture datasets generated by virtual 3D models. Our framework utilizes Unreal Engine to synthesize realistic hand gestures, offering customization options and reducing the risk of overfitting. Multiple variants, including gesture speed, performance, and hand shape, are generated to improve generalizability. In addition, we simulate different camera locations and types, such as RGB, infrared, and depth cameras, without incurring additional time and cost to obtain these cameras. Experimental results demonstrate that our proposed framework, SynthoGestures\footnote{\url{https://github.com/amrgomaaelhady/SynthoGestures}}, improves gesture recognition accuracy and can replace or augment real-hand datasets. By saving time and effort in the creation of the data set, our tool acc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#21452;&#27969;&#33258;&#27880;&#24847;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; DS-AGC&#65292;&#29992;&#20110;&#36328;&#20027;&#20307;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#24182;&#34892;&#27969;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#33041;&#30005;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#35299;&#20915;&#20998;&#24067;&#24046;&#24322;&#21644;&#25552;&#21462;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.11635</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#21452;&#27969;&#33258;&#27880;&#24847;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#22312;&#22522;&#20110;&#36328;&#20027;&#20307;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition. (arXiv:2308.11635v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#21452;&#27969;&#33258;&#27880;&#24847;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; DS-AGC&#65292;&#29992;&#20110;&#36328;&#20027;&#20307;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#24182;&#34892;&#27969;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#33041;&#30005;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#35299;&#20915;&#20998;&#24067;&#24046;&#24322;&#21644;&#25552;&#21462;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270; (EEG) &#26159;&#19968;&#31181;&#26377;&#30528;&#24191;&#27867;&#24212;&#29992;&#21069;&#26223;&#30340;&#23458;&#35266;&#24773;&#32490;&#35782;&#21035;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#20173;&#28982;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#22522;&#20110;&#33041;&#30005;&#30340;&#24773;&#32490;&#35782;&#21035;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#21452;&#27969;&#33258;&#27880;&#24847;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; (&#31616;&#31216;&#20026; DS-AGC)&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#36328;&#20027;&#20307;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;DS-AGC &#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#24182;&#34892;&#27969;&#65292;&#29992;&#20110;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#33041;&#30005;&#29305;&#24449;&#12290;&#38750;&#32467;&#26500;&#21270;&#27969;&#37319;&#29992;&#21322;&#30417;&#30563;&#22810;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#28304;&#22495;&#12289;&#26410;&#26631;&#35760;&#28304;&#22495;&#21644;&#26410;&#30693;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;&#32467;&#26500;&#21270;&#27969;&#21017;&#24320;&#21457;&#20102;&#19968;&#31181;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21322;&#30417;&#30563;&#26041;&#24335;&#20174;&#22810;&#20010;&#33041;&#30005;&#36890;&#36947;&#20013;&#25552;&#21462;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#33258;&#27880;&#24847;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-Attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentiv
&lt;/p&gt;</description></item></channel></rss>