<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>I-CEE&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#20102;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#22270;&#20687;&#12289;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2312.12102</link><description>&lt;p&gt;
I-CEE: &#23558;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#23450;&#21046;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
I-CEE: Tailoring Explanations of Image Classification Models to User Expertise. (arXiv:2312.12102v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12102
&lt;/p&gt;
&lt;p&gt;
I-CEE&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#20102;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#22270;&#20687;&#12289;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#23545;&#20110;&#20381;&#36182;&#23427;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36127;&#36131;&#20219;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#35782;&#21035;&#21040;&#20854;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#29983;&#25104;&#36825;&#20123;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#25552;&#20379;&#20102;&#20960;&#31181;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#30340;&#24037;&#20316;&#20013;&#65292;&#23545;&#29992;&#25143;&#65288;&#35299;&#37322;&#23545;&#35937;&#65289;&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;XAI&#25216;&#26415;&#20135;&#29983;&#30340;&#26159;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#23454;&#29616;&#26356;&#21152;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;XAI&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-CEE&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#22270;&#20687;&#20998;&#31867;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#21463;&#21040;&#29616;&#26377;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;I-CEE&#36890;&#36807;&#20026;&#29992;&#25143;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#65288;&#21363;&#31034;&#20363;&#22270;&#20687;&#65289;&#12289;&#30456;&#24212;&#30340;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#19982;&#27492;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;I-CEE&#27169;&#25311;&#20102;&#31034;&#20363;&#22270;&#20687;&#30340;&#20449;&#24687;&#37327;&#20381;&#36182;&#20110;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#20026;&#19981;&#21516;&#30340;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively explaining decisions of black-box machine learning models is critical to responsible deployment of AI systems that rely on them. Recognizing their importance, the field of explainable AI (XAI) provides several techniques to generate these explanations. Yet, there is relatively little emphasis on the user (the explainee) in this growing body of work and most XAI techniques generate "one-size-fits-all" explanations. To bridge this gap and achieve a step closer towards human-centered XAI, we present I-CEE, a framework that provides Image Classification Explanations tailored to User Expertise. Informed by existing work, I-CEE explains the decisions of image classification models by providing the user with an informative subset of training data (i.e., example images), corresponding local explanations, and model decisions. However, unlike prior work, I-CEE models the informativeness of the example images to depend on user expertise, resulting in different examples for different u
&lt;/p&gt;</description></item></channel></rss>