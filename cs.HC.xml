<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#23545;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#65292;&#35299;&#20915;&#20102;&#25214;&#21040;&#36866;&#24403;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#21452;&#31574;&#30053;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.14244</link><description>&lt;p&gt;
MENTOR&#65306;&#22312;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14244
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#23545;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#65292;&#35299;&#20915;&#20102;&#25214;&#21040;&#36866;&#24403;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#21452;&#31574;&#30053;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#20026;&#26234;&#33021;&#20307;&#30340;&#22797;&#26434;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#24182;&#20381;&#27425;&#23436;&#25104;&#30340;&#23618;&#27425;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#38590;&#20197;&#25214;&#21040;&#36866;&#24403;&#30340;&#23376;&#30446;&#26631;&#26469;&#30830;&#20445;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#25972;&#21512;&#21040;&#20854;&#20013;&#65288;MENTOR&#65289;&#12290;MENTOR&#20805;&#24403;&#8220;&#23548;&#24072;&#8221;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#32435;&#20837;&#39640;&#23618;&#31574;&#30053;&#23398;&#20064;&#20013;&#65292;&#20197;&#25214;&#21040;&#26356;&#22909;&#30340;&#23376;&#30446;&#26631;&#12290;&#33267;&#20110;&#20302;&#23618;&#31574;&#30053;&#65292;MENTOR&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#31574;&#30053;&#20197;&#20998;&#21035;&#36827;&#34892;&#25506;&#32034;-&#24320;&#21457;&#35299;&#32806;&#65292;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20154;&#31867;&#21487;&#20197;&#31616;&#21333;&#22320;&#23558;&#20219;&#21153;&#25286;&#20998;&#25104;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14244v1 Announce Type: new  Abstract: Hierarchical reinforcement learning (HRL) provides a promising solution for complex tasks with sparse rewards of intelligent agents, which uses a hierarchical framework that divides tasks into subgoals and completes them sequentially. However, current methods struggle to find suitable subgoals for ensuring a stable learning process. Without additional guidance, it is impractical to rely solely on exploration or heuristics methods to determine subgoals in a large goal space. To address the issue, We propose a general hierarchical reinforcement learning framework incorporating human feedback and dynamic distance constraints (MENTOR). MENTOR acts as a "mentor", incorporating human feedback into high-level policy learning, to find better subgoals. As for low-level policy, MENTOR designs a dual policy for exploration-exploitation decoupling respectively to stabilize the training. Furthermore, although humans can simply break down tasks into s
&lt;/p&gt;</description></item></channel></rss>