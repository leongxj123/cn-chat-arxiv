<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#20154;&#20204;&#20250;&#32473;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#36171;&#20104;&#30446;&#30340;&#23646;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#26102;&#34920;&#29616;&#20986;&#23545;&#30446;&#30340;&#35770;&#35299;&#37322;&#30340;&#20542;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.08828</link><description>&lt;p&gt;
&#24403;&#35299;&#37322;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#26102;&#65292;&#20154;&#20204;&#20250;&#32473;&#20104;&#20854;&#23646;&#24615;&#30446;&#30340;
&lt;/p&gt;
&lt;p&gt;
People Attribute Purpose to Autonomous Vehicles When Explaining Their Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08828
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#20250;&#32473;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#36171;&#20104;&#30446;&#30340;&#23646;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#26102;&#34920;&#29616;&#20986;&#23545;&#30446;&#30340;&#35770;&#35299;&#37322;&#30340;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27454;&#20248;&#31168;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26631;&#24535;&#26159;&#29992;&#25143;&#21487;&#20197;&#29702;&#35299;&#24182;&#37319;&#21462;&#34892;&#21160;&#30340;&#35299;&#37322;&#12290;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#38656;&#35201;&#31995;&#32479;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#22240;&#26524;&#25110;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#35748;&#30693;&#31185;&#23398;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#29992;&#25143;&#21487;&#33021;&#26399;&#26395;&#30340;&#35299;&#37322;&#31867;&#22411;&#65292;&#20197;&#21450;&#22312;&#21738;&#31181;&#26684;&#24335;&#19979;&#21576;&#29616;&#36825;&#20123;&#35299;&#37322;&#12290;&#26412;&#25991;&#31616;&#35201;&#22238;&#39038;&#20102;&#35748;&#30693;&#31185;&#23398;&#35299;&#37322;&#26041;&#38754;&#30340;&#30456;&#20851;&#25991;&#29486;&#65292;&#29305;&#21035;&#20851;&#27880;&#30446;&#30340;&#35770;&#65292;&#21363;&#20197;&#36798;&#21040;&#30446;&#30340;&#20026;&#35299;&#37322;&#20915;&#31574;&#30340;&#20542;&#21521;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#20154;&#20204;&#22914;&#20309;&#20026;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#20135;&#29983;&#35299;&#37322;&#20197;&#21450;&#20182;&#20204;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#32463;&#39564;&#25968;&#25454;&#12290;&#22312;&#31532;&#19968;&#39033;&#35843;&#26597;&#20013;&#65292;&#21442;&#19982;&#32773;&#65288;n = 54&#65289;&#35266;&#30475;&#20102;&#36947;&#36335;&#22330;&#26223;&#30340;&#35270;&#39057;&#65292;&#24182;&#34987;&#35201;&#27714;&#20026;&#36710;&#36742;&#30340;&#34892;&#20026;&#29983;&#25104;&#26426;&#26800;&#30340;&#12289;&#21453;&#20107;&#23454;&#30340;&#25110;&#30446;&#30340;&#35770;&#30340;&#35328;&#35821;&#35299;&#37322;&#12290;&#22312;&#31532;&#20108;&#39033;&#35843;&#26597;&#20013;&#65292;&#21478;&#19968;&#32452;&#21442;&#19982;&#32773;&#65288;n = 356&#65289;&#23545;&#36825;&#20123;&#36827;&#34892;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08828v1 Announce Type: cross  Abstract: A hallmark of a good XAI system is explanations that users can understand and act on. In many cases, this requires a system to offer causal or counterfactual explanations that are intelligible. Cognitive science can help us understand what kinds of explanations users might expect, and in which format to frame these explanations. We briefly review relevant literature from the cognitive science of explanation, particularly as it concerns teleology, the tendency to explain a decision in terms of the purpose it was meant to achieve. We then report empirical data on how people generate explanations for the behavior of autonomous vehicles, and how they evaluate these explanations. In a first survey, participants (n=54) were shown videos of a road scene and asked to generate either mechanistic, counterfactual, or teleological verbal explanations for a vehicle's actions. In the second survey, a different set of participants (n=356) rated these
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#26694;&#26550;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#25581;&#31034;&#20102;&#24403;&#20154;&#31867;&#22312;&#20915;&#31574;&#20013;&#36807;&#24230;&#20381;&#36182;AI&#26102;&#65292;&#25913;&#21892;&#20449;&#20219;&#21487;&#33021;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#26377;&#36259;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.08804</link><description>&lt;p&gt;
&#20851;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#19982;&#20934;&#30830;&#24615;&#30340;&#30456;&#20114;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making. (arXiv:2304.08804v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#26694;&#26550;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#25581;&#31034;&#20102;&#24403;&#20154;&#31867;&#22312;&#20915;&#31574;&#20013;&#36807;&#24230;&#20381;&#36182;AI&#26102;&#65292;&#25913;&#21892;&#20449;&#20219;&#21487;&#33021;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#65292;&#23558;&#20154;&#31867;&#32622;&#20110;&#20915;&#31574;&#29615;&#36335;&#20013;&#22830;&#30340;&#20027;&#35201;&#25215;&#35834;&#26159;&#65292;&#20182;&#20204;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#31526;&#21512;&#20854;&#27491;&#30830;&#30340;&#21644;&#35206;&#30422;&#20854;&#38169;&#35823;&#30340;&#24314;&#35758;&#26469;&#34917;&#20805;AI&#31995;&#32479;&#12290;&#28982;&#32780;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#30475;&#21040;&#20154;&#31867;&#20542;&#21521;&#20110;&#36807;&#24230;&#25110;&#19981;&#36275;&#22320;&#20381;&#36182;AI&#24314;&#35758;&#65292;&#36825;&#24847;&#21619;&#30528;&#20182;&#20204;&#35201;&#20040;&#20381;&#20174;&#38169;&#35823;&#30340;&#24314;&#35758;&#65292;&#35201;&#20040;&#35206;&#30422;&#27491;&#30830;&#30340;&#24314;&#35758;&#12290;&#36825;&#31181;&#20381;&#36182;&#34892;&#20026;&#23545;&#20915;&#31574;&#20934;&#30830;&#24615;&#26377;&#23475;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#24182;&#20998;&#26512;&#20102;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#26694;&#26550;&#65292;&#20351;&#36825;&#31181;&#30456;&#20114;&#20851;&#31995;&#26356;&#21152;&#20855;&#20307;&#21270;&#12290;&#35813;&#26694;&#26550;&#24110;&#21161;&#25105;&#20204;&#35299;&#37322;&#21644;&#27604;&#36739;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#33719;&#24471;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#24178;&#39044;&#65288;&#20363;&#22914;&#35299;&#37322;&#65289;&#24433;&#21709;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20174;&#26694;&#26550;&#20013;&#25512;&#20986;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#23646;&#24615;&#65306;&#65288;i&#65289;&#24403;&#20154;&#31867;&#19981;&#36275;&#22320;&#20381;&#36182;AI&#24314;&#35758;&#26102;&#65292;&#25913;&#21892;&#20449;&#20219;&#23558;&#26174;&#30528;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#20182;&#20204;&#36807;&#24230;&#20381;&#36182;&#26102;&#65292;&#20449;&#20219;&#30340;&#25913;&#21892;&#21364;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In AI-assisted decision-making, a central promise of putting a human in the loop is that they should be able to complement the AI system by adhering to its correct and overriding its mistaken recommendations. In practice, however, we often see that humans tend to over- or under-rely on AI recommendations, meaning that they either adhere to wrong or override correct recommendations. Such reliance behavior is detrimental to decision-making accuracy. In this work, we articulate and analyze the interdependence between reliance behavior and accuracy in AI-assisted decision-making, which has been largely neglected in prior work. We also propose a visual framework to make this interdependence more tangible. This framework helps us interpret and compare empirical findings, as well as obtain a nuanced understanding of the effects of interventions (e.g., explanations) in AI-assisted decision-making. Finally, we infer several interesting properties from the framework: (i) when humans under-rely o
&lt;/p&gt;</description></item></channel></rss>