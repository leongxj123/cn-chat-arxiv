<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#19981;&#21516;&#30340;&#26631;&#35760;&#26041;&#27861;&#23545;&#25968;&#25454;&#36136;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26377;&#30452;&#25509;&#24433;&#21709;&#65292;&#21407;&#20301;&#26041;&#27861;&#20135;&#29983;&#30340;&#26631;&#31614;&#36739;&#23569;&#20294;&#26356;&#31934;&#30830;&#12290;</title><link>https://arxiv.org/abs/2305.08752</link><description>&lt;p&gt;
&#27880;&#37322;&#38382;&#39064;&#65306;&#26469;&#33258;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#21407;&#20301;&#21644;&#33258;&#25105;&#22238;&#24518;&#27963;&#21160;&#27880;&#37322;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Matter of Annotation: An Empirical Study on In Situ and Self-Recall Activity Annotations from Wearable Sensors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.08752
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#26631;&#35760;&#26041;&#27861;&#23545;&#25968;&#25454;&#36136;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26377;&#30452;&#25509;&#24433;&#21709;&#65292;&#21407;&#20301;&#26041;&#27861;&#20135;&#29983;&#30340;&#26631;&#31614;&#36739;&#23569;&#20294;&#26356;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#23545;&#20174;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#20013;&#26816;&#27979;&#20154;&#31867;&#27963;&#21160;&#30340;&#30740;&#31350;&#26159;&#19968;&#20010;&#39640;&#24230;&#27963;&#36291;&#30340;&#39046;&#22495;&#65292;&#20351;&#35768;&#22810;&#24212;&#29992;&#21463;&#30410;&#65292;&#20174;&#36890;&#36807;&#20581;&#24247;&#25252;&#29702;&#24739;&#32773;&#30340;&#27493;&#34892;&#30417;&#27979;&#21040;&#20581;&#36523;&#25351;&#23548;&#20877;&#21040;&#31616;&#21270;&#25163;&#24037;&#20316;&#19994;&#27969;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#22312;&#37326;&#22806;&#25968;&#25454;&#29992;&#25143;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;4&#31181;&#19981;&#21516;&#24120;&#29992;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#29992;&#25143;&#39537;&#21160;&#30340;&#12289;&#21407;&#20301;&#27880;&#37322;-&#21363;&#22312;&#35760;&#24405;&#27963;&#21160;&#20043;&#21069;&#25110;&#26399;&#38388;&#25191;&#34892;&#30340;&#27880;&#37322;-&#21644;&#22238;&#24518;&#26041;&#27861;-&#21442;&#19982;&#32773;&#22312;&#24403;&#22825;&#32467;&#26463;&#26102;&#36861;&#28335;&#22320;&#23545;&#20854;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#26631;&#35760;&#26041;&#27861;&#30452;&#25509;&#24433;&#21709;&#27880;&#37322;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#30456;&#24212;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#21407;&#20301;&#26041;&#27861;&#20135;&#29983;&#30340;&#26631;&#31614;&#36739;&#23569;&#65292;&#20294;&#26356;&#31934;&#30830;&#65292;&#32780;&#22238;&#24518;&#26041;&#27861;&#20135;&#29983;&#30340;&#26631;&#31614;&#36739;&#22810;&#65292;&#20294;&#19981;&#22815;&#31934;&#30830;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#26412;&#27963;&#21160;&#26085;&#35760;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.08752v2 Announce Type: replace-cross  Abstract: Research into the detection of human activities from wearable sensors is a highly active field, benefiting numerous applications, from ambulatory monitoring of healthcare patients via fitness coaching to streamlining manual work processes. We present an empirical study that compares 4 different commonly used annotation methods utilized in user studies that focus on in-the-wild data. These methods can be grouped in user-driven, in situ annotations - which are performed before or during the activity is recorded - and recall methods - where participants annotate their data in hindsight at the end of the day. Our study illustrates that different labeling methodologies directly impact the annotations' quality, as well as the capabilities of a deep learning classifier trained with the data respectively. We noticed that in situ methods produce less but more precise labels than recall methods. Furthermore, we combined an activity diary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#19987;&#29992;&#36866;&#37197;&#22120;&#30340;&#26102;&#38388;-&#39057;&#35889;&#34701;&#21512;Transformer (TSformer-SA) &#29992;&#20110;&#22686;&#24378;RSVP-BCI&#35299;&#30721;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#22810;&#35270;&#22270;&#20449;&#24687;&#24182;&#20943;&#23569;&#20934;&#22791;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#35299;&#30721;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.06340</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#19987;&#29992;&#36866;&#37197;&#22120;&#30340;&#26102;&#38388;-&#39057;&#35889;&#34701;&#21512;Transformer&#29992;&#20110;&#22686;&#24378;RSVP-BCI&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
A Temporal-Spectral Fusion Transformer with Subject-specific Adapter for Enhancing RSVP-BCI Decoding. (arXiv:2401.06340v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#19987;&#29992;&#36866;&#37197;&#22120;&#30340;&#26102;&#38388;-&#39057;&#35889;&#34701;&#21512;Transformer (TSformer-SA) &#29992;&#20110;&#22686;&#24378;RSVP-BCI&#35299;&#30721;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#22810;&#35270;&#22270;&#20449;&#24687;&#24182;&#20943;&#23569;&#20934;&#22791;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#35299;&#30721;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20018;&#32852;&#35270;&#35273;&#21576;&#29616;&#65288;RSVP&#65289;&#22522;&#20110;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#33041;&#30005;&#20449;&#21495;&#36827;&#34892;&#30446;&#26631;&#26816;&#32034;&#30340;&#39640;&#25928;&#25216;&#26415;&#12290;&#20256;&#32479;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#25913;&#36827;&#20381;&#36182;&#20110;&#22823;&#37327;&#26469;&#33258;&#26032;&#27979;&#35797;&#23545;&#35937;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#22686;&#21152;&#20102;BCI&#31995;&#32479;&#30340;&#20934;&#22791;&#26102;&#38388;&#12290;&#19968;&#20123;&#30740;&#31350;&#24341;&#20837;&#20102;&#26469;&#33258;&#29616;&#26377;&#23545;&#35937;&#30340;&#25968;&#25454;&#20197;&#20943;&#23569;&#24615;&#33021;&#25913;&#36827;&#23545;&#26032;&#23545;&#35937;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#20294;&#23427;&#20204;&#22522;&#20110;&#23545;&#25239;&#23398;&#20064;&#30340;&#20248;&#21270;&#31574;&#30053;&#20197;&#21450;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#22686;&#21152;&#20102;&#20934;&#22791;&#36807;&#31243;&#20013;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#26041;&#27861;&#21482;&#20851;&#27880;&#33041;&#30005;&#20449;&#21495;&#30340;&#21333;&#35270;&#22270;&#20449;&#24687;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#35270;&#22270;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#20026;&#20102;&#22312;&#20943;&#23569;&#20934;&#22791;&#26102;&#38388;&#30340;&#21516;&#26102;&#25552;&#39640;&#35299;&#30721;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20027;&#39064;&#19987;&#29992;&#36866;&#37197;&#22120;&#30340;&#26102;&#38388;-&#39057;&#35889;&#34701;&#21512;Transformer&#65288;TSformer-SA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface (BCI) is an efficient technology for target retrieval using electroencephalography (EEG) signals. The performance improvement of traditional decoding methods relies on a substantial amount of training data from new test subjects, which increases preparation time for BCI systems. Several studies introduce data from existing subjects to reduce the dependence of performance improvement on data from new subjects, but their optimization strategy based on adversarial learning with extensive data increases training time during the preparation procedure. Moreover, most previous methods only focus on the single-view information of EEG signals, but ignore the information from other views which may further improve performance. To enhance decoding performance while reducing preparation time, we propose a Temporal-Spectral fusion transformer with Subject-specific Adapter (TSformer-SA). Specifically, a cross-view interaction 
&lt;/p&gt;</description></item></channel></rss>