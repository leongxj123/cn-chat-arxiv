<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#36890;&#36807;&#38142;&#24335;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#12289;&#20010;&#24615;&#21270;&#25351;&#23548;&#21644;&#28789;&#27963;&#27979;&#39564;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2309.08112</link><description>&lt;p&gt;
&#36890;&#36807;&#38142;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31169;&#20154;&#36741;&#23548;
&lt;/p&gt;
&lt;p&gt;
Empowering Private Tutoring by Chaining Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08112
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38142;&#24335;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#12289;&#20010;&#24615;&#21270;&#25351;&#23548;&#21644;&#28789;&#27963;&#27979;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#34987;&#24212;&#29992;&#20110;&#22312;&#32447;&#25945;&#32946;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20197;&#20419;&#36827;&#25945;&#23398;&#21644;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#23436;&#25972;&#30340;AI&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#30001;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#28085;&#30422;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#21644;&#35843;&#25972;&#12289;&#23450;&#21046;&#25351;&#23548;&#20197;&#21450;&#28789;&#27963;&#30340;&#27979;&#39564;&#35780;&#20272;&#12290;&#20026;&#20102;&#20351;&#31995;&#32479;&#33021;&#22815;&#32463;&#21463;&#20303;&#38271;&#26102;&#38388;&#20132;&#20114;&#24182;&#28385;&#36275;&#20010;&#24615;&#21270;&#25945;&#32946;&#30340;&#38656;&#27714;&#65292;&#31995;&#32479;&#34987;&#20998;&#35299;&#20026;&#19977;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#26680;&#24515;&#27969;&#31243;-&#20132;&#20114;&#12289;&#21453;&#24605;&#21644;&#21453;&#24212;&#12290;&#27599;&#20010;&#27969;&#31243;&#37117;&#36890;&#36807;&#38142;&#25509;LLM&#39537;&#21160;&#30340;&#24037;&#20855;&#20197;&#21450;&#21160;&#24577;&#26356;&#26032;&#30340;&#35760;&#24518;&#27169;&#22359;&#26469;&#23454;&#29616;&#12290;&#24037;&#20855;&#26159;LLMs&#65292;&#34987;&#25552;&#31034;&#25191;&#34892;&#19968;&#39033;&#29305;&#23450;&#20219;&#21153;&#65292;&#32780;&#35760;&#24518;&#26159;&#22312;&#25945;&#32946;&#36807;&#31243;&#20013;&#26356;&#26032;&#30340;&#25968;&#25454;&#23384;&#20648;&#12290;&#23398;&#20064;&#26085;&#24535;&#20013;&#30340;&#32479;&#35745;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.08112v1 Announce Type: cross  Abstract: Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches has been made toward a complete AI-powered tutoring system. In this work, we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs), covering automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation. To make the system robust to prolonged interaction and cater to individualized education, the system is decomposed into three inter-connected core processes-interaction, reflection, and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. Tools are LLMs prompted to execute one specific task at a time, while memories are data storage that gets updated during education process. Statistical results from learning logs demonstrate the effec
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.05631</link><description>&lt;p&gt;
DrawTalking&#65306;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05631
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;DrawTalking&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#12290;&#23427;&#24378;&#35843;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#31867;&#20284;&#32534;&#31243;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;iPad&#19978;&#23454;&#29616;&#20102;&#23427;&#12290;&#19968;&#39033;&#24320;&#25918;&#24335;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#35768;&#22810;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#29992;&#20363;&#30456;&#22865;&#21512;&#21644;&#36866;&#29992;&#12290;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#21644;&#25351;&#23548;&#26410;&#26469;&#33258;&#28982;&#29992;&#25143;&#20013;&#24515;&#30028;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
&lt;/p&gt;</description></item><item><title>&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#65288;HAII&#65289;&#21313;&#20998;&#37325;&#35201;&#65292;&#28982;&#32780;&#30446;&#21069;&#23545;&#20110;HAII&#30340;&#30740;&#31350;&#38646;&#25955;&#19988;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#20010;&#19968;&#33268;&#20351;&#29992;&#30340;&#26415;&#35821;&#25551;&#36848;HAII&#65292;&#24182;&#19988;&#20116;&#20010;&#22240;&#32032;&#24433;&#21709;HAII&#65292;&#21363;&#29992;&#25143;&#29305;&#24449;&#21644;&#32972;&#26223;&#12289;AI&#30028;&#38754;&#21644;&#21151;&#33021;&#12289;&#29992;&#25143;&#20449;&#20219;&#12289;&#30417;&#25511;&#21644;&#21453;&#39304;&#65292;&#20197;&#21450;&#24037;&#20316;&#19978;&#30340;&#22242;&#38431;&#32467;&#26500;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25913;&#36827;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30340;HAII&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.03392</link><description>&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review. (arXiv:2310.03392v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03392
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#65288;HAII&#65289;&#21313;&#20998;&#37325;&#35201;&#65292;&#28982;&#32780;&#30446;&#21069;&#23545;&#20110;HAII&#30340;&#30740;&#31350;&#38646;&#25955;&#19988;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#20010;&#19968;&#33268;&#20351;&#29992;&#30340;&#26415;&#35821;&#25551;&#36848;HAII&#65292;&#24182;&#19988;&#20116;&#20010;&#22240;&#32032;&#24433;&#21709;HAII&#65292;&#21363;&#29992;&#25143;&#29305;&#24449;&#21644;&#32972;&#26223;&#12289;AI&#30028;&#38754;&#21644;&#21151;&#33021;&#12289;&#29992;&#25143;&#20449;&#20219;&#12289;&#30417;&#25511;&#21644;&#21453;&#39304;&#65292;&#20197;&#21450;&#24037;&#20316;&#19978;&#30340;&#22242;&#38431;&#32467;&#26500;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25913;&#36827;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30340;HAII&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#65288;HAII&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19981;&#33021;&#20570;&#21040;&#36825;&#19968;&#28857;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21644;&#33268;&#21629;&#30340;&#21518;&#26524;&#12290;&#23613;&#31649;&#22914;&#27492;&#32039;&#36843;&#65292;&#20851;&#20110;HAII&#30340;&#30740;&#31350;&#24456;&#23569;&#19988;&#38646;&#25955;&#65292;&#19988;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#25913;&#36827;&#30740;&#31350;&#26368;&#20339;&#23454;&#36341;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#35843;&#26597;&#20998;&#20026;&#20197;&#19979;&#30740;&#31350;&#39046;&#22495;&#65306;&#65288;1&#65289;&#29992;&#20110;&#25551;&#36848;HAII&#30340;&#26415;&#35821;&#65292;&#65288;2&#65289;AI-enabled&#31995;&#32479;&#30340;&#20027;&#35201;&#35282;&#33394;&#65292;&#65288;3&#65289;&#24433;&#21709;HAII&#30340;&#22240;&#32032;&#65292;&#20197;&#21450;&#65288;4&#65289;&#22914;&#20309;&#34913;&#37327;HAII&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#36825;&#20123;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#20351;&#29992;&#30340;AI-enabled&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#25104;&#29087;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25991;&#29486;&#20013;&#27809;&#26377;&#19968;&#20010;&#26415;&#35821;&#34987;&#19968;&#33268;&#20351;&#29992;&#26469;&#25551;&#36848;HAII&#65292;&#32780;&#19968;&#20123;&#26415;&#35821;&#20855;&#26377;&#22810;&#20010;&#21547;&#20041;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#25991;&#29486;&#65292;&#26377;&#20116;&#20010;&#22240;&#32032;&#24433;&#21709;HAII&#65306;&#29992;&#25143;&#29305;&#24449;&#21644;&#32972;&#26223;&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#20010;&#24615;&#65292;&#24863;&#30693;&#65289;&#65292;AI&#30028;&#38754;&#21644;&#21151;&#33021;&#65288;&#20363;&#22914;&#65292;
&lt;/p&gt;
&lt;p&gt;
Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, what little research there is on HAII is fragmented and inconsistent. We present here a survey of that literature and recommendations for research best practices that will improve the field. We divided our investigation into the following research areas: (1) terms used to describe HAII, (2) primary roles of AI-enabled systems, (3) factors that influence HAII, and (4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, five factors influence HAII: user characteristics and background (e.g., user personality, perceptions), AI interface and features (e.g., in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#20197;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#23545;&#27604;PCA&#21644;&#23792;&#24230;&#25237;&#24433;&#36861;&#36394;&#20004;&#20010;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25490;&#38500;&#20808;&#39564;&#20449;&#24687;&#30456;&#20851;&#30340;&#32467;&#26500;&#24182;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2309.14857</link><description>&lt;p&gt;
&#20351;&#29992;&#20449;&#24687;&#27969;&#24418;&#25237;&#24433;&#36827;&#34892;&#32858;&#31867;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Cluster Exploration using Informative Manifold Projections. (arXiv:2309.14857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#20197;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#23545;&#27604;PCA&#21644;&#23792;&#24230;&#25237;&#24433;&#36861;&#36394;&#20004;&#20010;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25490;&#38500;&#20808;&#39564;&#20449;&#24687;&#30456;&#20851;&#30340;&#32467;&#26500;&#24182;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#32500;&#26159;&#21487;&#35270;&#21270;&#25506;&#32034;&#39640;&#32500;&#25968;&#25454;&#21644;&#21457;&#29616;&#20854;&#22312;&#20108;&#32500;&#25110;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#30340;&#20851;&#38190;&#24037;&#20855;&#20043;&#19968;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#22823;&#37096;&#20998;&#38477;&#32500;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#23454;&#36341;&#32773;&#21487;&#33021;&#23545;&#25152;&#32771;&#34385;&#25968;&#25454;&#38598;&#30340;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#19981;&#20165;&#25490;&#38500;&#19982;&#20808;&#39564;&#30693;&#35782;&#30456;&#20851;&#30340;&#32467;&#26500;&#65292;&#32780;&#19988;&#26088;&#22312;&#25581;&#31034;&#20219;&#20309;&#21097;&#20313;&#30340;&#28508;&#22312;&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#20010;&#30446;&#26631;&#30340;&#32447;&#24615;&#32452;&#21512;&#65306;&#39318;&#20808;&#26159;&#23545;&#27604;PCA&#65292;&#33021;&#22815;&#28040;&#38500;&#19982;&#20808;&#39564;&#20449;&#24687;&#30456;&#20851;&#30340;&#32467;&#26500;&#65292;&#20854;&#27425;&#26159;&#23792;&#24230;&#25237;&#24433;&#36861;&#36394;&#65292;&#21487;&#20197;&#30830;&#20445;&#22312;&#24471;&#21040;&#30340;&#23884;&#20837;&#20013;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#23450;&#20041;&#20026;&#27969;&#24418;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#32771;&#34385;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality reduction (DR) is one of the key tools for the visual exploration of high-dimensional data and uncovering its cluster structure in two- or three-dimensional spaces. The vast majority of DR methods in the literature do not take into account any prior knowledge a practitioner may have regarding the dataset under consideration. We propose a novel method to generate informative embeddings which not only factor out the structure associated with different kinds of prior knowledge but also aim to reveal any remaining underlying structure. To achieve this, we employ a linear combination of two objectives: firstly, contrastive PCA that discounts the structure associated with the prior information, and secondly, kurtosis projection pursuit which ensures meaningful data separation in the obtained embeddings. We formulate this task as a manifold optimization problem and validate it empirically across a variety of datasets considering three distinct types of prior knowledge. Lastly, 
&lt;/p&gt;</description></item><item><title>FreeDrag&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;DragGAN&#22312;&#28857;&#36861;&#36394;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2307.04684</link><description>&lt;p&gt;
FreeDrag: &#28857;&#36861;&#36394;&#24182;&#19981;&#36866;&#29992;&#20110;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
FreeDrag: Point Tracking is Not What You Need for Interactive Point-based Image Editing. (arXiv:2307.04684v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04684
&lt;/p&gt;
&lt;p&gt;
FreeDrag&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;DragGAN&#22312;&#28857;&#36861;&#36394;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#28385;&#36275;&#22270;&#20687;&#32534;&#36753;&#30340;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#38656;&#27714;&#65292;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#31934;&#30830;&#21644;&#28789;&#27963;&#30340;&#25805;&#32437;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#65292;DragGAN&#36890;&#36807;&#22522;&#20110;&#28857;&#30340;&#25805;&#32437;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;DragGAN&#22312;&#28857;&#30340;&#36861;&#36394;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#21253;&#25324;&#38169;&#35823;&#36861;&#36394;&#21644;&#27169;&#31946;&#36861;&#36394;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FreeDrag&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;DragGAN&#20013;&#28857;&#36861;&#36394;&#30340;&#36127;&#25285;&#12290;FreeDrag&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;DragGAN&#65292;&#24182;&#33021;&#22312;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#22256;&#38590;&#24773;&#26223;&#19979;&#23454;&#29616;&#31283;&#23450;&#30340;&#22522;&#20110;&#28857;&#30340;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable. Recently, DragGAN has achieved impressive editing results through point-based manipulation. However, we have observed that DragGAN struggles with miss tracking, where DragGAN encounters difficulty in effectively tracking the desired handle points, and ambiguous tracking, where the tracked points are situated within other regions that bear resemblance to the handle points. To deal with the above issues, we propose FreeDrag, which adopts a feature-oriented approach to free the burden on point tracking within the point-oriented methodology of DragGAN. The FreeDrag incorporates adaptive template features, line search, and fuzzy localization techniques to perform stable and efficient point-based image editing. Extensive experiments demonstrate that our method is superior to the DragGAN and enables stable point-based editing in challenging scenarios with similar st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#25972;&#21512;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#21644;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16143</link><description>&lt;p&gt;
&#20026;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#32780;&#36827;&#34892;&#30340;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications. (arXiv:2306.16143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#25972;&#21512;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#21644;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#20307;&#39564;&#65288;UX&#65289;&#26159;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#19987;&#27880;&#20110;&#25552;&#39640;&#31995;&#32479;&#29992;&#25143;&#30340;&#30452;&#35266;&#24615;&#12289;&#36879;&#26126;&#24230;&#12289;&#31616;&#27905;&#24615;&#21644;&#20449;&#20219;&#24230;&#12290;&#22823;&#22810;&#25968;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;UX&#30740;&#31350;&#37117;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21363;&#27809;&#26377;&#20851;&#27880;&#29992;&#25143;&#38656;&#27714;&#65292;&#24182;&#20165;&#20165;&#23558;&#39046;&#22495;&#29992;&#25143;&#29992;&#20110;&#21487;&#29992;&#24615;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#26356;&#20856;&#22411;&#30340;UX&#26041;&#27861;&#26159;&#20808;&#38024;&#23545;&#29992;&#25143;&#30340;&#21487;&#29992;&#24615;&#36827;&#34892;&#23450;&#21046;&#65292;&#32780;&#19981;&#26159;&#39318;&#20808;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;UX&#30740;&#31350;&#25972;&#21512;&#21040;&#24320;&#21457;&#39046;&#22495;NLP&#24212;&#29992;&#20013;&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#24335;UX&#30740;&#31350;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#21021;&#22987;&#38454;&#27573;&#65292;&#21363;&#26500;&#24605;&#21644;&#27010;&#24565;&#35780;&#20272;&#38454;&#27573;&#65292;&#20197;&#21450;&#26368;&#21518;&#19968;&#38454;&#27573;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#38024;&#23545;&#36807;&#31243;&#24037;&#19994;&#20013;&#26085;&#24120;&#25805;&#20316;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#20041;&#25628;&#32034;&#30340;&#23436;&#25972;&#21407;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
User experience (UX) is a part of human-computer interaction (HCI) research and focuses on increasing intuitiveness, transparency, simplicity, and trust for system users. Most of the UX research for machine learning (ML) or natural language processing (NLP) focuses on a data-driven methodology, i.e., it fails to focus on users' requirements, and engages domain users mainly for usability evaluation. Moreover, more typical UX methods tailor the systems towards user usability, unlike learning about the user needs first. The paper proposes a methodology for integrating generative UX research into developing domain NLP applications. Generative UX research employs domain users at the initial stages of prototype development, i.e., ideation and concept evaluation, and the last stage for evaluating the change in user value. In the case study, we report the full-cycle prototype development of a domain-specific semantic search for daily operations in the process industry. Our case study shows tha
&lt;/p&gt;</description></item></channel></rss>