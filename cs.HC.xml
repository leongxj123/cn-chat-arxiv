<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#35299;&#37322;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#24433;&#21709;&#39044;&#27979;&#22120;&#26469;&#24674;&#22797;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#38750;AI&#19987;&#23478;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2210.04723</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#32463;&#39564;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Experiential Explanations for Reinforcement Learning. (arXiv:2210.04723v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#35299;&#37322;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#24433;&#21709;&#39044;&#27979;&#22120;&#26469;&#24674;&#22797;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#38750;AI&#19987;&#23478;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#21644;&#26080;&#27861;&#35299;&#37322;&#65292;&#36825;&#20351;&#24471;&#38750;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#38590;&#20197;&#29702;&#35299;&#25110;&#24178;&#39044;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#35299;&#37322;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26049;&#36793;&#35757;&#32451;&#24433;&#21709;&#39044;&#27979;&#22120;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#24433;&#21709;&#39044;&#27979;&#22120;&#26159;&#23398;&#20064;&#22870;&#21169;&#26469;&#28304;&#22914;&#20309;&#24433;&#21709;&#20195;&#29702;&#22312;&#19981;&#21516;&#29366;&#24577;&#19979;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#24674;&#22797;&#26377;&#20851;&#31574;&#30053;&#22914;&#20309;&#21453;&#26144;&#29615;&#22659;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) systems can be complex and non-interpretable, making it challenging for non-AI experts to understand or intervene in their decisions. This is due, in part, to the sequential nature of RL in which actions are chosen because of future rewards. However, RL agents discard the qualitative features of their training, making it hard to recover user-understandable information for "why" an action is chosen. Proposed sentence chunking: We propose a technique Experiential Explanations to generate counterfactual explanations by training influence predictors alongside the RL policy. Influence predictors are models that learn how sources of reward affect the agent in different states, thus restoring information about how the policy reflects the environment. A human evaluation study revealed that participants presented with experiential explanations were better able to correctly guess what an agent would do than those presented with other standard types of explanations. Pa
&lt;/p&gt;</description></item></channel></rss>