<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24110;&#21161;&#29992;&#25143;&#21028;&#26029;&#26631;&#39064;&#20934;&#30830;&#24615;&#21644;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#26041;&#38754;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#35823;&#23548;&#29992;&#25143;&#23545;&#30495;&#23454;&#26631;&#39064;&#30340;&#20449;&#20208;&#65292;&#24182;&#22686;&#21152;&#23545;&#26410;&#30830;&#23450;&#34394;&#20551;&#26631;&#39064;&#30340;&#20449;&#20208;&#12290;</title><link>http://arxiv.org/abs/2308.10800</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#26080;&#25928;&#19988;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#24615;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is ineffective and potentially harmful for fact checking. (arXiv:2308.10800v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10800
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24110;&#21161;&#29992;&#25143;&#21028;&#26029;&#26631;&#39064;&#20934;&#30830;&#24615;&#21644;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#26041;&#38754;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#35823;&#23548;&#29992;&#25143;&#23545;&#30495;&#23454;&#26631;&#39064;&#30340;&#20449;&#20208;&#65292;&#24182;&#22686;&#21152;&#23545;&#26410;&#30830;&#23450;&#34394;&#20551;&#26631;&#39064;&#30340;&#20449;&#20208;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#26159;&#23545;&#25239;&#38169;&#35823;&#20449;&#24687;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#20294;&#26159;&#23427;&#22312;&#35268;&#27169;&#19978;&#30340;&#23454;&#26045;&#21463;&#21040;&#20102;&#32593;&#32476;&#19978;&#20449;&#24687;&#36807;&#20110;&#24222;&#22823;&#30340;&#38459;&#30861;&#12290;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#20204;&#22312;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#30340;&#20107;&#23454;&#26680;&#26597;&#20449;&#24687;&#26102;&#30340;&#20316;&#29992;&#26426;&#21046;&#24182;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#39044;&#20808;&#30331;&#35760;&#30340;&#38543;&#26426;&#23545;&#29031;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19968;&#27454;&#28909;&#38376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#20107;&#23454;&#26680;&#26597;&#23545;&#25919;&#27835;&#26032;&#38395;&#20449;&#20208;&#21644;&#20998;&#20139;&#24847;&#22270;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#35813;&#20154;&#24037;&#26234;&#33021;&#22312;&#25581;&#31359;&#34394;&#20551;&#26631;&#39064;&#26041;&#38754;&#34920;&#29616;&#24471;&#30456;&#24403;&#19981;&#38169;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#24182;&#27809;&#26377;&#23545;&#21442;&#19982;&#32773;&#35782;&#21035;&#26631;&#39064;&#20934;&#30830;&#24615;&#25110;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#30340;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#20107;&#23454;&#26680;&#26597;&#22120;&#20855;&#26377;&#21361;&#23475;&#24615;&#65306;&#23558;&#19968;&#20123;&#30495;&#23454;&#26631;&#39064;&#35823;&#26631;&#20026;&#34394;&#20551;&#20250;&#38477;&#20302;&#23545;&#20854;&#30340;&#20449;&#20208;&#65292;&#32780;&#23545;&#20854;&#26410;&#30830;&#23450;&#30340;&#34394;&#20551;&#26631;&#39064;&#21017;&#20250;&#22686;&#21152;&#23545;&#20854;&#30340;&#20449;&#20208;&#12290;&#22312;&#31215;&#26497;&#26041;&#38754;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#27491;&#30830;&#26631;&#23450;&#26631;&#39064;&#30340;&#20998;&#20139;&#24847;&#24895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. Recent artificial intelligence (AI) language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. Here we investigate the impact of fact checks generated by a popular AI model on belief in, and sharing intent of, political news in a preregistered randomized control experiment. Although the AI performs reasonably well in debunking false headlines, we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. However, the AI fact-checker is harmful in specific cases: it decreases beliefs in true headlines that it mislabels as false and increases beliefs for false headlines that it is unsure about. On the positive side, the AI increases sharing intents for correctly labeled t
&lt;/p&gt;</description></item><item><title>DR-HAI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20114;&#21160;&#35843;&#21644;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#24322;&#65292;&#20026;&#20419;&#36827;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.14694</link><description>&lt;p&gt;
DR-HAI: &#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#20013;&#22522;&#20110;&#35770;&#35777;&#30340;&#36777;&#35777;&#35843;&#21644;
&lt;/p&gt;
&lt;p&gt;
DR-HAI: Argumentation-based Dialectical Reconciliation in Human-AI Interactions. (arXiv:2306.14694v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14694
&lt;/p&gt;
&lt;p&gt;
DR-HAI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20114;&#21160;&#35843;&#21644;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#24322;&#65292;&#20026;&#20419;&#36827;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DR-HAI&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25193;&#23637;&#20154;&#31867;&#24863;&#30693;&#35268;&#21010;&#20013;&#24120;&#29992;&#30340;&#27169;&#22411;&#35843;&#21644;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#35770;&#35777;&#30340;&#23545;&#35805;&#33539;&#24335;&#65292;DR-HAI&#33021;&#22815;&#36827;&#34892;&#20114;&#21160;&#35843;&#21644;&#65292;&#35299;&#20915;&#35299;&#37322;&#32773;&#21644;&#34987;&#35299;&#37322;&#32773;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#24322;&#12290;&#25105;&#20204;&#23545;DR-HAI&#30340;&#25805;&#20316;&#35821;&#20041;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#25551;&#36848;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#23545;&#20854;&#25928;&#26524;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DR-HAI&#20026;&#20419;&#36827;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#28508;&#21147;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DR-HAI -- a novel argumentation-based framework designed to extend model reconciliation approaches, commonly used in human-aware planning, for enhanced human-AI interaction. By adopting an argumentation-based dialogue paradigm, DR-HAI enables interactive reconciliation to address knowledge discrepancies between an explainer and an explainee. We formally describe the operational semantics of DR-HAI, provide theoretical guarantees, and empirically evaluate its efficacy. Our findings suggest that DR-HAI offers a promising direction for fostering effective human-AI interactions.
&lt;/p&gt;</description></item></channel></rss>