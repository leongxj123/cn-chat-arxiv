<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#35813;&#30740;&#31350;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20114;&#21160;&#20013;&#26159;&#21542;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#20204;&#30340;&#34892;&#20026;&#21028;&#26029;&#21644;&#27807;&#36890;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#22312;&#29983;&#25104;&#31038;&#20250;&#21487;&#25509;&#21463;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.05701</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#19982;&#20154;&#20204;&#30340;&#31038;&#20132;&#30452;&#35273;&#30456;&#19968;&#33268;&#65292;&#29992;&#20110;&#20154;&#26426;&#20114;&#21160;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05701
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20114;&#21160;&#20013;&#26159;&#21542;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#20204;&#30340;&#34892;&#20026;&#21028;&#26029;&#21644;&#27807;&#36890;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#22312;&#29983;&#25104;&#31038;&#20250;&#21487;&#25509;&#21463;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#39640;&#23618;&#27425;&#30340;&#34892;&#21160;&#35268;&#21010;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35768;&#22810;&#26426;&#22120;&#20154;&#24212;&#29992;&#28041;&#21450;&#20154;&#31867;&#30417;&#30563;&#21592;&#25110;&#21512;&#20316;&#32773;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#29983;&#25104;&#19982;&#20154;&#20204;&#20559;&#22909;&#21644;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#30340;&#31038;&#20250;&#21487;&#25509;&#21463;&#34892;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;LLMs&#26159;&#21542;&#25429;&#25417;&#21040;&#20154;&#20204;&#22312;&#20154;&#26426;&#20114;&#21160;&#65288;HRI&#65289;&#22330;&#26223;&#20013;&#34892;&#20026;&#21028;&#26029;&#21644;&#27807;&#36890;&#20559;&#22909;&#26041;&#38754;&#30340;&#30452;&#35273;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#37325;&#29616;&#20102;&#19977;&#20010;HRI&#29992;&#25143;&#30740;&#31350;&#65292;&#23558;LLMs&#30340;&#36755;&#20986;&#19982;&#30495;&#23454;&#21442;&#19982;&#32773;&#30340;&#36755;&#20986;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#22312;&#38750;&#24120;&#20986;&#33394;&#22320;&#34920;&#29616;&#65292;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#20004;&#39033;&#30740;&#31350;&#30340;&#29992;&#25143;&#31572;&#26696;&#20855;&#26377;&#24456;&#24378;&#30456;&#20851;&#24615;&#8212;&#8212;&#31532;&#19968;&#39033;&#30740;&#31350;&#28041;&#21450;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#27807;&#36890;&#20030;&#21160;&#32473;&#26426;&#22120;&#20154;&#65288;$r_s$ = 0.82&#65289;&#65292;&#31532;&#20108;&#39033;&#28041;&#21450;&#21028;&#26029;&#34892;&#20026;&#30340;&#21487;&#21462;&#24615;&#12289;&#24847;&#22270;&#24615;&#21644;&#20196;&#20154;&#24778;&#35766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05701v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly used in robotics, especially for high-level action planning. Meanwhile, many robotics applications involve human supervisors or collaborators. Hence, it is crucial for LLMs to generate socially acceptable actions that align with people's preferences and values. In this work, we test whether LLMs capture people's intuitions about behavior judgments and communication preferences in human-robot interaction (HRI) scenarios. For evaluation, we reproduce three HRI user studies, comparing the output of LLMs with that of real participants. We find that GPT-4 strongly outperforms other models, generating answers that correlate strongly with users' answers in two studies $\unicode{x2014}$ the first study dealing with selecting the most appropriate communicative act for a robot in various situations ($r_s$ = 0.82), and the second with judging the desirability, intentionality, and surprisingness of beh
&lt;/p&gt;</description></item></channel></rss>