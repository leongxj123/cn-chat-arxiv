<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>InterpretCC&#26159;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#35745;&#31639;&#21644;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20154;&#31867;&#20013;&#24515;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#38656;&#35201;&#21487;&#20449;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#35299;&#37322;&#21644;&#20934;&#30830;&#39044;&#27979;&#30340;&#20154;&#31867;&#38754;&#21521;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.02933</link><description>&lt;p&gt;
InterpretCC: &#36866;&#20110;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26465;&#20214;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
InterpretCC: Conditional Computation for Inherently Interpretable Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02933
&lt;/p&gt;
&lt;p&gt;
InterpretCC&#26159;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#35745;&#31639;&#21644;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20154;&#31867;&#20013;&#24515;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#38656;&#35201;&#21487;&#20449;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#35299;&#37322;&#21644;&#20934;&#30830;&#39044;&#27979;&#30340;&#20154;&#31867;&#38754;&#21521;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#30495;&#23454;&#19990;&#30028;&#35299;&#37322;&#24615;&#22312;&#19977;&#20010;&#26041;&#38754;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65306;1&#65289;&#38656;&#35201;&#20154;&#31867;&#20449;&#20219;&#35299;&#37322;&#30340;&#36817;&#20284;&#65288;&#20363;&#22914;&#20107;&#21518;&#26041;&#27861;&#65289;&#65307;2&#65289;&#21066;&#24369;&#20102;&#35299;&#37322;&#30340;&#21487;&#29702;&#35299;&#24615;&#65288;&#20363;&#22914;&#33258;&#21160;&#35782;&#21035;&#30340;&#29305;&#24449;&#25513;&#30721;&#65289;&#65307;3&#65289;&#21066;&#24369;&#20102;&#27169;&#22411;&#24615;&#33021;&#65288;&#20363;&#22914;&#20915;&#31574;&#26641;&#65289;&#12290;&#36825;&#20123;&#32570;&#28857;&#23545;&#20110;&#38754;&#21521;&#20154;&#31867;&#30340;&#39046;&#22495;&#65288;&#22914;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#25110;&#33258;&#28982;&#35821;&#35328;&#65289;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#36825;&#20123;&#39046;&#22495;&#38656;&#35201;&#21487;&#20449;&#30340;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InterpretCC&#65288;&#21487;&#35299;&#37322;&#30340;&#26465;&#20214;&#35745;&#31639;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#30340;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#65292;&#36890;&#36807;&#22312;&#39044;&#27979;&#20043;&#21069;&#33258;&#36866;&#24212;&#21644;&#31232;&#30095;&#22320;&#28608;&#27963;&#29305;&#24449;&#65292;&#30830;&#20445;&#20154;&#31867;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24605;&#24819;&#25193;&#23637;&#20026;&#21487;&#35299;&#37322;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#20801;&#35768;&#20154;&#20204;&#31163;&#25955;&#22320;&#25351;&#23450;&#20852;&#36259;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world interpretability for neural networks is a tradeoff between three concerns: 1) it requires humans to trust the explanation approximation (e.g. post-hoc approaches), 2) it compromises the understandability of the explanation (e.g. automatically identified feature masks), and 3) it compromises the model performance (e.g. decision trees). These shortcomings are unacceptable for human-facing domains, like education, healthcare, or natural language, which require trustworthy explanations, actionable interpretations, and accurate predictions. In this work, we present InterpretCC (interpretable conditional computation), a family of interpretable-by-design neural networks that guarantee human-centric interpretability while maintaining comparable performance to state-of-the-art models by adaptively and sparsely activating features before prediction. We extend this idea into an interpretable mixture-of-experts model, that allows humans to specify topics of interest, discretely separate
&lt;/p&gt;</description></item></channel></rss>