<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24494;&#22411;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#19978;&#23545;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#26512;&#65292;&#20165;&#38656;6k&#20010;&#21442;&#25968;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98.97%&#65292;&#36866;&#29992;&#20110;&#35782;&#21035;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#20013;&#30340;5&#20010;&#26368;&#24120;&#35265;&#24515;&#24459;&#22833;&#24120;&#31867;&#21035;</title><link>https://arxiv.org/abs/2402.10748</link><description>&lt;p&gt;
&#20215;&#20540;16&#20010;&#23383;&#30340;&#22122;&#22768;&#33410;&#25293;: &#19968;&#31181;&#29992;&#20110;&#24494;&#25511;&#21046;&#22120;&#20302;&#21151;&#29575;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#30340;&#24494;&#22411;Transformer
&lt;/p&gt;
&lt;p&gt;
A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10748
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24494;&#22411;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#19978;&#23545;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#26512;&#65292;&#20165;&#38656;6k&#20010;&#21442;&#25968;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98.97%&#65292;&#36866;&#29992;&#20110;&#35782;&#21035;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#20013;&#30340;5&#20010;&#26368;&#24120;&#35265;&#24515;&#24459;&#22833;&#24120;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#30417;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#21487;&#31359;&#25140;&#31995;&#32479;&#27491;&#22312;&#25104;&#20026;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#24191;&#27867;&#24212;&#29992;&#19988;&#26377;&#20215;&#20540;&#30340;&#36164;&#20135;&#12290;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#20998;&#26512;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#65292;&#20197;&#21450;&#26816;&#27979;&#24515;&#33039;&#29366;&#20917;&#65288;&#22914;&#24515;&#24459;&#22833;&#24120;&#65289;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;Transformer&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26159;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20294;&#22312;&#21487;&#31359;&#25140;&#39046;&#22495;&#30340;&#39640;&#25928;&#23454;&#29616;&#21364;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#35774;&#35745;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#20860;&#39038;&#36275;&#22815;&#31934;&#24230;&#21644;&#36866;&#24403;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;ECG&#20449;&#21495;&#30340;&#24494;&#22411;Transformer&#27169;&#22411;&#65292;&#20165;&#38656;&#35201;6k&#20010;&#21442;&#25968;&#65292;&#22312;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#20013;&#35782;&#21035;5&#20010;&#26368;&#24120;&#35265;&#24515;&#24459;&#22833;&#24120;&#31867;&#21035;&#26102;&#36798;&#21040;&#20102;98.97%&#30340;&#20934;&#30830;&#29575;&#65292;&#32771;&#34385;&#21040;&#23545;&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#35774;&#22791;&#36827;&#34892;&#39640;&#25928;&#25191;&#34892;&#25152;&#38656;&#30340;8&#20301;&#25972;&#25968;&#25512;&#29702;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10748v1 Announce Type: cross  Abstract: Wearable systems for the long-term monitoring of cardiovascular diseases are becoming widespread and valuable assets in diagnosis and therapy. A promising approach for real-time analysis of the electrocardiographic (ECG) signal and the detection of heart conditions, such as arrhythmia, is represented by the transformer machine learning model. Transformers are powerful models for the classification of time series, although efficient implementation in the wearable domain raises significant design challenges, to combine adequate accuracy and a suitable complexity. In this work, we present a tiny transformer model for the analysis of the ECG signal, requiring only 6k parameters and reaching 98.97% accuracy in the recognition of the 5 most common arrhythmia classes from the MIT-BIH Arrhythmia database, assessed considering 8-bit integer inference as required for efficient execution on low-power microcontroller-based devices. We explored an 
&lt;/p&gt;</description></item><item><title>GestureGPT&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#20132;&#20114;&#25163;&#21183;&#29702;&#35299;&#21644;&#23545;&#25509;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#35299;&#35835;&#25163;&#21183;&#25551;&#36848;&#24182;&#26681;&#25454;&#20132;&#20114;&#29615;&#22659;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#24847;&#22270;&#23545;&#25509;&#21040;&#20132;&#20114;&#21151;&#33021;&#19978;&#12290;</title><link>http://arxiv.org/abs/2310.12821</link><description>&lt;p&gt;
GestureGPT: &#38646;&#26679;&#26412;&#20132;&#20114;&#25163;&#21183;&#29702;&#35299;&#19982;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents. (arXiv:2310.12821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12821
&lt;/p&gt;
&lt;p&gt;
GestureGPT&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#20132;&#20114;&#25163;&#21183;&#29702;&#35299;&#21644;&#23545;&#25509;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#35299;&#35835;&#25163;&#21183;&#25551;&#36848;&#24182;&#26681;&#25454;&#20132;&#20114;&#29615;&#22659;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#24847;&#22270;&#23545;&#25509;&#21040;&#20132;&#20114;&#21151;&#33021;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#20027;&#35201;&#20851;&#27880;&#35782;&#21035;&#39044;&#23450;&#20041;&#38598;&#21512;&#20013;&#30340;&#25163;&#21183;&#65292;&#26410;&#33021;&#23558;&#36825;&#20123;&#25163;&#21183;&#19982;&#20132;&#20114;&#24335;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#20803;&#32032;&#25110;&#31995;&#32479;&#21151;&#33021;&#30456;&#36830;&#25509;&#65288;&#20363;&#22914;&#65292;&#23558;&#8220;&#31446;&#36215;&#22823;&#25287;&#25351;&#8221;&#25163;&#21183;&#19982;&#8220;&#21916;&#27426;&#8221;&#25353;&#38062;&#20851;&#32852;&#36215;&#26469;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GestureGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#25163;&#21183;&#29702;&#35299;&#21644;&#23545;&#25509;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#25163;&#21183;&#25551;&#36848;&#26681;&#25454;&#25163;&#21183;&#35270;&#39057;&#20013;&#30340;&#25163;&#37096;&#20851;&#38190;&#28857;&#22352;&#26631;&#36827;&#34892;&#24418;&#24335;&#21270;&#65292;&#24182;&#36755;&#20837;&#21040;&#25105;&#20204;&#30340;&#21452;&#20195;&#29702;&#23545;&#35805;&#31995;&#32479;&#20013;&#12290;&#19968;&#20010;&#25163;&#21183;&#20195;&#29702;&#35299;&#35835;&#36825;&#20123;&#25551;&#36848;&#65292;&#24182;&#35810;&#38382;&#26377;&#20851;&#20132;&#20114;&#29615;&#22659;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#30028;&#38754;&#12289;&#21382;&#21490;&#35760;&#24405;&#12289;&#20957;&#35270;&#25968;&#25454;&#65289;&#65292;&#19968;&#20010;&#19978;&#19979;&#25991;&#20195;&#29702;&#36127;&#36131;&#32452;&#32455;&#24182;&#25552;&#20379;&#36825;&#20123;&#20449;&#24687;&#12290;&#32463;&#36807;&#36845;&#20195;&#30340;&#20132;&#27969;&#65292;&#25163;&#21183;&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#23558;&#20854;&#23545;&#25509;&#21040;&#19968;&#20010;&#20132;&#20114;&#21151;&#33021;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#30340;&#31532;&#19968;&#35270;&#35282;&#21644;&#31532;&#19977;&#35270;&#35282;&#25163;&#21183;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25163;&#21183;&#25551;&#36848;&#27169;&#22359;&#65292;&#24182;&#22312;&#35270;&#39057;&#27969;&#21644;&#26234;&#33021;&#23478;&#23621;&#29289;&#32852;&#32593;&#25511;&#21046;&#30340;&#20004;&#20010;&#30495;&#23454;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;&#25972;&#20010;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current gesture recognition systems primarily focus on identifying gestures within a predefined set, leaving a gap in connecting these gestures to interactive GUI elements or system functions (e.g., linking a 'thumb-up' gesture to a 'like' button). We introduce GestureGPT, a novel zero-shot gesture understanding and grounding framework leveraging large language models (LLMs). Gesture descriptions are formulated based on hand landmark coordinates from gesture videos and fed into our dual-agent dialogue system. A gesture agent deciphers these descriptions and queries about the interaction context (e.g., interface, history, gaze data), which a context agent organizes and provides. Following iterative exchanges, the gesture agent discerns user intent, grounding it to an interactive function. We validated the gesture description module using public first-view and third-view gesture datasets and tested the whole system in two real-world settings: video streaming and smart home IoT control. T
&lt;/p&gt;</description></item></channel></rss>