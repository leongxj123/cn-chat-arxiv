<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#20154;&#31867;&#36777;&#35770;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#23613;&#31649;&#23427;&#20204;&#33021;&#22815;&#34701;&#20837;&#21644;&#20419;&#36827;&#20154;&#31867;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20294;&#22312;&#36777;&#35770;&#20013;&#30340;&#35828;&#26381;&#21147;&#36739;&#24369;&#12290;&#22312;&#25104;&#20026;&#21487;&#34892;&#30340;&#36777;&#25163;&#20043;&#21069;&#65292;LLMs&#38656;&#35201;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.06049</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#20154;&#31867;&#36777;&#35770;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Limits of Large Language Models in Debating Humans
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06049
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#20154;&#31867;&#36777;&#35770;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#23613;&#31649;&#23427;&#20204;&#33021;&#22815;&#34701;&#20837;&#21644;&#20419;&#36827;&#20154;&#31867;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20294;&#22312;&#36777;&#35770;&#20013;&#30340;&#35828;&#26381;&#21147;&#36739;&#24369;&#12290;&#22312;&#25104;&#20026;&#21487;&#34892;&#30340;&#36777;&#25163;&#20043;&#21069;&#65292;LLMs&#38656;&#35201;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#38543;&#21518;&#65292;&#23558;&#23427;&#20204;&#20316;&#20026;&#20154;&#24037;&#20195;&#34920;&#21644;&#26367;&#20195;&#21697;&#36827;&#34892;&#31038;&#20250;&#23398;&#23454;&#39564;&#30340;&#28508;&#22312;&#24212;&#29992;&#26159;&#19968;&#20010;&#20196;&#20154;&#28608;&#21160;&#30340;&#21069;&#26223;&#12290;&#20294;&#26159;&#36825;&#20010;&#24819;&#27861;&#26377;&#22810;&#21487;&#34892;&#21602;&#65311;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#19968;&#39033;&#39044;&#20808;&#27880;&#20876;&#30340;&#30740;&#31350;&#26469;&#27979;&#35797;&#29616;&#38454;&#27573;LLMs&#30340;&#23616;&#38480;&#24615;&#65292;&#35813;&#30740;&#31350;&#23558;&#30495;&#23454;&#30340;&#20154;&#31867;&#19982;&#25198;&#28436;&#20154;&#31867;&#30340;LLM&#20195;&#29702;&#32467;&#21512;&#36215;&#26469;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#25506;&#35752;&#36777;&#35770;&#20026;&#22522;&#30784;&#30340;&#24847;&#35265;&#20849;&#35782;&#24418;&#25104;&#22312;&#19977;&#31181;&#29615;&#22659;&#19979;&#30340;&#24773;&#20917;&#65306;&#20165;&#20154;&#31867;&#12289;&#20195;&#29702;&#21644;&#20154;&#31867;&#12289;&#20165;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;LLM&#20195;&#29702;&#23545;&#20154;&#31867;&#30340;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#36777;&#35770;&#26041;&#38754;&#30340;&#33021;&#21147;&#26159;&#21542;&#19982;&#20154;&#31867;&#30456;&#20284;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#33021;&#22815;&#34701;&#20837;&#24182;&#20419;&#36827;&#20154;&#31867;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20294;&#22312;&#36777;&#35770;&#20013;&#30340;&#35828;&#26381;&#21147;&#36739;&#24369;&#65292;&#26368;&#32456;&#34892;&#20026;&#19982;&#20154;&#31867;&#26377;&#25152;&#20559;&#31163;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#20123;&#20027;&#35201;&#32570;&#38519;&#65292;&#24182;&#39044;&#35745;&#22312;&#25104;&#20026;&#21487;&#34892;&#30340;&#36777;&#25163;&#20043;&#21069;&#65292;LLMs&#24517;&#39035;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable promise in their ability to interact proficiently with humans. Subsequently, their potential use as artificial confederates and surrogates in sociological experiments involving conversation is an exciting prospect. But how viable is this idea? This paper endeavors to test the limits of current-day LLMs with a pre-registered study integrating real people with LLM agents acting as people. The study focuses on debate-based opinion consensus formation in three environments: humans only, agents and humans, and agents only. Our goal is to understand how LLM agents influence humans, and how capable they are in debating like humans. We find that LLMs can blend in and facilitate human productivity but are less convincing in debate, with their behavior ultimately deviating from human's. We elucidate these primary failings and anticipate that LLMs must evolve further before being viable debaters.
&lt;/p&gt;</description></item></channel></rss>