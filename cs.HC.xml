<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;XAI&#35780;&#20272;&#20013;&#26368;&#26222;&#36941;&#30340;&#20154;&#20026;&#27010;&#24565;&#8212;&#8212;&#35299;&#37322;&#21512;&#29702;&#24615;&#12290;&#34429;&#28982;&#19968;&#30452;&#34987;&#21046;&#23450;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#20219;&#21153;&#30340;&#37325;&#35201;&#35780;&#20272;&#30446;&#26631;&#65292;&#20294;&#26159;&#35780;&#20272;XAI&#30340;&#21512;&#29702;&#24615;&#26377;&#26102;&#26159;&#26377;&#23475;&#30340;&#65292;&#19988;&#26080;&#27861;&#36798;&#21040;&#27169;&#22411;&#21487;&#29702;&#35299;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.17707</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#21487;&#35299;&#37322;&#24615;&#19982;&#21512;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking AI Explainability and Plausibility. (arXiv:2303.17707v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;XAI&#35780;&#20272;&#20013;&#26368;&#26222;&#36941;&#30340;&#20154;&#20026;&#27010;&#24565;&#8212;&#8212;&#35299;&#37322;&#21512;&#29702;&#24615;&#12290;&#34429;&#28982;&#19968;&#30452;&#34987;&#21046;&#23450;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#20219;&#21153;&#30340;&#37325;&#35201;&#35780;&#20272;&#30446;&#26631;&#65292;&#20294;&#26159;&#35780;&#20272;XAI&#30340;&#21512;&#29702;&#24615;&#26377;&#26102;&#26159;&#26377;&#23475;&#30340;&#65292;&#19988;&#26080;&#27861;&#36798;&#21040;&#27169;&#22411;&#21487;&#29702;&#35299;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31639;&#27861;&#31526;&#21512;&#20154;&#31867;&#20132;&#27969;&#35268;&#33539;&#65292;&#25903;&#25345;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#28385;&#36275;&#20154;&#31867;&#23545;&#20110;AI&#35299;&#37322;&#30340;&#38656;&#27714;&#65292;&#35774;&#23450;&#36866;&#24403;&#30340;&#35780;&#20272;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35299;&#37322;&#21512;&#29702;&#24615;&#65292;&#36825;&#26159;XAI&#35780;&#20272;&#20013;&#26368;&#26222;&#36941;&#30340;&#20154;&#20026;&#27010;&#24565;&#12290;&#21512;&#29702;&#24615;&#34913;&#37327;&#26426;&#22120;&#35299;&#37322;&#19982;&#20154;&#31867;&#35299;&#37322;&#30456;&#27604;&#30340;&#21512;&#29702;&#31243;&#24230;&#12290;&#21512;&#29702;&#24615;&#19968;&#30452;&#34987;&#20256;&#32479;&#22320;&#21046;&#23450;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#20219;&#21153;&#30340;&#37325;&#35201;&#35780;&#20272;&#30446;&#26631;&#12290;&#25105;&#20204;&#21453;&#23545;&#36825;&#20010;&#24819;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20248;&#21270;&#21644;&#35780;&#20272;XAI&#30340;&#21512;&#29702;&#24615;&#26377;&#26102;&#26159;&#26377;&#23475;&#30340;&#65292;&#19988;&#26080;&#27861;&#36798;&#21040;&#27169;&#22411;&#21487;&#29702;&#35299;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#30446;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35780;&#20272;XAI&#31639;&#27861;&#30340;&#21512;&#29702;&#24615;&#20250;&#35268;&#33539;&#26426;&#22120;&#35299;&#37322;&#65292;&#20197;&#34920;&#36798;&#19982;&#20154;&#31867;&#35299;&#37322;&#23436;&#20840;&#30456;&#21516;&#30340;&#20869;&#23481;&#65292;&#36825;&#20559;&#31163;&#20102;&#20154;&#31867;&#35299;&#37322;&#30340;&#22522;&#26412;&#21160;&#26426;&#65306;&#34920;&#36798;&#33258;&#24049;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Setting proper evaluation objectives for explainable artificial intelligence (XAI) is vital for making XAI algorithms follow human communication norms, support human reasoning processes, and fulfill human needs for AI explanations. In this article, we examine explanation plausibility, which is the most pervasive human-grounded concept in XAI evaluation. Plausibility measures how reasonable the machine explanation is compared to the human explanation. Plausibility has been conventionally formulated as an important evaluation objective for AI explainability tasks. We argue against this idea, and show how optimizing and evaluating XAI for plausibility is sometimes harmful, and always ineffective to achieve model understandability, transparency, and trustworthiness. Specifically, evaluating XAI algorithms for plausibility regularizes the machine explanation to express exactly the same content as human explanation, which deviates from the fundamental motivation for humans to explain: expres
&lt;/p&gt;</description></item></channel></rss>