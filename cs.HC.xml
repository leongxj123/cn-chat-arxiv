<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#65288;JITAIs&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#27979;&#35797;GPT-4&#27169;&#22411;&#20197;&#20419;&#36827;&#38376;&#35786;&#24515;&#33039;&#24247;&#22797;&#20013;&#24515;&#30340;&#24515;&#33039;&#20581;&#24247;&#20307;&#32946;&#27963;&#21160;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;450&#20010;JITAI&#20915;&#31574;&#21644;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.08658</link><description>&lt;p&gt;
&#26368;&#21518;&#30340;JITAI&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#25918;&#21450;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#20013;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#65306;&#22312;&#21069;&#30651;&#24615;&#24515;&#33039;&#24247;&#22797;&#29615;&#22659;&#20013;&#20419;&#36827;&#20307;&#32946;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#65288;JITAIs&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#27979;&#35797;GPT-4&#27169;&#22411;&#20197;&#20419;&#36827;&#38376;&#35786;&#24515;&#33039;&#24247;&#22797;&#20013;&#24515;&#30340;&#24515;&#33039;&#20581;&#24247;&#20307;&#32946;&#27963;&#21160;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;450&#20010;JITAI&#20915;&#31574;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23383;&#20581;&#24247;&#20013;&#35302;&#21457;&#21644;&#20010;&#24615;&#21270;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#65288;JITAIs&#65289;&#20869;&#23481;&#30340;&#21487;&#34892;&#24615;&#12290;JITAIs&#34987;&#35270;&#20026;&#21487;&#25345;&#32493;&#34892;&#20026;&#25913;&#21464;&#30340;&#20851;&#38190;&#26426;&#21046;&#65292;&#23558;&#24178;&#39044;&#25514;&#26045;&#26681;&#25454;&#20010;&#20307;&#30340;&#24403;&#21069;&#24773;&#22659;&#21644;&#38656;&#27714;&#36827;&#34892;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;JITAI&#23454;&#26045;&#20013;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#32570;&#20047;&#20010;&#24615;&#21270;&#12289;&#31649;&#29702;&#22810;&#21442;&#25968;&#31995;&#32479;&#22256;&#38590;&#20197;&#21450;&#25968;&#25454;&#31232;&#30095;&#24615;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#30740;&#31350;&#36890;&#36807;LLMs&#23454;&#29616;JITAI&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22312;&#38376;&#35786;&#24515;&#33039;&#24247;&#22797;&#20013;&#20419;&#36827;&#24515;&#33039;&#20581;&#24247;&#20307;&#32946;&#27963;&#21160;&#30340;&#20351;&#29992;&#26696;&#20363;&#30340;&#29616;&#20195;&#26368;&#39640;&#24615;&#33021;&#27169;&#22411;&#8220;GPT-4&#8221;&#30340;&#23454;&#20363;&#20316;&#20026;&#35302;&#21457;&#21644;&#20010;&#24615;&#21270;JITAIs&#30340;&#22522;&#30784;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#24635;&#20849;450&#20010;&#24314;&#35758;&#30340;JITAI&#20915;&#31574;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explored the viability of Large Language Models (LLMs) for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health. JITAIs are being explored as a key mechanism for sustainable behavior change, adapting interventions to an individual's current context and needs. However, traditional rule-based and machine learning models for JITAI implementation face scalability and reliability limitations, such as lack of personalization, difficulty in managing multi-parametric systems, and issues with data sparsity. To investigate JITAI implementation via LLMs, we tested the contemporary overall performance-leading model 'GPT-4' with examples grounded in the use case of fostering heart-healthy physical activity in outpatient cardiac rehabilitation. Three personas and five sets of context information per persona were used as a basis of triggering and personalizing JITAIs. Subsequently, we generated a total of 450 proposed JITAI decisions and message c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#36793;&#38469;&#28789;&#25935;&#24230;&#27169;&#22411;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#20013;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32479;&#35745;&#24314;&#27169;&#65292;&#20197;&#25511;&#21046;&#28508;&#22312;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25512;&#36831;&#21512;&#20316;&#31995;&#32479;&#26469;&#21033;&#29992;&#19981;&#21516;&#20915;&#31574;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.08824</link><description>&lt;p&gt;
&#24102;&#26377;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#28151;&#28102;&#40065;&#26834;&#30340;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Confounding-Robust Policy Improvement with Human-AI Teams. (arXiv:2310.08824v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#36793;&#38469;&#28789;&#25935;&#24230;&#27169;&#22411;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#20013;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32479;&#35745;&#24314;&#27169;&#65292;&#20197;&#25511;&#21046;&#28508;&#22312;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25512;&#36831;&#21512;&#20316;&#31995;&#32479;&#26469;&#21033;&#29992;&#19981;&#21516;&#20915;&#31574;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#26377;&#21487;&#33021;&#36890;&#36807;&#20805;&#20998;&#21457;&#25381;&#20154;&#31867;&#19987;&#23478;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#30456;&#20114;&#34917;&#20805;&#20248;&#21183;&#26469;&#25913;&#21464;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21487;&#33021;&#20250;&#30772;&#22351;&#36825;&#31181;&#21512;&#20316;&#30340;&#26377;&#25928;&#24615;&#65292;&#23548;&#33268;&#20559;&#35265;&#21644;&#19981;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#20013;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#37319;&#29992;&#36793;&#38469;&#28789;&#25935;&#24230;&#27169;&#22411;&#65288;MSM&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#19982;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32479;&#35745;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#20197;&#32771;&#34385;&#28508;&#22312;&#30340;&#21487;&#33021;&#20250;&#38544;&#34255;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#36831;&#21512;&#20316;&#26694;&#26550;&#65292;&#23558;&#36793;&#38469;&#28789;&#25935;&#24230;&#27169;&#22411;&#32435;&#20837;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#31574;&#30053;&#23398;&#20064;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#25511;&#21046;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#25512;&#36831;&#21512;&#20316;&#31995;&#32479;&#65292;&#20197;&#21033;&#29992;&#19981;&#21516;&#20154;&#31867;&#20915;&#31574;&#32773;&#30340;&#22810;&#26679;&#21270;&#19987;&#19994;&#30693;&#35782;&#12290;&#36890;&#36807;&#35843;&#25972;&#28508;&#22312;&#30340;&#20559;&#35265;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#25552;&#39640;&#21512;&#20316;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-AI collaboration has the potential to transform various domains by leveraging the complementary strengths of human experts and Artificial Intelligence (AI) systems. However, unobserved confounding can undermine the effectiveness of this collaboration, leading to biased and unreliable outcomes. In this paper, we propose a novel solution to address unobserved confounding in human-AI collaboration by employing the marginal sensitivity model (MSM). Our approach combines domain expertise with AI-driven statistical modeling to account for potential confounders that may otherwise remain hidden. We present a deferral collaboration framework for incorporating the MSM into policy learning from observational data, enabling the system to control for the influence of unobserved confounding factors. In addition, we propose a personalized deferral collaboration system to leverage the diverse expertise of different human decision-makers. By adjusting for potential biases, our proposed solution e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#35299;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#36890;&#36807;&#22270;&#35299;&#22411;&#21644;&#20551;&#35774;&#24615;&#25512;&#29702;&#65292;&#32553;&#23567;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#36890;&#36807;&#20020;&#24202;&#24212;&#29992;&#30740;&#31350;&#21644;&#24314;&#27169;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;DiagramNet&#19981;&#20165;&#33021;&#25552;&#20379;&#24544;&#23454;&#30340;&#26434;&#38899;&#24418;&#29366;&#35299;&#37322;&#65292;&#36824;&#20855;&#26377;&#36739;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#22270;&#35299;&#22411;&#35299;&#37322;&#22312;&#20020;&#24202;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#26356;&#21463;&#25512;&#23815;&#12290;</title><link>http://arxiv.org/abs/2302.01241</link><description>&lt;p&gt;
&#22270;&#35299;&#21270;&#65306;&#21033;&#29992;&#22270;&#35299;&#22411;AI&#35299;&#37322;&#23545;&#20551;&#35774;&#24615;&#28436;&#32462;&#25512;&#29702;&#30340;&#29702;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses. (arXiv:2302.01241v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#35299;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#36890;&#36807;&#22270;&#35299;&#22411;&#21644;&#20551;&#35774;&#24615;&#25512;&#29702;&#65292;&#32553;&#23567;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#36890;&#36807;&#20020;&#24202;&#24212;&#29992;&#30740;&#31350;&#21644;&#24314;&#27169;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;DiagramNet&#19981;&#20165;&#33021;&#25552;&#20379;&#24544;&#23454;&#30340;&#26434;&#38899;&#24418;&#29366;&#35299;&#37322;&#65292;&#36824;&#20855;&#26377;&#36739;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#22270;&#35299;&#22411;&#35299;&#37322;&#22312;&#20020;&#24202;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#26356;&#21463;&#25512;&#23815;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21487;&#35270;&#21270;&#24037;&#20855;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#29992;&#25143;&#36827;&#19968;&#27493;&#25512;&#29702;&#26469;&#35299;&#37322;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;XAI&#24212;&#35813;&#25903;&#25345;&#22270;&#35299;&#22411;&#21644;&#20551;&#35774;&#24615;&#25512;&#29702;&#65292;&#20197;&#20415;AI&#33021;&#22815;&#36827;&#34892;&#20551;&#35774;&#29983;&#25104;&#21644;&#35780;&#20272;&#65292;&#20174;&#32780;&#20943;&#23569;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#35299;&#21270;&#26041;&#27861;&#65292;&#20197;i)&#36827;&#34892;Peircean&#25512;&#23548;-&#28436;&#32462;&#25512;&#29702;&#65292;ii)&#36981;&#24490;&#39046;&#22495;&#24815;&#20363;&#65292;&#21644;iii)&#29992;&#22270;&#31034;&#25110;&#35821;&#35328;&#36827;&#34892;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#20020;&#24202;&#24212;&#29992;&#39046;&#22495;&#23454;&#29616;&#20102;DiagramNet&#65292;&#20197;&#39044;&#27979;&#24515;&#33039;&#21548;&#35786;&#20013;&#30340;&#24515;&#33039;&#35786;&#26029;&#65292;&#24182;&#29992;&#22522;&#20110;&#24418;&#29366;&#30340;&#26434;&#38899;&#22270;&#35299;&#36827;&#34892;&#35299;&#37322;&#12290;&#22312;&#24314;&#27169;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;DiagramNet&#19981;&#20165;&#25552;&#20379;&#20102;&#24544;&#23454;&#30340;&#26434;&#38899;&#24418;&#29366;&#35299;&#37322;&#65292;&#32780;&#19988;&#27604;&#22522;&#32447;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21307;&#23398;&#29983;&#30340;&#23450;&#24615;&#29992;&#25143;&#30740;&#31350;&#23637;&#31034;&#20102;&#22270;&#35299;&#22411;&#35299;&#37322;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#34920;&#26126;&#22312;&#20020;&#24202;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#65292;&#22270;&#35299;&#24335;&#35299;&#37322;&#27604;&#20854;&#20182;&#26041;&#24335;&#26356;&#21463;&#25512;&#23815;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many visualizations have been developed for explainable AI (XAI), but they often require further reasoning by users to interpret. We argue that XAI should support diagrammatic and abductive reasoning for the AI to perform hypothesis generation and evaluation to reduce the interpretability gap. We propose Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii) follow domain conventions, and iii) explain with diagrams visually or verbally. We implemented DiagramNet for a clinical application to predict cardiac diagnoses from heart auscultation, and explain with shape-based murmur diagrams. In modeling studies, we found that DiagramNet not only provides faithful murmur shape explanations, but also has better prediction performance than baseline models. We further demonstrate the interpretability and trustworthiness of diagrammatic explanations in a qualitative user study with medical students, showing that clinically-relevant, diagrammatic explanations are preferred ov
&lt;/p&gt;</description></item></channel></rss>