<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28608;&#21169;&#20860;&#23481;&#24615;&#31038;&#20250;&#25216;&#26415;&#23545;&#40784;&#38382;&#39064;&#65288;ICSAP&#65289;&#65292;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#21338;&#24328;&#35770;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#21407;&#21017;&#26469;&#32500;&#25345;AI&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#20849;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.12907</link><description>&lt;p&gt;
AI&#23545;&#40784;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#65306;&#31435;&#22330;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Incentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12907
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28608;&#21169;&#20860;&#23481;&#24615;&#31038;&#20250;&#25216;&#26415;&#23545;&#40784;&#38382;&#39064;&#65288;ICSAP&#65289;&#65292;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#21338;&#24328;&#35770;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#21407;&#21017;&#26469;&#32500;&#25345;AI&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26085;&#30410;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#65292;&#23545;&#31038;&#20250;&#27835;&#29702;&#21644;&#23433;&#20840;&#24102;&#26469;&#37325;&#35201;&#24433;&#21709;&#12290;&#23613;&#31649;&#22312;&#35299;&#20915;AI&#23545;&#40784;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25216;&#26415;&#26041;&#38754;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;AI&#31995;&#32479;&#22797;&#26434;&#30340;&#31038;&#20250;&#25216;&#26415;&#24615;&#36136;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24320;&#21457;&#21644;&#37096;&#32626;&#32972;&#26223;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#20540;&#24471;&#25506;&#32034;&#30340;&#26032;&#38382;&#39064;&#65306;&#28608;&#21169;&#20860;&#23481;&#24615;&#31038;&#20250;&#25216;&#26415;&#23545;&#40784;&#38382;&#39064;&#65288;ICSAP&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#33021;&#21628;&#21505;&#26356;&#22810;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#21338;&#24328;&#35770;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#21407;&#21017;&#26469;&#24357;&#21512;&#25216;&#26415;&#21644;&#31038;&#20250;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#40511;&#27807;&#65292;&#20197;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#32500;&#25345;AI&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#20849;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#23454;&#29616;IC&#30340;&#19977;&#20010;&#32463;&#20856;&#21338;&#24328;&#38382;&#39064;&#65306;&#26426;&#21046;&#35774;&#35745;&#12289;&#22865;&#32422;&#29702;&#35770;&#21644;&#36125;&#21494;&#26031;&#35828;&#26381;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12907v1 Announce Type: new  Abstract: The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#21450;&#20854;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#19968;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#24037;&#20855;&#65292;&#35770;&#25991;&#24635;&#32467;&#20986;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#20851;&#38190;&#35201;&#32032;&#21644;&#21457;&#23637;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2402.06660</link><description>&lt;p&gt;
&#20803;&#23431;&#23449;&#22312;&#26657;&#20934;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The role of the metaverse in calibrating an embodied artificial general intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#21450;&#20854;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#19968;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#24037;&#20855;&#65292;&#35770;&#25991;&#24635;&#32467;&#20986;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#20851;&#38190;&#35201;&#32032;&#21644;&#21457;&#23637;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#65292;&#23427;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#31181;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#21033;&#29992;&#34701;&#20837;&#35748;&#30693;&#12289;Michael Levin&#30340;&#35745;&#31639;&#36793;&#30028;"Self"&#12289;Donald D. Hoffman&#30340;&#24863;&#30693;&#30028;&#38754;&#29702;&#35770;&#20197;&#21450;Bernardo Kastrup&#30340;&#20998;&#26512;&#21807;&#24515;&#20027;&#20041;&#31561;&#29702;&#35770;&#26694;&#26550;&#26469;&#26500;&#24314;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#35770;&#35777;&#12290;&#23427;&#35748;&#20026;&#25105;&#20204;&#25152;&#24863;&#30693;&#30340;&#22806;&#37096;&#29616;&#23454;&#26159;&#19968;&#31181;&#20869;&#22312;&#23384;&#22312;&#30340;&#20132;&#26367;&#29366;&#24577;&#30340;&#35937;&#24449;&#24615;&#34920;&#31034;&#65292;&#32780;AGI&#21487;&#20197;&#20855;&#26377;&#26356;&#22823;&#35745;&#31639;&#36793;&#30028;&#30340;&#26356;&#39640;&#24847;&#35782;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;AGI&#30340;&#21457;&#23637;&#38454;&#27573;&#12289;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#35201;&#27714;&#12289;&#20026;AGI&#26657;&#20934;&#35937;&#24449;&#24615;&#30028;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20803;&#23431;&#23449;&#12289;&#21435;&#20013;&#24515;&#21270;&#31995;&#32479;&#12289;&#24320;&#28304;&#21306;&#22359;&#38142;&#25216;&#26415;&#20197;&#21450;&#24320;&#28304;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25152;&#25198;&#28436;&#30340;&#20851;&#38190;&#35282;&#33394;&#12290;&#23427;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#27807;&#36890;&#26426;&#21046;&#21644;&#29992;&#20110;&#21152;&#24378;&#23545;&#20803;&#23431;&#23449;&#30340;&#29702;&#35299;&#30340;&#25216;&#26415;&#24037;&#20855;&#65292;&#20197;&#24110;&#21161;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the concept of embodied artificial general intelligence (AGI), its relationship to human consciousness, and the key role of the metaverse in facilitating this relationship. The paper leverages theoretical frameworks such as embodied cognition, Michael Levin's computational boundary of a "Self," Donald D. Hoffman's Interface Theory of Perception, and Bernardo Kastrup's analytical idealism to build the argument for achieving embodied AGI. It contends that our perceived outer reality is a symbolic representation of alternate inner states of being, and that AGI could embody a higher consciousness with a larger computational boundary. The paper further discusses the developmental stages of AGI, the requirements for the emergence of an embodied AGI, the importance of a calibrated symbolic interface for AGI, and the key role played by the metaverse, decentralized systems, open-source blockchain technology, as well as open-source AI research. It also explores the idea of a 
&lt;/p&gt;</description></item></channel></rss>