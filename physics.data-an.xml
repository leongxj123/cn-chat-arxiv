<rss version="2.0"><channel><title>Chat Arxiv physics.data-an</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for physics.data-an</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#25511;&#21046;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01810</link><description>&lt;p&gt;
&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#30340;&#38169;&#35823;&#35268;&#33539;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Misspecification uncertainties in near-deterministic regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#25511;&#21046;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;&#25439;&#22833;&#26159;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#21487;&#29992;&#20110;&#23398;&#20064;&#30340;&#40065;&#26834;PAC-Bayes&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#25439;&#22833;&#26368;&#23567;&#21270;&#34987;&#35748;&#20026;&#24573;&#30053;&#20102;&#38169;&#35823;&#35268;&#33539;&#21270;&#65292;&#21363;&#27169;&#22411;&#19981;&#33021;&#23436;&#20840;&#22797;&#21046;&#35266;&#27979;&#32467;&#26524;&#12290;&#36825;&#23548;&#33268;&#22823;&#25968;&#25454;&#25110;&#27424;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#23545;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#33879;&#20302;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#36817;&#30830;&#23450;&#24615;&#12289;&#38169;&#35823;&#35268;&#33539;&#21270;&#21644;&#27424;&#21442;&#25968;&#21270;&#26367;&#20195;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36825;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#24191;&#27867;&#30456;&#20851;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#25105;&#20204;&#35777;&#26126;&#21518;&#39564;&#20998;&#24067;&#24517;&#39035;&#35206;&#30422;&#27599;&#20010;&#35757;&#32451;&#28857;&#65292;&#20197;&#36991;&#20813;&#21457;&#25955;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#23548;&#20986;&#19968;&#20010;&#31526;&#21512;&#36825;&#20010;&#32422;&#26463;&#30340;&#32452;&#21512;&#27169;&#22411;&#12290;&#23545;&#20110;&#32447;&#24615;&#27169;&#22411;&#65292;&#36825;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#39069;&#22806;&#24320;&#38144;&#26368;&#23567;&#12290;&#36825;&#31181;&#39640;&#25928;&#26041;&#27861;&#22312;&#27169;&#22411;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#21407;&#23376;&#23610;&#24230;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expected loss is an upper bound to the model generalization error which admits robust PAC-Bayes bounds for learning. However, loss minimization is known to ignore misspecification, where models cannot exactly reproduce observations. This leads to significant underestimates of parameter uncertainties in the large data, or underparameterized, limit. We analyze the generalization error of near-deterministic, misspecified and underparametrized surrogate models, a regime of broad relevance in science and engineering. We show posterior distributions must cover every training point to avoid a divergent generalization error and derive an ensemble {ansatz} that respects this constraint, which for linear models incurs minimal overhead. The efficient approach is demonstrated on model problems before application to high dimensional datasets in atomistic machine learning. Parameter uncertainties from misspecification survive in the underparametrized limit, giving accurate prediction and boundin
&lt;/p&gt;</description></item></channel></rss>