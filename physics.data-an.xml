<rss version="2.0"><channel><title>Chat Arxiv physics.data-an</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for physics.data-an</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;&#65288;MPM&#65289;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#33021;&#29289;&#29702;&#31185;&#23398;&#25968;&#25454;&#20013;&#26080;&#24207;&#36755;&#20837;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#35757;&#32451;&#23398;&#20064;&#32622;&#25442;&#19981;&#21464;&#30340;&#20989;&#25968;&#65292;&#22312;&#26500;&#24314;&#36866;&#29992;&#20110;&#22810;&#31181;&#20219;&#21153;&#30340;&#39640;&#33021;&#29289;&#29702;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13537</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#21512;&#30340;&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;&#65306;&#36208;&#21521;&#33258;&#30417;&#30563;&#39640;&#33021;&#29289;&#29702;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models. (arXiv:2401.13537v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;&#65288;MPM&#65289;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#33021;&#29289;&#29702;&#31185;&#23398;&#25968;&#25454;&#20013;&#26080;&#24207;&#36755;&#20837;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#35757;&#32451;&#23398;&#20064;&#32622;&#25442;&#19981;&#21464;&#30340;&#20989;&#25968;&#65292;&#22312;&#26500;&#24314;&#36866;&#29992;&#20110;&#22810;&#31181;&#20219;&#21153;&#30340;&#39640;&#33021;&#29289;&#29702;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;"&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;"&#65288;MPM&#65289;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#33021;&#29289;&#29702;&#65288;HEP&#65289;&#31185;&#23398;&#25968;&#25454;&#20013;&#26080;&#24207;&#36755;&#20837;&#30340;&#36890;&#29992;&#12289;&#21487;&#36716;&#31227;&#21644;&#21487;&#37325;&#29992;&#34920;&#31034;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#36974;&#34109;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#26469;&#23398;&#20064;&#38598;&#21512;&#19978;&#30340;&#32622;&#25442;&#19981;&#21464;&#20989;&#25968;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#36825;&#39033;&#24037;&#20316;&#22312;&#26500;&#24314;&#21487;&#20197;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#36890;&#29992;&#39044;&#35757;&#32451;&#24182;&#31245;&#21518;&#31934;&#35843;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;HEP&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#22312;MPM&#20013;&#65292;&#38598;&#21512;&#20013;&#30340;&#31890;&#23376;&#34987;&#36974;&#34109;&#65292;&#35757;&#32451;&#30340;&#30446;&#26631;&#26159;&#24674;&#22797;&#23427;&#20204;&#30340;&#36523;&#20221;&#65292;&#36523;&#20221;&#30001;&#39044;&#35757;&#32451;&#30340;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31163;&#25955;&#21270;&#26631;&#35760;&#34920;&#31034;&#23450;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#23545;&#25758;&#26426;&#29289;&#29702;&#23454;&#39564;&#20013;&#39640;&#33021;&#21943;&#27880;&#26679;&#26412;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#31163;&#25955;&#21270;&#12289;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#25490;&#24207;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose \textit{masked particle modeling} (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capabili
&lt;/p&gt;</description></item></channel></rss>