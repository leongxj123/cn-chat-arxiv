<rss version="2.0"><channel><title>Chat Arxiv physics.data-an</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for physics.data-an</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#32479;&#19968;&#21407;&#29702;&#65292;&#29992;&#20110;&#37325;&#26032;&#25512;&#23548;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;&#21464;&#20998;&#38477;&#32500;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;&#35299;&#37322;&#20026;&#20004;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26435;&#34913;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#21387;&#32553;&#25968;&#25454;&#21644;&#20445;&#30041;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.03311</link><description>&lt;p&gt;
&#28145;&#24230;&#21464;&#20998;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;--&#19968;&#31181;&#21464;&#20998;&#25439;&#22833;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses. (arXiv:2310.03311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#32479;&#19968;&#21407;&#29702;&#65292;&#29992;&#20110;&#37325;&#26032;&#25512;&#23548;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;&#21464;&#20998;&#38477;&#32500;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;&#35299;&#37322;&#20026;&#20004;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26435;&#34913;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#21387;&#32553;&#25968;&#25454;&#21644;&#20445;&#30041;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#38477;&#32500;&#26041;&#27861;&#20197;&#20854;&#39640;&#31934;&#24230;&#12289;&#29983;&#25104;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#32780;&#38395;&#21517;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#24456;&#22810;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#32479;&#19968;&#21407;&#29702;&#65292;&#37325;&#26032;&#25512;&#23548;&#21644;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;&#30340;&#35299;&#37322;&#65292;&#20854;&#20013;&#20004;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#30456;&#20114;&#26435;&#34913;&#12290;&#25105;&#20204;&#23558;&#31532;&#19968;&#20010;&#32593;&#32476;&#35299;&#37322;&#20026;&#32534;&#30721;&#22120;&#22270;&#65292;&#23427;&#25351;&#23450;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#26102;&#35201;&#20445;&#30041;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#31532;&#20108;&#20010;&#32593;&#32476;&#35299;&#37322;&#20026;&#35299;&#30721;&#22120;&#22270;&#65292;&#23427;&#20026;&#25968;&#25454;&#25351;&#23450;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#37325;&#26032;&#25512;&#23548;&#20102;&#29616;&#26377;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#22914;&#28145;&#24230;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;(DVIB)&#12289;beta&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(beta-VAE)&#21644;&#28145;&#24230;&#21464;&#20998;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;(DVCCA)&#12290;&#35813;&#26694;&#26550;&#33258;&#28982;&#22320;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#21387;&#32553;&#25968;&#25454;&#21644;&#20445;&#30041;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational dimensionality reduction methods are known for their high accuracy, generative abilities, and robustness. These methods have many theoretical justifications. Here we introduce a unifying principle rooted in information theory to rederive and generalize existing variational methods and design new ones. We base our framework on an interpretation of the multivariate information bottleneck, in which two Bayesian networks are traded off against one another. We interpret the first network as an encoder graph, which specifies what information to keep when compressing the data. We interpret the second network as a decoder graph, which specifies a generative model for the data. Using this framework, we rederive existing dimensionality reduction methods such as the deep variational information bottleneck (DVIB), beta variational auto-encoders (beta-VAE), and deep variational canonical correlation analysis (DVCCA). The framework naturally introduces a trade-off parameter between compr
&lt;/p&gt;</description></item></channel></rss>