<rss version="2.0"><channel><title>Chat Arxiv physics.data-an</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for physics.data-an</description><item><title>&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#31283;&#23450;&#24615;&#26469;&#22238;&#31572;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.13093</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Probabilistic Stability of Stochastic Gradient Descent. (arXiv:2303.13093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#31283;&#23450;&#24615;&#26469;&#22238;&#31572;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#24320;&#25918;&#38382;&#39064;&#26159;&#22914;&#20309;&#23450;&#20041;&#21644;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#25509;&#36817;&#22266;&#23450;&#28857;&#30340;&#31283;&#23450;&#24615;&#12290;&#20256;&#32479;&#25991;&#29486;&#20381;&#36182;&#20110;&#21442;&#25968;&#32479;&#35745;&#30697;&#65292;&#29305;&#21035;&#26159;&#21442;&#25968;&#26041;&#24046;&#30340;&#25910;&#25947;&#26469;&#37327;&#21270;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;SGD&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;\textit{&#27010;&#29575;&#25910;&#25947;}&#26465;&#20214;&#26469;&#23450;&#20041;SGD&#30340;\textit{&#27010;&#29575;&#31283;&#23450;&#24615;}&#12290;&#25552;&#20986;&#30340;&#31283;&#23450;&#24615;&#30452;&#25509;&#22238;&#31572;&#20102;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#22312;&#27010;&#29575;&#24615;&#31283;&#23450;&#24615;&#30340;&#38236;&#22836;&#19979;&#65292;SGD&#25165;&#34920;&#29616;&#20986;&#20016;&#23500;&#32780;&#23454;&#38469;&#30456;&#20851;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#22914;&#23436;&#20840;&#22833;&#21435;&#31283;&#23450;&#24615;&#38454;&#27573;&#12289;&#19981;&#27491;&#30830;&#23398;&#20064;&#38454;&#27573;&#12289;&#25910;&#25947;&#21040;&#20302;&#31209;&#38797;&#28857;&#38454;&#27573;&#21644;&#27491;&#30830;&#23398;&#20064;&#38454;&#27573;&#12290;&#24403;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#36825;&#20123;&#30456;&#22270;&#24847;&#21619;&#30528;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#30340;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental open problem in deep learning theory is how to define and understand the stability of stochastic gradient descent (SGD) close to a fixed point. Conventional literature relies on the convergence of statistical moments, esp., the variance, of the parameters to quantify the stability. We revisit the definition of stability for SGD and use the \textit{convergence in probability} condition to define the \textit{probabilistic stability} of SGD. The proposed stability directly answers a fundamental question in deep learning theory: how SGD selects a meaningful solution for a neural network from an enormous number of solutions that may overfit badly. To achieve this, we show that only under the lens of probabilistic stability does SGD exhibit rich and practically relevant phases of learning, such as the phases of the complete loss of stability, incorrect learning, convergence to low-rank saddles, and correct learning. When applied to a neural network, these phase diagrams imply t
&lt;/p&gt;</description></item></channel></rss>