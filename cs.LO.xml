<rss version="2.0"><channel><title>Chat Arxiv cs.LO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LO</description><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#36229;&#36234;&#19968;&#38454;&#36923;&#36753;&#30340;&#25552;&#21319;&#25512;&#29702;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#25193;&#23637;&#30340;&#20004;&#20010;&#21464;&#37327;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#65292;&#24182;&#22312;&#38480;&#23450;&#20102;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11738</link><description>&lt;p&gt;
&#36229;&#20986;&#19968;&#38454;&#36923;&#36753;&#30340;&#25552;&#21319;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Lifted Inference beyond First-Order Logic. (arXiv:2308.11738v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11738
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#36229;&#36234;&#19968;&#38454;&#36923;&#36753;&#30340;&#25552;&#21319;&#25512;&#29702;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#25193;&#23637;&#30340;&#20004;&#20010;&#21464;&#37327;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#65292;&#24182;&#22312;&#38480;&#23450;&#20102;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#35745;&#25968;(WFOMC)&#26159;&#27010;&#29575;&#25512;&#29702;&#30340;&#22522;&#30784;&#12290;&#30001;&#20110;WFOMC&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65288;$\#$P&#23436;&#20840;&#65289;&#65292;&#22240;&#27492;&#33021;&#22815;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36827;&#34892;WFOMC&#30340;&#36923;&#36753;&#30862;&#29255;&#38750;&#24120;&#26377;&#24847;&#20041;&#12290;&#36825;&#26679;&#30340;&#30862;&#29255;&#34987;&#31216;&#20026;&#22495;&#21487;&#25552;&#21319;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35745;&#25968;&#37327;&#35789;&#65288;$\mathrm{C^2}$&#65289;&#25193;&#23637;&#30340;&#20004;&#20010;&#21464;&#37327;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#20013;&#65292;&#21487;&#20197;&#36827;&#34892;&#22495;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#22914;&#24341;&#29992;&#32593;&#32476;&#20013;&#30340;&#38750;&#24490;&#29615;&#24615;&#21644;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#36830;&#36890;&#24615;&#65292;&#19981;&#33021;&#22312;$\mathrm{C^2}$&#25110;&#19968;&#38454;&#36923;&#36753;&#20013;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;$\mathrm{C^2}$&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#65292;&#21253;&#25324;&#22810;&#20010;&#36825;&#26679;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23558;$\mathrm{C^2}$&#21477;&#23376;&#30340;&#19968;&#20010;&#20851;&#31995;&#38480;&#23450;&#20026;&#34920;&#31034;&#26377;&#21521;&#26080;&#29615;&#22270;&#12289;&#36830;&#36890;&#22270;&#12289;&#26641;&#65288;&#25110;&#26377;&#21521;&#26641;&#65289;&#25110;&#26862;&#26519;&#65288;&#25110;&#26377;&#21521;&#26862;&#26519;&#65289;&#26102;&#65292;&#23427;&#20173;&#28982;&#20445;&#25345;&#20102;&#22495;&#21487;&#25552;&#21319;&#24615;&#12290;&#25152;&#26377;&#25105;&#20204;&#30340;&#32467;&#26524;&#37117;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic inference in statistical relational learning models. As WFOMC is known to be intractable in general ($\#$P-complete), logical fragments that admit polynomial time WFOMC are of significant interest. Such fragments are called domain liftable. Recent works have shown that the two-variable fragment of first order logic extended with counting quantifiers ($\mathrm{C^2}$) is domain-liftable. However, many properties of real-world data, like acyclicity in citation networks and connectivity in social networks, cannot be modeled in $\mathrm{C^2}$, or first order logic in general. In this work, we expand the domain liftability of $\mathrm{C^2}$ with multiple such properties. We show that any $\mathrm{C^2}$ sentence remains domain liftable when one of its relations is restricted to represent a directed acyclic graph, a connected graph, a tree (resp. a directed tree) or a forest (resp. a directed forest). All our results r
&lt;/p&gt;</description></item></channel></rss>