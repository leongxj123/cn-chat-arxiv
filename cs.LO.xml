<rss version="2.0"><channel><title>Chat Arxiv cs.LO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LO</description><item><title>&#26412;&#25991;&#23545;transformers&#22312;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#29702;&#35299;&#20854;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2311.00208</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#22120;&#65306;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Transformers as Recognizers of Formal Languages: A Survey on Expressivity. (arXiv:2311.00208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;transformers&#22312;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#29702;&#35299;&#20854;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#31361;&#20986;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#23427;&#20204;&#33021;&#21542;&#35299;&#20915;&#38382;&#39064;&#65292;&#23558;&#38382;&#39064;&#35270;&#20026;&#24418;&#24335;&#35821;&#35328;&#12290;&#25506;&#32034;&#36825;&#31867;&#38382;&#39064;&#23558;&#26377;&#21161;&#20110;&#27604;&#36739;transformers&#19982;&#20854;&#20182;&#27169;&#22411;&#20197;&#21450;&#19981;&#21516;&#21464;&#31181;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#36825;&#20010;&#23376;&#39046;&#22495;&#30340;&#24037;&#20316;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#23545;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35760;&#24405;&#20102;&#19981;&#21516;&#32467;&#26524;&#32972;&#21518;&#30340;&#21508;&#31181;&#20551;&#35774;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#21327;&#35843;&#30475;&#20284;&#30456;&#20114;&#30683;&#30462;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring questions such as this will help to compare transformers with other models, and transformer variants with one another, for various tasks. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.
&lt;/p&gt;</description></item></channel></rss>