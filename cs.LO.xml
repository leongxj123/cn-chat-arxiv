<rss version="2.0"><channel><title>Chat Arxiv cs.LO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LO</description><item><title>Boolformer&#26159;&#31532;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#25191;&#34892;&#31471;&#21040;&#31471;&#30340;&#24067;&#23572;&#20989;&#25968;&#31526;&#21495;&#22238;&#24402;&#12290;&#23427;&#21487;&#20197;&#39044;&#27979;&#22797;&#26434;&#20989;&#25968;&#30340;&#31616;&#27905;&#20844;&#24335;&#65292;&#24182;&#22312;&#25552;&#20379;&#19981;&#23436;&#25972;&#21644;&#26377;&#22122;&#22768;&#35266;&#27979;&#26102;&#25214;&#21040;&#36817;&#20284;&#34920;&#36798;&#24335;&#12290;Boolformer&#22312;&#30495;&#23454;&#20108;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#28508;&#21147;&#20316;&#20026;&#21487;&#35299;&#37322;&#24615;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#21160;&#21147;&#23398;&#24314;&#27169;&#20219;&#21153;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12207</link><description>&lt;p&gt;
Boolformer: &#29992;Transformer&#36827;&#34892;&#36923;&#36753;&#20989;&#25968;&#30340;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Boolformer: Symbolic Regression of Logic Functions with Transformers. (arXiv:2309.12207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12207
&lt;/p&gt;
&lt;p&gt;
Boolformer&#26159;&#31532;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#25191;&#34892;&#31471;&#21040;&#31471;&#30340;&#24067;&#23572;&#20989;&#25968;&#31526;&#21495;&#22238;&#24402;&#12290;&#23427;&#21487;&#20197;&#39044;&#27979;&#22797;&#26434;&#20989;&#25968;&#30340;&#31616;&#27905;&#20844;&#24335;&#65292;&#24182;&#22312;&#25552;&#20379;&#19981;&#23436;&#25972;&#21644;&#26377;&#22122;&#22768;&#35266;&#27979;&#26102;&#25214;&#21040;&#36817;&#20284;&#34920;&#36798;&#24335;&#12290;Boolformer&#22312;&#30495;&#23454;&#20108;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#28508;&#21147;&#20316;&#20026;&#21487;&#35299;&#37322;&#24615;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#21160;&#21147;&#23398;&#24314;&#27169;&#20219;&#21153;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Boolformer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#25191;&#34892;&#31471;&#21040;&#31471;&#30340;&#24067;&#23572;&#20989;&#25968;&#31526;&#21495;&#22238;&#24402;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25552;&#20379;&#24178;&#20928;&#30340;&#30495;&#20540;&#34920;&#26102;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#22797;&#26434;&#20989;&#25968;&#30340;&#31616;&#27905;&#20844;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#25552;&#20379;&#19981;&#23436;&#25972;&#21644;&#26377;&#22122;&#22768;&#35266;&#27979;&#26102;&#25214;&#21040;&#36817;&#20284;&#34920;&#36798;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#30495;&#23454;&#20108;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Boolformer&#65292;&#35777;&#26126;&#20102;&#23427;&#20316;&#20026;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#26367;&#20195;&#21697;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#24314;&#27169;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#24120;&#35265;&#20219;&#21153;&#12290;&#20351;&#29992;&#26368;&#36817;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Boolformer&#19982;&#26368;&#20808;&#36827;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#27604;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce Boolformer, the first Transformer architecture trained to perform end-to-end symbolic regression of Boolean functions. First, we show that it can predict compact formulas for complex functions which were not seen during training, when provided a clean truth table. Then, we demonstrate its ability to find approximate expressions when provided incomplete and noisy observations. We evaluate the Boolformer on a broad set of real-world binary classification datasets, demonstrating its potential as an interpretable alternative to classic machine learning methods. Finally, we apply it to the widespread task of modelling the dynamics of gene regulatory networks. Using a recent benchmark, we show that Boolformer is competitive with state-of-the art genetic algorithms with a speedup of several orders of magnitude. Our code and models are available publicly.
&lt;/p&gt;</description></item></channel></rss>