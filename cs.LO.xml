<rss version="2.0"><channel><title>Chat Arxiv cs.LO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LO</description><item><title>&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#22256;&#38590;&#24615;&#20855;&#26377;&#32039;&#20945;&#30340;&#26377;&#38480;&#29305;&#24615;&#34920;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.10360</link><description>&lt;p&gt;
&#23398;&#20064;&#24615;&#26159;&#19968;&#31181;&#32039;&#20945;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Learnability is a Compact Property
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10360
&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#22256;&#38590;&#24615;&#20855;&#26377;&#32039;&#20945;&#30340;&#26377;&#38480;&#29305;&#24615;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#23398;&#20064;&#30340;&#24037;&#20316;&#21462;&#24471;&#20102;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#65306;&#21508;&#31181;&#38382;&#39064;&#30340;&#21487;&#23398;&#20064;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#65292;&#25110;&#32773;&#19982;&#26631;&#20934;&#38598;&#21512;&#35770;ZFC&#20844;&#29702;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#38382;&#39064;&#30340;&#21487;&#23398;&#20064;&#24615;&#21487;&#33021;&#19981;&#26159;&#20855;&#26377;&#26377;&#38480;&#29305;&#24615;&#30340;&#23646;&#24615;&#65306;&#38750;&#27491;&#24335;&#22320;&#35828;&#65292;&#23427;&#19981;&#33021;&#36890;&#36807;&#26816;&#26597;&#38382;&#39064;&#30340;&#26377;&#38480;&#25237;&#24433;&#26469;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10360v1 Announce Type: new  Abstract: Recent work on learning has yielded a striking result: the learnability of various problems can be undecidable, or independent of the standard ZFC axioms of set theory. Furthermore, the learnability of such problems can fail to be a property of finite character: informally, it cannot be detected by examining finite projections of the problem.   On the other hand, learning theory abounds with notions of dimension that characterize learning and consider only finite restrictions of the problem, i.e., are properties of finite character. How can these results be reconciled? More precisely, which classes of learning problems are vulnerable to logical undecidability, and which are within the grasp of finite characterizations?   We demonstrate that the difficulty of supervised learning with metric losses admits a tight finite characterization. In particular, we prove that the sample complexity of learning a hypothesis class can be detected by ex
&lt;/p&gt;</description></item><item><title>&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#21644;&#20005;&#26684;&#26410;&#26469;&#25513;&#30721;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#65292;&#36825;&#19968;&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#35821;&#35328;&#31867;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#30340;&#30740;&#31350;&#65292;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2310.13897</link><description>&lt;p&gt;
&#25513;&#30721;&#30828;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#21644;&#24067;&#23572;RASP&#20934;&#30830;&#35782;&#21035;&#26080;&#26143;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages. (arXiv:2310.13897v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13897
&lt;/p&gt;
&lt;p&gt;
&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#21644;&#20005;&#26684;&#26410;&#26469;&#25513;&#30721;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#65292;&#36825;&#19968;&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#35821;&#35328;&#31867;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#30340;&#30740;&#31350;&#65292;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#65288;&#21363;&#25152;&#26377;&#27880;&#24847;&#21147;&#37117;&#38598;&#20013;&#22312;&#19968;&#20010;&#20301;&#32622;&#19978;&#65289;&#21644;&#20005;&#26684;&#30340;&#26410;&#26469;&#25513;&#30721;&#65288;&#21363;&#27599;&#20010;&#20301;&#32622;&#21482;&#19982;&#20005;&#26684;&#24038;&#20391;&#30340;&#20301;&#32622;&#36827;&#34892;&#27880;&#24847;&#21147;&#20132;&#20114;&#65289;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#23558;&#34987;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#31867;&#21035;&#12290;&#36825;&#20123;&#35777;&#26126;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#23427;&#26159;&#19968;&#31181;&#21463;&#38480;&#20110;&#24067;&#23572;&#20540;&#30340;RASP&#21464;&#31181;&#12290;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#65292;&#25105;&#20204;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider transformer encoders with hard attention (in which all attention is focused on exactly one position) and strict future masking (in which each position only attends to positions strictly to its left), and prove that the class of languages recognized by these networks is exactly the star-free languages. Adding position embeddings increases the class of recognized languages to other well-studied classes. A key technique in these proofs is Boolean RASP, a variant of RASP that is restricted to Boolean values. Via the star-free languages, we relate transformers to first-order logic, temporal logic, and algebraic automata theory.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#24335;&#20027;&#20041;&#26469;&#25805;&#20316;&#21644;&#20998;&#26512;&#35777;&#26126;&#65292;&#29992;&#20110;&#29983;&#25104;&#26356;&#30701;&#30340;&#35777;&#26126;&#21644;&#20943;&#23569;&#25628;&#32034;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.12827</link><description>&lt;p&gt;
&#35777;&#26126;&#32467;&#26500;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigations into Proof Structures. (arXiv:2304.12827v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12827
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#24335;&#20027;&#20041;&#26469;&#25805;&#20316;&#21644;&#20998;&#26512;&#35777;&#26126;&#65292;&#29992;&#20110;&#29983;&#25104;&#26356;&#30701;&#30340;&#35777;&#26126;&#21644;&#20943;&#23569;&#25628;&#32034;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#19968;&#31181;&#26032;&#22411;&#24418;&#24335;&#20027;&#20041;&#26469;&#25805;&#20316;&#21644;&#20998;&#26512;&#35777;&#26126;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#30340;&#23545;&#35937;&#12290;&#22312;&#36825;&#31532;&#19968;&#27425;&#23581;&#35797;&#20013;&#65292;&#36825;&#20010;&#24418;&#24335;&#20027;&#20041;&#20165;&#38480;&#20110;&#30001;&#27987;&#32553;&#25512;&#23548;&#29305;&#24449;&#30340;&#19968;&#38454;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#19968;&#20010;&#20840;&#38754;&#30340;&#24418;&#24335;&#37325;&#26500;&#21644;&#20998;&#26512;&#21382;&#21490;&#19978;{\L}ukasiewicz&#24191;&#27867;&#30740;&#31350;&#36807;&#30340;&#38382;&#39064;&#30340;&#35777;&#26126;&#20026;&#20363;&#36827;&#34892;&#20102;&#38416;&#36848;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#22312;&#35777;&#26126;&#25628;&#32034;&#36807;&#31243;&#20013;&#29983;&#25104;&#24341;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#25628;&#32034;&#24037;&#20316;&#37327;&#24182;&#25214;&#21040;&#26356;&#30701;&#30340;&#35777;&#26126;&#12290;&#22312;&#36825;&#26465;&#36335;&#32447;&#19978;&#25253;&#21578;&#20102;&#35768;&#22810;&#23454;&#39564;&#65292;&#20854;&#20013;&#33258;&#21160;&#21457;&#29616;&#20102;&#19968;&#20010;&#35777;&#26126;{\L}ukasiewicz&#30340;&#38382;&#39064;&#65292;&#23427;&#27604;&#20197;&#21069;&#20219;&#20309;&#30001;&#20154;&#25110;&#26426;&#22120;&#21457;&#29616;&#30340;&#35777;&#26126;&#37117;&#35201;&#30701;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and elaborate a novel formalism for the manipulation and analysis of proofs as objects in a global manner. In this first approach the formalism is restricted to first-order problems characterized by condensed detachment. It is applied in an exemplary manner to a coherent and comprehensive formal reconstruction and analysis of historical proofs of a widely-studied problem due to {\L}ukasiewicz. The underlying approach opens the door towards new systematic ways of generating lemmas in the course of proof search to the effects of reducing the search effort and finding shorter proofs. Among the numerous reported experiments along this line, a proof of {\L}ukasiewicz's problem was automatically discovered that is much shorter than any proof found before by man or machine.
&lt;/p&gt;</description></item></channel></rss>