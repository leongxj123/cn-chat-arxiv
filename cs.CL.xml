<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>AutoRD&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#65292;&#23454;&#29616;&#20102;&#25972;&#20307;F1&#24471;&#20998;47.3%&#65292;&#30456;&#23545;&#20110;&#22522;&#30784;LLM&#26377;14.4%&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.00953</link><description>&lt;p&gt;
AutoRD&#65306;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#26500;&#24314;&#30340;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00953
&lt;/p&gt;
&lt;p&gt;
AutoRD&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#65292;&#23454;&#29616;&#20102;&#25972;&#20307;F1&#24471;&#20998;47.3%&#65292;&#30456;&#23545;&#20110;&#22522;&#30784;LLM&#26377;14.4%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#21517;&#20026;AutoRD&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#26377;&#20851;&#32597;&#35265;&#30142;&#30149;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#27979;&#35797;&#26469;&#35780;&#20272;AutoRD&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#24378;&#35843;&#20102;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30340;&#31995;&#32479;AutoRD&#26159;&#19968;&#20010;&#36719;&#20214;&#27969;&#27700;&#32447;&#65292;&#28041;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#23454;&#20307;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#23454;&#20307;&#26657;&#20934;&#21644;&#30693;&#35782;&#22270;&#26500;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30001;&#24320;&#28304;&#21307;&#23398;&#26412;&#20307;&#21457;&#23637;&#32780;&#26469;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20307;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#20197;&#21450;&#30693;&#35782;&#22270;&#26500;&#24314;&#24615;&#33021;&#23545;&#31995;&#32479;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#12290;&#32467;&#26524;&#65306;AutoRD&#21462;&#24471;&#20102;47.3%&#30340;&#25972;&#20307;F1&#20998;&#25968;&#65292;&#36739;&#22522;&#30784;LLM&#25552;&#39640;&#20102;14.4%&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AutoRD&#23454;&#29616;&#20102;56.1%&#30340;&#25972;&#20307;&#23454;&#20307;&#25552;&#21462;F1&#20998;&#25968;&#65288;&#32597;&#35265;&#30142;&#30149;&#65306;83.5%&#65292;&#30142;&#30149;&#65306;35.8%&#65292;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00953v1 Announce Type: cross  Abstract: Objectives: Our objective is to create an end-to-end system called AutoRD, which automates extracting information from clinical text about rare diseases. We have conducted various tests to evaluate the performance of AutoRD and highlighted its strengths and limitations in this paper.   Materials and Methods: Our system, AutoRD, is a software pipeline involving data preprocessing, entity extraction, relation extraction, entity calibration, and knowledge graph construction. We implement this using large language models and medical knowledge graphs developed from open-source medical ontologies. We quantitatively evaluate our system on entity extraction, relation extraction, and the performance of knowledge graph construction.   Results: AutoRD achieves an overall F1 score of 47.3%, a 14.4% improvement compared to the base LLM. In detail, AutoRD achieves an overall entity extraction F1 score of 56.1% (rare_disease: 83.5%, disease: 35.8%, s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22836;&#37096;&#20849;&#20139;&#27880;&#24847;&#21147;&#30340;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#27880;&#24847;&#21147;&#22836;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#24040;&#22823;&#23548;&#33268;&#37096;&#32626;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11819</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#20110;&#22836;&#37096;&#20849;&#20139;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Head-wise Shareable Attention for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22836;&#37096;&#20849;&#20139;&#27880;&#24847;&#21147;&#30340;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#27880;&#24847;&#21147;&#22836;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#24040;&#22823;&#23548;&#33268;&#37096;&#32626;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#21442;&#25968;&#25968;&#37327;&#24040;&#22823;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#12290;&#21442;&#25968;&#20849;&#20139;&#26159;&#19968;&#31181;&#26377;&#21033;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#40723;&#21169;&#26435;&#37325;&#37325;&#29992;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#37327;&#24182;&#38477;&#20302;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21442;&#25968;&#20849;&#20139;&#25216;&#26415;&#20027;&#35201;&#19987;&#27880;&#20110;&#20687;BERT&#36825;&#26679;&#30340;&#23567;&#35268;&#27169;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#31895;&#31890;&#24230;&#30340;&#20849;&#20139;&#35268;&#21017;&#65292;&#20363;&#22914;&#36880;&#23618;&#20849;&#20139;&#12290;&#37492;&#20110;LLMs&#30340;&#26222;&#21450;&#65292;&#36825;&#21464;&#24471;&#26377;&#38480;&#65292;&#24182;&#19988;&#20849;&#20139;&#25972;&#20010;&#23618;&#25110;&#22359;&#26174;&#28982;&#38477;&#20302;&#20102;&#21442;&#25968;&#20849;&#20139;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textbf{&#38754;&#21521;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22836;&#37096;&#20849;&#20139;&#27880;&#24847;&#21147;}$&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#27880;&#24847;&#21147;&#22836;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;LLMs&#12290;&#23427;&#20204;&#37117;&#20351;&#29992;&#30456;&#21516;&#30340;&#21160;&#24577;&#31574;&#30053;&#36873;&#25321;&#20849;&#20139;&#30340;&#21442;&#25968;&#30697;&#38453;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#30452;&#25509;&#37325;&#22797;&#20351;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11819v1 Announce Type: new  Abstract: Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices. Weight sharing is one promising solution that encourages weight reuse, effectively reducing memory usage with less performance drop. However, current weight sharing techniques primarily focus on small-scale models like BERT and employ coarse-grained sharing rules, e.g., layer-wise. This becomes limiting given the prevalence of LLMs and sharing an entire layer or block obviously diminishes the flexibility of weight sharing. In this paper, we present a perspective on $\textit{$\textbf{head-wise shareable attention for large language models}$}$. We further propose two memory-efficient methods that share parameters across attention heads, with a specific focus on LLMs. Both of them use the same dynamic strategy to select the shared weight matrices. The first method directly reuses the pre-trained weights without retraining, d
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#20197;&#21450;&#26032;&#20852;&#30340;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#36824;&#35752;&#35770;&#20102;LLMs&#30340;&#26368;&#26032;&#33539;&#20363;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05121</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Table Processing: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#20197;&#21450;&#26032;&#20852;&#30340;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#36824;&#35752;&#35770;&#20102;LLMs&#30340;&#26368;&#26032;&#33539;&#20363;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#36890;&#24120;&#26159;&#20108;&#32500;&#32467;&#26500;&#21270;&#30340;&#65292;&#29992;&#20110;&#23384;&#20648;&#22823;&#37327;&#25968;&#25454;&#65292;&#22312;&#25968;&#25454;&#24211;&#26597;&#35810;&#12289;&#30005;&#23376;&#34920;&#26684;&#35745;&#31639;&#21644;&#20174;&#32593;&#32476;&#34920;&#26684;&#29983;&#25104;&#25253;&#21578;&#31561;&#26085;&#24120;&#27963;&#21160;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#36825;&#20123;&#20197;&#34920;&#26684;&#20026;&#20013;&#24515;&#30340;&#20219;&#21153;&#21487;&#20197;&#24102;&#26469;&#37325;&#22823;&#30340;&#20844;&#20247;&#21033;&#30410;&#65292;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#20852;&#36259;&#12290;&#35813;&#35843;&#26597;&#23545;&#34920;&#26684;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27010;&#36848;&#65292;&#19981;&#20165;&#28085;&#30422;&#20256;&#32479;&#39046;&#22495;&#22914;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#65288;Table QA&#65289;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#36824;&#21253;&#25324;&#26368;&#36817;&#24378;&#35843;&#30340;&#26041;&#38754;&#65292;&#22914;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#36229;&#36234;&#20102;&#26089;&#26399;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;LLM&#20351;&#29992;&#20013;&#30340;&#26368;&#26032;&#33539;&#20363;&#12290;&#37325;&#28857;&#26159;LLMs&#39046;&#22495;&#20869;&#30340;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#20960;&#20010;&#25361;&#25112;&#65292;&#28085;&#30422;&#31169;&#26377;&#37096;&#32626;&#12289;&#39640;&#25928;&#25512;&#26029;&#21644; LLMS &#21457;&#23637;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet calculations, and generating reports from web tables. Automating these table-centric tasks with Large Language Models (LLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides an extensive overview of table tasks, encompassing not only the traditional areas like table question answering (Table QA) and fact verification, but also newly emphasized aspects such as table manipulation and advanced table data analysis. Additionally, it goes beyond the early strategies of pre-training and fine-tuning small language models, to include recent paradigms in LLM usage. The focus here is particularly on instruction-tuning, prompting, and agent-based approaches within the realm of LLMs. Finally, we highlight several challenges, ranging from private deployment and efficient inference to the developmen
&lt;/p&gt;</description></item></channel></rss>