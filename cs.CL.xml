<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2404.00656</link><description>&lt;p&gt;
WavLLM&#65306;&#38754;&#21521;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WavLLM: Towards Robust and Adaptive Speech Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00656
&lt;/p&gt;
&lt;p&gt;
WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36880;&#28176;&#25299;&#23485;&#20102;&#23427;&#20204;&#30340;&#33539;&#22260;&#21040;&#22810;&#27169;&#24577;&#24863;&#30693;&#21644;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#21548;&#35273;&#33021;&#21147;&#25972;&#21512;&#21040;LLMs&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#36328;&#19981;&#21516;&#35821;&#22659;&#21644;&#25191;&#34892;&#22797;&#26434;&#21548;&#35273;&#20219;&#21153;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WavLLM&#65292;&#19968;&#20010;&#20855;&#26377;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#30340;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#21033;&#29992;Whisper&#32534;&#30721;&#22120;&#22788;&#29702;&#35821;&#38899;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#21033;&#29992;WavLM&#32534;&#30721;&#22120;&#25429;&#25417;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#22312;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;WavLLM&#39318;&#20808;&#36890;&#36807;&#28151;&#21512;&#35201;&#32032;&#36827;&#34892;&#20248;&#21270;&#26469;&#24314;&#31435;&#20854;&#22522;&#30784;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00656v1 Announce Type: cross  Abstract: The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elemen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#21644;&#20462;&#21098;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.19631</link><description>&lt;p&gt;
&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26816;&#32034;&#22686;&#24378;&#30693;&#35782;&#32534;&#36753;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#21644;&#20462;&#21098;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#39640;&#25928;&#33021;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#25972;&#21512;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#65292;&#23548;&#33268;&#21487;&#33021;&#36807;&#26102;&#25110;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#24403;&#22788;&#29702;&#22810;&#36339;&#38382;&#39064;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;LLMs&#26356;&#26032;&#21644;&#25972;&#21512;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#30693;&#35782;&#29255;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#23450;&#21046;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#12290;RAE&#39318;&#20808;&#26816;&#32034;&#32534;&#36753;&#21518;&#30340;&#20107;&#23454;&#65292;&#28982;&#21518;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#23436;&#21892;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26816;&#32034;&#26041;&#27861;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#26469;&#35782;&#21035;&#38142;&#24335;&#20107;&#23454;&#65292;&#32780;&#22825;&#30495;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25628;&#32034;&#21487;&#33021;&#20250;&#24573;&#30053;&#36825;&#20123;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#37319;&#29992;&#20102;&#20462;&#21098;&#31574;&#30053;&#65292;&#20174;&#26816;&#32034;&#21040;&#30340;&#20107;&#23454;&#20013;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#65292;&#36825;&#22686;&#24378;&#20102;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19631v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that na\"ive similarity-based searches might miss. Additionally, our framework incorporates a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the edi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#27700;&#21360;&#20914;&#31361;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#21452;&#27700;&#21360;&#20914;&#31361;&#23384;&#22312;&#26102;&#20250;&#23545;&#27700;&#21360;&#31639;&#27861;&#30340;&#26816;&#27979;&#24615;&#33021;&#36896;&#25104;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2403.10020</link><description>&lt;p&gt;
&#22312;&#37325;&#21472;&#20013;&#36855;&#22833;&#65306;&#25506;&#32034;LLMs&#20013;&#30340;&#27700;&#21360;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Lost in Overlap: Exploring Watermark Collision in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#27700;&#21360;&#20914;&#31361;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#21452;&#27700;&#21360;&#20914;&#31361;&#23384;&#22312;&#26102;&#20250;&#23545;&#27700;&#21360;&#31639;&#27861;&#30340;&#26816;&#27979;&#24615;&#33021;&#36896;&#25104;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#26222;&#21450;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#25991;&#26412;&#29256;&#26435;&#30340;&#25285;&#24551;&#12290;&#27700;&#21360;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;logit&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#21487;&#23519;&#35273;&#30340;&#26631;&#35782;&#23884;&#20837;&#25991;&#26412;&#20013;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#27700;&#21360;&#26041;&#27861;&#22312;&#19981;&#21516;LLMs&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20102;&#19968;&#31181;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#24120;&#35265;&#20219;&#21153;&#65288;&#22914;&#38382;&#31572;&#21644;&#25913;&#20889;&#65289;&#20013;&#21457;&#29983;&#30340;&#27700;&#21360;&#20914;&#31361;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#21452;&#27700;&#21360;&#20914;&#31361;&#65292;&#21363;&#21516;&#19968;&#25991;&#26412;&#20013;&#21516;&#26102;&#23384;&#22312;&#20004;&#20010;&#27700;&#21360;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#27700;&#21360;&#20914;&#31361;&#23545;&#19978;&#28216;&#21644;&#19979;&#28216;&#27700;&#21360;&#31639;&#27861;&#30340;&#26816;&#27979;&#22120;&#30340;&#26816;&#27979;&#24615;&#33021;&#26500;&#25104;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10020v1 Announce Type: new  Abstract: The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing. This study focuses on dual watermark collisions, where two watermarks are present simultaneously in the same text. The research demonstrates that watermark collision poses a threat to detection performance for detectors of both upstream and downstream watermark algorithms.
&lt;/p&gt;</description></item><item><title>$\texttt{COSMIC}$&#26159;&#19968;&#31181;&#20197;&#30456;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#26032;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#31454;&#20105;&#24615;&#33021;&#20248;&#20110;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#12290;</title><link>https://arxiv.org/abs/2402.19457</link><description>&lt;p&gt;
$\texttt{COSMIC}$: &#30456;&#20114;&#20449;&#24687;&#29992;&#20110;&#20219;&#21153;&#26080;&#20851;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19457
&lt;/p&gt;
&lt;p&gt;
$\texttt{COSMIC}$&#26159;&#19968;&#31181;&#20197;&#30456;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#26032;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#31454;&#20105;&#24615;&#33021;&#20248;&#20110;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24635;&#32467;&#36136;&#37327;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#26681;&#25454;&#24635;&#32467;&#22120;&#29983;&#25104;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#19988;&#20445;&#30041;&#20219;&#21153;&#32467;&#26524;&#30340;&#25688;&#35201;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#36825;&#20123;&#20219;&#21153;&#30340;&#32467;&#26524;&#38169;&#35823;&#27010;&#29575;&#19982;&#28304;&#25991;&#26412;&#21644;&#29983;&#25104;&#25688;&#35201;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;$\texttt{COSMIC}$&#20316;&#20026;&#36825;&#19968;&#24230;&#37327;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#23637;&#31034;&#20102;&#23427;&#19982;&#22522;&#20110;&#20154;&#31867;&#21028;&#26029;&#30340;&#24230;&#37327;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;&#24050;&#24314;&#31435;&#30340;&#24230;&#37327;&#22914;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#30340;&#27604;&#36739;&#20998;&#26512;&#20984;&#26174;&#20102;$\texttt{COSMIC}$&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19457v1 Announce Type: cross  Abstract: Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks, while preserving task outcomes. We theoretically establish a direct relationship between the resulting error probability of these tasks and the mutual information between source texts and generated summaries. We introduce $\texttt{COSMIC}$ as a practical implementation of this metric, demonstrating its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$ highlight the competitive performance of $\texttt{COSMIC}$.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#28608;&#27963;&#29616;&#35937;&#65292;&#23427;&#20204;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#20540;&#24182;&#19988;&#22312;&#27169;&#22411;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17762</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22823;&#37327;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Massive Activations in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17762
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#28608;&#27963;&#29616;&#35937;&#65292;&#23427;&#20204;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#20540;&#24182;&#19988;&#22312;&#27169;&#22411;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19968;&#20010;&#32463;&#39564;&#29616;&#35937;&#8212;&#8212;&#24456;&#23569;&#30340;&#28608;&#27963;&#23637;&#29616;&#20986;&#27604;&#20854;&#20182;&#28608;&#27963;&#26126;&#26174;&#26356;&#22823;&#30340;&#20540;&#65288;&#20363;&#22914;&#65292;&#22823;&#20986; 100,000 &#20493;&#65289;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#22823;&#37327;&#28608;&#27963;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#37327;&#28608;&#27963;&#22312;&#21508;&#31181;LLMs&#20013;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#23545;&#20854;&#20301;&#32622;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#20540;&#22522;&#26412;&#19978;&#19981;&#21463;&#36755;&#20837;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;LLMs&#20013;&#36215;&#21040;&#19981;&#21487;&#25110;&#32570;&#30340;&#20559;&#32622;&#39033;&#20316;&#29992;&#12290;&#31532;&#19977;&#65292;&#36825;&#20123;&#22823;&#37327;&#28608;&#27963;&#23548;&#33268;&#20851;&#27880;&#27010;&#29575;&#38598;&#20013;&#20110;&#20854;&#23545;&#24212;&#30340;&#26631;&#35760;&#65292;&#24182;&#36827;&#19968;&#27493;&#25104;&#20026;&#33258;&#27880;&#24847;&#36755;&#20986;&#20013;&#30340;&#38544;&#24335;&#20559;&#32622;&#39033;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35270;&#35273;Transformer&#20013;&#30340;&#22823;&#37327;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17762v1 Announce Type: new  Abstract: We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.
&lt;/p&gt;</description></item><item><title>FIPO&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#30001;&#24418;&#24335;&#25351;&#23548;&#30340;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#27169;&#22359;&#21270;&#24494;&#35843;&#27169;&#24335;&#65292;&#37325;&#26032;&#26500;&#24605;&#20102;&#20248;&#21270;&#36807;&#31243;&#24182;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#20219;&#21153;&#25552;&#31034;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.11811</link><description>&lt;p&gt;
FIPO&#65306;&#22522;&#20110;&#33258;&#30001;&#24418;&#24335;&#25351;&#23548;&#30340;&#25552;&#31034;&#20248;&#21270;&#19982;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#27169;&#22359;&#21270;&#24494;&#35843;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11811
&lt;/p&gt;
&lt;p&gt;
FIPO&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#30001;&#24418;&#24335;&#25351;&#23548;&#30340;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#27169;&#22359;&#21270;&#24494;&#35843;&#27169;&#24335;&#65292;&#37325;&#26032;&#26500;&#24605;&#20102;&#20248;&#21270;&#36807;&#31243;&#24182;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#20219;&#21153;&#25552;&#31034;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20419;&#36827;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#26368;&#32456;&#29992;&#25143;-&#26426;&#22120;&#20154;&#20132;&#20114;&#20013;&#30340;&#28145;&#24230;&#26234;&#33021;&#26041;&#38754;&#65292;&#25552;&#31034;&#21019;&#20316;&#30340;&#33402;&#26415;&#34987;&#35270;&#20026;&#26222;&#36890;&#29992;&#25143;&#30340;&#19968;&#39033;&#20851;&#38190;&#20294;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#19982;&#20043;&#21069;&#22522;&#20110;&#27169;&#22411;&#32780;&#19981;&#32771;&#34385;&#25351;&#23548;&#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#20026;&#39044;&#23450;&#20041;&#30446;&#26631;&#27169;&#22411;&#20135;&#29983;&#20102;&#20809;&#28369;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#20351;&#29992;&#24320;&#31665;&#21363;&#29992;&#27169;&#22411;&#26102;&#23481;&#26131;&#24555;&#36895;&#36864;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#30001;&#24418;&#24335;&#25351;&#23548;&#30340;&#25552;&#31034;&#20248;&#21270;&#65288;FIPO&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#65292;&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#24494;&#35843;&#27169;&#24335;&#12290;FIPO&#27169;&#24335;&#37325;&#26032;&#26500;&#24605;&#20102;&#20248;&#21270;&#36807;&#31243;&#65292;&#23558;&#20854;&#20998;&#35299;&#20026;&#21487;&#31649;&#29702;&#30340;&#27169;&#22359;&#65292;&#20197;&#21160;&#24577;&#35843;&#25972;&#20869;&#23481;&#30340;&#20803;&#25552;&#31034;&#20026;&#38170;&#28857;&#12290;&#36825;&#20801;&#35768;&#28789;&#27963;&#25972;&#21512;&#21407;&#22987;&#20219;&#21153;&#25351;&#23548;&#12289;&#21487;&#36873;&#25351;&#23548;&#21709;&#24212;&#21644;&#21487;&#36873;&#30495;&#23454;&#20540;&#65292;&#20197;&#29983;&#25104;&#32463;&#36807;&#31934;&#24515;&#20248;&#21270;&#30340;&#20219;&#21153;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11811v1 Announce Type: new  Abstract: In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user. Contrast to previous model-oriented yet instruction-agnostic Automatic Prompt Optimization methodologies, yielding polished results for predefined target models while suffering rapid degradation with out-of-box models, we present Free-form Instruction-oriented Prompt Optimization (FIPO). This approach is supported by our large-scale prompt preference dataset and employs a modular fine-tuning schema. The FIPO schema reimagines the optimization process into manageable modules, anchored by a meta prompt that dynamically adapts content. This allows for the flexible integration of the raw task instruction, the optional instruction response, and the optional ground truth to produce finely optimized task prompts. The FIPO preference
&lt;/p&gt;</description></item><item><title>V-STaR&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#25913;&#36827;&#21644;&#39564;&#35777;&#26041;&#27861;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.06457</link><description>&lt;p&gt;
V-STaR: &#33258;&#23398;&#25512;&#29702;&#22120;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
V-STaR: Training Verifiers for Self-Taught Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06457
&lt;/p&gt;
&lt;p&gt;
V-STaR&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#25913;&#36827;&#21644;&#39564;&#35777;&#26041;&#27861;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#33258;&#25105;&#25913;&#36827;&#26041;&#27861;&#65292;&#20363;&#22914;STaR&#65288;Zelikman&#31561;&#20154;&#65292;2022&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#36845;&#20195;&#24494;&#35843;LLM&#20197;&#25552;&#39640;&#20854;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27492;&#36807;&#31243;&#20013;&#20002;&#24323;&#20102;&#22823;&#37327;&#30340;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#33021;&#24573;&#30053;&#20102;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;V-STaR&#65292;&#23427;&#21033;&#29992;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20351;&#29992;DPO&#35757;&#32451;&#19968;&#20010;&#21028;&#26029;&#27169;&#22411;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#27491;&#30830;&#24615;&#30340;&#39564;&#35777;&#22120;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#36825;&#20010;&#39564;&#35777;&#22120;&#29992;&#26469;&#22312;&#20247;&#22810;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#20013;&#36873;&#25321;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22810;&#27425;&#36816;&#34892;V-STaR&#20250;&#36880;&#27493;&#20135;&#29983;&#26356;&#22909;&#30340;&#25512;&#29702;&#22120;&#21644;&#39564;&#35777;&#22120;&#65292;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;LLaMA2&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common self-improvement approaches for large language models (LLMs), such as STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01766</link><description>&lt;p&gt;
LLM&#25237;&#31080;&#65306;&#20154;&#31867;&#36873;&#25321;&#21644;AI&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
LLM Voting: Human Choices and AI Collective Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#19982;&#20154;&#31867;&#25237;&#31080;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36827;&#34892;&#20154;&#31867;&#25237;&#31080;&#23454;&#39564;&#20197;&#24314;&#31435;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#19982;LLM&#20195;&#29702;&#36827;&#34892;&#24179;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#38598;&#20307;&#32467;&#26524;&#21644;&#20010;&#20307;&#20559;&#22909;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#21644;LLMs&#20043;&#38388;&#22312;&#20915;&#31574;&#21644;&#22266;&#26377;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#22312;&#20559;&#22909;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#30456;&#27604;&#20154;&#31867;&#36873;&#27665;&#30340;&#22810;&#26679;&#20559;&#22909;&#65292;LLMs&#26377;&#26356;&#36235;&#21521;&#20110;&#19968;&#33268;&#36873;&#25321;&#30340;&#20542;&#21521;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03710</link><description>&lt;p&gt;
&#20195;&#29702;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36890;&#29992;&#30340;&#38646;-shot&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Agent Instructs Large Language Models to be General Zero-Shot Reasoners. (arXiv:2310.03710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#36827;&#19968;&#27493;&#37322;&#25918;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#26356;&#22810;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#29983;&#25104;&#12289;&#20998;&#31867;&#21644;&#25512;&#29702;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#65292;&#24182;&#22312;&#25105;&#20204;&#35780;&#20272;&#30340;29&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#22312;20&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;Vicuna-13b&#65288;13.3%&#65289;&#65292;Llama-2-70b-chat&#65288;23.2%&#65289;&#21644;GPT-3.5 Turbo&#65288;17.0%&#65289;&#12290;&#19982;&#38646;-shot&#24605;&#32500;&#38142;&#30456;&#27604;&#65292;&#25105;&#20204;&#23545;&#25512;&#29702;&#30340;&#25913;&#36827;&#24456;&#26126;&#26174;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;10.5%&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Llama-2-70b-chat&#30340;&#24615;&#33021;&#36229;&#36807;&#38646;-shot GPT-3.5 Turbo 10.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item></channel></rss>