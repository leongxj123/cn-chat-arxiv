<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#30340;&#26041;&#24335;&#23545;&#24515;&#20869;&#30005;&#22270;&#36827;&#34892;&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#12290;&#30456;&#27604;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25151;&#39076;&#20998;&#31867;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01115</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#35299;&#35835;&#24515;&#20869;&#30005;&#22270;
&lt;/p&gt;
&lt;p&gt;
Interpretation of Intracardiac Electrograms Through Textual Representations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#30340;&#26041;&#24335;&#23545;&#24515;&#20869;&#30005;&#22270;&#36827;&#34892;&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#12290;&#30456;&#27604;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25151;&#39076;&#20998;&#31867;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#25151;&#39076;(AFib)&#30340;&#19981;&#35268;&#21017;&#30005;&#27963;&#21160;&#19968;&#30452;&#26159;&#24515;&#30005;&#22270;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#23545;&#20110;&#20005;&#37325;&#30340;&#25151;&#39076;&#30149;&#20363;&#65292;&#36827;&#34892;&#23548;&#31649;&#28040;&#34701;&#20197;&#33719;&#21462;&#24515;&#20869;&#30005;&#22270;(EGMs)&#12290;EGMs&#25552;&#20379;&#20102;&#24515;&#33039;&#30005;&#27963;&#21160;&#30340;&#22797;&#26434;&#32454;&#33410;&#21644;&#23616;&#37096;&#21270;&#20449;&#24687;&#65292;&#26159;&#21487;&#35299;&#37322;&#30340;&#24515;&#33039;&#30740;&#31350;&#30340;&#29702;&#24819;&#27169;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#36827;&#23637;&#20351;&#24471;&#19968;&#20123;&#30740;&#31350;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#37322;&#25151;&#39076;&#20013;&#30340;EGMs&#12290;&#27492;&#22806;&#65292;&#35821;&#35328;&#27169;&#22411;(LMs)&#22312;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LMs&#26469;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#23545;EGM&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23558;EGM&#24418;&#24335;&#21270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#19982;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#25151;&#39076;&#20998;&#31867;&#26041;&#38754;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the irregular electrical activity of atrial fibrillation (AFib) has been a key challenge in electrocardiography. For serious cases of AFib, catheter ablations are performed to collect intracardiac electrograms (EGMs). EGMs offer intricately detailed and localized electrical activity of the heart and are an ideal modality for interpretable cardiac studies. Recent advancements in artificial intelligence (AI) has allowed some works to utilize deep learning frameworks to interpret EGMs during AFib. Additionally, language models (LMs) have shown exceptional performance in being able to generalize to unseen domains, especially in healthcare. In this study, we are the first to leverage pretrained LMs for finetuning of EGM interpolation and AFib classification via masked language modeling. We formulate the EGM as a textual sequence and present competitive performances on AFib classification compared against other representations. Lastly, we provide a comprehensive interpretabilit
&lt;/p&gt;</description></item><item><title>CMAT&#26694;&#26550;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;</title><link>https://arxiv.org/abs/2404.01663</link><description>&lt;p&gt;
CMAT: &#29992;&#20110;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01663
&lt;/p&gt;
&lt;p&gt;
CMAT&#26694;&#26550;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;LLMs&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#26377;&#25928;&#25805;&#20316;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#31867;&#36755;&#20837;&#26469;&#20934;&#30830;&#24341;&#23548;&#23545;&#35805;&#27969;&#31243;&#65292;&#26234;&#33021;&#20307;&#35843;&#25972;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#28041;&#21450;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#35843;&#25972;&#65292;&#20197;&#26356;&#22909;&#22320;&#21709;&#24212;&#36825;&#31181;&#24341;&#23548;&#12290;&#38024;&#23545;&#36825;&#19968;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Collaborative Multi-Agent Tuning&#65288;CMAT&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#26681;&#25454;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#26469;&#22686;&#24378;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#23398;&#20064;&#21644;&#23454;&#26102;&#36866;&#24212;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01663v1 Announce Type: new  Abstract: Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this resear
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.14734</link><description>&lt;p&gt;
&#19968;&#39033;&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#30340;&#35843;&#26597;&#65306;&#33539;&#24335;&#12289;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14734
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#31070;&#32463;&#20195;&#30721;&#26234;&#33021;--&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;--&#22312;&#25972;&#20010;&#31038;&#20250;&#19978;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#36807;&#21435;&#20960;&#24180;&#24341;&#36215;&#20102;&#20004;&#20010;&#30740;&#31350;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#22238;&#39038;&#20102;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21450;&#20854;&#21464;&#20307;&#12289;20&#22810;&#31181;&#20219;&#21153;&#31867;&#21035;&#20197;&#21450;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#12290;&#25105;&#20204;&#36981;&#24490;&#21382;&#21490;&#36827;&#23637;&#65292;&#36319;&#36394;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#36716;&#21464;&#65288;&#20363;&#22914;&#65292;&#20174;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20195;&#30721;&#24314;&#27169;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#38454;&#27573;&#28085;&#30422;&#30340;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35780;&#20272;&#30340;&#20027;&#35201;&#25216;&#26415;&#36716;&#21464;&#12290;&#23545;&#20110;&#24212;&#29992;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 Announce Type: cross  Abstract: Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we 
&lt;/p&gt;</description></item><item><title>Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11905</link><description>&lt;p&gt;
Tur[k]ingBench&#65306;&#29992;&#20110;&#32593;&#32476;&#20195;&#29702;&#30340;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench: A Challenge Benchmark for Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11905
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#22312;&#21407;&#22987;&#25991;&#26412;&#24418;&#24335;&#19979;&#29702;&#35299;&#21644;&#20132;&#27969;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19990;&#30028;&#19978;&#19981;&#20165;&#20165;&#26159;&#21407;&#22987;&#25991;&#26412;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#22312;&#32593;&#39029;&#19978;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22312;&#36825;&#20123;&#32593;&#39029;&#19978;&#65292;&#25991;&#26412;&#19982;&#20854;&#20182;&#24418;&#24335;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#24182;&#20197;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#30340;&#24418;&#24335;&#23436;&#25104;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#36825;&#31181;&#22797;&#26434;&#30340;&#39046;&#22495;&#21602;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TurkingBench&#65292;&#19968;&#20010;&#30001;&#21253;&#21547;&#22810;&#27169;&#24577;&#32972;&#26223;&#30340;&#25991;&#26412;&#35828;&#26126;&#21046;&#23450;&#30340;&#20219;&#21153;&#22522;&#20934;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#20154;&#24037;&#21512;&#25104;&#30340;&#32593;&#39029;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21508;&#31181;&#27880;&#37322;&#30446;&#30340;&#30340;&#33258;&#28982;HTML&#39029;&#38754;&#12290;&#27599;&#20010;&#20219;&#21153;&#30340;HTML&#35828;&#26126;&#20063;&#34987;&#23454;&#20363;&#21270;&#20026;&#21508;&#31181;&#20540;&#65288;&#20174;&#20247;&#21253;&#20219;&#21153;&#33719;&#24471;&#65289;&#20197;&#24418;&#25104;&#20219;&#21153;&#30340;&#26032;&#23454;&#20363;&#12290;&#36825;&#20010;&#22522;&#20934;&#21253;&#21547;32.2K&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11807</link><description>&lt;p&gt;
LLM&#30340;&#20915;&#31574;&#27700;&#24179;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#31350;&#31455;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#21508;&#31181;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26497;&#22909;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35270;&#35282;&#25506;&#31350;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25903;&#25345;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#21442;&#19982;&#30340;&#28216;&#25103;&#65292;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;GAMA-Bench&#65292;&#21253;&#25324;&#20843;&#20010;&#32463;&#20856;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20998;&#26041;&#26696;&#65292;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;GAMA-Bench&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#31283;&#20581;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#22686;&#24378;&#31574;&#30053;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;GPT-3.5&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#20854;&#27867;&#21270;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19968;&#20123;&#26041;&#27861;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11807v1 Announce Type: new  Abstract: Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other mod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11169</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Correcting misinformation on social media with a large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#20449;&#24687;&#20250;&#30772;&#22351;&#20844;&#20247;&#23545;&#31185;&#23398;&#21644;&#27665;&#20027;&#30340;&#20449;&#20219;&#65292;&#29305;&#21035;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#19981;&#20934;&#30830;&#20449;&#24687;&#20250;&#36805;&#36895;&#20256;&#25773;&#12290;&#19987;&#23478;&#21644;&#26222;&#36890;&#20154;&#36890;&#36807;&#25163;&#21160;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#20934;&#30830;&#20449;&#24687;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#32416;&#27491;&#35823;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#25193;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#25285;&#24551;&#65292;&#22240;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31561;&#25216;&#26415;&#20351;&#35823;&#20449;&#24687;&#26356;&#23481;&#26131;&#29983;&#25104;&#12290;LLMs&#36824;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#65292;&#21487;&#20197;&#21152;&#36895;&#32416;&#27491;&#35823;&#20449;&#24687;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30001;&#20110;&#32570;&#20047;&#26368;&#26032;&#20449;&#24687;&#12289;&#20542;&#21521;&#20110;&#29983;&#25104;&#20284;&#26159;&#32780;&#38750;&#30340;&#20869;&#23481;&#21644;&#24341;&#29992;&#20197;&#21450;&#26080;&#27861;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#32780;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MUSE&#65292;&#36825;&#26159;&#19968;&#20010;&#24102;&#26377;&#26368;&#26032;&#20449;&#24687;&#35775;&#38382;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#30340;LLM&#12290;&#36890;&#36807;&#26816;&#32034;&#19978;&#19979;&#25991;&#35777;&#25454;&#21644;&#21453;&#39539;&#65292;MUSE&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#21487;&#20449;&#30340;&#35299;&#37322;&#21644;&#21442;&#32771;&#12290;&#23427;&#36824;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11169v1 Announce Type: cross  Abstract: Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce. LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information. To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#37327;&#21270;&#24615;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.08564</link><description>&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Non-discrimination Criteria for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#37327;&#21270;&#24615;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32463;&#21382;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#36234;&#26469;&#36234;&#26222;&#36941;&#22320;&#25552;&#20379;&#32473;&#20844;&#20247;&#20351;&#29992;&#65292;&#20154;&#20204;&#24320;&#22987;&#25285;&#24515;&#22312;&#24212;&#29992;&#20013;&#24310;&#32493;&#21644;&#25918;&#22823;&#26377;&#23475;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#21487;&#33021;&#23545;&#20854;&#38024;&#23545;&#30340;&#20010;&#20154;&#36896;&#25104;&#20260;&#23475;&#21644;&#38480;&#21046;&#65292;&#26080;&#35770;&#26159;&#30001;&#35823;&#20256;&#36824;&#26159;&#27495;&#35270;&#25152;&#26500;&#25104;&#12290;&#35782;&#21035;&#24615;&#21035;&#20559;&#35265;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#30340;&#31038;&#20250;&#26500;&#36896;&#65292;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21457;&#29616;&#21644;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19977;&#20010;&#26469;&#33258;&#20998;&#31867;&#30340;&#33879;&#21517;&#38750;&#27495;&#35270;&#26631;&#20934;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31867;&#27604;&#65292;&#21363;&#29420;&#31435;&#24615;&#12289;&#20998;&#31163;&#24615;&#21644;&#20805;&#20998;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#20123;&#26631;&#20934;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#27599;&#20010;&#26631;&#20934;&#30340;&#25552;&#31034;&#65292;&#37325;&#28857;&#20851;&#27880;&#32844;&#19994;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#20855;&#20307;&#21033;&#29992;&#21307;&#23398;&#27979;&#35797;&#26469;&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32972;&#26223;&#20013;&#24341;&#20837;&#22522;&#26412;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08564v1 Announce Type: cross  Abstract: Within recent years, generative AI, such as large language models, has undergone rapid development. As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications. Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination. Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in generative language models. In particular, we derive generative AI analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency. To demonstrate these criteria in action, we design prompts for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the generative AI context. 
&lt;/p&gt;</description></item><item><title>FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06764</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#22312;&#31532;&#20108;&#23618;&#20043;&#21518;&#20215;&#20540;1/2&#20195;&#24065;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06764
&lt;/p&gt;
&lt;p&gt;
FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#20013;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#23384;&#22312;&#20302;&#25928;&#29616;&#35937;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#21517;&#27169;&#22411;&#22914;LLaVA-1.5&#12289;QwenVL-Chat&#21644;Video-LLaVA&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27969;&#34892;&#30340;LVLMs&#30340;&#28145;&#23618;&#20013;&#65292;&#23545;&#35270;&#35273;&#20195;&#24065;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26497;&#20854;&#20302;&#25928;&#65292;&#26263;&#31034;&#30456;&#36739;&#20110;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#65292;&#38656;&#35201;&#26356;&#31232;&#30095;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FastV&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#26089;&#26399;&#23618;&#20013;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#21644;&#22312;&#38543;&#21518;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#26469;&#20248;&#21270;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;FastV&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;LLaVA-1.5-13B&#30340;FLOP&#20943;&#23569;&#20102;45%&#65289;&#65292;&#32780;&#19981;&#20250;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#29306;&#29298;&#24615;&#33021;&#12290;FastV&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#26435;&#34913;&#26159;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#65292;&#24182;&#19988;&#26159;&#24085;&#32047;&#25176;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06764v1 Announce Type: cross  Abstract: In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones. Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#20027;&#24352;&#25552;&#21462;&#30340;&#25688;&#35201;&#21487;&#20449;&#24230;&#35780;&#20272;&#25351;&#26631; FENICE&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#29983;&#25104;&#25688;&#35201;&#20013;&#23384;&#22312;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02270</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#20027;&#24352;&#25552;&#21462;&#30340;&#25688;&#35201;&#21487;&#20449;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#20027;&#24352;&#25552;&#21462;&#30340;&#25688;&#35201;&#21487;&#20449;&#24230;&#35780;&#20272;&#25351;&#26631; FENICE&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#29983;&#25104;&#25688;&#35201;&#20013;&#23384;&#22312;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#21363;&#22823;&#37327;&#33258;&#21160;&#29983;&#25104;&#30340;&#25688;&#35201;&#21576;&#29616;&#20107;&#23454;&#19981;&#19968;&#33268;&#65292;&#27604;&#22914;&#24187;&#35273;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#29992;&#20110;&#35780;&#20272;&#25688;&#35201;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#24341;&#20837;&#30340;&#24230;&#37327;&#26631;&#20934;&#38754;&#20020;&#30528;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#19987;&#27880;&#20110;&#30701;&#25991;&#26723;&#25688;&#35201;&#65288;&#20363;&#22914;&#26032;&#38395;&#25991;&#31456;&#65289;&#20197;&#21450;&#35745;&#31639;&#19978;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22522;&#20110;LLM&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#20027;&#24352;&#25552;&#21462;&#30340;&#25688;&#35201;&#21487;&#20449;&#24230;&#35780;&#20272;&#65288;FENICE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#21487;&#20449;&#24230;&#23548;&#21521;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02270v1 Announce Type: new  Abstract: Recent advancements in text summarization, particularly with the advent of Large Language Models (LLMs), have shown remarkable performance. However, a notable challenge persists as a substantial number of automatically-generated summaries exhibit factual inconsistencies, such as hallucinations. In response to this issue, various approaches for the evaluation of consistency for summarization have emerged. Yet, these newly-introduced metrics face several limitations, including lack of interpretability, focus on short document summaries (e.g., news articles), and computational impracticality, especially for LLM-based metrics. To address these shortcomings, we propose Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction (FENICE), a more interpretable and efficient factuality-oriented metric. FENICE leverages an NLI-based alignment between information in the source document and a set of atomic facts,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#21487;&#20449;&#24230;&#65292;&#25581;&#31034;&#20102;&#26089;&#26399;&#39044;&#35757;&#32451;LLMs&#24050;&#32463;&#33021;&#22815;&#21306;&#20998;&#21508;&#20010;&#21487;&#20449;&#24230;&#32500;&#24230;&#20013;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20174;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#20013;&#25552;&#21462;&#36716;&#21521;&#21521;&#37327;&#20197;&#22686;&#24378;LLM&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.19465</link><description>&lt;p&gt;
&#36861;&#36394;&#21487;&#20449;&#24230;&#21160;&#24577;&#65306;&#37325;&#35775;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#26399;
&lt;/p&gt;
&lt;p&gt;
Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#21487;&#20449;&#24230;&#65292;&#25581;&#31034;&#20102;&#26089;&#26399;&#39044;&#35757;&#32451;LLMs&#24050;&#32463;&#33021;&#22815;&#21306;&#20998;&#21508;&#20010;&#21487;&#20449;&#24230;&#32500;&#24230;&#20013;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20174;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#20013;&#25552;&#21462;&#36716;&#21521;&#21521;&#37327;&#20197;&#22686;&#24378;LLM&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20805;&#20998;&#39044;&#35757;&#32451;&#30340;LLMs&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25552;&#39640;LLMs&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#39044;&#35757;&#32451;&#30340;&#28508;&#21147;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;LLMs&#22312;&#27492;&#26399;&#38388;&#30340;&#21487;&#20449;&#24230;&#65292;&#19987;&#27880;&#20110;&#20116;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#21487;&#38752;&#24615;&#12289;&#38544;&#31169;&#12289;&#26377;&#23475;&#24230;&#12289;&#20844;&#24179;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;LLMs&#24212;&#29992;&#32447;&#24615;&#25506;&#27979;&#12290;&#39640;&#25506;&#27979;&#20934;&#30830;&#24230;&#34920;&#26126;&#65292;\textit{&#26089;&#26399;&#39044;&#35757;&#32451;&#30340;LLMs&#24050;&#32463;&#33021;&#22815;&#21306;&#20998;&#27599;&#20010;&#21487;&#20449;&#24230;&#32500;&#24230;&#20013;&#30340;&#27010;&#24565;}&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25581;&#31034;&#39044;&#35757;&#32451;&#30340;&#28508;&#22312;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#20174;LLM&#30340;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#20013;&#25552;&#21462;&#36716;&#21521;&#21521;&#37327;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#21487;&#20449;&#24230;&#12290;&#26368;&#21518;&#65292;&#21463;&#21040;~\citet{choi2023understanding} &#30340;&#21551;&#21457;&#65292;&#30456;&#20114;&#20449;&#24687;&#20272;&#35745;&#21463;&#32447;&#24615;&#25506;&#27979;&#20934;&#30830;&#24230;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#36824;&#29992;&#30456;&#20114;&#20449;&#24687;&#25506;&#27979;LLMs&#26469;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19465v1 Announce Type: cross  Abstract: Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#31574;&#30053;&#30340;&#27169;&#22411;&#22312;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#20013;&#30340;&#28508;&#22312;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17493</link><description>&lt;p&gt;
&#20026;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#24320;&#20855;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27491;&#30830;&#21058;&#37327;&#26159;&#22810;&#23569;&#65311;
&lt;/p&gt;
&lt;p&gt;
Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17493
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#31574;&#30053;&#30340;&#27169;&#22411;&#22312;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#20013;&#30340;&#28508;&#22312;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#21487;&#20197;&#25351;&#23548;&#26377;&#25928;&#30340;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#31649;&#29702;&#21644;&#35268;&#21010;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#39044;&#27979;&#26415;&#21518;&#39118;&#38505;&#12290;&#30740;&#31350;&#20027;&#35201;&#28041;&#21450;2018&#24180;&#33267;2021&#24180;&#38388;&#26469;&#33258;Barnes Jewish&#21307;&#38498;&#31995;&#32479;&#30340;84,875&#20221;&#35760;&#24405;&#12290;&#26041;&#27861;&#22312;Beth Israel Deaconess&#30340;MIMIC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22797;&#21046;&#12290;&#20004;&#39033;&#30740;&#31350;&#30340;&#24179;&#22343;&#38543;&#35775;&#26102;&#38388;&#22522;&#20110;&#26415;&#21518;ICU&#20303;&#38498;&#26102;&#38388;&#23567;&#20110;7&#22825;&#12290;&#23545;&#20110;BJH&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#21253;&#25324;30&#22825;&#27515;&#20129;&#29575;&#12289;&#32954;&#26643;&#22622;&#65288;PE&#65289;&#21644;&#32954;&#28814;&#12290;&#23545;BioGPT&#12289;ClinicalBERT&#21644;BioClinicalBERT&#23454;&#26045;&#20102;&#19977;&#31181;&#22495;&#33258;&#36866;&#24212;&#21644;&#24494;&#35843;&#31574;&#30053;&#65306;&#33258;&#30417;&#30563;&#30446;&#26631;&#65307;&#32467;&#21512;&#21322;&#30417;&#30563;&#24494;&#35843;&#30340;&#26631;&#31614;&#65307;&#20197;&#21450;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#22522;&#30784;&#24314;&#27169;&#12290;&#27169;&#22411;&#24615;&#33021;&#20351;&#29992;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24449;&#19979;&#30340;&#38754;&#31215;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17493v1 Announce Type: new  Abstract: Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating ch
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;&#21313;&#31181;&#22823;&#23567;&#21464;&#20307;&#30340;&#22235;&#20010;&#24320;&#28304;MLLMs&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.14154</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14154
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;&#21313;&#31181;&#22823;&#23567;&#21464;&#20307;&#30340;&#22235;&#20010;&#24320;&#28304;MLLMs&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#22810;&#27169;&#24577;&#20449;&#24687;&#20132;&#27969;&#30340;&#20013;&#24515;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#29255;&#21644;&#35270;&#39057;&#65292;&#36825;&#20351;&#24471;&#26426;&#22120;&#38590;&#20197;&#29702;&#35299;&#22312;&#32447;&#31354;&#38388;&#20013;&#20132;&#20114;&#25152;&#20851;&#32852;&#30340;&#20449;&#24687;&#25110;&#24773;&#32490;&#12290;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#31867;&#24773;&#32490;&#21644;&#35832;&#22914;&#34394;&#20551;&#20449;&#24687;&#31561;&#22797;&#26434;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;MLLMs&#23545;&#22810;&#27169;&#24577;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;MM-Soc&#25972;&#21512;&#20102;&#33879;&#21517;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#34701;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;YouTube&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#38024;&#23545;&#20174;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#12289;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21040;&#31038;&#20132;&#19978;&#19979;&#25991;&#29983;&#25104;&#31561;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#24320;&#28304;MLLMs&#30340;&#21313;&#31181;&#19981;&#21516;&#35268;&#27169;&#21464;&#20307;&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20984;&#26174;&#20986;&#20102;&#23545;&#24615;&#33021;&#24179;&#34913;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14154v1 Announce Type: new  Abstract: Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need
&lt;/p&gt;</description></item><item><title>&#25915;&#20987;&#32773;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#21644;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#26469;&#21152;&#24378;LM&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20445;&#30041;&#12290;</title><link>https://arxiv.org/abs/2402.12189</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20266;&#26631;&#31614;&#25104;&#21592;&#36164;&#26684;&#36827;&#34892;&#24494;&#35843;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#26333;&#20809;
&lt;/p&gt;
&lt;p&gt;
Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12189
&lt;/p&gt;
&lt;p&gt;
&#25915;&#20987;&#32773;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#21644;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#26469;&#21152;&#24378;LM&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;(LMs)&#30001;&#20110;&#25968;&#25454;&#35760;&#24518;&#32780;&#23481;&#26131;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#22330;&#26223;&#65292;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#25915;&#20987;&#32773;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#12290;&#35813;&#31574;&#30053;&#19981;&#21516;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#20854;&#30446;&#30340;&#26159;&#21152;&#24378;LM&#23545;&#20854;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20445;&#30041;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25915;&#20987;&#32773;&#38656;&#35201;&#25910;&#38598;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#23494;&#20999;&#30456;&#20851;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#34913;&#37327;&#29983;&#25104;&#25991;&#26412;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#37327;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#30446;&#26631;LM&#30340;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#25152;&#34920;&#31034;&#30340;&#25104;&#21592;&#36817;&#20284;&#20540;&#20026;&#36825;&#20123;&#29983;&#25104;&#25991;&#26412;&#20351;&#29992;&#20266;&#26631;&#31614;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;LM&#20197;&#25903;&#25345;&#37027;&#20123;&#26356;&#26377;&#21487;&#33021;&#28304;&#33258;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#29983;&#25104;&#25991;&#26412;&#65292;&#26681;&#25454;&#20854;&#25104;&#21592;&#36164;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12189v1 Announce Type: new  Abstract: Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their memb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#24341;&#29992;&#30340;&#26377;&#25928;&#26694;&#26550;&#65292;&#24182;&#22312;&#24120;&#35265;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.04315</link><description>&lt;p&gt;
&#20351;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#29992;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Training Language Models to Generate Text with Citations via Fine-grained Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#24341;&#29992;&#30340;&#26377;&#25928;&#26694;&#26550;&#65292;&#24182;&#22312;&#24120;&#35265;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#26041;&#38754;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#22238;&#31572;&#24120;&#24120;&#32570;&#20047;&#21487;&#38752;&#26469;&#28304;&#30340;&#24341;&#29992;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#30452;&#35266;&#26041;&#27861;&#26159;&#23558;&#22806;&#37096;&#25991;&#26723;&#30340;&#24341;&#29992;&#20316;&#20026;&#35777;&#25454;&#21253;&#21547;&#22312;&#25991;&#26412;&#20013;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#30452;&#25509;&#20419;&#20351;LLMs&#29983;&#25104;&#24341;&#29992;&#25991;&#26412;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#36828;&#38750;&#20196;&#20154;&#28385;&#24847;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36739;&#23567;&#30340;LLMs&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#25945;&#25480;LLMs&#29983;&#25104;&#39640;&#24230;&#25903;&#25345;&#21644;&#30456;&#20851;&#30340;&#24341;&#29992;&#65292;&#21516;&#26102;&#30830;&#20445;&#20854;&#21709;&#24212;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#23558;&#36825;&#20123;&#32454;&#31890;&#24230;&#22870;&#21169;&#24212;&#29992;&#20110;&#24120;&#35265;&#30340;LLMs&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#20256;&#32479;&#20570;&#27861;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#20174;ALCE&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#21462;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources. An intuitive solution to these issues would be to include in-text citations referring to external documents as evidence. While previous works have directly prompted LLMs to generate in-text citations, their performances are far from satisfactory, especially when it comes to smaller LLMs. In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses. We also conduct a systematic analysis of applying these fine-grained rewards to common LLM training strategies, demonstrating its advantage over conventional practices. We conduct extensive experiments on Question Answering (QA) datasets taken from the ALCE benchmark and validate the model's gene
&lt;/p&gt;</description></item><item><title>GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.08396</link><description>&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#32972;&#21518;&#30340;&#38544;&#34255;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine. (arXiv:2401.08396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08396
&lt;/p&gt;
&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;Vision&#21151;&#33021;&#30340;GPT-4&#22312;&#21307;&#23398;&#25361;&#25112;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20934;&#30830;&#24230;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;GPT-4V&#22312;&#35299;&#20915;&#26032;&#33521;&#26684;&#20848;&#21307;&#23398;&#26434;&#24535;&#22270;&#20687;&#25361;&#25112;&#20013;&#30340;&#22270;&#20687;&#29702;&#35299;&#12289;&#21307;&#23398;&#30693;&#35782;&#22238;&#24518;&#21644;&#36880;&#27493;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#21407;&#29702;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#23454;&#65292;GPT-4V&#22312;&#22810;&#39033;&#36873;&#25321;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#65288;88.0% vs. 77.0%&#65292;p=0.034&#65289;&#12290;GPT-4V&#22312;&#21307;&#29983;&#22238;&#31572;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#34920;&#29616;&#20986;&#36229;&#36807;80%&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4V&#22312;&#26368;&#32456;&#20570;&#20986;&#27491;&#30830;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#24120;&#25552;&#20379;&#26377;&#32570;&#38519;&#30340;&#25512;&#29702;&#65288;27.3%&#65289;&#65292;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#26159;&#22270;&#20687;&#29702;&#35299;&#65288;21.6%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies indicate that Generative Pre-trained Transformer 4 with Vision (GPT-4V) outperforms human physicians in medical challenge tasks. However, these evaluations primarily focused on the accuracy of multi-choice questions alone. Our study extends the current scope by conducting a comprehensive analysis of GPT-4V's rationales of image comprehension, recall of medical knowledge, and step-by-step multimodal reasoning when solving New England Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test the knowledge and diagnostic capabilities of medical professionals. Evaluation results confirmed that GPT-4V outperforms human physicians regarding multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in cases where physicians incorrectly answer, with over 80% accuracy. However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6
&lt;/p&gt;</description></item><item><title>AMERICANO&#26159;&#19968;&#20010;&#22522;&#20110;&#35770;&#36848;&#39537;&#21160;&#30340;&#20998;&#35299;&#21644;&#20195;&#29702;&#20132;&#20114;&#30340;&#35770;&#35777;&#29983;&#25104;&#26694;&#26550;&#65292;&#22312;&#35770;&#35777;&#29983;&#25104;&#20013;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#39034;&#24207;&#21160;&#20316;&#24182;&#32454;&#21270;&#35770;&#35777;&#33609;&#31295;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35770;&#35777;&#24615;&#35770;&#36848;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20352</link><description>&lt;p&gt;
AMERICANO:&#22522;&#20110;&#35770;&#36848;&#39537;&#21160;&#30340;&#20998;&#35299;&#21644;&#20195;&#29702;&#20132;&#20114;&#30340;&#35770;&#35777;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction. (arXiv:2310.20352v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20352
&lt;/p&gt;
&lt;p&gt;
AMERICANO&#26159;&#19968;&#20010;&#22522;&#20110;&#35770;&#36848;&#39537;&#21160;&#30340;&#20998;&#35299;&#21644;&#20195;&#29702;&#20132;&#20114;&#30340;&#35770;&#35777;&#29983;&#25104;&#26694;&#26550;&#65292;&#22312;&#35770;&#35777;&#29983;&#25104;&#20013;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#39034;&#24207;&#21160;&#20316;&#24182;&#32454;&#21270;&#35770;&#35777;&#33609;&#31295;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35770;&#35777;&#24615;&#35770;&#36848;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20005;&#26684;&#30340;&#25512;&#29702;&#21644;&#36866;&#24403;&#30340;&#20869;&#23481;&#32452;&#32455;&#12290;&#21463;&#26368;&#36817;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#21551;&#21457;&#65292;&#35813;&#25552;&#31034;&#23558;&#22797;&#26434;&#30340;&#20219;&#21153;&#20998;&#35299;&#20026;&#20013;&#38388;&#27493;&#39588;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AMERICANO&#65292;&#19968;&#20010;&#20855;&#26377;&#20195;&#29702;&#20132;&#20114;&#30340;&#26032;&#22411;&#35770;&#35777;&#29983;&#25104;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#22522;&#20110;&#35770;&#35777;&#35770;&#36848;&#30340;&#39034;&#24207;&#21160;&#20316;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#29983;&#25104;&#35770;&#35777;&#24615;&#35770;&#36848;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#21518;&#26681;&#25454;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#29983;&#25104;&#26368;&#32456;&#30340;&#35770;&#35777;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#27169;&#20223;&#20154;&#31867;&#20889;&#20316;&#36807;&#31243;&#65292;&#24182;&#25913;&#36827;&#24403;&#21069;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#20174;&#24038;&#21040;&#21491;&#29983;&#25104;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35770;&#35777;&#32454;&#21270;&#27169;&#22359;&#65292;&#26681;&#25454;&#25509;&#25910;&#21040;&#30340;&#21453;&#39304;&#33258;&#21160;&#35780;&#20272;&#21644;&#23436;&#21892;&#35770;&#35777;&#33609;&#31295;&#12290;&#25105;&#20204;&#20351;&#29992;Reddit/CMV&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21453;&#39539;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Argument generation is a challenging task in natural language processing, which requires rigorous reasoning and proper content organization. Inspired by recent chain-of-thought prompting that breaks down a complex task into intermediate steps, we propose Americano, a novel framework with agent interaction for argument generation. Our approach decomposes the generation process into sequential actions grounded on argumentation theory, which first executes actions sequentially to generate argumentative discourse components, and then produces a final argument conditioned on the components. To further mimic the human writing process and improve the left-to-right generation paradigm of current autoregressive language models, we introduce an argument refinement module which automatically evaluates and refines argument drafts based on feedback received. We evaluate our framework on the task of counterargument generation using a subset of Reddit/CMV dataset. The results show that our method out
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#23646;&#24615;/&#20540;&#25552;&#21462;&#25216;&#26415;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#23545;&#26410;&#30693;&#23646;&#24615;&#20540;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12537</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Product Attribute Value Extraction using Large Language Models. (arXiv:2310.12537v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#23646;&#24615;/&#20540;&#25552;&#21462;&#25216;&#26415;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#23545;&#26410;&#30693;&#23646;&#24615;&#20540;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#65288;&#22914;&#38754;&#21521;&#23646;&#24615;&#30340;&#20135;&#21697;&#25628;&#32034;&#25110;&#20135;&#21697;&#27604;&#36739;&#65289;&#22522;&#20110;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25551;&#36848;&#65292;&#22914;&#23646;&#24615;/&#20540;&#23545;&#12290;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#20379;&#24212;&#21830;&#19981;&#25552;&#20379;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25551;&#36848;&#65292;&#32780;&#26159;&#20351;&#29992;&#26631;&#39064;&#25110;&#25551;&#36848;&#26469;&#25551;&#36848;&#20135;&#21697;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#26679;&#30340;&#20135;&#21697;&#65292;&#26377;&#24517;&#35201;&#20174;&#25991;&#26412;&#20135;&#21697;&#23646;&#24615;&#20013;&#25552;&#21462;&#23646;&#24615;/&#20540;&#23545;&#12290;&#29616;&#26377;&#25216;&#26415;&#20013;&#65292;&#23646;&#24615;/&#20540;&#25552;&#21462;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#23646;&#24615;/&#20540;&#25552;&#21462;&#26041;&#38754;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;&#19968;&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;&#20108;&#65289;&#20248;&#21270;&#21518;&#30340;&#27169;&#22411;&#22312;&#25512;&#24191;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#21253;&#21547;&#30340;&#23646;&#24615;&#20540;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#39640;&#19988;&#40065;&#26834;&#24615;&#24378;&#30340;&#26367;&#20195;&#26041;&#27861;&#22312;&#23646;&#24615;/&#20540;&#25552;&#21462;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25176;&#31649;&#30340;LLMs&#65292;&#22914;GPT-3.5&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce applications such as faceted product search or product comparison are based on structured product descriptions like attribute/value pairs. The vendors on e-commerce platforms do not provide structured product descriptions but describe offers using titles or descriptions. To process such offers, it is necessary to extract attribute/value pairs from textual product attributes. State-of-the-art attribute/value extraction techniques rely on pre-trained language models (PLMs), such as BERT. Two major drawbacks of these models for attribute/value extraction are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models face challenges in generalizing to attribute values not included in the training data. This paper explores the potential of large language models (LLMs) as a training data-efficient and robust alternative to PLM-based attribute/value extraction methods. We consider hosted LLMs, such as GPT-3.5 and GPT-4, as well as 
&lt;/p&gt;</description></item><item><title>GPT&#36890;&#36807;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#26174;&#31034;&#20986;&#20855;&#22791;&#25104;&#20026;&#22823;&#20247;&#37329;&#34701;&#26426;&#22120;&#39038;&#38382;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#22522;&#20110;GPT-4&#30340;ChatGPT&#20960;&#20046;&#23436;&#32654;&#22320;&#24471;&#20998;99%&#65292;&#25581;&#31034;&#20102;&#37329;&#34701;&#32032;&#20859;&#27491;&#22312;&#25104;&#20026;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00649</link><description>&lt;p&gt;
GPT&#24050;&#32463;&#20855;&#22791;&#20102;&#37329;&#34701;&#32032;&#20859;&#65306;&#26469;&#33258;GPT&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#30340;&#35265;&#35299;&#20197;&#21450;&#20154;&#20204;&#20351;&#29992;&#20854;&#20316;&#20026;&#21672;&#35810;&#26469;&#28304;&#30340;&#21021;&#27493;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice. (arXiv:2309.00649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00649
&lt;/p&gt;
&lt;p&gt;
GPT&#36890;&#36807;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#26174;&#31034;&#20986;&#20855;&#22791;&#25104;&#20026;&#22823;&#20247;&#37329;&#34701;&#26426;&#22120;&#39038;&#38382;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#22522;&#20110;GPT-4&#30340;ChatGPT&#20960;&#20046;&#23436;&#32654;&#22320;&#24471;&#20998;99%&#65292;&#25581;&#31034;&#20102;&#37329;&#34701;&#32032;&#20859;&#27491;&#22312;&#25104;&#20026;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT&#65288;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#20316;&#20026;&#22823;&#20247;&#37329;&#34701;&#26426;&#22120;&#39038;&#38382;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;GPT-3.5&#30340;Davinci&#21644;ChatGPT&#20998;&#21035;&#22312;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#20013;&#24471;&#20998;&#20026;66%&#21644;65%&#65292;&#32780;&#22522;&#20110;GPT-4&#30340;ChatGPT&#20960;&#20046;&#23436;&#32654;&#22320;&#24471;&#21040;&#20102;99%&#30340;&#20998;&#25968;&#65292;&#36825;&#34920;&#26126;&#37329;&#34701;&#32032;&#20859;&#27491;&#22312;&#25104;&#20026;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;Judge-Advisor&#31995;&#32479;&#21644;&#19968;&#20010;&#20648;&#33988;&#22256;&#22659;&#26469;&#35828;&#26126;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#24314;&#35758;&#21033;&#29992;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We assess the ability of GPT -- a large language model -- to serve as a financial robo-advisor for the masses, by using a financial literacy test. Davinci and ChatGPT based on GPT-3.5 score 66% and 65% on the financial literacy test, respectively, compared to a baseline of 33%. However, ChatGPT based on GPT-4 achieves a near-perfect 99% score, pointing to financial literacy becoming an emergent ability of state-of-the-art models. We use the Judge-Advisor System and a savings dilemma to illustrate how researchers might assess advice-utilization from large language models. We also present a number of directions for future research.
&lt;/p&gt;</description></item><item><title>RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.00267</link><description>&lt;p&gt;
RLAIF: &#20351;&#29992;AI&#21453;&#39304;&#26469;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. (arXiv:2309.00267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00267
&lt;/p&gt;
&lt;p&gt;
RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#26159;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;RLHF&#21644;&#21033;&#29992;&#29616;&#25104;&#30340;LLM&#36827;&#34892;&#26631;&#35760;&#30340;RL from AI Feedback (RLAIF)&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#33021;&#33719;&#24471;&#31867;&#20284;&#30340;&#25913;&#21892;&#25928;&#26524;&#12290;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#22312;&#32422;70%&#30340;&#26696;&#20363;&#20013;&#37117;&#26356;&#21916;&#27426;RLAIF&#21644;RLHF&#20135;&#29983;&#30340;&#25991;&#26412;&#65292;&#32780;&#19981;&#26159;&#22522;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#34987;&#35201;&#27714;&#35780;&#20272;RLAIF&#21644;RLHF&#30340;&#25688;&#35201;&#26102;&#65292;&#20154;&#31867;&#20197;&#30456;&#21516;&#30340;&#27604;&#29575;&#26356;&#21916;&#27426;&#20004;&#32773;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;RLAIF&#21487;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20026;&#20811;&#26381;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20174;&#24418;&#24335;&#21270;&#30340;&#35282;&#24230;&#23545;Byte-Pair&#32534;&#30721;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36845;&#20195;&#36138;&#23146;&#29256;&#26412;&#26159;&#23545;&#26368;&#20248;&#21512;&#24182;&#24207;&#21015;&#30340;&#36817;&#20284;&#35299;&#65292;&#24182;&#20248;&#21270;&#20102;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.16837</link><description>&lt;p&gt;
Byte-Pair&#32534;&#30721;&#30340;&#24418;&#24335;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Formal Perspective on Byte-Pair Encoding. (arXiv:2306.16837v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20174;&#24418;&#24335;&#21270;&#30340;&#35282;&#24230;&#23545;Byte-Pair&#32534;&#30721;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36845;&#20195;&#36138;&#23146;&#29256;&#26412;&#26159;&#23545;&#26368;&#20248;&#21512;&#24182;&#24207;&#21015;&#30340;&#36817;&#20284;&#35299;&#65292;&#24182;&#20248;&#21270;&#20102;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Byte-Pair&#32534;&#30721;&#65288;BPE&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25968;&#25454;&#26631;&#35760;&#31639;&#27861;&#65292;&#23613;&#31649;&#26368;&#21021;&#26159;&#20316;&#20026;&#19968;&#31181;&#21387;&#32553;&#26041;&#27861;&#32780;&#35774;&#35745;&#30340;&#12290;BPE&#34920;&#38754;&#19978;&#30475;&#36215;&#26469;&#26159;&#19968;&#31181;&#36138;&#23146;&#31639;&#27861;&#65292;&#20294;&#26159;BPE&#23547;&#27714;&#35299;&#20915;&#30340;&#24213;&#23618;&#20248;&#21270;&#38382;&#39064;&#23578;&#26410;&#26126;&#30830;&#12290;&#25105;&#20204;&#23558;BPE&#24418;&#24335;&#21270;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23376;&#27169;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36845;&#20195;&#36138;&#23146;&#29256;&#26412;&#26159;&#19968;&#20010;&#23545;&#20110;&#26368;&#20248;&#21512;&#24182;&#24207;&#21015;&#30340;$\frac{1}{{\sigma(\boldsymbol{\mu}^\star)}}(1-e^{-{\sigma(\boldsymbol{\mu}^\star)}})$-&#36817;&#20284;&#35299;&#65292;&#20854;&#20013;${\sigma(\boldsymbol{\mu}^\star)}$&#26159;&#30456;&#23545;&#20110;&#26368;&#20248;&#21512;&#24182;&#24207;&#21015;$\boldsymbol{\mu}^\star$&#30340;&#24635;&#21521;&#21518;&#26354;&#29575;&#12290;&#32463;&#39564;&#35777;&#36817;&#20284;&#35299;&#30340;&#19979;&#30028;&#32422;&#20026;$\approx 0.37$&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#24555;&#30340;BPE&#23454;&#29616;&#65292;&#23558;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$\mathcal{O}\left(N M\right)$&#20248;&#21270;&#20026;$\mathcal{O}\left(N \log M\right)$&#65292;&#20854;&#20013;$N$&#26159;&#24207;&#21015;&#38271;&#24230;&#65292;$M$&#26159;&#21512;&#24182;&#27425;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#26292;&#21147;&#25628;&#32034;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method. BPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down. We formalize BPE as a combinatorial optimization problem. Via submodular functions, we prove that the iterative greedy version is a $\frac{1}{{\sigma(\boldsymbol{\mu}^\star)}}(1-e^{-{\sigma(\boldsymbol{\mu}^\star)}})$-approximation of an optimal merge sequence, where ${\sigma(\boldsymbol{\mu}^\star)}$ is the total backward curvature with respect to the optimal merge sequence $\boldsymbol{\mu}^\star$. Empirically the lower bound of the approximation is $\approx 0.37$.  We provide a faster implementation of BPE which improves the runtime complexity from $\mathcal{O}\left(N M\right)$ to $\mathcal{O}\left(N \log M\right)$, where $N$ is the sequence length and $M$ is the merge count. Finally, we optimize the brute
&lt;/p&gt;</description></item><item><title>Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.03509</link><description>&lt;p&gt;
Diffusion Explainer&#65306;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#31283;&#23450;&#25193;&#25955;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03509
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#21019;&#36896;&#36924;&#30495;&#30340;&#22270;&#20687;&#32780;&#33719;&#24471;&#20102;&#20840;&#29699;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22797;&#26434;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#25805;&#20316;&#24448;&#24448;&#20351;&#24471;&#38750;&#19987;&#19994;&#20154;&#21592;&#38590;&#20197;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Diffusion Explainer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#12290;Diffusion Explainer&#32039;&#23494;&#22320;&#23558;&#31283;&#23450;&#25193;&#25955;&#30340;&#22797;&#26434;&#32452;&#20214;&#30340;&#35270;&#35273;&#27010;&#36848;&#19982;&#20854;&#28508;&#22312;&#25805;&#20316;&#30340;&#35814;&#32454;&#35828;&#26126;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#20351;&#29992;&#25143;&#21487;&#20197;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#12290;&#36890;&#36807;&#27604;&#36739;&#30001;&#20004;&#20010;&#30456;&#20851;&#25991;&#26412;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#34920;&#31034;&#30340;&#28436;&#21464;&#26469;&#25351;&#23548;&#31934;&#32454;&#26102;&#38388;&#27493;&#38271;&#65292;&#29992;&#25143;&#21487;&#20197;&#21457;&#29616;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;Diffusion Explainer&#22312;&#29992;&#25143;&#30340;Web&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#36816;&#34892;&#65292;&#26080;&#38656;&#23433;&#35013;&#25110;&#19987;&#38376;&#30340;&#30828;&#20214;&#65292;&#25193;&#22823;&#20102;&#20844;&#20247;&#23545;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#25945;&#32946;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07687</link><description>&lt;p&gt;
MLRegTest&#65306;&#26426;&#22120;&#23398;&#20064;&#27491;&#21017;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MLRegTest: A Benchmark for the Machine Learning of Regular Languages. (arXiv:2304.07687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#24050;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#20801;&#35768;&#32454;&#33268;&#22320;&#26816;&#26597;&#23427;&#20204;&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#27169;&#24335;&#65292;&#24182;&#22312;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#26410;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#26102;&#24314;&#31435;&#20449;&#24515;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#30340;&#24207;&#21015;&#20998;&#31867;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#35757;&#32451;&#12289;&#24320;&#21457;&#21644;&#27979;&#35797;&#38598;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#24418;&#24335;&#35821;&#35328;&#20195;&#34920;&#30528;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#65292;&#24182;&#27491;&#30830;&#22320;&#35782;&#21035;&#24207;&#21015;&#20013;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26159;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25104;&#21151;&#27867;&#21270;&#30340;&#24050;&#30693;&#25361;&#25112;&#12290;MLRegTest&#26681;&#25454;&#23427;&#20204;&#30340;&#36923;&#36753;&#22797;&#26434;&#24230;&#65288;&#21333;&#35843;&#20108;&#38454;&#65292;&#19968;&#38454;&#65292;&#21629;&#39064;&#25110;&#21333;&#39033;&#24335;&#34920;&#36798;&#24335;&#65289;&#21644;&#36923;&#36753;&#25991;&#23383;&#30340;&#31181;&#31867;&#65288;&#23383;&#31526;&#20018;&#65292;&#23450;&#32423;&#23383;&#31526;&#20018;&#65292;&#23376;&#24207;&#21015;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#65289;&#32452;&#32455;&#20854;&#35821;&#35328;&#12290;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#25991;&#23383;&#30340;&#36873;&#25321;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#29702;&#35299;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#23427;&#20204;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating machine learning (ML) systems on their ability to learn known classifiers allows fine-grained examination of the patterns they can learn, which builds confidence when they are applied to the learning of unknown classifiers. This article presents a new benchmark for ML systems on sequence classification called MLRegTest, which contains training, development, and test sets from 1,800 regular languages.  Different kinds of formal languages represent different kinds of long-distance dependencies, and correctly identifying long-distance dependencies in sequences is a known challenge for ML systems to generalize successfully. MLRegTest organizes its languages according to their logical complexity (monadic second order, first order, propositional, or monomial expressions) and the kind of logical literals (string, tier-string, subsequence, or combinations thereof). The logical complexity and choice of literal provides a systematic way to understand different kinds of long-distance d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12816</link><description>&lt;p&gt;
&#20174;&#23485;&#21040;&#28145;&#65306;&#32500;&#24230;&#25552;&#21319;&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#34920;&#31034;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;KGE&#26041;&#27861;&#38656;&#35201;&#30456;&#23545;&#39640;&#32500;&#30340;&#23454;&#20307;&#34920;&#31034;&#26469;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#20250;&#23548;&#33268;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#26469;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#24320;&#21457;&#25216;&#26415;&#65288;&#20363;&#22914;&#30693;&#35782;&#33976;&#39311;&#65289;&#26469;&#34917;&#20607;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25805;&#20316;&#20250;&#23548;&#33268;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#23454;&#20307;&#34920;&#31034;&#30340;&#32423;&#32852;&#35270;&#20026;&#23884;&#20837;&#23618;&#65292;&#37027;&#20040;&#37319;&#29992;&#39640;&#32500;&#23454;&#20307;&#34920;&#31034;&#30340;&#20256;&#32479;KGE&#26041;&#27861;&#31561;&#21516;&#20110;&#25193;&#23637;&#23884;&#20837;&#23618;&#30340;&#23485;&#24230;&#20197;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#30456;&#21453;&#22320;&#22686;&#21152;&#28145;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#28145;&#30340;&#23454;&#20307;&#23884;&#20837;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
&lt;/p&gt;</description></item></channel></rss>