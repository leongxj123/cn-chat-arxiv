<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;GraphLM&#21644;&#25552;&#20986;GraphLM+&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2403.04483</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#29702;&#35299;&#21644;&#25512;&#29702;&#21151;&#33021;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;GraphInstruct
&lt;/p&gt;
&lt;p&gt;
GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;GraphLM&#21644;&#25552;&#20986;GraphLM+&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36890;&#29992;&#33021;&#21147;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22270;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#29702;&#35299;&#22270;&#25968;&#25454;&#23545;&#20110;&#25512;&#36827;&#36890;&#29992;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;&#21644;&#22686;&#24378;LLMs&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#20840;&#38754;&#21253;&#25324;21&#20010;&#32463;&#20856;&#22270;&#25512;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#22810;&#26679;&#30340;&#22270;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#35814;&#32454;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;&#22522;&#20110;GraphInstruct&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#39640;&#25928;&#30340;&#25351;&#23548;&#35843;&#25972;&#26500;&#24314;&#20102;GraphLM&#65292;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;LLM&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27493;&#39588;&#25513;&#30721;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;GraphLM+&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#22686;&#24378;LLMs&#22270;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20808;&#39537;&#24615;&#21162;&#21147;&#20043;&#19968;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04483v1 Announce Type: new  Abstract: Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demons
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04678</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#20449;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As Faithful Explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#37096;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24050;&#32463;&#33021;&#22815;&#29087;&#32451;&#35299;&#20915;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#20256;&#32479;&#30340;&#20197;&#36755;&#20837;&#20026;&#37325;&#28857;&#30340;&#35299;&#37322;&#31639;&#27861;&#26469;&#35299;&#37322;LLMs&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#26426;&#21046;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#36827;&#34892;&#21333;&#21521;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;LLMs&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#32463;&#24120;&#22240;&#20026;&#32570;&#20047;&#21487;&#20449;&#24230;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#22240;&#20026;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#19981;&#20934;&#30830;&#22320;&#21453;&#26144;LLMs&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;xLLM&#65292;&#20197;&#25552;&#39640;LLMs&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;xLLM&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#25991;&#26723;&#36130;&#21153;&#38382;&#31572;&#20219;&#21153;&#65292;&#23558;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;700&#20010;&#35789;&#25193;&#23637;&#21040;123k&#20010;&#35789;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.06915</link><description>&lt;p&gt;
DocFinQA&#65306;&#19968;&#20010;&#38271;&#25991;&#26412;&#36130;&#21153;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DocFinQA: A Long-Context Financial Reasoning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06915
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#25991;&#26723;&#36130;&#21153;&#38382;&#31572;&#20219;&#21153;&#65292;&#23558;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;700&#20010;&#35789;&#25193;&#23637;&#21040;123k&#20010;&#35789;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#39046;&#22495;&#21457;&#25381;&#20316;&#29992;&#65292;&#38656;&#35201;&#30740;&#31350;&#29616;&#23454;&#20219;&#21153;&#21644;&#25968;&#25454;&#12290;&#37329;&#34701;&#19987;&#19994;&#20154;&#22763;&#32463;&#24120;&#19982;&#38271;&#36798;&#25968;&#30334;&#39029;&#30340;&#25991;&#26723;&#36827;&#34892;&#20132;&#20114;&#65292;&#20294;&#22823;&#22810;&#25968;&#37329;&#34701;&#30740;&#31350;&#25968;&#25454;&#38598;&#20165;&#22788;&#29702;&#36825;&#20123;&#25991;&#26723;&#30340;&#31616;&#30701;&#25688;&#24405;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#25991;&#26723;&#36130;&#21153;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29616;&#26377;FinQA&#25968;&#25454;&#38598;&#20013;&#30340;7,437&#20010;&#38382;&#39064;&#20013;&#22686;&#21152;&#23436;&#25972;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#23558;FinQA&#20013;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;&#19981;&#21040;700&#20010;&#35789;&#25193;&#23637;&#21040;DocFinQA&#20013;&#30340;123k&#20010;&#35789;&#12290;&#25105;&#20204;&#22312;&#26816;&#32034;&#24335;QA&#31649;&#36947;&#21644;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#65292;DocFinQA&#20063;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#23545;DocFinQA&#20013;&#26368;&#38271;&#25991;&#26723;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#36825;&#20123;&#25991;&#26723;&#19978;&#29305;&#21035;&#22256;&#38590;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06915v2 Announce Type: replace-cross  Abstract: For large language models (LLMs) to be effective in the financial domain -- where each decision can have a significant impact -- it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents that are hundreds of pages long, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with the full-document context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents. Addressing these challen
&lt;/p&gt;</description></item></channel></rss>