<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#25512;&#29702;&#21160;&#24577;&#30340;&#35282;&#24230;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#23545;&#20107;&#23454;&#24615;&#38382;&#39064;&#21644;&#36755;&#20986; token &#27010;&#29575;&#21160;&#24577;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.20009</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#30340;&#24187;&#35273;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
On Large Language Models' Hallucination with Regard to Known Facts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20009
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#21160;&#24577;&#30340;&#35282;&#24230;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#23545;&#20107;&#23454;&#24615;&#38382;&#39064;&#21644;&#36755;&#20986; token &#27010;&#29575;&#21160;&#24577;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#20107;&#23454;&#31867;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#20063;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#29702;&#21160;&#24577;&#30340;&#35282;&#24230;&#30740;&#31350;LLMs&#20855;&#26377;&#27491;&#30830;&#31572;&#26696;&#30693;&#35782;&#21364;&#20173;&#28982;&#20135;&#29983;&#24187;&#35273;&#30340;&#29616;&#35937;&#65292;&#36825;&#26159;&#20197;&#24448;&#20851;&#20110;&#24187;&#35273;&#30740;&#31350;&#23578;&#26410;&#28085;&#30422;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#24605;&#36335;&#36827;&#34892;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26597;&#35810;&#30456;&#21516;&#19977;&#20803;&#30693;&#35782;&#20294;&#23548;&#33268;&#19981;&#21516;&#31572;&#26696;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#12290;&#27169;&#22411;&#22312;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#36755;&#20986;&#19978;&#30340;&#34892;&#20026;&#24046;&#24322;&#22240;&#27492;&#26263;&#31034;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#27169;&#24335;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#34913;&#37327;&#36825;&#31181;&#27169;&#24335;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#21097;&#20313;&#27969;&#21040;&#35789;&#27719;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#36755;&#20986;&#20196;&#29260;&#27010;&#29575;&#22312;&#27491;&#30830;&#21644;&#24187;&#35273;&#24773;&#20917;&#19979;&#22312;&#23618;&#28145;&#24230;&#19978;&#30340;&#19981;&#21516;&#21160;&#24577;&#12290;&#22312;&#24187;&#35273;&#26696;&#20363;&#20013;&#65292;&#36755;&#20986;&#20196;&#29260;&#30340;&#20449;&#24687;&#24456;&#23569;&#34920;&#29616;&#20986;&#31361;&#22686;&#21644;&#25345;&#32493;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20009v1 Announce Type: new  Abstract: Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen. Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space. We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output token's information rarely demonstrates abrupt increases and consistent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#39046;&#22495;&#30340;&#21457;&#23637;&#21382;&#21490;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#23558;&#26041;&#27861;&#20998;&#20026;&#22522;&#20110;&#25163;&#24037;&#29305;&#24449;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#31867;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.14118</link><description>&lt;p&gt;
&#20174;&#25163;&#24037;&#29305;&#24449;&#21040;LLMs&#65306;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#31616;&#35201;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#39046;&#22495;&#30340;&#21457;&#23637;&#21382;&#21490;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#23558;&#26041;&#27861;&#20998;&#20026;&#22522;&#20110;&#25163;&#24037;&#29305;&#24449;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#31867;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#65288;MTQE&#65289;&#26159;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#20272;&#35745;&#26426;&#22120;&#32763;&#35793;&#25991;&#26412;&#36136;&#37327;&#30340;&#20219;&#21153;&#65292;&#26080;&#38656;&#21442;&#32771;&#32763;&#35793;&#65292;&#23545;&#20110;MT&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;QE&#25968;&#25454;&#38598;&#12289;&#26631;&#27880;&#26041;&#27861;&#12289;&#20849;&#20139;&#20219;&#21153;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#12290;&#23427;&#20174;&#20171;&#32461;QE&#30340;&#32972;&#26223;&#21644;&#24847;&#20041;&#24320;&#22987;&#65292;&#28982;&#21518;&#35299;&#37322;&#20102;&#35789;&#32423;QE&#12289;&#21477;&#32423;QE&#12289;&#25991;&#26723;&#32423;QE&#21644;&#21487;&#35299;&#37322;QE&#30340;&#27010;&#24565;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25991;&#31456;&#23558;QE&#21457;&#23637;&#21382;&#21490;&#20013;&#20135;&#29983;&#30340;&#26041;&#27861;&#20998;&#20026;&#22522;&#20110;&#25163;&#24037;&#29305;&#24449;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20998;&#20026;&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#21644;&#21253;&#21547;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14118v1 Announce Type: new  Abstract: Machine Translation Quality Estimation (MTQE) is the task of estimating the quality of machine-translated text in real time without the need for reference translations, which is of great importance for the development of MT. After two decades of evolution, QE has yielded a wealth of results. This article provides a comprehensive overview of QE datasets, annotation methods, shared tasks, methodologies, challenges, and future research directions. It begins with an introduction to the background and significance of QE, followed by an explanation of the concepts and evaluation metrics for word-level QE, sentence-level QE, document-level QE, and explainable QE. The paper categorizes the methods developed throughout the history of QE into those based on handcrafted features, deep learning, and Large Language Models (LLMs), with a further division of deep learning-based methods into classic deep learning and those incorporating pre-trained lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08773</link><description>&lt;p&gt;
Veagle: &#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Veagle: Advancements in Multimodal Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#35821;&#35328;&#21644;&#35270;&#35273;&#22914;&#20309;&#32467;&#21512;&#20135;&#29983;&#20102;&#27987;&#21402;&#20852;&#36259;&#65292;&#20174;&#32780;&#20652;&#29983;&#20102;&#26088;&#22312;&#26080;&#32541;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24310;&#20280;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#33539;&#22260;&#20174;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#21040;&#35270;&#35273;&#23450;&#20301;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#20934;&#30830;&#35299;&#37322;&#22270;&#20687;&#24182;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#38024;&#23545;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20013;&#35266;&#23519;&#21040;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;Veagle&#65292;&#34701;&#21512;&#20102;&#21463;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08773v1 Announce Type: cross  Abstract: Lately, researchers in artificial intelligence have been really interested in how language and vision come together, giving rise to the development of multimodal models that aim to seamlessly integrate textual and visual information. Multimodal models, an extension of Large Language Models (LLMs), have exhibited remarkable capabilities in addressing a diverse array of tasks, ranging from image captioning and visual question answering (VQA) to visual grounding. While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios. This paper introduces a novel approach to enhance the multimodal capabilities of existing models. In response to the limitations observed in current Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs), our proposed model Veagle, incorporates a unique mechanism inspired by th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;StreamingDialogue&#65292;&#36890;&#36807;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#65292;&#24182;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;</title><link>https://arxiv.org/abs/2403.08312</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#25439;&#22833;&#36827;&#34892;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;StreamingDialogue&#65306;&#38271;&#23545;&#35805;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08312
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;StreamingDialogue&#65292;&#36890;&#36807;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#65292;&#24182;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#26102;&#36935;&#21040;&#20102;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#23545;&#35805;&#19978;&#19979;&#25991;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#24182;&#19988;&#23545;&#35805;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;\textit{End-of-Utterance} (EoU) &#26377;&#32858;&#21512;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23558;EoU&#26631;&#35760;&#31216;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65288;conv-attn sinks&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;StreamingDialogue&#65292;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;conv-attn&#27785;&#28857;&#65292;&#24182;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20174;&#32780;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#27785;&#28857;&#25968;&#37327;&#65288;&#21363;&#35805;&#35821;&#25968;&#37327;&#65289;&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#12290;&#24403;&#21069;&#30340;LLMs&#24050;&#32463;&#23637;&#31034;&#20102;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#31383;&#21475;&#22823;&#23567;&#36798;&#21040;200k&#29978;&#33267;&#26356;&#22823;&#12290;&#36890;&#36807;&#23558;&#35805;&#35821;&#21387;&#32553;&#20026;EoUs&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08312v1 Announce Type: cross  Abstract: Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200k or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200k of utterances, resulting in a prolonged dialogue learning. In order to minimize informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#35757;&#32451;&#25968;&#25454;&#33021;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#27604;&#20363;&#30340;&#27700;&#21360;&#35757;&#32451;&#25991;&#26412;&#65292;&#20173;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#22320;&#26816;&#27979;&#20986;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.14904</link><description>&lt;p&gt;
&#25968;&#23383;&#27700;&#21360;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#25918;&#23556;&#24615;
&lt;/p&gt;
&lt;p&gt;
Watermarking Makes Language Models Radioactive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#35757;&#32451;&#25968;&#25454;&#33021;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#27604;&#20363;&#30340;&#27700;&#21360;&#35757;&#32451;&#25991;&#26412;&#65292;&#20173;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#22320;&#26816;&#27979;&#20986;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#26816;&#27979;&#21040;&#36825;&#31181;&#36755;&#20837;&#34987;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#25104;&#21592;&#25512;&#26029;&#21487;&#20197;&#20197;&#19968;&#23450;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#36825;&#31181;&#26816;&#27979;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;&#35757;&#32451;&#25968;&#25454;&#30041;&#19979;&#30340;&#30165;&#36857;&#27604;&#25104;&#21592;&#25512;&#26029;&#26356;&#23481;&#26131;&#26816;&#27979;&#19988;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#23558;&#27745;&#26579;&#27700;&#24179;&#19982;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#12289;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#27604;&#20363;&#21644;&#24494;&#35843;&#36807;&#31243;&#32852;&#31995;&#36215;&#26469;&#12290;&#29305;&#21035;&#26159;&#25105;&#20204;&#23637;&#31034;&#65292;&#21363;&#20351;&#21482;&#26377;5&#65285;&#30340;&#35757;&#32451;&#25991;&#26412;&#34987;&#25968;&#23383;&#27700;&#21360;&#26631;&#35760;&#65292;&#35757;&#32451;&#22312;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;&#21512;&#25104;&#25351;&#20196;&#19978;&#20173;&#28982;&#21487;&#20197;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#65288;p&#20540;&lt;1e-5&#65289;&#34987;&#26816;&#27979;&#21040;&#12290;&#22240;&#27492;&#65292;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;LLM&#27700;&#21360;&#25216;&#26415;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#30830;&#23450;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;LLM&#30340;&#36755;&#20986;&#26159;&#21542;&#34987;&#29992;&#26469;&#23545;&#21478;&#19968;&#20010;LLM&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14904v1 Announce Type: cross  Abstract: This paper investigates the radioactivity of LLM-generated texts, i.e. whether it is possible to detect that such input was used as training data. Conventional methods like membership inference can carry out this detection with some level of accuracy. We show that watermarked training data leaves traces easier to detect and much more reliable than membership inference. We link the contamination level to the watermark robustness, its proportion in the training set, and the fine-tuning process. We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (p-value &lt; 1e-5) even when as little as 5% of training text is watermarked. Thus, LLM watermarking, originally designed for detecting machine-generated text, gives the ability to easily identify if the outputs of a watermarked LLM were used to fine-tune another LLM.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.14744</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22478;&#24066;&#23621;&#27665;&#65306;&#29992;&#20110;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#30340;LLM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#28789;&#27963;&#39640;&#25928;&#30340;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#12290;LLMs&#36890;&#36807;&#39640;&#25928;&#22788;&#29702;&#35821;&#20041;&#25968;&#25454;&#24182;&#22312;&#24314;&#27169;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#20379;&#22810;&#21151;&#33021;&#24615;, &#20811;&#26381;&#20102;&#20197;&#24448;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#36843;&#20999;&#38656;&#27714;, &#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;: &#23558;LLMs&#19982;&#20016;&#23500;&#30340;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;, &#24320;&#21457;&#21487;&#38752;&#30340;&#27963;&#21160;&#29983;&#25104;&#31574;&#30053;, &#20197;&#21450;&#25506;&#32034;LLMs&#22312;&#22478;&#24066;&#31227;&#21160;&#20013;&#30340;&#24212;&#29992;&#12290;&#20854;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20195;&#29702;&#26694;&#26550;, &#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#20010;&#20307;&#27963;&#21160;&#27169;&#24335;&#21644;&#21160;&#26426;, &#21253;&#25324;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#33258;&#27965;&#26041;&#27861;&#21644;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;, &#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14744v1 Announce Type: new  Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This 
&lt;/p&gt;</description></item><item><title>LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14086</link><description>&lt;p&gt;
LexC-Gen: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21452;&#35821;&#35789;&#27719;&#34920;&#20026;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#29983;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14086
&lt;/p&gt;
&lt;p&gt;
LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#21294;&#20047;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#35760;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#36880;&#23383;&#32763;&#35793;&#26469;&#35299;&#20915;&#65292;&#28982;&#32780;&#65292;&#21452;&#35821;&#35789;&#20856;&#36890;&#24120;&#19982;&#20219;&#21153;&#25968;&#25454;&#26377;&#38480;&#30340;&#35789;&#27719;&#37325;&#21472;&#65292;&#23548;&#33268;&#32763;&#35793;&#35206;&#30422;&#21644;&#35789;&#20856;&#21033;&#29992;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LexC-Gen&#30340;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LexC-Gen&#39318;&#20808;&#20351;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#21333;&#35789;&#29983;&#25104;&#19982;&#35789;&#20856;&#20860;&#23481;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21333;&#35789;&#32763;&#35793;&#23558;&#20854;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;17&#31181;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;LexC-Gen&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#24615;&#33021;&#19978;&#19982;&#19987;&#23478;&#32763;&#35793;&#30340;&#40644;&#37329;&#25968;&#25454;&#31454;&#20105;&#21147;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#19978;&#24179;&#22343;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#21333;&#35789;&#32763;&#35793;&#26041;&#27861;&#25552;&#39640;&#20102;5.6&#21644;8.9&#20010;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14086v1 Announce Type: cross  Abstract: Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classificati
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#24895;&#26223;&#65292;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20026;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.12327</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#20132;&#27969;&#21527;&#65306;&#25506;&#32034;&#31454;&#20105;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21457;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#24895;&#26223;&#65292;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20026;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#20195;&#29702;&#20855;&#26377;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21644;&#31038;&#20250;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#30740;&#31350;LLM&#20195;&#29702;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#33258;&#21457;&#24314;&#31435;&#21512;&#20316;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#19981;&#20165;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#31454;&#20105;&#19982;&#21512;&#20316;&#30340;&#33021;&#21147;&#65292;&#20063;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#24895;&#26223;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#37027;&#20123;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#31038;&#20250;&#29616;&#35937;&#30340;&#27934;&#23519;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/wuzengqing001225/SABM_ShallWe &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12327v1 Announce Type: new  Abstract: Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics. However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied. To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings. This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science. Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena. The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OneBit&#30340;1&#20301;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#30697;&#38453;&#37327;&#21270;&#20026;1&#20301;&#65292;&#20026;&#26497;&#20302;&#27604;&#29305;&#23485;&#24230;&#30340;LLMs&#37096;&#32626;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.11295</link><description>&lt;p&gt;
OneBit:&#26397;&#30528;&#26497;&#20302;&#27604;&#29305;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
OneBit: Towards Extremely Low-bit Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OneBit&#30340;1&#20301;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#30697;&#38453;&#37327;&#21270;&#20026;1&#20301;&#65292;&#20026;&#26497;&#20302;&#27604;&#29305;&#23485;&#24230;&#30340;LLMs&#37096;&#32626;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37327;&#21270;&#20351;&#29992;&#20302;&#27604;&#29305;&#23485;&#24230;&#20540;&#26469;&#34920;&#31034;&#27169;&#22411;&#30340;&#26435;&#37325;&#30697;&#38453;&#65292;&#36825;&#26159;&#20943;&#23569;&#37096;&#32626;&#39640;&#24230;&#26399;&#24453;&#30340;LLMs&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#24320;&#38144;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#27861;&#22312;&#27604;&#29305;&#23485;&#24230;&#26497;&#23567;&#26102;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#22240;&#27492;&#19987;&#27880;&#20110;&#21033;&#29992;4&#20301;&#25110;8&#20301;&#20540;&#26469;&#37327;&#21270;&#27169;&#22411;&#12290;&#26412;&#25991;&#22823;&#32966;&#22320;&#23558;LLMs&#30340;&#26435;&#37325;&#30697;&#38453;&#37327;&#21270;&#20026;1&#20301;&#65292;&#20026;LLMs&#30340;&#26497;&#20302;&#27604;&#29305;&#23485;&#24230;&#37096;&#32626;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;OneBit&#30340;1&#20301;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26356;&#22909;&#22320;&#37327;&#21270;LLMs&#30340;&#26032;&#39062;&#30340;1&#20301;&#21442;&#25968;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#26377;&#25928;&#21442;&#25968;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;QAT&#26694;&#26550;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#20805;&#20998;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OneBit&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65288;&#33267;&#23569;&#26159;&#38750;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11295v1 Announce Type: new  Abstract: Model quantification uses low bit-width values to represent the weight matrices of models, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated LLMs. However, existing quantization methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs. For this target, we introduce a 1-bit quantization-aware training (QAT) framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the QAT framework. Sufficient experimental results indicate that OneBit achieves good performance (at least 83% of the non
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#31687;&#29983;&#25104;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.06544</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26657;&#20934;&#38271;&#31687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Calibrating Long-form Generations from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#31687;&#29983;&#25104;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#38752;&#24615;&#65292;&#26657;&#20934;&#26159;&#24517;&#35201;&#30340; - &#27169;&#22411;&#30340;&#35780;&#20272;&#32622;&#20449;&#24230;&#24212;&#35813;&#19982;&#20854;&#21709;&#24212;&#27491;&#30830;&#24615;&#30340;&#23454;&#38469;&#21487;&#33021;&#24615;&#30456;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#21644;&#26657;&#20934;&#25351;&#26631;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#21709;&#24212;&#27491;&#30830;&#24615;&#30340;&#20108;&#20803;&#30495;/&#20551;&#35780;&#20272;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#38271;&#31687;&#29983;&#25104;&#20013;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#31572;&#26696;&#21487;&#33021;&#37096;&#20998;&#27491;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#20854;&#20013;LLMs&#30340;&#21709;&#24212;&#27491;&#30830;&#24615;&#21644;&#20851;&#32852;&#30340;&#32622;&#20449;&#27700;&#24179;&#37117;&#34987;&#35270;&#20026;&#19968;&#31995;&#21015;&#20998;&#25968;&#30340;&#20998;&#24067;&#12290;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#26469;&#31934;&#30830;&#35780;&#20272;LLM&#30340;&#26657;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#19968;&#33268;&#24615;&#21644;&#33258;&#35780;&#20272;&#30340;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21253;&#25324;&#38271;&#31687;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibratio
&lt;/p&gt;</description></item><item><title>L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04902</link><description>&lt;p&gt;
L4Q: &#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#37327;&#21270;&#35757;&#32451;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04902
&lt;/p&gt;
&lt;p&gt;
L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#26041;&#27861;&#27491;&#22312;&#27969;&#34892;&#36215;&#26469;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25152;&#24102;&#26469;&#30340;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#20943;&#23569;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#36890;&#24120;&#39318;&#36873;&#21518;&#35757;&#32451;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#24182;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#37327;&#21270;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#37327;&#21270;&#27169;&#22411;&#30340;&#37197;&#32622;&#12290;&#30001;&#38750;&#32447;&#24615;&#37327;&#21270;&#25110;&#28151;&#21512;&#31934;&#24230;&#26435;&#37325;&#24341;&#36215;&#30340;&#25928;&#26524;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#24182;&#19988;&#37325;&#26032;&#35757;&#32451;&#29305;&#23450;&#37327;&#21270;&#21442;&#25968;&#21487;&#33021;&#20250;&#24433;&#21709;&#26368;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L4Q&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#12290;L4Q&#21033;&#29992;&#20102;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#30340;&#25200;&#21160;&#26412;&#20307;&#20197;&#21450;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#35780;&#20272;&#20102;LLMs&#22312;&#25968;&#23383;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#22312;&#20840;&#38754;&#35780;&#20272;&#20013;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#25200;&#21160;&#38382;&#39064;&#19978;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLMs&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.09395</link><description>&lt;p&gt;
&#34987;&#29702;&#24615;&#30340;&#27969;&#27801;&#25152;&#22256;&#65292;&#36828;&#31163;AGI&#23792;&#20250;&#65306;&#36890;&#36807;&#26412;&#20307;&#24341;&#23548;&#24178;&#39044;&#35780;&#20272;LLMs&#30340;&#25968;&#23398;&#21644;&#32534;&#30721;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09395
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#30340;&#25200;&#21160;&#26412;&#20307;&#20197;&#21450;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#35780;&#20272;&#20102;LLMs&#22312;&#25968;&#23383;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#22312;&#20840;&#38754;&#35780;&#20272;&#20013;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#25200;&#21160;&#38382;&#39064;&#19978;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLMs&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21457;&#23637;&#23637;&#31034;&#20102;&#22312;&#29616;&#26377;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#26524;&#65292;&#20854;&#20013;&#19968;&#20123;&#27169;&#22411;&#29978;&#33267;&#36229;&#36807;&#20102;&#20154;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#21644;&#31283;&#20581;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20851;&#27880;&#20004;&#20010;&#27969;&#34892;&#30340;&#25512;&#29702;&#20219;&#21153;&#65306;&#31639;&#26415;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#65306;&#65288;i&#65289;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#30340;&#36890;&#29992;&#25200;&#21160;&#26412;&#20307;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#21322;&#33258;&#21160;&#26041;&#27861;&#26469;&#24212;&#29992;&#36825;&#20123;&#25200;&#21160;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#20004;&#20010;&#25968;&#25454;&#38598;MORE&#21644;CORE&#65292;&#20998;&#21035;&#29992;&#20110;&#25200;&#21160;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#65292;&#20197;&#25506;&#31350;LLM&#22312;&#25968;&#23383;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#26497;&#38480;&#12290;&#36890;&#36807;&#23545;&#23553;&#38381;&#28304;&#21644;&#24320;&#28304;LLMs&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#26377;&#27169;&#22411;&#23545;&#25200;&#21160;&#38382;&#39064;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLMs&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.09395v2 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe the limits of LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust probl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02469</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#33391;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Good Privacy Protection Learners. (arXiv:2310.02469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#21019;&#24314;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#30340;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#65288;PII&#65289;&#12290;&#22312;&#27809;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#24494;&#35843; LLMs &#20250;&#23384;&#22312;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26032;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#27604;&#22914;&#35821;&#26009;&#24211;&#31574;&#23637;&#12289;&#22522;&#20110;&#24809;&#32602;&#30340;&#38750;&#27010;&#28982;&#24615;&#35757;&#32451;&#25439;&#22833;&#20197;&#21450;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;&#31561;&#31561;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65292;&#26174;&#31034;&#20986;&#24456;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, eff
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#12298;Media of Langue&#12299;&#36825;&#19968;&#20840;&#26032;&#35789;&#20856;&#21644;&#20844;&#20849;&#38613;&#22609;&#65292;&#36890;&#36807;&#25551;&#36848;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24847;&#20041;&#22320;&#22270;&#21644;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#27010;&#24565;&#65306;&#12298;Inter-Langue Map/Dictionary&#12299;&#12289;&#12298;Inter-Langue Space&#12299;&#21644;&#12298;Inter-Langue Network&#12299;&#12290;</title><link>http://arxiv.org/abs/2309.08609</link><description>&lt;p&gt;
&#12298;Media of Langue&#12299;&#30340;&#23186;&#20307;
&lt;/p&gt;
&lt;p&gt;
Media of Langue. (arXiv:2309.08609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#12298;Media of Langue&#12299;&#36825;&#19968;&#20840;&#26032;&#35789;&#20856;&#21644;&#20844;&#20849;&#38613;&#22609;&#65292;&#36890;&#36807;&#25551;&#36848;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24847;&#20041;&#22320;&#22270;&#21644;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#27010;&#24565;&#65306;&#12298;Inter-Langue Map/Dictionary&#12299;&#12289;&#12298;Inter-Langue Space&#12299;&#21644;&#12298;Inter-Langue Network&#12299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23384;&#26723;Goki Muramoto&#31561;&#20154;&#30340;&#12298;Media of Langue&#12299;&#21518;&#38754;&#30340;&#26448;&#26009;&#12290;&#12298;Media of Langue&#12299;&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#23383;&#20856;&#21644;&#20844;&#20849;&#38613;&#22609;&#65292;&#23427;&#20165;&#20174;&#8220;&#36825;&#20010;&#35789;&#34987;&#32763;&#35793;&#25104;&#37027;&#20010;&#35789;&#8221;&#30340;&#24191;&#27867;&#20107;&#20214;&#21644;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#36793;&#30028;&#19978;&#25551;&#36848;&#20986;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24847;&#20041;&#22320;&#22270;&#12290;&#39318;&#20808;&#65292;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#27010;&#24565;&#65306;&#12298;Inter-Langue Map/Dictionary&#12299;&#12289;&#12298;Inter-Langue Space&#12299;&#21644;&#12298;Inter-Langue Network&#12299;&#24182;&#23558;&#20854;&#19982;&#23383;&#20856;&#12289;&#35821;&#20041;&#31354;&#38388;&#21644;&#35821;&#20041;&#32593;&#32476;&#30340;&#19977;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25509;&#19979;&#26469;&#65292;&#25551;&#36848;&#20102;&#35813;&#20316;&#21697;&#20013;&#23454;&#26045;&#30340;&#20855;&#20307;&#31639;&#27861;&#21644;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to archive the materials behind "Media of Langue" by Goki Muramoto et al. Media of Langue is a new dictionary and public sculpture that depicts the map of meaning on the boundary between languages solely from the vast events of "this word was translated into that word" and two forces: repulsion between all words in the same language and attraction between translated words in different languages. First, the three new concepts proposed, Inter-Langue Map/Dictionary, Inter-Langue Space, and then Inter-Langue Network, are introduced, comparing them to the three domains of dictionary, semantic space, and semantic network. Next, the specific algorithms and designs implemented in the work were described.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#35780;&#20272;&#20219;&#21153;&#65292;&#21517;&#20026;ToMChallenges&#65292;&#20197;&#25506;&#32034;&#24515;&#26234;&#29702;&#35770;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#26234;&#29702;&#35770;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#31283;&#23450;&#22320;&#25191;&#34892;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15068</link><description>&lt;p&gt;
ToMChallenges: &#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#35780;&#20272;&#20219;&#21153;&#65292;&#29992;&#20110;&#25506;&#32034;&#24515;&#26234;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind. (arXiv:2305.15068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#35780;&#20272;&#20219;&#21153;&#65292;&#21517;&#20026;ToMChallenges&#65292;&#20197;&#25506;&#32034;&#24515;&#26234;&#29702;&#35770;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#26234;&#29702;&#35770;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#31283;&#23450;&#22320;&#25191;&#34892;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#26159;&#29702;&#35299;&#19981;&#21516;&#20010;&#20307;&#24515;&#26234;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#25191;&#34892;ToM&#20219;&#21153;&#23384;&#22312;&#28608;&#28872;&#30340;&#20105;&#35758;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25552;&#31034;&#26469;&#27979;&#35797;LLMs&#19978;&#30340;ToM&#65292;&#32467;&#26524;&#19981;&#19968;&#33268;&#65306;&#19968;&#20123;&#30740;&#31350;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23637;&#31034;ToM&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#25345;&#30456;&#21453;&#35266;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToMChallenges&#65292;&#19968;&#20010;&#22522;&#20110;Sally-Anne&#21644;Smarties&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#24515;&#26234;&#29702;&#35770;&#24182;&#21253;&#21547;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#35780;&#20998;&#22120;&#26469;&#31616;&#21270;&#31572;&#26696;&#35780;&#20272;&#36807;&#31243;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19977;&#20010;&#27169;&#22411;&#65306;davinci&#12289;turbo&#21644;gpt-4&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#21644;&#38169;&#35823;&#20998;&#26512;&#26174;&#31034;&#65292;LLMs&#22312;&#25552;&#31034;&#21644;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#19981;&#19968;&#33268;&#12290;&#23545;LLMs&#26469;&#35828;&#65292;&#31283;&#23450;&#22320;&#25191;&#34892;ToM&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM), the capacity to comprehend the mental states of distinct individuals, is essential for numerous practical applications. With the development of large language models (LLMs), there is a heated debate about whether they are able to perform ToM tasks. Previous studies have used different tasks and prompts to test the ToM on LLMs and the results are inconsistent: some studies asserted these models are capable of exhibiting ToM, while others suggest the opposite. In this study, We present ToMChallenges, a dataset for comprehensively evaluating the Theory of Mind based on the Sally-Anne and Smarties tests with a diverse set of tasks. In addition, we also propose an auto-grader to streamline the answer evaluation process. We tested three models: davinci, turbo, and gpt-4. Our evaluation results and error analyses show that LLMs have inconsistent behaviors across prompts and tasks. Performing the ToM tasks robustly remains a challenge for the LLMs. In addition, our paper 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.15613</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#26131;&#26631;&#31614;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Easy Label Projection for Cross-lingual Transfer. (arXiv:2211.15613v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35757;&#32451;&#25968;&#25454;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#24050;&#25104;&#20026;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#28041;&#21450;&#36328;&#24230;&#32423;&#21035;&#27880;&#37322;&#65288;&#20363;&#22914;&#20449;&#24687;&#25552;&#21462;&#25110;&#38382;&#39064;&#22238;&#31572;&#65289;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#26631;&#31614;&#25237;&#24433;&#27493;&#39588;&#65292;&#23558;&#24050;&#27880;&#37322;&#30340;&#36328;&#24230;&#26144;&#23556;&#21040;&#32763;&#35793;&#21518;&#30340;&#25991;&#26412;&#20013;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#23545;&#36825;&#31181;&#26041;&#27861;&#19982;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#30340;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#65288;QA&#65292;NER&#21644;&#20107;&#20214;&#25552;&#21462;&#65289;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#22635;&#34917;&#25991;&#29486;&#20013;&#30340;&#37325;&#35201;&#31354;&#30333;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature. Experimental results show that our optimized version of mark-then-translate, which we
&lt;/p&gt;</description></item></channel></rss>