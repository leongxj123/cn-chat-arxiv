<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#24265;&#20215;&#25512;&#29702;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#25104;&#26412;&#30340;&#38480;&#21046;&#19979;&#25214;&#21040;&#20102;&#27604;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#22522;&#26412;&#36716;&#25442;&#22120;&#27169;&#22411;&#26356;&#20248;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#36229;&#32593;&#32476;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22411;&#19987;&#29992;&#39044;&#31639;&#19979;&#65292;&#35757;&#32451;&#37325;&#35201;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#23567;&#22411;&#27169;&#22411;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01093</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#24265;&#20215;&#25512;&#29702;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Specialized Language Models with Cheap Inference from Limited Domain Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#24265;&#20215;&#25512;&#29702;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#25104;&#26412;&#30340;&#38480;&#21046;&#19979;&#25214;&#21040;&#20102;&#27604;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#22522;&#26412;&#36716;&#25442;&#22120;&#27169;&#22411;&#26356;&#20248;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#36229;&#32593;&#32476;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22411;&#19987;&#29992;&#39044;&#31639;&#19979;&#65292;&#35757;&#32451;&#37325;&#35201;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#23567;&#22411;&#27169;&#22411;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#32570;&#20047;&#22823;&#35268;&#27169;&#25512;&#29702;&#39044;&#31639;&#21644;&#22823;&#35268;&#27169;&#39046;&#22495;&#20869;&#35757;&#32451;&#38598;&#30340;&#20219;&#21153;&#20013;&#24212;&#29992;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#23545;&#36825;&#20123;&#38480;&#21046;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#24182;&#21306;&#20998;&#20102;&#22235;&#20010;&#37325;&#35201;&#30340;&#21464;&#37327;&#65306;&#39044;&#35757;&#32451;&#39044;&#31639;&#65288;&#29992;&#20110;&#22312;&#30446;&#26631;&#39046;&#22495;&#20986;&#29616;&#20043;&#21069;&#36827;&#34892;&#35757;&#32451;&#65289;&#65292;&#19987;&#29992;&#39044;&#31639;&#65288;&#29992;&#20110;&#22312;&#30446;&#26631;&#39046;&#22495;&#20986;&#29616;&#20043;&#21518;&#36827;&#34892;&#35757;&#32451;&#65289;&#65292;&#25512;&#29702;&#39044;&#31639;&#21644;&#39046;&#22495;&#20869;&#35757;&#32451;&#38598;&#22823;&#23567;&#12290;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#21463;&#21040;&#25512;&#29702;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27604;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#22522;&#26412;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#26631;&#20934;&#20570;&#27861;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36229;&#32593;&#32476;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#39044;&#31639;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#22256;&#24785;&#24230;&#65292;&#32780;&#22312;&#22823;&#22411;&#19987;&#29992;&#39044;&#31639;&#19979;&#65292;&#35757;&#32451;&#37325;&#35201;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#23567;&#22411;&#27169;&#22411;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#26032;&#39062;&#30340;&#21465;&#36848;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#30284;&#30151;&#24739;&#32773;&#24739;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11425</link><description>&lt;p&gt;
&#21465;&#20107;&#29305;&#24449;&#36824;&#26159;&#32467;&#26500;&#29305;&#24449;&#65311;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#35782;&#21035;&#24739;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;
&lt;/p&gt;
&lt;p&gt;
Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11425
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#26032;&#39062;&#30340;&#21465;&#36848;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#30284;&#30151;&#24739;&#32773;&#24739;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#27835;&#30103;&#24050;&#30693;&#20250;&#24341;&#20837;&#24515;&#27602;&#24615;&#65292;&#23545;&#39044;&#21518;&#21644;&#29983;&#23384;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#35782;&#21035;&#24739;&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;&#23545;&#20110;&#25913;&#21892;&#30284;&#30151;&#27835;&#30103;&#32467;&#26524;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26469;&#33258;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#21253;&#25324;&#20256;&#32479;ML&#12289;&#26102;&#38388;&#24863;&#30693;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;T-LSTM&#65289;&#21644;&#20351;&#29992;&#20174;&#32467;&#26500;&#21270;&#21307;&#23398;&#20195;&#30721;&#34893;&#29983;&#30340;&#26032;&#39062;&#21465;&#36848;&#29305;&#24449;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35782;&#21035;&#24739;HF&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;&#12290;&#25105;&#20204;&#20174;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#20013;&#24515;&#35782;&#21035;&#20102;&#19968;&#32452;&#21253;&#25324;12,806&#21517;&#32954;&#30284;&#12289;&#20083;&#33146;&#30284;&#21644;&#32467;&#30452;&#32928;&#30284;&#24739;&#32773;&#30340;&#30284;&#30151;&#38431;&#21015;&#65292;&#20854;&#20013;1,602&#20154;&#22312;&#30284;&#30151;&#21518;&#21457;&#23637;&#20026;HF&#12290;LLM GatorTron-3.9B&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;F1&#20998;&#25968;&#65292;&#27604;&#20256;&#32479;&#25903;&#25345;&#21521;&#37327;&#26426;&#39640;&#20986;39%&#65292;&#27604;T-LSTM&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#20986;7%&#65292;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;Transformer&#27169;&#22411;BERT&#39640;&#20986;5.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11425v1 Announce Type: cross  Abstract: Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%. The analysi
&lt;/p&gt;</description></item><item><title>ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;</title><link>https://arxiv.org/abs/2403.05266</link><description>&lt;p&gt;
ERBench&#65306;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05266
&lt;/p&gt;
&lt;p&gt;
ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24187;&#35273;&#22522;&#20934;&#35201;&#20040;&#26159;&#38745;&#24577;&#30340;&#65292;&#35201;&#20040;&#32570;&#20047;&#21487;&#35843;&#25972;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#29616;&#26377;&#30340;&#20851;&#31995;&#25968;&#25454;&#24211;&#26159;&#26500;&#24314;&#22522;&#20934;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ERBench&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#36716;&#25442;&#20026;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#65288;ER&#65289;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#24211;&#27169;&#24335;&#12289;&#35760;&#24405;&#21644;&#21151;&#33021;&#20381;&#36182;&#26469;&#26500;&#24314;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22806;&#38190;&#32422;&#26463;&#26469;&#36830;&#25509;&#20851;&#31995;&#21644;&#26500;&#24314;&#22810;&#36339;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20219;&#24847;&#22797;&#26434;&#65292;&#29992;&#20110;&#35843;&#35797;LLMs&#30340;&#20013;&#38388;&#31572;&#26696;&#12290;&#26368;&#21518;&#65292;ERBench&#25903;&#25345;&#25345;&#32493;&#35780;&#20272;&#65292;&#22810;&#27169;&#24577;qu
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05266v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal qu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24314;&#31569;&#22914;&#20309;&#24433;&#21709;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#21457;&#29616;&#20102;FFN-Wider&#21464;&#21387;&#22120;&#27169;&#22411;&#38477;&#20302;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#36129;&#29486;&#27604;&#65292;&#20174;&#32780;&#23548;&#33268;&#22522;&#26412;&#33021;&#21147;&#30340;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.02436</link><description>&lt;p&gt;
&#24314;&#31569;&#22914;&#20309;&#24433;&#21709;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65311;&#22522;&#20110;FFN-Wider&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24314;&#31569;&#22914;&#20309;&#24433;&#21709;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#21457;&#29616;&#20102;FFN-Wider&#21464;&#21387;&#22120;&#27169;&#22411;&#38477;&#20302;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#36129;&#29486;&#27604;&#65292;&#20174;&#32780;&#23548;&#33268;&#22522;&#26412;&#33021;&#21147;&#30340;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#20998;&#24067;&#24335;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#22312;&#36229;&#20986;&#20998;&#24067;&#24335;&#35821;&#35328;&#24314;&#27169;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#20063;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#35268;&#27169;&#23545;&#22522;&#26412;&#33021;&#21147;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#26550;&#26500;&#23545;&#20854;&#24433;&#21709;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20851;&#24515;&#30340;&#26159;&#65306;&#24314;&#31569;&#22914;&#20309;&#24433;&#21709;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#35299;&#37322;&#24182;&#36870;&#36716;FFN-Wider&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;&#23548;&#33268;&#22522;&#26412;&#33021;&#21147;&#19979;&#38477;&#30340;&#24773;&#20917;&#65292;&#21147;&#27714;&#25552;&#20379;&#19968;&#20123;&#35265;&#35299;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#22836;&#27880;&#24847;&#21147;&#65288;&#19968;&#31181;&#32452;&#21512;&#20989;&#25968;&#65289;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#24314;&#27169;&#30340;&#36129;&#29486;&#27604;&#26159;&#24433;&#21709;&#22522;&#26412;&#33021;&#21147;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;FFN-Wider&#21464;&#21387;&#22120;&#20943;&#23569;&#20102;&#36825;&#31181;&#32452;&#21512;&#20989;&#25968;&#30340;&#36129;&#29486;&#27604;&#65292;&#23548;&#33268;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02436v1 Announce Type: new  Abstract: Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot learning. Unlike existing work focusing on the influence of scale on base capabilities, our work examines the influence of architecture on those. Specifically, our concern is: How does architecture influence the base capabilities of pre-trained language models? In this work, we attempt to explain and reverse the decline in base capabilities caused by the architecture of FFN-Wider Transformers, seeking to provide some insights. Through analysis, we found the contribution ratio of Multi-Head Attention (a combination function) to pre-trained language modeling is a key factor affecting base capabilities. FFN-Wider Transformers reduce the contribution ratio of this combination function, leading to a d
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#35757;&#32451;&#19979;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#20102;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20854;&#19979;&#30028;&#65307;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#28982;&#25104;&#31435;&#12290;</title><link>https://arxiv.org/abs/2402.18551</link><description>&lt;p&gt;
&#38544;&#24615;&#20559;&#35265;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Next-Token Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18551
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#35757;&#32451;&#19979;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#20102;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20854;&#19979;&#30028;&#65307;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65288;NTP&#65289;&#26159;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39318;&#36873;&#33539;&#24335;&#65292;&#23427;&#28041;&#21450;&#39044;&#27979;&#24207;&#21015;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#19982;&#20256;&#32479;&#30340;&#29420;&#28909;&#20998;&#31867;&#19981;&#21516;&#65292;&#22312;NTP&#20013;&#65292;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#39057;&#29575;&#30340;&#26631;&#35760;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#21518;&#32487;&#12290;&#26412;&#25991;&#23558;NTP&#35757;&#32451;&#26694;&#26550;&#21270;&#20026;&#36328;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#29109;&#26368;&#23567;&#21270;&#65292;&#27599;&#20010;&#19978;&#19979;&#25991;&#37117;&#19982;&#26377;&#38480;&#35789;&#27719;&#34920;&#20013;&#30340;&#31232;&#30095;&#32463;&#39564;&#27010;&#29575;&#21521;&#37327;&#30456;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#23427;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#24403;NTP&#35757;&#32451;&#25439;&#22833;&#36798;&#21040;&#20854;&#19979;&#30028;&#65288;&#29109;&#65289;&#26102;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#26159;&#21542;&#20250;&#23545;&#20855;&#26377;&#29305;&#23450;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#20559;&#35265;&#65311;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#65292;&#25105;&#20204;&#20570;&#20986;&#20197;&#19979;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25968;&#25454;&#19978;&#30340;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;GD&#33021;&#22815;&#36798;&#21040;&#20854;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18551v1 Announce Type: cross  Abstract: Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11821</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#24494;&#32467;&#26500;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Microstructures and Accuracy of Graph Recall by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#25968;&#25454;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20197;&#25991;&#26412;&#26684;&#24335;&#25551;&#36848;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#21484;&#22238;&#21644;&#32534;&#30721;&#20808;&#21069;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#22270;&#24418;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38656;&#35201;&#23637;&#31034;&#30340;&#22522;&#26412;&#20294;&#20851;&#38190;&#33021;&#21147;&#65292;&#20197;&#25191;&#34892;&#28041;&#21450;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20154;&#31867;&#22312;&#22270;&#24418;&#21484;&#22238;&#26041;&#38754;&#30340;&#34920;&#29616;&#24050;&#34987;&#35748;&#30693;&#31185;&#23398;&#23478;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#65292;&#21457;&#29616;&#20854;&#32463;&#24120;&#21576;&#29616;&#19982;&#20154;&#31867;&#22788;&#29702;&#31038;&#20250;&#20851;&#31995;&#19968;&#33268;&#30340;&#26576;&#20123;&#32467;&#26500;&#24615;&#20559;&#35265;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#25105;&#20204;&#24456;&#23569;&#20102;&#35299;LLMs&#22312;&#31867;&#20284;&#22270;&#24418;&#21484;&#22238;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65306;&#23427;&#20204;&#21484;&#22238;&#30340;&#22270;&#24418;&#26159;&#21542;&#20063;&#21576;&#29616;&#26576;&#20123;&#20559;&#35265;&#27169;&#24335;&#65292;&#22914;&#26524;&#26159;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#26377;&#20309;&#19981;&#21516;&#24182;&#22914;&#20309;&#24433;&#21709;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;LLMs&#36827;&#34892;&#22270;&#24418;&#21484;&#22238;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#30740;&#31350;&#20854;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65288;&#23616;&#37096;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.06255</link><description>&lt;p&gt;
&#36827;&#21462;&#30340;&#40077;&#21187;&#36890;&#36807;&#25552;&#31034;&#23545;&#25239;&#35843;&#25972;&#25269;&#21046;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#32469;&#36807;&#20869;&#32622;&#30340;&#23433;&#20840;&#25514;&#26045;&#24182;&#25552;&#20379;&#21361;&#38505;&#25110;&#38750;&#27861;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36234;&#29425;&#34892;&#20026;&#12290;&#20026;&#20102;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38598;&#20013;&#22312;&#20869;&#23481;&#36807;&#28388;&#25110;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning&#65288;PAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#25105;&#20204;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20132;&#26367;&#26356;&#26032;&#25915;&#20987;&#21644;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25552;&#31034;&#35843;&#25972;&#30340;&#35282;&#24230;&#23454;&#26045;&#38450;&#24481;&#30340;&#20154;&#12290;&#19968;&#26086;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;LLMs&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25269;&#24481;&#36234;&#29425;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#22312;&#20449;&#20219;&#28216;&#25103;&#20013;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#20559;&#35265;&#21644;&#23545;&#20195;&#29702;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.04559</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Model Agents Simulate Human Trust Behaviors?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04559
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#22312;&#20449;&#20219;&#28216;&#25103;&#20013;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#20559;&#35265;&#21644;&#23545;&#20195;&#29702;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#32463;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#20316;&#20026;&#27169;&#25311;&#24037;&#20855;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#22312;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;LLM&#20195;&#29702;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20154;&#31867;&#20114;&#21160;&#20013;&#26368;&#20851;&#38190;&#30340;&#34892;&#20026;&#20043;&#19968;&#65292;&#20449;&#20219;&#65292;&#26088;&#22312;&#35843;&#26597;LLM&#20195;&#29702;&#26159;&#21542;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#65292;&#22312;&#34987;&#34892;&#20026;&#32463;&#27982;&#23398;&#24191;&#27867;&#25509;&#21463;&#30340;&#20449;&#20219;&#28216;&#25103;&#26694;&#26550;&#19979;&#65292;LLM&#20195;&#29702;&#36890;&#24120;&#34920;&#29616;&#20986;&#20449;&#20219;&#34892;&#20026;&#65292;&#31216;&#20026;&#20195;&#29702;&#20449;&#20219;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#20195;&#29702;&#22312;&#20449;&#20219;&#34892;&#20026;&#26041;&#38754;&#19982;&#20154;&#31867;&#20855;&#26377;&#36739;&#39640;&#30340;&#34892;&#20026;&#19968;&#33268;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;LLM&#20195;&#29702;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20195;&#29702;&#20449;&#20219;&#20013;&#30340;&#20559;&#35265;&#20197;&#21450;&#20195;&#29702;&#20449;&#20219;&#22312;&#23545;&#20195;&#29702;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#26041;&#38754;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21253;&#25324;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#22312;&#20869;&#30340;&#26465;&#20214;&#19979;&#20195;&#29702;&#20449;&#20219;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#30495;&#23454;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#25351;&#20986;&#20102;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#30340;&#38556;&#30861;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#20851;&#27880;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.02420</link><description>&lt;p&gt;
2024&#24180;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Factuality of Large Language Models in the Year 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#30495;&#23454;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#25351;&#20986;&#20102;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#30340;&#38556;&#30861;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#20851;&#27880;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23588;&#20854;&#26159;&#22312;&#32842;&#22825;&#26041;&#38754;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#21518;&#65292;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#22320;&#26041;&#30452;&#25509;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#65292;&#20351;&#20154;&#20204;&#20174;&#25628;&#32034;&#12289;&#25552;&#21462;&#21644;&#25972;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#30340;&#36807;&#31243;&#20013;&#24471;&#21040;&#35299;&#33073;&#12290;&#28982;&#32780;&#65292;&#24456;&#22810;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#22238;&#31572;&#26159;&#38169;&#35823;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;LLM&#30495;&#23454;&#24615;&#30340;&#30740;&#31350;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#26088;&#22312;&#25214;&#20986;&#20027;&#35201;&#25361;&#25112;&#21450;&#20854;&#21407;&#22240;&#65292;&#24182;&#25351;&#20986;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#20998;&#26512;&#24320;&#25918;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#38754;&#20020;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#36824;&#23637;&#26395;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.
&lt;/p&gt;</description></item><item><title>AutoTimes&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#25442;&#33021;&#21147;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#19982;&#20808;&#21069;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02370</link><description>&lt;p&gt;
AutoTimes: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoTimes: Autoregressive Time Series Forecasters via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02370
&lt;/p&gt;
&lt;p&gt;
AutoTimes&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#25442;&#33021;&#21147;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#19982;&#20808;&#21069;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#23578;&#26410;&#23436;&#20840;&#21457;&#23637;&#12290;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#30456;&#20284;&#39034;&#24207;&#32467;&#26500;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#24573;&#35270;&#20102;&#26102;&#38388;&#24207;&#21015;&#21644;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#30340;&#19968;&#33268;&#24615;&#65292;&#23548;&#33268;&#23545;LLM&#28508;&#21147;&#30340;&#21033;&#29992;&#19981;&#36275;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#20174;&#35821;&#35328;&#24314;&#27169;&#20013;&#23398;&#21040;&#30340;&#36890;&#29992;&#20196;&#29260;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoTimes&#65292;&#23558;LLM&#37325;&#26032;&#29992;&#20316;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#36825;&#19982;LLM&#30340;&#33719;&#21462;&#21644;&#21033;&#29992;&#19968;&#33268;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#21442;&#25968;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#39044;&#27979;&#22120;&#21487;&#20197;&#22788;&#29702;&#28789;&#27963;&#30340;&#31995;&#21015;&#38271;&#24230;&#65292;&#24182;&#23454;&#29616;&#19982;&#27969;&#34892;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#24212;&#30340;&#26102;&#38388;&#25139;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models of time series have not been fully developed due to the limited availability of large-scale time series and the underexploration of scalable pre-training. Based on the similar sequential structure of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, prior methods may overlook the consistency in aligning time series and natural language, resulting in insufficient utilization of the LLM potentials. To fully exploit the general-purpose token transitions learned from language modeling, we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which is consistent with the acquisition and utilization of LLMs without updating the parameters. The consequent forecasters can handle flexible series lengths and achieve competitive performance as prevalent models. Further, we present token-wise prompting that utilizes corresponding timestamps to make ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#20219;&#21153;&#24341;&#20837;&#20102;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02130</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#28210;&#26579;&#22270;&#24418;
&lt;/p&gt;
&lt;p&gt;
Rendering Graphs for Graph Reasoning in Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#20219;&#21153;&#24341;&#20837;&#20102;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26426;&#22120;&#20154;&#35268;&#21010;&#12289;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22270;&#32467;&#26500;&#65292;LLMs&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#26684;&#24335;&#30340;&#22270;&#20449;&#24687;&#65292;&#20294;&#24573;&#35270;&#20102;&#20016;&#23500;&#30340;&#35270;&#35273;&#27169;&#24577;&#65292;&#32780;&#35270;&#35273;&#26159;&#20154;&#31867;&#29702;&#35299;&#32467;&#26500;&#20449;&#24687;&#21644;&#36827;&#34892;&#22270;&#25512;&#29702;&#30340;&#30452;&#35266;&#26041;&#24335;&#12290;&#23558;&#22270;&#32467;&#26500;&#34920;&#31034;&#20026;&#35270;&#35273;&#22270;&#20687;(&#21363;&#35270;&#35273;&#22270;)&#30340;&#28508;&#22312;&#30410;&#22788;&#21644;&#33021;&#21147;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#39318;&#27425;&#24341;&#20837;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#26159;&#19968;&#20010;&#20803;&#32452;(&#22270;&#12289;&#22270;&#20687;&#12289;&#25991;&#26412;&#25551;&#36848;)&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;LLMs&#22312;GITQA&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#22312;LLaVA-7B/13B&#27169;&#22411;&#30340;&#24494;&#35843;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2312.04828</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;
&lt;/p&gt;
&lt;p&gt;
Human-Readable Fingerprint for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#21644;&#37197;&#22871;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#35768;&#21487;&#35777;&#65292;&#20445;&#25252;LLM&#30340;&#29256;&#26435;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#30340;&#21442;&#25968;&#20462;&#25913;&#65292;&#30830;&#23450;LLM&#30340;&#21407;&#22987;&#22522;&#26412;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;LLM&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#22320;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#65292;&#32780;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#27169;&#22411;&#25910;&#25947;&#21518;&#65292;LLM&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#20445;&#25345;&#31283;&#23450;&#65292;&#36890;&#36807;&#21518;&#32493;&#30340;&#35757;&#32451;&#27493;&#39588;&#65292;&#21253;&#25324;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;RLHF&#65292;&#20960;&#20046;&#27809;&#26377;&#25200;&#21160;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#36275;&#22815;&#26465;&#20214;&#12290;&#36890;&#36807;&#32487;&#32493;&#35757;&#32451;LLM&#24182;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#39033;&#26469;&#25512;&#24320;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#21521;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#24517;&#35201;&#24615;&#65292;&#32467;&#26524;&#20351;&#24471;&#27169;&#22411;&#21463;&#25439;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26041;&#21521;&#23481;&#26131;&#21463;&#21040;&#31616;&#21333;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22914;&#32500;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2312.02119</link><description>&lt;p&gt;
&#25915;&#20987;&#26641;&#65306;&#33258;&#21160;&#30772;&#35299;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02119
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#20173;&#22312;&#29983;&#25104;&#26377;&#23475;&#12289;&#24102;&#20559;&#35265;&#21644;&#26377;&#27602;&#20869;&#23481;&#65292;&#36825;&#19968;&#28857;&#30001;&#20154;&#20026;&#35774;&#35745;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#26222;&#36941;&#23384;&#22312;&#24471;&#20197;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#65292;&#20165;&#38656;&#35201;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;TAP&#21033;&#29992;LLM&#26469;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#36845;&#20195;&#22320;&#20248;&#21270;&#20505;&#36873;&#65288;&#25915;&#20987;&#65289;&#25552;&#31034;&#65292;&#30452;&#21040;&#29983;&#25104;&#30340;&#25552;&#31034;&#20043;&#19968;&#36234;&#29425;&#30446;&#26631;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#22312;&#23558;&#25552;&#31034;&#21457;&#36865;&#32473;&#30446;&#26631;&#20043;&#21069;&#65292;TAP&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#24182;&#31227;&#38500;&#21487;&#33021;&#19981;&#20250;&#23548;&#33268;&#36234;&#29425;&#30340;&#25552;&#31034;&#12290;&#20351;&#29992;&#24605;&#32500;&#26641;&#25512;&#29702;&#20351;TAP&#33021;&#22815;&#22312;&#22823;&#37327;&#25552;&#31034;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#32780;&#20462;&#21098;&#21017;&#20943;&#23569;&#20102;&#21457;&#36865;&#32473;&#30446;&#26631;&#30340;&#24635;&#26597;&#35810;&#25968;&#37327;&#12290;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;TAP&#29983;&#25104;&#30340;&#25552;&#31034;&#36234;&#29425;&#20102;&#36229;&#36807;80%&#30340;&#26368;&#20808;&#36827;LLMs&#65288;&#21253;&#25324;GPT4&#21644;GPT4-Turbo&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02119v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thought reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80%
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35780;&#20272;&#25351;&#26631;UAcc&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#39640;&#20934;&#30830;&#24230;&#30340;LLMs&#21487;&#33021;&#20855;&#26377;&#36739;&#20302;&#30340;&#30830;&#23450;&#24615;&#65292;&#32780;&#22823;&#35268;&#27169;LLMs&#21487;&#33021;&#27604;&#36739;&#23567;&#35268;&#27169;LLMs&#26356;&#19981;&#30830;&#23450;&#12290;&#25351;&#20196;&#24494;&#35843;&#20542;&#21521;&#20110;&#22686;&#21152;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12794</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking LLMs via Uncertainty Quantification. (arXiv:2401.12794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35780;&#20272;&#25351;&#26631;UAcc&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#39640;&#20934;&#30830;&#24230;&#30340;LLMs&#21487;&#33021;&#20855;&#26377;&#36739;&#20302;&#30340;&#30830;&#23450;&#24615;&#65292;&#32780;&#22823;&#35268;&#27169;LLMs&#21487;&#33021;&#27604;&#36739;&#23567;&#35268;&#27169;LLMs&#26356;&#19981;&#30830;&#23450;&#12290;&#25351;&#20196;&#24494;&#35843;&#20542;&#21521;&#20110;&#22686;&#21152;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#20010;&#26426;&#26500;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22686;&#22810;&#65292;&#24432;&#26174;&#20102;&#23545;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#24179;&#21488;&#65292;&#22914;&#24191;&#20026;&#20154;&#30693;&#30340;HuggingFace&#24320;&#25918;&#30340;LLM&#25490;&#34892;&#27036;&#65292;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;--&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#20840;&#38754;&#35780;&#20272;LLMs&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;LLMs&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38598;&#25104;&#36827;&#21435;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#20116;&#20010;&#20856;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20843;&#20010;LLMs&#65288;LLM&#31995;&#21015;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32771;&#34385;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35780;&#20272;&#25351;&#26631;UAcc&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;I&#65289;&#20934;&#30830;&#24230;&#36234;&#39640;&#30340;LLMs&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#30830;&#23450;&#24615;&#65307;II&#65289;&#35268;&#27169;&#36739;&#22823;&#30340;LLMs&#21487;&#33021;&#19982;&#36739;&#23567;&#30340;LLMs&#30456;&#27604;&#26174;&#31034;&#20986;&#26356;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#65307;III&#65289;&#25351;&#20196;&#24494;&#35843;&#20542;&#21521;&#20110;&#22686;&#21152;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves eight LLMs (LLM series) spanning five representative natural language processing tasks. Additionally, we introduce an uncertainty-aware evaluation metric, UAcc, which takes into account both prediction accuracy and prediction uncertainty. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. By taking uncertainty
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10862</link><description>&lt;p&gt;
&#22522;&#20110;&#21098;&#26525;&#30340;&#20445;&#25252;: &#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#23545;&#40784;&#30340;LLMs&#30340;&#36234;&#29425;&#25269;&#25239;&#21147;
&lt;/p&gt;
&lt;p&gt;
Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#35825;&#20351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#21644;&#36829;&#27861;&#20869;&#23481;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21098;&#26525;LLM&#21442;&#25968;&#22810;&#36798;20&#65285;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#23427;&#20204;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#24182;&#19988;&#19981;&#25439;&#23475;&#20854;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21098;&#26525;&#21518;&#35266;&#23519;&#21040;&#30340;&#22686;&#24378;&#23433;&#20840;&#24615;&#19982;&#27169;&#22411;&#30340;&#21021;&#22987;&#23433;&#20840;&#35757;&#32451;&#27700;&#24179;&#30456;&#20851;&#65292;&#36825;&#26263;&#31034;&#21098;&#26525;&#30340;&#25928;&#26524;&#21487;&#33021;&#26356;&#26222;&#36941;&#65292;&#20063;&#21487;&#33021;&#36866;&#29992;&#20110;&#36229;&#20986;&#23433;&#20840;&#24615;&#33539;&#30068;&#30340;&#20854;&#20182;LLM&#34892;&#20026;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#12289;&#25554;&#20837;&#21040;&#21313;&#20010;&#19981;&#21516;&#36234;&#29425;&#25552;&#31034;&#20013;&#30340;225&#20010;&#26377;&#23475;&#20219;&#21153;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;LLMs&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#36234;&#29425;&#25552;&#31034;&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#65288;&#22914;LLaMA-2 Chat&#65292;Vicuna&#21644;Mistral Instruct&#65289;&#20855;&#26377;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#32500;&#22522;&#30334;&#31185;&#32534;&#36753;&#35752;&#35770;&#21644;&#20915;&#31574;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#22312;&#35299;&#37322;&#20869;&#23481;&#31649;&#29702;&#20915;&#31574;&#26041;&#38754;&#22686;&#21152;&#20102;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.05779</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#24212;&#35813;&#21024;&#38500;&#36825;&#31687;&#25991;&#31456;&#65311;&#22810;&#35821;&#35328;&#32500;&#22522;&#30334;&#31185;&#32534;&#36753;&#35752;&#35770;&#20013;&#30340;&#36879;&#26126;&#31435;&#22330;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions. (arXiv:2310.05779v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#32500;&#22522;&#30334;&#31185;&#32534;&#36753;&#35752;&#35770;&#21644;&#20915;&#31574;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#22312;&#35299;&#37322;&#20869;&#23481;&#31649;&#29702;&#20915;&#31574;&#26041;&#38754;&#22686;&#21152;&#20102;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#31649;&#29702;&#36890;&#24120;&#26159;&#38750;&#36879;&#26126;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#32500;&#22522;&#30334;&#31185;&#19978;&#65292;&#36825;&#31181;&#35752;&#35770;&#26159;&#20844;&#24320;&#36827;&#34892;&#30340;&#65292;&#32534;&#36753;&#34987;&#40723;&#21169;&#20351;&#29992;&#20869;&#23481;&#31649;&#29702;&#25919;&#31574;&#26469;&#35299;&#37322;&#20182;&#20204;&#30340;&#20915;&#31574;&#12290;&#30446;&#21069;&#65292;&#21482;&#26377;&#23569;&#25968;&#35780;&#35770;&#26126;&#30830;&#25552;&#21040;&#36825;&#20123;&#25919;&#31574;-&#33521;&#25991;&#35780;&#35770;20%&#65292;&#20294;&#24503;&#25991;&#21644;&#22303;&#32819;&#20854;&#35780;&#35770;&#21482;&#26377;2%&#12290;&#20026;&#20102;&#24110;&#21161;&#29702;&#35299;&#20869;&#23481;&#31649;&#29702;&#36807;&#31243;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#32500;&#22522;&#30334;&#31185;&#32534;&#36753;&#35752;&#35770;&#20197;&#21450;&#20182;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#32534;&#36753;&#30340;&#31435;&#22330;&#65288;&#20445;&#30041;&#12289;&#21024;&#38500;&#12289;&#21512;&#24182;&#12289;&#35780;&#35770;&#65289;&#65292;&#20197;&#21450;&#27599;&#20010;&#32534;&#36753;&#20915;&#31574;&#25152;&#38472;&#36848;&#30340;&#21407;&#22240;&#21644;&#20869;&#23481;&#31649;&#29702;&#25919;&#31574;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31435;&#22330;&#21644;&#30456;&#24212;&#30340;&#21407;&#22240;&#65288;&#25919;&#31574;&#65289;&#21487;&#20197;&#34987;&#39640;&#24230;&#20934;&#30830;&#22320;&#32852;&#21512;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20915;&#31574;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#32852;&#21512;&#39044;&#27979;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The moderation of content on online platforms is usually non-transparent. On Wikipedia, however, this discussion is carried out publicly and the editors are encouraged to use the content moderation policies as explanations for making moderation decisions. Currently, only a few comments explicitly mention those policies -- 20% of the English ones, but as few as 2% of the German and Turkish comments. To aid in this process of understanding how content is moderated, we construct a novel multilingual dataset of Wikipedia editor discussions along with their reasoning in three languages. The dataset contains the stances of the editors (keep, delete, merge, comment), along with the stated reason, and a content moderation policy, for each edit decision. We demonstrate that stance and corresponding reason (policy) can be predicted jointly with a high degree of accuracy, adding transparency to the decision-making process. We release both our joint prediction models and the multilingual content m
&lt;/p&gt;</description></item></channel></rss>