<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>GINopic&#26159;&#19968;&#31181;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02115</link><description>&lt;p&gt;
GINopic&#65306;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GINopic: Topic Modeling with Graph Isomorphism Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02115
&lt;/p&gt;
&lt;p&gt;
GINopic&#26159;&#19968;&#31181;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#26159;&#20998;&#26512;&#21644;&#25506;&#32034;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#30340;&#24191;&#27867;&#20351;&#29992;&#26041;&#27861;&#12290; &#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#23558;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#23884;&#20837;&#65292;&#32435;&#20837;&#20027;&#39064;&#24314;&#27169;&#20013;&#12290; &#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20102;&#21333;&#35789;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#20256;&#36798;&#30340;&#22266;&#26377;&#20449;&#24687;&#20215;&#20540;&#12290; &#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GINopic&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290; &#36890;&#36807;&#22312;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20869;&#22312;&#30340;&#65288;&#23450;&#37327;&#21644;&#23450;&#24615;&#65289;&#21644;&#22806;&#37096;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#30456;&#27604;&#65292;GINopic&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02115v1 Announce Type: new  Abstract: Topic modeling is a widely used approach for analyzing and exploring large document collections. Recent research efforts have incorporated pre-trained contextualized language models, such as BERT embeddings, into topic modeling. However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words. In this study, we introduce GINopic, a topic modeling framework based on graph isomorphism networks to capture the correlation between words. By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing topic models and highlight its potential for advancing topic modeling.
&lt;/p&gt;</description></item><item><title>TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.00297</link><description>&lt;p&gt;
TRABSA&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;BiLSTM&#21644;Twitter-RoBERTa&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00297
&lt;/p&gt;
&lt;p&gt;
TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#20844;&#20247;&#33286;&#35770;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TRABSA&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#21033;&#29992;&#22312;124M&#26465;&#25512;&#25991;&#19978;&#35757;&#32451;&#30340;RoBERTa&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24046;&#36317;&#65292;&#30830;&#20445;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;32&#20010;&#22269;&#23478;&#21644;&#32654;&#22269;&#21508;&#24030;&#30340;&#25512;&#25991;&#19982;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20845;&#31181;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19977;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26631;&#27880;&#25216;&#26415;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#25216;&#26415;&#20197;&#23454;&#29616;&#26368;&#20339;&#24773;&#24863;&#20998;&#26512;&#25928;&#26524;&#12290;TRABSA&#20197;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#22686;&#30410;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#19968;&#33268;&#30340;&#20248;&#36234;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;SHAP&#21644;LIME&#20998;&#26512;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22686;&#24378;&#20102;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
&lt;/p&gt;</description></item><item><title>COIG-CQIA &#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#24191;&#27867;&#30340;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20132;&#20114;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2403.18058</link><description>&lt;p&gt;
COIG-CQIA&#65306;&#21482;&#38656;&#36136;&#37327;&#8212;&#8212;&#38754;&#21521;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18058
&lt;/p&gt;
&lt;p&gt;
COIG-CQIA &#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#24191;&#27867;&#30340;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20132;&#20114;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#33521;&#35821;&#39046;&#22495;&#12290;&#36825;&#20123;&#36827;&#23637;&#20351;&#24471;&#36825;&#20123;LLMs&#33021;&#22815;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#29702;&#35299;&#24182;&#25191;&#34892;&#22797;&#26434;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#30340;&#21457;&#23637;&#20173;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#20013;&#25991;&#35821;&#35328;&#30340;&#29420;&#29305;&#35821;&#35328;&#29305;&#24449;&#21644;&#25991;&#21270;&#28145;&#24230;&#20026;&#25351;&#20196;&#24494;&#35843;&#20219;&#21153;&#24102;&#26469;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#35201;&#20040;&#28304;&#33258;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#65292;&#35201;&#20040;&#19981;&#36866;&#21512;&#19982;&#29616;&#23454;&#20013;&#25991;&#29992;&#25143;&#30340;&#20132;&#20114;&#27169;&#24335;&#30456;&#31526;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;COIG-CQIA&#65292;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#24191;&#27867;&#30340;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20132;&#20114;&#20445;&#25345;&#19968;&#33268;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#26469;&#28304;&#25910;&#38598;&#20102;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#32534;&#20889;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18058v1 Announce Type: cross  Abstract: Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language. These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency. However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users. To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions. To this end, we collect a high-quality human-written corpus from various so
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20197;&#20943;&#23569;NLP&#27169;&#22411;&#39640;&#32622;&#20449;&#24230;&#35823;&#20998;&#31867;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.17860</link><description>&lt;p&gt;
&#25506;&#31350;LLMs&#20316;&#20026;&#30446;&#26631;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#26469;&#28304;&#65292;&#20197;&#20943;&#23569;&#39640;&#32622;&#20449;&#24230;&#35823;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17860
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20197;&#20943;&#23569;NLP&#27169;&#22411;&#39640;&#32622;&#20449;&#24230;&#35823;&#20998;&#31867;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#32463;&#36807;&#20248;&#21270;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#26102;&#65292;&#24120;&#24120;&#23384;&#22312;&#39640;&#32622;&#20449;&#24230;&#38169;&#35823;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20316;&#20026;&#35299;&#20915;NLP&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#20135;&#29983;&#39640;&#32622;&#20449;&#24230;&#38169;&#35823;&#39044;&#27979;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#30001;LLMs&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19982;&#36890;&#36807;&#30456;&#21516;&#36807;&#31243;&#33719;&#24471;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#20943;&#36731;&#38169;&#35823;&#65292;&#20154;&#31867;&#25110;LLMs&#25552;&#20379;&#39640;&#32622;&#20449;&#24230;&#35823;&#20998;&#31867;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20197;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#28982;&#21518;&#29992;&#20110;&#25193;&#23637;&#35757;&#32451;&#38598;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20943;&#23569;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17860v1 Announce Type: new  Abstract: Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21333;&#35843;&#37322;&#20041;&#65288;MonoPara&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37322;&#20041;LM&#21644;&#30446;&#26631;LM&#38598;&#25104;&#35299;&#30721;&#36807;&#31243;&#65292;&#23558;&#25552;&#31034;&#25110;&#25351;&#20196;&#37322;&#20041;&#20026;&#20302;&#22256;&#24785;&#24230;&#30340;&#29256;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.16038</link><description>&lt;p&gt;
&#21333;&#35843;&#37322;&#20041;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Monotonic Paraphrasing Improves Generalization of Language Model Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16038
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21333;&#35843;&#37322;&#20041;&#65288;MonoPara&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37322;&#20041;LM&#21644;&#30446;&#26631;LM&#38598;&#25104;&#35299;&#30721;&#36807;&#31243;&#65292;&#23558;&#25552;&#31034;&#25110;&#25351;&#20196;&#37322;&#20041;&#20026;&#20302;&#22256;&#24785;&#24230;&#30340;&#29256;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#38543;&#30528;&#21516;&#19968;&#20219;&#21153;&#30340;&#19981;&#21516;&#25552;&#31034;&#25110;&#25351;&#20196;&#32780;&#21464;&#21270;&#12290;&#36825;&#31181;&#29616;&#35937;&#30340;&#19968;&#20010;&#20844;&#35748;&#22240;&#32032;&#26159;&#27169;&#22411;&#23545;&#32473;&#23450;&#25552;&#31034;&#25110;&#25351;&#20196;&#30340;&#29087;&#24713;&#31243;&#24230;&#65292;&#36890;&#24120;&#36890;&#36807;&#20854;&#22256;&#24785;&#24230;&#26469;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#21487;&#33021;&#25552;&#31034;&#30701;&#35821;&#30340;&#24040;&#22823;&#31354;&#38388;&#65292;&#25214;&#21040;&#22256;&#24785;&#24230;&#26368;&#20302;&#30340;&#25552;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#35843;&#37322;&#20041;&#65288;MonoPara&#65289;&#65292;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#30721;&#31574;&#30053;&#65292;&#26681;&#25454;&#37322;&#20041;LM&#21644;&#30446;&#26631;LM&#65288;&#21363;&#25552;&#31034;&#25110;&#25351;&#20196;&#25191;&#34892;&#22120;&#65289;&#30340;&#38598;&#21512;&#26469;&#23558;&#32473;&#23450;&#25552;&#31034;&#25110;&#25351;&#20196;&#37322;&#20041;&#21270;&#20026;&#20854;&#20302;&#22256;&#24785;&#24230;&#30340;&#23545;&#24212;&#29289;&#12290;&#38598;&#21512;&#35299;&#30721;&#36807;&#31243;&#21487;&#20197;&#26377;&#25928;&#22320;&#37322;&#20041;&#21407;&#22987;&#25552;&#31034;&#32780;&#19981;&#25913;&#21464;&#20854;&#35821;&#20041;&#21547;&#20041;&#65292;&#21516;&#26102;&#21333;&#35843;&#22320;&#38477;&#20302;&#27599;&#20010;&#29983;&#25104;&#29289;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16038v1 Announce Type: new  Abstract: Performance of large language models (LLMs) may vary with different prompts or instructions of even the same task. One commonly recognized factor for this phenomenon is the model's familiarity with the given prompt or instruction, which is typically estimated by its perplexity. However, finding the prompt with the lowest perplexity is challenging, given the enormous space of possible prompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara), an end-to-end decoding strategy that paraphrases given prompts or instructions into their lower perplexity counterparts based on an ensemble of a paraphrase LM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or instruction executor) that constrains the generation for lower perplexity. The ensemble decoding process can efficiently paraphrase the original prompt without altering its semantic meaning, while monotonically decreasing the perplexity of each gene
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.14734</link><description>&lt;p&gt;
&#19968;&#39033;&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#30340;&#35843;&#26597;&#65306;&#33539;&#24335;&#12289;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14734
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#31070;&#32463;&#20195;&#30721;&#26234;&#33021;--&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;--&#22312;&#25972;&#20010;&#31038;&#20250;&#19978;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#36807;&#21435;&#20960;&#24180;&#24341;&#36215;&#20102;&#20004;&#20010;&#30740;&#31350;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#22238;&#39038;&#20102;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21450;&#20854;&#21464;&#20307;&#12289;20&#22810;&#31181;&#20219;&#21153;&#31867;&#21035;&#20197;&#21450;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#12290;&#25105;&#20204;&#36981;&#24490;&#21382;&#21490;&#36827;&#23637;&#65292;&#36319;&#36394;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#36716;&#21464;&#65288;&#20363;&#22914;&#65292;&#20174;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20195;&#30721;&#24314;&#27169;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#38454;&#27573;&#28085;&#30422;&#30340;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35780;&#20272;&#30340;&#20027;&#35201;&#25216;&#26415;&#36716;&#21464;&#12290;&#23545;&#20110;&#24212;&#29992;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 Announce Type: cross  Abstract: Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#26032;&#39062;&#30340;&#21465;&#36848;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#30284;&#30151;&#24739;&#32773;&#24739;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11425</link><description>&lt;p&gt;
&#21465;&#20107;&#29305;&#24449;&#36824;&#26159;&#32467;&#26500;&#29305;&#24449;&#65311;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#35782;&#21035;&#24739;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;
&lt;/p&gt;
&lt;p&gt;
Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11425
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#26032;&#39062;&#30340;&#21465;&#36848;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#30284;&#30151;&#24739;&#32773;&#24739;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#27835;&#30103;&#24050;&#30693;&#20250;&#24341;&#20837;&#24515;&#27602;&#24615;&#65292;&#23545;&#39044;&#21518;&#21644;&#29983;&#23384;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#35782;&#21035;&#24739;&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;&#23545;&#20110;&#25913;&#21892;&#30284;&#30151;&#27835;&#30103;&#32467;&#26524;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26469;&#33258;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#21253;&#25324;&#20256;&#32479;ML&#12289;&#26102;&#38388;&#24863;&#30693;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;T-LSTM&#65289;&#21644;&#20351;&#29992;&#20174;&#32467;&#26500;&#21270;&#21307;&#23398;&#20195;&#30721;&#34893;&#29983;&#30340;&#26032;&#39062;&#21465;&#36848;&#29305;&#24449;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35782;&#21035;&#24739;HF&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;&#12290;&#25105;&#20204;&#20174;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#20013;&#24515;&#35782;&#21035;&#20102;&#19968;&#32452;&#21253;&#25324;12,806&#21517;&#32954;&#30284;&#12289;&#20083;&#33146;&#30284;&#21644;&#32467;&#30452;&#32928;&#30284;&#24739;&#32773;&#30340;&#30284;&#30151;&#38431;&#21015;&#65292;&#20854;&#20013;1,602&#20154;&#22312;&#30284;&#30151;&#21518;&#21457;&#23637;&#20026;HF&#12290;LLM GatorTron-3.9B&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;F1&#20998;&#25968;&#65292;&#27604;&#20256;&#32479;&#25903;&#25345;&#21521;&#37327;&#26426;&#39640;&#20986;39%&#65292;&#27604;T-LSTM&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#20986;7%&#65292;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;Transformer&#27169;&#22411;BERT&#39640;&#20986;5.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11425v1 Announce Type: cross  Abstract: Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%. The analysi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RetinaQA&#30340;&#26032;&#22411;KBQA&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#22635;&#22270;&#30340;&#36923;&#36753;&#24418;&#24335;&#26500;&#24314;&#21644;&#20351;&#29992;&#36776;&#21035;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#22238;&#31572;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21487;&#22238;&#31572;&#21644;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#19978;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;KBQA&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.10849</link><description>&lt;p&gt;
RETINAQA&#65306;&#19968;&#31181;&#23545;&#21487;&#22238;&#31572;&#21644;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RETINAQA : A Knowledge Base Question Answering Model Robust to both Answerable and Unanswerable Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10849
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RetinaQA&#30340;&#26032;&#22411;KBQA&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#22635;&#22270;&#30340;&#36923;&#36753;&#24418;&#24335;&#26500;&#24314;&#21644;&#20351;&#29992;&#36776;&#21035;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#22238;&#31572;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21487;&#22238;&#31572;&#21644;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#19978;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;KBQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#27169;&#22411;&#36890;&#24120;&#20551;&#23450;&#38382;&#39064;&#26159;&#21487;&#22238;&#31572;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#35757;&#32451;&#21644;&#38408;&#20540;&#35774;&#23450;&#36866;&#24212;&#26816;&#27979;&#19981;&#21487;&#22238;&#31572;&#24615;&#65292;&#20294;&#36825;&#23558;&#20197;&#29306;&#29298;&#21487;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#65292;&#19988;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#25152;&#26377;&#19981;&#21487;&#22238;&#31572;&#24615;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RetinaQA&#30340;&#26032;&#22411;KBQA&#27169;&#22411;&#65292;&#23427;&#23545;&#19981;&#21487;&#22238;&#31572;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#23427;&#23558;&#22522;&#20110;KB&#36941;&#21382;&#30340;&#36923;&#36753;&#24418;&#24335;&#26816;&#32034;&#19982;&#22522;&#20110;&#22635;&#22270;&#30340;&#36923;&#36753;&#24418;&#24335;&#26500;&#24314;&#30456;&#32467;&#21512;&#12290;&#36825;&#26377;&#21161;&#20110;&#22788;&#29702;&#20855;&#26377;&#26377;&#25928;&#36923;&#36753;&#24418;&#24335;&#20294;&#22312;&#30693;&#35782;&#24211;&#20013;&#27809;&#26377;&#36890;&#21521;&#31572;&#26696;&#30340;&#25968;&#25454;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#29992;&#36776;&#21035;&#32780;&#38750;&#29983;&#25104;&#26469;&#26356;&#22909;&#22320;&#35782;&#21035;&#27809;&#26377;&#26377;&#25928;&#36923;&#36753;&#24418;&#24335;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RetinaQA&#22312;&#21487;&#22238;&#31572;&#21644;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#19978;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;KBQA&#27169;&#22411;&#30340;&#35843;&#25972;&#65292;&#24182;&#19988;&#22312;&#19981;&#21487;&#22238;&#31572;&#24615;&#31867;&#21035;&#19978;&#26174;&#31034;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10849v1 Announce Type: new  Abstract: State-of-the-art KBQA models assume answerability of questions. Recent research has shown that while these can be adapted to detect unaswerability with suitable training and thresholding, this comes at the expense of accuracy for answerable questions, and no single model is able to handle all categories of unanswerability. We propose a new model for KBQA named RetinaQA that is robust against unaswerability. It complements KB-traversal based logical form retrieval with sketch-filling based logical form construction. This helps with questions that have valid logical forms but no data paths in the KB leading to an answer. Additionally, it uses discrimination instead of generation to better identify questions that do not have valid logical forms. We demonstrate that RetinaQA significantly outperforms adaptations of state-of-the-art KBQA models across answerable and unanswerable questions, while showing robustness across unanswerability categ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;StreamingDialogue&#65292;&#36890;&#36807;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#65292;&#24182;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;</title><link>https://arxiv.org/abs/2403.08312</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#25439;&#22833;&#36827;&#34892;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;StreamingDialogue&#65306;&#38271;&#23545;&#35805;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08312
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;StreamingDialogue&#65292;&#36890;&#36807;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#65292;&#24182;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#26102;&#36935;&#21040;&#20102;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#23545;&#35805;&#19978;&#19979;&#25991;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#24182;&#19988;&#23545;&#35805;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;\textit{End-of-Utterance} (EoU) &#26377;&#32858;&#21512;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23558;EoU&#26631;&#35760;&#31216;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65288;conv-attn sinks&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;StreamingDialogue&#65292;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;conv-attn&#27785;&#28857;&#65292;&#24182;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20174;&#32780;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#27785;&#28857;&#25968;&#37327;&#65288;&#21363;&#35805;&#35821;&#25968;&#37327;&#65289;&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#12290;&#24403;&#21069;&#30340;LLMs&#24050;&#32463;&#23637;&#31034;&#20102;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#31383;&#21475;&#22823;&#23567;&#36798;&#21040;200k&#29978;&#33267;&#26356;&#22823;&#12290;&#36890;&#36807;&#23558;&#35805;&#35821;&#21387;&#32553;&#20026;EoUs&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08312v1 Announce Type: cross  Abstract: Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200k or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200k of utterances, resulting in a prolonged dialogue learning. In order to minimize informatio
&lt;/p&gt;</description></item><item><title>ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;</title><link>https://arxiv.org/abs/2403.05266</link><description>&lt;p&gt;
ERBench&#65306;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05266
&lt;/p&gt;
&lt;p&gt;
ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24187;&#35273;&#22522;&#20934;&#35201;&#20040;&#26159;&#38745;&#24577;&#30340;&#65292;&#35201;&#20040;&#32570;&#20047;&#21487;&#35843;&#25972;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#29616;&#26377;&#30340;&#20851;&#31995;&#25968;&#25454;&#24211;&#26159;&#26500;&#24314;&#22522;&#20934;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ERBench&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#36716;&#25442;&#20026;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#65288;ER&#65289;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#24211;&#27169;&#24335;&#12289;&#35760;&#24405;&#21644;&#21151;&#33021;&#20381;&#36182;&#26469;&#26500;&#24314;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22806;&#38190;&#32422;&#26463;&#26469;&#36830;&#25509;&#20851;&#31995;&#21644;&#26500;&#24314;&#22810;&#36339;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20219;&#24847;&#22797;&#26434;&#65292;&#29992;&#20110;&#35843;&#35797;LLMs&#30340;&#20013;&#38388;&#31572;&#26696;&#12290;&#26368;&#21518;&#65292;ERBench&#25903;&#25345;&#25345;&#32493;&#35780;&#20272;&#65292;&#22810;&#27169;&#24577;qu
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05266v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal qu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20998;&#25674;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#22312;&#32447;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#22320;&#25552;&#21462;&#12289;&#21387;&#32553;&#24182;&#23384;&#20648;&#20449;&#24687;&#20197;&#20445;&#25345;&#24378;&#22823;&#30340;&#30693;&#35782;&#20445;&#30041;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.04317</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#25674;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22312;&#32447;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Online Adaptation of Language Models with a Memory of Amortized Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20998;&#25674;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#22312;&#32447;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#22320;&#25552;&#21462;&#12289;&#21387;&#32553;&#24182;&#23384;&#20648;&#20449;&#24687;&#20197;&#20445;&#25345;&#24378;&#22823;&#30340;&#30693;&#35782;&#20445;&#30041;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20449;&#24687;&#30340;&#24555;&#36895;&#29983;&#25104;&#21644;&#20256;&#25773;&#65292;&#21363;&#20351;&#24320;&#21457;&#25104;&#26412;&#24040;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20063;&#24456;&#24555;&#36807;&#26102;&#12290;&#37492;&#20110;&#20445;&#25345;&#27169;&#22411;&#26356;&#26032;&#30340;&#37325;&#35201;&#24615;&#65292;&#24403;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;LLMs&#26102;&#65292;&#22312;&#32447;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#19981;&#26029;&#25193;&#22823;&#30340;&#26410;&#35265;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#29616;&#20195;LLMs&#30340;&#22823;&#21442;&#25968;&#31354;&#38388;&#65292;&#39640;&#25928;&#30340;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Memory of Amortized Contexts&#65288;MAC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;LLMs&#30340;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#22312;&#32447;&#36866;&#24212;&#26694;&#26550;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30693;&#35782;&#20445;&#30041;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25674;&#38144;&#29305;&#24449;&#25552;&#21462;&#21644;&#35760;&#24518;&#22686;&#24378;&#26041;&#27861;&#65292;&#23558;&#26032;&#25991;&#26723;&#20013;&#30340;&#20449;&#24687;&#21387;&#32553;&#24182;&#25552;&#21462;&#20026;&#23384;&#20648;&#22312;&#35760;&#24518;&#24211;&#20013;&#30340;&#32039;&#20945;&#35843;&#21046;&#12290;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20851;&#27880;&#24182;&#20174;&#35813;&#35760;&#24518;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#26377;&#20449;&#24687;&#37327;&#30340;&#35843;&#21046;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04317v1 Announce Type: cross  Abstract: Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00801</link><description>&lt;p&gt;
&#33258;&#20027;&#26816;&#32034;&#65306;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Self-Retrieval: Building an Information Retrieval System with One Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00801
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#25913;&#21464;&#20102;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#22312;&#20154;&#31867;&#33719;&#21462;&#20449;&#24687;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#12290;&#30001;&#20110;&#29616;&#26377;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20855;&#26377;&#23396;&#31435;&#30340;&#26550;&#26500;&#21644;&#26377;&#38480;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26080;&#27861;&#23436;&#20840;&#36866;&#24212;&#30452;&#25509;&#21521;&#20154;&#31867;&#25552;&#20379;&#20449;&#24687;&#36716;&#21464;&#20026;&#38388;&#25509;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#12289;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#20449;&#24687;&#26816;&#32034;&#26550;&#26500;&#65292;&#21487;&#20197;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#25152;&#38656;&#30340;&#33021;&#21147;&#21040;&#21333;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#33258;&#20027;&#26816;&#32034;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32034;&#24341;&#26550;&#26500;&#23558;&#35201;&#26816;&#32034;&#30340;&#35821;&#26009;&#20869;&#21270;&#20026;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#21518;&#25972;&#20010;&#26816;&#32034;&#36807;&#31243;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#25991;&#26723;&#29983;&#25104;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31471;&#21040;&#31471;&#25191;&#34892;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;S
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00801v1 Announce Type: cross  Abstract: The rise of large language models (LLMs) has transformed the role of information retrieval (IR) systems in the way to humans accessing information. Due to the isolated architecture and the limited interaction, existing IR systems are unable to fully accommodate the shift from directly providing information to humans to indirectly serving large language models. In this paper, we propose Self-Retrieval, an end-to-end, LLM-driven information retrieval architecture that can fully internalize the required abilities of IR systems into a single LLM and deeply leverage the capabilities of LLMs during IR process. Specifically, Self-retrieval internalizes the corpus to retrieve into a LLM via a natural language indexing architecture. Then the entire retrieval process is redefined as a procedure of document generation and self-assessment, which can be end-to-end executed using a single large language model. Experimental results demonstrate that S
&lt;/p&gt;</description></item><item><title>AmbigNLG&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#25351;&#20196;&#27169;&#31946;&#24615;&#25361;&#25112;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20943;&#36731;&#25351;&#20196;&#20013;&#30340;&#27169;&#31946;&#24615;&#65292;&#25913;&#36827;&#20102;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#31361;&#20986;&#20102;&#28165;&#26224;&#21644;&#20855;&#20307;&#25351;&#20196;&#22312;&#25552;&#21319;LLM&#22312;NLG&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17717</link><description>&lt;p&gt;
AmbigNLG: &#35299;&#20915;NLG&#25351;&#20196;&#20013;&#30340;&#20219;&#21153;&#27169;&#31946;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
AmbigNLG: Addressing Task Ambiguity in Instruction for NLG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17717
&lt;/p&gt;
&lt;p&gt;
AmbigNLG&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#25351;&#20196;&#27169;&#31946;&#24615;&#25361;&#25112;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20943;&#36731;&#25351;&#20196;&#20013;&#30340;&#27169;&#31946;&#24615;&#65292;&#25913;&#36827;&#20102;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#31361;&#20986;&#20102;&#28165;&#26224;&#21644;&#20855;&#20307;&#25351;&#20196;&#22312;&#25552;&#21319;LLM&#22312;NLG&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AmbigNLG&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#25351;&#20196;&#27169;&#31946;&#24615;&#25361;&#25112;&#30340;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#29616;&#23454;&#25351;&#20196;&#20013;&#30340;&#27169;&#31946;&#24615;&#30340;&#26174;&#33879;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;AmbigNLG&#35797;&#22270;&#35782;&#21035;&#24182;&#20943;&#36731;&#36825;&#31181;&#27169;&#31946;&#24615;&#65292;&#26088;&#22312;&#31934;&#32454;&#21270;&#25351;&#20196;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#29992;&#25143;&#26399;&#26395;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;2,500&#20010;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;AmbigSNI-NLG&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#31946;&#24615;&#20998;&#31867;&#27861;&#65292;&#29992;&#20110;&#23545;&#25351;&#20196;&#20013;&#30340;&#27169;&#31946;&#24615;&#36827;&#34892;&#20998;&#31867;&#21644;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#31361;&#20986;&#20102;&#28165;&#26224;&#21644;&#20855;&#20307;&#25351;&#20196;&#22312;&#22686;&#24378;LLM&#22312;NLG&#20219;&#21153;&#20013;&#34920;&#29616;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17717v1 Announce Type: new  Abstract: In this study, we introduce AmbigNLG, a new task designed to tackle the challenge of task ambiguity in instructions for Natural Language Generation (NLG) tasks. Despite the impressive capabilities of Large Language Models (LLMs) in understanding and executing a wide range of tasks through natural language interaction, their performance is significantly hindered by the ambiguity present in real-world instructions. To address this, AmbigNLG seeks to identify and mitigate such ambiguities, aiming to refine instructions to match user expectations better. We introduce a dataset, AmbigSNI-NLG, consisting of 2,500 instances, and develop an ambiguity taxonomy for categorizing and annotating instruction ambiguities. Our approach demonstrates substantial improvements in text generation quality, highlighting the critical role of clear and specific instructions in enhancing LLM performance in NLG tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#26631;&#20934;&#25968;&#25454;&#38598;DREsS&#65292;&#29992;&#20110;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#65292;&#22312;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#22351;&#30340;&#20316;&#25991;&#22686;&#24378;&#31574;&#30053;CASE&#21518;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#32447;&#32467;&#26524;&#25552;&#39640;&#20102;45.44&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.16733</link><description>&lt;p&gt;
DREsS: &#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#20889;&#20316;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#26631;&#20934;&#25968;&#25454;&#38598;DREsS&#65292;&#29992;&#20110;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#65292;&#22312;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#22351;&#30340;&#20316;&#25991;&#22686;&#24378;&#31574;&#30053;CASE&#21518;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#32447;&#32467;&#26524;&#25552;&#39640;&#20102;45.44&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#65288;AES&#65289;&#26159;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#20889;&#20316;&#25945;&#32946;&#20013;&#19968;&#31181;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#20026;&#23398;&#29983;&#21644;&#25945;&#24072;&#25552;&#20379;&#23454;&#26102;&#20316;&#25991;&#35780;&#20998;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;AES&#27169;&#22411;&#26159;&#22312;&#19982;EFL&#20889;&#20316;&#25945;&#32946;&#23454;&#38469;&#22330;&#26223;&#19981;&#30456;&#20851;&#30340;&#20316;&#25991;&#21644;&#20998;&#25968;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#32780;&#25552;&#20379;&#21333;&#19968;&#30340;&#25972;&#20307;&#35780;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;DREsS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#30340;&#22823;&#22411;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;DREsS&#21253;&#25324;&#19977;&#20010;&#23376;&#25968;&#25454;&#38598;&#65306;DREsS_New&#65292;DREsS_Std.&#21644;DREsS_CASE&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;DREsS_New&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;EFL&#26412;&#31185;&#29983;&#25776;&#20889;&#24182;&#30001;&#33521;&#35821;&#25945;&#32946;&#19987;&#23478;&#35780;&#20998;&#30340;&#30495;&#23454;&#35838;&#22530;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#29616;&#26377;&#30340;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#20316;&#25991;&#35780;&#20998;&#25968;&#25454;&#38598;&#26631;&#20934;&#21270;&#20026;DREsS_Std&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CASE&#30340;&#22522;&#20110;&#30772;&#22351;&#30340;&#20316;&#25991;&#22686;&#24378;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;20K&#20010;DREsS_CASE&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#22522;&#32447;&#32467;&#26524;&#25552;&#39640;&#20102;45.44&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16733v1 Announce Type: new  Abstract: Automated essay scoring (AES) is a useful tool in English as a Foreign Language (EFL) writing education, offering real-time essay scores for students and instructors. However, previous AES models were trained on essays and scores irrelevant to the practical scenarios of EFL writing education and usually provided a single holistic score due to the lack of appropriate datasets. In this paper, we release DREsS, a large-scale, standard dataset for rubric-based automated essay scoring. DREsS comprises three sub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with 1.7K essays authored by EFL undergraduate students and scored by English education experts. We also standardize existing rubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation strategy for essays, which generates 20K synthetic samples of DREsS_CASE and improves the baseline results by 45.44%. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;ArEEG_Chars&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.15733</link><description>&lt;p&gt;
ArEEG_Chars: &#29992;&#20110;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#35774;&#24819;&#35821;&#38899;&#35782;&#21035;&#30340;&#38463;&#25289;&#20271;&#23383;&#31526;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15733
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;ArEEG_Chars&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#26159;&#36817;&#24180;&#26469;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21487;&#20197;&#24110;&#21161;&#30251;&#30186;&#24739;&#32773;&#25913;&#21892;&#29983;&#27963;&#12290;&#26377;&#20960;&#39033;&#30740;&#31350;&#33258;&#21160;&#23558;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#20998;&#31867;&#20026;&#33521;&#25991;&#23383;&#31526;&#21644;&#21333;&#35789;&#12290;&#38463;&#25289;&#20271;&#35821;&#26159;&#19990;&#30028;&#19978;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#35821;&#35328;&#20043;&#19968;&#12290;&#28982;&#32780;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#38024;&#23545;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;&#65292;&#24182;&#21629;&#21517;&#20026;ArEEG_Chars&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;ArEEG_Chars&#36827;&#34892;&#20102;&#22810;&#39033;&#23454;&#39564;&#12290;&#22312;&#20351;&#29992;LSTM&#26102;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#12290;ArEEG_Chars&#25968;&#25454;&#38598;&#23558;&#23545;&#30740;&#31350;&#20154;&#21592;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15733v1 Announce Type: cross  Abstract: Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
&lt;/p&gt;</description></item><item><title>Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2402.12749</link><description>&lt;p&gt;
Me LLaMA: &#20026;&#21307;&#30103;&#24212;&#29992;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Me LLaMA: Foundation Large Language Models for Medical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12749
&lt;/p&gt;
&lt;p&gt;
Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19981;&#22815;&#29702;&#24819;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22312;&#22823;&#22411;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Me LLaMA&#65292;&#19968;&#20010;&#21307;&#23398;LLM&#31995;&#21015;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;- Me LLaMA 13/70B&#21450;&#20854; chat-enhanced &#29256;&#26412;- Me LLaMA 13/70B-chat&#65292;&#36890;&#36807;&#25345;&#32493;&#23545;LLaMA2&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25968;&#25454;&#24320;&#21457;&#32780;&#25104;&#12290;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#22871;&#20214;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;129B tokens&#30340;&#22823;&#35268;&#27169;&#25345;&#32493;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;214k&#20010;&#26679;&#26412;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#36328;&#36234;14&#20010;&#25968;&#25454;&#38598;&#30340;&#20845;&#39033;&#20219;&#21153;&#30340;&#21307;&#23398;&#35780;&#20272;&#22522;&#20934;(MIBE)&#12290;&#25105;&#20204;&#20351;&#29992;MIBE&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#65292;Me LLaMA&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24320;&#28304;&#21307;&#23398;LLMs&#65292;&#24182;&#19988;&#22312;&#21830;&#19994;&#24040;&#22836;&#22914;ChatGPT&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OneBit&#30340;1&#20301;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#30697;&#38453;&#37327;&#21270;&#20026;1&#20301;&#65292;&#20026;&#26497;&#20302;&#27604;&#29305;&#23485;&#24230;&#30340;LLMs&#37096;&#32626;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.11295</link><description>&lt;p&gt;
OneBit:&#26397;&#30528;&#26497;&#20302;&#27604;&#29305;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
OneBit: Towards Extremely Low-bit Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OneBit&#30340;1&#20301;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#30697;&#38453;&#37327;&#21270;&#20026;1&#20301;&#65292;&#20026;&#26497;&#20302;&#27604;&#29305;&#23485;&#24230;&#30340;LLMs&#37096;&#32626;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37327;&#21270;&#20351;&#29992;&#20302;&#27604;&#29305;&#23485;&#24230;&#20540;&#26469;&#34920;&#31034;&#27169;&#22411;&#30340;&#26435;&#37325;&#30697;&#38453;&#65292;&#36825;&#26159;&#20943;&#23569;&#37096;&#32626;&#39640;&#24230;&#26399;&#24453;&#30340;LLMs&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#24320;&#38144;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#27861;&#22312;&#27604;&#29305;&#23485;&#24230;&#26497;&#23567;&#26102;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#22240;&#27492;&#19987;&#27880;&#20110;&#21033;&#29992;4&#20301;&#25110;8&#20301;&#20540;&#26469;&#37327;&#21270;&#27169;&#22411;&#12290;&#26412;&#25991;&#22823;&#32966;&#22320;&#23558;LLMs&#30340;&#26435;&#37325;&#30697;&#38453;&#37327;&#21270;&#20026;1&#20301;&#65292;&#20026;LLMs&#30340;&#26497;&#20302;&#27604;&#29305;&#23485;&#24230;&#37096;&#32626;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;OneBit&#30340;1&#20301;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26356;&#22909;&#22320;&#37327;&#21270;LLMs&#30340;&#26032;&#39062;&#30340;1&#20301;&#21442;&#25968;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#26377;&#25928;&#21442;&#25968;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;QAT&#26694;&#26550;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#20805;&#20998;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OneBit&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65288;&#33267;&#23569;&#26159;&#38750;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11295v1 Announce Type: new  Abstract: Model quantification uses low bit-width values to represent the weight matrices of models, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated LLMs. However, existing quantization methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs. For this target, we introduce a 1-bit quantization-aware training (QAT) framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the QAT framework. Sufficient experimental results indicate that OneBit achieves good performance (at least 83% of the non
&lt;/p&gt;</description></item><item><title>OpenMathInstruct-1&#26159;&#19968;&#20010;&#21253;&#21547;180&#19975;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21512;&#25104;&#24320;&#28304;LLM&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#26469;&#26500;&#24314;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#24320;&#28304;LLM&#22312;&#25968;&#23398;&#25216;&#33021;&#26041;&#38754;&#19982;&#38381;&#28304;LLM&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.10176</link><description>&lt;p&gt;
OpenMathInstruct-1: &#19968;&#20010;&#25317;&#26377;180&#19975;&#20010;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10176
&lt;/p&gt;
&lt;p&gt;
OpenMathInstruct-1&#26159;&#19968;&#20010;&#21253;&#21547;180&#19975;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21512;&#25104;&#24320;&#28304;LLM&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#26469;&#26500;&#24314;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#24320;&#28304;LLM&#22312;&#25968;&#23398;&#25216;&#33021;&#26041;&#38754;&#19982;&#38381;&#28304;LLM&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#21512;&#25104;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20026;&#20102;&#33719;&#24471;&#29305;&#23450;&#30340;&#25216;&#33021;&#12290;&#30446;&#21069;&#30340;&#22823;&#35268;&#27169;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#22914;MetaMathQA&#21644;MAmmoTH&#65292;&#26159;&#20351;&#29992;&#26469;&#33258;&#21830;&#19994;&#38480;&#21046;&#35768;&#21487;&#30340;&#38381;&#28304;LLM&#30340;&#36755;&#20986;&#26500;&#24314;&#30340;&#12290;&#38480;&#21046;&#22312;&#36825;&#20123;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#20013;&#20351;&#29992;&#24320;&#28304;LLM&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#30446;&#21069;&#26368;&#22909;&#30340;&#38381;&#28304;LLM&#65288;&#22914;GPT-4&#65289;&#21644;&#26368;&#22909;&#30340;&#24320;&#28304;LLM&#20043;&#38388;&#22312;&#25968;&#23398;&#25216;&#33021;&#19978;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;&#22522;&#20110;&#24320;&#28304;LLM&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#26041;&#24335;&#21644;&#19968;&#20123;&#24378;&#21147;&#32553;&#25918;&#65292;&#26500;&#24314;&#20102;OpenMathInstruct-1&#65292;&#19968;&#20010;&#25317;&#26377;180&#19975;&#20010;&#38382;&#39064;-&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20351;&#29992;GSM8K&#21644;MATH&#36825;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#21512;&#25104;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10176v1 Announce Type: cross  Abstract: Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using t
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2401.11944</link><description>&lt;p&gt;
CMMMU&#65306;&#19968;&#20010;&#20013;&#22269;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11944
&lt;/p&gt;
&lt;p&gt;
CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#21319;&#65292;&#35780;&#20272;LMMs&#30340;&#34920;&#29616;&#26085;&#30410;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#22312;&#35780;&#20272;LMMs&#22312;&#20013;&#25991;&#31561;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26356;&#22823;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CMMMU&#65292;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LMMs&#22312;&#38656;&#35201;&#22823;&#23398;&#27700;&#24179;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;CMMMU&#21463;&#21040;&#20102;MMMUs&#30340;&#26631;&#27880;&#21644;&#20998;&#26512;&#27169;&#24335;&#30340;&#21551;&#21457;&#24182;&#20005;&#26684;&#36981;&#24490;&#12290;CMMMU&#21253;&#25324;&#26469;&#33258;&#22823;&#23398;&#32771;&#35797;&#12289;&#27979;&#39564;&#21644;&#25945;&#31185;&#20070;&#30340;1.2&#19975;&#20010;&#25163;&#21160;&#25910;&#38598;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#28085;&#30422;&#20845;&#20010;&#26680;&#24515;&#23398;&#31185;&#65306;&#33402;&#26415;&#19982;&#35774;&#35745;&#12289;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#20581;&#24247;&#19982;&#21307;&#23398;&#12289;&#20154;&#25991;&#31038;&#31185;&#20197;&#21450;&#25216;&#26415;&#19982;&#24037;&#31243;&#65292;&#23601;&#20687;&#20854;&#20249;&#20276;MMMMU&#19968;&#26679;&#12290;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;30&#20010;&#23398;&#31185;&#65292;&#21253;&#25324;39&#20010;&#39640;&#24230;&#24322;&#36136;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11944v2 Announce Type: replace-cross  Abstract: As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp; Social Science, and Tech &amp; Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2312.04828</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;
&lt;/p&gt;
&lt;p&gt;
Human-Readable Fingerprint for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#21644;&#37197;&#22871;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#35768;&#21487;&#35777;&#65292;&#20445;&#25252;LLM&#30340;&#29256;&#26435;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#30340;&#21442;&#25968;&#20462;&#25913;&#65292;&#30830;&#23450;LLM&#30340;&#21407;&#22987;&#22522;&#26412;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;LLM&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#22320;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#65292;&#32780;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#27169;&#22411;&#25910;&#25947;&#21518;&#65292;LLM&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#20445;&#25345;&#31283;&#23450;&#65292;&#36890;&#36807;&#21518;&#32493;&#30340;&#35757;&#32451;&#27493;&#39588;&#65292;&#21253;&#25324;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;RLHF&#65292;&#20960;&#20046;&#27809;&#26377;&#25200;&#21160;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#36275;&#22815;&#26465;&#20214;&#12290;&#36890;&#36807;&#32487;&#32493;&#35757;&#32451;LLM&#24182;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#39033;&#26469;&#25512;&#24320;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#21521;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#24517;&#35201;&#24615;&#65292;&#32467;&#26524;&#20351;&#24471;&#27169;&#22411;&#21463;&#25439;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26041;&#21521;&#23481;&#26131;&#21463;&#21040;&#31616;&#21333;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22914;&#32500;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension 
&lt;/p&gt;</description></item><item><title>ChemDFM&#26159;&#39318;&#20010;&#38754;&#21521;&#21270;&#23398;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#21270;&#23398;&#25991;&#29486;&#21644;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20855;&#22791;&#20102;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14818</link><description>&lt;p&gt;
ChemDFM: &#21270;&#23398;&#39046;&#22495;&#23545;&#35805;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChemDFM: Dialogue Foundation Model for Chemistry. (arXiv:2401.14818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14818
&lt;/p&gt;
&lt;p&gt;
ChemDFM&#26159;&#39318;&#20010;&#38754;&#21521;&#21270;&#23398;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#21270;&#23398;&#25991;&#29486;&#21644;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20855;&#22791;&#20102;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#33324;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23427;&#20204;&#30340;&#20219;&#21153;&#27010;&#25324;&#21644;&#33258;&#30001;&#23545;&#35805;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#24110;&#21161;&#35774;&#35745;&#21270;&#23398;&#26234;&#33021;(CGI)&#65292;&#20197;&#21327;&#21161;&#21270;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#19987;&#19994;&#35821;&#35328;&#21644;&#30693;&#35782;&#65292;&#22914;&#39640;&#24230;&#20449;&#24687;&#21270;&#30340;SMILES&#31526;&#21495;&#34920;&#31034;&#27861;&#65292;&#38459;&#30861;&#20102;&#19968;&#33324;&#39046;&#22495;LLMs&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ChemDFM&#65292;&#36825;&#26159;&#39318;&#20010;&#38754;&#21521;CGI&#30340;LLM&#12290;ChemDFM-13B&#26159;&#22312;&#21270;&#23398;&#25991;&#29486;&#12289;&#25945;&#31185;&#20070;&#12289;&#35828;&#26126;&#20070;&#20197;&#21450;&#21508;&#31181;&#19968;&#33324;&#39046;&#22495;&#30340;&#25968;&#25454;&#20013;&#35757;&#32451;&#30340;34B&#20196;&#29260;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#65292;&#21516;&#26102;&#20855;&#26377;&#20808;&#36827;&#30340;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;ChemDFM&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#20195;&#34920;&#24615;&#30340;&#24320;&#28304;LLMs&#12290;&#27492;&#22806;&#65292;ChemDFM&#36824;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have established great success in the general domain of natural language processing. Their emerging task generalization and free-form dialogue capabilities can greatly help to design Chemical General Intelligence (CGI) to assist real-world research in chemistry. However, the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry. To this end, we develop ChemDFM, the first LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature, textbooks, and instructions as well as various data from the general domain. Therefore, it can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities. Extensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs. Moreover, ChemDFM can also
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#20247;&#21253;&#21465;&#20107;&#21644;AI&#21465;&#20107;&#65292;&#25506;&#31350;&#20102;&#25991;&#21270;&#20135;&#29289;&#21644;&#31038;&#20250;&#20559;&#35265;&#22312;&#25925;&#20107;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#30340;&#21465;&#20107;&#26356;&#20855;&#36827;&#23637;&#24615;&#65292;&#24182;&#19988;&#26222;&#32599;&#31859;&#20462;&#26031;&#31070;&#35805;&#22312;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24819;&#35937;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.12902</link><description>&lt;p&gt;
&#23454;&#39564;&#21465;&#20107;&#65306;&#20154;&#31867;&#20247;&#21253;&#21465;&#20107;&#21644;AI&#21465;&#20107;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling. (arXiv:2310.12902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#20247;&#21253;&#21465;&#20107;&#21644;AI&#21465;&#20107;&#65292;&#25506;&#31350;&#20102;&#25991;&#21270;&#20135;&#29289;&#21644;&#31038;&#20250;&#20559;&#35265;&#22312;&#25925;&#20107;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#30340;&#21465;&#20107;&#26356;&#20855;&#36827;&#23637;&#24615;&#65292;&#24182;&#19988;&#26222;&#32599;&#31859;&#20462;&#26031;&#31070;&#35805;&#22312;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24819;&#35937;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#34892;&#20026;&#21644;&#35745;&#31639;&#23454;&#39564;&#65292;&#21033;&#29992;&#34394;&#26500;&#30340;&#25552;&#31034;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;&#30740;&#31350;&#20154;&#31867;&#21644;&#29983;&#25104;&#24335;AI&#21465;&#20107;&#20013;&#30340;&#25991;&#21270;&#20135;&#29289;&#21644;&#31038;&#20250;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;2019&#24180;6&#26376;&#30001;&#20247;&#21253;&#24037;&#20316;&#32773;&#21019;&#20316;&#30340;250&#20010;&#25925;&#20107;&#21644;2023&#24180;3&#26376;&#30001;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#30340;80&#20010;&#25925;&#20107;&#65292;&#23558;&#21465;&#20107;&#23398;&#21644;&#25512;&#29702;&#32479;&#35745;&#23398;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20247;&#21253;&#24037;&#20316;&#32773;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37117;&#22238;&#31572;&#20102;&#20851;&#20110;&#19982;&#20154;&#24037;&#26234;&#33021;&#20154;&#31867;&#30456;&#24651;&#30340;&#20027;&#39064;&#30340;&#30456;&#21516;&#25552;&#31034;&#12290;&#25552;&#20986;&#30340;&#23454;&#39564;&#33539;&#24335;&#20351;&#20154;&#31867;&#21644;LLM&#29983;&#25104;&#30340;&#21465;&#20107;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;&#25552;&#21040;&#26222;&#32599;&#31859;&#20462;&#26031;&#20027;&#39064;&#30340;&#22238;&#24212;&#35777;&#23454;&#20102;&#26222;&#32599;&#31859;&#20462;&#26031;&#31070;&#35805;&#22312;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#20307;&#24819;&#35937;&#20013;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;&#25152;&#26377;&#25552;&#20379;&#30340;&#21465;&#20107;&#37117;&#34920;&#29616;&#20986;&#31185;&#23398;&#25110;&#25216;&#26415;&#30340;&#36861;&#27714;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;GPT-3.5&#21644;&#23588;&#20854;&#26159;GPT-4&#29983;&#25104;&#30340;&#21465;&#20107;&#26356;&#20855;&#36827;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a framework that combines behavioral and computational experiments employing fictional prompts as a novel tool for investigating cultural artifacts and social biases in storytelling both by humans and generative AI. The study analyzes 250 stories authored by crowdworkers in June 2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging methods from narratology and inferential statistics. Both crowdworkers and large language models responded to identical prompts about creating and falling in love with an artificial human. The proposed experimental paradigm allows a direct comparison between human and LLM-generated storytelling. Responses to the Pygmalionesque prompts confirm the pervasive presence of the Pygmalion myth in the collective imaginary of both humans and large language models. All solicited narratives present a scientific or technological pursuit. The analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are more more progre
&lt;/p&gt;</description></item><item><title>GestureGPT&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#20132;&#20114;&#25163;&#21183;&#29702;&#35299;&#21644;&#23545;&#25509;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#35299;&#35835;&#25163;&#21183;&#25551;&#36848;&#24182;&#26681;&#25454;&#20132;&#20114;&#29615;&#22659;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#24847;&#22270;&#23545;&#25509;&#21040;&#20132;&#20114;&#21151;&#33021;&#19978;&#12290;</title><link>http://arxiv.org/abs/2310.12821</link><description>&lt;p&gt;
GestureGPT: &#38646;&#26679;&#26412;&#20132;&#20114;&#25163;&#21183;&#29702;&#35299;&#19982;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents. (arXiv:2310.12821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12821
&lt;/p&gt;
&lt;p&gt;
GestureGPT&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#20132;&#20114;&#25163;&#21183;&#29702;&#35299;&#21644;&#23545;&#25509;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#35299;&#35835;&#25163;&#21183;&#25551;&#36848;&#24182;&#26681;&#25454;&#20132;&#20114;&#29615;&#22659;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#24847;&#22270;&#23545;&#25509;&#21040;&#20132;&#20114;&#21151;&#33021;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#20027;&#35201;&#20851;&#27880;&#35782;&#21035;&#39044;&#23450;&#20041;&#38598;&#21512;&#20013;&#30340;&#25163;&#21183;&#65292;&#26410;&#33021;&#23558;&#36825;&#20123;&#25163;&#21183;&#19982;&#20132;&#20114;&#24335;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#20803;&#32032;&#25110;&#31995;&#32479;&#21151;&#33021;&#30456;&#36830;&#25509;&#65288;&#20363;&#22914;&#65292;&#23558;&#8220;&#31446;&#36215;&#22823;&#25287;&#25351;&#8221;&#25163;&#21183;&#19982;&#8220;&#21916;&#27426;&#8221;&#25353;&#38062;&#20851;&#32852;&#36215;&#26469;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GestureGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#25163;&#21183;&#29702;&#35299;&#21644;&#23545;&#25509;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#25163;&#21183;&#25551;&#36848;&#26681;&#25454;&#25163;&#21183;&#35270;&#39057;&#20013;&#30340;&#25163;&#37096;&#20851;&#38190;&#28857;&#22352;&#26631;&#36827;&#34892;&#24418;&#24335;&#21270;&#65292;&#24182;&#36755;&#20837;&#21040;&#25105;&#20204;&#30340;&#21452;&#20195;&#29702;&#23545;&#35805;&#31995;&#32479;&#20013;&#12290;&#19968;&#20010;&#25163;&#21183;&#20195;&#29702;&#35299;&#35835;&#36825;&#20123;&#25551;&#36848;&#65292;&#24182;&#35810;&#38382;&#26377;&#20851;&#20132;&#20114;&#29615;&#22659;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#30028;&#38754;&#12289;&#21382;&#21490;&#35760;&#24405;&#12289;&#20957;&#35270;&#25968;&#25454;&#65289;&#65292;&#19968;&#20010;&#19978;&#19979;&#25991;&#20195;&#29702;&#36127;&#36131;&#32452;&#32455;&#24182;&#25552;&#20379;&#36825;&#20123;&#20449;&#24687;&#12290;&#32463;&#36807;&#36845;&#20195;&#30340;&#20132;&#27969;&#65292;&#25163;&#21183;&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#23558;&#20854;&#23545;&#25509;&#21040;&#19968;&#20010;&#20132;&#20114;&#21151;&#33021;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#30340;&#31532;&#19968;&#35270;&#35282;&#21644;&#31532;&#19977;&#35270;&#35282;&#25163;&#21183;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25163;&#21183;&#25551;&#36848;&#27169;&#22359;&#65292;&#24182;&#22312;&#35270;&#39057;&#27969;&#21644;&#26234;&#33021;&#23478;&#23621;&#29289;&#32852;&#32593;&#25511;&#21046;&#30340;&#20004;&#20010;&#30495;&#23454;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;&#25972;&#20010;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current gesture recognition systems primarily focus on identifying gestures within a predefined set, leaving a gap in connecting these gestures to interactive GUI elements or system functions (e.g., linking a 'thumb-up' gesture to a 'like' button). We introduce GestureGPT, a novel zero-shot gesture understanding and grounding framework leveraging large language models (LLMs). Gesture descriptions are formulated based on hand landmark coordinates from gesture videos and fed into our dual-agent dialogue system. A gesture agent deciphers these descriptions and queries about the interaction context (e.g., interface, history, gaze data), which a context agent organizes and provides. Following iterative exchanges, the gesture agent discerns user intent, grounding it to an interactive function. We validated the gesture description module using public first-view and third-view gesture datasets and tested the whole system in two real-world settings: video streaming and smart home IoT control. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AV2Wav&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#65292;&#20811;&#26381;&#20102;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22768;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08030</link><description>&lt;p&gt;
AV2Wav&#65306;&#22522;&#20110;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#30340;&#25193;&#25955;&#37325;&#21512;&#25104;&#25216;&#26415;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement. (arXiv:2309.08030v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AV2Wav&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#65292;&#20811;&#26381;&#20102;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22768;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#24178;&#20928;&#21644;&#22122;&#22768;&#35821;&#38899;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#20013;&#65292;&#24178;&#20928;&#30340;&#25968;&#25454;&#19981;&#22815;&#22810;&#65307;&#22823;&#22810;&#25968;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#38598;&#37117;&#26159;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#65292;&#21253;&#21547;&#32972;&#26223;&#22122;&#22768;&#21644;&#28151;&#21709;&#65292;&#36825;&#38459;&#30861;&#20102;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AV2Wav&#65292;&#19968;&#31181;&#22522;&#20110;&#37325;&#21512;&#25104;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#19979;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#36136;&#37327;&#20272;&#35745;&#22120;&#20174;&#38899;&#39057;-&#35270;&#35273;&#35821;&#26009;&#24211;&#20013;&#33719;&#21462;&#20960;&#20046;&#24178;&#20928;&#30340;&#35821;&#38899;&#23376;&#38598;&#65292;&#24182;&#22312;&#27492;&#23376;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#26469;&#33258;AV-HuBERT&#30340;&#36830;&#32493;&#35821;&#38899;&#34920;&#31034;&#29983;&#25104;&#22768;&#27874;&#24418;&#65292;&#20855;&#26377;&#22122;&#22768;&#40065;&#26834;&#35757;&#32451;&#12290;&#25105;&#20204;&#20351;&#29992;&#36830;&#32493;&#32780;&#19981;&#26159;&#31163;&#25955;&#34920;&#31034;&#26469;&#20445;&#30041;&#38901;&#24459;&#21644;&#35828;&#35805;&#32773;&#20449;&#24687;&#12290;&#20165;&#20165;&#36890;&#36807;&#22768;&#30721;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#23601;&#27604;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26356;&#22909;&#22320;&#25191;&#34892;&#35821;&#38899;&#22686;&#24378;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;fine-tune&#27169;&#22411;&#65292;&#20197;&#36716;&#21270;&#20026;&#22312;&#22810;&#20219;&#21153;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#32852;&#21512;&#22810;&#24103;&#22768;&#23398;&#21040;&#35821;&#38899;&#36716;&#21270;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech enhancement systems are typically trained using pairs of clean and noisy speech. In audio-visual speech enhancement (AVSE), there is not as much ground-truth clean data available; most audio-visual datasets are collected in real-world environments with background noise and reverberation, hampering the development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based audio-visual speech enhancement approach that can generate clean speech despite the challenges of real-world training data. We obtain a subset of nearly clean speech from an audio-visual corpus using a neural quality estimator, and then train a diffusion model on this subset to generate waveforms conditioned on continuous speech representations from AV-HuBERT with noise-robust training. We use continuous rather than discrete representations to retain prosody and speaker information. With this vocoding task alone, the model can perform speech enhancement better than a masking-based baseline. We further fine-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26159;&#19968;&#31867;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#65292;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30446;&#26631;&#20316;&#32773;&#39118;&#26684;&#25991;&#26412;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#38750;&#30693;&#21517;&#20316;&#32773;&#30340;&#39118;&#26684;&#36716;&#31227;&#23578;&#26410;&#26377;&#20805;&#20998;&#30340;&#30740;&#31350;&#65292;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#36866;&#29992;&#20110;&#24050;&#21457;&#34920;&#30340;&#20316;&#23478;&#12289;&#25919;&#27835;&#23478;&#25110;&#20854;&#20182;&#30693;&#21517;&#20154;&#22763;&#21644;&#20316;&#32773;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2212.08986</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#65306;&#38750;&#30693;&#21517;&#20316;&#32773;&#33021;&#22815;&#34987;&#27169;&#20223;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Authorship Style Transfer: Can Non-Famous Authors Be Imitated?. (arXiv:2212.08986v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08986
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26159;&#19968;&#31867;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#65292;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30446;&#26631;&#20316;&#32773;&#39118;&#26684;&#25991;&#26412;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#38750;&#30693;&#21517;&#20316;&#32773;&#30340;&#39118;&#26684;&#36716;&#31227;&#23578;&#26410;&#26377;&#20805;&#20998;&#30340;&#30740;&#31350;&#65292;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#36866;&#29992;&#20110;&#24050;&#21457;&#34920;&#30340;&#20316;&#23478;&#12289;&#25919;&#27835;&#23478;&#25110;&#20854;&#20182;&#30693;&#21517;&#20154;&#22763;&#21644;&#20316;&#32773;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#26159;&#25351;&#23558;&#25991;&#26412;&#25913;&#20889;&#25104;&#30446;&#26631;&#20316;&#32773;&#30340;&#39118;&#26684;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#24847;&#24605;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#22823;&#22810;&#19987;&#27880;&#20110;&#23558;&#39118;&#26684;&#36716;&#31227;&#21040;&#22312;&#20070;&#31821;&#12289;&#28436;&#35762;&#25110;&#20854;&#20182;&#24050;&#21457;&#34920;&#20316;&#21697;&#20013;&#20855;&#26377;&#35768;&#22810;&#31034;&#20363;&#30340;&#30446;&#26631;&#20316;&#32773;&#36523;&#19978;&#12290;&#36825;&#31181;&#39640;&#36164;&#28304;&#30340;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#65288;&#36890;&#24120;&#22823;&#20110;10&#19975;&#20010;&#35789;&#65289;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#36866;&#29992;&#20110;&#23558;&#39118;&#26684;&#36716;&#31227;&#21040;&#24050;&#21457;&#34920;&#30340;&#20316;&#23478;&#12289;&#25919;&#27835;&#23478;&#25110;&#20854;&#20182;&#30693;&#21517;&#20154;&#22763;&#21644;&#20316;&#32773;&#39118;&#26684;&#19978;&#65292;&#32780;&#36716;&#31227;&#21040;&#38750;&#30693;&#21517;&#20316;&#32773;&#30340;&#39118;&#26684;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#8221;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#31867;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#65292;&#20165;&#23384;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#30446;&#26631;&#20316;&#32773;&#39118;&#26684;&#25991;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#36873;&#25321;&#20102;Reddit&#19978;&#30340;&#28304;&#20316;&#32773;&#21644;&#30446;&#26631;&#20316;&#32773;&#65292;&#24182;&#23545;&#20182;&#20204;&#30340;Reddit&#24086;&#23376;&#36827;&#34892;&#39118;&#26684;&#36716;&#31227;&#65292;&#38480;&#21046;&#33258;&#24049;&#20165;&#20351;&#29992;&#20102;16&#31687;&#24086;&#23376;&#65288;&#24179;&#22343;&#32422;500&#20010;&#35789;&#65289;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Authorship style transfer involves altering text to match the style of a target author whilst preserving the original meaning. Existing unsupervised approaches like STRAP have largely focused on style transfer to target authors with many examples of their writing style in books, speeches, or other published works. This high-resource training data requirement (often greater than 100,000 words) makes these approaches primarily useful for style transfer to published authors, politicians, or other well-known figures and authorship styles, while style transfer to non-famous authors has not been well-studied. We introduce the \textit{low-resource authorship style transfer} task, a more challenging class of authorship style transfer where only a limited amount of text in the target author's style may exist. In our experiments, we specifically choose source and target authors from Reddit and style transfer their Reddit posts, limiting ourselves to just 16 posts (on average ~500 words) of the t
&lt;/p&gt;</description></item></channel></rss>