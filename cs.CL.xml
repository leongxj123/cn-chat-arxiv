<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.01476</link><description>&lt;p&gt;
TraveLER&#65306;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;&#30340;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TraveLER: A Multi-LMM Agent Framework for Video Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01476
&lt;/p&gt;
&lt;p&gt;
TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#20197;&#24103;&#20026;&#21333;&#20301;&#36827;&#34892;&#22788;&#29702;&#12290;&#34429;&#28982;&#22522;&#20110;&#22270;&#20687;&#30340;&#35270;&#39057;&#26041;&#27861;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#30340;&#23616;&#38480;&#26159;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20102;&#22914;&#20309;&#36873;&#25321;&#20851;&#38190;&#26102;&#38388;&#25139;&#65292;&#24182;&#19988;&#26080;&#27861;&#22312;&#30830;&#23450;&#38169;&#35823;&#26102;&#38388;&#25139;&#26102;&#36827;&#34892;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#25552;&#21462;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#32454;&#33410;&#65292;&#32780;&#26159;&#25552;&#20379;&#24103;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#23427;&#27839;&#30528;&#35270;&#39057;&#36827;&#34892;&#31227;&#21160;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#30340;&#26041;&#24335;&#36845;&#20195;&#22320;&#20174;&#20851;&#38190;&#24103;&#25910;&#38598;&#30456;&#20851;&#20449;&#24687;&#65292;&#30452;&#21040;&#33719;&#24471;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TraveLER&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#21046;&#23450;&#8220;&#36941;&#21382;&#8221;&#35270;&#39057;&#35745;&#21010;&#30340;&#27169;&#22411;&#65292;&#35810;&#38382;&#20851;&#20110;&#21333;&#20010;&#24103;&#30340;&#38382;&#39064;&#20197;&#8220;&#23450;&#20301;&#8221;&#24182;&#23384;&#20648;&#20851;&#38190;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01476v1 Announce Type: cross  Abstract: Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to "Traverse" through the video, ask questions about individual frames to "Locate" and store key info
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#26631;&#20934;&#21270;&#35821;&#31687;&#29702;&#35299;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#25512;&#26029;&#26410;&#26126;&#30830;&#38472;&#36848;&#20449;&#24687;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#23454;&#21147;</title><link>https://arxiv.org/abs/2403.17196</link><description>&lt;p&gt;
GPT-4&#33267;&#23569;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#35821;&#31687;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Understands Discourse at Least as Well as Humans Do
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17196
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#26631;&#20934;&#21270;&#35821;&#31687;&#29702;&#35299;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#25512;&#26029;&#26410;&#26126;&#30830;&#38472;&#36848;&#20449;&#24687;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#23454;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#31181;&#39046;&#20808;&#30340;AI&#31995;&#32479;GPT-4&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#35821;&#31687;&#65292;&#20351;&#29992;&#20102;&#19968;&#39033;&#26631;&#20934;&#21270;&#30340;&#35821;&#31687;&#29702;&#35299;&#27979;&#35797;&#12290;&#21442;&#19982;&#32773;&#20250;&#34987;&#21576;&#29616;&#31616;&#30701;&#30340;&#25925;&#20107;&#65292;&#28982;&#21518;&#22238;&#31572;&#20843;&#20010;&#26159;/&#21542;&#38382;&#39064;&#65292;&#25506;&#31350;&#20182;&#20204;&#23545;&#25925;&#20107;&#30340;&#29702;&#35299;&#12290;&#36825;&#20123;&#38382;&#39064;&#30340;&#26684;&#24335;&#26088;&#22312;&#35780;&#20272;&#30452;&#25509;&#24615;&#65288;&#38472;&#36848; vs. &#26263;&#31034;&#65289;&#21644;&#26174;&#33879;&#24615;&#65288;&#20027;&#35201;&#35266;&#28857; vs. &#32454;&#33410;&#65289;&#30340;&#29420;&#31435;&#24433;&#21709;&#12290;&#37492;&#20110;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#38750;&#24120;&#39640;&#65292;GPT-4&#30340;&#34920;&#29616;&#30053;&#22909;&#20110;&#20154;&#31867;&#65292;&#20294;&#24182;&#26080;&#32479;&#35745;&#23398;&#26174;&#33879;&#24046;&#24322;&#12290;GPT-4&#21644;&#20154;&#31867;&#37117;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#25512;&#26029;&#25925;&#20107;&#20013;&#26410;&#26126;&#30830;&#38472;&#36848;&#30340;&#20449;&#24687;&#65292;&#36825;&#26159;&#23545;&#29702;&#35299;&#21147;&#30340;&#37325;&#35201;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17196v1 Announce Type: new  Abstract: We test whether a leading AI system GPT-4 understands discourse as well as humans do, using a standardized test of discourse comprehension. Participants are presented with brief stories and then answer eight yes/no questions probing their comprehension of the story. The questions are formatted to assess the separate impacts of directness (stated vs. implied) and salience (main idea vs. details). GPT-4 performs slightly, but not statistically significantly, better than humans given the very high level of human performance. Both GPT-4 and humans exhibit a strong ability to make inferences about information that is not explicitly stated in a story, a critical test of understanding.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#22522;&#30784;&#21644;&#25351;&#20196;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#21477;&#23376;&#21487;&#20449;&#24230;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#20998;&#25968;&#26159;&#26368;&#21487;&#38752;&#30340;&#21477;&#23376;&#21487;&#20449;&#24230;&#25351;&#26631;&#65292;&#20294;&#20173;&#20302;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.14859</link><description>&lt;p&gt;
&#22312;&#22522;&#30784;&#27169;&#22411;&#21644;&#25351;&#20196;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27604;&#36739;&#21487;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14859
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22522;&#30784;&#21644;&#25351;&#20196;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#21477;&#23376;&#21487;&#20449;&#24230;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#20998;&#25968;&#26159;&#26368;&#21487;&#38752;&#30340;&#21477;&#23376;&#21487;&#20449;&#24230;&#25351;&#26631;&#65292;&#20294;&#20173;&#20302;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#30340;LLM&#21487;&#20197;&#21709;&#24212;&#26126;&#30830;&#21046;&#23450;&#20026;&#25552;&#31034;&#30340;&#26597;&#35810;&#65292;&#36825;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#19982;&#20154;&#31867;&#29992;&#25143;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#33021;&#22815;&#21033;&#29992;LLM&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#30340;&#38544;&#24335;&#30693;&#35782;&#12290;&#26412;&#25991;&#23545;&#35780;&#20272;LLM&#20013;&#35821;&#20041;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#65288;a&#65289;&#26126;&#30830;&#25552;&#31034;&#21644;&#65288;b&#65289;&#30452;&#25509;&#35835;&#21462;&#27169;&#22411;&#20998;&#37197;&#32473;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#30340;&#38544;&#24335;&#20272;&#35745;&#65292;&#22312;&#33521;&#35821;&#21477;&#23376;&#21487;&#20449;&#24230;&#20219;&#21153;&#20013;&#27604;&#36739;&#20102;&#22522;&#30784;&#21644;&#25351;&#20196;&#35843;&#20248;LLM&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;1&#34920;&#26126;&#65292;&#36328;&#27169;&#22411;&#26550;&#26500;&#21644;&#21487;&#20449;&#24230;&#25968;&#25454;&#38598;&#65292;&#65288;i&#65289;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#20998;&#25968;&#26159;&#21477;&#23376;&#21487;&#20449;&#24230;&#26368;&#21487;&#38752;&#30340;&#25351;&#26631;&#65292;&#38646;&#29031;&#23556;&#25552;&#31034;&#20135;&#29983;&#19981;&#19968;&#33268;&#19988;&#36890;&#24120;&#25928;&#26524;&#19981;&#20339;&#30340;&#32467;&#26524;&#65307;&#65288;ii&#65289;&#22522;&#20110;LL&#30340;&#24615;&#33021;&#20173;&#20302;&#20110;&#20154;&#31867;&#34920;&#29616;&#65307;&#65288;iii&#65289;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14859v1 Announce Type: cross  Abstract: Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood ($\textit{LL}$) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) $\textit{LL}$-based performance is still inferior to human performance; (iii) instruction-tuned models hav
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#34920;&#31034;&#30340;&#22270;&#20687;&#26469;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#22797;&#21512;&#25552;&#31034;&#21644;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#30340;&#19968;&#20123;&#37325;&#35201;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.05846</link><description>&lt;p&gt;
&#25193;&#25955;&#38236;&#22836;&#65306;&#35299;&#37322;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#31649;&#36947;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#34920;&#31034;&#30340;&#22270;&#20687;&#26469;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#22797;&#21512;&#25552;&#31034;&#21644;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#30340;&#19968;&#20123;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05846v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#36328; &#23384;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;T2I&#65289;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#32534;&#30721;&#22120;&#20135;&#29983;&#25991;&#26412;&#34920;&#31034;&#30340;&#36807;&#31243;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#38236;&#22836;&#65292;&#19968;&#31181;&#20998;&#26512; T2I &#27169;&#22411;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20854;&#20013;&#38388;&#34920;&#31034;&#30340;&#22270;&#20687;&#12290;&#20351;&#29992;&#25193;&#25955;&#38236;&#22836;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#26368;&#36817;&#30340; T2I &#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#12290;&#22312;&#25506;&#32034;&#22797;&#21512;&#25552;&#31034;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#25551;&#36848;&#22810;&#20010;&#23545;&#35937;&#30340;&#22797;&#26434;&#22330;&#26223;&#30456;&#23545;&#20110;&#31616;&#21333;&#22330;&#26223;&#26159;&#36880;&#27493;&#19988;&#36739;&#24930;&#22320;&#26500;&#24314;&#30340;&#65307;&#22312;&#25506;&#32034;&#30693;&#35782;&#26816;&#32034;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#34920;&#31034;&#19981;&#24120;&#35265;&#27010;&#24565;&#38656;&#35201;&#27604;&#24120;&#35265;&#27010;&#24565;&#26356;&#22810;&#30340;&#35745;&#31639;&#65292;&#24182;&#19988;&#30693;&#35782;&#26816;&#32034;&#22312;&#23618;&#20043;&#38388;&#26159;&#28176;&#36827;&#30340;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026; T2I &#31649;&#36947;&#20013;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#32452;&#20214;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05846v1 Announce Type: cross  Abstract: Text-to-image diffusion models (T2I) use a latent representation of a text prompt to guide the image generation process. However, the process by which the encoder produces the text representation is unknown. We propose the Diffusion Lens, a method for analyzing the text encoder of T2I models by generating images from its intermediate representations. Using the Diffusion Lens, we perform an extensive analysis of two recent T2I models. Exploring compound prompts, we find that complex scenes describing multiple objects are composed progressively and more slowly compared to simple scenes; Exploring knowledge retrieval, we find that representation of uncommon concepts requires further computation compared to common concepts, and that knowledge retrieval is gradual across layers. Overall, our findings provide valuable insights into the text encoder component in T2I pipelines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#30456;&#21516;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#26597;&#35810;&#32452;&#21512;&#20026;&#21333;&#20010;&#25552;&#31034;&#65292;&#20197;&#26368;&#23567;&#21270;&#37325;&#22797;&#35843;&#29992;&#26469;&#20248;&#21270;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.00067</link><description>&lt;p&gt;
Query-OPT&#65306;&#36890;&#36807;&#22810;&#26597;&#35810;&#25351;&#20196;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#30456;&#21516;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#26597;&#35810;&#32452;&#21512;&#20026;&#21333;&#20010;&#25552;&#31034;&#65292;&#20197;&#26368;&#23567;&#21270;&#37325;&#22797;&#35843;&#29992;&#26469;&#20248;&#21270;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20851;&#27880;&#22522;&#20110;&#26597;&#35810;&#30340;&#20250;&#35758;&#25688;&#35201;&#20219;&#21153;&#65292;&#22312;&#27492;&#20219;&#21153;&#20013;&#65292;&#38024;&#23545;&#29305;&#23450;&#26597;&#35810;&#23545;&#19978;&#19979;&#25991;&#65288;&#20250;&#35758;&#35760;&#24405;&#65289;&#29983;&#25104;&#25688;&#35201;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#27492;&#20219;&#21153;&#26102;&#65292;&#21363;&#20351;&#19978;&#19979;&#25991;&#20445;&#25345;&#19981;&#21464;&#65292;&#27599;&#20010;&#26032;&#26597;&#35810;&#20063;&#38656;&#35201;&#23545;LLM&#25512;&#29702;&#31471;&#28857;/API&#36827;&#34892;&#19968;&#27425;&#26032;&#35843;&#29992;&#12290;&#28982;&#32780;&#65292;&#21453;&#22797;&#35843;&#29992;LLM&#25512;&#29702;&#31471;&#28857;&#20250;&#26174;&#33879;&#22686;&#21152;&#22312;&#29983;&#20135;&#20013;&#20351;&#29992;&#23427;&#20204;&#30340;&#25104;&#26412;&#65292;&#36825;&#20351;&#24471;&#35768;&#22810;&#23454;&#38469;&#29992;&#20363;&#20013;LLMs&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#25104;&#21151;&#22320;&#23558;&#30456;&#21516;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#26597;&#35810;&#32452;&#21512;&#20026;&#21333;&#20010;&#25552;&#31034;&#20197;&#26368;&#23567;&#21270;&#37325;&#22797;&#35843;&#29992;&#65292;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#21508;&#31181;&#27969;&#34892;&#30340;LLM&#65288;GPT-4&#12289;PaLM-2&#12289;LLaMA-2&#12289;Mistral&#21644;FLAN-T5&#65289;&#22312;&#21333;&#26597;&#35810;&#21644;&#22810;&#26597;&#35810;&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00067v1 Announce Type: new  Abstract: This work focuses on the task of query-based meeting summarization in which the summary of a context (meeting transcript) is generated in response to a specific query. When using Large Language Models (LLMs) for this task, a new call to the LLM inference endpoint/API is required for each new query even if the context stays the same. However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases. To address this problem, in this paper, we investigate whether combining the queries for the same input context in a single prompt to minimize repeated calls can be successfully used in meeting summarization. In this regard, we conduct extensive experiments by comparing the performance of various popular LLMs: GPT-4, PaLM-2, LLaMA-2, Mistral, and FLAN-T5 in single-query and multi-query settings. We observe that while most LLMs tend to
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20250;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#22914;&#20309;&#21462;&#20915;&#20110;&#27169;&#22411;&#21442;&#25968;&#26159;&#29702;&#35299;&#20854;&#28508;&#22312;&#24433;&#21709;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.15518</link><description>&lt;p&gt;
&#24403;&#24515;&#35328;&#36766;&#65306;&#35780;&#20272;&#20250;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#20016;&#23500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Beware of Words: Evaluating the Lexical Richness of Conversational Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15518
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20250;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#22914;&#20309;&#21462;&#20915;&#20110;&#27169;&#22411;&#21442;&#25968;&#26159;&#29702;&#35299;&#20854;&#28508;&#22312;&#24433;&#21709;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24456;&#22810;&#19981;&#21516;&#20219;&#21153;&#20013;&#27491;&#22312;&#35780;&#20272;&#20250;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;ChatGPT&#65292;&#20174;&#36923;&#36753;&#25512;&#29702;&#25110;&#25968;&#23398;&#21040;&#22238;&#31572;&#21508;&#31181;&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#30340;&#30740;&#31350;&#21364;&#23569;&#20043;&#21448;&#23569;&#12290;&#36825;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#22240;&#20026;LLMs&#26159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20102;&#35299;&#23427;&#20204;&#22914;&#20309;&#20351;&#29992;&#35821;&#35328;&#26159;&#37325;&#35201;&#30340;&#12290;&#20107;&#23454;&#19978;&#65292;&#20250;&#35805;&#24335;LLMs&#21487;&#33021;&#23545;&#35821;&#35328;&#30340;&#28436;&#21464;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#20204;&#26368;&#32456;&#21487;&#33021;&#20027;&#23548;&#26032;&#25991;&#26412;&#30340;&#21019;&#20316;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#23427;&#20204;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#22914;&#20309;&#21462;&#20915;&#20110;&#27169;&#22411;&#21442;&#25968;&#26159;&#20102;&#35299;&#28508;&#22312;&#24433;&#21709;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15518v1 Announce Type: new  Abstract: The performance of conversational Large Language Models (LLMs) in general, and of ChatGPT in particular, is currently being evaluated on many different tasks, from logical reasoning or maths to answering questions on a myriad of topics. Instead, much less attention is being devoted to the study of the linguistic features of the texts generated by these LLMs. This is surprising since LLMs are models for language, and understanding how they use the language is important. Indeed, conversational LLMs are poised to have a significant impact on the evolution of languages as they may eventually dominate the creation of new text. This means that for example, if conversational LLMs do not use a word it may become less and less frequent and eventually stop being used altogether. Therefore, evaluating the linguistic features of the text they produce and how those depend on the model parameters is the first step toward understanding the potential im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26657;&#20934;&#26469;&#20943;&#36731;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#38376;&#25511;&#26657;&#20934;&#32593;&#32476;&#24182;&#26500;&#24314;&#20102;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.14296</link><description>&lt;p&gt;
&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#36890;&#36807;&#26657;&#20934;&#20943;&#36731;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases of Large Language Models in Stance Detection with Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26657;&#20934;&#26469;&#20943;&#36731;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#38376;&#25511;&#26657;&#20934;&#32593;&#32476;&#24182;&#26500;&#24314;&#20102;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#20250;&#29983;&#25104;&#20559;&#35265;&#31435;&#22330;&#65292;&#36825;&#26159;&#30001;&#20110;&#34394;&#20551;&#24773;&#24863;-&#31435;&#22330;&#30456;&#20851;&#24615;&#21644;&#23545;&#26576;&#20123;&#20010;&#20154;&#21644;&#20027;&#39064;&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26657;&#20934;&#26469;&#20943;&#36731;LLMs&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#30340;&#20559;&#35265;&#65288;MB-Cal&#65289;&#12290;&#22312;&#20854;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38376;&#25511;&#26657;&#20934;&#32593;&#32476;&#65292;&#20197;&#20943;&#36731;LLMs&#20135;&#29983;&#30340;&#31435;&#22330;&#25512;&#29702;&#32467;&#26524;&#19978;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;&#26657;&#20934;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#26469;&#30699;&#27491;&#31435;&#22330;&#20559;&#35265;&#12290;&#38024;&#23545;&#30446;&#26631;&#21644;&#38646;&#23556;&#20987;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MB-Cal&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;LLMs&#30340;&#20559;&#35265;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14296v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved remarkable progress in many natural language processing tasks. However, our experiment reveals that, in stance detection tasks, LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance. Therefore, in this paper, we propose to Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). In which, a novel gated calibration network is devised to mitigate the biases on the stance reasoning results from LLMs. Further, to make the calibration more accurate and generalizable, we construct counterfactual augmented data to rectify stance biases. Experimental results on in-target and zero-shot stance detection tasks show that the proposed MB-Cal can effectively mitigate biases of LLMs, achieving state-of-the-art results.
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.13764</link><description>&lt;p&gt;
CriticBench: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#35770;&#23478;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Evaluating Large Language Models as Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13764
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102; CriticBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22235;&#20010;&#20851;&#38190;&#35780;&#35770;&#33021;&#21147;&#32500;&#24230;&#65288;&#21453;&#39304;&#12289;&#27604;&#36739;&#12289;&#25913;&#36827;&#21644;&#20803;&#21453;&#39304;&#65289;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;CriticBench&#21253;&#21547;&#20061;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#36136;&#37327;&#32454;&#31890;&#24230;&#27700;&#24179;&#19978;&#35780;&#35770;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#25581;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#26377;&#36259;&#30340;&#20851;&#31995;&#12290;CriticBench&#30340;&#25968;&#25454;&#38598;&#12289;&#36164;&#28304;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#23558;&#22312;https://github.com/gmftbyGMFTBY/Cri&#19978;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13764v1 Announce Type: cross  Abstract: Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/Cri
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38416;&#26126;&#20102;&#23558;&#19987;&#26377;&#24040;&#22836;&#30340;&#20808;&#36827;&#21151;&#33021;&#36716;&#31227;&#21040;&#24320;&#28304;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13116</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Distillation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38416;&#26126;&#20102;&#23558;&#19987;&#26377;&#24040;&#22836;&#30340;&#20808;&#36827;&#21151;&#33021;&#36716;&#31227;&#21040;&#24320;&#28304;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#20013;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#37325;&#28857;&#20851;&#27880;KD&#22312;&#23558;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#19987;&#26377;&#24040;&#22836;&#30340;&#22797;&#26434;&#33021;&#21147;&#36716;&#31227;&#21040;&#21487;&#35775;&#38382;&#30340;&#24320;&#28304;&#27169;&#22411;&#65288;&#22914;LLaMA&#21644;Mistral&#65289;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#26412;&#39033;&#24037;&#20316;&#38416;&#26126;&#20102;&#19987;&#26377;&#21644;&#24320;&#28304;LLMs&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#23637;&#31034;&#20102;KD&#22914;&#20309;&#25104;&#20026;&#31532;&#20108;&#32773;&#36171;&#20104;&#31532;&#19968;&#32773;&#20808;&#36827;&#21151;&#33021;&#21644;&#32454;&#33268;&#29702;&#35299;&#30340;&#37325;&#35201;&#23186;&#20171;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#22260;&#32469;&#31639;&#27861;&#12289;&#25216;&#33021;&#21644;&#22402;&#30452;&#21270;&#36825;&#19977;&#20010;&#22522;&#30784;&#25903;&#26609;&#31934;&#24515;&#26500;&#24314;&#65292;&#20840;&#38754;&#25506;&#35752;&#20102;KD&#26426;&#21046;&#12289;&#29305;&#23450;&#35748;&#30693;&#33021;&#21147;&#30340;&#22686;&#24378;&#20197;&#21450;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35843;&#26597;&#24341;&#23548;&#30528;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#21644;KD&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13116v1 Announce Type: new  Abstract: This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, i
&lt;/p&gt;</description></item><item><title>SymBa&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#21270;&#21521;&#21518;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#25552;&#21319;&#65292;&#33021;&#22815;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.12806</link><description>&lt;p&gt;
SymBa&#65306;&#31526;&#21495;&#21270;&#21521;&#21518;&#25512;&#29702;&#29992;&#20110;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12806
&lt;/p&gt;
&lt;p&gt;
SymBa&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#21270;&#21521;&#21518;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#25552;&#21319;&#65292;&#33021;&#22815;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#19968;&#31995;&#21015;&#24605;&#32500;&#25552;&#31034;&#20013;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#24544;&#23454;&#30340;&#22810;&#27493;&#25512;&#29702;&#20381;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21521;&#21518;&#25512;&#29702;&#65292;&#21363;&#36890;&#36807;&#36923;&#36753;&#35268;&#21017;&#36882;&#24402;&#22320;&#20998;&#35299;&#26597;&#35810;&#65292;&#30452;&#21040;&#35777;&#26126;&#20026;&#27490;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#21521;&#21518;&#25512;&#29702;&#23454;&#29616;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SymBa&#65288;&#31526;&#21495;&#21270;&#21521;&#21518;&#25512;&#29702;&#65289;&#12290;&#22312;SymBa&#20013;&#65292;&#31526;&#21495;&#21270;&#33258;&#39030;&#21521;&#19979;&#27714;&#35299;&#22120;&#25511;&#21046;&#25972;&#20010;&#35777;&#26126;&#36807;&#31243;&#65292;&#24403;&#27714;&#35299;&#22120;&#36935;&#21040;&#27515;&#32993;&#21516;&#26102;&#65292;&#25165;&#35843;&#29992;LLM&#29983;&#25104;&#21333;&#20010;&#25512;&#29702;&#27493;&#39588;&#12290;&#36890;&#36807;&#36825;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;-LLM&#38598;&#25104;&#65292;SymBa&#22312;&#21508;&#31181;&#22810;&#27493;&#25512;&#29702;&#22522;&#20934;&#65288;ProofWriter&#65292;Birds-Electricity&#65292;GSM8k&#65292;CLUTRR-TF&#65292;ECtHR Article 6&#65289;&#20013;&#30456;&#27604;&#21521;&#21518;&#25512;&#29702;&#22522;&#32447;&#21462;&#24471;&#20102;&#24615;&#33021;&#12289;&#35777;&#26126;&#24544;&#23454;&#24615;&#21644;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#65292;&#33021;&#22815;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12806v1 Announce Type: new  Abstract: Large Language Models (LLMs) have recently demonstrated remarkable reasoning ability as in Chain-of-thought prompting, but faithful multi-step reasoning remains a challenge. We specifically focus on backward chaining, where the query is recursively decomposed using logical rules until proven. To address the limitations of current backward chaining implementations, we propose SymBa (Symbolic Backward Chaining). In SymBa, the symbolic top-down solver controls the entire proof process and the LLM is called to generate a single reasoning step only when the solver encounters a dead end. By this novel solver-LLM integration, while being able to produce an interpretable, structured proof, SymBa achieves significant improvement in performance, proof faithfulness, and efficiency in diverse multi-step reasoning benchmarks (ProofWriter, Birds-Electricity, GSM8k, CLUTRR-TF, ECtHR Article 6) compared to backward chaining baselines.
&lt;/p&gt;</description></item><item><title>&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11355</link><description>&lt;p&gt;
&#25913;&#21464;&#20102;&#20160;&#20040;&#65311;&#23558;&#34920;&#24449;&#24178;&#39044;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
What Changed? Converting Representational Interventions to Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11355
&lt;/p&gt;
&lt;p&gt;
&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34920;&#24449;&#31354;&#38388;&#30340;&#24178;&#39044;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#29992;&#26469;&#28040;&#38500;&#25110;&#25913;&#21464;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#32534;&#30721;&#65292;&#21019;&#24314;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24178;&#39044;&#25805;&#20316;&#22312;&#34920;&#31034;&#31354;&#38388;&#20869;&#65292;&#20934;&#30830;&#29702;&#35299;&#23427;&#20462;&#25913;&#20102;&#21738;&#20123;&#29305;&#24449;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#23545;&#24212;&#20110;&#32473;&#23450;&#34920;&#31034;&#31354;&#38388;&#24178;&#39044;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#35299;&#37322;&#29992;&#20110;&#32534;&#30721;&#29305;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#29992;&#20110;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11355v1 Announce Type: new  Abstract: Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a counterfactual representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space counterfactuals can be converted into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20110;ICD-9&#20195;&#30721;&#30340;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10940</link><description>&lt;p&gt;
&#20020;&#24202;&#31243;&#24207;&#20195;&#30721;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10940
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20110;ICD-9&#20195;&#30721;&#30340;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#26088;&#22312;&#36890;&#36807;&#23558;&#31995;&#32479;&#29983;&#25104;&#30340;&#24314;&#35758;&#19982;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#32467;&#21512;&#26469;&#22686;&#24378;&#20020;&#24202;&#21307;&#29983;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#22522;&#20110;&#25163;&#26415;ICD-9&#20195;&#30721;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26469;&#37327;&#21270;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19981;&#20165;&#23637;&#31034;&#20102;&#31243;&#24207;&#20195;&#30721;&#19982;&#23454;&#38469;&#21307;&#30103;&#32467;&#26524;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10940v1 Announce Type: new  Abstract: A Clinical Decision Support System (CDSS) is designed to enhance clinician decision-making by combining system-generated recommendations with medical expertise. Given the high costs, intensive labor, and time-sensitive nature of medical treatments, there is a pressing need for efficient decision support, especially in complex emergency scenarios. In these scenarios, where information can be limited, an advanced CDSS framework that leverages AI (artificial intelligence) models to effectively reduce diagnostic uncertainty has utility. Such an AI-enabled CDSS framework with quantified uncertainty promises to be practical and beneficial in the demanding context of real-world medical care. In this study, we introduce the concept of Medical Entropy, quantifying uncertainties in patient outcomes predicted by neural machine translation based on the ICD-9 code of procedures. Our experimental results not only show strong correlations between proce
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.08644</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#26029;&#39640;&#25928;LLMs&#30340;&#20018;&#32852;Transformer
&lt;/p&gt;
&lt;p&gt;
Tandem Transformers for Inference Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08644
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs )&#20855;&#26377;&#33258;&#22238;&#24402;&#30340;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#35789;&#20803;&#26159;&#25353;&#39034;&#24207;&#29983;&#25104;&#30340;&#12290;&#23613;&#31649;&#26377;&#20123;&#39044;&#27979;&#21644;&#24182;&#34892;&#35299;&#30721;&#25216;&#26415;&#35797;&#22270;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#38480;&#21046;&#65306;&#35201;&#20040;&#20381;&#36182;&#26356;&#31934;&#31616;&#20294;&#20934;&#30830;&#24230;&#36739;&#20302;&#30340;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#65292;&#35201;&#20040;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22522;&#30784;LLM&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#21363;&#20018;&#32852;Transformer&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#26550;&#26500;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;(1)&#19968;&#20010;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;(2)&#19968;&#20010;&#20197;&#22359;&#27169;&#24335;&#36816;&#34892;&#30340;&#22823;&#27169;&#22411;(&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#35789;&#20803;)&#12290;&#36890;&#36807;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#22823;&#24133;&#25552;&#21319;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;PaLM2&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;PaLM2-Bison&#21644;PaLM2-Gecko&#30340;&#20018;&#32852;&#30456;&#36739;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;3.3%&#65292;&#19982;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#30456;&#27604;&#65292;&#25552;&#20379;&#20102;1.16&#20493;&#30340;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06900</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#35782;&#21035;&#27602;&#24615;&#21527;&#65311;&#32467;&#26500;&#21270;&#27602;&#24615;&#35843;&#26597;&#26694;&#26550;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#36981;&#23432;&#31038;&#20250;&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#23384;&#22312;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#27602;&#24615;&#24230;&#37327;&#20381;&#36182;&#20110;&#22312;&#29305;&#23450;&#27602;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#30721;&#22120;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#22806;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#25152;&#20551;&#23450;&#30340;&#27602;&#24615;&#23450;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#33258;&#21160;&#40065;&#26834;&#24230;&#37327;&#65292;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#22238;&#24212;&#26159;&#21542;&#20855;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#27602;&#24615;&#22240;&#32032;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#24230;&#37327;&#25351;&#26631;LLMs As ToxiciTy Evaluators&#65288;LATTE&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;F1&#24471;&#20998;&#27604;&#29616;&#26377;&#25216;&#26415;&#25351;&#26631;&#25552;&#39640;&#20102;12&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19978;&#28216;&#27602;&#24615;&#23545;&#24230;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
&lt;/p&gt;</description></item><item><title>SPIRIT-LM&#26159;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#21644;&#35821;&#38899;&#36830;&#32493;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#21475;&#35821;&#21644;&#20070;&#38754;&#35821;&#35328;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#23427;&#23637;&#31034;&#20102;&#25991;&#26412;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#21644;&#35821;&#38899;&#27169;&#22411;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;SPIRIT-LM&#36824;&#33021;&#20197;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.05755</link><description>&lt;p&gt;
SpiRit-LM: &#20132;&#32455;&#30340;&#21475;&#35821;&#21644;&#20070;&#38754;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SpiRit-LM: Interleaved Spoken and Written Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05755
&lt;/p&gt;
&lt;p&gt;
SPIRIT-LM&#26159;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#21644;&#35821;&#38899;&#36830;&#32493;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#21475;&#35821;&#21644;&#20070;&#38754;&#35821;&#35328;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#23427;&#23637;&#31034;&#20102;&#25991;&#26412;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#21644;&#35821;&#38899;&#27169;&#22411;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;SPIRIT-LM&#36824;&#33021;&#20197;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SPIRIT-LM&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#21644;&#35821;&#38899;&#33258;&#30001;&#28151;&#21512;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#22312;&#25991;&#26412;&#21644;&#35821;&#38899;&#21333;&#20803;&#19978;&#36827;&#34892;&#35757;&#32451;&#23558;&#20854;&#25193;&#23637;&#21040;&#35821;&#38899;&#27169;&#24577;&#12290;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#34987;&#36830;&#25509;&#20026;&#19968;&#32452;&#21333;&#35789;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#33258;&#21160;&#31579;&#36873;&#30340;&#35821;&#38899;-&#25991;&#26412;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#36827;&#34892;&#35789;&#32423;&#20132;&#32455;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;SPIRIT-LM&#26377;&#20004;&#20010;&#29256;&#26412;&#65306;&#19968;&#20010;&#26159;&#20351;&#29992;&#35821;&#38899;&#35821;&#20041;&#21333;&#20803;&#30340;BASE&#29256;&#26412;&#65292;&#21478;&#19968;&#20010;&#26159;&#22312;&#35821;&#20041;&#21333;&#20803;&#20043;&#22806;&#36824;&#20351;&#29992;&#20102;&#38899;&#39640;&#21644;&#39118;&#26684;&#21333;&#20803;&#26469;&#27169;&#25311;&#34920;&#29616;&#21147;&#30340;EXPRESSIVE&#29256;&#26412;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#29256;&#26412;&#65292;&#25991;&#26412;&#26159;&#29992;&#23376;&#35789;BPE&#26631;&#35760;&#32534;&#30721;&#30340;&#12290;&#32467;&#26524;&#27169;&#22411;&#23637;&#31034;&#20102;&#25991;&#26412;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#21644;&#35821;&#38899;&#27169;&#22411;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;SPIRIT-LM&#33021;&#22815;&#22312;&#36328;&#27169;&#24577;&#65288;&#21363;ASR&#12289;TTS&#12289;&#35821;&#38899;&#20998;&#31867;&#65289;&#20013;&#20197;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SPIRIT-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single set of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus. SPIRIT-LM comes in two versions: a BASE version that uses speech semantic units and an EXPRESSIVE version that models expressivity using pitch and style units in addition to the semantic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SPIRIT-LM is able to learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;LLMs&#20869;&#37096;&#29366;&#24577;&#20013;&#20445;&#30041;&#23494;&#38598;&#35821;&#20041;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;EigenScore&#24230;&#37327;&#26041;&#27861;&#35780;&#20272;&#22238;&#31572;&#30340;&#33258;&#27965;&#24615;&#65292;&#24182;&#25506;&#32034;&#27979;&#35797;&#26102;&#38388;&#30340;&#29305;&#24449;&#21098;&#20999;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#36807;&#24230;&#33258;&#20449;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.03744</link><description>&lt;p&gt;
INSIDE: LLMs&#30340;&#20869;&#37096;&#29366;&#24577;&#20445;&#30041;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;LLMs&#20869;&#37096;&#29366;&#24577;&#20013;&#20445;&#30041;&#23494;&#38598;&#35821;&#20041;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;EigenScore&#24230;&#37327;&#26041;&#27861;&#35780;&#20272;&#22238;&#31572;&#30340;&#33258;&#27965;&#24615;&#65292;&#24182;&#25506;&#32034;&#27979;&#35797;&#26102;&#38388;&#30340;&#29305;&#24449;&#21098;&#20999;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#36807;&#24230;&#33258;&#20449;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24187;&#35273;&#23545;&#37096;&#32626;&#30340;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#24187;&#35273;&#30340;&#26816;&#27979;&#19978;&#65292;&#37319;&#29992;&#20102;&#36923;&#36753;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25110;&#35821;&#35328;&#32423;&#21035;&#30340;&#33258;&#27965;&#24615;&#35780;&#20272;&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#20002;&#22833;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#25506;&#32034;LLMs&#20869;&#37096;&#29366;&#24577;&#20013;&#20445;&#30041;&#30340;&#23494;&#38598;&#35821;&#20041;&#20449;&#24687;&#29992;&#20110;&#24187;&#35273;&#26816;&#27979;&#65288;INSIDE&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;EigenScore&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#22238;&#31572;&#30340;&#33258;&#27965;&#24615;&#65292;&#23427;&#21033;&#29992;&#22238;&#31572;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#26469;&#34913;&#37327;&#23494;&#38598;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;/&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#20174;&#33258;&#27965;&#30340;&#24187;&#35273;&#26816;&#27979;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#30340;&#29305;&#24449;&#21098;&#20999;&#26041;&#27861;&#65292;&#29992;&#20110;&#25130;&#26029;&#20869;&#37096;&#29366;&#24577;&#20013;&#30340;&#26497;&#31471;&#28608;&#27963;&#65292;&#20197;&#20943;&#23569;&#36807;&#24230;&#33258;&#20449;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' \textbf{IN}ternal \textbf{S}tates for halluc\textbf{I}nation \textbf{DE}tection (\textbf{INSIDE}). In particular, a simple yet effective \textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25512;&#29702;&#26463;&#25628;&#32034;&#65288;DBS&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#38142;&#24335;&#24605;&#32500;&#21644;&#28436;&#32462;&#25512;&#29702;&#19982;&#36880;&#27493;&#26463;&#25628;&#32034;&#26080;&#32541;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#39564;&#35777;&#22120;&#26469;&#20943;&#23569;&#38169;&#35823;&#30340;&#32047;&#31215;&#65292;&#24182;&#36890;&#36807;&#21487;&#25193;&#23637;&#21644;&#26080;&#38656;&#20154;&#24037;&#21171;&#21160;&#30340;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#25552;&#21319;&#27169;&#22411;&#30340;&#39564;&#35777;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17686</link><description>&lt;p&gt;
&#25512;&#29702;&#26463;&#25628;&#32034;&#65306;&#20026;&#38142;&#24335;&#24605;&#32500;&#25512;&#26029;&#23547;&#25214;&#21487;&#25512;&#23548;&#30340;&#29702;&#30001;
&lt;/p&gt;
&lt;p&gt;
Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25512;&#29702;&#26463;&#25628;&#32034;&#65288;DBS&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#38142;&#24335;&#24605;&#32500;&#21644;&#28436;&#32462;&#25512;&#29702;&#19982;&#36880;&#27493;&#26463;&#25628;&#32034;&#26080;&#32541;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#39564;&#35777;&#22120;&#26469;&#20943;&#23569;&#38169;&#35823;&#30340;&#32047;&#31215;&#65292;&#24182;&#36890;&#36807;&#21487;&#25193;&#23637;&#21644;&#26080;&#38656;&#20154;&#24037;&#21171;&#21160;&#30340;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#25552;&#21319;&#27169;&#22411;&#30340;&#39564;&#35777;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#26410;&#33021;&#35299;&#20915;&#20013;&#38388;&#27493;&#39588;&#30340;&#25512;&#29702;&#38169;&#35823;&#38382;&#39064;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#32047;&#31215;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25512;&#29702;&#26463;&#25628;&#32034;&#65288;DBS&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#38142;&#24335;&#24605;&#32500;&#21644;&#28436;&#32462;&#25512;&#29702;&#19982;&#36880;&#27493;&#26463;&#25628;&#32034;&#26080;&#32541;&#38598;&#25104;&#21040;LLMs&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37096;&#32626;&#20102;&#19968;&#20010;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#39564;&#35777;&#25512;&#29702;&#27493;&#39588;&#21450;&#20854;&#21069;&#25552;&#30340;&#21487;&#25512;&#23548;&#24615;&#65292;&#20174;&#32780;&#20943;&#23569;&#38169;&#35823;&#30340;&#32047;&#31215;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#26080;&#38656;&#20154;&#24037;&#21171;&#21160;&#30340;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#65292;&#26469;&#22686;&#24378;&#25105;&#20204;&#27169;&#22411;&#30340;&#39564;&#35777;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;&#35268;&#27169;&#30340;LLMs&#65288;7B&#12289;13B&#12289;70B&#21644;ChatGPT&#65289;&#30340;&#22522;&#30784;&#24615;&#33021;&#65292;&#22312;3&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#22330;&#26223;&#65288;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#65289;&#30340;8&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors.In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#33258;&#30456;&#30683;&#30462;&#30340;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#28041;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#29702;&#35299;&#25110;&#24120;&#35782;&#30340;&#20219;&#21153;&#20013;&#32463;&#24120;&#23384;&#22312;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;&#39640;&#20934;&#30830;&#24615;&#24182;&#19981;&#24635;&#26159;&#23545;&#24212;&#36739;&#20302;&#30340;&#33258;&#30456;&#30683;&#30462;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.09603</link><description>&lt;p&gt;
&#33258;&#30456;&#30683;&#30462;&#25512;&#29702;&#35780;&#20272;&#19982;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Contradictory Reasoning Evaluation and Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09603
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#33258;&#30456;&#30683;&#30462;&#30340;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#28041;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#29702;&#35299;&#25110;&#24120;&#35782;&#30340;&#20219;&#21153;&#20013;&#32463;&#24120;&#23384;&#22312;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;&#39640;&#20934;&#30830;&#24615;&#24182;&#19981;&#24635;&#26159;&#23545;&#24212;&#36739;&#20302;&#30340;&#33258;&#30456;&#30683;&#30462;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#22823;&#37327;&#24037;&#20316;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#35768;&#22810;&#25552;&#20986;&#30340;&#19979;&#28216;&#25512;&#29702;&#20219;&#21153;&#20027;&#35201;&#20851;&#27880;&#24615;&#33021;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;1&#65289;&#25512;&#29702;&#36136;&#37327;&#26377;&#22810;&#21487;&#38752;&#65292;2&#65289;&#27169;&#22411;&#33021;&#21542;&#26816;&#27979;&#21040;&#19981;&#21487;&#38752;&#30340;&#25512;&#29702;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30456;&#30683;&#30462;&#65288;Self-Contra&#65289;&#25512;&#29702;&#65292;&#21363;&#27169;&#22411;&#25512;&#29702;&#19981;&#25903;&#25345;&#39044;&#27979;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;Self-Contra&#29575;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#33258;&#30456;&#30683;&#30462;&#25512;&#29702;&#30340;&#26356;&#32454;&#31890;&#24230;&#31867;&#21035;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#28041;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#29702;&#35299;&#25110;&#24120;&#35782;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#32463;&#24120;&#33258;&#30456;&#30683;&#30462;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#24182;&#19981;&#19968;&#23450;&#23545;&#24212;&#26356;&#20302;&#30340;&#33258;&#30456;&#30683;&#30462;&#29575;&#12290;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#27491;&#30830;&#31572;&#26696;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#37319;&#21462;&#25463;&#24452;&#25110;&#24573;&#30053;&#19978;&#19979;&#25991;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09603v2 Announce Type: replace  Abstract: In a plethora of recent work, large language models (LLMs) demonstrated impressive reasoning ability, but many proposed downstream reasoning tasks focus on performance-wise evaluation. Two fundamental questions persist: 1) how reliable is the quality of reasoning, and 2) can models detect unreliable reasoning? In this paper, we investigate self-contradictory (Self-Contra) reasoning, where the model reasoning does not support predictions. To address 1), we assess the Self-Contra rate across four datasets and delve into finer-grained categories of Self-Contra reasoning. We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information understanding or commonsense. Importantly, a higher accuracy does not necessarily correspond to a lower Self-Contra rate. The model may appear to generate correct answers but it may take shortcuts in reasoning or skip over contextual evidence, thereby displa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35782;&#21035;&#26377;&#27602;&#12289;&#20882;&#29359;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;&#20869;&#23481;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#25913;&#36827;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#20102;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#24037;&#20316;&#20013;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2311.07879</link><description>&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#24182;&#19981;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#65306;&#24357;&#21512;&#25903;&#25345;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35782;&#21035;&#26377;&#27602;&#12289;&#20882;&#29359;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;&#20869;&#23481;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#25913;&#36827;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#20102;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#24037;&#20316;&#20013;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35782;&#21035;&#26377;&#27602;&#12289;&#20882;&#29359;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;&#20869;&#23481;&#26041;&#38754;&#21462;&#24471;&#20102;&#38271;&#36275;&#30340;&#36827;&#23637;&#65292;&#26088;&#22312;&#20943;&#36731;&#31649;&#29702;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#20219;&#21153;&#30340;&#25913;&#36827;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#20102;&#31649;&#29702;&#21592;&#22312;&#24037;&#20316;&#20013;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#36807;&#21435;&#30740;&#31350;&#21162;&#21147;&#33268;&#21147;&#20110;&#20026;&#20869;&#23481;&#31649;&#29702;&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#33258;&#21160;&#21270;&#25903;&#25345;&#19982;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#30340;&#38656;&#27714;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#23588;&#20854;&#26159;&#22312;&#35782;&#21035;&#36829;&#21453;&#21508;&#31181;&#31649;&#29702;&#35268;&#21017;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;Hugging Face&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#25581;&#31034;&#28085;&#30422;&#19977;&#20010;&#31034;&#33539;&#35770;&#22363;&#30340;&#21508;&#31181;&#31649;&#29702;&#35268;&#21017;&#21644;&#25351;&#21335;&#30340;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#26631;&#35760;&#26576;&#20010;&#29305;&#23450;&#35770;&#22363;&#30340;&#24179;&#21488;&#35268;&#21017;&#36829;&#35268;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29992;&#25143;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07879v2 Announce Type: replace-cross  Abstract: Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators' needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey stud
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#25972;&#29702;&#27969;&#31243;&#26469;&#26500;&#24314;&#25991;&#21270;&#30456;&#20851;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;GPT-4&#26080;&#21442;&#32771;&#35780;&#20272;&#32763;&#35793;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.14328</link><description>&lt;p&gt;
&#22312;&#25991;&#21270;&#24847;&#35782;&#19978;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking LLM-based Machine Translation on Cultural Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14328
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#25972;&#29702;&#27969;&#31243;&#26469;&#26500;&#24314;&#25991;&#21270;&#30456;&#20851;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;GPT-4&#26080;&#21442;&#32771;&#35780;&#20272;&#32763;&#35793;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#25991;&#21270;&#29305;&#23450;&#20869;&#23481;&#23545;&#20110;&#26377;&#25928;&#30340;&#36328;&#25991;&#21270;&#27807;&#36890;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20173;&#28982;&#38590;&#20197;&#20934;&#30830;&#29702;&#35299;&#21644;&#32763;&#35793;&#21253;&#21547;&#25991;&#21270;&#29305;&#23450;&#23454;&#20307;&#30340;&#21477;&#23376;&#12290;&#26368;&#36817;&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36827;&#23637;&#21033;&#29992;&#36731;&#37327;&#32423;&#25552;&#31034;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#25552;&#39640;&#20855;&#26377;&#25991;&#21270;&#24847;&#35782;&#30340;&#26426;&#22120;&#32763;&#35793;&#30340;&#26377;&#25928;&#24615;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#25972;&#29702;&#27969;&#31243;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#25991;&#21270;&#30456;&#20851;&#24182;&#20016;&#23500;&#20102;&#25991;&#21270;&#29305;&#23450;&#39033;&#30446;&#27880;&#37322;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;GPT-4&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#32763;&#35793;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#21508;&#31181;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#21644;LLM-based MT&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14328v2 Announce Type: replace  Abstract: Translating cultural-specific content is crucial for effective cross-cultural communication. However, many MT systems still struggle to translate sentences containing cultural-specific entities accurately and understandably. Recent advancements in in-context learning utilize lightweight prompts to guide large language models (LLMs) in machine translation tasks. Nevertheless, the effectiveness of this approach in enhancing machine translation with cultural awareness remains uncertain. To address this gap, we introduce a new data curation pipeline to construct a culturally relevant parallel corpus, enriched with annotations of cultural-specific items. Furthermore, we devise a novel evaluation metric to assess the understandability of translations in a reference-free manner by GPT-4. We evaluate a variety of neural machine translation (NMT) and LLM-based MT systems using our dataset. Additionally, we propose several prompting strategies
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#23545;&#38382;&#39064;&#27714;&#35299;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31616;&#27905;&#24615;&#19981;&#20165;&#38477;&#20302;&#20102;&#22238;&#31572;&#38271;&#24230;&#65292;&#19988;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#12290;&#28982;&#32780;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.05618</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#27714;&#35299;&#20013;&#65292;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models. (arXiv:2401.05618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#23545;&#38382;&#39064;&#27714;&#35299;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31616;&#27905;&#24615;&#19981;&#20165;&#38477;&#20302;&#20102;&#22238;&#31572;&#38271;&#24230;&#65292;&#19988;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#12290;&#28982;&#32780;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;(CCoT)&#25552;&#31034;&#12290;&#25105;&#20204;&#23558;&#26631;&#20934;&#30340;CoT&#21644;CCoT&#25552;&#31034;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#20102;&#35299;&#31616;&#27905;&#24615;&#23545;&#22238;&#31572;&#38271;&#24230;&#21644;&#27491;&#30830;&#31572;&#26696;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;(MCQA)&#22522;&#20934;&#30340;&#35780;&#20272;&#12290;CCoT&#23558;GPT-3.5&#21644;GPT-4&#30340;&#24179;&#22343;&#22238;&#31572;&#38271;&#24230;&#20998;&#21035;&#20943;&#23569;&#20102;48.70&#65285;&#65292;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#65292;&#24102;&#26377;CCoT&#30340;GPT-3.5&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;27.69&#65285;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;CCoT&#23548;&#33268;&#27599;&#20010;&#26631;&#35760;&#30340;&#25104;&#26412;&#24179;&#22343;&#38477;&#20302;&#20102;22.67&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20351;&#29992;CoT&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#26469;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;LLM&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#20026;&#30740;&#31350;LLM&#20013;&#36880;&#27493;&#25512;&#29702;&#30340;&#24418;&#25104;&#34892;&#20026;&#30340;AI&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#33021;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36328;&#35821;&#35328;&#35299;&#37322;&#22256;&#38590;&#35789;&#30340;&#26032;&#26041;&#27861;&#26469;&#23545;&#40784;&#32763;&#35793;&#29305;&#23450;&#29702;&#35299;&#21644;&#19968;&#33324;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.05072</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32763;&#35793;&#29305;&#23450;&#29702;&#35299;&#19982;&#19968;&#33324;&#29702;&#35299;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Translation-Specific Understanding to General Understanding in Large Language Models. (arXiv:2401.05072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05072
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#33021;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36328;&#35821;&#35328;&#35299;&#37322;&#22256;&#38590;&#35789;&#30340;&#26032;&#26041;&#27861;&#26469;&#23545;&#40784;&#32763;&#35793;&#29305;&#23450;&#29702;&#35299;&#21644;&#19968;&#33324;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#22312;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#23578;&#26410;&#21462;&#24471;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#36896;&#25104;&#24615;&#33021;&#26377;&#38480;&#30340;&#19968;&#20010;&#28508;&#22312;&#21407;&#22240;&#26159;LLMs&#20013;&#32763;&#35793;&#29305;&#23450;&#29702;&#35299;&#19982;&#19968;&#33324;&#29702;&#35299;&#30340;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#23558;&#32763;&#35793;&#29305;&#23450;&#29702;&#35299;&#19982;&#19968;&#33324;&#29702;&#35299;&#23545;&#40784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32763;&#35793;&#36807;&#31243;xIoD&#65288;&#36328;&#35821;&#35328;&#35299;&#37322;&#22256;&#38590;&#35789;&#65289;&#65292;&#26126;&#30830;&#22320;&#34701;&#20837;&#19968;&#33324;&#29702;&#35299;&#23545;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#29702;&#35299;&#20197;&#25351;&#23548;&#32763;&#35793;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;xIoD&#23545;&#38590;&#20197;&#32763;&#35793;&#30340;&#21333;&#35789;&#36827;&#34892;&#36328;&#35821;&#35328;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#30340;&#35299;&#37322;&#22686;&#24378;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37325;&#26032;&#26500;&#24314;&#20102;&#22806;&#37096;&#24037;&#20855;QE&#65292;&#20197;&#35299;&#20915;xIoD&#22312;&#26816;&#27979;&#22256;&#38590;&#35789;&#21644;&#29983;&#25104;&#26377;&#24110;&#21161;&#30340;&#35299;&#37322;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) have shown surprising language understanding and generation capabilities, they have yet to gain a revolutionary advancement in the field of machine translation. One potential cause of the limited performance is the misalignment between the translation-specific understanding and general understanding inside LLMs. To align the translation-specific understanding to the general one, we propose a novel translation process xIoD (Cross-Lingual Interpretation of Difficult words), explicitly incorporating the general understanding on the content incurring inconsistent understanding to guide the translation. Specifically, xIoD performs the cross-lingual interpretation for the difficult-to-translate words and enhances the translation with the generated interpretations. Furthermore, we reframe the external tools of QE to tackle the challenges of xIoD in the detection of difficult words and the generation of helpful interpretations. We conduct experiments on th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36741;&#21161;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#25991;&#31038;&#31185;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;16&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#20998;&#26512;&#12289;&#25991;&#26412;&#25366;&#25496;&#12289;&#31038;&#20132;&#32593;&#32476;&#25512;&#26029;&#31561;&#12290;</title><link>http://arxiv.org/abs/2309.14379</link><description>&lt;p&gt;
&#26426;&#22120;&#36741;&#21161;&#30340;&#28151;&#21512;&#26041;&#27861;&#65306;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#20154;&#25991;&#31038;&#31185;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence. (arXiv:2309.14379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36741;&#21161;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#25991;&#31038;&#31185;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;16&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#20998;&#26512;&#12289;&#25991;&#26412;&#25366;&#25496;&#12289;&#31038;&#20132;&#32593;&#32476;&#25512;&#26029;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19981;&#26029;&#36827;&#21270;&#20026;&#20154;&#25991;&#31038;&#31185;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#33021;&#22815;&#22312;&#20197;&#21069;&#36890;&#24120;&#30001;&#20154;&#21147;&#23436;&#25104;&#30340;&#23450;&#24615;&#20998;&#26512;&#20219;&#21153;&#20013;&#23454;&#29616;&#35268;&#27169;&#21270;&#12289;&#33258;&#21160;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#20197;&#21033;&#29992;&#23450;&#24615;&#20998;&#26512;&#19987;&#19994;&#30693;&#35782;&#12289;&#26426;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20005;&#35880;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#27880;&#37325;&#36879;&#26126;&#24230;&#21644;&#21487;&#22797;&#21046;&#24615;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;16&#20010;&#26426;&#22120;&#36741;&#21161;&#30340;&#26696;&#20363;&#30740;&#31350;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#12290;&#20219;&#21153;&#21253;&#25324;&#35821;&#35328;&#21644;&#35805;&#35821;&#20998;&#26512;&#12289;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#12289;&#37319;&#35775;&#20998;&#26512;&#12289;&#21382;&#21490;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#21644;&#25991;&#26412;&#25366;&#25496;&#12289;&#25919;&#27835;&#31435;&#22330;&#26816;&#27979;&#12289;&#25991;&#26412;&#21644;&#24605;&#24819;&#37325;&#22797;&#20351;&#29992;&#12289;&#25991;&#23398;&#21644;&#30005;&#24433;&#20013;&#30340;&#25991;&#31867;&#26500;&#25104;&#12289;&#31038;&#20132;&#32593;&#32476;&#25512;&#26029;&#12289;&#33258;&#21160;&#35789;&#20856;&#32534;&#32386;&#12289;&#20803;&#25968;&#25454;&#34917;&#20805;&#21644;&#22810;&#27169;&#24577;&#35270;&#35273;&#25991;&#21270;&#20998;&#26512;&#12290;&#19982;&#29616;&#26377;LLM&#24212;&#29992;&#25991;&#29486;&#20013;&#23545;&#33521;&#25991;&#30340;&#20851;&#27880;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#28085;&#30422;&#22810;&#31181;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing capacities of large language models (LLMs) present an unprecedented opportunity to scale up data analytics in the humanities and social sciences, augmenting and automating qualitative analytic tasks previously typically allocated to human labor. This contribution proposes a systematic mixed methods framework to harness qualitative analytic expertise, machine scalability, and rigorous quantification, with attention to transparency and replicability. 16 machine-assisted case studies are showcased as proof of concept. Tasks include linguistic and discourse analysis, lexical semantic change detection, interview analysis, historical event cause inference and text mining, detection of political stance, text and idea reuse, genre composition in literature and film; social network inference, automated lexicography, missing metadata augmentation, and multimodal visual cultural analytics. In contrast to the focus on English in the emerging LLM applicability literature, many exampl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;PARADOX&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20110;&#30495;&#23454;&#20010;&#20307;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#25991;&#26412;&#12290;&#35813;&#27169;&#22411;&#20197;&#29992;&#25143;&#30340;&#20154;&#29289;&#24418;&#35937;&#20026;&#26465;&#20214;&#26469;&#32534;&#30721;&#23545;&#35805;&#65292;&#24182;&#29983;&#25104;&#19981;&#24102;&#21333;&#35821;&#21442;&#32771;&#25968;&#25454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#27169;&#22411;&#36824;&#36827;&#34892;&#23545;&#40784;&#65292;&#20351;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#25509;&#36817;&#30495;&#23454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#20041;&#19978;&#26356;&#26377;&#24847;&#20041;&#65292;&#22312;&#35821;&#35328;&#19978;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.02915</link><description>&lt;p&gt;
&#12298;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#12299;
&lt;/p&gt;
&lt;p&gt;
Persona-aware Generative Model for Code-mixed Language. (arXiv:2309.02915v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;PARADOX&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20110;&#30495;&#23454;&#20010;&#20307;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#25991;&#26412;&#12290;&#35813;&#27169;&#22411;&#20197;&#29992;&#25143;&#30340;&#20154;&#29289;&#24418;&#35937;&#20026;&#26465;&#20214;&#26469;&#32534;&#30721;&#23545;&#35805;&#65292;&#24182;&#29983;&#25104;&#19981;&#24102;&#21333;&#35821;&#21442;&#32771;&#25968;&#25454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#27169;&#22411;&#36824;&#36827;&#34892;&#23545;&#40784;&#65292;&#20351;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#25509;&#36817;&#30495;&#23454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#20041;&#19978;&#26356;&#26377;&#24847;&#20041;&#65292;&#22312;&#35821;&#35328;&#19978;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#21644;&#22810;&#35821;&#35328;&#31038;&#20250;&#20013;&#65292;&#20195;&#30721;&#28151;&#21512;&#21644;&#33050;&#26412;&#28151;&#21512;&#38750;&#24120;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#23545;&#20110;&#20195;&#30721;&#28151;&#21512;&#30340;&#20559;&#22909;&#21462;&#20915;&#20110;&#29992;&#25143;&#30340;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#12289;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#24403;&#22320;&#29615;&#22659;&#65292;&#32780;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#26102;&#22823;&#22810;&#24573;&#35270;&#20102;&#36825;&#20123;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#24320;&#21457;&#19968;&#31181;&#20154;&#29289;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#31867;&#20284;&#20110;&#30495;&#23454;&#20010;&#20307;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#28151;&#21512;&#29983;&#25104;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#65288;PARADOX&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#32473;&#23450;&#29992;&#25143;&#30340;&#20154;&#29289;&#24418;&#35937;&#30340;&#26465;&#20214;&#19979;&#23545;&#35805;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#29983;&#25104;&#19981;&#24102;&#21333;&#35821;&#21442;&#32771;&#25968;&#25454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#40784;&#27169;&#22359;&#65292;&#23545;&#29983;&#25104;&#30340;&#24207;&#21015;&#36827;&#34892;&#37325;&#26032;&#26657;&#20934;&#65292;&#20351;&#20854;&#26356;&#25509;&#36817;&#30495;&#23454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;PARADOX&#29983;&#25104;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#22312;&#35821;&#20041;&#19978;&#26356;&#26377;&#24847;&#20041;&#65292;&#22312;&#35821;&#35328;&#19978;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-mixing and script-mixing are prevalent across online social networks and multilingual societies. However, a user's preference toward code-mixing depends on the socioeconomic status, demographics of the user, and the local context, which existing generative models mostly ignore while generating code-mixed texts. In this work, we make a pioneering attempt to develop a persona-aware generative model to generate texts resembling real-life code-mixed texts of individuals. We propose a Persona-aware Generative Model for Code-mixed Generation, PARADOX, a novel Transformer-based encoder-decoder model that encodes an utterance conditioned on a user's persona and generates code-mixed texts without monolingual reference data. We propose an alignment module that re-calibrates the generated sequence to resemble real-life code-mixed texts. PARADOX generates code-mixed texts that are semantically more meaningful and linguistically more valid. To evaluate the personification capabilities of PARAD
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01472</link><description>&lt;p&gt;
&#21453;&#21521;&#31283;&#23450;&#25193;&#25955;&#65306;&#29983;&#25104;&#35813;&#22270;&#20687;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65292;&#26368;&#36817;&#21560;&#24341;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#29983;&#25104;&#36807;&#31243;&#21644;&#22914;&#20309;&#35774;&#35745;&#25552;&#31034;&#20197;&#33719;&#24471;&#25152;&#38656;&#22270;&#20687;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#31995;&#21015;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65288;&#26377;&#21644;&#26080;&#23545;&#25193;&#25955;&#32593;&#32476;&#26435;&#37325;&#36827;&#34892;&#35775;&#38382;&#65289;&#26469;&#22788;&#29702;&#25152;&#25552;&#20986;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#32852;&#21512;&#25552;&#31034;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#35789;&#27719;&#20998;&#31867;&#30446;&#26631;&#65292;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#20855;&#26377;&#26356;&#20302;&#26631;&#27880;&#22122;&#22768;&#65288;&#21363;&#26356;&#22909;&#23545;&#40784;&#65289;&#30340;&#22270;&#20687;&#25552;&#31034;&#23545;&#30340;&#23398;&#20064;&#65292;&#24182;&#19988;&#20351;&#29992;&#30456;&#20284;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models such as Stable Diffusion have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we introduce the new task of predicting the text prompt given an image generated by a generative diffusion model. We combine a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising of a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise (i.e. that are better aligned), and an unsupervised domain-adaptive kernel learning method that uses the similarities b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05079</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25200;&#21160;&#28155;&#21152;&#21040;&#23433;&#20840;&#24615;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#30340;&#20195;&#30721;&#25551;&#36848;&#20013;&#30340;&#26041;&#27861;&#65292;&#21363;&#26469;&#33258;&#21892;&#24847;&#24320;&#21457;&#32773;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#65288;NL&#65289;&#65292;&#24182;&#20998;&#26512;&#20102;&#25200;&#21160;&#22914;&#20309;&#20197;&#21450;&#22312;&#20160;&#20040;&#31243;&#24230;&#19978;&#24433;&#21709;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NL&#25551;&#36848;&#20013;&#30340;&#25200;&#21160;&#39640;&#24230;&#24433;&#21709;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26041;&#27861;&#25191;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#21363;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a method to add perturbations to the code descriptions, i.e., new inputs in natural language (NL) from well-intentioned developers, in the context of security-oriented code, and analyze how and to what extent perturbations affect the performance of AI offensive code generators. Our experiments show that the performance of the code generators is highly affected by perturbations in the NL descriptions. To enhance the robustness of the code generators, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#38024;&#23545;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20171;&#32461;&#20102;&#28041;&#21450;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20197;&#21450;&#25551;&#36848;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#21644;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#35758;&#24615;&#30340;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2211.09172</link><description>&lt;p&gt;
&#25991;&#23383;&#23545;&#35805;&#20013;&#30340;&#28145;&#24230;&#24773;&#24863;&#35782;&#21035;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#38024;&#23545;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20171;&#32461;&#20102;&#28041;&#21450;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20197;&#21450;&#25551;&#36848;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#21644;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#35758;&#24615;&#30340;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#24180;&#26469;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#26032;&#30340;&#24212;&#29992;&#21644;&#23454;&#26045;&#22330;&#26223;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#21033;&#29992;&#23545;&#35805;&#35821;&#22659;&#12289;&#35828;&#35805;&#20154;&#21644;&#24773;&#24863;&#21160;&#24577;&#24314;&#27169;&#65292;&#35299;&#37322;&#24120;&#35782;&#34920;&#36798;&#12289;&#38750;&#27491;&#24335;&#35821;&#35328;&#21644;&#35773;&#21050;&#65292;&#24212;&#23545;&#23454;&#26102;&#24773;&#24863;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#35782;&#21035;&#24773;&#24863;&#21407;&#22240;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#31181;&#20998;&#31867;&#27861;&#65292;&#22810;&#35821;&#35328;&#24773;&#24863;&#35782;&#21035;&#20197;&#21450;&#35299;&#37322;&#24615;&#12290;&#26412;&#35843;&#30740;&#39318;&#20808;&#20171;&#32461;&#20102;&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#19982;&#27492;&#20219;&#21153;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#28982;&#21518;&#65292;&#23427;&#20171;&#32461;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#22810;&#31181;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#24773;&#24863;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#23427;&#25551;&#36848;&#20102;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#65292;&#24182;&#35299;&#37322;&#20102;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#26368;&#21518;&#65292;&#23427;&#25552;&#20379;&#20102;&#23545;&#20110;&#26356;&#22909;&#30340;&#26694;&#26550;&#30340;&#24314;&#35758;&#24615;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#22788;&#29702;&#20027;&#35266;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Emotion Recognition in Conversations (ERC) has seen a tremendous advancement in the last few years, new applications and implementation scenarios present novel challenges and opportunities. These range from leveraging the conversational context, speaker and emotion dynamics modelling, to interpreting common sense expressions, informal language and sarcasm, addressing challenges of real time ERC, recognizing emotion causes, different taxonomies across datasets, multilingual ERC to interpretability. This survey starts by introducing ERC, elaborating on the challenges and opportunities pertaining to this task. It proceeds with a description of the emotion taxonomies and a variety of ERC benchmark datasets employing such taxonomies. This is followed by descriptions of the most prominent works in ERC with explanations of the Deep Learning architectures employed. Then, it provides advisable ERC practices towards better frameworks, elaborating on methods to deal with subjectivity in ann
&lt;/p&gt;</description></item></channel></rss>