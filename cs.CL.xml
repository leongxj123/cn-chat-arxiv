<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;</title><link>https://rss.arxiv.org/abs/2312.05356</link><description>&lt;p&gt;
Neuron Patching: &#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#19982;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.05356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#12290;&#26356;&#26032;&#36825;&#20123;&#27169;&#22411;&#30340;&#26032;&#30693;&#35782;&#38750;&#24120;&#26114;&#36149;&#65292;&#36890;&#24120;&#38656;&#35201;&#20840;&#38754;&#23454;&#29616;&#20854;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;MENT&#65292;&#29992;&#20110;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#12290;&#22522;&#20110;&#29983;&#25104;&#24335;LLM&#30340;&#26426;&#21046;&#65292;MENT&#21487;&#20197;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#20196;&#29260;&#26102;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#65292;&#24182;&#36827;&#19968;&#27493;&#25903;&#25345;&#24120;&#35265;&#30340;&#32534;&#30721;&#20219;&#21153;&#12290;MENT&#20855;&#26377;&#39640;&#25928;&#12289;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#29305;&#28857;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#20462;&#34917;1&#25110;2&#20010;&#31070;&#32463;&#20803;&#26469;&#32416;&#27491;&#31070;&#32463;&#27169;&#22411;&#12290;&#20316;&#20026;&#31070;&#32463;&#20803;&#23618;&#38754;&#19978;&#29983;&#25104;&#27169;&#22411;&#32534;&#36753;&#30340;&#20808;&#39537;&#24037;&#20316;&#65292;&#25105;&#20204;&#35268;&#33539;&#20102;&#32534;&#36753;&#36807;&#31243;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#34913;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#32534;&#30721;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;API&#24207;&#21015;&#25512;&#33616;&#12289;&#34892;&#32423;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are successfully adopted in software engineering, especially in code generation. Updating these models with new knowledge is very expensive, and is often required to fully realize their value. In this paper, we propose a novel and effective model editing approach, \textsc{MENT}, to patch LLMs in coding tasks. Based on the mechanism of generative LLMs, \textsc{MENT} enables model editing in next-token predictions, and further supports common coding tasks. \textsc{MENT} is effective, efficient, and reliable. It can correct a neural model by patching 1 or 2 neurons. As the pioneer work on neuron-level model editing of generative models, we formalize the editing process and introduce the involved concepts. Besides, we also introduce new measures to evaluate its generalization ability, and build a benchmark for further study. Our approach is evaluated on three coding tasks, including API-seq recommendation, line-level code generation, and pseudocode-to-code transaction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32508;&#36848;&#36229;&#36234;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#26356;&#28145;&#20837;&#20102;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;LLMs&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#38754;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01869</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;--&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32508;&#36848;&#36229;&#36234;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#26356;&#28145;&#20837;&#20102;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;LLMs&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#38754;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#28041;&#21450;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#28608;&#28872;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#28145;&#24230;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#37096;&#20998;&#28304;&#33258;&#23545;&#27169;&#22411;&#25512;&#29702;&#34892;&#20026;&#30340;&#28145;&#20837;&#35843;&#26597;&#32780;&#38750;&#20165;&#20165;&#36890;&#36807;&#34920;&#38754;&#20934;&#30830;&#24615;&#25351;&#26631;&#26469;&#34913;&#37327;&#20219;&#21153;&#34920;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#32508;&#36848;&#36229;&#36234;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35780;&#20272;LLMs&#25512;&#29702;&#34892;&#20026;&#30340;&#20027;&#35201;&#26041;&#27861;&#35770;&#65292;&#24378;&#35843;&#20102;&#24403;&#21069;&#23545;&#26356;&#32454;&#33268;&#25512;&#29702;&#20998;&#26512;&#30340;&#36235;&#21183;&#21644;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#34920;&#26126;&#65292;LLMs&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#38754;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01869v1 Announce Type: cross  Abstract: Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#32462;&#24335;&#30340;&#22270;&#35889;&#36777;&#35770;&#26041;&#27861;&#65288;BDoG&#65289;&#65292;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#38450;&#27490;&#24847;&#35265;&#38472;&#33104;&#21270;&#21644;&#20943;&#23569;&#30001;&#22270;&#20687;&#24341;&#20837;&#30340;&#20998;&#24515;&#27010;&#24565;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#31185;&#23398;&#38382;&#31572;&#21644;MMBench&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14972</link><description>&lt;p&gt;
&#19968;&#22270;&#32988;&#21315;&#35328;&#65306;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#22270;&#35889;&#36777;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#32462;&#24335;&#30340;&#22270;&#35889;&#36777;&#35770;&#26041;&#27861;&#65288;BDoG&#65289;&#65292;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#38450;&#27490;&#24847;&#35265;&#38472;&#33104;&#21270;&#21644;&#20943;&#23569;&#30001;&#22270;&#20687;&#24341;&#20837;&#30340;&#20998;&#24515;&#27010;&#24565;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#31185;&#23398;&#38382;&#31572;&#21644;MMBench&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26088;&#22312;&#23558;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#24341;&#20837;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#35797;&#28857;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#30001;&#20110;&#36807;&#24230;&#24635;&#32467;&#32780;&#23548;&#33268;&#24847;&#35265;&#38472;&#33104;&#21270;&#65292;&#20197;&#21450;&#30001;&#20110;&#22270;&#20687;&#24341;&#20837;&#36716;&#31227;&#24615;&#27010;&#24565;&#32780;&#23548;&#33268;&#27880;&#24847;&#21147;&#20998;&#25955;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#33258;&#29616;&#26377;&#36777;&#35770;&#26041;&#26696;&#30340;&#24402;&#32435;&#65288;&#33258;&#19979;&#32780;&#19978;&#65289;&#24615;&#36136;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#32462;&#65288;&#33258;&#19978;&#32780;&#19979;&#65289;&#30340;&#36777;&#35770;&#26041;&#27861;&#65292;&#31216;&#20026;&#22270;&#35889;&#36777;&#35770;&#65288;BDoG&#65289;&#12290;&#22312;BDoG&#20013;&#65292;&#36777;&#35770;&#20165;&#38480;&#20110;&#34013;&#22270;&#22270;&#20013;&#65292;&#20197;&#38450;&#27490;&#36890;&#36807;&#19990;&#30028;&#32423;&#25688;&#35201;&#32780;&#23548;&#33268;&#24847;&#35265;&#38472;&#33104;&#21270;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#22270;&#20013;&#30340;&#20998;&#25903;&#20013;&#23384;&#20648;&#35777;&#25454;&#65292;BDoG&#32531;&#35299;&#20102;&#39057;&#32321;&#20294;&#26080;&#20851;&#30340;&#27010;&#24565;&#24102;&#26469;&#30340;&#20998;&#25955;&#27880;&#24847;&#21147;&#29616;&#35937;&#12290;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;BDoG&#65292;&#22312;&#31185;&#23398;&#38382;&#31572;&#21644;MMBench&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#65292;&#24182;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14972v1 Announce Type: new  Abstract: This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivialization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate BDoG, achieving state-of-the-art results in Science QA and MMBench with significant improvements over previous methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#27969;&#30021;&#21477;&#23376;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#36807;&#28388;&#21644;&#23567;&#22411;&#27169;&#22411;&#35757;&#32451;&#23454;&#29616;&#20102;&#19981;&#27969;&#30021;&#26816;&#27979;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.08229</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#31687;&#29983;&#25104;&#22120;&#25552;&#21319;&#19981;&#27969;&#30021;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Boosting Disfluency Detection with Large Language Model as Disfluency Generator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#27969;&#30021;&#21477;&#23376;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#36807;&#28388;&#21644;&#23567;&#22411;&#27169;&#22411;&#35757;&#32451;&#23454;&#29616;&#20102;&#19981;&#27969;&#30021;&#26816;&#27979;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#19981;&#27969;&#30021;&#26816;&#27979;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#26114;&#36149;&#19988;&#31232;&#32570;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#19968;&#20123;&#26041;&#27861;&#37319;&#29992;&#21551;&#21457;&#24335;&#25110;&#32479;&#35745;&#29305;&#24449;&#26469;&#29983;&#25104;&#19981;&#27969;&#30021;&#21477;&#23376;&#65292;&#37096;&#20998;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21477;&#23376;&#24120;&#24120;&#20559;&#31163;&#30495;&#23454;&#22330;&#26223;&#65292;&#38480;&#21046;&#20102;&#25972;&#20307;&#27169;&#22411;&#25913;&#21892;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21331;&#36234;&#30340;&#29983;&#25104;&#21644;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#19981;&#27969;&#30021;&#21477;&#23376;&#20316;&#20026;&#22686;&#24378;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#29983;&#25104;&#22810;&#26679;&#19988;&#26356;&#30495;&#23454;&#30340;&#21477;&#23376;&#65292;&#36890;&#36807;&#20855;&#20307;&#25552;&#31034;&#36827;&#34892;&#24341;&#23548;&#65292;&#26080;&#38656;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36807;&#28388;&#26041;&#27861;&#26469;&#25552;&#39640;&#29983;&#25104;&#21477;&#23376;&#30340;&#36136;&#37327;&#65292;&#29992;&#20110;&#35757;&#32451;&#23567;&#22411;&#26816;&#27979;&#27169;&#22411;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08229v1 Announce Type: new  Abstract: Current disfluency detection methods heavily rely on costly and scarce human-annotated data. To tackle this issue, some approaches employ heuristic or statistical features to generate disfluent sentences, partially improving detection performance. However, these sentences often deviate from real-life scenarios, constraining overall model enhancement. In this study, we propose a lightweight data augmentation approach for disfluency detection, utilizing the superior generative and semantic understanding capabilities of large language model (LLM) to generate disfluent sentences as augmentation data. We leverage LLM to generate diverse and more realistic sentences guided by specific prompts, without the need for fine-tuning the LLM. Subsequently, we apply an uncertainty-aware data filtering approach to improve the quality of the generated sentences, utilized in training a small detection model for improved performance. Experiments using enha
&lt;/p&gt;</description></item><item><title>FakeNewsGPT4&#26159;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#29305;&#23450;&#20110;&#20266;&#36896;&#30340;&#30693;&#35782;&#65292;&#25552;&#21319;&#20102;LVLMs&#22312;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.01988</link><description>&lt;p&gt;
FakeNewsGPT4&#65306;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;LVLMs&#25512;&#36827;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
FakeNewsGPT4: Advancing Multimodal Fake News Detection through Knowledge-Augmented LVLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01988
&lt;/p&gt;
&lt;p&gt;
FakeNewsGPT4&#26159;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#29305;&#23450;&#20110;&#20266;&#36896;&#30340;&#30693;&#35782;&#65292;&#25552;&#21319;&#20102;LVLMs&#22312;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#23384;&#22312;&#23454;&#36136;&#24615;&#30340;&#20998;&#24067;&#24046;&#24322;&#65292;&#20419;&#20351;&#38656;&#35201;&#24191;&#20041;&#26816;&#27979;&#22120;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#30340;&#23396;&#31435;&#24615;&#38480;&#21046;&#20102;&#20256;&#32479;&#26816;&#27979;&#22120;&#33719;&#24471;&#24320;&#25918;&#19990;&#30028;&#20107;&#23454;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FakeNewsGPT4&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#28155;&#29305;&#23450;&#20110;&#20266;&#36896;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#36827;&#34892;&#25805;&#32437;&#25512;&#29702;&#65292;&#21516;&#26102;&#32487;&#25215;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#20316;&#20026;&#34917;&#20805;&#12290;FakeNewsGPT4&#20013;&#30340;&#30693;&#35782;&#22686;&#24378;&#28041;&#21450;&#33719;&#21462;&#20004;&#31181;&#20266;&#36896;&#29305;&#23450;&#30693;&#35782;&#65292;&#21363;&#35821;&#20041;&#30456;&#20851;&#21644;&#24037;&#20214;&#36861;&#36394;&#65292;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;LVLMs&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01988v1 Announce Type: new  Abstract: The massive generation of multimodal fake news exhibits substantial distribution discrepancies, prompting the need for generalized detectors. However, the insulated nature of training within specific domains restricts the capability of classical detectors to obtain open-world facts. In this paper, we propose FakeNewsGPT4, a novel framework that augments Large Vision-Language Models (LVLMs) with forgery-specific knowledge for manipulation reasoning while inheriting extensive world knowledge as complementary. Knowledge augmentation in FakeNewsGPT4 involves acquiring two types of forgery-specific knowledge, i.e., semantic correlation and artifact trace, and merging them into LVLMs. Specifically, we design a multi-level cross-modal reasoning module that establishes interactions across modalities for extracting semantic correlations. Concurrently, a dual-branch fine-grained verification module is presented to comprehend localized details to e
&lt;/p&gt;</description></item><item><title>GAOKAO-MM &#26159;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20154;&#31867;&#27700;&#24179;&#35201;&#27714;&#65292;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#30340;LVLMs&#30340;&#20934;&#30830;&#29575;&#26222;&#36941;&#19981;&#36275;50%&#12290;</title><link>https://arxiv.org/abs/2402.15745</link><description>&lt;p&gt;
GAOKAO-MM: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#30340;&#20013;&#22269;&#20154;&#31867;&#27700;&#24179;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15745
&lt;/p&gt;
&lt;p&gt;
GAOKAO-MM &#26159;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20154;&#31867;&#27700;&#24179;&#35201;&#27714;&#65292;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#30340;LVLMs&#30340;&#20934;&#30830;&#29575;&#26222;&#36941;&#19981;&#36275;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#24050;&#32463;&#22312;&#22270;&#20687;&#24863;&#30693;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26497;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#22522;&#26412;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#36825;&#20123;&#26080;&#27861;&#20805;&#20998;&#21453;&#26144;&#20986;LVLMs&#30340;&#20840;&#38754;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GAOKAO-MM&#65292;&#19968;&#20010;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#21253;&#25324;8&#20010;&#31185;&#30446;&#21644;12&#31181;&#31867;&#22411;&#30340;&#22270;&#29255;&#65292;&#22914;&#22270;&#34920;&#12289;&#20989;&#25968;&#22270;&#12289;&#22320;&#22270;&#21644;&#29031;&#29255;&#12290;GAOKAO-MM&#26469;&#28304;&#20110;&#20013;&#22269;&#26412;&#22303;&#32972;&#26223;&#65292;&#24182;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#35201;&#27714;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#29702;&#35299;&#12289;&#30693;&#35782;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;10&#20010;LVLMs&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#20934;&#30830;&#29575;&#37117;&#20302;&#20110;50%&#65292;&#20854;&#20013;GPT-4-Vision&#65288;48.1%&#65289;&#12289;Qwen-VL-Plus&#65288;41.2%&#65289;&#21644;Gemini-Pro-Vision&#65288;35.1%&#65289;&#20301;&#21015;&#21069;&#19977;&#21517;&#12290;&#25105;&#20204;&#30340;&#22810;&#32500;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;LVLMs&#20855;&#26377;&#36866;&#24230;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15745v1 Announce Type: cross  Abstract: The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs have moder
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23558;LLMs&#30340;&#33021;&#21147;&#21387;&#32553;&#21040; TAG &#23398;&#20064;&#30340;&#26412;&#22320;&#22270;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.12022</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#29992;&#20110;&#25991;&#26412;&#23646;&#24615;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Language Models for Text-Attributed Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23558;LLMs&#30340;&#33021;&#21147;&#21387;&#32553;&#21040; TAG &#23398;&#20064;&#30340;&#26412;&#22320;&#22270;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#26159;&#36830;&#25509;&#30340;&#25991;&#26412;&#25991;&#26723;&#22270;&#12290;&#22270;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;TAGs&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#26631;&#31614;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#36825;&#20123;&#26631;&#31614;&#24456;&#23569;&#25110;&#29978;&#33267;&#19981;&#21487;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;TAG&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#21487;&#20280;&#32553;&#24615;&#12289;&#25104;&#26412;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#23558;LLMs&#30340;&#33021;&#21147;&#20256;&#25480;&#32473;TAG&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#22270;&#27169;&#22411;&#65292;&#20174;&#32780;&#21327;&#21516;LLMs&#21644;&#22270;&#27169;&#22411;&#30340;&#20114;&#34917;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12022v1 Announce Type: new  Abstract: Text-Attributed Graphs (TAGs) are graphs of connected textual documents. Graph models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications. Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues. Therefore, in this work, we focus on synergizing LLMs and graph models with their complementary strengths by distilling the power of LLMs to a local graph model on TAG learning. To address the inherent gaps between LLMs (generative models for texts) and graph models (discriminative models for graphs), we propose first to let LLMs teach an interpreter with rich textual rationale and then let a student model mimic the interpreter's reasoning without LLMs' textual rationale. Extensive experiments validate the efficacy of our proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ColorSwap&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#21305;&#37197;&#29289;&#20307;&#21644;&#39068;&#33394;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#39068;&#33394;&#35789;&#37325;&#26032;&#25490;&#24207;&#20197;&#20462;&#25913;&#19981;&#21516;&#30340;&#23545;&#35937;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#27979;&#35797;&#27169;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#20173;&#19981;&#22815;&#31283;&#23450;&#65292;&#20294;&#36890;&#36807;&#26356;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#33021;&#20250;&#26377;&#25152;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.04492</link><description>&lt;p&gt;
ColorSwap: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#35780;&#20272;&#30340;&#39068;&#33394;&#21644;&#21333;&#35789;&#25490;&#24207;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ColorSwap&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#21305;&#37197;&#29289;&#20307;&#21644;&#39068;&#33394;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#39068;&#33394;&#35789;&#37325;&#26032;&#25490;&#24207;&#20197;&#20462;&#25913;&#19981;&#21516;&#30340;&#23545;&#35937;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#27979;&#35797;&#27169;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#20173;&#19981;&#22815;&#31283;&#23450;&#65292;&#20294;&#36890;&#36807;&#26356;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#33021;&#20250;&#26377;&#25152;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ColorSwap&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#25913;&#36827;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#21305;&#37197;&#29289;&#20307;&#21644;&#20854;&#39068;&#33394;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;2000&#20010;&#29420;&#29305;&#30340;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65292;&#20998;&#20026;1000&#20010;&#31034;&#20363;&#12290;&#27599;&#20010;&#31034;&#20363;&#21253;&#25324;&#19968;&#20010;&#26631;&#39064;-&#22270;&#20687;&#23545;&#65292;&#20197;&#21450;&#19968;&#20010;&#8220;&#39068;&#33394;&#20132;&#25442;&#8221;&#23545;&#12290;&#25105;&#20204;&#36981;&#24490;Winoground&#26041;&#26696;&#65306;&#31034;&#20363;&#20013;&#30340;&#20004;&#20010;&#26631;&#39064;&#20855;&#26377;&#30456;&#21516;&#30340;&#21333;&#35789;&#65292;&#20294;&#39068;&#33394;&#21333;&#35789;&#34987;&#37325;&#26032;&#25490;&#21015;&#20197;&#20462;&#25913;&#19981;&#21516;&#30340;&#23545;&#35937;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#26631;&#39064;&#21644;&#22270;&#20687;&#29983;&#25104;&#19982;&#20154;&#31867;&#30340;&#20132;&#20114;&#21019;&#36896;&#32780;&#25104;&#12290;&#25105;&#20204;&#35780;&#20272;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;ITM&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21457;&#29616;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#20173;&#28982;&#19981;&#22815;&#31283;&#20581;&#12290;GPT-4V&#21644;LLaVA&#22312;&#25105;&#20204;&#30340;&#20027;&#35201;VLM&#25351;&#26631;&#19978;&#24471;&#20998;&#20998;&#21035;&#20026;72%&#21644;42%&#65292;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#36890;&#36807;&#26356;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#25552;&#21319;&#12290;&#22312;&#20027;&#35201;&#30340;ITM&#25351;&#26631;&#19978;&#65292;&#20687;CLIP&#21644;SigLIP&#36825;&#26679;&#30340;&#23545;&#27604;&#27169;&#22411;&#25509;&#36817;&#20110;&#38543;&#26426;&#29468;&#27979;&#65288;&#20998;&#21035;&#20026;12%&#21644;30%&#65289;&#65292;&#23613;&#31649;&#38750;&#23545;&#27604;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of multimodal models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a ``color-swapped'' pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate image-text matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced prompting techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to chance (at 12% and 30%, respectively), although the non-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#23569;&#37327;&#26597;&#35810;&#23545;&#25552;&#21462;&#8220;&#23433;&#20840;&#27169;&#24335;&#8221;&#65292;&#25104;&#21151;&#35268;&#36991;&#30446;&#26631;&#27169;&#22411;&#30340;&#38450;&#24481;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36234;&#29425;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.06824</link><description>&lt;p&gt;
&#25171;&#24320;LLMs&#30340;&#28504;&#22810;&#25289;&#39764;&#30418;&#65306;&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06824
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#23569;&#37327;&#26597;&#35810;&#23545;&#25552;&#21462;&#8220;&#23433;&#20840;&#27169;&#24335;&#8221;&#65292;&#25104;&#21151;&#35268;&#36991;&#30446;&#26631;&#27169;&#22411;&#30340;&#38450;&#24481;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36234;&#29425;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#29425;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#35825;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#23545;&#24694;&#24847;&#26597;&#35810;&#20135;&#29983;&#26377;&#27602;&#21709;&#24212;&#65292;&#26469;&#25506;&#32034;LLMs&#23433;&#20840;&#24615;&#36793;&#30028;&#65292;&#36825;&#22312;LLMs&#31038;&#21306;&#20869;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#65288;Jailbreaking LLMs through Representation Engineering&#65292;JRE&#65289;&#30340;&#26032;&#39062;&#36234;&#29425;&#26041;&#27861;&#65292;&#20854;&#20165;&#38656;&#35201;&#23569;&#37327;&#26597;&#35810;&#23545;&#20197;&#25552;&#21462;&#21487;&#29992;&#20110;&#35268;&#36991;&#30446;&#26631;&#27169;&#22411;&#38450;&#24481;&#30340;&#8220;&#23433;&#20840;&#27169;&#24335;&#8221;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36234;&#29425;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06824v2 Announce Type: replace-cross  Abstract: Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community. While existing jailbreaking methods primarily rely on prompt engineering, altering inputs to evade LLM safety mechanisms, they suffer from low attack success rates and significant time overheads, rendering them inflexible. To overcome these limitations, we propose a novel jailbreaking approach, named Jailbreaking LLMs through Representation Engineering (JRE). Our method requires only a small number of query pairs to extract ``safety patterns'' that can be used to circumvent the target model's defenses, achieving unprecedented jailbreaking performance. Building upon these findings, we also introduce a novel defense framework inspired by JRE principles, which demonstrates notable effectiveness. Extensive experimentation conf
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#39044;&#27979;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#24182;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#35843;&#25972;&#65292;&#35813;&#26694;&#26550;&#38477;&#20302;&#20102;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#24182;&#23558;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;1.9&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.10660</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21333;&#35821;&#25991;&#26412;&#29983;&#25104;&#30340;&#21152;&#36895;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simple Framework to Accelerate Multilingual Language Model for Monolingual Text Generation. (arXiv:2401.10660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10660
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#39044;&#27979;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#24182;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#35843;&#25972;&#65292;&#35813;&#26694;&#26550;&#38477;&#20302;&#20102;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#24182;&#23558;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;1.9&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#19981;&#20165;&#22312;&#33521;&#35821;&#32780;&#19988;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#37117;&#20419;&#36827;&#20102;&#22797;&#26434;&#30340;&#35821;&#35328;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#35760;&#22120;&#65288;&#22914;Llama&#65289;&#22312;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#65292;&#20542;&#21521;&#20110;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#36807;&#20998;&#20998;&#21106;&#26631;&#35760;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38750;&#32599;&#39532;&#23383;&#27597;&#35821;&#35328;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#36825;&#20123;&#35821;&#35328;&#36890;&#24120;&#22312;&#23383;&#31526;&#25110;Unicode&#32423;&#21035;&#19978;&#34987;&#21010;&#20998;&#65292;&#23548;&#33268;&#25991;&#26412;&#29983;&#25104;&#36895;&#24230;&#36739;&#24930;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#36825;&#20123;&#35821;&#35328;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#39044;&#27979;&#27604;&#20256;&#32479;&#30340;&#22810;&#35821;&#35328;&#26631;&#35760;&#22120;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#65292;&#24182;&#19988;&#19987;&#38376;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35299;&#30721;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#35299;&#30721;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;1.9&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models have facilitated the execution of complex language tasks, not only in English but also in non-English languages. However, the tokenizers of most language models, such as Llama, trained on English-centric corpora, tend to excessively fragment tokens in non-English languages. This issue is especially pronounced in non-roman alphabetic languages, which are often divided at a character or even Unicode level, leading to slower text generation. To address this, our study introduces a novel framework designed to expedite text generation in these languages. This framework predicts larger linguistic units than those of conventional multilingual tokenizers and is specifically tailored to the target language, thereby reducing the number of decoding steps required. Our empirical results demonstrate that the proposed framework increases the generation speed by a factor of 1.9 compared to standard decoding while maintaining the performance of a pre-traine
&lt;/p&gt;</description></item><item><title>ARIES&#26159;&#19968;&#20221;&#21253;&#21547;&#31185;&#23398;&#35770;&#25991;&#20462;&#35746;&#30340;&#35821;&#26009;&#24211;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#65292;&#21457;&#29616;&#20854;&#22312;&#23547;&#25214;&#23545;&#24212;&#30340;&#20462;&#35746;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#20462;&#35746;&#26102;&#36807;&#20998;&#36981;&#24490;&#21453;&#39304;&#30340;&#25514;&#36766;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#25972;&#20307;&#30340;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.12587</link><description>&lt;p&gt;
ARIES: &#19968;&#20221;&#21253;&#21547;&#31185;&#23398;&#35770;&#25991;&#20462;&#35746;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#20123;&#20462;&#35746;&#26159;&#20316;&#20026;&#23545;&#21516;&#34892;&#35780;&#23457;&#30340;&#22238;&#24212;&#32780;&#36827;&#34892;&#30340;
&lt;/p&gt;
&lt;p&gt;
ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews. (arXiv:2306.12587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12587
&lt;/p&gt;
&lt;p&gt;
ARIES&#26159;&#19968;&#20221;&#21253;&#21547;&#31185;&#23398;&#35770;&#25991;&#20462;&#35746;&#30340;&#35821;&#26009;&#24211;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#65292;&#21457;&#29616;&#20854;&#22312;&#23547;&#25214;&#23545;&#24212;&#30340;&#20462;&#35746;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#20462;&#35746;&#26102;&#36807;&#20998;&#36981;&#24490;&#21453;&#39304;&#30340;&#25514;&#36766;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#25972;&#20307;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#21516;&#34892;&#21453;&#39304;&#20462;&#25913;&#31185;&#23398;&#35770;&#25991;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#28145;&#21402;&#30340;&#31185;&#23398;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#35782;&#21035;&#39640;&#32423;&#21453;&#39304;&#20013;&#30340;&#38544;&#21547;&#24847;&#20041;&#65292;&#24182;&#22312;&#20247;&#22810;&#21487;&#33021;&#30340;&#26041;&#24335;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#26041;&#24335;&#26469;&#26356;&#26032;&#25163;&#31295;&#12290;&#25105;&#20204;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;ARIES&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35780;&#35770;&#21450;&#20854;&#30456;&#24212;&#30340;&#35770;&#25991;&#20462;&#35746;&#65292;&#20197;&#20415;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#30340;&#20004;&#20010;&#29256;&#26412;&#65306;&#35780;&#35770;-&#20462;&#35746;&#23545;&#40784;&#21644;&#20462;&#35746;&#29983;&#25104;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;&#21253;&#25324;GPT-4&#12290;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#35780;&#35770;&#20197;&#38388;&#25509;&#26041;&#24335;&#34920;&#36848;&#25110;&#20462;&#35746;&#28041;&#21450;&#35780;&#35770;&#30340;&#20027;&#26088;&#32780;&#38750;&#31934;&#30830;&#35201;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#30830;&#23450;&#23545;&#24212;&#20110;&#35780;&#35770;&#30340;&#20462;&#35746;&#12290;&#22312;&#29983;&#25104;&#20462;&#35746;&#26102;&#65292;GPT-4&#36890;&#24120;&#33021;&#22815;&#22312;&#34920;&#38754;&#19978;&#22788;&#29702;&#22909;&#35780;&#35770;&#65292;&#20294;&#23427;&#36807;&#20998;&#36981;&#24490;&#21453;&#39304;&#30340;&#25514;&#36766;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#25972;&#20307;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revising scientific papers based on peer feedback is a challenging task that requires not only deep scientific knowledge and reasoning, but also the ability to recognize the implicit requests in high-level feedback and to choose the best of many possible ways to update the manuscript in response. We introduce this task for large language models and release ARIES, a dataset of review comments and their corresponding paper edits, to enable training and evaluating models. We study two versions of the task: comment-edit alignment and edit generation, and evaluate several baselines, including GPT-4. We find that models struggle even to identify the edits that correspond to a comment, especially in cases where the comment is phrased in an indirect way or where the edit addresses the spirit of a comment but not the precise request. When tasked with generating edits, GPT-4 often succeeds in addressing comments on a surface level, but it rigidly follows the wording of the feedback rather than t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#27979;&#35797;&#38382;&#39064;&#30340;&#29305;&#24615;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#26356;&#23569;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#36731;&#26494;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.10512</link><description>&lt;p&gt;
&#39640;&#25928;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65306;&#33258;&#36866;&#24212;&#27979;&#35797;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective. (arXiv:2306.10512v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#27979;&#35797;&#38382;&#39064;&#30340;&#29305;&#24615;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#26356;&#23569;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#36731;&#26494;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#23637;&#29616;&#20102;&#19968;&#20123;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#20026;&#20102;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#36825;&#20123;&#33021;&#21147;&#65292;&#36890;&#24120;&#37319;&#29992;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#22914;&#25991;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#24515;&#29702;&#23398;&#65289;&#30340;&#22810;&#20010;&#22522;&#20934;&#65288;&#21363;&#26631;&#20934;&#27979;&#35797;&#38382;&#39064;&#38598;&#65289;&#65292;&#24182;&#25253;&#21578;&#20256;&#32479;&#24230;&#37327;&#25351;&#26631;&#65288;&#22914;&#20934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#65289;&#12290;&#28982;&#32780;&#65292;&#20174;&#35748;&#30693;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#35780;&#20272;LLMs&#30340;&#26041;&#27861;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#19988;&#19981;&#20934;&#30830;&#12290;&#21463;&#24515;&#29702;&#27979;&#37327;&#23398;&#20013;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;LLM&#35780;&#20272;&#30340;&#33258;&#36866;&#24212;&#27979;&#35797;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#26681;&#25454;&#27169;&#22411;&#30340;&#34920;&#29616;&#21160;&#24577;&#35843;&#25972;&#27979;&#35797;&#38382;&#39064;&#30340;&#29305;&#24615;&#65288;&#22914;&#38590;&#24230;&#65289;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#26631;&#20934;&#30340;&#27979;&#35797;&#38598;&#24182;&#31616;&#21333;&#25253;&#21578;&#20934;&#30830;&#29575;&#12290;&#36825;&#20351;&#24471;&#33021;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#26356;&#23569;&#30340;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20351;LLMs&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#36731;&#26494;&#27604;&#36739;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), like ChatGPT, have shown some human-like cognitive abilities. For comparing these abilities of different models, several benchmarks (i.e. sets of standard test questions) from different fields (e.g., Literature, Biology and Psychology) are often adopted and the test results under traditional metrics such as accuracy, recall and F1, are reported. However, such way for evaluating LLMs can be inefficient and inaccurate from the cognitive science perspective. Inspired by Computerized Adaptive Testing (CAT) used in psychometrics, we propose an adaptive testing framework for LLM evaluation. Rather than using a standard test set and simply reporting accuracy, this approach dynamically adjusts the characteristics of the test questions, such as difficulty, based on the model's performance. This allows for a more accurate estimation of the model's abilities, using fewer questions. More importantly, it allows LLMs to be compared with humans easily, which is essential
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#33258;&#21160;&#21270;&#21019;&#36896;&#30340;&#20250;&#35805;&#25509;&#21475;&#26469;&#26041;&#20415;&#22823;&#20247;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11326</link><description>&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#20250;&#35805;&#25509;&#21475;&#20197;&#20415;&#20110;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data. (arXiv:2305.11326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#33258;&#21160;&#21270;&#21019;&#36896;&#30340;&#20250;&#35805;&#25509;&#21475;&#26469;&#26041;&#20415;&#22823;&#20247;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#22312;&#32447;&#21457;&#24067;&#21644;&#20132;&#25442;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26368;&#24120;&#35265;&#26684;&#24335;&#12290;&#19968;&#20010;&#26126;&#30830;&#30340;&#20363;&#23376;&#26159;&#21508;&#31181;&#31867;&#22411;&#30340;&#20844;&#20849;&#34892;&#25919;&#26426;&#26500;&#21457;&#24067;&#30340;&#24320;&#25918;&#25968;&#25454;&#38376;&#25143;&#25968;&#37327;&#30340;&#22686;&#38271;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#30340;&#21033;&#29992;&#30446;&#21069;&#20165;&#38480;&#20110;&#33021;&#22815;&#20197;&#31243;&#24207;&#26041;&#24335;&#22788;&#29702;&#21644;&#28040;&#21270;&#27492;&#31867;&#25968;&#25454;&#30340;&#25216;&#26415;&#20154;&#21592;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20250;&#35805;&#25509;&#21475;&#65292;&#20197;&#20415;&#20110;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#28304;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20219;&#20309;&#26222;&#36890;&#20844;&#27665;&#37117;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#24182;&#21033;&#29992;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#19981;&#26159;&#25163;&#21160;&#21019;&#24314;&#30340;&#65306;&#30456;&#21453;&#65292;&#23427;&#20204;&#26159;&#36890;&#36807;&#23454;&#20363;&#21270;&#21487;&#37197;&#32622;&#30340;&#23545;&#35805;&#27169;&#24335;&#38598;&#20174;&#25968;&#25454;&#28304;&#26412;&#36523;&#33258;&#21160;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is the most common format to publish and exchange structured data online. A clear example is the growing number of open data portals published by all types of public administrations. However, exploitation of these data sources is currently limited to technical people able to programmatically manipulate and digest such data. As an alternative, we propose the use of chatbots to offer a conversational interface to facilitate the exploration of tabular data sources. With our approach, any regular citizen can benefit and leverage them. Moreover, our chatbots are not manually created: instead, they are automatically generated from the data source itself thanks to the instantiation of a configurable collection of conversation patterns.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.08354</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tool Learning with Foundation Models. (arXiv:2304.08354v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08354
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25317;&#26377;&#38750;&#20961;&#30340;&#21019;&#36896;&#21644;&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#20182;&#20204;&#33021;&#22815;&#20811;&#26381;&#29289;&#29702;&#38480;&#21046;&#24182;&#25506;&#32034;&#26032;&#30340;&#39046;&#22495;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#26377;&#26395;&#20687;&#20154;&#31867;&#19968;&#26679;&#29087;&#32451;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;&#36825;&#31181;&#33539;&#24335;&#21363;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#23613;&#31649;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#35813;&#39046;&#22495;&#20173;&#32570;&#20047;&#23545;&#20851;&#38190;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#24037;&#20855;&#23398;&#20064;&#30340;&#32972;&#26223;&#65292;&#21253;&#25324;&#20854;&#35748;&#30693;&#36215;&#28304;&#12289;&#22522;&#30784;&#27169;&#22411;&#30340;&#33539;&#24335;&#36716;&#25442;&#21644;&#24037;&#20855;&#21644;&#27169;&#22411;&#30340;&#20114;&#34917;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#30740;&#31350;&#65292;&#21253;&#25324;&#22522;&#20110;&#24037;&#20855;&#21644;&#38754;&#21521;&#24037;&#20855;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#31181;&#31995;&#32479;&#30340;&#25506;&#32034;&#23558;&#20026;&#26410;&#26469;&#24320;&#21457;&#20855;&#26377;&#22797;&#26434;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#36339;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool l
&lt;/p&gt;</description></item></channel></rss>