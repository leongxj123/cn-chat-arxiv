<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#36136;&#37327;&#24863;&#30693;&#22810;&#26679;&#36873;&#25321;&#65288;QaDS&#65289;&#31574;&#30053;&#65292;&#24182;&#39564;&#35777;&#20854;&#22312;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#19978;&#30340;&#20248;&#36234;&#24615;&#65307;&#25193;&#22823;&#20102;&#25968;&#25454;&#35268;&#27169;&#12289;&#29992;QaDS&#36873;&#25321;&#30340;&#36890;&#29992;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#23545;&#25968;&#23398;&#25512;&#29702;&#26377;&#24110;&#21161;&#65292;&#26368;&#21518;&#23450;&#20041;&#20102;&#26368;&#20339;&#28151;&#21512;OpenMathMix&#12290;</title><link>https://arxiv.org/abs/2404.01067</link><description>&lt;p&gt;
&#25506;&#32034;&#25968;&#23398;&#25512;&#29702;&#20013;&#25968;&#25454;&#23545;&#25512;&#29702;&#30340;&#24433;&#21709;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
Exploring the Mystery of Influential Data for Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01067
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#36136;&#37327;&#24863;&#30693;&#22810;&#26679;&#36873;&#25321;&#65288;QaDS&#65289;&#31574;&#30053;&#65292;&#24182;&#39564;&#35777;&#20854;&#22312;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#19978;&#30340;&#20248;&#36234;&#24615;&#65307;&#25193;&#22823;&#20102;&#25968;&#25454;&#35268;&#27169;&#12289;&#29992;QaDS&#36873;&#25321;&#30340;&#36890;&#29992;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#23545;&#25968;&#23398;&#25512;&#29702;&#26377;&#24110;&#21161;&#65292;&#26368;&#21518;&#23450;&#20041;&#20102;&#26368;&#20339;&#28151;&#21512;OpenMathMix&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#23545;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#26159;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#22312;&#36890;&#29992;&#20219;&#21153;&#19978;&#21487;&#20197;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#65292;&#36825;&#31181;&#21487;&#34892;&#24615;&#23578;&#26410;&#24471;&#21040;&#39564;&#35777;&#12290;&#20026;&#27492;&#65292;&#38024;&#23545;&#25968;&#23398;&#25512;&#29702;&#23384;&#22312;&#20004;&#20010;&#24320;&#25918;&#38382;&#39064;&#65306;&#22914;&#20309;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#21450;&#20160;&#20040;&#26159;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#32452;&#25104;&#12290;&#23545;&#20110;&#21069;&#32773;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#36136;&#37327;&#24863;&#30693;&#22810;&#26679;&#36873;&#25321;&#65288;QaDS&#65289;&#31574;&#30053;&#12290;&#19982;&#20854;&#20182;&#36873;&#25321;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#39564;&#35777;&#20102;QaDS&#30340;&#20248;&#36234;&#24615;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#39318;&#20808;&#25193;&#22823;&#20102;&#25105;&#20204;&#30340;&#35774;&#32622;&#24182;&#25506;&#32034;&#20102;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#32452;&#25104;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#24182;&#24378;&#35843;&#65306;&#25193;&#22823;&#25512;&#29702;&#25968;&#25454;&#35268;&#27169;&#65292;&#24182;&#35757;&#32451;&#36873;&#29992;QaDS&#36873;&#25321;&#30340;&#36890;&#29992;&#25968;&#25454;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26368;&#20339;&#28151;&#21512;&#23450;&#20041;&#20026;OpenMathMix&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01067v1 Announce Type: new  Abstract: Selecting influential data for fine-tuning on downstream tasks is a key factor for both performance and computation efficiency. Recent works have shown that training with only limited data can show a superior performance on general tasks. However, the feasibility on mathematical reasoning tasks has not been validated. To go further, there exist two open questions for mathematical reasoning: how to select influential data and what is an influential data composition. For the former one, we propose a Quality-aware Diverse Selection (QaDS) strategy adaptable for mathematical reasoning. A comparison with other selection strategies validates the superiority of QaDS. For the latter one, we first enlarge our setting and explore the influential data composition. We conduct a series of experiments and highlight: scaling up reasoning data, and training with general data selected by QaDS is helpful. Then, we define our optimal mixture as OpenMathMix
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#20013;&#30340;&#38271;&#24230;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;DPO&#20013;&#26174;&#33879;&#30340;&#21033;&#29992;&#24773;&#20917;&#65292;&#24182;&#23558;&#20854;&#19982;&#20998;&#24067;&#22806;&#24341;&#23548;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.19159</link><description>&lt;p&gt;
&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#20013;&#23558;&#38271;&#24230;&#19982;&#36136;&#37327;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Disentangling Length from Quality in Direct Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19159
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#20013;&#30340;&#38271;&#24230;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;DPO&#20013;&#26174;&#33879;&#30340;&#21033;&#29992;&#24773;&#20917;&#65292;&#24182;&#23558;&#20854;&#19982;&#20998;&#24067;&#22806;&#24341;&#23548;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF)&#26159;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;RLHF&#34987;&#35748;&#20026;&#21033;&#29992;&#20102;&#20154;&#31867;&#20559;&#22909;&#20013;&#30340;&#20559;&#35265;&#65292;&#27604;&#22914;&#20887;&#38271;&#24615;&#12290;&#31934;&#24515;&#26684;&#24335;&#21270;&#21644;&#38596;&#36777;&#30340;&#31572;&#26696;&#36890;&#24120;&#20250;&#34987;&#29992;&#25143;&#26356;&#39640;&#35780;&#20215;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#24110;&#21161;&#24615;&#21644;&#23458;&#35266;&#24615;&#19978;&#36739;&#20302;&#12290;&#19968;&#20123;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#26469;&#25511;&#21046;&#36825;&#20123;&#20559;&#35265;&#65292;&#22312;&#21476;&#20856;RLHF&#25991;&#29486;&#20013;&#36825;&#20010;&#38382;&#39064;&#24050;&#26377;&#25152;&#25506;&#35752;&#65292;&#20294;&#23545;&#20110;&#30452;&#25509;&#23545;&#40784;&#31639;&#27861;&#22914;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#36825;&#20010;&#38382;&#39064;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#19982;&#21476;&#20856;RLHF&#19981;&#21516;&#65292;DPO&#19981;&#35757;&#32451;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#25110;&#30452;&#25509;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#22240;&#27492;&#20043;&#21069;&#29992;&#26469;&#25511;&#21046;&#20887;&#38271;&#24615;&#30340;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20570;&#20986;&#20102;&#20960;&#28857;&#36129;&#29486;&#12290;&#39318;&#27425;&#22312;DPO&#29615;&#22659;&#20013;&#30740;&#31350;&#38271;&#24230;&#38382;&#39064;&#65292;&#26174;&#31034;DPO&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#21033;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;&#20998;&#24067;&#22806;&#24341;&#23548;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19159v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has been a crucial component in the recent success of Large Language Models. However, RLHF is know to exploit biases in human preferences, such as verbosity. A well-formatted and eloquent answer is often more highly rated by users, even when it is less helpful and objective. A number of approaches have been developed to control those biases in the classical RLHF literature, but the problem remains relatively under-explored for Direct Alignment Algorithms such as Direct Preference Optimization (DPO). Unlike classical RLHF, DPO does not train a separate reward model or use reinforcement learning directly, so previous approaches developed to control verbosity cannot be directly applied to this setting. Our work makes several contributions. For the first time, we study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to out-of-distribution bootstra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Fact-and-Reflection&#65288;FaR&#65289;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#24050;&#30693;&#8220;&#20107;&#23454;&#8221;&#24182;&#35201;&#27714;&#27169;&#22411;&#8220;&#21453;&#24605;&#8221;&#65292;&#22312;&#20004;&#20010;&#27493;&#39588;&#20013;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#26657;&#20934;</title><link>https://arxiv.org/abs/2402.17124</link><description>&lt;p&gt;
Fact-and-Reflection&#65288;FaR&#65289;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17124
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Fact-and-Reflection&#65288;FaR&#65289;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#24050;&#30693;&#8220;&#20107;&#23454;&#8221;&#24182;&#35201;&#27714;&#27169;&#22411;&#8220;&#21453;&#24605;&#8221;&#65292;&#22312;&#20004;&#20010;&#27493;&#39588;&#20013;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#20351;LLM&#20540;&#24471;&#20449;&#36182;&#65292;&#20854;&#32622;&#20449;&#27700;&#24179;&#24212;&#19982;&#23454;&#38469;&#34920;&#29616;&#33391;&#22909;&#26657;&#20934;&#12290;&#23613;&#31649;&#29616;&#22312;&#26222;&#36941;&#35748;&#20026;LLM&#30340;&#34920;&#29616;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20294;&#25552;&#31034;LLM&#20013;&#30340;&#32622;&#20449;&#26657;&#20934;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#25506;&#35752;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#22914;&#20309;&#24433;&#21709;LLM&#30340;&#32622;&#20449;&#26657;&#20934;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#38382;&#31572;&#29615;&#22659;&#20013;&#23545;&#20845;&#31181;&#25552;&#31034;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#25913;&#36827;LLM&#30340;&#39044;&#26399;&#26657;&#20934;&#65292;&#20294;&#20063;&#20250;&#23548;&#33268;LLM&#22312;&#21709;&#24212;&#26576;&#20123;&#23454;&#20363;&#26102;&#36807;&#20110;&#33258;&#20449;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fact-and-Reflection&#65288;FaR&#65289;&#25552;&#31034;&#65292;&#23427;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#25913;&#21892;&#20102;LLM&#30340;&#26657;&#20934;&#12290;&#39318;&#20808;&#65292;FaR&#20174;LLM&#20013;&#33719;&#21462;&#19982;&#36755;&#20837;&#25552;&#31034;&#30456;&#20851;&#30340;&#24050;&#30693;&#8220;&#20107;&#23454;&#8221;&#12290;&#28982;&#21518;&#35201;&#27714;&#27169;&#22411;&#8220;&#21453;&#24605;&#8221;&#23427;&#20204;&#20197;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17124v1 Announce Type: new  Abstract: For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known "facts" that are relevant to the input prompt from the LLM. And then it asks the model to "reflect" over them to generate the final answer. Expe
&lt;/p&gt;</description></item><item><title>CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2401.11944</link><description>&lt;p&gt;
CMMMU&#65306;&#19968;&#20010;&#20013;&#22269;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11944
&lt;/p&gt;
&lt;p&gt;
CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#21319;&#65292;&#35780;&#20272;LMMs&#30340;&#34920;&#29616;&#26085;&#30410;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#22312;&#35780;&#20272;LMMs&#22312;&#20013;&#25991;&#31561;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26356;&#22823;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CMMMU&#65292;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LMMs&#22312;&#38656;&#35201;&#22823;&#23398;&#27700;&#24179;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;CMMMU&#21463;&#21040;&#20102;MMMUs&#30340;&#26631;&#27880;&#21644;&#20998;&#26512;&#27169;&#24335;&#30340;&#21551;&#21457;&#24182;&#20005;&#26684;&#36981;&#24490;&#12290;CMMMU&#21253;&#25324;&#26469;&#33258;&#22823;&#23398;&#32771;&#35797;&#12289;&#27979;&#39564;&#21644;&#25945;&#31185;&#20070;&#30340;1.2&#19975;&#20010;&#25163;&#21160;&#25910;&#38598;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#28085;&#30422;&#20845;&#20010;&#26680;&#24515;&#23398;&#31185;&#65306;&#33402;&#26415;&#19982;&#35774;&#35745;&#12289;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#20581;&#24247;&#19982;&#21307;&#23398;&#12289;&#20154;&#25991;&#31038;&#31185;&#20197;&#21450;&#25216;&#26415;&#19982;&#24037;&#31243;&#65292;&#23601;&#20687;&#20854;&#20249;&#20276;MMMMU&#19968;&#26679;&#12290;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;30&#20010;&#23398;&#31185;&#65292;&#21253;&#25324;39&#20010;&#39640;&#24230;&#24322;&#36136;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11944v2 Announce Type: replace-cross  Abstract: As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp; Social Science, and Tech &amp; Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image 
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2310.07644</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rethinking the BERT-like Pretraining for DNA Sequences. (arXiv:2310.07644v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07644
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25104;&#21151;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;DNA&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22240;&#20854;&#25429;&#25417;&#22522;&#22240;&#30340;&#36890;&#29992;&#20449;&#24687;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DNA&#24207;&#21015;&#39044;&#35757;&#32451;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30452;&#25509;&#24341;&#20837;&#30340;BERT&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#19987;&#38376;&#23450;&#21046;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#25506;&#32034;&#24615;&#23454;&#39564;&#65292;&#24182;&#33719;&#24471;&#20102;&#20960;&#20010;&#26377;&#21551;&#21457;&#24615;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;1&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#65292;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#32780;&#19981;&#26159;K-mer&#38750;&#37325;&#21472;&#26631;&#35760;&#21270;&#26102;&#65292;&#37325;&#21472;&#21644;&#38750;&#37325;&#21472;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#22343;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;2&#65289;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#20250;&#36805;&#36895;&#20135;&#29983;&#28165;&#26224;&#30340;K-mer&#23884;&#20837;&#65292;&#24182;&#23558;&#25439;&#22833;&#38477;&#20302;&#21040;&#38750;&#24120;&#20302;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the success of large-scale pretraining in NLP, there is an increasing trend of applying it to the domain of life sciences. In particular, pretraining methods based on DNA sequences have garnered growing attention due to their potential to capture generic information about genes. However, existing pretraining methods for DNA sequences largely rely on direct adoptions of BERT pretraining from NLP, lacking a comprehensive understanding and a specifically tailored approach. To address this research gap, we first conducted a series of exploratory experiments and gained several insightful observations: 1) In the fine-tuning phase of downstream tasks, when using K-mer overlapping tokenization instead of K-mer non-overlapping tokenization, both overlapping and non-overlapping pretraining weights show consistent performance improvement.2) During the pre-training process, using K-mer overlapping tokenization quickly produces clear K-mer embeddings and reduces the loss to a very low level, w
&lt;/p&gt;</description></item><item><title>PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16911</link><description>&lt;p&gt;
PointLLM&#65306;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16911
&lt;/p&gt;
&lt;p&gt;
PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#65292;&#20294;&#22312;3D&#29702;&#35299;&#39046;&#22495;&#20173;&#26377;&#24453;&#23436;&#20840;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PointLLM&#65292;&#36825;&#26159;&#19968;&#39033;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#30340;&#21021;&#27493;&#24037;&#20316;&#65292;&#20351;LLM&#33021;&#22815;&#29702;&#35299;&#28857;&#20113;&#65292;&#24182;&#25552;&#20379;&#20102;&#36229;&#36234;2D&#35270;&#35273;&#25968;&#25454;&#30340;&#26032;&#36884;&#24452;&#12290;PointLLM&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#22788;&#29702;&#24102;&#26377;&#39068;&#33394;&#30340;&#29289;&#20307;&#28857;&#20113;&#65292;&#24182;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#28857;&#20113;&#21644;&#24120;&#35782;&#30340;&#25484;&#25569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#24378;&#22823;&#30340;LLM&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#20102;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;66&#19975;&#20010;&#31616;&#21333;&#21644;7&#19975;&#20010;&#22797;&#26434;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#65292;&#20197;&#23454;&#29616;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#23545;&#40784;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#23545;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unprecedented advancements in Large Language Models (LLMs) have created a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, thereby enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM processes colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: initially aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SICCK&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22797;&#26434;&#32452;&#21512;&#30693;&#35782;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#32467;&#26500;&#21644;&#35821;&#20041;&#32452;&#21512;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.05034</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22797;&#26434;&#32452;&#21512;&#30693;&#35782;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference. (arXiv:2307.05034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SICCK&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22797;&#26434;&#32452;&#21512;&#30693;&#35782;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#32467;&#26500;&#21644;&#35821;&#20041;&#32452;&#21512;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Sentences Involving Complex Compositional Knowledge (SICCK)&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#23545;&#36923;&#36753;&#32452;&#25104;&#24615;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;SICK&#25968;&#25454;&#38598;&#20013;&#30340;15&#20010;&#31034;&#20363;&#65292;&#29983;&#25104;&#20102;1,304&#20010;&#21477;&#23376;&#23545;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#30701;&#35821; - &#19982;&#33258;&#28982;&#36923;&#36753;&#20013;&#30340;&#26222;&#36941;&#37327;&#35789;&#12289;&#23384;&#22312;&#37327;&#35789;&#12289;&#21542;&#23450;&#21644;&#20854;&#20182;&#27010;&#24565;&#20462;&#39280;&#31526;&#30456;&#23545;&#24212;&#30340;&#20462;&#39280;&#31526; - &#20462;&#25913;&#20102;&#21407;&#22987;&#25991;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#30701;&#35821;&#20462;&#25913;&#21069;&#25552;&#21644;&#20551;&#35774;&#30340;&#20027;&#35821;&#12289;&#35859;&#35821;&#21644;&#23486;&#35821;&#37096;&#20998;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#33258;&#28982;&#36923;&#36753;&#35268;&#21017;&#20026;&#36825;&#20123;&#20462;&#25913;&#21518;&#30340;&#25991;&#26412;&#26631;&#27880;&#30456;&#24212;&#30340;&#21253;&#21547;&#20851;&#31995;&#26631;&#31614;&#12290;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#23545;&#32467;&#26500;&#21644;&#35821;&#20041;&#32452;&#21512;&#21464;&#21270;&#30340;&#25429;&#25417;&#33021;&#21147;&#36827;&#34892;&#20102;&#21021;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;NLI&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic. We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014). To this end, we modify the original texts using a set of phrases - modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis. Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules. We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios. We found that the performance of NLI models under th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#27979;&#35797;MQuAKE&#65292;&#36890;&#36807;&#22810;&#36339;&#38382;&#39064;&#35780;&#20272;&#32534;&#36753;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#22238;&#31572;&#22240;&#32534;&#36753;&#20107;&#23454;&#32780;&#31572;&#26696;&#24212;&#35813;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#21484;&#22238;&#24050;&#32534;&#36753;&#30340;&#20107;&#23454;&#65292;&#20294;&#22312;&#22810;&#36339;&#38382;&#39064;&#19978;&#34920;&#29616;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2305.14795</link><description>&lt;p&gt;
MQuAKE&#65306;&#36890;&#36807;&#22810;&#36339;&#38382;&#39064;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions. (arXiv:2305.14795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#27979;&#35797;MQuAKE&#65292;&#36890;&#36807;&#22810;&#36339;&#38382;&#39064;&#35780;&#20272;&#32534;&#36753;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#22238;&#31572;&#22240;&#32534;&#36753;&#20107;&#23454;&#32780;&#31572;&#26696;&#24212;&#35813;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#21484;&#22238;&#24050;&#32534;&#36753;&#30340;&#20107;&#23454;&#65292;&#20294;&#22312;&#22810;&#36339;&#38382;&#39064;&#19978;&#34920;&#29616;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#23384;&#20648;&#30340;&#20449;&#24687;&#24456;&#24555;&#23601;&#20250;&#36807;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#24182;&#38750;&#24635;&#26159;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;&#36825;&#20419;&#20351;&#20154;&#20204;&#24320;&#21457;&#20102;&#36890;&#36807;&#26356;&#26032;&#27169;&#22411;&#26435;&#37325;&#27880;&#20837;&#26032;&#20107;&#23454;&#30340;&#19968;&#31995;&#21015;&#25216;&#26415;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#38750;&#24120;&#26377;&#38480;&#65292;&#20027;&#35201;&#39564;&#35777;&#32534;&#36753;&#20107;&#23454;&#30340;&#21484;&#22238;&#29575;&#65292;&#20294;&#26356;&#25913;&#19968;&#20010;&#20107;&#23454;&#24212;&#35813;&#20250;&#23545;&#27169;&#22411;&#30340;&#30456;&#20851;&#20449;&#24565;&#20135;&#29983;&#36830;&#38145;&#21453;&#24212;&#12290;&#22914;&#26524;&#25105;&#20204;&#32534;&#36753;&#33521;&#22269;&#39318;&#30456;&#20026;Rishi Sunak&#65292;&#37027;&#20040;&#23545;&#20110;&#8220;&#35841;&#26159;&#33521;&#22269;&#39318;&#30456;&#30340;&#37197;&#20598;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#35813;&#24471;&#21040;&#19968;&#20010;&#19981;&#21516;&#30340;&#31572;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;MQuAKE&#65288;&#29992;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#22810;&#36339;&#38382;&#31572;&#65289;&#65292;&#21253;&#25324;&#22810;&#36339;&#38382;&#39064;&#65292;&#35780;&#20272;&#32534;&#36753;&#21518;&#30340;&#27169;&#22411;&#26159;&#21542;&#27491;&#30830;&#22238;&#31572;&#37027;&#20123;&#22240;&#32534;&#36753;&#20107;&#23454;&#32780;&#31572;&#26696;&#24212;&#35813;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#21484;&#22238;&#24050;&#32534;&#36753;&#30340;&#20107;&#23454;&#65292;&#20294;&#23427;&#20204;&#22312;&#26500;&#24314;&#30340;&#22810;&#36339;&#38382;&#39064;&#19978;&#36973;&#36935;&#20102;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;LLMs&#30340;&#35780;&#20272;&#24517;&#39035;&#36229;&#36234;&#31616;&#21333;&#30340;&#20107;&#23454;&#21484;&#22238;&#65292;&#24182;&#32435;&#20837;&#26356;&#24494;&#22937;&#30340;&#30693;&#35782;&#32534;&#36753;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#35843;&#25972;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#30340;&#23610;&#24230;&#19981;&#19968;&#33268;&#65292;&#35299;&#20915;&#20102;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#26631;&#27880;&#32773;&#20043;&#38388;&#20998;&#27495;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14770</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#37325;&#26032;&#35843;&#25972;&#20154;&#31867;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Using Natural Language Explanations to Rescale Human Judgments. (arXiv:2305.14770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#35843;&#25972;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#30340;&#23610;&#24230;&#19981;&#19968;&#33268;&#65292;&#35299;&#20915;&#20102;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#26631;&#27880;&#32773;&#20043;&#38388;&#20998;&#27495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#24102;&#26469;&#20102;&#38656;&#35201;&#39640;&#36136;&#37327;&#20154;&#26631;&#35760;&#25968;&#25454;&#30340;&#32039;&#36843;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20154;&#30340;&#21453;&#39304;&#21644;&#35780;&#20272;&#31561;&#36807;&#31243;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#36890;&#36807;&#22810;&#20010;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#20849;&#35782;&#26469;&#26631;&#27880;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#26631;&#27880;&#32773;&#21487;&#33021;&#23545;&#26631;&#27880;&#26041;&#26696;&#26377;&#19981;&#21516;&#30340;&#35299;&#37322;&#65292;&#38500;&#38750;&#25509;&#21463;&#20102;&#24191;&#27867;&#30340;&#22521;&#35757;&#65292;&#21542;&#21017;&#23545;&#20110;&#20027;&#35266;&#30340;NLP&#20219;&#21153;&#65292;&#29978;&#33267;&#21463;&#36807;&#35757;&#32451;&#30340;&#19987;&#23478;&#26631;&#27880;&#32773;&#20063;&#21487;&#33021;&#20250;&#20986;&#29616;&#24040;&#22823;&#30340;&#20998;&#27495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#32454;&#24494;&#24046;&#21035;&#21487;&#20197;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36827;&#34892;&#25429;&#25417;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLM&#22312;&#23384;&#22312;&#20998;&#27495;&#26102;&#37325;&#26032;&#35843;&#25972;&#22823;&#23567;&#25490;&#24207;&#27880;&#37322;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;Likert&#35780;&#20998;&#21644;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36755;&#20837;LLM&#65292;&#24182;&#25552;&#31034;&#23427;&#20135;&#29983;&#19968;&#20010;&#25968;&#23383;&#24471;&#20998;&#12290;&#36825;&#20010;&#24471;&#20998;&#24212;&#35813;&#21453;&#26144;&#27880;&#37322;&#32773;&#23545;&#31034;&#20363;&#30340;&#22522;&#26412;&#35780;&#20272;&#12290;&#35299;&#37322;&#30340;&#23384;&#22312;&#20351;LLM&#33021;&#22815;&#22312;&#23610;&#24230;&#20351;&#29992;&#24046;&#24322;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20351;&#35780;&#32423;&#22312;&#26631;&#27880;&#32773;&#20043;&#38388;&#21516;&#36136;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of large language models (LLMs) has brought a critical need for high-quality human-labeled data, particularly for processes like human feedback and evaluation. A common practice is to label data via consensus annotation over the judgments of multiple crowdworkers. However, different annotators may have different interpretations of labeling schemes unless given extensive training, and for subjective NLP tasks, even trained expert annotators can diverge heavily. We show that these nuances can be captured by high quality natural language explanations, and propose a method to rescale ordinal annotation in the presence of disagreement using LLMs. Specifically, we feed Likert ratings and corresponding natural language explanations into an LLM and prompt it to produce a numeric score. This score should reflect the underlying assessment of the example by the annotator. The presence of explanations allows the LLM to homogenize ratings across annotators in spite of scale usage differenc
&lt;/p&gt;</description></item></channel></rss>