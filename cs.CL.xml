<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#36981;&#24490;&#20854;&#20182;&#31070;&#32463;&#27169;&#22411;&#30340;&#32553;&#25918;&#35268;&#24459;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23545;&#25968;&#20284;&#28982;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.18684</link><description>&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#30340;&#25193;&#23637;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws For Dense Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18684
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#36981;&#24490;&#20854;&#20182;&#31070;&#32463;&#27169;&#22411;&#30340;&#32553;&#25918;&#35268;&#24459;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23545;&#25968;&#20284;&#28982;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#27169;&#22411;&#25193;&#23637;&#21040;&#26356;&#22823;&#35268;&#27169;&#24050;&#32463;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#27169;&#22411;&#30340;&#24615;&#33021;&#24120;&#36981;&#24490;&#21487;&#39044;&#27979;&#30340;&#25193;&#23637;&#35268;&#24459;&#65292;&#19982;&#35757;&#32451;&#38598;&#22823;&#23567;&#21644;&#27169;&#22411;&#22823;&#23567;&#31561;&#22240;&#32032;&#30456;&#20851;&#12290;&#36825;&#19968;&#27934;&#23519;&#21147;&#38750;&#24120;&#23453;&#36149;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#35268;&#27169;&#23454;&#39564;&#21464;&#24471;&#36234;&#26469;&#36234;&#32791;&#36153;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26816;&#32034;&#25351;&#26631;&#30340;&#31163;&#25955;&#24615;&#20197;&#21450;&#26816;&#32034;&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#22823;&#23567;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23494;&#38598;&#26816;&#32034;&#20013;&#30340;&#36825;&#31181;&#25193;&#23637;&#35268;&#24459;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#36981;&#24490;&#20854;&#20182;&#31070;&#32463;&#27169;&#22411;&#30340;&#32553;&#25918;&#35268;&#24459;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#23545;&#27604;&#23545;&#25968;&#20284;&#28982;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23545;&#23454;&#29616;&#20102;&#19981;&#21516;&#21442;&#25968;&#25968;&#37327;&#24182;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18684v1 Announce Type: cross  Abstract: Scaling up neural models has yielded significant advancements in a wide array of tasks, particularly in language generation. Previous studies have found that the performance of neural models frequently adheres to predictable scaling laws, correlated with factors such as training set size and model size. This insight is invaluable, especially as large-scale experiments grow increasingly resource-intensive. Yet, such scaling law has not been fully explored in dense retrieval due to the discrete nature of retrieval metrics and complex relationships between training data and model sizes in retrieval tasks. In this study, we investigate whether the performance of dense retrieval models follows the scaling law as other neural models. We propose to use contrastive log-likelihood as the evaluation metric and conduct extensive experiments with dense retrieval models implemented with different numbers of parameters and trained with different amo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;EAP-IG&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20445;&#25345;&#30005;&#36335;&#30340;&#26680;&#24515;&#23646;&#24615;&#65306;&#24544;&#23454;</title><link>https://arxiv.org/abs/2403.17806</link><description>&lt;p&gt;
&#22362;&#20449;&#24544;&#23454;&#65306;&#22312;&#25214;&#21040;&#27169;&#22411;&#26426;&#21046;&#26102;&#36229;&#36234;&#30005;&#36335;&#37325;&#21472;
&lt;/p&gt;
&lt;p&gt;
Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17806
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;EAP-IG&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20445;&#25345;&#30005;&#36335;&#30340;&#26680;&#24515;&#23646;&#24615;&#65306;&#24544;&#23454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#24050;&#37319;&#29992;&#30005;&#36335;&#26694;&#26550;&#65292;&#26088;&#22312;&#25214;&#21040;&#35299;&#37322;LM&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#34892;&#20026;&#30340;&#26368;&#23567;&#35745;&#31639;&#23376;&#22270;&#25110;&#30005;&#36335;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#36890;&#36807;&#29420;&#31435;&#23545;&#27599;&#20010;&#36793;&#25191;&#34892;&#22240;&#26524;&#24178;&#39044;&#26469;&#30830;&#23450;&#21738;&#20123;&#36793;&#23646;&#20110;LM&#30340;&#30005;&#36335;&#65292;&#20294;&#36825;&#22312;&#27169;&#22411;&#35268;&#27169;&#36739;&#22823;&#26102;&#25928;&#29575;&#20302;&#19979;&#12290;&#36793;&#32536;&#24402;&#22240;&#20462;&#34917;&#65288;EAP&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#36817;&#20284;&#24178;&#39044;&#26041;&#27861;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#20294;&#19981;&#23436;&#32654;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; - &#24102;&#26377;&#38598;&#25104;&#26799;&#24230;&#30340;EAP&#65288;EAP-IG&#65289;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20445;&#25345;&#30005;&#36335;&#30340;&#26680;&#24515;&#23646;&#24615;&#65306;&#24544;&#23454;&#12290;&#22914;&#26524;&#30005;&#36335;&#26159;&#24544;&#23454;&#30340;&#65292;&#21017;&#21487;&#20197;&#21435;&#25481;&#30005;&#36335;&#20043;&#22806;&#30340;&#25152;&#26377;&#27169;&#22411;&#36793;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#22312;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65307;&#24544;&#23454;&#24615;&#26159;&#30740;&#31350;&#30005;&#36335;&#32780;&#19981;&#26159;&#23436;&#25972;&#27169;&#22411;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;EAP&#25214;&#21040;&#30340;&#30005;&#36335;&#19981;&#22826;&#24544;&#23454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17806v1 Announce Type: cross  Abstract: Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faith
&lt;/p&gt;</description></item><item><title>&#26827;&#31867;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19979;&#19968;&#20010;&#23383;&#31526;&#39044;&#27979;&#35757;&#32451;&#65292;&#20173;&#33021;&#23398;&#20064;&#20986;&#20869;&#37096;&#34920;&#31034;&#30340;&#26827;&#30424;&#29366;&#24577;</title><link>https://arxiv.org/abs/2403.15498</link><description>&lt;p&gt;
&#26827;&#31867;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#19990;&#30028;&#27169;&#22411;&#21644;&#28508;&#21464;&#37327;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15498
&lt;/p&gt;
&lt;p&gt;
&#26827;&#31867;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19979;&#19968;&#20010;&#23383;&#31526;&#39044;&#27979;&#35757;&#32451;&#65292;&#20173;&#33021;&#23398;&#20064;&#20986;&#20869;&#37096;&#34920;&#31034;&#30340;&#26827;&#30424;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#24615;&#33021;&#26469;&#28304;&#30340;&#35752;&#35770;&#12290;&#26159;&#20165;&#20165;&#23398;&#20064;&#21477;&#27861;&#27169;&#24335;&#21644;&#34920;&#38754;&#32479;&#35745;&#32467;&#26524;&#65292;&#36824;&#26159;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#35821;&#20041;&#21644;&#19990;&#30028;&#27169;&#22411;&#65311;&#25105;&#20204;&#22312;&#35937;&#26827;&#36825;&#20010;&#26356;&#22797;&#26434;&#30340;&#39046;&#22495;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#28216;&#25103;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#29992;&#32447;&#24615;&#25506;&#27979;&#21644;&#23545;&#27604;&#28608;&#27963;&#26469;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#23613;&#31649;&#27169;&#22411;&#27809;&#26377;&#20808;&#39564;&#30340;&#28216;&#25103;&#30693;&#35782;&#65292;&#20165;&#20165;&#36890;&#36807;&#19979;&#19968;&#20010;&#23383;&#31526;&#39044;&#27979;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20851;&#20110;&#26827;&#30424;&#29366;&#24577;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15498v1 Announce Type: cross  Abstract: Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Un
&lt;/p&gt;</description></item><item><title>LLM2LLM &#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#25945;&#24072;LLM&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#24182;&#23558;&#20854;&#28155;&#21152;&#22238;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#24110;&#21161;&#20302;&#25968;&#25454;&#29615;&#22659;&#19979;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.15042</link><description>&lt;p&gt;
LLM2LLM: &#21033;&#29992;&#26032;&#30340;&#36845;&#20195;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22686;&#24378;LLM
&lt;/p&gt;
&lt;p&gt;
LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15042
&lt;/p&gt;
&lt;p&gt;
LLM2LLM &#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#25945;&#24072;LLM&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#24182;&#23558;&#20854;&#28155;&#21152;&#22238;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#24110;&#21161;&#20302;&#25968;&#25454;&#29615;&#22659;&#19979;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30446;&#21069;&#26159;&#35299;&#20915;&#32477;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#23613;&#31649;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20173;&#38656;&#35201;&#24494;&#35843;&#20197;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#24212;&#29992;&#22788;&#20110;&#20302;&#25968;&#25454;&#33539;&#22260;&#65292;&#20351;&#24471;&#24494;&#35843;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM2LLM&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#38024;&#23545;&#24615;&#21644;&#36845;&#20195;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#19968;&#20010;&#25945;&#24072;LLM&#26469;&#22686;&#24378;&#19968;&#20010;&#23567;&#30340;&#31181;&#23376;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22686;&#21152;&#39069;&#22806;&#30340;&#25968;&#25454;&#29992;&#20110;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;LLM2LLM&#65288;1&#65289;&#22312;&#21021;&#22987;&#31181;&#23376;&#25968;&#25454;&#19978;&#24494;&#35843;&#22522;&#32447;&#23398;&#29983;LLM&#65292;&#65288;2&#65289;&#35780;&#20272;&#21644;&#25552;&#21462;&#27169;&#22411;&#38169;&#35823;&#30340;&#25968;&#25454;&#28857;&#65292;&#65288;3&#65289;&#20351;&#29992;&#25945;&#24072;LLM&#26681;&#25454;&#36825;&#20123;&#38169;&#35823;&#30340;&#25968;&#25454;&#28857;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#28982;&#21518;&#23558;&#20854;&#28155;&#21152;&#22238;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#24378;LLM&#23545;&#38169;&#35823;&#39044;&#27979;&#25968;&#25454;&#28857;&#30340;&#20449;&#21495;&#65292;&#24182;&#37325;&#26032;&#25972;&#21512;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15042v1 Announce Type: new  Abstract: Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11299</link><description>&lt;p&gt;
SQ-LLaVA&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#32463;&#36807;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#21518;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#30528;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#40511;&#27807;&#25104;&#20026;&#25972;&#20010;&#32593;&#32476;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#25913;&#21892;&#36328;&#27169;&#24577;&#23545;&#40784;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#32771;&#34385;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#35270;&#35273;&#20219;&#21153;&#33539;&#22260;&#30340;&#26356;&#22810;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#29992;&#20110;&#38382;&#31572;&#65292;&#20294;&#36825;&#31181;&#25805;&#20316;&#25104;&#26412;&#36739;&#39640;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#21253;&#21547;&#22823;&#37327;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20294;&#36825;&#19968;&#26041;&#38754;&#19968;&#30452;&#40092;&#26377;&#20154;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20869;&#37096;&#34987;&#24573;&#35270;&#30340;&#19978;&#19979;&#25991;&#65292;&#35757;&#32451;&#27169;&#22411;&#33258;&#25105;&#35757;&#32451;'&#23398;&#20064;'&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;&#12290;SQ-LLaVA&#22312;&#29983;&#25104;&#28789;&#27963;&#19988;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11299v1 Announce Type: cross  Abstract: Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20197;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#65292;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10707</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#38598;&#25104;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#30340;&#28508;&#22312;&#20027;&#39064;&#65306;&#27668;&#20505;&#36816;&#21160;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20197;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#65292;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25581;&#31034;&#21644;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#37492;&#20110;&#20256;&#32479;&#20027;&#39064;&#32423;&#20998;&#26512;&#30340;&#23616;&#38480;&#24615;&#65292;&#24448;&#24448;&#21482;&#25429;&#25417;&#21040;&#25972;&#20307;&#27169;&#24335;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#23545;&#26356;&#31934;&#32454;&#12289;&#20027;&#39064;&#32858;&#28966;&#30340;&#25506;&#32034;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#20027;&#39064;&#21457;&#29616;&#26041;&#27861;&#65292;&#28041;&#21450;&#25163;&#21160;&#27969;&#31243;&#21644;&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#22312;&#20280;&#32553;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#36164;&#28304;&#24378;&#24230;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#28041;&#21450;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20808;&#36827;&#21151;&#33021;&#30340;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#26356;&#28145;&#20837;&#22320;&#35843;&#26597;&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;&#30340;&#20027;&#39064;&#26041;&#38754;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#22810;&#26679;&#30340;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#26356;&#24191;&#27867;&#20027;&#39064;&#20869;&#26377;&#30340;&#24494;&#22937;&#32454;&#33410;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10707v1 Announce Type: cross  Abstract: This paper introduces a novel approach to uncovering and analyzing themes in social media messaging. Recognizing the limitations of traditional topic-level analysis, which tends to capture only the overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Conventional methods of theme discovery, involving manual processes and a human-in-the-loop approach, are valuable but face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). This approach allows for a deeper investigation into the thematic aspects of social media discourse, enabling us to uncover a diverse array of themes, each with unique characteristics and relevance, thereby offering a comprehensive understanding of the nuances present within broader topics.
&lt;/p&gt;</description></item><item><title>SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.08370</link><description>&lt;p&gt;
SMART: &#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#23376;&#27169;&#22359;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SMART: Submodular Data Mixture Strategy for Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08370
&lt;/p&gt;
&lt;p&gt;
SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#28041;&#21450;&#22312;&#19968;&#32452;&#20197;&#25351;&#20196;&#26684;&#24335;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#27604;&#20363;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#25214;&#21040;&#21512;&#36866;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#25110;&#20381;&#36182;&#20174;&#19994;&#32773;&#30340;&#30452;&#35273;&#22806;&#65292;&#23578;&#26080;&#31995;&#32479;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SMART&#65288;Submodular data Mixture strAtegy for instRuction Tuning&#65289;- &#19968;&#31181;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#26032;&#39062;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#20998;&#25968;&#26469;&#30830;&#23450;&#28151;&#21512;&#26435;&#37325;&#12290;&#32473;&#23450;&#24494;&#35843;&#39044;&#31639;&#65292;SMART&#37325;&#26032;&#20998;&#37197;&#20219;&#21153;&#38388;&#30340;&#39044;&#31639;&#65292;&#24182;&#20174;&#27599;&#20010;&#20219;&#21153;&#20013;&#36873;&#25321;&#38750;&#20887;&#20313;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SMART&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#20363;&#23376;&#27604;&#20363;&#28151;&#21512;&#21644;&#22343;&#31561;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08370v1 Announce Type: cross  Abstract: Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoAT&#30340;Chain-of-Action-Thought&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#20808;&#21069;&#21160;&#20316;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#24773;&#20917;&#20197;&#21450;&#26410;&#26469;&#21160;&#20316;&#24605;&#32771;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#26426;GUI&#20195;&#29702;&#30340;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02713</link><description>&lt;p&gt;
Android&#22312;&#21160;&#29289;&#22253;&#20013;: GUI&#20195;&#29702;&#30340;&#21160;&#20316;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Android in the Zoo: Chain-of-Action-Thought for GUI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoAT&#30340;Chain-of-Action-Thought&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#20808;&#21069;&#21160;&#20316;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#24773;&#20917;&#20197;&#21450;&#26410;&#26469;&#21160;&#20316;&#24605;&#32771;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#26426;GUI&#20195;&#29702;&#30340;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23548;&#33268;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#22823;&#37327;&#33258;&#20027;GUI&#20195;&#29702;&#28608;&#22686;&#65292;&#36825;&#20123;&#20195;&#29702;&#36890;&#36807;&#39044;&#27979;API&#30340;&#19968;&#31995;&#21015;&#21160;&#20316;&#26469;&#23436;&#25104;&#30001;&#33258;&#28982;&#35821;&#35328;&#35302;&#21457;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#35813;&#20219;&#21153;&#39640;&#24230;&#20381;&#36182;&#20110;&#36807;&#21435;&#30340;&#21160;&#20316;&#21644;&#35270;&#35273;&#35266;&#23519;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#24456;&#23569;&#32771;&#34385;&#20013;&#38388;&#25130;&#22270;&#21644;&#23631;&#24149;&#25805;&#20316;&#20256;&#36882;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#20316;&#24605;&#32500;&#38142;&#65288;CoAT&#65289;&#65292;&#23427;&#32771;&#34385;&#20102;&#20808;&#21069;&#21160;&#20316;&#30340;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#20998;&#26512;&#24212;&#24403;&#25191;&#34892;&#30340;&#21160;&#20316;&#20197;&#21450;&#36873;&#25321;&#30340;&#21160;&#20316;&#24102;&#26469;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#29616;&#25104;LLM&#36827;&#34892;&#38646;&#27425;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;CoAT&#30456;&#27604;&#20110;&#26631;&#20934;&#19978;&#19979;&#25991;&#24314;&#27169;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#26631;&#30340;&#23436;&#25104;&#24773;&#20917;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Android-In-The-Zoo&#65288;AitZ&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;18,643&#20010;&#23631;&#24149;&#21160;&#20316;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02713v1 Announce Type: new  Abstract: Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3.5&#21644;GPT-4&#20174;&#20135;&#21697;&#26631;&#39064;&#21644;&#25551;&#36848;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#23646;&#24615;&#20540;&#30340;&#28508;&#21147;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;WDC PAVE&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.02130</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#20135;&#21697;&#23646;&#24615;&#20540;
&lt;/p&gt;
&lt;p&gt;
Using LLMs for the Extraction and Normalization of Product Attribute Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3.5&#21644;GPT-4&#20174;&#20135;&#21697;&#26631;&#39064;&#21644;&#25551;&#36848;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#23646;&#24615;&#20540;&#30340;&#28508;&#21147;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;WDC PAVE&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;&#20135;&#21697;&#25552;&#20379;&#36890;&#24120;&#21253;&#25324;&#25991;&#26412;&#20135;&#21697;&#26631;&#39064;&#21644;&#25991;&#26412;&#20135;&#21697;&#25551;&#36848;&#12290;&#20026;&#20102;&#25552;&#20379;&#35832;&#22914;&#20998;&#38754;&#20135;&#21697;&#36807;&#28388;&#25110;&#22522;&#20110;&#20869;&#23481;&#30340;&#20135;&#21697;&#25512;&#33616;&#31561;&#21151;&#33021;&#65292;&#32593;&#31449;&#38656;&#35201;&#20174;&#38750;&#32467;&#26500;&#21270;&#20135;&#21697;&#25551;&#36848;&#20013;&#25552;&#21462;&#23646;&#24615;&#20540;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#65292;&#20174;&#20135;&#21697;&#26631;&#39064;&#21644;&#20135;&#21697;&#25551;&#36848;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#23646;&#24615;&#20540;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WDC&#20135;&#21697;&#23646;&#24615;-&#20540;&#25552;&#21462;&#65288;WDC PAVE&#65289;&#25968;&#25454;&#38598;&#12290;WDC PAVE&#21253;&#21547;&#26469;&#33258;&#25552;&#20379;schema.org&#27880;&#37322;&#30340;87&#20010;&#32593;&#31449;&#30340;&#20135;&#21697;&#25552;&#20379;&#12290;&#36825;&#20123;&#25552;&#20379;&#23646;&#20110;&#20116;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#27599;&#20010;&#31867;&#21035;&#37117;&#20855;&#26377;&#19968;&#32452;&#29305;&#23450;&#30340;&#23646;&#24615;&#12290;&#35813;&#25968;&#25454;&#38598;&#20197;&#20004;&#31181;&#24418;&#24335;&#25552;&#20379;&#25163;&#21160;&#39564;&#35777;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65306;&#65288;i&#65289;&#30452;&#25509;&#25552;&#21462;&#30340;&#20540;&#21644;&#65288;ii&#65289;&#35268;&#33539;&#21270;&#30340;&#23646;&#24615;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02130v1 Announce Type: new  Abstract: Product offers on e-commerce websites often consist of a textual product title and a textual product description. In order to provide features such as faceted product filtering or content-based product recommendation, the websites need to extract attribute-value pairs from the unstructured product descriptions. This paper explores the potential of using large language models (LLMs), such as OpenAI's GPT-3.5 and GPT-4, to extract and normalize attribute values from product titles and product descriptions. For our experiments, we introduce the WDC Product Attribute-Value Extraction (WDC PAVE) dataset. WDC PAVE consists of product offers from 87 websites that provide schema.org annotations. The offers belong to five different categories, each featuring a specific set of attributes. The dataset provides manually verified attribute-value pairs in two forms: (i) directly extracted values and (ii) normalized attribute values. The normalization 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18659</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#28216;&#25103;&#65306;&#35843;&#30740;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Games: A Survey and Roadmap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#24613;&#21095;&#22686;&#21152;&#65292;&#24182;&#20276;&#38543;&#30528;&#20844;&#20247;&#23545;&#35813;&#20027;&#39064;&#30340;&#21442;&#19982;&#12290;&#23613;&#31649;&#36215;&#21021;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;LLMs&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#21253;&#25324;&#28216;&#25103;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21450;&#20026;&#28216;&#25103;&#25552;&#20379;&#25903;&#25345;&#30340;&#21508;&#31181;&#24212;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#26126;&#30830;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21487;&#20197;&#25198;&#28436;&#30340;&#19981;&#21516;&#35282;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23578;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#21644;LLMs&#22312;&#28216;&#25103;&#20013;&#26410;&#26469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;LLMs&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#20316;&#20026;LLMs&#21644;&#28216;&#25103;&#20132;&#21449;&#39046;&#22495;&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#21644;&#36335;&#32447;&#22270;&#65292;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#25104;&#20026;&#36825;&#19968;&#28608;&#21160;&#20154;&#24515;&#30340;&#26032;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#21644;&#21019;&#26032;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18659v1 Announce Type: cross  Abstract: Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.
&lt;/p&gt;</description></item><item><title>ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15220</link><description>&lt;p&gt;
ChunkAttention: &#20855;&#26377;&#21069;&#32512;&#24863;&#30693;KV&#32531;&#23384;&#21644;&#20004;&#38454;&#27573;&#20998;&#21306;&#30340;&#39640;&#25928;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15220
&lt;/p&gt;
&lt;p&gt;
ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;&#38271;&#24207;&#21015;&#26469;&#35828;&#26159;&#25512;&#29702;&#24310;&#36831;&#30340;&#19968;&#20010;&#26174;&#33879;&#26469;&#28304;&#12290;&#22312;&#22810;&#31199;&#25143;LLMs&#26381;&#21153;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;LLM&#35831;&#27714;&#22312;&#21069;&#32512;&#20013;&#20849;&#20139;&#31995;&#32479;&#25552;&#31034;&#30340;&#27010;&#29575;&#65292;&#21487;&#20197;&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25805;&#20316;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChunkAttention&#65292;&#19968;&#31181;&#20855;&#26377;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#26816;&#27979;&#22810;&#20010;&#35831;&#27714;&#20043;&#38388;&#21305;&#37197;&#30340;&#25552;&#31034;&#21069;&#32512;&#65292;&#24182;&#20849;&#20139;&#23427;&#20204;&#30340;&#38190;/&#20540;&#24352;&#37327;&#20197;&#25913;&#36827;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#25972;&#20307;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#26469;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;KV&#32531;&#23384;&#20043;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#33258;&#27880;&#24847;&#21147;&#20869;&#26680;&#65292;&#20854;&#20013;&#23454;&#29616;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19977;&#37325;&#32534;&#30721;&#22120;&#35745;&#31639;&#35805;&#35821;&#28151;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#27169;&#22411;&#30340;&#26174;&#30528;&#25913;&#36827;&#21644;&#38646;-shot&#27867;&#21270;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.12332</link><description>&lt;p&gt;
&#19977;&#37325;&#32534;&#30721;&#22120;&#65306;&#20849;&#21516;&#28608;&#27963;&#30340;&#34920;&#31034;&#19968;&#36215;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Triple-Encoders: Representations That Fire Together, Wire Together
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12332
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19977;&#37325;&#32534;&#30721;&#22120;&#35745;&#31639;&#35805;&#35821;&#28151;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#27169;&#22411;&#30340;&#26174;&#30528;&#25913;&#36827;&#21644;&#38646;-shot&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#23548;&#21521;&#30340;&#23545;&#35805;&#27169;&#22411;&#36890;&#24120;&#22312;&#27599;&#20010;&#36718;&#27425;&#37325;&#26032;&#23545;&#23545;&#35805;&#21382;&#21490;&#36827;&#34892;&#32534;&#30721;&#65292;&#36896;&#25104;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#26354;&#29575;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#26368;&#36817;&#23637;&#31034;&#20986;&#23545;&#35805;&#24314;&#27169;&#36828;&#36229;&#25928;&#29575;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#32534;&#30721;&#22120;&#23558;&#35805;&#35821;&#20043;&#38388;&#30340;&#30456;&#23545;&#36317;&#31163;&#32534;&#30721;&#21040;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;&#39640;&#25928;&#29575;&#26159;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#35805;&#35821;&#23454;&#29616;&#30340;&#65292;&#28982;&#32780;&#36825;&#24573;&#30053;&#20102;&#19978;&#19979;&#25991;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19977;&#37325;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Hebbian&#21551;&#21457;&#30340;&#20849;&#23384;&#23398;&#20064;&#30446;&#26631;&#65292;&#20174;&#36825;&#20123;&#29420;&#31435;&#32534;&#30721;&#30340;&#35805;&#35821;&#20013;&#39640;&#25928;&#35745;&#31639;&#20998;&#24067;&#24335;&#35805;&#35821;&#28151;&#21512;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#26435;&#37325;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#19977;&#37325;&#32534;&#30721;&#22120;&#22312;&#27604;&#32534;&#30721;&#22120;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#29978;&#33267;&#27604;&#21333;&#21521;&#37327;&#34920;&#31034;&#27169;&#22411;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;-shot&#27867;&#21270;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12332v1 Announce Type: new  Abstract: Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost. Curved Contrastive Learning, a representation learning method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency. While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization. To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective without using any weights. Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better zero-shot generalization than single-vector representation models without requiring re-encoding. Our code/model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25506;&#32034;&#39537;&#21160;&#31574;&#30053;&#20248;&#21270;&#30340;RLHF&#31639;&#27861;&#65292;&#36890;&#36807;&#36712;&#36857;&#27604;&#36739;&#21453;&#39304;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#65292;&#20026;&#35299;&#37322;&#23569;&#37327;&#20154;&#31867;&#21453;&#39304;&#36275;&#20197;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;</title><link>https://arxiv.org/abs/2402.10342</link><description>&lt;p&gt;
&#22312;RLHF&#20013;&#22522;&#20110;&#25506;&#32034;&#39537;&#21160;&#30340;&#31574;&#30053;&#20248;&#21270;&#65306;&#20851;&#20110;&#26377;&#25928;&#25968;&#25454;&#21033;&#29992;&#30340;&#29702;&#35770;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25506;&#32034;&#39537;&#21160;&#31574;&#30053;&#20248;&#21270;&#30340;RLHF&#31639;&#27861;&#65292;&#36890;&#36807;&#36712;&#36857;&#27604;&#36739;&#21453;&#39304;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#65292;&#20026;&#35299;&#37322;&#23569;&#37327;&#20154;&#31867;&#21453;&#39304;&#36275;&#20197;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#22312;&#20381;&#36182;&#23569;&#37327;&#20154;&#31867;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#29616;&#35937;&#23384;&#22312;&#30528;&#26377;&#38480;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#26368;&#36817;&#30340;&#32463;&#39564;&#25104;&#21151;&#37319;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#30740;&#31350;&#20173;&#20391;&#37325;&#20110;&#22522;&#20110;&#20215;&#20540;&#30340;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#65288;PO-RLHF&#65289;&#30340;RLHF&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#27969;&#34892;&#30340;&#31574;&#30053;&#35206;&#30422;-&#31574;&#30053;&#26799;&#24230;&#65288;PC-PG&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20551;&#35774;&#23545;&#22870;&#21169;&#20989;&#25968;&#26377;&#30693;&#35782;&#12290;&#22312;PO-RLHF&#20013;&#65292;&#19981;&#20551;&#35774;&#30693;&#36947;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#36712;&#36857;&#30340;&#27604;&#36739;&#21453;&#39304;&#26469;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;PO-RLHF&#25552;&#20379;&#20102;&#20302;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#24615;&#33021;&#30028;&#38480;&#65292;&#36825;&#20026;&#35299;&#37322;&#20026;&#20160;&#20040;&#23569;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#21487;&#33021;&#36275;&#20197;&#22312;RLHF&#20013;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#21019;&#26032;&#26159;&#25105;&#20204;&#30340;&#36712;&#36857;&#32423;el
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10342v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with RLHF. A key novelty is our trajectory-level el
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.08787</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Rethinking Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65288;MU&#65289;&#65292;&#31216;&#20026;LLM&#28040;&#38500;&#25216;&#26415;&#12290;&#36825;&#20010;&#30740;&#31350;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#25935;&#24863;&#25110;&#38750;&#27861;&#20449;&#24687;&#65289;&#20197;&#21450;&#30456;&#20851;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#22522;&#26412;&#30340;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#24182;&#19981;&#24433;&#21709;&#22240;&#26524;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35774;&#24819;LLM&#28040;&#38500;&#25216;&#26415;&#23558;&#25104;&#20026;LLM&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#21487;&#33021;&#25104;&#20026;&#24320;&#21457;&#26082;&#23433;&#20840;&#12289;&#21487;&#38752;&#21448;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#20840;&#37325;&#35757;&#32451;&#12290;&#25105;&#20204;&#20174;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#24212;&#29992;&#31561;&#26041;&#38754;&#25506;&#32034;&#20102;LLM&#28040;&#38500;&#25216;&#26415;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#29616;&#26377;LLM&#28040;&#38500;&#25216;&#26415;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#20363;&#22914;&#28040;&#38500;&#33539;&#22260;&#12289;&#25968;&#25454;&#27169;&#22411;&#20132;&#20114;&#21644;&#22810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07754</link><description>&lt;p&gt;
&#24605;&#24819;&#20256;&#25773;&#65306;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#22788;&#29702;&#20013;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#30456;&#23545;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;CoT&#26159;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#32500;&#25193;&#25955;&#65288;DoT&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#25512;&#29702;&#27493;&#39588;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#22312;&#26102;&#38388;&#19978;&#20256;&#25773;&#12290;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36880;&#20010;token&#20174;&#24038;&#21040;&#21491;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;DoT&#22312;&#35745;&#31639;&#21644;&#25512;&#29702;&#24615;&#33021;&#20043;&#38388;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DoT&#22312;&#22810;&#20301;&#25968;&#20056;&#27861;&#21644;&#23567;&#23398;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;DoT&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#65292;&#24182;&#20174;&#29616;&#26377;&#30340;&#22686;&#24378;&#25512;&#29702;&#25216;&#26415;&#65288;&#22914;&#33258;&#19968;&#33268;&#35299;&#30721;&#65289;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#21457;&#23637;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.05119</link><description>&lt;p&gt;
&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Limitations of Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;&#26159;&#20351;&#29992;&#25351;&#20196;-&#22238;&#24212;&#23545;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36807;&#31243;&#65292;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#39044;&#35757;&#32451;LLM&#36716;&#21270;&#20026;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;IT&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#24182;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21644;&#19981;&#36275;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#23545;LLM&#36890;&#36807;IT&#21457;&#29983;&#30340;&#21464;&#21270;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;IT&#30340;&#22810;&#31181;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;IT&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#25110;&#25216;&#33021;&#12290;LoRA&#24494;&#35843;&#20165;&#38480;&#20110;&#23398;&#20064;&#22238;&#24212;&#30340;&#21551;&#21160;&#21644;&#26679;&#24335;&#20196;&#29260;&#65292;&#32780;&#20840;&#21442;&#25968;&#24494;&#35843;&#20250;&#23548;&#33268;&#30693;&#35782;&#36864;&#21270;&#12290;&#65288;2&#65289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;IT&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#20250;&#23548;&#33268;&#22238;&#24212;&#36136;&#37327;&#19979;&#38477;&#12290;&#65288;3&#65289;&#20840;&#21442;&#25968;&#24494;&#35843;&#36890;&#36807;&#19981;&#20934;&#30830;&#22320;&#20174;IT&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#27010;&#24565;&#19978;&#30456;&#20284;&#23454;&#20363;&#30340;&#26631;&#35760;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;AI&#31995;&#32479;&#33021;&#22815;&#26381;&#21153;&#20110;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#38656;&#27714;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#40784;&#23450;&#20041;&#21644;&#23454;&#29616;&#22810;&#20803;&#20027;&#20041;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#26469;&#35780;&#20272;&#21644;&#27979;&#35797;&#22810;&#20803;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05070</link><description>&lt;p&gt;
&#36890;&#24448;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
A Roadmap to Pluralistic Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05070
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;AI&#31995;&#32479;&#33021;&#22815;&#26381;&#21153;&#20110;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#38656;&#27714;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#40784;&#23450;&#20041;&#21644;&#23454;&#29616;&#22810;&#20803;&#20027;&#20041;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#26469;&#35780;&#20272;&#21644;&#27979;&#35797;&#22810;&#20803;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26435;&#21147;&#21644;&#26222;&#21450;&#31243;&#24230;&#30340;&#22686;&#21152;&#65292;&#35774;&#35745;&#33021;&#22815;&#20026;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#20154;&#26381;&#21153;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#20197;&#26381;&#21153;&#22810;&#20803;&#20154;&#31867;&#20215;&#20540;&#35266;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20855;&#20307;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#30830;&#23450;&#21644;&#24418;&#24335;&#21270;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#26041;&#24335;&#26469;&#23450;&#20041;&#21644;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#22810;&#20803;&#20027;&#20041;&#65306;1&#65289;Overton&#22810;&#20803;&#27169;&#22411;&#65292;&#23637;&#31034;&#21512;&#29702;&#21453;&#24212;&#30340;&#20809;&#35889;&#65307;2&#65289;&#21487;&#25805;&#25511;&#30340;&#22810;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#35843;&#25972;&#20197;&#21453;&#26144;&#29305;&#23450;&#30340;&#35266;&#28857;&#65307;3&#65289;&#20998;&#24067;&#22810;&#20803;&#27169;&#22411;&#65292;&#22312;&#20998;&#24067;&#20013;&#24456;&#22909;&#22320;&#26657;&#20934;&#32473;&#23450;&#20154;&#32676;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#21644;&#24418;&#24335;&#21270;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#65306;1&#65289;&#22810;&#30446;&#26631;&#22522;&#20934;&#65307;2&#65289;&#26435;&#34913;&#21487;&#25805;&#25511;&#22522;&#20934;&#65292;&#40723;&#21169;&#27169;&#22411;&#23545;&#20219;&#24847;&#26435;&#34913;&#36827;&#34892;&#35843;&#25972;&#65307;3&#65289;&#38506;&#23457;&#22242;&#22810;&#20803;&#22522;&#20934;&#65292;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#19981;&#21516;&#38506;&#23457;&#22242;&#30340;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25945;&#23398;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#25506;&#35752;&#20102;&#36890;&#36807;&#24314;&#35774;&#24615;&#21453;&#39304;&#21644;&#25552;&#31034;&#25351;&#23548;&#23398;&#29983;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#31181;&#23545;&#40784;&#26041;&#27861;&#20197;&#21450;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23545;&#40784;LLM&#65292;&#25552;&#20379;&#26356;&#20248;&#36136;&#30340;&#25945;&#32946;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.05000</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25945;&#23398;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pedagogical Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25945;&#23398;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#25506;&#35752;&#20102;&#36890;&#36807;&#24314;&#35774;&#24615;&#21453;&#39304;&#21644;&#25552;&#31034;&#25351;&#23548;&#23398;&#29983;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#31181;&#23545;&#40784;&#26041;&#27861;&#20197;&#21450;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23545;&#40784;LLM&#65292;&#25552;&#20379;&#26356;&#20248;&#36136;&#30340;&#25945;&#32946;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25945;&#23398;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#36825;&#22312;&#25945;&#32946;&#32972;&#26223;&#19979;&#24212;&#29992;LLM&#20855;&#26377;&#36716;&#21464;&#24615;&#30340;&#24847;&#20041;&#12290;&#19982;&#30452;&#25509;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#19981;&#21516;&#65292;&#25945;&#23398;&#23545;&#40784;&#30340;LLM&#20316;&#20026;&#36741;&#21161;&#24037;&#20855;&#65292;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#21487;&#31649;&#29702;&#30340;&#23376;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24314;&#35774;&#24615;&#30340;&#21453;&#39304;&#21644;&#25552;&#31034;&#25351;&#23548;&#23398;&#29983;&#25214;&#21040;&#26368;&#32456;&#31572;&#26696;&#12290;&#20854;&#30446;&#26631;&#26159;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#35299;&#20915;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#20197;&#21152;&#28145;&#20182;&#20204;&#23545;&#20027;&#39064;&#30340;&#29702;&#35299;&#21644;&#20869;&#21270;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20102;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#65292;&#27809;&#26377;&#23558;&#30446;&#26631;&#23450;&#20041;&#20026;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#26410;&#20351;&#29992;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLHF&#65289;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#40784;&#30340;&#35270;&#35282;&#37325;&#26032;&#35299;&#37322;&#20102;&#36825;&#19968;&#35770;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;RLHF&#26041;&#27861;&#20316;&#20026;&#23545;&#40784;LLM&#30340;&#20248;&#36234;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the novel concept of pedagogically aligned Large Language Models (LLMs) that signifies a transformative shift in the application of LLMs within educational contexts. Rather than providing direct responses to user queries, pedagogically-aligned LLMs function as scaffolding tools, breaking complex problems into manageable subproblems and guiding students towards the final answer through constructive feedback and hints. The objective is to equip learners with problem-solving strategies that deepen their understanding and internalization of the subject matter. Previous research in this field has primarily applied the supervised finetuning approach without framing the objective as an alignment problem, hence not employing reinforcement learning through human feedback (RLHF) methods. This study reinterprets the narrative by viewing the task through the lens of alignment and demonstrates how RLHF methods emerge naturally as a superior alternative for aligning LLM b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#21407;&#22987;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#38405;&#35835;&#29702;&#35299;&#25991;&#26412;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24615;&#33021;&#22987;&#32456;&#24471;&#21040;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2309.09530</link><description>&lt;p&gt;
&#36890;&#36807;&#38405;&#35835;&#29702;&#35299;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models via Reading Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09530
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#21407;&#22987;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#38405;&#35835;&#29702;&#35299;&#25991;&#26412;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24615;&#33021;&#22987;&#32456;&#24471;&#21040;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#35821;&#26009;&#24211;&#19978;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#21407;&#22987;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#36171;&#20104;&#27169;&#22411;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#26497;&#22823;&#22320;&#25439;&#23475;&#20102;&#20854;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#21463;&#20154;&#31867;&#36890;&#36807;&#38405;&#35835;&#29702;&#35299;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#21363;&#38405;&#35835;&#21518;&#32451;&#20064;&#25552;&#39640;&#22522;&#20110;&#25152;&#23398;&#30693;&#35782;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21407;&#22987;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#38405;&#35835;&#29702;&#35299;&#25991;&#26412;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#27599;&#20010;&#21407;&#22987;&#25991;&#26412;&#37117;&#20250;&#34987;&#19968;&#31995;&#21015;&#19982;&#20854;&#20869;&#23481;&#30456;&#20851;&#30340;&#20219;&#21153;&#20016;&#23500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#21487;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#65288;&#29983;&#29289;&#21307;&#23398;&#12289;&#37329;&#34701;&#21644;&#27861;&#24459;&#65289;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#21319;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;7B&#35821;&#35328;&#27169;&#22411;&#22312;&#31454;&#20105;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#19982;&#35268;&#27169;&#26356;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#65288;&#22914;BloombergGPT-50B&#65289;&#30456;&#23218;&#32654;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09530v2 Announce Type: replace  Abstract: We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on the learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specif
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#25552;&#20986;&#20102;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#23450;&#20041;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20998;&#31867;&#20307;&#31995;&#65292;&#20197;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#36827;&#34892;&#20844;&#24179;&#24615;&#20998;&#26512;&#21644;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2309.00770</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#19982;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bias and Fairness in Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#25552;&#20986;&#20102;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#23450;&#20041;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20998;&#31867;&#20307;&#31995;&#65292;&#20197;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#36827;&#34892;&#20844;&#24179;&#24615;&#20998;&#26512;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#20154;&#20204;&#33021;&#22815;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#65292;&#36880;&#28176;&#34701;&#20837;&#35302;&#21450;&#25105;&#20204;&#31038;&#20132;&#39046;&#22495;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23398;&#20064;&#12289;&#24310;&#32493;&#21644;&#25918;&#22823;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#25972;&#21512;&#12289;&#24418;&#24335;&#21270;&#21644;&#25193;&#23637;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#65292;&#23450;&#20041;&#20102;&#20260;&#23475;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#20960;&#20010;&#23454;&#29616;LLMs&#20844;&#24179;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19977;&#20010;&#30452;&#35266;&#30340;&#20998;&#31867;&#20307;&#31995;&#32479;&#19968;&#20102;&#25991;&#29486;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#20559;&#35265;&#35780;&#20272;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#21363;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#32531;&#35299;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00770v2 Announce Type: replace-cross  Abstract: Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26356;&#20840;&#38754;&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#25152;&#26377;&#32452;&#20214;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.17043</link><description>&lt;p&gt;
CRUD-RAG: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20013;&#25991;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models. (arXiv:2401.17043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17043
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26356;&#20840;&#38754;&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#25152;&#26377;&#32452;&#20214;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#28304;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;LLM&#30340;&#24120;&#35265;&#38480;&#21046;&#65292;&#21253;&#25324;&#36807;&#26102;&#30340;&#20449;&#24687;&#21644;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#8220;&#34394;&#26500;&#8221;&#20869;&#23481;&#30340;&#20542;&#21521;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;RAG&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#22312;&#33539;&#22260;&#21644;&#22810;&#26679;&#24615;&#19978;&#23384;&#22312;&#38480;&#21046;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#35780;&#20272;&#38382;&#31572;&#24212;&#29992;&#65292;&#24573;&#35270;&#20102;RAG&#21487;&#33021;&#26377;&#20248;&#21183;&#30340;&#26356;&#24191;&#27867;&#30340;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21482;&#35780;&#20272;RAG&#27969;&#31243;&#20013;LLM&#32452;&#20214;&#30340;&#24615;&#33021;&#65292;&#24182;&#24573;&#35270;&#26816;&#32034;&#32452;&#20214;&#21644;&#22806;&#37096;&#30693;&#35782;&#25968;&#25454;&#24211;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#22312;&#21508;&#31181;RAG&#24212;&#29992;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;RAG&#31995;&#32479;&#30340;&#25152;&#26377;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content. However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous. Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we have categorized the ran
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10712</link><description>&lt;p&gt;
Q&amp;A&#25552;&#31034;&#65306;&#36890;&#36807;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#25552;&#31034;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#28385;&#36275;&#23545;&#22810;&#26679;&#19990;&#30028;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#22238;&#31572;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;AI&#27169;&#22411;&#37197;&#22791;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#30340;&#35748;&#30693;&#26041;&#26696;&#23578;&#26410;&#31995;&#32479;&#22320;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30456;&#20449;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#23613;&#21487;&#33021;&#25910;&#38598;&#32473;&#23450;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#25105;&#20204;&#23558;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#65292;&#26356;&#23481;&#26131;&#22238;&#24518;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#26368;&#32456;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#36825;&#20123;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#25552;&#31034;&#21457;&#36865;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22270;&#20687;-&#31572;&#26696;&#23545;&#21644;&#30456;&#24212;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35757;&#32451;&#19968;&#20010;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#36164;&#28304;&#65292;&#22522;&#20110;135&#20010;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#36523;&#20221;&#32676;&#20307;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#22320;&#29702;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21051;&#26495;&#23646;&#24615;&#22312;&#22270;&#20687;&#20013;&#30340;&#23384;&#22312;&#21487;&#33021;&#24615;&#26159;&#21051;&#26495;&#23646;&#24615;&#30340;&#19977;&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.06310</link><description>&lt;p&gt;
&#36229;&#36234;&#34920;&#38754;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#35270;&#35273;&#21051;&#26495;&#21360;&#35937;&#30340;&#20840;&#29699;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation. (arXiv:2401.06310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#36164;&#28304;&#65292;&#22522;&#20110;135&#20010;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#36523;&#20221;&#32676;&#20307;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#22320;&#29702;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21051;&#26495;&#23646;&#24615;&#22312;&#22270;&#20687;&#20013;&#30340;&#23384;&#22312;&#21487;&#33021;&#24615;&#26159;&#21051;&#26495;&#23646;&#24615;&#30340;&#19977;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#24050;&#32463;&#24378;&#35843;&#20102;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#29983;&#25104;&#30340;&#20154;&#29289;&#24418;&#35937;&#20013;&#23384;&#22312;&#30340;&#19981;&#21516;&#36523;&#20221;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#38480;&#21046;&#65292;&#21253;&#25324;&#22312;&#35780;&#20272;&#20013;&#23545;&#20840;&#29699;&#36523;&#20221;&#32676;&#20307;&#30340;&#35206;&#30422;&#29575;&#26126;&#26174;&#19981;&#36275;&#65292;&#20197;&#21450;&#30456;&#20851;&#21051;&#26495;&#21360;&#35937;&#30340;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#23545;&#26412;&#36136;&#19978;&#26159;&#35270;&#35273;&#21051;&#26495;&#21360;&#35937;&#65288;&#22914;&#8220;&#30246;&#24369;&#8221;&#25110;&#8220;&#22696;&#35199;&#21733;&#33609;&#24125;&#8221;&#65289;&#21644;&#25991;&#21270;&#30456;&#20851;&#30340;&#21051;&#26495;&#21360;&#35937;&#65288;&#22914;&#8220;&#21560;&#24341;&#20154;&#8221;&#25110;&#8220;&#24656;&#24598;&#20998;&#23376;&#8221;&#65289;&#20043;&#38388;&#30340;&#37325;&#35201;&#21306;&#21035;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#36164;&#28304;&#26469;&#23558;&#25105;&#20204;&#23545;&#26469;&#33258;T2I&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#19982;&#22320;&#29702;&#25991;&#21270;&#30456;&#20851;&#30340;&#21051;&#26495;&#21360;&#35937;&#30340;&#35780;&#20272;&#36827;&#34892;&#22522;&#30784;&#32465;&#23450;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#21051;&#26495;&#21360;&#35937;&#22522;&#20934;&#26469;&#35782;&#21035;&#21644;&#35780;&#20272;&#20840;&#29699;&#33539;&#22260;&#20869;&#28041;&#21450;135&#20010;&#22522;&#20110;&#22269;&#31821;&#30340;&#36523;&#20221;&#32676;&#20307;&#30340;&#35270;&#35273;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#22270;&#20687;&#20013;&#23384;&#22312;&#21051;&#26495;&#21360;&#35937;&#30340;&#21487;&#33021;&#24615;&#26159;&#21051;&#26495;&#23646;&#24615;&#30340;&#19977;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have highlighted the issue of stereotypical depictions for people of different identity groups in Text-to-Image (T2I) model generations. However, these existing approaches have several key limitations, including a noticeable lack of coverage of global identity groups in their evaluation, and the range of their associated stereotypes. Additionally, they often lack a critical distinction between inherently visual stereotypes, such as `underweight' or `sombrero', and culturally dependent stereotypes like `attractive' or `terrorist'. In this work, we address these limitations with a multifaceted approach that leverages existing textual resources to ground our evaluation of geo-cultural stereotypes in the generated images from T2I models. We employ existing stereotype benchmarks to identify and evaluate visual stereotypes at a global scale, spanning 135 nationality-based identity groups. We demonstrate that stereotypical attributes are thrice as likely to be present in images
&lt;/p&gt;</description></item><item><title>CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.05544</link><description>&lt;p&gt;
CodePrompt&#65306;&#36890;&#36807;Prompt&#23398;&#20064;&#30340;&#30693;&#35782;&#29305;&#24449;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning. (arXiv:2401.05544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05544
&lt;/p&gt;
&lt;p&gt;
CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CodeBERT&#65289;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;CodeBERT&#30340;&#25991;&#26412;&#23884;&#20837;&#33021;&#21147;&#21644;"[CLS]"&#21477;&#23376;&#23884;&#20837;&#20449;&#24687;&#20316;&#20026;&#19979;&#28216;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#35821;&#20041;&#34920;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#26469;&#25552;&#21462;&#26377;&#25928;&#29305;&#24449;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#33021;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CodePrompt&#65292;&#36890;&#36807;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have explored the potential of utilizing pre-trained language models, such as CodeBERT, to improve source code-related tasks. Previous studies have mainly relied on CodeBERT's text embedding capability and the `[CLS]' sentence embedding information as semantic representations for fine-tuning downstream source code-related tasks. However, these methods require additional neural network layers to extract effective features, resulting in higher computational costs. Furthermore, existing approaches have not leveraged the rich knowledge contained in both source code and related text, which can lead to lower accuracy. This paper presents a novel approach, CodePrompt, which utilizes rich knowledge recalled from a pre-trained model by prompt learning and an attention mechanism to improve source code-related classification tasks. Our approach initially motivates the language model with prompt information to retrieve abundant knowledge associated with the input as representative feat
&lt;/p&gt;</description></item><item><title>DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.12128</link><description>&lt;p&gt;
DiagrammerGPT: &#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12128
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#20351;&#29992;T2I&#27169;&#22411;&#29983;&#25104;&#22270;&#34920;&#26041;&#38754;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22270;&#34920;&#26159;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#20016;&#23500;&#21644;&#31354;&#38388;&#22797;&#26434;&#30340;&#21487;&#35270;&#21270;&#26469;&#35299;&#37322;&#20449;&#24687;&#30340;&#31526;&#21495;/&#31034;&#24847;&#24615;&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#19968;&#31181;&#23494;&#38598;&#30340;&#30456;&#20851;&#23545;&#35937;&#12289;&#25991;&#26412;&#26631;&#31614;&#12289;&#26041;&#21521;&#31661;&#22836;&#12289;&#36830;&#25509;&#32447;&#31561;&#32452;&#21512;&#65289;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#34920;&#26102;&#32463;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#23545;&#35937;&#36890;&#36807;&#22797;&#26434;&#30340;&#20851;&#31995;&#65288;&#22914;&#31661;&#22836;/&#32447;&#65289;&#23494;&#38598;&#36830;&#25509;&#26102;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#23545;&#35937;&#24067;&#23616;&#25511;&#21046;&#65292;&#24182;&#19988;&#32463;&#24120;&#19981;&#33021;&#28210;&#26579;&#20986;&#21487;&#29702;&#35299;&#30340;&#25991;&#26412;&#26631;&#31614;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiagrammerGPT&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;&#22270;&#34920;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLM&#65288;&#22914;GPT-4&#65289;&#30340;&#24067;&#23616;&#24341;&#23548;&#33021;&#21147;&#26469;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#8220;&#22270;&#34920;&#35268;&#21010;&#8221;&#65288;&#22312;&#19968;&#20010;&#35268;&#21010;&#26041;&#26696;&#20013;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;</title><link>http://arxiv.org/abs/2309.15091</link><description>&lt;p&gt;
VideoDirectorGPT: &#36890;&#36807;LLM&#24341;&#23548;&#30340;&#35268;&#21010;&#23454;&#29616;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning. (arXiv:2309.15091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#29983;&#25104;&#21333;&#20010;&#20107;&#20214;&#21644;&#21333;&#19968;&#32972;&#26223;&#30340;&#30701;&#35270;&#39057;&#29255;&#27573;&#65288;&#21363;&#21333;&#22330;&#26223;&#35270;&#39057;&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#24067;&#23616;&#21644;&#25511;&#21046;&#19979;&#28216;&#35270;&#35273;&#27169;&#22359;&#65288;&#22914;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65289;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;&#36825;&#20123;LLMs&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#36827;&#34892;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#21333;&#20010;&#25991;&#26412;&#25552;&#31034;&#36755;&#20837;&#25105;&#20204;&#30340;&#35270;&#39057;&#35268;&#21010;&#22120;LLM&#65288;GPT-4&#65289;&#20013;&#65292;&#23558;&#20854;&#25193;&#23637;&#20026;&#8220;&#35270;&#39057;&#35745;&#21010;&#8221;&#65292;&#20854;&#20013;&#21253;&#25324;&#29983;&#25104;&#22330;&#26223;&#25551;&#36848;&#12289;&#23454;&#20307;&#21450;&#20854;&#24067;&#23616;&#12289;&#27599;&#20010;&#22330;&#26223;&#30340;&#32972;&#26223;&#20197;&#21450;&#20445;&#25345;&#19968;&#33268;&#24615;&#31561;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency 
&lt;/p&gt;</description></item><item><title>BatchPrompt&#26159;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23558;&#22810;&#20010;&#25968;&#25454;&#28857;&#25209;&#37327;&#25171;&#21253;&#21040;&#19968;&#20010;&#25552;&#31034;&#20013;&#26469;&#25552;&#39640;LLM&#30340;&#20196;&#29260;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#20110;&#20196;&#29260;&#35745;&#25968;&#24046;&#24322;&#23548;&#33268;&#30340;&#25104;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#30340;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00384</link><description>&lt;p&gt;
BatchPrompt: &#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#23454;&#29616;&#26356;&#22810;&#20219;&#21153;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
BatchPrompt: Accomplish more with less. (arXiv:2309.00384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00384
&lt;/p&gt;
&lt;p&gt;
BatchPrompt&#26159;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23558;&#22810;&#20010;&#25968;&#25454;&#28857;&#25209;&#37327;&#25171;&#21253;&#21040;&#19968;&#20010;&#25552;&#31034;&#20013;&#26469;&#25552;&#39640;LLM&#30340;&#20196;&#29260;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#20110;&#20196;&#29260;&#35745;&#25968;&#24046;&#24322;&#23548;&#33268;&#30340;&#25104;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;LLM&#65288;Language Model&#65289;&#34987;&#35757;&#32451;&#26469;&#20351;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#23454;&#29616;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#25512;&#29702;&#12290;&#20026;&#36825;&#20123;LLM&#21046;&#20316;&#25552;&#31034;&#36890;&#24120;&#38656;&#35201;&#29992;&#25143;&#25552;&#20379;&#35814;&#32454;&#30340;&#20219;&#21153;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#21644;&#23436;&#25104;&#31034;&#20363;&#20197;&#21450;&#25512;&#29702;&#19978;&#19979;&#25991;&#30340;&#21333;&#20010;&#31034;&#20363;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#24120;&#35268;&#25552;&#31034;&#22522;&#20934;&#31216;&#20026;SinglePrompt&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#20010;&#25512;&#29702;&#25968;&#25454;&#28857;&#19981;&#19968;&#23450;&#24456;&#38271;&#30340;NLP&#20219;&#21153;&#20013;&#65292;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#20196;&#29260;&#35745;&#25968;&#21487;&#33021;&#27604;&#25968;&#25454;&#28857;&#30340;&#20196;&#29260;&#35745;&#25968;&#22823;&#24471;&#22810;&#65292;&#19982;Fine-tuned BERT&#31561;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#23548;&#33268;&#20196;&#29260;&#36164;&#28304;&#21033;&#29992;&#29575;&#38477;&#20302;&#12290;&#36825;&#20010;&#25104;&#26412;&#25928;&#29575;&#38382;&#39064;&#24433;&#21709;&#20102;&#25512;&#29702;&#36895;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#25269;&#28040;&#20102;LLM&#25152;&#33021;&#25552;&#20379;&#30340;&#35768;&#22810;&#22909;&#22788;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;&#22810;&#20010;&#25968;&#25454;&#28857;&#25209;&#37327;&#25171;&#21253;&#21040;&#19968;&#20010;&#25552;&#31034;&#20013;&#26469;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#25552;&#31034;&#31574;&#30053;&#31216;&#20026;BatchPrompt&#12290;&#36825;&#31181;&#31574;&#30053;&#22686;&#21152;&#20102;&#25968;&#25454;&#28857;&#30340;&#23494;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
Many LLMs are trained to perform zero-shot or few-shot inference using instruction-based prompts. Crafting prompts for these LLMs typically requires the user to provide a detailed task description, examples of context and completion, and single example of context for inference. This regular prompt baseline is referred to as SinglePrompt in this paper. However, for NLP tasks where each data point for inference is not necessarily lengthy, the token count for instructions and few-shot examples in the prompt may be considerably larger than that of the data point, resulting in lower token-resource utilization compared with encoder-based models like fine-tuned BERT. This cost-efficiency issue, affecting inference speed and compute budget, counteracts the many benefits LLMs have to offer. This paper aims to alleviate the preceding problem by batching multiple data points into a single prompt, a prompting strategy we refer to as BatchPrompt. This strategy increases the density of data points, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10864</link><description>&lt;p&gt;
&#23558;&#27880;&#24847;&#21147;&#20998;&#21106;&#19982;&#32465;&#23450;&#29992;&#20110;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#36924;&#30495;&#30340;&#21387;&#20498;&#24615;&#32467;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23436;&#20840;&#20381;&#29031;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#8212;&#8212;&#20851;&#27880;&#19982;&#28608;&#21457;&#65292;&#24341;&#20837;&#20102;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#65288;GSN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#26029;&#26102;&#20248;&#21270;&#36328;&#27880;&#24847;&#21147;&#20197;&#26356;&#22909;&#22320;&#34701;&#20837;&#35821;&#20041;&#12290;&#23427;&#22312;&#29983;&#25104;&#31616;&#21333;&#25552;&#31034;&#65292;&#22914;&#8220;&#19968;&#21482;&#29483;&#21644;&#19968;&#21482;&#29399;&#8221;&#65292;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;&#20197;&#21450;&#35299;&#20915;&#19981;&#36866;&#24403;&#30340;&#23646;&#24615;&#32465;&#23450;&#38382;&#39064;&#26041;&#38754;&#30340;&#21151;&#25928;&#26377;&#25152;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#25552;&#31034;&#25110;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#25913;&#36827;&#30340;&#23646;&#24615;&#32465;&#23450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#21106;&#19982;&#32465;&#23450;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;GSN&#25439;&#22833;&#30446;&#26631;&#65306;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20002;&#22833;&#21644;&#19968;&#31181;&#32465;&#23450;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#32435;&#20837;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#29305;&#28857;&#19978;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend &amp; Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide &amp; Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
&lt;/p&gt;</description></item><item><title>pysentimiento&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35266;&#28857;&#25366;&#25496;&#21644;&#31038;&#20132;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#24211;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2106.09462</link><description>&lt;p&gt;
pysentimiento: &#19968;&#20010;&#29992;&#20110;&#35266;&#28857;&#25366;&#25496;&#21644;&#31038;&#20132;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;Python&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks. (arXiv:2106.09462v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.09462
&lt;/p&gt;
&lt;p&gt;
pysentimiento&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35266;&#28857;&#25366;&#25496;&#21644;&#31038;&#20132;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#24211;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#25552;&#21462;&#35266;&#28857;&#21644;&#20449;&#24687;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#20013;&#20869;&#23481;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#31038;&#20250;&#30740;&#31350;&#20154;&#21592;&#22312;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#24037;&#20855;&#36827;&#34892;&#36825;&#20123;&#20219;&#21153;&#26102;&#20250;&#36935;&#21040;&#19968;&#20123;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#24037;&#20855;&#36890;&#24120;&#33853;&#21518;&#20110;&#21830;&#19994;API&#65292;&#19981;&#36866;&#29992;&#20110;&#38500;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#65292;&#25110;&#32773;&#23545;&#38750;&#19987;&#23478;&#26469;&#35828;&#38750;&#24120;&#22797;&#26434;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;pysentimiento&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;Python&#24037;&#20855;&#21253;&#65292;&#19987;&#20026;&#35266;&#28857;&#25366;&#25496;&#21644;&#20854;&#20182;&#31038;&#20132;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#32780;&#35774;&#35745;&#12290;&#36825;&#20010;&#24320;&#28304;&#24211;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#33889;&#33796;&#29273;&#35821;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21487;&#20197;&#35753;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#35821;&#35328;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#32467;&#26524;&#20844;&#24179;&#24615;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the extraction of opinions and information from user-generated text has attracted a lot of interest, largely due to the unprecedented volume of content in Social Media. However, social researchers face some issues in adopting cutting-edge tools for these tasks, as they are usually behind commercial APIs, unavailable for other languages than English, or very complex to use for non-experts. To address these issues, we present pysentimiento, a comprehensive multilingual Python toolkit designed for opinion mining and other Social NLP tasks. This open-source library brings state-of-the-art models for Spanish, English, Italian, and Portuguese in an easy-to-use Python library, allowing researchers to leverage these techniques. We present a comprehensive assessment of performance for several pre-trained language models across a variety of tasks, languages, and datasets, including an evaluation of fairness in the results.
&lt;/p&gt;</description></item></channel></rss>