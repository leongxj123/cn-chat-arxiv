<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29436;&#20154;&#28216;&#25103;&#27169;&#25311;&#24179;&#21488;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35266;&#28857;&#39046;&#23548;&#20316;&#29992;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2404.01602</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29436;&#20154;&#28216;&#25103;&#20013;&#30340;&#33333;&#25163;&#65311;&#35780;&#20272;&#20854;&#35266;&#28857;&#24341;&#39046;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29436;&#20154;&#28216;&#25103;&#27169;&#25311;&#24179;&#21488;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35266;&#28857;&#39046;&#23548;&#20316;&#29992;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#38590;&#24536;&#30340;&#25112;&#30053;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;LLM&#20195;&#29702;&#25152;&#23637;&#31034;&#30340;&#35266;&#28857;&#39046;&#23548;&#21147;&#30340;&#37325;&#35201;&#24615;&#34987;&#24573;&#35270;&#20102;&#65292;&#32780;&#36825;&#23545;&#20110;&#22810;&#26234;&#33021;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#35774;&#32622;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27492;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#29436;&#20154;&#28216;&#25103;&#20316;&#20026;&#27169;&#25311;&#24179;&#21488;&#65292;&#35780;&#20272;LLMs&#30340;&#35266;&#28857;&#24341;&#39046;&#20316;&#29992;&#12290;&#35813;&#28216;&#25103;&#20013;&#26377;&#35686;&#38271;&#35282;&#33394;&#65292;&#36127;&#36131;&#24635;&#32467;&#35770;&#25454;&#24182;&#25512;&#33616;&#20915;&#31574;&#36873;&#39033;&#65292;&#22240;&#27492;&#21487;&#20316;&#20026;&#35266;&#28857;&#39046;&#34966;&#30340;&#21487;&#20449;&#20195;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25972;&#21512;&#20102;&#35686;&#38271;&#35282;&#33394;&#30340;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#20010;&#22522;&#20110;&#35266;&#28857;&#39046;&#34966;&#20851;&#38190;&#29305;&#24449;&#30340;&#26032;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#31532;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#34913;&#37327;&#35266;&#28857;&#39046;&#34966;&#30340;&#21487;&#38752;&#24615;&#65292;&#31532;&#20108;&#20010;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01602v1 Announce Type: cross  Abstract: Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by LLM-based agents has been overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs. The game features the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics for evaluation based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the 
&lt;/p&gt;</description></item><item><title>GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05527</link><description>&lt;p&gt;
GEAR: &#19968;&#31181;&#29992;&#20110;&#20960;&#20046;&#26080;&#25439;&#29983;&#25104;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05527
&lt;/p&gt;
&lt;p&gt;
GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#24555;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#26029;&#29983;&#25104;&#36895;&#24230;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#38271;&#30340;&#32531;&#23384;&#38656;&#27714;&#24050;&#23558;LLM&#25512;&#26029;&#36716;&#21464;&#20026;&#19968;&#20010;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#65292;&#26174;&#33879;&#22320;&#38480;&#21046;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#26631;&#35760;&#25110;&#22343;&#21248;&#37327;&#21270;&#25152;&#26377;&#26465;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#30697;&#38453;&#12290;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27599;&#20010;&#27493;&#39588;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#37325;&#22823;&#20559;&#24046;&#21644;&#24615;&#33021;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GEAR&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35299;&#30721;&#22120;&#20197;&#21450;&#33258;&#25105;&#35843;&#33410;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;TEncDM&#30340;&#25991;&#26412;&#32534;&#30721;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#20004;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2402.19097</link><description>&lt;p&gt;
TEncDM: &#22312;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#20013;&#29702;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19097
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35299;&#30721;&#22120;&#20197;&#21450;&#33258;&#25105;&#35843;&#33410;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;TEncDM&#30340;&#25991;&#26412;&#32534;&#30721;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#20004;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#35768;&#22810;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#20854;&#24212;&#29992;&#20110;&#25991;&#26412;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#20294;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#23545;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#20214;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Text Encoding Diffusion Model (TEncDM)&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#20013;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#36890;&#24120;&#20351;&#29992;&#30340;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35299;&#30721;&#22120;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#36827;&#34892;&#25991;&#26412;&#37325;&#26500;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#33258;&#25105;&#35843;&#33410;&#65292;&#24182;&#21457;&#29616;&#36825;&#20250;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#30340;&#25968;&#37327;&#32423;&#65292;&#20174;&#32780;&#20943;&#23569;&#25512;&#29702;&#38454;&#27573;&#30340;&#21435;&#22122;&#27493;&#39588;&#25968;&#37327;&#12290;&#22312;&#20004;&#20010;&#19979;&#28216;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;QQP&#21644;XSum&#19978;&#23545;TEncDM&#30340;&#35780;&#20272;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19097v1 Announce Type: new  Abstract: Drawing inspiration from the success of diffusion models in various domains, numerous research papers proposed methods for adapting them to text data. Despite these efforts, none of them has managed to achieve the quality of the large language models. In this paper, we conduct a comprehensive analysis of key components of the text diffusion models and introduce a novel approach named Text Encoding Diffusion Model (TEncDM). Instead of the commonly used token embedding space, we train our model in the space of the language model encodings. Additionally, we propose to use a Transformer-based decoder that utilizes contextual information for text reconstruction. We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage. Evaluation of TEncDM on two downstream text generation tasks, QQP and XSum, demonstrates its superiority ove
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PsychoGAT&#65288;&#24515;&#29702;&#28216;&#25103;&#20195;&#29702;&#65289;&#20197;&#23454;&#29616;&#24515;&#29702;&#35780;&#20272;&#30340;&#36890;&#29992;&#28216;&#25103;&#21270;&#65292;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;LLM&#20195;&#29702;&#32435;&#20837;&#35282;&#33394;&#65292;&#23558;&#26631;&#20934;&#37327;&#34920;&#36716;&#21270;&#20026;&#20010;&#24615;&#21270;&#19988;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#12290;</title><link>https://arxiv.org/abs/2402.12326</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#24515;&#29702;&#23398;&#26234;&#33021;&#20195;&#29702;&#65306;&#19968;&#39033;&#20851;&#20110;&#28216;&#25103;&#21270;&#35780;&#20272;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM Agents for Psychology: A Study on Gamified Assessments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PsychoGAT&#65288;&#24515;&#29702;&#28216;&#25103;&#20195;&#29702;&#65289;&#20197;&#23454;&#29616;&#24515;&#29702;&#35780;&#20272;&#30340;&#36890;&#29992;&#28216;&#25103;&#21270;&#65292;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;LLM&#20195;&#29702;&#32435;&#20837;&#35282;&#33394;&#65292;&#23558;&#26631;&#20934;&#37327;&#34920;&#36716;&#21270;&#20026;&#20010;&#24615;&#21270;&#19988;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#27979;&#37327;&#23545;&#20110;&#31934;&#31070;&#20581;&#24247;&#12289;&#33258;&#25105;&#29702;&#35299;&#21644;&#20010;&#20154;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#33258;&#25105;&#25253;&#21578;&#37327;&#34920;&#21644;&#24515;&#29702;&#23398;&#23478;&#35775;&#35848;&#65292;&#24120;&#24120;&#38754;&#20020;&#21442;&#19982;&#24230;&#21644;&#21487;&#33719;&#24471;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#24050;&#32463;&#25506;&#35752;&#20102;&#22522;&#20110;&#28216;&#25103;&#21644;LLM&#30340;&#24037;&#20855;&#26469;&#25552;&#39640;&#29992;&#25143;&#20852;&#36259;&#24182;&#33258;&#21160;&#21270;&#35780;&#20272;&#65292;&#20294;&#23427;&#20204;&#38590;&#20197;&#24179;&#34913;&#21442;&#19982;&#24230;&#21644;&#26222;&#36866;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PsychoGAT&#65288;&#24515;&#29702;&#28216;&#25103;&#20195;&#29702;&#65289;&#65292;&#20197;&#23454;&#29616;&#24515;&#29702;&#35780;&#20272;&#30340;&#36890;&#29992;&#28216;&#25103;&#21270;&#12290;&#20027;&#35201;&#27934;&#23519;&#26159;&#24378;&#22823;&#30340;LLM&#26082;&#21487;&#20197;&#20805;&#24403;&#29087;&#32451;&#30340;&#24515;&#29702;&#23398;&#23478;&#65292;&#20063;&#21487;&#20197;&#26159;&#21019;&#26032;&#30340;&#28216;&#25103;&#35774;&#35745;&#24072;&#12290;&#36890;&#36807;&#23558;LLM&#20195;&#29702;&#32435;&#20837;&#25351;&#23450;&#35282;&#33394;&#24182;&#31934;&#24515;&#31649;&#29702;&#23427;&#20204;&#30340;&#20114;&#21160;&#65292;PsychoGAT&#21487;&#20197;&#23558;&#20219;&#20309;&#26631;&#20934;&#37327;&#34920;&#36716;&#21270;&#20026;&#20010;&#24615;&#21270;&#19988;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#12290;&#20026;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#24515;&#29702;&#24230;&#37327;&#35780;&#20272;&#20197;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#20351;&#29992;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12326v1 Announce Type: new  Abstract: Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ huma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.01805</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22270;&#25512;&#29702;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limitations of Graph Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20165;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#30340;&#25552;&#31034;&#23601;&#23637;&#31034;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22270;&#25512;&#29702;&#38382;&#39064;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65292;GPT-3.5&#65292;Claude-2&#65292;Llama-2&#21644;Palm-2&#65289;&#30340;&#25512;&#29702;&#28145;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;10&#20010;&#19981;&#21516;&#30340;&#22270;&#36941;&#21382;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#20195;&#34920;&#30528;&#36880;&#27493;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#22270;&#22823;&#23567;&#20197;&#21450;&#19981;&#21516;&#24418;&#24335;&#30340;k-shot&#25552;&#31034;&#30340;&#35774;&#32622;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#36807;&#31243;&#65292;&#25105;&#20204;&#20984;&#26174;&#20102;LLMs&#30340;&#21508;&#31181;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#65292;&#27604;&#22914;&#19982;&#27599;&#20010;&#33410;&#28857;&#30340;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#30340;&#25972;&#20307;&#36127;&#38754;&#24433;&#21709;&#65292;&#20197;&#21450;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#23548;&#33268;LLMs&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Large Language Models have demonstrated various types of reasoning capabilities through language-based prompts alone. However, in this paper, we test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In particular, we design 10 distinct problems of graph traversal, each representing increasing levels of complexity. Further, we analyze the performance of models across various settings such as varying sizes of graphs as well as different forms of k-shot prompting. We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution. Finally, we propose a new prompting technique specially designed f
&lt;/p&gt;</description></item></channel></rss>