<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#23454;&#38469;&#20250;&#35758;&#24212;&#29992;&#20013;Speaker-Attributed ASR&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#36755;&#20986;&#26469;&#24494;&#35843;&#27169;&#22411;&#20197;&#20943;&#23569;&#35828;&#35805;&#32773;&#38169;&#35823;&#29575;&#65292;&#24182;&#25506;&#35752;&#20102;&#22686;&#24378;&#35828;&#35805;&#32773;&#23884;&#20837;&#27169;&#26495;&#25552;&#21462;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.06570</link><description>&lt;p&gt;
&#25913;&#36827;&#23454;&#38469;&#20250;&#35758;&#24212;&#29992;&#20013;&#35828;&#35805;&#32773;&#24402;&#22240;ASR&#20013;&#30340;&#35828;&#35805;&#32773;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Improving Speaker Assignment in Speaker-Attributed ASR for Real Meeting Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06570
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#23454;&#38469;&#20250;&#35758;&#24212;&#29992;&#20013;Speaker-Attributed ASR&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#36755;&#20986;&#26469;&#24494;&#35843;&#27169;&#22411;&#20197;&#20943;&#23569;&#35828;&#35805;&#32773;&#38169;&#35823;&#29575;&#65292;&#24182;&#25506;&#35752;&#20102;&#22686;&#24378;&#35828;&#35805;&#32773;&#23884;&#20837;&#27169;&#26495;&#25552;&#21462;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#31471;&#21040;&#31471;&#20250;&#35758;&#36716;&#24405;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#26550;&#26500;&#19978;&#65292;&#24182;&#19988;&#22823;&#22810;&#25968;&#26159;&#22312;&#27169;&#25311;&#20250;&#35758;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26088;&#22312;&#20248;&#21270;Speaker-Attributed ASR (SA-ASR)&#31995;&#32479;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65288;&#22914;AMI&#20250;&#35758;&#35821;&#26009;&#24211;&#65289;&#20351;&#29992;&#30340;&#30740;&#31350;&#65292;&#20197;&#25913;&#36827;&#35821;&#38899;&#27573;&#30340;&#35828;&#35805;&#32773;&#20998;&#37197;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#28041;&#21450;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;VAD&#65289;&#12289;&#35828;&#35805;&#32773;&#36776;&#35782;&#65288;SD&#65289;&#21644;SA-ASR&#30340;&#29616;&#23454;&#29983;&#27963;&#24212;&#29992;&#30340;&#27969;&#31243;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20513;&#20351;&#29992;VAD&#36755;&#20986;&#27573;&#26469;&#24494;&#35843;SA-ASR&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#22312;&#27979;&#35797;&#26399;&#38388;&#23427;&#20063;&#34987;&#24212;&#29992;&#20110;VAD&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#23548;&#33268;&#35828;&#35805;&#32773;&#38169;&#35823;&#29575;&#65288;SER&#65289;&#30456;&#23545;&#20943;&#23569;&#39640;&#36798;28&#65285;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#22686;&#24378;&#20174;SD&#36755;&#20986;&#20013;&#25552;&#21462;&#35828;&#35805;&#32773;&#23884;&#20837;&#27169;&#26495;&#30340;&#31574;&#30053;&#65292;&#36825;&#20123;&#27169;&#26495;&#20316;&#20026;SA-ASR&#31995;&#32479;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23558;&#23427;&#20204;&#20174;SD&#36755;&#20986;&#20013;&#25552;&#21462;&#32780;&#19981;&#26159;&#20174;&#27880;&#37322;&#30340;&#35828;&#35805;&#32773;&#27573;&#20013;&#25552;&#21462;&#65292;&#20250;&#23548;&#33268;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06570v1 Announce Type: new  Abstract: Past studies on end-to-end meeting transcription have focused on model architecture and have mostly been evaluated on simulated meeting data. We present a novel study aiming to optimize the use of a Speaker-Attributed ASR (SA-ASR) system in real-life scenarios, such as the AMI meeting corpus, for improved speaker assignment of speech segments. First, we propose a pipeline tailored to real-life applications involving Voice Activity Detection (VAD), Speaker Diarization (SD), and SA-ASR. Second, we advocate using VAD output segments to fine-tune the SA-ASR model, considering that it is also applied to VAD segments during test, and show that this results in a relative reduction of Speaker Error Rate (SER) up to 28%. Finally, we explore strategies to enhance the extraction of the speaker embedding templates used as inputs by the SA-ASR system. We show that extracting them from SD output rather than annotated speaker segments results in a rela
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#24211;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#29983;&#25104;NL-GQL&#25968;&#25454;&#23545;&#24182;&#24494;&#35843;LLMs&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.16567</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models to a Domain-specific Graph Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#24211;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#29983;&#25104;NL-GQL&#25968;&#25454;&#23545;&#24182;&#24494;&#35843;LLMs&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#24211;&#65288;Graph DB&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#37329;&#34701;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#21307;&#33647;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#36716;&#25442;&#20026;&#22270;&#26597;&#35810;&#35821;&#35328;&#65288;GQL&#65289;&#65292;&#36890;&#24120;&#31216;&#20026;NL2GQL&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#19987;&#19994;&#21270;&#29305;&#24615;&#32780;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#20123;&#26041;&#27861;&#35797;&#22270;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#31867;&#20284;&#30340;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#36716;SQL&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;NL2GQL&#20219;&#21153;&#20013;&#65292;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;NL-GQL&#25968;&#25454;&#23545;&#20351;&#24471;&#38590;&#20197;&#24314;&#31435;LLMs&#21644;&#22270;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#27969;&#27700;&#32447;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;ChatGPT&#22522;&#20110;&#32473;&#23450;&#30340;&#22270;&#25968;&#25454;&#24211;&#33258;&#25105;&#29983;&#25104;NL-GQL&#25968;&#25454;&#23545;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21019;&#24314;&#30340;&#25968;&#25454;&#26469;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;LLMs&#19982;&#22270;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16567v1 Announce Type: new  Abstract: Graph Databases (Graph DB) are widely applied in various fields, including finance, social networks, and medicine. However, translating Natural Language (NL) into the Graph Query Language (GQL), commonly known as NL2GQL, proves to be challenging due to its inherent complexity and specialized nature. Some approaches have sought to utilize Large Language Models (LLMs) to address analogous tasks like text2SQL. Nevertheless, when it comes to NL2GQL taskson a particular domain, the absence of domain-specific NL-GQL data pairs makes it difficult to establish alignment between LLMs and the graph DB. To address this challenge, we propose a well-defined pipeline. Specifically, we utilize ChatGPT to create NL-GQL data pairs based on the given graph DB with self-instruct. Then, we use the created data to fine-tune LLMs, thereby achieving alignment between LLMs and the graph DB. Additionally, during inference, we propose a method that extracts relev
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#26631;&#27880;&#32773;&#35843;&#36866;&#26469;&#23454;&#29616;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#36827;&#34892;&#39640;&#25928;&#26631;&#27880;&#19982;&#24314;&#27169;&#65292;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#26631;&#27880;&#39044;&#31639;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14101</link><description>&lt;p&gt;
&#36890;&#36807;&#23569;&#26679;&#26412;&#26631;&#27880;&#32773;&#35843;&#36866;&#23454;&#29616;&#39640;&#25928;&#20027;&#35266;&#20219;&#21153;&#26631;&#27880;&#19982;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14101
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#26631;&#27880;&#32773;&#35843;&#36866;&#26469;&#23454;&#29616;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#36827;&#34892;&#39640;&#25928;&#26631;&#27880;&#19982;&#24314;&#27169;&#65292;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#26631;&#27880;&#39044;&#31639;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20027;&#35266;&#24615;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#19981;&#23384;&#22312;&#21333;&#19968;&#30340;&#26631;&#20934;&#31572;&#26696;&#65292;&#21253;&#25324;&#19981;&#21516;&#26631;&#27880;&#32773;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#20182;&#20204;&#29420;&#29305;&#30340;&#35270;&#35282;&#26174;&#33879;&#24433;&#21709;&#26631;&#27880;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26631;&#27880;&#39044;&#31639;&#36890;&#24120;&#25104;&#20026;&#20915;&#23450;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#35270;&#35282;&#25968;&#37327;&#65288;&#21363;&#26631;&#27880;&#32773;&#65289;&#21450;&#21518;&#32493;&#24314;&#27169;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#36827;&#34892;&#26631;&#27880;&#25910;&#38598;&#21644;&#24314;&#27169;&#65292;&#26088;&#22312;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#26631;&#27880;&#39044;&#31639;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20004;&#38454;&#27573;&#35774;&#35745;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#20381;&#36182;&#19968;&#23567;&#32452;&#26631;&#27880;&#32773;&#26469;&#26500;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#27599;&#20010;&#26631;&#27880;&#32773;&#31574;&#30053;&#24615;&#22320;&#26631;&#27880;&#23569;&#37327;&#26679;&#26412;&#65292;&#26469;&#20026;&#26032;&#35270;&#35282;&#22686;&#24378;&#27169;&#22411;&#12290;&#20026;&#20102;&#22312;&#35268;&#27169;&#19978;&#27979;&#35797;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#12298;&#36947;&#24503;&#22522;&#30784;&#20027;&#35266;&#35821;&#26009;&#24211;&#12299;&#65292;&#21253;&#21547;2000&#20010;Reddit&#24086;&#23376;&#65292;&#30001;24&#21517;&#26631;&#27880;&#32773;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14101v1 Announce Type: new  Abstract: In subjective NLP tasks, where a single ground truth does not exist, the inclusion of diverse annotators becomes crucial as their unique perspectives significantly influence the annotations. In realistic scenarios, the annotation budget often becomes the main determinant of the number of perspectives (i.e., annotators) included in the data and subsequent modeling. We introduce a novel framework for annotation collection and modeling in subjective tasks that aims to minimize the annotation budget while maximizing the predictive performance for each annotator. Our framework has a two-stage design: first, we rely on a small set of annotators to build a multitask model, and second, we augment the model for a new perspective by strategically annotating a few samples per annotator. To test our framework at scale, we introduce and release a unique dataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by 24 annotators for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32676;&#32452;&#21644;&#23545;&#31216;&#24615;&#21407;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#30740;&#31350;&#22235;&#20010;&#32676;&#32452;&#23646;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#20445;&#25345;&#32676;&#32452;&#23646;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.06120</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32676;&#32452;&#21644;&#23545;&#31216;&#24615;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
Exploring Group and Symmetry Principles in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32676;&#32452;&#21644;&#23545;&#31216;&#24615;&#21407;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#30740;&#31350;&#22235;&#20010;&#32676;&#32452;&#23646;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#20445;&#25345;&#32676;&#32452;&#23646;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#35780;&#20272;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;&#32676;&#32452;&#21644;&#23545;&#31216;&#24615;&#21407;&#29702;&#20026;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#36825;&#20123;&#21407;&#29702;&#22312;&#29289;&#29702;&#23398;&#21644;&#25968;&#23398;&#31561;&#39046;&#22495;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#35780;&#20272;&#23427;&#20204;&#33021;&#21147;&#30340;&#26041;&#24335;&#12290;&#34429;&#28982;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#20026;&#20102;&#23637;&#31034;&#20351;&#29992;&#36825;&#20123;&#23646;&#24615;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#20851;&#27880;&#31639;&#26415;&#25512;&#29702;&#65292;&#24182;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#32676;&#32452;&#23646;&#24615;&#65288;&#23553;&#38381;&#24615;&#12289;&#24658;&#31561;&#24615;&#12289;&#36870;&#24615;&#21644;&#32467;&#21512;&#24615;&#65289;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#30740;&#31350;&#30340;LLM&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#26041;&#26696;&#20013;&#38590;&#20197;&#20445;&#25345;&#32676;&#32452;&#23646;&#24615;&#12290;&#22312;&#23553;&#38381;&#24615;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23545;&#29305;&#23450;&#36755;&#20986;&#30340;&#20559;&#35265;&#65292;&#24182;&#22312;&#29305;&#23450;&#30340;&#24207;&#21015;&#38271;&#24230;&#21518;&#20174;100&#65285;&#30340;&#24615;&#33021;&#36805;&#36895;&#19979;&#38477;&#21040;0&#65285;&#12290;&#23427;&#20204;&#22312;&#24658;&#31561;&#24615;&#27979;&#35797;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#20195;&#34920;&#20102;&#30456;&#21152;&#24471;&#21040;&#21407;&#25968;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive performance across a wide range of applications; however, assessing their reasoning capabilities remains a significant challenge. In this paper, we introduce a framework grounded in group and symmetry principles, which have played a crucial role in fields such as physics and mathematics, and offer another way to evaluate their capabilities. While the proposed framework is general, to showcase the benefits of employing these properties, we focus on arithmetic reasoning and investigate the performance of these models on four group properties: closure, identity, inverse, and associativity. Our findings reveal that LLMs studied in this work struggle to preserve group properties across different test regimes. In the closure test, we observe biases towards specific outputs and an abrupt degradation in their performance from 100% to 0% after a specific sequence length. They also perform poorly in the identity test, which represents add
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.01695</link><description>&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language-Guided World Models: A Model-Based Approach to AI Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01695
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#23433;&#35013;&#21040;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20013;&#65292;&#20026;&#20154;&#31867;&#19982;&#36825;&#20123;&#20195;&#29702;&#27807;&#36890;&#21644;&#25511;&#21046;&#25171;&#24320;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#28192;&#36947;&#12290;&#38500;&#20102;&#26356;&#26032;&#20195;&#29702;&#31574;&#30053;&#65292;&#20154;&#31867;&#36824;&#21487;&#20197;&#20462;&#25913;&#20182;&#20204;&#30340;&#20869;&#37096;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#24433;&#21709;&#20195;&#29702;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#38590;&#20197;&#36866;&#24212;&#20154;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#33258;&#28982;&#30340;&#36890;&#20449;&#30028;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;&#23427;&#20204;&#36824;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#26368;&#21021;&#29992;&#20110;&#25351;&#23548;&#20154;&#31867;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#33258;&#25105;&#23398;&#20064;&#12290;&#20026;&#20102;&#20419;&#36827;LWMs&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;MESSENGER&#28216;&#25103;&#65288;Hanjie&#31561;&#20154;&#65292;2021&#65289;&#30340;&#25361;&#25112;&#22522;&#20934;&#65292;&#38656;&#35201;&#23545;&#26032;&#22330;&#26223;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new l
&lt;/p&gt;</description></item><item><title>Kun&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#36890;&#36807;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#25913;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#12290;&#23427;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#25552;&#39640;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20943;&#23569;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2401.06477</link><description>&lt;p&gt;
Kun: &#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#30340;&#20013;&#22269;&#33258;&#23545;&#40784;&#38382;&#39064;&#30340;&#31572;&#26696;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation. (arXiv:2401.06477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06477
&lt;/p&gt;
&lt;p&gt;
Kun&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#36890;&#36807;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#25913;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#12290;&#23427;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#25552;&#39640;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20943;&#23569;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Kun&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;Kun&#21033;&#29992;&#26469;&#33258;&#21566;&#36947;&#12289;&#23436;&#21367;&#21644;SkyPile&#31561;&#22810;&#20010;&#26469;&#28304;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#37319;&#29992;&#22522;&#20110;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#20013;&#25991;&#25351;&#23548;&#25968;&#25454;&#28857;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#23436;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#65292;&#26174;&#33879;&#20559;&#31163;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;6B&#21442;&#25968;&#30340;Yi&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;Kun&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#19988;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#23545;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#36825;&#31181;&#26041;&#27861;ological&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20013;&#25991;&#33258;&#23545;&#40784;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Kun, a novel approach for creating high-quality instruction-tuning datasets for large language models (LLMs) without relying on manual annotations. Adapting a self-training algorithm based on instruction back-translation and answer polishment, Kun leverages unlabelled data from diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial dataset of over a million Chinese instructional data points. This approach significantly deviates from traditional methods by using a self-curation process to refine and select the most effective instruction-output pairs. Our experiments with the 6B-parameter Yi model across various benchmarks demonstrate Kun's robustness and scalability. Our method's core contributions lie in its algorithmic advancement, which enhances data retention and clarity, and its innovative data generation approach that substantially reduces the reliance on costly and time-consuming manual annotations. This methodology presents a sc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14735</link><description>&lt;p&gt;
&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#28508;&#21147;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. (arXiv:2310.14735v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25552;&#31034;&#24037;&#31243;&#26159;&#20026;LLM&#26500;&#24314;&#36755;&#20837;&#25991;&#26412;&#30340;&#36807;&#31243;&#65292;&#26159;&#20248;&#21270;LLM&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#38416;&#26126;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#35282;&#33394;&#25552;&#31034;&#12289;&#19968;&#27425;&#24615;&#25552;&#31034;&#21644;&#23569;&#37327;&#25552;&#31034;&#65292;&#20197;&#21450;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#65292;&#22914;&#24605;&#32500;&#38142;&#21644;&#24605;&#32500;&#26641;&#25552;&#31034;&#12290;&#26412;&#25991;&#36824;&#38416;&#36848;&#20102;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#27492;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21246;&#21202;&#20102;&#25552;&#31034;&#24037;&#31243;&#30740;&#31350;&#30340;&#21069;&#26223;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#23545;&#32467;&#26500;&#21644;&#20195;&#29702;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#24037;&#20855;&#20013;&#30340;&#20316;&#29992;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;&#35282;&#24230;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#35780;&#20272;&#25552;&#31034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23637;&#26395;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#27169;&#22411;PESTS&#65292;&#24182;&#36890;&#36807;&#27874;&#26031;&#35821;-&#33521;&#35821;&#30340;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07893</link><description>&lt;p&gt;
PESTS: &#27874;&#26031;&#35821;-&#33521;&#35821;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#29992;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity. (arXiv:2305.07893v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#27169;&#22411;PESTS&#65292;&#24182;&#36890;&#36807;&#27874;&#26031;&#35821;-&#33521;&#35821;&#30340;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22791;&#21463;&#20851;&#27880;&#30340;&#32452;&#20214;&#12290;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#35780;&#20272;&#21333;&#35789;&#12289;&#30701;&#35821;&#12289;&#27573;&#33853;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24456;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#35201;&#27714;&#22312;&#28304;&#21644;&#30446;&#26631;&#35821;&#35328;&#20013;&#25552;&#20379;&#20855;&#26377;&#19968;&#23450;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23545;&#12290;&#35768;&#22810;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#27169;&#22411;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#26469;&#24357;&#34917;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#19981;&#21487;&#29992;&#30340;&#19981;&#36275;&#65292;&#20294;&#26426;&#22120;&#32763;&#35793;&#30340;&#35823;&#24046;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24230;&#29305;&#24449;&#23454;&#29616;&#26426;&#22120;&#32763;&#35793;&#26102;&#65292;&#29992;&#30456;&#21516;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the components of natural language processing that has received a lot of investigation recently is semantic textual similarity. In computational linguistics and natural language processing, assessing the semantic similarity of words, phrases, paragraphs, and texts is crucial. Calculating the degree of semantic resemblance between two textual pieces, paragraphs, or phrases provided in both monolingual and cross-lingual versions is known as semantic similarity. Cross lingual semantic similarity requires corpora in which there are sentence pairs in both the source and target languages with a degree of semantic similarity between them. Many existing cross lingual semantic similarity models use a machine translation due to the unavailability of cross lingual semantic similarity dataset, which the propagation of the machine translation error reduces the accuracy of the model. On the other hand, when we want to use semantic similarity features for machine translation the same machine t
&lt;/p&gt;</description></item></channel></rss>