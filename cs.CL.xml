<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Latxa&#26159;&#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#25152;&#26377;&#20197;&#21069;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.20266</link><description>&lt;p&gt;
Latxa: &#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Latxa: An Open Language Model and Evaluation Suite for Basque
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20266
&lt;/p&gt;
&lt;p&gt;
Latxa&#26159;&#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#25152;&#26377;&#20197;&#21069;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Latxa&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Llama 2&#30340;&#22823;&#22411;&#24052;&#26031;&#20811;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;7&#21040;700&#20159;&#12290;Latxa&#22522;&#20110;&#26032;&#30340;&#24052;&#26031;&#20811;&#35821;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;430&#19975;&#20010;&#25991;&#26723;&#21644;42&#20159;&#20010;&#26631;&#35760;&#12290;&#38024;&#23545;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;4&#20010;&#22810;&#39033;&#36873;&#25321;&#35780;&#20272;&#25968;&#25454;&#38598;&#65306;EusProficiency&#65292;&#21253;&#25324;&#26469;&#33258;&#23448;&#26041;&#35821;&#35328;&#33021;&#21147;&#32771;&#35797;&#30340;5169&#20010;&#38382;&#39064;&#65307;EusReading&#65292;&#21253;&#25324;352&#20010;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#65307;EusTrivia&#65292;&#21253;&#25324;&#26469;&#33258;5&#20010;&#30693;&#35782;&#39046;&#22495;&#30340;1715&#20010;&#29712;&#20107;&#38382;&#39064;&#65307;&#20197;&#21450;EusExams&#65292;&#21253;&#25324;&#26469;&#33258;&#20844;&#20849;&#32771;&#35797;&#30340;16774&#20010;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#20013;&#65292;Latxa&#22312;&#19982;&#25105;&#20204;&#27604;&#36739;&#30340;&#25152;&#26377;&#20808;&#21069;&#24320;&#25918;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#38405;&#35835;&#29702;&#35299;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26041;&#38754;&#33853;&#21518;&#65292;&#20294;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#65292;&#23427;&#19982;GPT-4 Turbo&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;Latxa&#27169;&#22411;&#31995;&#21015;&#65292;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20266v1 Announce Type: cross  Abstract: We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#36890;&#36807;&#24819;&#35937;&#21147;&#65292;&#32780;&#38750;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#34917;&#20805;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#30693;&#35782;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15268</link><description>&lt;p&gt;
&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65306;&#23398;&#20064;&#24819;&#35937;&#26356;&#20016;&#23500;&#30340;&#32972;&#26223;&#26469;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#36890;&#36807;&#24819;&#35937;&#21147;&#65292;&#32780;&#38750;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#34917;&#20805;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#30693;&#35782;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#29983;&#25104;&#22686;&#24378;&#29983;&#25104;&#24050;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#25152;&#38656;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20381;&#36182;&#20110;&#22806;&#37096;&#36164;&#28304;&#65292;&#32780;&#19988;&#20004;&#32773;&#37117;&#38656;&#35201;&#23558;&#26174;&#24335;&#25991;&#26723;&#21512;&#24182;&#21040;&#19978;&#19979;&#25991;&#20013;&#65292;&#23548;&#33268;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#28040;&#32791;&#26356;&#22810;&#36164;&#28304;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#24050;&#32463;&#24314;&#27169;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#23613;&#31649;&#27809;&#26377;&#34987;&#26377;&#25928;&#22320;&#35302;&#21457;&#25110;&#28608;&#27963;&#12290;&#22312;&#27492;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#23427;&#27169;&#25311;&#20102;&#20154;&#31867;&#36890;&#36807;&#24819;&#35937;&#21147;&#22312;&#20165;&#20973;&#24819;&#35937;&#22238;&#31572;&#38382;&#39064;&#26102;&#24357;&#34917;&#30693;&#35782;&#32570;&#38519;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#22312;IAG&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#65292;&#36890;&#36807;&#20197;&#19979;&#20004;&#20010;&#27169;&#22359;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#32972;&#26223;&#65306;&#36890;&#36807;&#29983;&#25104;&#31616;&#21333;&#30340;&#24819;&#35937;&#23454;&#29616;&#26174;&#24335;&#24819;&#35937;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15268v1 Announce Type: new  Abstract: Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results in longer contexts that lead to more resource consumption. Recent works indicate that LLMs have modeled rich knowledge, albeit not effectively triggered or activated. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering questions solely through imagination, without relying on external resources. Guided by IAG, we propose an imagine richer context method for question answering (IMcQA), which obtains richer context through the following two modules: explicit imagination by generating a sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20196;&#29260;&#21270;&#31574;&#30053;&#21644;&#35789;&#27719;&#37327;&#23545;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;BPE&#65289;&#19982;Farasa&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#31361;&#26174;&#20102;&#24418;&#24577;&#20998;&#26512;&#22312;&#25429;&#25417;&#38463;&#25289;&#20271;&#35821;&#35328;&#32454;&#24494;&#24046;&#24322;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11130</link><description>&lt;p&gt;
&#25506;&#32034;&#20196;&#29260;&#21270;&#31574;&#30053;&#21644;&#35789;&#27719;&#37327;&#23545;&#22686;&#24378;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20196;&#29260;&#21270;&#31574;&#30053;&#21644;&#35789;&#27719;&#37327;&#23545;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;BPE&#65289;&#19982;Farasa&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#31361;&#26174;&#20102;&#24418;&#24577;&#20998;&#26512;&#22312;&#25429;&#25417;&#38463;&#25289;&#20271;&#35821;&#35328;&#32454;&#24494;&#24046;&#24322;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20196;&#29260;&#21270;&#31574;&#30053;&#21644;&#35789;&#27719;&#37327;&#23545;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#20102;&#22235;&#31181;&#20196;&#29260;&#21270;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#26032;&#38395;&#20998;&#31867;&#12289;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12290;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#35789;&#27719;&#37327;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#20196;&#29260;&#21270;&#26041;&#27861;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;Farasa&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;BPE&#65289;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#65292;&#24378;&#35843;&#20102;&#22312;&#25429;&#25417;&#38463;&#25289;&#20271;&#35821;&#35328;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#24418;&#24577;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#26041;&#35328;&#29305;&#23450;&#30340;&#20998;&#21106;&#38382;&#39064;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#35745;&#31639;&#25928;&#29575;&#20998;&#26512;&#34920;&#26126;&#65292;BPE&#19982;Farasa&#30340;&#31283;&#23450;&#24615;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11130v1 Announce Type: new  Abstract: This paper presents a comprehensive examination of the impact of tokenization strategies and vocabulary sizes on the performance of Arabic language models in downstream natural language processing tasks. Our investigation focused on the effectiveness of four tokenizers across various tasks, including News Classification, Hate Speech Detection, Sentiment Analysis, and Natural Language Inference. Leveraging a diverse set of vocabulary sizes, we scrutinize the intricate interplay between tokenization approaches and model performance. The results reveal that Byte Pair Encoding (BPE) with Farasa outperforms other strategies in multiple tasks, underscoring the significance of morphological analysis in capturing the nuances of the Arabic language. However, challenges arise in sentiment analysis, where dialect specific segmentation issues impact model efficiency. Computational efficiency analysis demonstrates the stability of BPE with Farasa, su
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#20219;&#21153;&#21644;&#23545;&#40784;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.19404</link><description>&lt;p&gt;
&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;&#29992;&#20110;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Entity-Aware Multimodal Alignment Framework for News Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19404
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#20219;&#21153;&#21644;&#23545;&#40784;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#26159;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#35201;&#27714;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#23383;&#24149;&#65292;&#20854;&#20013;&#21253;&#21547;&#26032;&#38395;&#22270;&#20687;&#21644;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#12290;&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#22312;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#24120;&#35265;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#23450;&#19979;&#29983;&#25104;&#23454;&#20307;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#21363;&#20351;&#22312;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#65292;&#23427;&#20204;&#22788;&#29702;&#23454;&#20307;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#23454;&#20307;&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#22810;&#27169;&#24577;&#23454;&#20307;&#24863;&#30693;&#23545;&#40784;&#20219;&#21153;&#21644;&#19968;&#20010;&#23545;&#40784;&#26694;&#26550;&#65292;&#20197;&#23545;&#40784;&#27169;&#22411;&#24182;&#29983;&#25104;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GoodNews&#25968;&#25454;&#38598;&#19978;&#23558;CIDEr&#20998;&#25968;&#25552;&#39640;&#21040;86.29&#65288;&#20174;72.33&#65289;&#65292;&#22312;NYTimes800k&#25968;&#25454;&#38598;&#19978;&#23558;&#20854;&#25552;&#39640;&#21040;85.61&#65288;&#20174;70.83&#65289;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19404v1 Announce Type: cross  Abstract: News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article. Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-tuned on news image captioning dataset. To obtain a more powerful model to handle the multimodal entity information, we design two multimodal entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions. Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -&gt; 86.29) on GoodNews dataset and (70.83 -&gt; 85.61) on NYTimes800k dataset.
&lt;/p&gt;</description></item><item><title>MultiPoT &#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#20248;&#21183;&#21644;&#22810;&#26679;&#24615;&#65292;&#22312;&#34920;&#29616;&#19978;&#26174;&#33879;&#20248;&#20110; Python &#33258;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10691</link><description>&lt;p&gt;
MultiPoT: &#22810;&#35821;&#35328;&#24605;&#32500;&#31243;&#24207;&#21033;&#29992;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
MultiPoT: Multilingual Program of Thoughts Harnesses Multiple Programming Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10691
&lt;/p&gt;
&lt;p&gt;
MultiPoT &#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#20248;&#21183;&#21644;&#22810;&#26679;&#24615;&#65292;&#22312;&#34920;&#29616;&#19978;&#26174;&#33879;&#20248;&#20110; Python &#33258;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10691v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#24605;&#32500;&#31243;&#24207;&#65288;PoT&#65289;&#26159;&#19968;&#31181;&#20197;&#20854;&#21487;&#25191;&#34892;&#20013;&#38388;&#27493;&#39588;&#20026;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20854;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#20013;&#25968;&#20540;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#12290;&#30446;&#21069;&#65292;PoT&#20027;&#35201;&#20351;&#29992;Python&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#21333;&#19968;&#35821;&#35328;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#24573;&#35270;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;PoT&#20013;&#20351;&#29992;&#30340;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#35821;&#35328;&#22312;&#25152;&#26377;&#20219;&#21153;&#21644;&#27169;&#22411;&#19978;&#22987;&#32456;&#25552;&#20379;&#26368;&#20339;&#24615;&#33021;&#12290;&#27599;&#31181;&#35821;&#35328;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20855;&#20307;&#24773;&#26223;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MultiPoT&#30340;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#21508;&#31181;&#35821;&#35328;&#20013;&#33719;&#21462;&#24378;&#22823;&#21644;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MultiPoT &#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;Python &#33258;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#26368;&#20339;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10691v1 Announce Type: new  Abstract: Program of Thoughts (PoT) is an approach characterized by its executable intermediate steps, which ensure the accuracy of the numerical calculations in the reasoning process. Currently, PoT primarily uses Python. However, relying solely on a single language may result in suboptimal solutions and overlook the potential benefits of other programming languages. In this paper, we conduct comprehensive experiments on the programming languages used in PoT and find that no single language consistently delivers optimal performance across all tasks and models. The effectiveness of each language varies depending on the specific scenarios. Inspired by this, we propose a task and model agnostic approach called MultiPoT, which harnesses strength and diversity from various languages. Experimental results reveal that it significantly outperforms Python Self-Consistency. Furthermore, it achieves comparable or superior performance compared to the best mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#21644;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#21644;&#22256;&#38590;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;LLMs&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.03877</link><description>&lt;p&gt;
&#36229;&#36234;&#32447;&#26465;&#21644;&#22278;&#22280;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20309;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#21644;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#21644;&#22256;&#38590;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;LLMs&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23398;&#21644;&#31639;&#27861;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#25216;&#33021;&#36824;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#26500;&#36896;&#24615;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#20154;&#31867;&#25968;&#23398;&#25512;&#29702;&#21457;&#23637;&#20013;&#26368;&#22522;&#30784;&#30340;&#27493;&#39588;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30340;&#26174;&#33879;&#25361;&#25112;&#65292;&#23613;&#31649;&#22312;&#31867;&#20284;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#12290;LLMs&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#65292;&#24182;&#19988;&#22312;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#65292;&#32463;&#24120;&#20250;&#38169;&#35823;&#22320;&#34920;&#31034;&#21644;&#33222;&#36896;&#23545;&#35937;&#21450;&#20854;&#25918;&#32622;&#20301;&#32622;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#36827;&#34892;&#20869;&#37096;&#23545;&#35805;&#26469;&#22686;&#24378;&#23427;&#20204;&#29616;&#26377;&#30340;&#25512;&#29702;&#28508;&#21147;&#12290;&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;LLMs&#22312;&#20960;&#20309;&#25512;&#29702;&#20013;&#30340;&#29616;&#26377;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;</title><link>https://arxiv.org/abs/2310.04910</link><description>&lt;p&gt;
&#20851;&#20110;&#24120;&#35782;&#25512;&#29702;&#30340;&#30693;&#35782;&#22270;&#35889;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful Knowledge Graph Explanations for Commonsense Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;(LMs)&#21644;&#30693;&#35782;&#22270;&#35889;(KGs)&#24050;&#25104;&#20026;&#24120;&#35782;&#38382;&#31572;&#30740;&#31350;&#20013;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#20294;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;&#24605;&#36335;&#38142;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#25216;&#26415;&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#26159;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#24573;&#35270;&#20102;&#29983;&#25104;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631; - &#22270;&#19968;&#33268;&#24615;&#21644;&#22270;&#20445;&#30495;&#24230; - &#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;Consistent GNN (CGNN)&#65292;&#35813;&#26041;&#27861;&#28155;&#21152;&#20102;&#19968;&#39033;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;KG&#30340;&#39044;&#27979;&#32463;&#24120;&#20559;&#31163;&#21407;&#22987;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;CGNN&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#23637;&#31034;&#20102;&#23427;&#20135;&#29983;&#26356;&#21487;&#20449;&#35299;&#37322;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26126;&#30830;&#35780;&#20272;&#35299;&#37322;&#21487;&#20449;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fusing language models (LMs) and knowledge graphs (KGs) has become common in commonsense question answering research, enabling faithful chain-of-thought explanations in these models remains an open problem. One major weakness of current KG-based explanation techniques is that they overlook the faithfulness of generated explanations during evaluation. To address this gap, we make two main contributions: (1) We propose and validate two quantitative metrics - graph consistency and graph fidelity - to measure the faithfulness of KG-based explanations. (2) We introduce Consistent GNN (CGNN), a novel training method that adds a consistency regularization term to improve explanation faithfulness. Our analysis shows that predictions from KG often diverge from original model predictions. The proposed CGNN approach boosts consistency and fidelity, demonstrating its potential for producing more faithful explanations. Our work emphasises the importance of explicitly evaluating suggest a path
&lt;/p&gt;</description></item><item><title>ChemDFM&#26159;&#39318;&#20010;&#38754;&#21521;&#21270;&#23398;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#21270;&#23398;&#25991;&#29486;&#21644;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20855;&#22791;&#20102;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14818</link><description>&lt;p&gt;
ChemDFM: &#21270;&#23398;&#39046;&#22495;&#23545;&#35805;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChemDFM: Dialogue Foundation Model for Chemistry. (arXiv:2401.14818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14818
&lt;/p&gt;
&lt;p&gt;
ChemDFM&#26159;&#39318;&#20010;&#38754;&#21521;&#21270;&#23398;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#21270;&#23398;&#25991;&#29486;&#21644;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20855;&#22791;&#20102;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#33324;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23427;&#20204;&#30340;&#20219;&#21153;&#27010;&#25324;&#21644;&#33258;&#30001;&#23545;&#35805;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#24110;&#21161;&#35774;&#35745;&#21270;&#23398;&#26234;&#33021;(CGI)&#65292;&#20197;&#21327;&#21161;&#21270;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#19987;&#19994;&#35821;&#35328;&#21644;&#30693;&#35782;&#65292;&#22914;&#39640;&#24230;&#20449;&#24687;&#21270;&#30340;SMILES&#31526;&#21495;&#34920;&#31034;&#27861;&#65292;&#38459;&#30861;&#20102;&#19968;&#33324;&#39046;&#22495;LLMs&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ChemDFM&#65292;&#36825;&#26159;&#39318;&#20010;&#38754;&#21521;CGI&#30340;LLM&#12290;ChemDFM-13B&#26159;&#22312;&#21270;&#23398;&#25991;&#29486;&#12289;&#25945;&#31185;&#20070;&#12289;&#35828;&#26126;&#20070;&#20197;&#21450;&#21508;&#31181;&#19968;&#33324;&#39046;&#22495;&#30340;&#25968;&#25454;&#20013;&#35757;&#32451;&#30340;34B&#20196;&#29260;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#65292;&#21516;&#26102;&#20855;&#26377;&#20808;&#36827;&#30340;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;ChemDFM&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#20195;&#34920;&#24615;&#30340;&#24320;&#28304;LLMs&#12290;&#27492;&#22806;&#65292;ChemDFM&#36824;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have established great success in the general domain of natural language processing. Their emerging task generalization and free-form dialogue capabilities can greatly help to design Chemical General Intelligence (CGI) to assist real-world research in chemistry. However, the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry. To this end, we develop ChemDFM, the first LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature, textbooks, and instructions as well as various data from the general domain. Therefore, it can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities. Extensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs. Moreover, ChemDFM can also
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20250;&#25298;&#32477;&#65288;L2R&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#25298;&#32477;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01041</link><description>&lt;p&gt;
&#23398;&#20250;&#25298;&#32477;&#65306;&#36890;&#36807;&#30693;&#35782;&#33539;&#22260;&#38480;&#21046;&#21644;&#25298;&#32477;&#26426;&#21046;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#21487;&#25511;&#21644;&#21487;&#38752;
&lt;/p&gt;
&lt;p&gt;
Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism. (arXiv:2311.01041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20250;&#25298;&#32477;&#65288;L2R&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#25298;&#32477;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22238;&#31572;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24182;&#19981;&#23436;&#32654;&#65292;&#32463;&#24120;&#20135;&#29983;&#21547;&#26377;&#38169;&#35823;&#25110;&#38169;&#35823;&#20449;&#24687;&#30340;&#22238;&#31572;&#12290;&#36825;&#20123;&#19981;&#20934;&#30830;&#24615;&#65292;&#36890;&#24120;&#31216;&#20026;&#24187;&#35273;&#65292;&#20351;&#24471;LLMs&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#19981;&#21487;&#38752;&#29978;&#33267;&#19981;&#21487;&#29992;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#22312;LLMs&#20013;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38382;&#31572;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#25298;&#32477;&#26426;&#21046;&#65292;&#25351;&#23548;LLMs&#25298;&#32477;&#22238;&#31572;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20197;&#36991;&#20813;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;Learn to Refuse (L2R)&#65292;&#23427;&#23558;&#25298;&#32477;&#26426;&#21046;&#32435;&#20837;&#21040;LLMs&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#37027;&#20123;&#23427;&#20204;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#26469;&#34920;&#31034;&#25152;&#26377;LLMs&#25152;&#38656;&#35201;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16218</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16218
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#26399;&#20197;&#20854;&#20986;&#33394;&#30340;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26681;&#25454;&#20854;&#24191;&#21338;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25913;&#21464;&#20102;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#26102;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#20854;&#21442;&#25968;&#25968;&#37327;&#21069;&#25152;&#26410;&#26377;&#12290;&#24403;&#38656;&#35201;&#39057;&#32321;&#24341;&#20837;&#26032;&#30693;&#35782;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26102;&#65292;&#36825;&#20010;&#32570;&#28857;&#26356;&#21152;&#26174;&#33879;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20256;&#32479;&#26041;&#27861;&#26159;&#36890;&#36807;&#30452;&#25509;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#32534;&#30721;&#21040;&#39044;&#35757;&#32451;LLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#37325;&#26032;&#35757;&#32451;LLMs&#21487;&#33021;&#35745;&#31639;&#36164;&#28304;&#23494;&#38598;&#65292;&#24182;&#19988;&#23384;&#22312;&#23558;&#19982;&#27169;&#22411;&#26356;&#26032;&#26080;&#20851;&#30340;&#26377;&#20215;&#20540;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#36864;&#21270;&#30340;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#32534;&#36753;(KME)&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;LLMs&#20197;&#32435;&#20837;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#23646;&#24615;/&#20540;&#25552;&#21462;&#25216;&#26415;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#23545;&#26410;&#30693;&#23646;&#24615;&#20540;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12537</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Product Attribute Value Extraction using Large Language Models. (arXiv:2310.12537v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#23646;&#24615;/&#20540;&#25552;&#21462;&#25216;&#26415;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#23545;&#26410;&#30693;&#23646;&#24615;&#20540;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#65288;&#22914;&#38754;&#21521;&#23646;&#24615;&#30340;&#20135;&#21697;&#25628;&#32034;&#25110;&#20135;&#21697;&#27604;&#36739;&#65289;&#22522;&#20110;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25551;&#36848;&#65292;&#22914;&#23646;&#24615;/&#20540;&#23545;&#12290;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#20379;&#24212;&#21830;&#19981;&#25552;&#20379;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25551;&#36848;&#65292;&#32780;&#26159;&#20351;&#29992;&#26631;&#39064;&#25110;&#25551;&#36848;&#26469;&#25551;&#36848;&#20135;&#21697;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#26679;&#30340;&#20135;&#21697;&#65292;&#26377;&#24517;&#35201;&#20174;&#25991;&#26412;&#20135;&#21697;&#23646;&#24615;&#20013;&#25552;&#21462;&#23646;&#24615;/&#20540;&#23545;&#12290;&#29616;&#26377;&#25216;&#26415;&#20013;&#65292;&#23646;&#24615;/&#20540;&#25552;&#21462;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#23646;&#24615;/&#20540;&#25552;&#21462;&#26041;&#38754;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;&#19968;&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;&#20108;&#65289;&#20248;&#21270;&#21518;&#30340;&#27169;&#22411;&#22312;&#25512;&#24191;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#21253;&#21547;&#30340;&#23646;&#24615;&#20540;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#39640;&#19988;&#40065;&#26834;&#24615;&#24378;&#30340;&#26367;&#20195;&#26041;&#27861;&#22312;&#23646;&#24615;/&#20540;&#25552;&#21462;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25176;&#31649;&#30340;LLMs&#65292;&#22914;GPT-3.5&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce applications such as faceted product search or product comparison are based on structured product descriptions like attribute/value pairs. The vendors on e-commerce platforms do not provide structured product descriptions but describe offers using titles or descriptions. To process such offers, it is necessary to extract attribute/value pairs from textual product attributes. State-of-the-art attribute/value extraction techniques rely on pre-trained language models (PLMs), such as BERT. Two major drawbacks of these models for attribute/value extraction are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models face challenges in generalizing to attribute values not included in the training data. This paper explores the potential of large language models (LLMs) as a training data-efficient and robust alternative to PLM-based attribute/value extraction methods. We consider hosted LLMs, such as GPT-3.5 and GPT-4, as well as 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#29305;&#36136;&#29702;&#35770;&#26694;&#26550;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;ChatGPT&#22987;&#32456;&#34920;&#29616;&#20986;ENFJ&#22411;&#20154;&#26684;&#65292;&#26080;&#35770;&#25351;&#20196;&#25110;&#24773;&#22659;&#22914;&#20309;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#30340;&#20010;&#24615;&#21270;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#26356;&#22909;&#30340;&#27807;&#36890;&#21644;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.19926</link><description>&lt;p&gt;
ChatGPT&#26159;ENFJ&#65292;Bard&#26159;ISTJ&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT an ENFJ, Bard an ISTJ: Empirical Study on Personalities of Large Language Models. (arXiv:2305.19926v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#29305;&#36136;&#29702;&#35770;&#26694;&#26550;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;ChatGPT&#22987;&#32456;&#34920;&#29616;&#20986;ENFJ&#22411;&#20154;&#26684;&#65292;&#26080;&#35770;&#25351;&#20196;&#25110;&#24773;&#22659;&#22914;&#20309;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#30340;&#20010;&#24615;&#21270;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#26356;&#22909;&#30340;&#27807;&#36890;&#21644;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22823;&#22823;&#37325;&#22609;&#20102;&#20154;&#26426;&#20132;&#20114;&#12290;&#25105;&#20204;&#19981;&#20165;&#20851;&#27880;LLMs&#30340;&#24615;&#33021;&#65292;&#36824;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#25506;&#32034;&#23427;&#20204;&#30340;&#29305;&#28857;&#65292;&#35748;&#35782;&#21040;&#20102;&#29702;&#35299;&#23427;&#20204;&#34892;&#20026;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#24515;&#29702;&#23398;&#30340;&#19968;&#20010;&#26694;&#26550;&#8212;&#8212;&#29305;&#36136;&#29702;&#35770;&#30740;&#31350;LLMs&#25152;&#23637;&#31034;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#20851;&#27880;&#35780;&#20272;ChatGPT&#25152;&#23637;&#31034;&#30340;&#20154;&#26684;&#31867;&#22411;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#28041;&#21450;&#19971;&#31181;&#38468;&#21152;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#24433;&#21709;&#65292;&#20197;&#21450;&#20845;&#31181;&#20854;&#20182;LLMs&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35843;&#26597;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#23637;&#31034;&#23545;&#25351;&#20196;&#25110;&#24773;&#22659;&#32447;&#32034;&#30340;&#20154;&#26684;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#25351;&#20196;&#25110;&#24773;&#22659;&#22914;&#20309;&#65292;ChatGPT&#22987;&#32456;&#20445;&#25345;&#20854;ENFJ&#20154;&#26684;&#12290;&#36890;&#36807;&#25581;&#31034;LLMs&#30340;&#20010;&#24615;&#21270;&#65292;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#20419;&#36827;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#26356;&#22909;&#30340;&#27807;&#36890;&#21644;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made remarkable advancements in the field of artificial intelligence, significantly reshaping the human-computer interaction. We not only focus on the performance of LLMs, but also explore their features from a psychological perspective, acknowledging the importance of understanding their behavioral characteristics. Our study examines the behavioral patterns displayed by LLMs by employing trait theory, a psychological framework. We first focus on evaluating the consistency of personality types exhibited by ChatGPT. Furthermore, experiments include cross-lingual effects on seven additional languages, and the investigation of six other LLMs. Moreover, the study investigates whether ChatGPT can exhibit personality changes in response to instructions or contextual cues. The findings show that ChatGPT consistently maintains its ENFJ personality regardless of instructions or contexts. By shedding light on the personalization of LLMs, we anticipate that our s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;: ContraSim&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;ContraSim&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#33719;&#24471;&#20102;&#27604;&#20043;&#21069;&#30456;&#20284;&#24230;&#37327;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16992</link><description>&lt;p&gt;
ContraSim -- &#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ContraSim -- A Similarity Measure Based on Contrastive Learning. (arXiv:2303.16992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;: ContraSim&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;ContraSim&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#33719;&#24471;&#20102;&#27604;&#20043;&#21069;&#30456;&#20284;&#24230;&#37327;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#26512;&#27604;&#36739;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#26041;&#38754;&#65288;&#22914;&#26550;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#31561;&#65289;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#30456;&#20284;&#24230;&#37327;&#30340;&#36136;&#37327;&#36890;&#24120;&#36890;&#36807;&#20854;&#22312;&#39044;&#26399;&#21305;&#37197;&#30340;&#34920;&#31034;&#20013;&#20998;&#37197;&#39640;&#20998;&#25968;&#30340;&#25104;&#21151;&#26469;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30456;&#20284;&#24230;&#37327;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24179;&#24248;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ContraSim&#65292;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#65292;&#19982;&#24120;&#35265;&#30340;&#38381;&#24335;&#30456;&#20284;&#24615;&#24230;&#37327;&#19981;&#21516;&#65292;ContraSim&#20351;&#29992;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#31034;&#20363;&#26469;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#22270;&#23618;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#21644;&#25105;&#20204;&#20171;&#32461;&#30340;&#20004;&#20010;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#20351;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65306;&#22810;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#21644;&#22270;&#20687;&#23383;&#24149;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;ContraSim&#30340;&#20934;&#30830;&#24615;&#37117;&#27604;&#20043;&#21069;&#30340;&#30456;&#20284;&#24230;&#37327;&#26041;&#27861;&#39640;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has compared neural network representations via similarity-based analyses, shedding light on how different aspects (architecture, training data, etc.) affect models' internal representations. The quality of a similarity measure is typically evaluated by its success in assigning a high score to representations that are expected to be matched. However, existing similarity measures perform mediocrely on standard benchmarks. In this work, we develop a new similarity measure, dubbed ContraSim, based on contrastive learning. In contrast to common closed-form similarity measures, ContraSim learns a parameterized measure by using both similar and dissimilar examples. We perform an extensive experimental evaluation of our method, with both language and vision models, on the standard layer prediction benchmark and two new benchmarks that we introduce: the multilingual benchmark and the image-caption benchmark. In all cases, ContraSim achieves much higher accuracy than previous simila
&lt;/p&gt;</description></item></channel></rss>