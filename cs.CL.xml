<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Jamba&#26159;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;Transformer-Mamba&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21333;&#20010;80GB GPU&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23545;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#21644;&#38271;&#19978;&#19979;&#25991;&#35780;&#20272;&#20855;&#26377;state-of-the-art&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.19887</link><description>&lt;p&gt;
Jamba: &#19968;&#20010;&#28151;&#21512;Transformer-Mamba&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jamba: A Hybrid Transformer-Mamba Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19887
&lt;/p&gt;
&lt;p&gt;
Jamba&#26159;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;Transformer-Mamba&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21333;&#20010;80GB GPU&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23545;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#21644;&#38271;&#19978;&#19979;&#25991;&#35780;&#20272;&#20855;&#26377;state-of-the-art&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Jamba&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26032;&#39062;&#30340;&#28151;&#21512;Transformer-Mamba&#28151;&#21512;&#19987;&#23478;(MoE)&#26550;&#26500;&#30340;&#26032;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Jamba&#20132;&#38169;&#20351;&#29992;Transformer&#21644;Mamba&#23618;&#65292;&#20174;&#20004;&#31181;&#27169;&#22411;&#23478;&#26063;&#20013;&#33719;&#30410;&#12290;MoE&#34987;&#28155;&#21152;&#22312;&#20854;&#20013;&#19968;&#20123;&#23618;&#20013;&#65292;&#20197;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27963;&#36291;&#21442;&#25968;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#26550;&#26500;&#20801;&#35768;&#29305;&#23450;&#36164;&#28304;&#21644;&#30446;&#26631;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19887v1 Announce Type: new  Abstract: We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REval&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#19982;&#31243;&#24207;&#25191;&#34892;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16437</link><description>&lt;p&gt;
&#20351;&#29992;&#31243;&#24207;&#25191;&#34892;&#36816;&#34892;&#26102;&#34892;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models with Runtime Behavior of Program Execution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REval&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#19982;&#31243;&#24207;&#25191;&#34892;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#20195;&#30721;LLMs&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#20195;&#30721;LLMs&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20934;&#65288;&#22914;HumanEval&#21644;ClassEval&#65289;&#12290;&#20195;&#30721;&#25512;&#29702;&#26159;&#20195;&#30721;LLMs&#26368;&#37325;&#35201;&#30340;&#33021;&#21147;&#20043;&#19968;&#65292;&#20294;&#29616;&#26377;&#30340;&#20195;&#30721;&#25512;&#29702;&#22522;&#20934;&#19981;&#36275;&#12290;&#36890;&#24120;&#65292;&#23427;&#20204;&#37325;&#28857;&#39044;&#27979;&#31243;&#24207;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#24573;&#30053;&#20102;&#31243;&#24207;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#20013;&#38388;&#34892;&#20026;&#35780;&#20272;&#65292;&#20197;&#21450;&#36923;&#36753;&#19968;&#33268;&#24615;&#65288;&#20363;&#22914;&#65292;&#22914;&#26524;&#25191;&#34892;&#36335;&#24452;&#39044;&#27979;&#38169;&#35823;&#65292;&#21017;&#27169;&#22411;&#19981;&#24212;&#35813;&#32473;&#20986;&#27491;&#30830;&#30340;&#36755;&#20986;&#65289;&#22312;&#25191;&#34892;&#25512;&#29702;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REval&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#19982;&#31243;&#24207;&#25191;&#34892;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20195;&#30721;&#22522;&#20934;&#65292;&#24182;&#23558;&#23427;&#20204;&#36866;&#24212;&#21040;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30340;&#26032;&#22522;&#20934;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16437v1 Announce Type: cross  Abstract: Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs, but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely REval, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framew
&lt;/p&gt;</description></item><item><title>LLMs&#20511;&#21161;Reasoning-Path-Editing (Readi)&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#24544;&#23454;&#22320;&#25512;&#29702;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#20010;KGQA&#21644;TableQA&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.08593</link><description>&lt;p&gt;
&#24403;&#38656;&#35201;&#26102;&#32473;&#25105;&#25171;&#30005;&#35805;&#65306;LLM&#21487;&#20197;&#39640;&#25928;&#32780;&#24544;&#23454;&#22320;&#25512;&#29702;&#32467;&#26500;&#21270;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08593
&lt;/p&gt;
&lt;p&gt;
LLMs&#20511;&#21161;Reasoning-Path-Editing (Readi)&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#24544;&#23454;&#22320;&#25512;&#29702;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#20010;KGQA&#21644;TableQA&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#25512;&#29702;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#21644;&#34920;&#26684;&#12290;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#65292;&#21363;&#23558;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#19982;&#29615;&#22659;&#20013;&#30340;&#23454;&#20363;&#21305;&#37197;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#36880;&#27493;&#26500;&#24314;&#25512;&#29702;&#36335;&#24452;&#65292;&#20854;&#20013;LLMs&#36890;&#36807;&#19982;&#29615;&#22659;&#36880;&#27493;&#20132;&#20114;&#26469;&#35843;&#29992;&#24037;&#20855;&#25110;&#36873;&#25321;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;Reasoning-Path-Editing&#65288;Readi&#65289;&#65292;&#22312;&#20854;&#20013;LLMs&#21487;&#20197;&#39640;&#25928;&#19988;&#24544;&#23454;&#22320;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;Readi&#20013;&#65292;LLMs&#22312;&#32473;&#23450;&#26597;&#35810;&#26102;&#26368;&#21021;&#29983;&#25104;&#19968;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#21482;&#26377;&#22312;&#24517;&#35201;&#26102;&#25165;&#32534;&#36753;&#36335;&#24452;&#12290;&#25105;&#20204;&#23558;&#36335;&#24452;&#23454;&#20363;&#21270;&#21040;&#32467;&#26500;&#21270;&#29615;&#22659;&#19978;&#65292;&#24182;&#22312;&#20986;&#29616;&#38382;&#39064;&#26102;&#25552;&#20379;&#21453;&#39304;&#20197;&#32534;&#36753;&#36335;&#24452;&#12290;&#23545;&#19977;&#20010;KGQA&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;TableQA&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Readi&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#25152;&#26377;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#65288;&#22312;WebQ&#19978;&#25552;&#39640;&#20102;9.1&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08593v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graph and table. Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment. Previous methods leverage LLMs to incrementally build a reasoning path, where the LLMs either invoke tools or pick up schemas by step-by-step interacting with the environment. We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments. In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA datasets and two TableQA datasets show the effectiveness of Readi, significantly surpassing all LLM-based methods (by 9.1% on WebQ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39034;&#24207;&#25351;&#20196;&#24494;&#35843;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#25191;&#34892;&#22810;&#20010;&#39034;&#24207;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#20248;&#20110;&#20256;&#32479;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.07794</link><description>&lt;p&gt;
&#20351;&#29992;&#39034;&#24207;&#25351;&#20196;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models with Sequential Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07794
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39034;&#24207;&#25351;&#20196;&#24494;&#35843;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#25191;&#34892;&#22810;&#20010;&#39034;&#24207;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#20248;&#20110;&#20256;&#32479;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21333;&#20010;&#26597;&#35810;&#20013;&#36981;&#24490;&#19968;&#31995;&#21015;&#25351;&#20196;&#26102;&#24448;&#24448;&#20250;&#24573;&#30053;&#25110;&#35823;&#35299;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#65292;&#36825;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#38656;&#35201;&#22810;&#20010;&#20013;&#38388;&#27493;&#39588;&#30340;&#22797;&#26434;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#22810;&#35821;&#35328;&#65288;&#20808;&#32763;&#35793;&#20877;&#22238;&#31572;&#65289;&#21644;&#22810;&#27169;&#24577;&#65288;&#26631;&#39064;&#21518;&#22238;&#31572;&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#28304;LLMs&#65288;&#22914;LLaMA-2 70B&#21644;Mixtral-8x7B&#65289;&#30340;&#23454;&#35777;&#39564;&#35777;&#20102;&#36825;&#19968;&#28857;&#12290;&#38024;&#23545;&#24403;&#21069;&#25968;&#25454;&#20013;&#39034;&#24207;&#25351;&#20196;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39034;&#24207;&#25351;&#20196;&#24494;&#35843;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#33258;&#21160;&#22686;&#21152;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#65292;&#20351;LLMs&#20855;&#22791;&#25191;&#34892;&#22810;&#20010;&#39034;&#24207;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#22312;&#25506;&#32034;&#29616;&#26377;&#25968;&#25454;&#38598;&#65288;&#22914;Alpaca&#65289;&#20013;&#25554;&#20837;&#25351;&#20196;&#24182;&#36827;&#34892;&#19968;&#31995;&#21015;&#20013;&#38388;&#20219;&#21153;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#39034;&#24207;&#25351;&#20196;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20256;&#32479;&#30340;&#25351;&#20196;&#24494;&#35843;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07794v1 Announce Type: new  Abstract: Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks. We empirically verify this with open-source LLMs as large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential instructions in present-day data, we propose sequential instruction tuning, a simple yet effective strategy to automatically augment instruction tuning data and equip LLMs with the ability to execute multiple sequential instructions. After exploring interleaving instructions in existing datasets, such as Alpaca, with a wide range of intermediate tasks, we find that sequential instruction-tuned models consistently outperform the conventional instruction-tuned baselines in downstream tas
&lt;/p&gt;</description></item><item><title>GraphWiz&#26159;&#19968;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#35299;&#20915;&#21508;&#31181;&#22270;&#38382;&#39064;&#31867;&#22411;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#36229;&#36807;&#20102;GPT-4&#30340;43.8%&#12290;</title><link>https://arxiv.org/abs/2402.16029</link><description>&lt;p&gt;
GraphWiz&#65306;&#29992;&#20110;&#22270;&#38382;&#39064;&#30340;&#25351;&#20196;&#36319;&#38543;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphWiz: An Instruction-Following Language Model for Graph Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16029
&lt;/p&gt;
&lt;p&gt;
GraphWiz&#26159;&#19968;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#35299;&#20915;&#21508;&#31181;&#22270;&#38382;&#39064;&#31867;&#22411;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#36229;&#36807;&#20102;GPT-4&#30340;43.8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#29702;&#35299;&#21644;&#35299;&#20915;&#22797;&#26434;&#22270;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GraphInstruct&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22788;&#29702;&#21508;&#31181;&#22270;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#26126;&#30830;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;&#21033;&#29992;GraphInstruct&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;GraphWiz&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#22270;&#38382;&#39064;&#31867;&#22411;&#24182;&#29983;&#25104;&#28165;&#26224;&#25512;&#29702;&#36807;&#31243;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26694;&#26550;&#32435;&#20837;&#22270;&#38382;&#39064;&#27714;&#35299;&#29615;&#22659;&#20013;&#12290;&#22686;&#24378;&#27169;&#22411;GraphWiz-DPO&#22312;&#20061;&#20010;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24615;&#27700;&#24179;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;65%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;43.8%&#30340;GPT-4&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16029v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;DPO-Positive&#65288;DPOP&#65289;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#20013;&#28508;&#22312;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;</title><link>https://arxiv.org/abs/2402.13228</link><description>&lt;p&gt;
Smaug&#65306;&#20351;&#29992;DPO-Positive&#20462;&#22797;&#20559;&#22909;&#20248;&#21270;&#30340;&#22833;&#36133;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13228
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;DPO-Positive&#65288;DPOP&#65289;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#20013;&#28508;&#22312;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#22312;&#26174;&#33879;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#24635;&#32467;&#21644;&#23545;&#40784;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290; DPO&#20351;&#29992;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#25968;&#25454;&#23545;&#27169;&#22411;&#36873;&#25321;&#19968;&#20010;&#21709;&#24212;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#30340;&#8220;&#30456;&#23545;&#8221;&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#21482;&#35201;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#23545;&#27010;&#29575;&#22686;&#21152;&#65292;&#26631;&#20934;DPO&#25439;&#22833;&#23601;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23545;&#39318;&#36873;&#31034;&#20363;&#30340;&#21487;&#33021;&#24615;&#38477;&#20302;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#24403;&#22312;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;LLMs&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#23436;&#25104;&#20043;&#38388;&#30340;&#32534;&#36753;&#36317;&#31163;&#36739;&#30701;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#20250;&#20986;&#29616;&#36825;&#31181;&#29616;&#35937;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DPO-Positive&#65288;DPOP&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#36825;&#31181;&#22833;&#36133;&#27169;&#24335;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13228v1 Announce Type: cross  Abstract: Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32534;&#36753;HotpotQA&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;LLM MHQA&#35780;&#20272;&#22522;&#20934;&#65292;&#21516;&#26102;&#27880;&#37322;&#21644;&#35780;&#20272;&#20102;&#25512;&#29702;&#38142;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;MHQA&#22522;&#20934;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.11924</link><description>&lt;p&gt;
MRKE&#65306;&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23545;LLMs&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11924
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32534;&#36753;HotpotQA&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;LLM MHQA&#35780;&#20272;&#22522;&#20934;&#65292;&#21516;&#26102;&#27880;&#37322;&#21644;&#35780;&#20272;&#20102;&#25512;&#29702;&#38142;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;MHQA&#22522;&#20934;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65288;MHQA&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30495;&#27491;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24453;&#25506;&#35752;&#12290;&#30446;&#21069;&#30340;LLM QA&#35780;&#20272;&#22522;&#20934;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;1&#65289;&#25968;&#25454;&#27745;&#26579;&#65292;&#35780;&#20272;&#25968;&#25454;&#21487;&#33021;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#32473;LLMs&#65307;&#20197;&#21450;2&#65289;&#24573;&#35270;&#25512;&#29702;&#38142;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;LLM MHQA&#35780;&#20272;&#22522;&#20934;&#65292;&#36825;&#26159;&#22522;&#20110;&#32534;&#36753;&#29616;&#25104;HotpotQA&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#12289;&#21069;&#25152;&#26410;&#26377;&#30340;&#30693;&#35782;&#30340;&#31532;&#19968;&#20010;QA&#22522;&#20934;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27880;&#37322;&#21644;&#35780;&#20272;&#20102;&#25512;&#29702;&#38142;&#65292;&#20197;&#23376;&#38382;&#39064;&#21644;&#20013;&#38388;&#31572;&#26696;&#30340;&#24418;&#24335;&#23545;&#24212;&#20110;&#22810;&#36339;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#65292;1&#65289;LLMs&#22312;&#21407;&#22987;HotpotQA&#21644;&#25105;&#20204;&#32534;&#36753;&#30340;&#25968;&#25454;&#20043;&#38388;&#26174;&#31034;&#24615;&#33021;&#24046;&#36317;&#65292;&#35748;&#20026;&#24403;&#21069;&#30340;MHQA&#22522;&#20934;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#38590;&#20197;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11924v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) have shown strong performance in Multi-hop Question Answering (MHQA) tasks, their real reasoning ability remains exploration. Current LLM QA evaluation benchmarks have shown limitations, including 1) data contamination, the evaluation data are potentially exposed to LLMs during the pretraining stage; and 2) ignoration of the reasoning chain evaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QA benchmark based on the new, unprecedented knowledge by editing the off-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the reasoning chain in the form of sub-questions and intermediate answers corresponding to the multi-hop questions. Specifically, based on the observation, 1) LLMs show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA benchmarks have the potential risk of data contamination that hard to evaluate LLMs' perfor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24037;&#20316;&#27969;&#33539;&#24335;&#26041;&#27861;&#26469;&#25913;&#21892;LLMs&#22312;&#25991;&#26412;&#21040;SQL&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;&#20998;&#35299;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21644;&#38382;&#39064;&#35299;&#20915;&#33539;&#22260;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#30340;&#19978;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.10671</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#26469;&#22686;&#24378;&#27880;&#24847;&#21147;&#65306;&#36890;&#36807;&#24037;&#20316;&#27969;&#33539;&#24335;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#21040;SQL&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10671
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24037;&#20316;&#27969;&#33539;&#24335;&#26041;&#27861;&#26469;&#25913;&#21892;LLMs&#22312;&#25991;&#26412;&#21040;SQL&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;&#20998;&#35299;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21644;&#38382;&#39064;&#35299;&#20915;&#33539;&#22260;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#30340;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#32780;&#24191;&#27867;&#30340;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#21333;&#27493;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#26041;&#27861;&#22312;&#22797;&#26434;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#21040;SQL&#65289;&#20013;&#38754;&#20020;&#27880;&#24847;&#21147;&#25193;&#25955;&#21644;&#24615;&#33021;&#19981;&#36275;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#25913;&#21892;LLMs&#22312;&#25991;&#26412;&#21040;SQL&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20316;&#27969;&#33539;&#24335;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#22686;&#24378;LLMs&#30340;&#27880;&#24847;&#21147;&#21644;&#38382;&#39064;&#35299;&#20915;&#33539;&#22260;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#29992;&#20110;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#30340;&#20449;&#24687;&#30830;&#23450;&#27169;&#22359;&#21644;&#22522;&#20110;&#38382;&#39064;&#20998;&#31867;&#30340;&#20840;&#26032;&#25552;&#31034;&#32467;&#26500;&#26497;&#22823;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#33258;&#26657;&#27491;&#21644;&#20027;&#21160;&#23398;&#20064;&#27169;&#22359;&#26497;&#22823;&#25193;&#23637;&#20102;LLMs&#30340;&#38382;&#39064;&#35299;&#20915;&#33539;&#22260;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#26041;&#27861;&#30340;&#19978;&#38480;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10671v1 Announce Type: new  Abstract: In-context learning of large-language models (LLMs) has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step chain-of-thought prompting approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of LLMs through decomposition. Specifically, the information determination module for eliminating redundant information and the brand-new prompt structure based on problem classification greatly enhance the model's attention. Additionally, the inclusion of self-correcting and active learning modules greatly expands the problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches. Extensive experiments conducted on three da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#36890;&#29992;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#26126;&#30830;&#27880;&#37322;&#30340;&#22870;&#21169;&#25968;&#25454;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.05369</link><description>&lt;p&gt;
&#20197;&#26174;&#24335;&#22870;&#21169;&#30340;&#22122;&#22768;&#23545;&#27604;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Alignment of Language Models with Explicit Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#36890;&#29992;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#26126;&#30830;&#27880;&#37322;&#30340;&#22870;&#21169;&#25968;&#25454;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24847;&#22270;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#38656;&#35201;&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26102;&#26368;&#22823;&#21270;&#30340;&#35780;&#20272;&#22870;&#21169;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#22914;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#65288;DPO&#65289;&#65292;&#20027;&#35201;&#36866;&#29992;&#20110;&#38544;&#21547;&#23450;&#20041;&#32780;&#38750;&#26126;&#30830;&#32473;&#23450;&#22870;&#21169;&#30340;&#20004;&#20004;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#21033;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#26469;&#35299;&#20915;&#26126;&#30830;&#27880;&#37322;&#26377;&#26631;&#37327;&#35780;&#20272;&#30340;&#22870;&#21169;&#25968;&#25454;&#22788;&#29702;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#24182;&#34892;&#31639;&#27861;&#65292;NCA&#21644;InfoNCA&#65292;&#20004;&#32773;&#37117;&#33021;&#20174;&#22870;&#21169;&#25968;&#25454;&#21644;&#20559;&#22909;&#25968;&#25454;&#20013;&#30452;&#25509;&#25552;&#21462;LM&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DPO&#25439;&#22833;&#26159;&#25105;&#20204;&#25552;&#20986;&#30340;InfoNCA&#30446;&#26631;&#22312;&#20004;&#20004;&#20559;&#22909;&#35774;&#32622;&#19979;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#32780;&#38598;&#25104;&#21644;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;&#36890;&#36807;&#23545;&#27604;NCA&#21644;InfoNCA&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;InfoNCA&#21644;DPO&#22914;&#20309;&#22312;&#19981;&#21516;&#21709;&#24212;&#23545;&#20110;&#21333;&#20010;&#25351;&#20196;&#30340;&#30456;&#23545;&#21487;&#33021;&#24615;&#19978;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;</title><link>https://arxiv.org/abs/2402.04854</link><description>&lt;p&gt;
&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#23398;&#26415;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32570;&#20047;&#30740;&#31350;&#22521;&#35757;&#30340;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#30740;&#31350;&#35843;&#26597;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#30740;&#31350;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#24456;&#38590;&#29702;&#35299;&#20182;&#20204;&#30740;&#31350;&#20027;&#39064;&#20869;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#21457;&#29616;&#26032;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#20026;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#25552;&#20379;&#30452;&#35266;&#30340;&#24110;&#21161;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#25552;&#20379;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#24182;&#25512;&#33616;&#30456;&#20851;&#30340;&#23398;&#26415;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#20027;&#35201;&#20381;&#36182;&#20110;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#23383;&#65292;&#24120;&#24120;&#26080;&#27861;&#28165;&#26970;&#22320;&#21576;&#29616;&#22810;&#20010;&#30456;&#20851;&#35770;&#25991;&#20043;&#38388;&#30340;&#36923;&#36753;&#23618;&#27425;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20165;&#20165;&#20381;&#36182;&#20110;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#35753;&#30740;&#31350;&#20154;&#21592;&#22256;&#24785;&#20026;&#20160;&#20040;&#25512;&#33616;&#20102;&#29305;&#23450;&#30340;&#25991;&#31456;&#12290;&#20182;&#20204;&#21487;&#33021;&#32570;&#20047;&#20102;&#35299;&#20851;&#20110;&#20182;&#20204;&#24076;&#26395;&#33719;&#24471;&#30340;"&#38382;&#39064;&#35299;&#20915;"&#21644;"&#38382;&#39064;&#21457;&#29616;"&#20043;&#38388;&#30340;&#35265;&#35299;&#36830;&#25509;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25903;&#25345;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
&lt;/p&gt;</description></item><item><title>DistiLLM&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03898</link><description>&lt;p&gt;
DistiLLM: &#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DistiLLM: Towards Streamlined Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03898
&lt;/p&gt;
&lt;p&gt;
DistiLLM&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#23558;&#25945;&#24072;&#27169;&#22411;&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#65288;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;KD&#26041;&#27861;&#23384;&#22312;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#20351;&#29992;&#23398;&#29983;&#29983;&#25104;&#30340;&#36755;&#20986;&#26469;&#35299;&#20915;&#35757;&#32451;-&#25512;&#29702;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#20570;&#27861;&#26174;&#33879;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DistiLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#12290;DistiLLM&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#65292;&#25105;&#20204;&#25581;&#31034;&#24182;&#21033;&#29992;&#20102;&#23427;&#30340;&#29702;&#35770;&#23646;&#24615;&#65307;&#65288;2&#65289;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#21033;&#29992;&#23398;&#29983;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#25928;&#29575;&#12290;&#21253;&#25324;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#22312;&#20869;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;DistiLLM&#22312;&#26500;&#24314;&#39640;&#24615;&#33021;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing 
&lt;/p&gt;</description></item><item><title>&#20381;&#36182;&#22522;&#20934;&#25490;&#34892;&#27036;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23384;&#22312;&#36739;&#39640;&#25935;&#24863;&#24615;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#25490;&#21517;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#31572;&#26696;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01781</link><description>&lt;p&gt;
&#24403;&#22522;&#20934;&#25104;&#20026;&#30446;&#26631;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25490;&#34892;&#27036;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01781
&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#22522;&#20934;&#25490;&#34892;&#27036;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23384;&#22312;&#36739;&#39640;&#25935;&#24863;&#24615;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#25490;&#21517;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#31572;&#26696;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#20934;&#25490;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#25490;&#34892;&#27036;&#32463;&#24120;&#34987;&#29992;&#26469;&#25351;&#23548;&#23454;&#36341;&#32773;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#12290;&#36890;&#24120;&#65292;&#21457;&#24067;&#30340;&#25490;&#34892;&#27036;&#25490;&#21517;&#34987;&#30452;&#25509;&#25509;&#21463; - &#25105;&#20204;&#34920;&#26126;&#36825;&#26159;&#19968;&#20010;&#65288;&#28508;&#22312;&#26114;&#36149;&#30340;&#65289;&#38169;&#35823;&#12290;&#22312;&#29616;&#26377;&#30340;&#25490;&#34892;&#27036;&#19979;&#65292;LLM&#30340;&#30456;&#23545;&#24615;&#33021;&#23545;&#65288;&#36890;&#24120;&#24494;&#23567;&#30340;&#65289;&#32454;&#33410;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#27969;&#34892;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22522;&#20934;&#65288;&#20363;&#22914;MMLU&#65289;&#65292;&#23545;&#22522;&#20934;&#30340;&#24494;&#23567;&#25200;&#21160;&#65292;&#22914;&#25913;&#21464;&#36873;&#39033;&#39034;&#24207;&#25110;&#31572;&#26696;&#36873;&#25321;&#26041;&#27861;&#65292;&#20250;&#23548;&#33268;&#25490;&#21517;&#21464;&#21270;&#36798;&#21040;8&#20010;&#20301;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#25200;&#21160;&#31867;&#21035;&#36827;&#34892;&#31995;&#32479;&#23454;&#39564;&#24182;&#30830;&#23450;&#36825;&#19968;&#34892;&#20026;&#30340;&#26469;&#28304;&#26469;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#20248;&#21270;&#30340;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#36827;&#34892;&#31572;&#26696;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20381;&#36182;&#31616;&#21333;&#22522;&#20934;&#35780;&#20272;&#30340;&#39118;&#38505;&#65292;&#24182;&#20026;&#26356;&#20581;&#22766;&#30340;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#25351;&#23548;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#19982;&#23545;&#35805;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;DST&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2311.15623</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Injecting linguistic knowledge into BERT for Dialogue State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#19982;&#23545;&#35805;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;DST&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#25512;&#29702;&#36807;&#31243;&#32570;&#20047;&#36879;&#26126;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26694;&#26550;&#25552;&#21462;&#35821;&#35328;&#30693;&#35782;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#22686;&#24378;BERT&#22312;DST&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#30693;&#35782;&#25552;&#21462;&#36807;&#31243;&#35745;&#31639;&#32463;&#27982;&#39640;&#25928;&#65292;&#19981;&#38656;&#35201;&#27880;&#37322;&#25110;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27880;&#20837;&#25552;&#21462;&#30340;&#30693;&#35782;&#21482;&#38656;&#35201;&#28155;&#21152;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#12290;&#25105;&#20204;&#20351;&#29992;&#20984;&#22810;&#38754;&#20307;&#27169;&#22411;(CPM)&#20316;&#20026;DST&#20219;&#21153;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#65292;&#24182;&#34920;&#26126;&#25152;&#33719;&#21462;&#30340;&#29305;&#24449;&#19982;&#23545;&#35805;&#20013;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#12290;&#36825;&#31181;&#30456;&#20851;&#24615;&#26377;&#21161;&#20110;&#20840;&#38754;&#29702;&#35299;&#24433;&#21709;DST&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;DST&#20219;&#21153;&#19978;&#23545;&#36825;&#20010;&#26694;&#26550;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and ob
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#24037;&#31243;&#20219;&#21153;&#23545;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;PE2&#26041;&#27861;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#30340;&#27880;&#20837;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.05661</link><description>&lt;p&gt;
Prompt Engineering a Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering a Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#20219;&#21153;&#23545;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;PE2&#26041;&#27861;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#30340;&#27880;&#20837;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#26159;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#26816;&#26597;&#27169;&#22411;&#30340;&#38169;&#35823;&#65292;&#20551;&#35774;&#24403;&#21069;&#25552;&#31034;&#20013;&#32570;&#23569;&#25110;&#35823;&#23548;&#20102;&#20160;&#20040;&#65292;&#24182;&#28165;&#26224;&#22320;&#20256;&#36798;&#20219;&#21153;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#20803;&#25552;&#31034;&#26469;&#25191;&#34892;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#30001;&#20110;&#20803;&#25552;&#31034;&#20013;&#32570;&#20047;&#22797;&#26434;&#25512;&#29702;&#30340;&#20805;&#20998;&#25351;&#23548;&#65292;&#23427;&#20204;&#30340;&#28508;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#27880;&#20837;&#21040;&#20803;&#25552;&#31034;&#20013;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25152;&#24471;&#21040;&#30340;&#26041;&#27861;&#31216;&#20026;PE2&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20219;&#21153;&#20013;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#12290;&#23427;&#25214;&#21040;&#30340;&#25552;&#31034;&#22312;MultiArith&#19978;&#27604;&#8220;&#25353;&#27493;&#39588;&#24605;&#32771;&#8221;&#39640;&#20986;6.3%&#65292;&#22312;GSM8K&#19978;&#39640;&#20986;3.1%&#65292;&#24182;&#22312;&#23545;&#31435;&#20219;&#21153;&#19978;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05661v2 Announce Type: replace-cross  Abstract: Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, showcases remarkable versatility across diverse language tasks. It finds prompts that outperform "let's think step by step" by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20195;&#30721;&#29420;&#29305;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#27861;SWEET&#65292;&#36890;&#36807;&#22312;&#20855;&#26377;&#39640;&#29109;&#30340;&#20301;&#32622;&#20165;&#25918;&#32622;&#8220;&#32511;&#33394;&#8221;&#20196;&#29260;&#26469;&#30830;&#20445;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#27979;&#35797;&#21644;Z&#20998;&#25968;&#36827;&#34892;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2305.15060</link><description>&lt;p&gt;
&#35841;&#32534;&#20889;&#20102;&#36825;&#27573;&#20195;&#30721;&#65311;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Who Wrote this Code? Watermarking for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15060
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20195;&#30721;&#29420;&#29305;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#27861;SWEET&#65292;&#36890;&#36807;&#22312;&#20855;&#26377;&#39640;&#29109;&#30340;&#20301;&#32622;&#20165;&#25918;&#32622;&#8220;&#32511;&#33394;&#8221;&#20196;&#29260;&#26469;&#30830;&#20445;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#27979;&#35797;&#21644;Z&#20998;&#25968;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#33394;&#29983;&#25104;&#24615;&#33021;&#65292;&#20851;&#20110;&#20351;&#29992;&#23427;&#20204;&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#38382;&#39064;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#22914;&#25220;&#34989;&#21644;&#29256;&#26435;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#27700;&#21360;&#21644;&#26816;&#27979;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;&#26041;&#27861;&#30001;&#20110;&#20195;&#30721;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#26080;&#27861;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#12290;&#22522;&#20110;Kirchenbauer&#31561;&#20154;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#21517;&#20026;Selective WatErmarking via Entropy Thresholding&#65288;SWEET&#65289;&#65292;&#35813;&#26041;&#27861;&#20165;&#22312;&#29983;&#25104;&#26399;&#38388;&#23558;&#8220;&#32511;&#33394;&#8221;&#20196;&#29260;&#25918;&#32622;&#22312;&#20855;&#26377;&#39640;&#29109;&#30340;&#20196;&#29260;&#20998;&#24067;&#20301;&#32622;&#65292;&#20174;&#32780;&#20445;&#30041;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;&#27700;&#21360;&#20195;&#30721;&#36890;&#36807;&#22522;&#20110;&#29109;&#20449;&#24687;&#30340;&#32479;&#35745;&#27979;&#35797;&#21644;Z&#20998;&#25968;&#36827;&#34892;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;HumanEval&#21644;MBPP&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SWEET&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.15060v3 Announce Type: replace  Abstract: With the remarkable generation performance of large language models, ethical and legal concerns about using them have been raised, such as plagiarism and copyright issues. For such concerns, several approaches to watermark and detect LLM-generated text have been proposed very recently. However, we discover that the previous methods fail to function appropriately with code generation tasks because of the syntactic and semantic characteristics of code. Based on \citet{Kirchenbauer2023watermark}, we propose a new watermarking method, Selective WatErmarking via Entropy Thresholding (SWEET), that promotes "green" tokens only at the position with high entropy of the token distribution during generation, thereby preserving the correctness of the generated code. The watermarked code is detected by the statistical test and Z-score based on the entropy information. Our experiments on HumanEval and MBPP show that SWEET significantly improves th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;transformer&#32593;&#32476;&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#30382;&#23618;&#27874;&#22312;&#25552;&#21462;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14267</link><description>&lt;p&gt;
Transformers&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#65306;&#22312;&#26102;&#38388;&#19978;&#20256;&#36882;&#19978;&#19979;&#25991;&#30340;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers and Cortical Waves: Encoders for Pulling In Context Across Time. (arXiv:2401.14267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;transformer&#32593;&#32476;&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#30382;&#23618;&#27874;&#22312;&#25552;&#21462;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;ChatGPT&#21644;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;transformer&#32593;&#32476;&#30340;&#33021;&#21147;&#24050;&#32463;&#24341;&#36215;&#20102;&#19990;&#30028;&#30340;&#20851;&#27880;&#12290;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#23558;&#23436;&#25972;&#30340;&#36755;&#20837;&#24207;&#21015;&#65288;&#20363;&#22914;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#21333;&#35789;&#65289;&#36716;&#21270;&#20026;&#19968;&#20010;&#38271;&#30340;&#8220;&#32534;&#30721;&#21521;&#37327;&#8221;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#23398;&#20064;&#33258;&#28982;&#24207;&#21015;&#20013;&#30340;&#38271;&#31243;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#8220;&#33258;&#27880;&#24847;&#21147;&#8221;&#24212;&#29992;&#20110;&#36825;&#20010;&#32534;&#30721;&#21521;&#37327;&#65292;&#36890;&#36807;&#35745;&#31639;&#36755;&#20837;&#24207;&#21015;&#20013;&#21333;&#35789;&#23545;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#22686;&#24378;&#20102;transformer&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#35748;&#20026;&#31070;&#32463;&#27963;&#21160;&#22312;&#21333;&#20010;&#30382;&#23618;&#21306;&#22495;&#20869;&#25110;&#25972;&#20010;&#22823;&#33041;&#33539;&#22260;&#20869;&#20256;&#25773;&#30340;&#27874;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#32534;&#30721;&#21407;&#29702;&#12290;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#21051;&#23558;&#26368;&#36817;&#30340;&#36755;&#20837;&#21382;&#21490;&#23553;&#35013;&#20026;&#21333;&#20010;&#31354;&#38388;&#27169;&#24335;&#65292;&#30382;&#23618;&#27874;&#21487;&#20197;&#20174;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#25552;&#21462;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#36825;&#19982;&#35745;&#31639;&#21407;&#29702;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention. The crucial computational mechanism underlying their performance relies on transforming a complete input sequence - for example, all the words in a sentence into a long "encoding vector" - that allows transformers to learn long-range temporal dependencies in naturalistic sequences. Specifically, "self-attention" applied to this encoding vector enhances temporal context in transformers by computing associations between pairs of words in the input sequence. We suggest that waves of neural activity, traveling across single cortical regions or across multiple regions at the whole-brain scale, could implement a similar encoding principle. By encapsulating recent input history into a single spatial pattern at each moment in time, cortical waves may enable temporal context to be extracted from sequences of sensory inputs, the same computational principle used in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#36923;&#36753;&#22238;&#24402;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33609;&#22270;&#24341;&#23548;&#32422;&#26463;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#36741;&#21161;&#27169;&#22411;&#20248;&#21270;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#21021;&#27493;&#36755;&#20986;&#20316;&#20026;&#36827;&#19968;&#27493;&#25193;&#23637;&#30340; "&#33609;&#22270;"&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26377;&#38480;&#32422;&#26463;&#35299;&#30721;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09967</link><description>&lt;p&gt;
&#26080;&#38656;&#35775;&#38382;&#36923;&#36753;&#22238;&#24402;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33609;&#22270;&#24341;&#23548;&#32422;&#26463;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access. (arXiv:2401.09967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#36923;&#36753;&#22238;&#24402;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33609;&#22270;&#24341;&#23548;&#32422;&#26463;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#36741;&#21161;&#27169;&#22411;&#20248;&#21270;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#21021;&#27493;&#36755;&#20986;&#20316;&#20026;&#36827;&#19968;&#27493;&#25193;&#23637;&#30340; "&#33609;&#22270;"&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26377;&#38480;&#32422;&#26463;&#35299;&#30721;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#32422;&#26463;&#22312;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#25511;&#21046;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#26550;&#26500;&#20462;&#25913;&#30340;&#26041;&#24335;&#65292;&#20294;&#36890;&#24120;&#21482;&#36866;&#29992;&#20110;&#25317;&#26377;&#36923;&#36753;&#22238;&#24402;&#35775;&#38382;&#26435;&#38480;&#30340;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33609;&#22270;&#24341;&#23548;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32422;&#26463;&#35299;&#30721;&#65288;SGCD&#65289;&#26041;&#27861;&#65292;&#26080;&#38656;&#35775;&#38382;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#22238;&#24402;&#12290;SGCD&#21033;&#29992;&#26412;&#22320;&#36741;&#21161;&#27169;&#22411;&#26469;&#20248;&#21270;&#26080;&#32422;&#26463;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23558;&#20854;&#20316;&#20026;&#36827;&#19968;&#27493;&#25193;&#23637;&#30340;&#8220;&#33609;&#22270;&#8221;&#12290;&#27492;&#26041;&#27861;&#21487;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#36923;&#36753;&#22238;&#24402;&#30340;&#25216;&#26415;&#30456;&#20114;&#34917;&#20805;&#65292;&#20351;&#26377;&#38480;&#32422;&#26463;&#35299;&#30721;&#22312;&#26080;&#27861;&#23436;&#20840;&#36879;&#26126;&#30340;&#27169;&#22411;&#29615;&#22659;&#20013;&#24212;&#29992;&#12290;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;SGCD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constrained decoding, a technique for enforcing constraints on language model outputs, offers a way to control text generation without retraining or architectural modifications. Its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a novel approach to constrained decoding for blackbox LLMs, which operates without access to the logits of the blackbox LLM. SGCD utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, effectively treating this initial output as a "sketch" for further elaboration. This approach is complementary to traditional logit-based techniques and enables the application of constrained decoding in settings where full model transparency is unavailable. We demonstrate the efficacy of SGCD through experiments in cl
&lt;/p&gt;</description></item><item><title>GlotLID-M&#26159;&#19968;&#20010;&#28385;&#36275;&#24191;&#27867;&#35206;&#30422;&#12289;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#35201;&#27714;&#30340;&#35821;&#35328;&#35782;&#21035;&#27169;&#22411;&#65292;&#20855;&#26377;1665&#20010;&#21487;&#35782;&#21035;&#35821;&#35328;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#35299;&#20915;&#20102;&#20302;&#36164;&#28304;LID&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22686;&#24378;&#35775;&#38382;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16248</link><description>&lt;p&gt;
GlotLID: &#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GlotLID: Language Identification for Low-Resource Languages. (arXiv:2310.16248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16248
&lt;/p&gt;
&lt;p&gt;
GlotLID-M&#26159;&#19968;&#20010;&#28385;&#36275;&#24191;&#27867;&#35206;&#30422;&#12289;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#35201;&#27714;&#30340;&#35821;&#35328;&#35782;&#21035;&#27169;&#22411;&#65292;&#20855;&#26377;1665&#20010;&#21487;&#35782;&#21035;&#35821;&#35328;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#35299;&#20915;&#20102;&#20302;&#36164;&#28304;LID&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22686;&#24378;&#35775;&#38382;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20960;&#31687;&#35770;&#25991;&#21457;&#34920;&#20102;&#38024;&#23545;&#32422;300&#31181;&#39640;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#35328;&#35782;&#21035;&#65288;LID&#65289;&#30340;&#33391;&#22909;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21487;&#29992;&#30340;LID&#28385;&#36275;&#20197;&#19979;&#35201;&#27714;&#65306;&#65288;i&#65289;&#28085;&#30422;&#24191;&#27867;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#65288;ii&#65289;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#19988;&#21487;&#38752;&#65292;&#65288;iii&#65289;&#39640;&#25928;&#26131;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;GlotLID-M&#65292;&#19968;&#20010;&#28385;&#36275;&#24191;&#27867;&#35206;&#30422;&#12289;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#35201;&#27714;&#30340;LID&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#35782;&#21035;1665&#31181;&#35821;&#35328;&#65292;&#22312;&#35206;&#30422;&#33539;&#22260;&#19978;&#30456;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#26377;&#20102;&#22823;&#24133;&#22686;&#21152;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;GlotLID-M&#22312;&#24179;&#34913;F1&#20998;&#25968;&#21644;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;&#26041;&#38754;&#20248;&#20110;&#22235;&#20010;&#22522;&#20934;&#27169;&#22411;&#65288;CLD3&#65292;FT176&#65292;OpenLID&#21644;NLLB&#65289;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20302;&#36164;&#28304;LID&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65306;&#19981;&#27491;&#30830;&#30340;&#35821;&#26009;&#24211;&#20803;&#25968;&#25454;&#65292;&#26469;&#33258;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#27844;&#28431;&#65292;&#38590;&#20197;&#21306;&#20998;&#23494;&#20999;&#30456;&#20851;&#30340;&#35821;&#35328;&#65292;&#22788;&#29702;&#23439;&#35821;&#35328;&#19982;&#26041;&#35328;&#65292;&#20197;&#21450;&#19968;&#33324;&#30340;&#22122;&#22768;&#25968;&#25454;&#12290;&#25105;&#20204;&#24076;&#26395;&#23558;GlotLID-M&#38598;&#25104;&#21040;&#25968;&#25454;&#38598;&#21019;&#24314;&#27969;&#31243;&#20013;&#65292;&#20197;&#25552;&#39640;&#36136;&#37327;&#21644;&#22686;&#24378;&#35775;&#38382;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent papers have published good solutions for language identification (LID) for about 300 high-resource and medium-resource languages. However, there is no LID available that (i) covers a wide range of low-resource languages, (ii) is rigorously evaluated and reliable and (iii) efficient and easy to use. Here, we publish GlotLID-M, an LID model that satisfies the desiderata of wide coverage, reliability and efficiency. It identifies 1665 languages, a large increase in coverage compared to prior work. In our experiments, GlotLID-M outperforms four baselines (CLD3, FT176, OpenLID and NLLB) when balancing F1 and false positive rate (FPR). We analyze the unique challenges that low-resource LID poses: incorrect corpus metadata, leakage from high-resource languages, difficulty separating closely related languages, handling of macrolanguage vs varieties and in general noisy data. We hope that integrating GlotLID-M into dataset creation pipelines will improve quality and enhance acces
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31264;&#23494;&#19977;&#32500;&#24341;&#29992;&#32593;&#32476;ConcreteNet&#65292;&#21253;&#21547;&#19977;&#20010;&#26032;&#27169;&#22359;&#65292;&#26088;&#22312;&#25913;&#21892;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#24178;&#25200;&#22240;&#32032;&#30340;&#37325;&#22797;&#23454;&#20363;&#30340;&#24341;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04561</link><description>&lt;p&gt;
&#25913;&#36827;&#31264;&#23494;&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#30340;&#19977;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Three Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding. (arXiv:2309.04561v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31264;&#23494;&#19977;&#32500;&#24341;&#29992;&#32593;&#32476;ConcreteNet&#65292;&#21253;&#21547;&#19977;&#20010;&#26032;&#27169;&#22359;&#65292;&#26088;&#22312;&#25913;&#21892;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#24178;&#25200;&#22240;&#32032;&#30340;&#37325;&#22797;&#23454;&#20363;&#30340;&#24341;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#26159;&#25351;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#23450;&#20301;&#19977;&#32500;&#22330;&#26223;&#20013;&#34987;&#24341;&#29992;&#30340;&#29289;&#20307;&#30340;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#22312;&#33258;&#20027;&#23460;&#20869;&#26426;&#22120;&#20154;&#21040;AR/VR&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#30446;&#21069;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#26816;&#27979;&#26469;&#23436;&#25104;&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#65292;&#21363;&#36890;&#36807;&#36793;&#30028;&#26694;&#26469;&#23450;&#20301;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#36827;&#34892;&#29289;&#29702;&#20132;&#20114;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36793;&#30028;&#26694;&#19981;&#36275;&#20197;&#25551;&#36848;&#29289;&#20307;&#30340;&#20960;&#20309;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31264;&#23494;&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#30340;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#24341;&#29992;&#30340;&#19977;&#32500;&#23454;&#20363;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31264;&#23494;&#19977;&#32500;&#24341;&#29992;&#32593;&#32476;ConcreteNet&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#20010;&#29420;&#31435;&#30340;&#26032;&#27169;&#22359;&#65292;&#26088;&#22312;&#25913;&#36827;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#24178;&#25200;&#22240;&#32032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#22797;&#23454;&#20363;&#30340;&#24341;&#29992;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#19979;&#32780;&#19978;&#30340;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#65292;&#26088;&#22312;&#28040;&#38500;&#23454;&#20363;&#38388;&#20851;&#31995;&#32447;&#32034;&#30340;&#27495;&#20041;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#36896;&#19968;&#20010;cont
&lt;/p&gt;
&lt;p&gt;
3D visual grounding is the task of localizing the object in a 3D scene which is referred by a description in natural language. With a wide range of applications ranging from autonomous indoor robotics to AR/VR, the task has recently risen in popularity. A common formulation to tackle 3D visual grounding is grounding-by-detection, where localization is done via bounding boxes. However, for real-life applications that require physical interactions, a bounding box insufficiently describes the geometry of an object. We therefore tackle the problem of dense 3D visual grounding, i.e. referral-based 3D instance segmentation. We propose a dense 3D grounding network ConcreteNet, featuring three novel stand-alone modules which aim to improve grounding performance for challenging repetitive instances, i.e. instances with distractors of the same semantic class. First, we introduce a bottom-up attentive fusion module that aims to disambiguate inter-instance relational cues, next we construct a cont
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.00081</link><description>&lt;p&gt;
&#20026;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26500;&#24314;&#35821;&#20041;&#20016;&#23500;&#30340;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Semantically Enriched Embeddings for Knowledge Graph Completion. (arXiv:2308.00081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#31639;&#27861;&#23558;&#30693;&#35782;&#22270;&#35889;&#35270;&#20026;&#19968;&#20010;&#22810;&#21521;&#26631;&#35760;&#22270;&#65292;&#32570;&#20047;&#25429;&#25417;&#24213;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25429;&#33719;&#20102;&#22823;&#37327;&#20449;&#24687;&#65292;&#36825;&#19968;&#25429;&#33719;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;&#30693;&#35782;&#22270;&#35889;&#21487;&#20197;&#20174;LLMs&#20013;&#21463;&#30410;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#19981;&#21516;&#29983;&#25104;&#23884;&#20837;&#27169;&#22411;&#21464;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#21508;&#31181;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#65292;&#22914;&#36716;&#23548;&#21644;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20197;&#21450;&#23454;&#20307;&#31867;&#22411;&#39044;&#27979;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#31867;&#22411;&#20449;&#24687;&#12289;LLMs&#20197;&#21450;&#25429;&#25417;&#19981;&#21516;&#25551;&#36848;&#36923;&#36753;&#20844;&#29702;&#20013;&#30340;&#35821;&#20041;&#30340;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#31639;&#27861;&#30340;&#20851;&#38190;&#21453;&#24605;&#23545;&#35770;&#25991;&#36827;&#34892;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding based Knowledge Graph (KG) Completion has gained much attention over the past few years. Most of the current algorithms consider a KG as a multidirectional labeled graph and lack the ability to capture the semantics underlying the schematic information. In a separate development, a vast amount of information has been captured within the Large Language Models (LLMs) which has revolutionized the field of Artificial Intelligence. KGs could benefit from these LLMs and vice versa. This vision paper discusses the existing algorithms for KG completion based on the variations for generating KG embeddings. It starts with discussing various KG completion algorithms such as transductive and inductive link prediction and entity type prediction algorithms. It then moves on to the algorithms utilizing type information within the KGs, LLMs, and finally to algorithms capturing the semantics represented in different description logic axioms. We conclude the paper with a critical reflection on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRD&#31639;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#33258;&#25105;&#25552;&#21319;&#21644;&#20301;&#32622;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02762</link><description>&lt;p&gt;
PRD: &#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations. (arXiv:2307.02762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRD&#31639;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#33258;&#25105;&#25552;&#21319;&#21644;&#20301;&#32622;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#22238;&#31572;&#36136;&#37327;&#22312;&#33258;&#21160;&#21270;&#26041;&#38754;&#24456;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#24182;&#20027;&#35201;&#20351;&#29992;LLMs&#20316;&#20026;&#26080;&#21442;&#32771;&#24230;&#37327;&#34913;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21442;&#32771;&#25351;&#26631;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20182;&#20204;&#20197;&#34987;&#35748;&#20026;&#26159;&#8220;&#26368;&#24378;&#8221;&#30340;LLM&#20316;&#20026;&#35780;&#20272;&#22120;&#65292;&#23545;&#20505;&#36873;&#27169;&#22411;&#30340;&#31572;&#26696;&#36827;&#34892;&#20004;&#20004;&#27604;&#36739;&#24182;&#25552;&#20379;&#25490;&#21517;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#23384;&#22312;&#22810;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#24102;&#26469;&#33258;&#25105;&#25552;&#21319;&#65288;&#38738;&#30544;&#33258;&#24049;&#30340;&#31572;&#26696;&#65289;&#21644;&#20301;&#32622;&#20559;&#35265;&#12290;&#25105;&#20204;&#20174;&#25945;&#32946;&#39046;&#22495;&#65288;Cho and MacArthur, 2011&#65307;Walsh, 2014&#65289;&#20013;&#27762;&#21462;&#35265;&#35299;&#21644;&#25945;&#35757;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#21516;&#34892;&#35780;&#32423;&#65288;PR&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32771;&#34385;&#27599;&#20010;&#21516;&#34892;LLM&#23545;&#25152;&#26377;&#31572;&#26696;&#23545;&#30340;&#20004;&#20004;&#20559;&#22909;&#65292;&#24182;&#36755;&#20986;&#27169;&#22411;&#30340;&#26368;&#32456;&#25490;&#21517;&#65307;&#20197;&#21450;&#65288;2&#65289;&#21516;&#34892;&#35752;&#35770;&#65288;PD&#65289;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#20419;&#20351;&#20004;&#20010;LLMs&#36827;&#34892;&#35752;&#35770;&#24182;&#23581;&#35797;&#23601;&#20004;&#20010;&#20559;&#22909;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the quality of responses generated by different modern large language models (LLMs) are hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs as a reference-free metric for open-ended question answering. More specifically, they use the recognized "strongest" LLM as the evaluator, which conducts pairwise comparisons of candidate models' answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose the (1) peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on preferences of two an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#32593;&#32476;&#65292;&#33258;&#21160;&#21270;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#20892;&#27665;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#20998;&#26512;&#20026;&#20363;&#65292;&#25552;&#21462;&#21464;&#37327;&#20851;&#31995;&#24182;&#20351;&#29992;&#32593;&#32476;&#32508;&#21512;&#20854;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.09737</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#32593;&#32476;&#33258;&#21160;&#21270;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#65306;&#20197;&#20892;&#27665;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Using Natural Language Processing and Networks to Automate Structured Literature Reviews: An Application to Farmers Climate Change Adaptation. (arXiv:2306.09737v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#32593;&#32476;&#65292;&#33258;&#21160;&#21270;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#20892;&#27665;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#20998;&#26512;&#20026;&#20363;&#65292;&#25552;&#21462;&#21464;&#37327;&#20851;&#31995;&#24182;&#20351;&#29992;&#32593;&#32476;&#32508;&#21512;&#20854;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30740;&#31350;&#25991;&#31456;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23398;&#32773;&#20204;&#24456;&#38590;&#36319;&#19978;&#19982;&#33258;&#24049;&#19987;&#19994;&#39046;&#22495;&#30456;&#20851;&#30340;&#26032;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#38656;&#35201;&#36328;&#23398;&#31185;&#35299;&#20915;&#26041;&#26696;&#30340;&#22797;&#26434;&#20027;&#39064;&#65292;&#22914;&#27668;&#20505;&#21464;&#21270;&#65292;&#36328;&#23398;&#31185;&#30740;&#31350;&#20043;&#38388;&#30340;&#30693;&#35782;&#38142;&#25509;&#20063;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#40657;&#21283;&#23376;&#31867;&#22411;&#30340;&#25991;&#26412;&#25688;&#35201;&#30340;&#20852;&#36215;&#20351;&#24471;&#29702;&#35299;&#25991;&#26412;&#20851;&#31995;&#30340;&#24314;&#31435;&#21464;&#24471;&#22256;&#38590;&#65292;&#26356;&#19981;&#29992;&#35828;&#19982;&#24050;&#26377;&#29702;&#35770;&#27010;&#24565;&#21270;&#22240;&#26524;&#20851;&#31995;&#24182;&#36827;&#34892;&#20551;&#35774;&#30340;&#30456;&#20851;&#24615;&#20102;&#12290;&#26412;&#25991;&#26088;&#22312;&#21512;&#29702;&#22320;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#36890;&#36807;&#25552;&#21462;&#21464;&#37327;&#20851;&#31995;&#24182;&#20351;&#29992;&#32593;&#32476;&#32508;&#21512;&#20854;&#21457;&#29616;&#65292;&#21516;&#26102;&#19982;&#30456;&#20851;&#23398;&#31185;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#20851;&#38190;&#27010;&#24565;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#20197;&#20892;&#27665;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#20998;&#26512;&#20026;&#20363;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;Scopus&#20110;2022&#24180;8&#26376;&#36820;&#22238;&#30340;&#20986;&#29256;&#29289;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#12290;&#32467;&#26524;&#23637;&#31034;...
&lt;/p&gt;
&lt;p&gt;
The fast-growing number of research articles makes it problematic for scholars to keep track of the new findings related to their areas of expertise. Furthermore, linking knowledge across disciplines in rapidly developing fields becomes challenging for complex topics like climate change that demand interdisciplinary solutions. At the same time, the rise of Black Box types of text summarization makes it difficult to understand how text relationships are built, let alone relate to existing theories conceptualizing cause-effect relationships and permitting hypothesizing. This work aims to sensibly use Natural Language Processing by extracting variables relations and synthesizing their findings using networks while relating to key concepts dominant in relevant disciplines. As an example, we apply our methodology to the analysis of farmers' adaptation to climate change. For this, we perform a Natural Language Processing analysis of publications returned by Scopus in August 2022. Results sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17401</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework For Refining Text Classification and Object Recognition from Academic Articles. (arXiv:2305.17401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#39640;&#25928;&#22320;&#20174;&#22823;&#37327;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#21462;&#29305;&#23450;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25366;&#25496;&#23398;&#26415;&#35770;&#25991;&#30340;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33258;&#21160;&#20174;&#22797;&#26434;&#30340;&#38750;&#32467;&#26500;&#21270;&#24067;&#23616;&#25991;&#26723;&#20013;&#25552;&#21462;&#29305;&#23450;&#27169;&#24335;&#12290;&#24403;&#21069;&#30340;&#23398;&#26415;&#35770;&#25991;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#65288;RB&#65289;&#25110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#38656;&#35201;&#32534;&#20889;&#22797;&#26434;&#25490;&#29256;&#35770;&#25991;&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#23545;&#25991;&#31456;&#20013;&#22797;&#26434;&#20869;&#23481;&#31867;&#22411;&#36827;&#34892;&#27880;&#37322;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#23481;&#26131;&#35782;&#21035;&#30340;&#27169;&#24335;&#34987;&#38169;&#35823;&#25552;&#21462;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#20998;&#26512;&#25351;&#23450;&#33879;&#20316;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#24067;&#23616;&#21644;&#25490;&#29256;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of the internet, it has become increasingly crucial to extract specific information from vast amounts of academic articles efficiently. Data mining techniques are generally employed to solve this issue. However, data mining for academic articles is challenging since it requires automatically extracting specific patterns in complex and unstructured layout documents. Current data mining methods for academic articles employ rule-based(RB) or machine learning(ML) approaches. However, using rule-based methods incurs a high coding cost for complex typesetting articles. On the other hand, simply using machine learning methods requires annotation work for complex content types within the paper, which can be costly. Furthermore, only using machine learning can lead to cases where patterns easily recognized by rule-based methods are mistakenly extracted. To overcome these issues, from the perspective of analyzing the standard layout and typesetting used in the specified p
&lt;/p&gt;</description></item></channel></rss>