<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;</title><link>https://rss.arxiv.org/abs/2312.02783</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Graphs: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT4&#21644;LLaMA&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#25991;&#26412;&#32534;&#30721;/&#35299;&#30721;&#33021;&#21147;&#21644;&#26032;&#21457;&#29616;&#30340;&#32039;&#24613;&#33021;&#21147;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#34429;&#28982;LLMs&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#32431;&#25991;&#26412;&#65292;&#20294;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#19982;&#22270;&#24418;&#24418;&#24335;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#30456;&#20851;&#32852;&#65288;&#20363;&#22914;&#23398;&#26415;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#65289;&#65292;&#25110;&#32773;&#22270;&#24418;&#25968;&#25454;&#19982;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#37197;&#23545;&#65288;&#20363;&#22914;&#24102;&#26377;&#25551;&#36848;&#30340;&#20998;&#23376;&#65289;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;LLMs&#24050;&#32463;&#23637;&#31034;&#20102;&#20854;&#22522;&#20110;&#32431;&#25991;&#26412;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#27492;&#31867;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22270;&#24418;&#19978;&#65288;&#21363;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#22330;&#26223;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#12290;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102;&#37319;&#29992;LLMs&#22312;&#22270;&#24418;&#19978;&#30340;&#28508;&#22312;&#22330;&#26223;&#65292;&#20998;&#20026;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-pa
&lt;/p&gt;</description></item><item><title>Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11585</link><description>&lt;p&gt;
Linguacodus&#65306;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#20013;&#36827;&#34892;&#21464;&#38761;&#24615;&#20195;&#30721;&#29983;&#25104;&#30340;&#21327;&#21516;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11585
&lt;/p&gt;
&lt;p&gt;
Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26080;&#32541;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Linguacodus&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#37096;&#32626;&#19968;&#20010;&#21160;&#24577;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#39640;&#32423;&#25968;&#25454;&#22609;&#24418;&#25351;&#20196;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36845;&#20195;&#22320;&#36716;&#25442;&#20026;&#20195;&#30721;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;Linguacodus&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#35780;&#20272;&#21508;&#31181;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20026;&#29305;&#23450;&#20219;&#21153;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#65292;&#24182;&#38416;&#26126;&#20102;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#20195;&#30721;&#12290;Linguacodus&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#20219;&#21153;&#25551;&#36848;&#21644;&#21487;&#25191;&#34892;&#20195;&#30721;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#23545;&#25512;&#36827;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;Design2Code&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03163</link><description>&lt;p&gt;
Design2Code&#65306;&#25105;&#20204;&#31163;&#33258;&#21160;&#21270;&#21069;&#31471;&#24037;&#31243;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Design2Code: How Far Are We From Automating Front-End Engineering?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03163
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;Design2Code&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#39134;&#29467;&#36827;&#30340;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#20197;&#23454;&#29616;&#19968;&#31181;&#26032;&#30340;&#21069;&#31471;&#24320;&#21457;&#33539;&#24335;&#65292;&#20854;&#20013;&#22810;&#27169;&#24577;LLMs&#21487;&#33021;&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;Design2Code&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25163;&#21160;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;484&#20010;&#22810;&#26679;&#21270;&#30495;&#23454;&#32593;&#39029;&#30340;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#22871;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;&#22810;&#27169;&#24577;LLMs&#33021;&#21542;&#29983;&#25104;&#30452;&#25509;&#28210;&#26579;&#20026;&#32473;&#23450;&#21442;&#32771;&#32593;&#39029;&#30340;&#20195;&#30721;&#23454;&#29616;&#65292;&#20197;&#36755;&#20837;&#20026;&#23631;&#24149;&#25130;&#22270;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;&#20154;&#24037;&#35780;&#20272;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;GPT-4V&#21644;Gemini Pro Vision&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#19968;&#20010;&#24320;&#28304;&#30340;Design2Code-18B&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03163v1 Announce Type: new  Abstract: Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development, in which multimodal LLMs might directly convert visual designs into code implementations. In this work, we formalize this as a Design2Code task and conduct comprehensive benchmarking. Specifically, we manually curate a benchmark of 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations. We develop a suite of multimodal prompting methods and show their effectiveness on GPT-4V and Gemini Pro Vision. We further finetune an open-source Design2Code-18B model that su
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REAR&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22686;&#24378;&#23545;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#33258;&#25105;&#24847;&#35782;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.17497</link><description>&lt;p&gt;
REAR&#65306;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#20851;&#27880;&#24230;&#24863;&#30693;&#26816;&#32034;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17497
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REAR&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22686;&#24378;&#23545;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#33258;&#25105;&#24847;&#35782;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#26377;&#38480;&#30340;&#20869;&#37096;&#21442;&#25968;&#21270;&#30693;&#35782;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#33539;&#22260;&#12290;&#23613;&#31649;&#22312;RAG&#30740;&#31350;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;LLMs &#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#26816;&#32034;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#24456;&#21487;&#33021;&#23548;&#33268;&#23545;&#22806;&#37096;&#30693;&#35782;&#65288;&#21363;&#26816;&#32034;&#25991;&#26723;&#65289;&#30340;&#35823;&#23548;&#29978;&#33267;&#38169;&#35823;&#21033;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; REAR&#65292;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;QA&#65289;&#30340;&#20851;&#27880;&#24230;&#24863;&#30693;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#12290;&#20316;&#20026;&#20851;&#38190;&#21160;&#26426;&#65292;&#25105;&#20204;&#26088;&#22312;&#22686;&#24378;LLMs&#23545;&#26469;&#28304;&#30456;&#20851;&#24615;&#30340;&#33258;&#25105;&#24847;&#35782;&#65292;&#20197;&#20415;&#22312;RAG&#31995;&#32479;&#20013;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;LLM&#30340;RAG&#31995;&#32479;&#26550;&#26500;&#65292;&#36890;&#36807;&#25972;&#21512;&#19968;&#20010;&#31934;&#30830;&#35780;&#20272;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#29305;&#21035;&#35774;&#35745;&#30340;&#25490;&#21517;&#22836;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17497v1 Announce Type: new  Abstract: Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#20851;&#27880;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#29702;&#35299;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.17304</link><description>&lt;p&gt;
&#25506;&#31350;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#23616;&#21644;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Probing Multimodal Large Language Models for Global and Local Semantic Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17304
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#20851;&#27880;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#29702;&#35299;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#23558;&#20854;&#20248;&#31168;&#30340;&#34920;&#31034;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#24577;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#21033;&#29992;&#22270;&#20687;&#25551;&#36848;&#23545;&#40784;&#25968;&#25454;&#38598;&#35757;&#32451;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;MLLMs&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#23436;&#25972;&#30340;&#22270;&#20687;&#20449;&#24687;&#65292;&#21363;&#20840;&#23616;&#20449;&#24687;&#65292;&#25110;&#32773;&#23427;&#20204;&#21482;&#33021;&#25429;&#25417;&#19968;&#20123;&#23616;&#37096;&#23545;&#35937;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#21487;&#20197;&#32534;&#30721;&#26356;&#22810;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#20854;&#34920;&#31034;&#21521;&#37327;&#22312;&#35270;&#35273;-&#35821;&#35328;&#34164;&#28085;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#19981;&#26159;&#39030;&#23618;&#12290;&#25105;&#20204;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#36827;&#19968;&#27493;&#25506;&#31350;&#27169;&#22411;&#30340;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#19987;&#27880;&#20110;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#20943;&#24369;&#20102;&#23545;&#20840;&#23616;&#20449;&#24687;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17304v1 Announce Type: cross  Abstract: The success of large language models has inspired researchers to transfer their exceptional representing ability to other modalities. Several recent works leverage image-caption alignment datasets to train multimodal large language models (MLLMs), which achieve state-of-the-art performance on image-to-text tasks. However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local object information. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models for local semantic representation through object detection tasks. And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to en
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22312;&#31526;&#21495;&#32570;&#22833;&#26102;&#36890;&#36807;&#27880;&#24847;&#21147;&#35268;&#33539;&#21270;&#25913;&#36827;&#20195;&#30721;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#31243;&#24207;&#20998;&#26512;&#25552;&#21462;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#25513;&#30721;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;&#20851;&#27880;&#24230;&#30340;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2402.11842</link><description>&lt;p&gt;
CodeArt&#65306;&#24403;&#31526;&#21495;&#32570;&#22833;&#26102;&#36890;&#36807;&#27880;&#24847;&#21147;&#35268;&#33539;&#21270;&#25913;&#36827;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11842
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22312;&#31526;&#21495;&#32570;&#22833;&#26102;&#36890;&#36807;&#27880;&#24847;&#21147;&#35268;&#33539;&#21270;&#25913;&#36827;&#20195;&#30721;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#31243;&#24207;&#20998;&#26512;&#25552;&#21462;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#25513;&#30721;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;&#20851;&#27880;&#24230;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#20195;&#30721;&#27169;&#22411;&#22312;&#35768;&#22810;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;&#31526;&#21495;&#32570;&#22833;&#25110;&#32773;&#19981;&#20855;&#20449;&#24687;&#37327;&#26102;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20250;&#19979;&#38477;&#12290;&#36825;&#26159;&#22240;&#20026;&#27169;&#22411;&#21487;&#33021;&#27809;&#26377;&#23398;&#20250;&#22312;&#27809;&#26377;&#31526;&#21495;&#30340;&#24773;&#20917;&#19979;&#27491;&#30830;&#22320;&#20851;&#27880;&#30456;&#20851;&#24615;/&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#31526;&#21495;&#32570;&#22833;&#26102;&#39044;&#35757;&#32451;&#36890;&#29992;&#20195;&#30721;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31243;&#24207;&#20250;&#36864;&#21270;&#20026;&#29992;&#38750;&#24120;&#21407;&#22987;&#30340;&#35821;&#35328;&#32534;&#20889;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#31243;&#24207;&#20998;&#26512;&#26469;&#20107;&#20808;&#25552;&#21462;&#19978;&#19979;&#25991;&#65288;&#32780;&#19981;&#26159;&#20687;&#20256;&#32479;&#27169;&#22411;&#20013;&#20381;&#36182;&#31526;&#21495;&#21644;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#26041;&#27861;&#65292;&#21482;&#20801;&#35768;&#27169;&#22411;&#20851;&#27880;&#36825;&#20123;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#21452;&#21521;&#31243;&#24207;&#20381;&#36182;&#20256;&#36882;&#38381;&#21253;&#21644;&#20196;&#29260;&#20849;&#29616;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20869;&#22312;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#34987;&#29992;&#20110;&#23398;&#20064;&#20801;&#35768;&#30340;&#20851;&#27880;&#24230;&#21738;&#20123;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11842v1 Announce Type: cross  Abstract: Transformer based code models have impressive performance in many software engineering tasks. However, their effectiveness degrades when symbols are missing or not informative. The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. We propose a new method to pre-train general code models when symbols are lacking. We observe that in such cases, programs degenerate to something written in a very primitive language. We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models). We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. In the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more impo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04838</link><description>&lt;p&gt;
PaDeLLM-NER&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24182;&#34892;&#35299;&#30721;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;LLMs&#30340;&#39640;&#24310;&#36831;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#39034;&#24207;&#35299;&#30721;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;NER&#30340;&#25152;&#26377;&#26631;&#31614;&#21644;&#25552;&#21450;&#65292;&#26174;&#33879;&#22686;&#21152;&#20102;&#24207;&#21015;&#38271;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PaDeLLM-NER&#65288;Parallel Decoding in LLM for NE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#27169;&#22359;&#25110;&#26550;&#26500;&#20462;&#25913;&#21363;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#12290;PaDeLLM-NER&#20801;&#35768;&#21516;&#26102;&#35299;&#30721;&#25152;&#26377;&#25552;&#21450;&#65292;&#20174;&#32780;&#20943;&#23569;&#29983;&#25104;&#24310;&#36831;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PaDeLLM-NER&#30340;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#23545;&#33521;&#35821;&#21644;&#20013;&#25991;&#26469;&#35828;&#27604;&#33258;&#22238;&#24402;&#26041;&#27861;&#24555;1.76&#21040;10.22&#20493;&#12290;&#19982;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#32500;&#25345;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#22810;&#39046;&#22495;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26102;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#27969;&#27700;&#32447;&#25110;&#26631;&#31614;&#65292;&#21516;&#26102;&#20445;&#25345;&#25104;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2305.14336</link><description>&lt;p&gt;
&#26469;&#33258;&#24322;&#26500;&#34920;&#26684;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Schema-Driven Information Extraction from Heterogeneous Tables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#22810;&#39046;&#22495;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26102;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#27969;&#27700;&#32447;&#25110;&#26631;&#31614;&#65292;&#21516;&#26102;&#20445;&#25345;&#25104;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25903;&#25345;&#39640;&#25928;&#22320;&#20174;&#34920;&#26684;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#36825;&#26159;&#19968;&#39033;&#23558;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#25353;&#29031;&#20154;&#31867;&#32534;&#20889;&#30340;&#27169;&#24335;&#32452;&#32455;&#30340;&#35760;&#24405;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#35780;&#20272;&#21508;&#31181;LLM&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#34920;&#26684;&#65306;&#26426;&#22120;&#23398;&#20064;&#35770;&#25991;&#12289;&#21270;&#23398;&#25991;&#29486;&#12289;&#26448;&#26009;&#31185;&#23398;&#26399;&#21002;&#21644;&#32593;&#39029;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#34920;&#26684;&#38598;&#21512;&#26469;&#35780;&#20272;&#24320;&#28304;&#21644;&#22522;&#20110;API&#30340;&#35821;&#35328;&#27169;&#22411;&#20174;&#28085;&#30422;&#22810;&#31181;&#39046;&#22495;&#21644;&#25968;&#25454;&#26684;&#24335;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#30340;&#27969;&#27700;&#32447;&#25110;&#26631;&#31614;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#20986;&#20154;&#24847;&#26009;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;F1&#20998;&#25968;&#33539;&#22260;&#20174;74.2&#21040;96.1&#65292;&#21516;&#26102;&#20445;&#25345;&#25104;&#26412;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14336v3 Announce Type: replace  Abstract: In this paper, we explore the question of whether large language models can support cost-efficient information extraction from tables. We introduce schema-driven information extraction, a new task that transforms tabular data into structured records following a human-authored schema. To assess various LLM's capabilities on this task, we present a benchmark comprised of tables from four diverse domains: machine learning papers, chemistry literature, material science journals, and webpages. We use this collection of annotated tables to evaluate the ability of open-source and API-based language models to extract information from tables covering diverse domains and data formats. Our experiments demonstrate that surprisingly competitive performance can be achieved without requiring task-specific pipelines or labels, achieving F1 scores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover, through detailed ablation studie
&lt;/p&gt;</description></item><item><title>CAMELL&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#38656;&#19987;&#23478;&#26631;&#27880;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#12289;&#33258;&#30417;&#30563;&#21644;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#26469;&#35299;&#20915;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.08944</link><description>&lt;p&gt;
CAMELL&#65306;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#39640;&#25928;&#33258;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#19982;&#26631;&#31614;&#39564;&#35777;&#33719;&#21462;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CAMELL: Confidence-based Acquisition Model for Efficient Self-supervised Active Learning with Label Validation. (arXiv:2310.08944v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08944
&lt;/p&gt;
&lt;p&gt;
CAMELL&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#38656;&#19987;&#23478;&#26631;&#27880;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#12289;&#33258;&#30417;&#30563;&#21644;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#26469;&#35299;&#20915;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#21463;&#22823;&#35268;&#27169;&#19988;&#31934;&#30830;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#65292;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#21463;&#21040;&#38459;&#30861;&#12290;&#26631;&#27880;&#36136;&#37327;&#38543;&#30528;&#20174;&#19987;&#23478;&#26631;&#27880;&#21521;&#20247;&#21253;&#26631;&#27880;&#30340;&#36716;&#21464;&#32780;&#36880;&#28176;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMELL&#65288;Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#27744;&#21270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;CAMELL&#20855;&#26377;&#19977;&#20010;&#26680;&#24515;&#29305;&#28857;&#65306;(1)&#20165;&#35201;&#27714;&#19987;&#23478;&#26631;&#27880;&#25152;&#36873;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;(2)&#20026;&#20854;&#20313;&#24207;&#21015;&#25552;&#20379;&#33258;&#30417;&#30563;&#65292;(3)&#37319;&#29992;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#65292;&#38450;&#27490;&#38169;&#35823;&#26631;&#31614;&#27745;&#26579;&#25968;&#25454;&#38598;&#24182;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#24207;&#21015;&#20219;&#21153;&#20013;&#23545;CAMELL&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#29305;&#21035;&#24378;&#35843;&#23545;&#35805;&#20449;&#24565;&#36319;&#36394;&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labelling. To address these challenges, we present \textbf{CAMELL} (Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation), a pool-based active learning framework tailored for sequential multi-output problems. CAMELL possesses three core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, (2) it facilitates self-supervision for the remainder of the sequence, and (3) it employs a label validation mechanism to prevent erroneous labels from contaminating the dataset and harming model performance. We evaluate CAMELL on sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited
&lt;/p&gt;</description></item><item><title>BODEGA&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#30340;&#20869;&#23481;&#31649;&#29702;&#22330;&#26223;&#65292;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25991;&#26412;&#20462;&#25913;&#65292;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.08032</link><description>&lt;p&gt;
BODEGA: &#38024;&#23545;&#21487;&#20449;&#24230;&#35780;&#20272;&#20013;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment. (arXiv:2303.08032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08032
&lt;/p&gt;
&lt;p&gt;
BODEGA&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#30340;&#20869;&#23481;&#31649;&#29702;&#22330;&#26223;&#65292;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25991;&#26412;&#20462;&#25913;&#65292;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26816;&#27979;&#19981;&#21487;&#20449;&#20869;&#23481;&#65292;&#22914;&#20551;&#26032;&#38395;&#12289;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12289;&#23459;&#20256;&#31561;&#12290;&#36739;&#20026;&#20934;&#30830;&#30340;&#27169;&#22411;&#65288;&#21487;&#33021;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#26377;&#21161;&#20110;&#31649;&#29702;&#20844;&#20849;&#30005;&#23376;&#24179;&#21488;&#65292;&#24182;&#32463;&#24120;&#23548;&#33268;&#20869;&#23481;&#21019;&#24314;&#32773;&#38754;&#20020;&#25552;&#20132;&#25298;&#32477;&#25110;&#24050;&#21457;&#24067;&#25991;&#26412;&#30340;&#25764;&#19979;&#12290;&#20026;&#20102;&#36991;&#20813;&#36827;&#19968;&#27493;&#34987;&#26816;&#27979;&#65292;&#20869;&#23481;&#21019;&#24314;&#32773;&#23581;&#35797;&#20135;&#29983;&#19968;&#20010;&#31245;&#24494;&#20462;&#25913;&#36807;&#30340;&#25991;&#26412;&#29256;&#26412;&#65288;&#21363;&#25915;&#20987;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#30340;&#24369;&#28857;&#23548;&#33268;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BODEGA&#65306;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#22312;&#27169;&#25311;&#20869;&#23481;&#31649;&#29702;&#30340;&#30495;&#23454;&#29992;&#20363;&#20013;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#21463;&#27426;&#36814;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#23545;&#21487;&#29992;&#25915;&#20987;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#25991;&#26412;&#20013;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. Quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. Having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. Here we introduce BODEGA: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. We also systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases barely signif
&lt;/p&gt;</description></item></channel></rss>