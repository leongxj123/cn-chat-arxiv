<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#37325;&#26032;&#24605;&#32771;Kullback-Leibler&#25955;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kullback-Leibler&#25955;&#24230;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#65292;&#22312;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#19982;&#20043;&#21069;&#26029;&#35328;&#30340;&#36870;Kullback-Leibler&#65288;RKL&#65289;&#25955;&#24230;&#23547;&#25214;&#27169;&#24335;&#24182;&#22240;&#27492;&#20248;&#20110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#27491;&#21521;Kullback-Leibler&#65288;FKL&#65289;&#25955;&#24230;&#30456;&#21453;&#65292;&#23454;&#38469;&#19978;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#37117;&#27809;&#26377;&#20307;&#29616;&#20986;&#23547;&#25214;&#27169;&#24335;&#25110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#29305;&#24615;&#12290;&#30456;&#21453;&#65292;&#21457;&#29616;RKL&#21644;FKL&#20855;&#26377;&#30456;&#21516;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#22312;&#36275;&#22815;&#25968;&#37327;&#30340;&#26102;&#20195;&#20043;&#21518;&#37117;&#20250;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#32422;&#26463;&#65292;LLMs&#24456;&#23569;&#34987;&#35757;&#32451;&#22914;&#27492;&#22810;&#30340;&#26102;&#20195;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;RKL&#22312;&#20998;&#24067;&#30340;&#23614;&#37096;&#65292;&#32780;FKL&#22312;&#24320;&#22987;&#26102;&#20195;&#20391;&#37325;&#20110;&#20998;&#24067;&#30340;&#22836;&#37096;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;Kullback-Leiber&#65288;AKL&#65289;&#25955;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#26435;&#37325;&#26469;&#32452;&#21512;F
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02657v1 Announce Type: cross  Abstract: Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20808;&#39564;&#32422;&#26463;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00978</link><description>&lt;p&gt;
&#22522;&#20110;&#20808;&#39564;&#32422;&#26463;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#20197;&#23545;&#40784;&#22823;&#23610;&#23544;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prior Constraints-based Reward Model Training for Aligning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20808;&#39564;&#32422;&#26463;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#35757;&#32451;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#27604;&#36739;&#23545;&#26469;&#35745;&#31639;&#25490;&#21517;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36807;&#31243;&#23384;&#22312;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#65306;&#30001;&#20110;&#32570;&#20047;&#32422;&#26463;&#65292;&#22870;&#21169;&#20998;&#25968;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#21576;&#29616;&#19981;&#21463;&#25511;&#21046;&#30340;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#32422;&#26463;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;PCRM&#65289;&#35757;&#32451;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;PCRM&#22312;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#20013;&#34701;&#21512;&#20102;&#20808;&#39564;&#32422;&#26463;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#27599;&#20010;&#27604;&#36739;&#23545;&#36755;&#20986;&#20043;&#38388;&#30340;&#38271;&#24230;&#27604;&#21644;&#20313;&#24358;&#30456;&#20284;&#24615;&#65292;&#20197;&#35843;&#33410;&#20248;&#21270;&#24133;&#24230;&#24182;&#25511;&#21046;&#24471;&#20998;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;PCRM&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#25490;&#21517;&#30456;&#20851;&#24615;&#20197;&#21450;&#36890;&#36807;RL&#23545;LLMs&#23545;&#40784;&#30340;&#26377;&#25928;&#24615;&#26469;&#20840;&#38754;&#35780;&#20272;PCRM&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PCRM&#36890;&#36807;&#26377;&#25928;&#22320;&#32422;&#26463;&#22870;&#21169;&#26174;&#33879;&#25552;&#21319;&#20102;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00978v1 Announce Type: new  Abstract: Reinforcement learning with human feedback for aligning large language models (LLMs) trains a reward model typically using ranking loss with comparison pairs.However, the training procedure suffers from an inherent problem: the uncontrolled scaling of reward scores during reinforcement learning due to the lack of constraints while training the reward model.This paper proposes a Prior Constraints-based Reward Model (namely PCRM) training method to mitigate this problem. PCRM incorporates prior constraints, specifically, length ratio and cosine similarity between outputs of each comparison pair, during reward model training to regulate optimization magnitude and control score margins. We comprehensively evaluate PCRM by examining its rank correlation with human preferences and its effectiveness in aligning LLMs via RL. Experimental results demonstrate that PCRM significantly improves alignment performance by effectively constraining reward
&lt;/p&gt;</description></item><item><title>TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.00297</link><description>&lt;p&gt;
TRABSA&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;BiLSTM&#21644;Twitter-RoBERTa&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00297
&lt;/p&gt;
&lt;p&gt;
TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#20844;&#20247;&#33286;&#35770;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TRABSA&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#21033;&#29992;&#22312;124M&#26465;&#25512;&#25991;&#19978;&#35757;&#32451;&#30340;RoBERTa&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24046;&#36317;&#65292;&#30830;&#20445;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;32&#20010;&#22269;&#23478;&#21644;&#32654;&#22269;&#21508;&#24030;&#30340;&#25512;&#25991;&#19982;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20845;&#31181;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19977;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26631;&#27880;&#25216;&#26415;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#25216;&#26415;&#20197;&#23454;&#29616;&#26368;&#20339;&#24773;&#24863;&#20998;&#26512;&#25928;&#26524;&#12290;TRABSA&#20197;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#22686;&#30410;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#19968;&#33268;&#30340;&#20248;&#36234;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;SHAP&#21644;LIME&#20998;&#26512;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22686;&#24378;&#20102;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36339;&#34920;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#20889;&#38382;&#39064;&#21644;&#27874;&#26463;&#25628;&#32034;&#26469;&#20943;&#23569;&#30456;&#20284;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22810;&#36339;&#26816;&#32034;&#20013;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.10666</link><description>&lt;p&gt;
&#24320;&#25918;&#22495;&#25991;&#26412;&#21040;SQL&#30340;&#22810;&#36339;&#34920;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Hop Table Retrieval for Open-Domain Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10666
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36339;&#34920;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#20889;&#38382;&#39064;&#21644;&#27874;&#26463;&#25628;&#32034;&#26469;&#20943;&#23569;&#30456;&#20284;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22810;&#36339;&#26816;&#32034;&#20013;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#25991;&#26412;&#21040;SQL&#26159;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#23427;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#34920;&#65292;&#28982;&#21518;&#29983;&#25104;SQL&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21333;&#36339;&#26816;&#32034;&#26041;&#27861;&#24182;&#26410;&#20851;&#27880;&#25991;&#26412;&#21040;SQL&#25361;&#25112;&#20013;&#30340;&#27169;&#24335;&#38142;&#25509;&#65292;&#36825;&#28041;&#21450;&#21040;&#23558;&#38382;&#39064;&#20013;&#30340;&#23454;&#20307;&#19982;&#34920;&#20013;&#23454;&#20307;&#23545;&#40784;&#65292;&#20027;&#35201;&#20307;&#29616;&#22312;&#20004;&#20010;&#26041;&#38754;&#65306;&#30456;&#20284;&#30340;&#26080;&#20851;&#23454;&#20307;&#21644;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#24102;&#37325;&#20889;&#21644;&#27874;&#26463;&#25628;&#32034;&#30340;&#22810;&#36339;&#34920;&#26816;&#32034;&#65288;Murre&#65289;&#12290;&#20026;&#20102;&#20943;&#23569;&#30456;&#20284;&#30340;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#27599;&#20010;&#36339;&#36291;&#20013;&#26410;&#26816;&#32034;&#21040;&#30340;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#27874;&#26463;&#25628;&#32034;&#32771;&#34385;&#25490;&#21517;&#36739;&#20302;&#30340;&#34920;&#12290;&#20026;&#20102;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;Murre&#22522;&#20110;&#22810;&#20010;&#36339;&#36291;&#20013;&#26816;&#32034;&#21040;&#30340;&#34920;&#37325;&#20889;&#38382;&#39064;&#65292;&#20943;&#23569;&#19982;&#30456;&#20851;&#34920;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;SpiderUnion&#21644;BirdUnion+&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10666v1 Announce Type: new  Abstract: Open-domain text-to-SQL is an important task that retrieves question-relevant tables from massive databases and then generates SQL. However, existing retrieval methods that retrieve in a single hop do not pay attention to the text-to-SQL challenge of schema linking, which is aligning the entities in the question with table entities, reflected in two aspects: similar irrelevant entity and domain mismatch entity. Therefore, we propose our method, the multi-hop table retrieval with rewrite and beam search (Murre). To reduce the effect of the similar irrelevant entity, our method focuses on unretrieved entities at each hop and considers the low-ranked tables by beam search. To alleviate the limitation of domain mismatch entity, Murre rewrites the question based on retrieved tables in multiple hops, decreasing the domain gap with relevant tables. We conduct experiments on SpiderUnion and BirdUnion+, reaching new state-of-the-art results with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#12290;&#36890;&#36807;&#32508;&#36848;&#25991;&#29486;&#21644;&#19987;&#23478;&#21672;&#35810;&#65292;&#26500;&#24314;&#20102;&#25968;&#25454;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#29983;&#25104;&#20102;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;MLLM&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#20027;&#27969;&#25945;&#32946;&#20013;&#30340;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#20559;&#37325;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.06264</link><description>&lt;p&gt;
LLaVA-Docent&#65306;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#30340;&#25945;&#23398;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#12290;&#36890;&#36807;&#32508;&#36848;&#25991;&#29486;&#21644;&#19987;&#23478;&#21672;&#35810;&#65292;&#26500;&#24314;&#20102;&#25968;&#25454;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#29983;&#25104;&#20102;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;MLLM&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#20027;&#27969;&#25945;&#32946;&#20013;&#30340;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#20559;&#37325;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#37492;&#36175;&#23545;&#20110;&#22521;&#20859;&#23398;&#20064;&#32773;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#24773;&#24863;&#26234;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#24120;&#38754;&#20020;&#33402;&#26415;&#36164;&#28304;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24369;&#21183;&#23398;&#29983;&#65292;&#24182;&#19988;&#22312;&#20027;&#27969;&#25945;&#32946;&#20013;&#36807;&#24230;&#24378;&#35843;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#31185;&#30446;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#25216;&#26415;&#36827;&#27493;&#20026;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#26469;&#21033;&#29992;&#36825;&#20123;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#21672;&#35810;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#25968;&#25454;&#26694;&#26550;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;GPT-4&#21033;&#29992;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;MLLM&#65288;&#21363;LLaVA-Docent&#65289;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#20845;&#21517;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Art appreciation is vital in nurturing critical thinking and emotional intelligence among learners. However, traditional art appreciation education has often been hindered by limited access to art resources, especially for disadvantaged students, and an imbalanced emphasis on STEM subjects in mainstream education. In response to these challenges, recent technological advancements have paved the way for innovative solutions. This study explores the application of multi-modal large language models (MLLMs) in art appreciation education, focusing on developing LLaVA-Docent, a model that leverages these advancements. Our approach involved a comprehensive literature review and consultations with experts in the field, leading to developing a robust data framework. Utilizing this framework, we generated a virtual dialogue dataset that was leveraged by GPT-4. This dataset was instrumental in training the MLLM, named LLaVA-Docent. Six researchers conducted quantitative and qualitative evaluation
&lt;/p&gt;</description></item><item><title>LLMRefine&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21453;&#39304;&#27169;&#22411;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#20301;&#32570;&#38519;&#24182;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#38271;&#31687;&#38382;&#31572;&#21644;&#20027;&#39064;&#24635;&#32467;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2311.09336</link><description>&lt;p&gt;
LLMRefine&#65306;&#36890;&#36807;&#32454;&#31890;&#24230;&#21487;&#25805;&#20316;&#21453;&#39304;&#31934;&#30830;&#23450;&#20301;&#21644;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLMRefine: Pinpointing and Refining Large Language Models via Fine-Grained Actionable Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09336
&lt;/p&gt;
&lt;p&gt;
LLMRefine&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21453;&#39304;&#27169;&#22411;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#20301;&#32570;&#38519;&#24182;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#38271;&#31687;&#38382;&#31572;&#21644;&#20027;&#39064;&#24635;&#32467;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMRefine&#65292;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#25512;&#29702;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;LLM&#30340;&#36755;&#20986;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#23398;&#20064;&#30340;&#32454;&#31890;&#24230;&#21453;&#39304;&#27169;&#22411;&#26469;&#20934;&#30830;&#23450;&#20301;&#32570;&#38519;&#65292;&#24182;&#24341;&#23548;LLM&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;LLM&#20316;&#20026;&#32534;&#36753;&#24314;&#35758;&#65292;LLMRefine&#36890;&#36807;&#27169;&#25311;&#36864;&#28779;&#25628;&#32034;&#26080;&#32570;&#38519;&#25991;&#26412;&#65292;&#26435;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#65292;&#38271;&#31687;&#38382;&#31572;&#65288;QA&#65289;&#21644;&#20027;&#39064;&#24635;&#32467;&#12290;LLMRefine&#22312;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#19978;&#19968;&#36143;&#34920;&#29616;&#20248;&#24322;&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#39640;&#36798;1.7 MetricX&#28857;&#30340;&#25913;&#36827;&#65292;&#22312;ASQA&#19978;&#20026;8.1 ROUGE-L&#65292;&#22312;&#20027;&#39064;&#24635;&#32467;&#19978;&#20026;2.2 ROUGE-L&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09336v2 Announce Type: replace  Abstract: Recent large language models (LLM) are leveraging human feedback to improve their generation quality. However, human feedback is costly to obtain, especially during inference. In this work, we propose LLMRefine, an inference time optimization method to refine LLM's output. The core idea is to use a learned fine-grained feedback model to pinpoint defects and guide LLM to refine them iteratively. Using original LLM as a proposal of edits, LLMRefine searches for defect-less text via simulated annealing, trading off the exploration and exploitation. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA), and topical summarization. LLMRefine consistently outperforms all baseline approaches, achieving improvements up to 1.7 MetricX points on translation tasks, 8.1 ROUGE-L on ASQA, 2.2 ROUGE-L on topical summarization.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38024;&#23545;&#26041;&#35328;&#30340;&#26041;&#27861;&#21644;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#26041;&#35328;&#23545;&#20110;NLP&#27169;&#22411;&#24615;&#33021;&#21644;&#35821;&#35328;&#25216;&#26415;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26041;&#35328;&#30456;&#20851;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2401.05632</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#26041;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing for Dialects of a Language: A Survey. (arXiv:2401.05632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05632
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38024;&#23545;&#26041;&#35328;&#30340;&#26041;&#27861;&#21644;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#26041;&#35328;&#23545;&#20110;NLP&#27169;&#22411;&#24615;&#33021;&#21644;&#35821;&#35328;&#25216;&#26415;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26041;&#35328;&#30456;&#20851;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#37325;&#35201;&#23646;&#24615;&#65306;&#35821;&#35328;&#26041;&#35328;&#12290;&#32771;&#34385;&#21040;&#38024;&#23545;&#26041;&#35328;&#25968;&#25454;&#38598;&#30340;NLP&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#21450;&#20854;&#23545;&#35821;&#35328;&#25216;&#26415;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26377;&#20851;&#26041;&#35328;NLP&#30340;&#36807;&#21435;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#31867;&#21035;&#30340;&#35270;&#35282;&#25551;&#36848;&#20102;&#21508;&#31181;NLP&#20219;&#21153;&#65306;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#65288;&#22914;&#26041;&#35328;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#35299;&#26512;&#21644;NLU&#22522;&#20934;&#27979;&#35797;&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#65288;&#22914;&#25688;&#35201;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#23545;&#35805;&#31995;&#32479;&#65289;&#12290;&#36825;&#39033;&#35843;&#26597;&#36824;&#24191;&#27867;&#28085;&#30422;&#20102;&#33521;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#24503;&#35821;&#31561;&#22810;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#26377;&#20851;&#26041;&#35328;&#30340;&#36807;&#21435;NLP&#24037;&#20316;&#19981;&#27490;&#20110;&#26041;&#35328;&#20998;&#31867;&#65292;&#32780;&#26159;...
&lt;/p&gt;
&lt;p&gt;
State-of-the-art natural language processing (NLP) models are trained on massive training corpora, and report a superlative performance on evaluation datasets. This survey delves into an important attribute of these datasets: the dialect of a language. Motivated by the performance degradation of NLP models for dialectic datasets and its implications for the equity of language technologies, we survey past research in NLP for dialects in terms of datasets, and approaches. We describe a wide range of NLP tasks in terms of two categories: natural language understanding (NLU) (for tasks such as dialect classification, sentiment analysis, parsing, and NLU benchmarks) and natural language generation (NLG) (for summarisation, machine translation, and dialogue systems). The survey is also broad in its coverage of languages which include English, Arabic, German among others. We observe that past work in NLP concerning dialects goes deeper than mere dialect classification, and . This includes ear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#23646;&#24615;/&#20540;&#25552;&#21462;&#25216;&#26415;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#23545;&#26410;&#30693;&#23646;&#24615;&#20540;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12537</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Product Attribute Value Extraction using Large Language Models. (arXiv:2310.12537v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#23646;&#24615;/&#20540;&#25552;&#21462;&#25216;&#26415;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#23545;&#26410;&#30693;&#23646;&#24615;&#20540;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#65288;&#22914;&#38754;&#21521;&#23646;&#24615;&#30340;&#20135;&#21697;&#25628;&#32034;&#25110;&#20135;&#21697;&#27604;&#36739;&#65289;&#22522;&#20110;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25551;&#36848;&#65292;&#22914;&#23646;&#24615;/&#20540;&#23545;&#12290;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#20379;&#24212;&#21830;&#19981;&#25552;&#20379;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25551;&#36848;&#65292;&#32780;&#26159;&#20351;&#29992;&#26631;&#39064;&#25110;&#25551;&#36848;&#26469;&#25551;&#36848;&#20135;&#21697;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#26679;&#30340;&#20135;&#21697;&#65292;&#26377;&#24517;&#35201;&#20174;&#25991;&#26412;&#20135;&#21697;&#23646;&#24615;&#20013;&#25552;&#21462;&#23646;&#24615;/&#20540;&#23545;&#12290;&#29616;&#26377;&#25216;&#26415;&#20013;&#65292;&#23646;&#24615;/&#20540;&#25552;&#21462;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#23646;&#24615;/&#20540;&#25552;&#21462;&#26041;&#38754;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;&#19968;&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;&#20108;&#65289;&#20248;&#21270;&#21518;&#30340;&#27169;&#22411;&#22312;&#25512;&#24191;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#21253;&#21547;&#30340;&#23646;&#24615;&#20540;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#39640;&#19988;&#40065;&#26834;&#24615;&#24378;&#30340;&#26367;&#20195;&#26041;&#27861;&#22312;&#23646;&#24615;/&#20540;&#25552;&#21462;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25176;&#31649;&#30340;LLMs&#65292;&#22914;GPT-3.5&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce applications such as faceted product search or product comparison are based on structured product descriptions like attribute/value pairs. The vendors on e-commerce platforms do not provide structured product descriptions but describe offers using titles or descriptions. To process such offers, it is necessary to extract attribute/value pairs from textual product attributes. State-of-the-art attribute/value extraction techniques rely on pre-trained language models (PLMs), such as BERT. Two major drawbacks of these models for attribute/value extraction are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models face challenges in generalizing to attribute values not included in the training data. This paper explores the potential of large language models (LLMs) as a training data-efficient and robust alternative to PLM-based attribute/value extraction methods. We consider hosted LLMs, such as GPT-3.5 and GPT-4, as well as 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2309.00236</link><description>&lt;p&gt;
&#22270;&#20687;&#21163;&#25345;&#65306;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Image Hijacking: Adversarial Images can Control Generative Models at Runtime. (arXiv:2309.00236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20813;&#21463;&#24694;&#24847;&#34892;&#20026;&#32773;&#30340;&#25915;&#20987;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#22270;&#20687;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#22270;&#20687;&#21163;&#25345;&#65292;&#21363;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34892;&#20026;&#21305;&#37197;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#65292;&#24182;&#29992;&#23427;&#26469;&#25506;&#32034;&#19977;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#20855;&#20307;&#23383;&#31526;&#20018;&#25915;&#20987;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#34987;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#36755;&#20986;&#65307;&#27844;&#38706;&#19978;&#19979;&#25991;&#25915;&#20987;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#20449;&#24687;&#27844;&#38706;&#21040;&#36755;&#20986;&#20013;&#65307;&#36234;&#29425;&#25915;&#20987;&#21487;&#20197;&#32469;&#36807;&#27169;&#22411;&#30340;&#23433;&#20840;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;CLIP&#21644;LLaMA-2&#30340;&#26368;&#26032;VLM&#27169;&#22411;LLaVA-2&#36827;&#34892;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25152;&#26377;&#30340;&#25915;&#20987;&#31867;&#22411;&#25104;&#21151;&#29575;&#22343;&#22312;90&#65285;&#20197;&#19978;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#21482;&#38656;&#35201;&#23545;&#22270;&#20687;&#36827;&#34892;&#23567;&#30340;&#25200;&#21160;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;&#22914;&#26524;&#22270;&#20687;&#21163;&#25345;&#19982;CIFAR-10&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19968;&#26679;&#38590;&#20197;&#38450;&#24481;&#65292;&#37027;&#20040;&#21487;&#33021;&#38656;&#35201;&#24456;&#22810;&#24180;&#25165;&#33021;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control generative models at runtime. We introduce Behavior Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. Specific string attacks generate arbitrary output of the adversary's choosing. Leak context attacks leak information from the context window into the output. Jailbreak attacks circumvent a model's safety training. We study these attacks against LLaVA-2, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90\% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a s
&lt;/p&gt;</description></item><item><title>PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2305.19472</link><description>&lt;p&gt;
PlaSma: &#20026; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#21046;&#23450;&#22686;&#24378;&#36807;&#31243;&#30693;&#35782;&#27169;&#22411;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19472
&lt;/p&gt;
&lt;p&gt;
PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35268;&#21010;&#26159;&#26426;&#22120;&#30340;&#19968;&#39033;&#37325;&#35201;&#32780;&#21448;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23427;&#23558;&#19968;&#20010;&#39640;&#32423;&#30446;&#26631;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26102;&#38388;&#39034;&#24207;&#30340;&#27493;&#39588;&#12290;&#23427;&#38656;&#35201;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#25512;&#29702;&#20986;&#24120;&#24120;&#26159;&#21453;&#20107;&#23454;&#30340;&#22797;&#26434;&#24773;&#22659;&#65292;&#20363;&#22914; "&#27809;&#26377;&#30005;&#35805;&#26102;&#23433;&#25490;&#21307;&#29983;&#30340;&#32422;&#20250;"&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#21463;&#21040;&#26114;&#36149;&#30340; API &#35843;&#29992;&#21644;&#21487;&#22797;&#29616;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; PlaSma&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#26041;&#27861;&#65292;&#20351;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36807;&#31243;&#30693;&#35782;&#21644; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31526;&#21495;&#36807;&#31243;&#30693;&#35782;&#33976;&#39311;&#26469;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#65292;&#20197;&#21450;&#19968;&#31181;&#25512;&#29702;&#31639;&#27861;&#26469;&#20419;&#36827;&#26356;&#32467;&#26500;&#21270;&#21644;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21453;&#20107;&#23454;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#20204;&#22914;&#20309;&#23558;&#36523;&#20221;&#26631;&#31614;&#24212;&#29992;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#20197;&#21450;&#37327;&#21270;&#31361;&#20986;&#30340;&#31038;&#20250;&#32500;&#24230;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09548</link><description>&lt;p&gt;
&#20351;&#29992;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#26469;&#34913;&#37327;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Measuring Stereotypes using Entity-Centric Data. (arXiv:2305.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#20204;&#22914;&#20309;&#23558;&#36523;&#20221;&#26631;&#31614;&#24212;&#29992;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#20197;&#21450;&#37327;&#21270;&#31361;&#20986;&#30340;&#31038;&#20250;&#32500;&#24230;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21051;&#26495;&#21360;&#35937;&#24433;&#21709;&#25105;&#20204;&#22914;&#20309;&#23637;&#31034;&#33258;&#24049;&#21644;&#20182;&#20154;&#65292;&#20174;&#32780;&#24433;&#21709;&#25105;&#20204;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#34913;&#37327;&#21051;&#26495;&#21360;&#35937;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#65288;DSM&#65289;&#65288;&#22914;BERT&#65289;&#20013;&#23884;&#20837;&#30340;&#25237;&#24433;&#26469;&#36827;&#34892;&#36825;&#20123;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;DSMs&#25429;&#25417;&#21040;&#30340;&#35748;&#30693;&#32852;&#24819;&#19981;&#19968;&#23450;&#19982;&#21051;&#26495;&#21360;&#35937;&#30340;&#20154;&#38469;&#24615;&#36136;&#30456;&#20851;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20174;Twitter&#21644;Wikipedia&#20256;&#35760;&#20013;&#23398;&#20064;&#21051;&#26495;&#21360;&#35937;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#30701;&#35821;&#24212;&#29992;&#20110;&#21516;&#19968;&#20010;&#20154;&#30340;&#20107;&#23454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25193;&#22823;&#20102;&#23398;&#20064;&#32852;&#24819;&#30340;&#20154;&#26412;&#36523;&#20013;&#24515;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#20204;&#22914;&#20309;&#23558;&#36523;&#20221;&#26631;&#31614;&#24212;&#29992;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#20197;&#21450;&#37327;&#21270;&#31361;&#20986;&#30340;&#31038;&#20250;&#32500;&#24230;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#23545;&#26410;&#26469;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stereotypes inform how we present ourselves and others, and in turn how we behave. They are thus important to measure. Recent work has used projections of embeddings from Distributional Semantic Models (DSMs), such as BERT, to perform these measurements. However, DSMs capture cognitive associations that are not necessarily relevant to the interpersonal nature of stereotyping. Here, we propose and evaluate three novel, entity-centric methods for learning stereotypes from Twitter and Wikipedia biographies. Models are trained by leveraging the fact that multiple phrases are applied to the same person, magnifying the person-centric nature of the learned associations. We show that these models outperform existing approaches to stereotype measurement with respect to 1) predicting which identities people apply to themselves and others, and 2) quantifying stereotypes on salient social dimensions (e.g. gender). Via a case study, we also show the utility of these models for future questions in c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00008</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00008
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#39072;&#35206;&#20154;&#24037;&#26234;&#33021;&#30340;&#22810;&#20010;&#39046;&#22495;&#12290;&#20854;&#20013;&#26368;&#26174;&#33879;&#30340;&#24212;&#29992;&#20043;&#19968;&#26159;&#21019;&#20316;&#65292;&#20363;&#22914;&#35799;&#27468;&#25110;&#25925;&#20107;&#65306;&#29983;&#25104;&#30340;&#36755;&#20986;&#36890;&#24120;&#20855;&#26377;&#24778;&#20154;&#30340;&#36136;&#37327;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;LLMs&#30495;&#30340;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21019;&#36896;&#24615;&#30340;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21019;&#36896;&#24615;&#29702;&#35770;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLMs&#30340;&#21457;&#23637;&#65292;&#25506;&#35752;&#20102;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19982;LLMs&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#26041;&#38754;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#26131;&#8221;&#21644;&#8220;&#38590;&#8221;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arise: can LLMs really be considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. Then, we identify a set of "easy" and "hard" problems in machine creativity, discussing them in relation to LLMs. Finally, we analyze the societal impact of these technologies with a particular focus on the creative industries.
&lt;/p&gt;</description></item></channel></rss>