<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>AutoRE &#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF&#30340;&#26032;&#39062;&#20851;&#31995;&#25277;&#21462;&#33539;&#24335;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#22312;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;</title><link>https://arxiv.org/abs/2403.14888</link><description>&lt;p&gt;
AutoRE&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
AutoRE: Document-Level Relation Extraction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14888
&lt;/p&gt;
&lt;p&gt;
AutoRE &#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF&#30340;&#26032;&#39062;&#20851;&#31995;&#25277;&#21462;&#33539;&#24335;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#22312;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#24322;&#24120;&#33021;&#21147;&#65292;&#36825;&#28608;&#21169;&#30528;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#23427;&#20204;&#36827;&#34892;&#20449;&#24687;&#25277;&#21462;(IE)&#20219;&#21153;&#65292;&#21253;&#25324;&#20851;&#31995;&#25277;&#21462;(RE)&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;(SentRE)&#20219;&#21153;&#65292;&#36825;&#36890;&#24120;&#28085;&#30422;&#20102;&#21333;&#20010;&#21477;&#23376;&#20869;&#30340;&#19968;&#32452;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26041;&#27861;&#37319;&#29992;&#23558;&#20851;&#31995;&#20316;&#20026;&#20505;&#36873;&#36873;&#25321;&#38598;&#25104;&#21040;&#25552;&#31034;&#27169;&#26495;&#20013;&#30340;&#26041;&#24335;&#65292;&#23548;&#33268;&#22312;&#22788;&#29702;&#20998;&#24067;&#22312;&#32473;&#23450;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#26102;&#25928;&#29575;&#20302;&#19979;&#65292;&#24615;&#33021;&#20122;&#20248;&#65292;&#24182;&#22312;&#22788;&#29702;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;(DocRE)&#20219;&#21153;&#26102;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoRE&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;DocRE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF(Re
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14888v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional abilities in comprehending and generating text, motivating numerous researchers to utilize them for Information Extraction (IE) purposes, including Relation Extraction (RE). Nonetheless, most existing methods are predominantly designed for Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a restricted set of relations and triplet facts within a single sentence. Furthermore, certain approaches resort to treating relations as candidate choices integrated into prompt templates, leading to inefficient processing and suboptimal performance when tackling Document-Level Relation Extraction (DocRE) tasks, which entail handling multiple relations and triplet facts distributed across a given document, posing distinct challenges. To overcome these limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel RE extraction paradigm named RHF (Re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#32452;&#21512;&#27867;&#21270;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22914;&#20309;&#25345;&#32493;&#33719;&#21462;&#21407;&#22987;&#25512;&#29702;&#20219;&#21153;&#30340;&#30693;&#35782;&#24182;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#65292;&#30740;&#31350;&#20102;&#19981;&#26029;&#23398;&#20064;&#23545;NLI&#20013;&#32452;&#21512;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.04400</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#25506;&#32034;&#32452;&#21512;&#27867;&#21270;&#30340;&#19981;&#38388;&#26029;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Continual Learning of Compositional Generalization in NLI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#32452;&#21512;&#27867;&#21270;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22914;&#20309;&#25345;&#32493;&#33719;&#21462;&#21407;&#22987;&#25512;&#29702;&#20219;&#21153;&#30340;&#30693;&#35782;&#24182;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#65292;&#30740;&#31350;&#20102;&#19981;&#26029;&#23398;&#20064;&#23545;NLI&#20013;&#32452;&#21512;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#24050;&#34987;&#29992;&#26469;&#35780;&#20272;&#31070;&#32463;&#27169;&#22411;&#25191;&#34892;NLI&#30340;&#30495;&#23454;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#20551;&#35774;&#27169;&#22411;&#20107;&#20808;&#23436;&#20840;&#35775;&#38382;&#25152;&#26377;&#21407;&#22987;&#25512;&#29702;&#65292;&#19982;&#20154;&#31867;&#19981;&#26029;&#33719;&#24471;&#25512;&#29702;&#30693;&#35782;&#30340;&#26041;&#24335;&#30456;&#21453;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#20013;&#30340;&#36830;&#32493;&#32452;&#21512;&#27867;&#21270;&#65288;C2Gen NLI&#65289;&#25361;&#25112;&#65292;&#20854;&#20013;&#27169;&#22411;&#25345;&#32493;&#33719;&#21462;&#26500;&#25104;&#21407;&#22987;&#25512;&#29702;&#20219;&#21153;&#30340;&#30693;&#35782;&#20316;&#20026;&#32452;&#21512;&#25512;&#29702;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#26029;&#23398;&#20064;&#22914;&#20309;&#24433;&#21709;NLI&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#65292;&#36890;&#36807;&#20026;&#32452;&#21512;NLI&#25512;&#29702;&#20219;&#21153;&#35774;&#35745;&#20102;&#19968;&#20010;&#19981;&#26029;&#23398;&#20064;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#27169;&#22411;&#22312;&#19981;&#26029;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#32452;&#21512;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#21508;&#31181;&#19981;&#26029;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#24182;&#39564;&#35777;&#23427;&#20204;&#30340;&#21151;&#25928;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;C2Gen&#65292;&#37325;&#28857;&#20851;&#27880;&#22914;&#20309;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04400v1 Announce Type: new  Abstract: Compositional Natural Language Inference has been explored to assess the true abilities of neural models to perform NLI. Yet, current evaluations assume models to have full access to all primitive inferences in advance, in contrast to humans that continuously acquire inference knowledge. In this paper, we introduce the Continual Compositional Generalization in Inference (C2Gen NLI) challenge, where a model continuously acquires knowledge of constituting primitive inference tasks as a basis for compositional inferences. We explore how continual learning affects compositional generalization in NLI, by designing a continual learning setup for compositional NLI inference tasks. Our experiments demonstrate that models fail to compositionally generalize in a continual scenario. To address this problem, we first benchmark various continual learning algorithms and verify their efficacy. We then further analyze C2Gen, focusing on how to order pri
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;AutoVER&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#35273;&#23454;&#20307;&#35782;&#21035;&#20013;&#24212;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;&#32422;&#26463;&#29983;&#25104;&#65292;&#25104;&#21151;&#21306;&#20998;&#24040;&#22823;&#26631;&#31614;&#31354;&#38388;&#20013;&#30456;&#20284;&#30340;&#23454;&#20307;&#65292;&#24182;&#22312;Oven-Wiki&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.18695</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#22312;&#35270;&#35273;&#23454;&#20307;&#35782;&#21035;&#19978;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Models for Visual Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18695
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;AutoVER&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#35273;&#23454;&#20307;&#35782;&#21035;&#20013;&#24212;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;&#32422;&#26463;&#29983;&#25104;&#65292;&#25104;&#21151;&#21306;&#20998;&#24040;&#22823;&#26631;&#31614;&#31354;&#38388;&#20013;&#30456;&#20284;&#30340;&#23454;&#20307;&#65292;&#24182;&#22312;Oven-Wiki&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;AutoVER&#65292;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#32422;&#26463;&#29983;&#25104;&#65292;&#25193;&#23637;&#20102;&#33258;&#22238;&#24402;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#22788;&#29702;&#36328;&#39046;&#22495;&#23454;&#20307;&#26102;&#20943;&#36731;&#20102;&#20302;&#24615;&#33021;&#65292;&#22312;&#38656;&#35201;&#35270;&#35273;&#25512;&#29702;&#30340;&#26597;&#35810;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#30828;&#36127;&#23545;&#19978;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#65292;&#24182;&#22312;&#24207;&#21015;-&#24207;&#21015;&#30446;&#26631;&#20013;&#24182;&#34892;&#36827;&#34892;&#35757;&#32451;&#65292;&#23398;&#20064;&#22312;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#20013;&#21306;&#20998;&#30456;&#20284;&#30340;&#23454;&#20307;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#19968;&#31995;&#21015;&#26816;&#32034;&#30340;&#20505;&#36873;&#31572;&#26696;&#26126;&#30830;&#25351;&#23548;&#35821;&#35328;&#29983;&#25104;&#65292;&#36890;&#36807;&#28040;&#38500;&#26080;&#25928;&#30340;&#35299;&#30721;&#36335;&#24452;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;Oven-Wiki&#22522;&#20934;&#27979;&#35797;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#25286;&#20998;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#24050;&#30693;&#23454;&#20307;&#25286;&#20998;&#19978;&#30340;&#20934;&#30830;&#29575;&#20174;32.7%&#25552;&#39640;&#21040;61.5%&#12290;&#35813;&#26041;&#27861;&#36824;&#36890;&#36807;&#22823;&#24133;&#24230;&#25552;&#21319;&#22312;&#26410;&#30693;&#21644;&#26597;&#35810;&#25286;&#20998;&#19978;&#30340;&#24615;&#33021;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18695v1 Announce Type: cross  Abstract: We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of-domain entities while excelling in queries that require visually-situated reasoning. Our method learns to distinguish similar entities within a vast label space by contrastively training on hard negative pairs in parallel with a sequence-to-sequence objective without an external retriever. During inference, a list of retrieved candidate answers explicitly guides language generation by removing invalid decoding paths. The proposed method achieves significant improvements across different dataset splits in the recently proposed Oven-Wiki benchmark. Accuracy on the Entity seen split rises from 32.7% to 61.5%. It also demonstrates superior performance on the unseen and query splits by a substantial dou
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#27169;&#22411;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#25945;&#24072;&#30340;&#32852;&#21512;&#32454;&#31890;&#24230;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#22937;&#21327;&#20316;&#20196;&#29260;&#21644;&#23454;&#20307;&#34920;&#31034;&#65292;&#22788;&#29702;&#22797;&#26434;&#30340;&#34920;&#21333;&#25991;&#26723;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#22312;&#22788;&#29702;&#35270;&#35273;&#22797;&#26434;&#34920;&#21333;&#25991;&#26723;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.17983</link><description>&lt;p&gt;
M3-VRD: &#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#25945;&#24072;&#35270;&#35273;&#20016;&#23500;&#34920;&#21333;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17983
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#27169;&#22411;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#25945;&#24072;&#30340;&#32852;&#21512;&#32454;&#31890;&#24230;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#22937;&#21327;&#20316;&#20196;&#29260;&#21644;&#23454;&#20307;&#34920;&#31034;&#65292;&#22788;&#29702;&#22797;&#26434;&#30340;&#34920;&#21333;&#25991;&#26723;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#22312;&#22788;&#29702;&#35270;&#35273;&#22797;&#26434;&#34920;&#21333;&#25991;&#26723;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31361;&#30772;&#24615;&#30340;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#25945;&#24072;&#32852;&#21512;&#32454;&#31890;&#24230;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65292;&#29992;&#20110;&#35270;&#35273;&#20016;&#23500;&#30340;&#34920;&#21333;&#25991;&#26723;&#29702;&#35299;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#20196;&#29260;&#21644;&#23454;&#20307;&#34920;&#31034;&#20043;&#38388;&#30340;&#24494;&#22937;&#30456;&#20851;&#24615;&#26469;&#21033;&#29992;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#32423;&#21035;&#30340;&#35265;&#35299;&#65292;&#35299;&#20915;&#34920;&#21333;&#25991;&#26723;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#36328;&#32454;&#31890;&#24230;&#21644;&#36328;&#31895;&#31890;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;&#20256;&#36882;&#36807;&#31243;&#65292;&#21576;&#29616;&#20998;&#24067;&#24046;&#36317;&#21644;&#23545;&#34920;&#21333;&#25991;&#26723;&#30340;&#32479;&#19968;&#29702;&#35299;&#12290;&#36890;&#36807;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#34920;&#21333;&#25991;&#26723;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#22320;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22788;&#29702;&#22797;&#26434;&#35270;&#35273;&#34920;&#21333;&#25991;&#26723;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#20869;&#23481;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17983v1 Announce Type: new  Abstract: This paper presents a groundbreaking multimodal, multi-task, multi-teacher joint-grained knowledge distillation model for visually-rich form document understanding. The model is designed to leverage insights from both fine-grained and coarse-grained levels by facilitating a nuanced correlation between token and entity representations, addressing the complexities inherent in form documents. Additionally, we introduce new inter-grained and cross-grained loss functions to further refine diverse multi-teacher knowledge distillation transfer process, presenting distribution gaps and a harmonised understanding of form documents. Through a comprehensive evaluation across publicly available form document understanding datasets, our proposed model consistently outperforms existing baselines, showcasing its efficacy in handling the intricate structures and content of visually complex form documents.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#22411;&#32452;&#21512;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12750</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Model Composition for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12750
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#32452;&#21512;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#21457;&#23637;&#26174;&#31034;&#20986;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#26397;&#30528;&#21019;&#24314;&#33021;&#22815;&#29702;&#35299;&#21508;&#31181;&#27169;&#24577;&#36755;&#20837;&#30340;&#22810;&#21151;&#33021;MLLMs&#30340;&#30446;&#26631;&#36808;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#37197;&#23545;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#36825;&#23545;&#36164;&#28304;&#35201;&#27714;&#39640;&#19988;&#38590;&#20197;&#25193;&#23637;&#21040;&#26032;&#30340;&#27169;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29616;&#26377;MLLMs&#30340;&#27169;&#22411;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#27169;&#22411;&#30340;&#26032;&#33539;&#24335;&#65292;&#35813;&#26032;&#27169;&#22411;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#22522;&#26412;&#23454;&#29616;NaiveMC&#36890;&#36807;&#37325;&#29992;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#21512;&#24182;LLM&#21442;&#25968;&#23637;&#31034;&#20102;&#36825;&#19968;&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DAMC&#26469;&#35299;&#20915;&#22312;&#21512;&#24182;&#36807;&#31243;&#20013;&#30340;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MCUB&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;MLLMs&#29702;&#35299;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12750v1 Announce Type: cross  Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to unders
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;</title><link>https://arxiv.org/abs/2402.10962</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#27979;&#37327;&#21644;&#25511;&#21046;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Measuring and Controlling Persona Drift in Language Model Dialogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#25215;&#25285;&#29305;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#12290;&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#65292;&#23427;&#20204;&#23558;&#26159;&#31283;&#23450;&#30340;&#65292;&#22240;&#27492;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#22312;&#25972;&#20010;&#23545;&#35805;&#36807;&#31243;&#20013;&#32487;&#32493;&#26681;&#25454;&#35268;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#36890;&#36807;&#20004;&#20010;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#33258;&#25105;&#23545;&#35805;&#26469;&#35780;&#20272;&#8220;&#20154;&#35774;&#8221;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#27169;&#22411;&#22914;LLaMA2-chat-70B&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;&#20843;&#36718;&#23545;&#35805;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#12290;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#30001;&#20110;&#38271;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#34928;&#20943;&#65292;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#19982;&#20004;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.08983</link><description>&lt;p&gt;
SafeDecoding: &#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#21161;&#31561;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#20026;&#20102;&#20351;LLM&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#21253;&#25324;&#23433;&#20840;&#24615;&#22312;&#20869;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#36234;&#29425;&#25915;&#20987;&#26088;&#22312;&#24341;&#21457;LLM&#30340;&#38750;&#39044;&#26399;&#21644;&#19981;&#23433;&#20840;&#34892;&#20026;&#65292;&#20173;&#28982;&#26159;LLM&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#23041;&#32961;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;SafeDecoding&#26469;&#38450;&#24481;LLM&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#23433;&#20840;&#24863;&#30693;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#24320;&#21457;SafeDecoding&#26102;&#30340;&#27934;&#23519;&#21147;&#22522;&#20110;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#20195;&#34920;&#26377;&#23475;&#20869;&#23481;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#36229;&#36807;&#20195;&#34920;&#26080;&#23475;&#21709;&#24212;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#20173;&#28982;&#20986;&#29616;&#22312;&#25353;&#27010;&#29575;&#38477;&#24207;&#25490;&#24207;&#30340;&#26631;&#35760;&#20013;&#30340;&#21069;&#20960;&#20010;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#35782;&#21035;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#24182;&#22686;&#24378;&#20854;&#33391;&#24615;&#24433;&#21709;&#21147;&#26469;&#20943;&#36731;&#36234;&#29425;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08983v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;AIR-Bench&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07729</link><description>&lt;p&gt;
AIR-Bench: &#36890;&#36807;&#29983;&#25104;&#24615;&#29702;&#35299;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;AIR-Bench&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#23548;&#24615;&#30340;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#23545;&#20154;&#19982;&#38899;&#39057;&#30340;&#20114;&#21160;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#33021;&#22815;&#35780;&#20272;&#20197;&#38899;&#39057;&#20026;&#20013;&#24515;&#30340;&#20114;&#21160;&#33021;&#21147;&#30340;&#22522;&#20934;&#24050;&#32463;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20197;&#24448;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;&#19981;&#21516;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#32570;&#20047;&#23545;&#22260;&#32469;&#38899;&#39057;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#36861;&#36394;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65288;LALMs&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24182;&#20026;&#26410;&#26469;&#30340;&#25913;&#36827;&#25552;&#20379;&#25351;&#23548;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AIR-Bench&#65288;&#38899;&#39057;&#25351;&#23548;&#22522;&#20934;&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LALMs&#29702;&#35299;&#21508;&#31181;&#31867;&#22411;&#38899;&#39057;&#20449;&#21495;&#65288;&#21253;&#25324;&#20154;&#31867;&#35821;&#38899;&#12289;&#33258;&#28982;&#22768;&#38899;&#21644;&#38899;&#20048;&#65289;&#20197;&#21450;&#19982;&#20154;&#20197;&#25991;&#26412;&#24418;&#24335;&#36827;&#34892;&#20132;&#20114;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;AIR-Bench&#21253;&#21547;&#20004;&#20010;&#32500;&#24230;&#65306;&#22522;&#30784;&#21644;&#29983;&#25104;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (\textbf{A}udio \textbf{I}nst\textbf{R}uction \textbf{Bench}mark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: \textit{foundation} and \textit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.06782</link><description>&lt;p&gt;
&#19982;&#26356;&#26377;&#35828;&#26381;&#21147;&#30340;LLMs&#36777;&#35770;&#20250;&#23548;&#33268;&#26356;&#30495;&#23454;&#30340;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Debating with More Persuasive LLMs Leads to More Truthful Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#25152;&#38656;&#34892;&#20026;&#19968;&#33268;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#23558;&#36229;&#36807;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#65292;&#20154;&#31867;&#35780;&#20272;&#30340;&#35282;&#33394;&#23558;&#28436;&#21464;&#20026;&#38750;&#19987;&#23478;&#30417;&#30563;&#19987;&#23478;&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#25105;&#20204;&#38382;&#65306;&#26356;&#24369;&#30340;&#27169;&#22411;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#21527;&#65311;&#25105;&#20204;&#22312;&#31867;&#20284;&#30340;&#29615;&#22659;&#20013;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26356;&#24378;&#30340;&#27169;&#22411;&#65288;&#19987;&#23478;&#65289;&#25317;&#26377;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#32780;&#26356;&#24369;&#30340;&#27169;&#22411;&#65288;&#38750;&#19987;&#23478;&#65289;&#32570;&#20047;&#36825;&#20123;&#20449;&#24687;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#26041;&#27861;&#26159;\textit{&#36777;&#35770;}&#65292;&#20854;&#20013;&#20004;&#20010;LLM&#19987;&#23478;&#20998;&#21035;&#25903;&#25345;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#19968;&#20010;&#38750;&#19987;&#23478;&#36873;&#25321;&#31572;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#36777;&#35770; consistently&#24110;&#21161;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#65292;&#20998;&#21035;&#36798;&#21040;76%&#21644;88%&#30340;&#20934;&#30830;&#24615;&#65288;&#26420;&#32032;&#22522;&#20934;&#20998;&#21035;&#20026;48%&#21644;60%&#65289;&#12290;&#27492;&#22806;&#65292;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#20248;&#21270;&#19987;&#23478;&#36777;&#35770;&#32773;&#30340;&#35828;&#26381;&#21147;&#20250;&#25552;&#39640;&#38750;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05140</link><description>&lt;p&gt;
Tag-LLM: &#23558;&#36890;&#29992;&#30340;LLM&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#20877;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#19987;&#38376;&#39046;&#22495;&#20013;&#65292;&#22914;&#29289;&#29702;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26410;&#20805;&#20998;&#28085;&#30422;&#30340;&#39046;&#22495;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36890;&#29992;LLMs&#37325;&#26032;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26377;&#25928;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#34987;&#21442;&#25968;&#21270;&#20026;&#36830;&#32493;&#21521;&#37327;&#24182;&#38468;&#21152;&#21040;LLMs&#30340;&#23884;&#20837;&#23618;&#65292;&#20197;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#26631;&#31614;&#65306;&#39046;&#22495;&#26631;&#31614;&#29992;&#20110;&#38480;&#23450;&#19987;&#19994;&#34920;&#31034;&#65288;&#20363;&#22914;&#21270;&#23398;&#24335;&#65289;&#24182;&#25552;&#20379;&#39046;&#22495;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65307;&#21151;&#33021;&#26631;&#31614;&#29992;&#20110;&#34920;&#31034;&#29305;&#23450;&#30340;&#21151;&#33021;&#65288;&#20363;&#22914;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65289;&#24182;&#21387;&#32553;&#21151;&#33021;&#35299;&#20915;&#25351;&#20196;&#12290;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#39046;&#22495;&#30693;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#36825;&#20123;&#26631;&#31614;&#30340;&#21327;&#35758;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#20197;&#21450;&#26032;&#20852;&#30340;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#36824;&#35752;&#35770;&#20102;LLMs&#30340;&#26368;&#26032;&#33539;&#20363;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05121</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Table Processing: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#20197;&#21450;&#26032;&#20852;&#30340;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#36824;&#35752;&#35770;&#20102;LLMs&#30340;&#26368;&#26032;&#33539;&#20363;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#36890;&#24120;&#26159;&#20108;&#32500;&#32467;&#26500;&#21270;&#30340;&#65292;&#29992;&#20110;&#23384;&#20648;&#22823;&#37327;&#25968;&#25454;&#65292;&#22312;&#25968;&#25454;&#24211;&#26597;&#35810;&#12289;&#30005;&#23376;&#34920;&#26684;&#35745;&#31639;&#21644;&#20174;&#32593;&#32476;&#34920;&#26684;&#29983;&#25104;&#25253;&#21578;&#31561;&#26085;&#24120;&#27963;&#21160;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#36825;&#20123;&#20197;&#34920;&#26684;&#20026;&#20013;&#24515;&#30340;&#20219;&#21153;&#21487;&#20197;&#24102;&#26469;&#37325;&#22823;&#30340;&#20844;&#20247;&#21033;&#30410;&#65292;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#20852;&#36259;&#12290;&#35813;&#35843;&#26597;&#23545;&#34920;&#26684;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27010;&#36848;&#65292;&#19981;&#20165;&#28085;&#30422;&#20256;&#32479;&#39046;&#22495;&#22914;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#65288;Table QA&#65289;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#36824;&#21253;&#25324;&#26368;&#36817;&#24378;&#35843;&#30340;&#26041;&#38754;&#65292;&#22914;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#36229;&#36234;&#20102;&#26089;&#26399;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;LLM&#20351;&#29992;&#20013;&#30340;&#26368;&#26032;&#33539;&#20363;&#12290;&#37325;&#28857;&#26159;LLMs&#39046;&#22495;&#20869;&#30340;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#20960;&#20010;&#25361;&#25112;&#65292;&#28085;&#30422;&#31169;&#26377;&#37096;&#32626;&#12289;&#39640;&#25928;&#25512;&#26029;&#21644; LLMS &#21457;&#23637;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet calculations, and generating reports from web tables. Automating these table-centric tasks with Large Language Models (LLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides an extensive overview of table tasks, encompassing not only the traditional areas like table question answering (Table QA) and fact verification, but also newly emphasized aspects such as table manipulation and advanced table data analysis. Additionally, it goes beyond the early strategies of pre-training and fine-tuning small language models, to include recent paradigms in LLM usage. The focus here is particularly on instruction-tuning, prompting, and agent-based approaches within the realm of LLMs. Finally, we highlight several challenges, ranging from private deployment and efficient inference to the developmen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20840;&#38754;&#25552;&#39640;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#24615;&#33021;</title><link>https://arxiv.org/abs/2209.00568</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Contrastive Knowledge Co-Distillation for Event Temporal Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00568
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20840;&#38754;&#25552;&#39640;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#65288;ETRE&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20107;&#20214;&#23545;&#20301;&#20110;&#19981;&#21516;&#36317;&#31163;&#30340;&#35805;&#35821;&#20013;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25509;&#36817;&#24615;&#24102;&#12290;&#20851;&#20110;&#20301;&#20110;&#26356;&#36828;&#65288;&#21363;&#8220;&#38271;&#8221;&#65289;&#25110;&#26356;&#36817;&#65288;&#21363;&#8220;&#30701;&#8221;&#65289;&#25509;&#36817;&#24615;&#24102;&#30340;&#20107;&#20214;&#23545;&#30340;&#26102;&#38388;&#39034;&#24207;&#20256;&#36798;&#26041;&#24335;&#19981;&#21516;&#12290;&#30446;&#21069;ETRE&#27169;&#22411;&#24448;&#24448;&#22312;&#20301;&#20110;&#30701;&#25110;&#38271;&#25509;&#36817;&#24615;&#24102;&#30340;&#20107;&#20214;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#19981;&#33021;&#21516;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#25991;&#26412;&#21253;&#21547;&#25152;&#26377;&#31867;&#22411;&#30340;&#26102;&#38388;&#20107;&#20214;&#23545;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#65292;&#36825;&#26159;&#19968;&#31181;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#22810;&#20010;&#20107;&#20214;&#23545;&#25509;&#36817;&#24615;&#24102;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#23545;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MulCo&#25104;&#21151;&#22320;&#25972;&#21512;&#20102;&#36328;&#30701;&#21644;&#38271;&#25509;&#36817;&#24615;&#24102;&#30340;&#19982;&#26102;&#38388;&#25512;&#29702;&#30456;&#20851;&#30340;&#35821;&#35328;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00568v2 Announce Type: replace-cross  Abstract: Event Temporal Relation Extraction (ETRE) is a crucial yet challenging problem. Event pairs are situated within a discourse at different distances, which we refer to as proximity bands. The temporal ordering communicated about event pairs situated at more remote (i.e., ``long'') or less remote (i.e., ``short'') proximity bands is encoded differently. SOTA ETRE models have tended to perform well on events situated at either short or long proximity bands, but not both. Yet, real-world, natural texts contain all types of temporal event-pairs. In this paper, we present MulCo: Multi-Scale Contrastive Knowledge Co-Distillation, a fusion approach that shares knowledge across multiple event pair proximity bands in order to improve performance on all types of temporal datasets. Our experimental results show that MulCo successfully integrates linguistic cues pertaining to temporal reasoning across both short and long proximity bands and 
&lt;/p&gt;</description></item><item><title>RCAgent&#26159;&#19968;&#20010;&#24037;&#20855;&#22686;&#24378;&#30340;LLM&#33258;&#20027;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20113;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#33021;&#22815;&#23454;&#29616;&#33258;&#30001;&#26684;&#24335;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#24182;&#22312;&#21508;&#20010;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16340</link><description>&lt;p&gt;
RCAgent&#65306;&#22522;&#20110;&#33258;&#20027;&#20195;&#29702;&#21644;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20113;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models. (arXiv:2310.16340v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16340
&lt;/p&gt;
&lt;p&gt;
RCAgent&#26159;&#19968;&#20010;&#24037;&#20855;&#22686;&#24378;&#30340;LLM&#33258;&#20027;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20113;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#33021;&#22815;&#23454;&#29616;&#33258;&#30001;&#26684;&#24335;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#24182;&#22312;&#21508;&#20010;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20113;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#21463;&#21040;&#20102;&#31215;&#26497;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#25163;&#21160;&#24037;&#20316;&#27969;&#35774;&#32622;&#65292;&#24182;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;LLMs&#30340;&#20915;&#31574;&#21644;&#29615;&#22659;&#20132;&#20114;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RCAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#29992;&#21644;&#27880;&#37325;&#38544;&#31169;&#30340;&#24037;&#20855;&#22686;&#24378;LLM&#33258;&#20027;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#38469;&#30340;&#24037;&#19994;RCA&#20351;&#29992;&#12290;RCAgent&#22312;&#20869;&#37096;&#37096;&#32626;&#30340;&#27169;&#22411;&#19978;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;GPT&#31995;&#21015;&#65292;&#33021;&#22815;&#36827;&#34892;&#33258;&#30001;&#26684;&#24335;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#24182;&#32467;&#21512;&#21508;&#31181;&#22686;&#24378;&#21151;&#33021;&#65292;&#21253;&#25324;&#29420;&#29305;&#30340;&#34892;&#21160;&#36712;&#36857;&#33258;&#19968;&#33268;&#24615;&#21644;&#19968;&#22871;&#29992;&#20110;&#19978;&#19979;&#25991;&#31649;&#29702;&#12289;&#31283;&#23450;&#21270;&#21644;&#23548;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;RCAgent&#22312;RCA&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#39044;&#27979;&#26681;&#26412;&#21407;&#22240;&#12289;&#35299;&#20915;&#26041;&#26696;&#12289;&#35777;&#25454;&#21644;&#36131;&#20219;&#65289;&#20197;&#21450;&#24403;&#21069;&#35268;&#21017;&#26410;&#28085;&#30422;&#30340;&#20219;&#21153;&#19978;&#37117;&#26126;&#26174;&#20248;&#20110;ReAct&#65292;&#24471;&#21040;&#20102;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#39564;&#35777;&#30340;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM) applications in cloud root cause analysis (RCA) have been actively explored recently. However, current methods are still reliant on manual workflow settings and do not unleash LLMs' decision-making and environment interaction capabilities. We present RCAgent, a tool-augmented LLM autonomous agent framework for practical and privacy-aware industrial RCA usage. Running on an internally deployed model rather than GPT families, RCAgent is capable of free-form data collection and comprehensive analysis with tools. Our framework combines a variety of enhancements, including a unique Self-Consistency for action trajectories, and a suite of methods for context management, stabilization, and importing domain knowledge. Our experiments show RCAgent's evident and consistent superiority over ReAct across all aspects of RCA -- predicting root causes, solutions, evidence, and responsibilities -- and tasks covered or uncovered by current rules, as validated by both automate
&lt;/p&gt;</description></item><item><title>&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#23545;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.15469</link><description>&lt;p&gt;
&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks. (arXiv:2310.15469v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15469
&lt;/p&gt;
&lt;p&gt;
&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#23545;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2018&#24180;&#21518;&#30340;&#26102;&#20195;&#26631;&#24535;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;OpenAI&#30340;ChatGPT&#31561;&#21019;&#26032;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#38543;&#30528;&#34892;&#19994;&#22312;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#24182;&#21033;&#29992;&#22823;&#37327;&#30340;&#20154;&#31867;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#30340;&#21162;&#21147;&#65292;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#20063;&#20986;&#29616;&#20102;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;&#22312;&#22522;&#20110;&#32593;&#32476;&#30340;&#25968;&#25454;&#33719;&#21462;&#36807;&#31243;&#20013;&#65292;&#21487;&#33021;&#20250;&#24847;&#22806;&#31215;&#32047;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;&#24847;&#22806;&#30340;PII&#27844;&#38706;&#39118;&#38505;&#12290;&#34429;&#28982;&#20687;RLHF&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#36825;&#26679;&#30340;&#31574;&#30053;&#24050;&#34987;&#29992;&#26469;&#25511;&#21046;&#38544;&#31169;&#20405;&#26435;&#30340;&#39118;&#38505;&#65292;&#20294;LLM&#30340;&#26368;&#26032;&#36827;&#23637;&#65288;&#20197;OpenAI&#30340;GPT-3.5&#30340;&#24494;&#35843;&#30028;&#38754;&#20026;&#20195;&#34920;&#65289;&#37325;&#26032;&#24341;&#21457;&#20102;&#20851;&#27880;&#12290;&#26377;&#20154;&#21487;&#33021;&#20250;&#38382;&#65306;LLM&#30340;&#24494;&#35843;&#26159;&#21542;&#20250;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#30340;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#65311;&#26412;&#25991;&#25253;&#36947;&#20102;&#39318;&#27425;&#23581;&#35797;&#23547;&#27714;&#31572;&#26696;&#30340;&#21162;&#21147;&#65292;&#37325;&#28857;&#26159;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The era post-2018 marked the advent of Large Language Models (LLMs), with innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess. As the industry galloped toward augmenting model parameters and capitalizing on vast swaths of human language data, security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure. While strategies like RLHF during training and Catastrophic Forgetting have been marshaled to control the risk of privacy infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning interface for GPT-3.5, have reignited concerns. One may ask: can the fine-tuning of LLMs precipitate the leakage of personal information embedded within training datasets? This paper reports the first endeavor to seek the answer to the question, particularly our discovery of a new LLM exploitation avenue
&lt;/p&gt;</description></item></channel></rss>