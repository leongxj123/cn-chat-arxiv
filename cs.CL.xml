<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#23574;&#23792;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25214;&#20986;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#21457;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2312.16903</link><description>&lt;p&gt;
&#21035;&#20877;&#20986;&#29616;&#23574;&#23792;&#20102;&#65306;&#31283;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Spike No More: Stabilizing the Pre-training of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.16903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#23574;&#23792;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25214;&#20986;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#32463;&#24120;&#20986;&#29616;&#25439;&#22833;&#23574;&#23792;&#12290;&#36825;&#20123;&#23574;&#23792;&#20250;&#38477;&#20302;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#20250;&#30772;&#22351;&#39044;&#35757;&#32451;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#24212;&#35813;&#36991;&#20813;&#36825;&#31181;&#23574;&#23792;&#30340;&#20986;&#29616;&#12290;&#20026;&#20102;&#30740;&#31350;&#25439;&#22833;&#23574;&#23792;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#20851;&#27880;&#20869;&#37096;&#23618;&#30340;&#26799;&#24230;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#20004;&#20010;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#38450;&#26799;&#24230;&#29190;&#28856;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#21021;&#22987;&#21270;&#26041;&#27861;&#21644;&#23545;&#23884;&#20837;&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#26469;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training. Since the pre-training needs a vast computational budget, we should avoid such spikes. To investigate the cause of loss spikes, we focus on gradients of internal layers. Through theoretical analyses, we reveal two causes of the exploding gradients, and provide requirements to prevent the explosion. In addition, we propose a method to satisfy the requirements by combining the initialization method and a simple modification to embeddings. We conduct various experiments to verify our theoretical analyses empirically. Experimental results indicate that the combination is effective in preventing spikes during pre-training.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#65292;&#22312;&#21508;&#31181;&#32422;&#26463;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.13244</link><description>&lt;p&gt;
&#20351;&#29992;&#24072;&#29983;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13244
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#65292;&#22312;&#21508;&#31181;&#32422;&#26463;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#27169;&#22411;&#21644;&#35745;&#31639;&#24037;&#20855;&#29992;&#20110;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#20998;&#26512;&#65292;&#20294;&#29983;&#25104;&#31526;&#21512;&#25152;&#26377;&#26399;&#26395;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#20998;&#23376;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#31867;&#20284;&#20110;&#23398;&#29983;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#26469;&#33258;&#21508;&#31181;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#65288;&#21363;&#8220;&#32769;&#24072;&#8221;&#65289;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35757;&#32451;TSMMG&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#36825;&#20123;&#8216;&#32769;&#24072;&#8217;&#20013;&#25552;&#21462;&#30340;&#20998;&#23376;&#30693;&#35782;&#26500;&#24314;&#20102;&#22823;&#37327;&#25991;&#26412;-&#20998;&#23376;&#23545;&#65292;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#21508;&#31181;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;TSMMG&#22312;&#29983;&#25104;&#31526;&#21512;&#22797;&#26434;&#12289;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20004;&#12289;&#19977;&#21644;&#22235;&#32422;&#26463;&#20219;&#21153;&#30340;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24179;&#22343;&#20998;&#23376;&#26377;&#25928;&#24615;&#36229;&#36807;99&#65285;&#65292;&#25104;&#21151;&#29575;&#20998;&#21035;&#20026;88.08&#65285;&#12289;65.27&#65285;&#21644;61.44&#65285;&#12290;&#35813;&#27169;&#22411;&#36824;ex
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13244v1 Announce Type: new  Abstract: While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge. Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'. To train TSMMG, we construct a large set of text-molecule pairs by extracting molecular knowledge from these 'teachers', enabling it to generate novel molecules that conform to the descriptions through various text prompts. We experimentally show that TSMMG remarkably performs in generating molecules meeting complex, natural language-described property requirements across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 88.08%, 65.27%, and 61.44%, respectively. The model also ex
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12242</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;
&lt;/p&gt;
&lt;p&gt;
Reference-based Metrics Disprove Themselves in Question Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12242
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLEU&#21644;BERTScore&#31561;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;(QG)&#12290;&#26412;&#30740;&#31350;&#22312;SQuAD&#21644;HotpotQA&#31561;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#24182;&#19981;&#33021;&#20445;&#35777;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;&#22823;&#22810;&#25968;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#21482;&#26377;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#65307;&#25105;&#20204;&#22797;&#21046;&#20102;&#27880;&#37322;&#36807;&#31243;&#24182;&#25910;&#38598;&#20102;&#21478;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#12290;&#39044;&#26399;&#22909;&#30340;&#25351;&#26631;&#24212;&#35813;&#23545;&#20154;&#24037;&#39564;&#35777;&#30340;&#38382;&#39064;&#30340;&#35780;&#20998;&#19981;&#20250;&#20302;&#20110;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#26032;&#25910;&#38598;&#30340;&#21442;&#32771;&#25991;&#29486;&#19978;&#65292;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#32467;&#26524;&#21364;&#35777;&#26126;&#20102;&#36825;&#20123;&#25351;&#26631;&#26412;&#36523;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#65292;&#30001;&#22810;&#32500;&#26631;&#20934;&#32452;&#25104;&#65292;&#22914;&#33258;&#28982;&#24615;&#12289;&#21487;&#22238;&#31572;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#26631;&#20934;&#19981;&#21463;&#38480;&#20110;&#21333;&#20010;&#21442;&#32771;&#38382;&#39064;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#65292;&#35813;&#25351;&#26631;&#20063;&#19981;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12242v1 Announce Type: cross  Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2403.09919</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#24555;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Recurrent Drafter for Fast Speculative Decoding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#25913;&#36827;&#30340;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#25104;&#29087;&#25216;&#26415;&#30340;&#20248;&#21183;&#65306;&#32463;&#20856;&#30340;&#21452;&#27169;&#22411;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#21644;&#36739;&#26032;&#30340;&#21333;&#27169;&#22411;&#26041;&#27861;Medusa&#12290;&#20174;Medusa&#24471;&#21040;&#28789;&#24863;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#21333;&#27169;&#22411;&#31574;&#30053;&#36827;&#34892;&#25512;&#27979;&#35299;&#30721;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#30340;&#21333;&#20010;&#36731;&#37327;&#32423;&#33609;&#31295;&#22836;&#26469;&#21306;&#20998;&#33258;&#24049;&#65292;&#26412;&#36136;&#19978;&#31867;&#20284;&#20110;&#32463;&#20856;&#25512;&#27979;&#35299;&#30721;&#20013;&#20351;&#29992;&#30340;&#23567;&#22411;&#33609;&#31295;&#27169;&#22411;&#65292;&#20294;&#36991;&#20813;&#20102;&#23436;&#25972;transformer&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#24490;&#29615;&#20381;&#36182;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#24555;&#36895;&#36807;&#28388;&#20986;&#33609;&#31295;&#22836;&#20013;&#19981;&#38656;&#35201;&#30340;&#20505;&#36873;&#39033;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21333;&#27169;&#22411;&#35774;&#35745;&#31616;&#26131;&#24615;&#24182;&#36991;&#20813;&#20102;&#21019;&#24314;&#25968;&#25454;&#30456;&#20851;&#26641;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
&lt;/p&gt;</description></item><item><title>&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26102;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#26681;&#25454;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#26469;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2403.09559</link><description>&lt;p&gt;
&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#23545;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Less is More: Data Value Estimation for Visual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09559
&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26102;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#26681;&#25454;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#26469;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26159;&#26500;&#24314;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLLMs&#20027;&#35201;&#20381;&#36182;&#20110;&#22810;&#20010;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#28151;&#21512;&#35757;&#32451;&#65288;&#29978;&#33267;&#36229;&#36807;&#19968;&#30334;&#19975;&#26465;&#25351;&#23548;&#65289;&#65292;&#36825;&#21487;&#33021;&#24341;&#20837;&#25968;&#25454;&#20887;&#20313;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#20869;&#23384;&#22312;&#26174;&#33879;&#20887;&#20313;&#65292;&#24182;&#26174;&#31034;&#22823;&#22823;&#20943;&#23569;&#20960;&#20010;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#29978;&#33267;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#20197;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;TIVE&#39318;&#20808;&#26681;&#25454;&#35745;&#31639;&#30340;&#26799;&#24230;&#20272;&#35745;&#35270;&#35273;&#25351;&#23548;&#30340;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#20272;&#35745;&#30340;&#20215;&#20540;&#65292;TIVE&#30830;&#23450;&#20102;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#25351;&#23548;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09559v1 Announce Type: new  Abstract: Visual instruction tuning is the key to building multimodal large language models (MLLMs), which greatly improves the reasoning capabilities of large language models (LLMs) in vision scenario. However, existing MLLMs mostly rely on a mixture of multiple highly diverse visual instruction datasets for training (even more than a million instructions), which may introduce data redundancy. To investigate this issue, we conduct a series of empirical studies, which reveal a significant redundancy within the visual instruction datasets, and show that greatly reducing the amount of several instruction dataset even do not affect the performance. Based on the findings, we propose a new data selection approach TIVE, to eliminate redundancy within visual instruction data. TIVE first estimates the task-level and instance-level value of the visual instructions based on computed gradients. Then, according to the estimated values, TIVE determines the tas
&lt;/p&gt;</description></item><item><title>AutoRD&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#65292;&#23454;&#29616;&#20102;&#25972;&#20307;F1&#24471;&#20998;47.3%&#65292;&#30456;&#23545;&#20110;&#22522;&#30784;LLM&#26377;14.4%&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.00953</link><description>&lt;p&gt;
AutoRD&#65306;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#26500;&#24314;&#30340;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00953
&lt;/p&gt;
&lt;p&gt;
AutoRD&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#65292;&#23454;&#29616;&#20102;&#25972;&#20307;F1&#24471;&#20998;47.3%&#65292;&#30456;&#23545;&#20110;&#22522;&#30784;LLM&#26377;14.4%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#21517;&#20026;AutoRD&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#26377;&#20851;&#32597;&#35265;&#30142;&#30149;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#27979;&#35797;&#26469;&#35780;&#20272;AutoRD&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#24378;&#35843;&#20102;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30340;&#31995;&#32479;AutoRD&#26159;&#19968;&#20010;&#36719;&#20214;&#27969;&#27700;&#32447;&#65292;&#28041;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#23454;&#20307;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#23454;&#20307;&#26657;&#20934;&#21644;&#30693;&#35782;&#22270;&#26500;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30001;&#24320;&#28304;&#21307;&#23398;&#26412;&#20307;&#21457;&#23637;&#32780;&#26469;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20307;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#20197;&#21450;&#30693;&#35782;&#22270;&#26500;&#24314;&#24615;&#33021;&#23545;&#31995;&#32479;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#12290;&#32467;&#26524;&#65306;AutoRD&#21462;&#24471;&#20102;47.3%&#30340;&#25972;&#20307;F1&#20998;&#25968;&#65292;&#36739;&#22522;&#30784;LLM&#25552;&#39640;&#20102;14.4%&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AutoRD&#23454;&#29616;&#20102;56.1%&#30340;&#25972;&#20307;&#23454;&#20307;&#25552;&#21462;F1&#20998;&#25968;&#65288;&#32597;&#35265;&#30142;&#30149;&#65306;83.5%&#65292;&#30142;&#30149;&#65306;35.8%&#65292;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00953v1 Announce Type: cross  Abstract: Objectives: Our objective is to create an end-to-end system called AutoRD, which automates extracting information from clinical text about rare diseases. We have conducted various tests to evaluate the performance of AutoRD and highlighted its strengths and limitations in this paper.   Materials and Methods: Our system, AutoRD, is a software pipeline involving data preprocessing, entity extraction, relation extraction, entity calibration, and knowledge graph construction. We implement this using large language models and medical knowledge graphs developed from open-source medical ontologies. We quantitatively evaluate our system on entity extraction, relation extraction, and the performance of knowledge graph construction.   Results: AutoRD achieves an overall F1 score of 47.3%, a 14.4% improvement compared to the base LLM. In detail, AutoRD achieves an overall entity extraction F1 score of 56.1% (rare_disease: 83.5%, disease: 35.8%, s
&lt;/p&gt;</description></item><item><title>DiaHalu&#26159;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#32423;&#21035;&#19978;&#30340;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.00896</link><description>&lt;p&gt;
DiaHalu&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00896
&lt;/p&gt;
&lt;p&gt;
DiaHalu&#26159;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#32423;&#21035;&#19978;&#30340;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#26368;&#36817;&#20960;&#24180;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#24187;&#35273;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26377;&#35768;&#22810;&#22522;&#20934;&#34987;&#25552;&#20986;&#26469;&#26816;&#27979;&#36825;&#31181;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#22522;&#20934;&#19981;&#26159;&#30001;LLMs&#33258;&#28982;&#29983;&#25104;&#30340;&#65292;&#32780;&#26159;&#26377;&#24847;&#24341;&#21457;&#30340;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22522;&#20934;&#20165;&#20851;&#27880;&#20107;&#23454;&#19978;&#30340;&#24187;&#35273;&#65292;&#32780;&#24573;&#35270;&#20102;&#24544;&#23454;&#24230;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;LLMs&#26102;&#20195;&#23545;&#35805;&#27169;&#24335;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#30446;&#21069;&#30340;&#22522;&#20934;&#20165;&#38598;&#20013;&#22312;&#21477;&#23376;&#32423;&#21644;&#27573;&#33853;&#32423;&#30340;&#24187;&#35273;&#19978;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; DiaHalu&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25910;&#38598;&#30340;&#20027;&#39064;&#38598;&#25104;&#21040;&#31995;&#32479;&#25552;&#31034;&#20013;&#65292;&#20419;&#36827;&#20004;&#20010;ChatGPT3.5&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25163;&#21160;&#20462;&#25913;&#19981;&#31526;&#21512;&#20154;&#31867;&#35821;&#35328;&#32422;&#23450;&#30340;&#20869;&#23481;&#65292;&#28982;&#21518;&#35753;LLMs&#37325;&#26032;&#29983;&#25104;&#65292;&#27169;&#25311;&#30495;&#23454;&#30340;&#20154;&#31867;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00896v1 Announce Type: cross  Abstract: Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, numerous benchmarks are proposed to detect the hallucination. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation benchmark to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-
&lt;/p&gt;</description></item><item><title>TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19467</link><description>&lt;p&gt;
TV-TREES&#65306;&#29992;&#20110;&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;
&lt;/p&gt;
&lt;p&gt;
TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19467
&lt;/p&gt;
&lt;p&gt;
TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#30005;&#35270;&#21098;&#36753;&#31561;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#37096;&#20998;&#26159;&#22240;&#20026;&#24403;&#21069;&#30340;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#25512;&#29702;&#65292;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TV-TREES&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#12290;TV-TREES&#20316;&#20026;&#19968;&#31181;&#20419;&#36827;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#20219;&#21153;&#26469;&#35780;&#20272;&#27492;&#31867;&#26041;&#27861;&#30340;&#25512;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#12289;&#20855;&#26377;&#26368;&#20808;&#36827;&#38646;-shot&#24615;&#33021;&#30340;&#23436;&#25972;&#35270;&#39057;&#21098;&#36753;&#65292;&#23637;&#31034;&#20102;&#19982;&#40657;&#30418;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19467v1 Announce Type: cross  Abstract: It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#36753;&#65288;UKE&#65289;&#65292;&#26088;&#22312;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20316;&#20026;&#30693;&#35782;&#26356;&#26032;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#26500;&#24314;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#21644;&#21709;&#24212;&#24615;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18909</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#20107;&#23454;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#65306;&#36808;&#21521;&#23454;&#29992;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#36753;&#65288;UKE&#65289;&#65292;&#26088;&#22312;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20316;&#20026;&#30693;&#35782;&#26356;&#26032;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#26500;&#24314;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#21644;&#21709;&#24212;&#24615;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#23558;&#30693;&#35782;&#26356;&#26032;&#27880;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#20854;&#20445;&#25345;&#27491;&#30830;&#24615;&#21644;&#26368;&#26032;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#31574;&#30053;&#26126;&#26174;&#19981;&#20999;&#23454;&#38469;&#65306;&#23427;&#20204;&#20165;&#20351;&#29992;&#31934;&#24515;&#31574;&#21010;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#65288;&#20027;&#39064;&#12289;&#20851;&#31995;&#21644;&#23545;&#35937;&#30340;&#19977;&#20803;&#32452;&#65289;&#36827;&#34892;&#26356;&#26032;&#65292;&#32780;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#26356;&#26032;&#36890;&#24120;&#20986;&#29616;&#22312;&#26032;&#38395;&#25991;&#31456;&#31561;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#36753;&#65288;UKE&#65289;&#12290;&#23427;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#30452;&#25509;&#35780;&#20272;&#32534;&#36753;&#24615;&#33021;&#65292;&#31216;&#20026;&#38750;&#32467;&#26500;&#21270;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;UKE&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#26500;&#24314;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#21709;&#24212;&#36805;&#36895;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#25104;&#20026;&#19968;&#20010;&#26356;&#23454;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#22312;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;UKE&#23545;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#23427;&#20204;&#30340;&#20851;&#38190;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18909v1 Announce Type: cross  Abstract: Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date. However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with subjects, relations, and objects), whereas real-world knowledge updates commonly emerge in unstructured texts like news articles. In this paper, we propose a new benchmark, Unstructured Knowledge Editing (UKE). It evaluates editing performance directly using unstructured texts as knowledge updates, termed unstructured facts. Hence UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, becoming a more practical benchmark. We conduct extensive experiments on newly built datasets and demonstrate that UKE poses a significant challenge to state-of-the-art knowledge editing methods, resulting in their critical performance declines. We further
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.16048</link><description>&lt;p&gt;
LLMs&#24102;&#26377;&#24605;&#32500;&#38142;&#26465;&#26159;&#38750;&#22240;&#26524;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
LLMs with Chain-of-Thought Are Non-Causal Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#26377;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;LLMs&#20013;&#27491;&#30830;&#31572;&#26696;&#36319;&#38543;&#19981;&#27491;&#30830;CoTs&#30340;&#39057;&#29575;&#21450;&#21453;&#20043;&#12290;&#25105;&#20204;&#37319;&#29992;&#22240;&#26524;&#20998;&#26512;&#26469;&#35780;&#20272;CoTs/&#25351;&#20196;&#19982;LLMs&#31572;&#26696;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25581;&#31034;LLMs&#36817;&#20284;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#12290;&#36890;&#36807;&#27604;&#36739;&#26263;&#31034;SCM&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;SCM&#65292;&#25105;&#20204;&#31361;&#26174;&#20102;LLM&#21644;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24433;&#21709;&#26263;&#31034;SCM&#22240;&#26524;&#32467;&#26500;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26174;&#33879;&#24433;&#21709;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;https://github.com/StevenZHB/CoT_Causal_Analysis&#21457;&#24067;&#20102;&#20195;&#30721;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16048v1 Announce Type: cross  Abstract: This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.
&lt;/p&gt;</description></item><item><title>ChatEA&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23454;&#20307;&#23545;&#40784;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;KG-code&#32763;&#35793;&#27169;&#22359;&#21644;&#20004;&#38454;&#27573;EA&#31574;&#30053;&#26469;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15048</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#20307;&#23545;&#40784;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Power of Large Language Models for Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15048
&lt;/p&gt;
&lt;p&gt;
ChatEA&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23454;&#20307;&#23545;&#40784;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;KG-code&#32763;&#35793;&#27169;&#22359;&#21644;&#20004;&#38454;&#27573;EA&#31574;&#30053;&#26469;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#23545;&#20110;&#25972;&#21512;&#19981;&#21516;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;EA&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#27604;&#36739;&#23454;&#20307;&#23884;&#20837;&#65292;&#20294;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#36755;&#20837;KG&#25968;&#25454;&#21644;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;&#33021;&#21147;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#32422;&#26463;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatEA&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34701;&#20837;&#20197;&#25913;&#21892;EA&#12290;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#30340;&#36755;&#20837;KG&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;ChatEA&#24341;&#20837;&#20102;&#19968;&#20010;KG-code&#32763;&#35793;&#27169;&#22359;&#65292;&#23558;KG&#32467;&#26500;&#32763;&#35793;&#25104;LLMs&#21487;&#29702;&#35299;&#30340;&#26684;&#24335;&#65292;&#20174;&#32780;&#20351;LLMs&#33021;&#22815;&#21033;&#29992;&#20854;&#24191;&#27867;&#30340;&#32972;&#26223;&#30693;&#35782;&#25552;&#39640;EA&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#23545;&#23454;&#20307;&#23884;&#20837;&#27604;&#36739;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;ChatEA&#23454;&#29616;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;EA&#31574;&#30053;&#65292;&#21033;&#29992;LLMs&#22312;&#23545;&#35805;&#26684;&#24335;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15048v1 Announce Type: cross  Abstract: Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs' capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy wh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#27010;&#29575;&#20581;&#22766;&#26041;&#27861;&#65292;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15020</link><description>&lt;p&gt;
&#20855;&#26377;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#20581;&#22766;&#26463;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistically-sound beam search with masked language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#27010;&#29575;&#20581;&#22766;&#26041;&#27861;&#65292;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#30340;&#26463;&#25628;&#32034;&#23384;&#22312;&#25361;&#25112;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#24207;&#21015;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#19981;&#20687;&#33258;&#22238;&#24402;&#27169;&#22411;&#37027;&#26679;readily available&#12290;&#28982;&#32780;&#65292;&#20272;&#31639;&#36825;&#26679;&#30340;&#20998;&#24067;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#24212;&#29992;&#65292;&#21253;&#25324;&#34507;&#30333;&#24037;&#31243;&#21644;&#21476;&#20195;&#25991;&#26412;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#29575;&#20581;&#22766;&#24615;&#30340;&#20351;&#29992;MLMs&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#20351;&#29992;&#26631;&#20934;&#26463;&#25628;&#32034;&#23545;MLMs&#25191;&#34892;&#25991;&#26412;&#22635;&#20805;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#12290;&#24403;&#36825;&#20123;&#26465;&#20214;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#29575;&#20581;&#22766;&#24615;&#30340;&#20462;&#25913;&#65292;&#32780;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#39044;&#26399;&#26465;&#20214;&#19979;&#23427;&#20248;&#20110;&#21069;&#36848;&#30340;&#26463;&#25628;&#32034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27604;&#36739;&#22810;&#20010;&#39046;&#22495;&#20013;&#20960;&#31181;&#20351;&#29992;MLMs&#36827;&#34892;&#22635;&#20805;&#30340;&#26041;&#27861;&#30340;&#32463;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15020v1 Announce Type: cross  Abstract: Beam search with masked language models (MLMs) is challenging in part because joint probability distributions over sequences are not readily available, unlike for autoregressive models. Nevertheless, estimating such distributions has applications in many domains, including protein engineering and ancient text restoration. We present probabilistically-sound methods for beam search with MLMs. First, we clarify the conditions under which it is theoretically sound to perform text infilling with MLMs using standard beam search. When these conditions fail, we provide a probabilistically-sound modification with no additional computational complexity and demonstrate that it is superior to the aforementioned beam search in the expected conditions. We then present empirical results comparing several infilling approaches with MLMs across several domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#23545;&#22810;&#35821;&#27169;&#22411;&#22312;&#19981;&#21516;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#24182;&#34892;&#25945;&#23398;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25945;&#23398;&#35843;&#25972;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#36328;&#35821;&#35328;&#36981;&#24490;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#34920;&#38754;&#23545;&#40784;&#20551;&#35774;&#30340;&#36136;&#30097;</title><link>https://arxiv.org/abs/2402.13703</link><description>&lt;p&gt;
&#35843;&#26597;&#22810;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#65306;&#22810;&#35821;&#27169;&#22411;&#26159;&#21542;&#38656;&#35201;&#22810;&#35821;&#25945;&#23398;&#65311;
&lt;/p&gt;
&lt;p&gt;
Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#23545;&#22810;&#35821;&#27169;&#22411;&#22312;&#19981;&#21516;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#24182;&#34892;&#25945;&#23398;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25945;&#23398;&#35843;&#25972;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#36328;&#35821;&#35328;&#36981;&#24490;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#34920;&#38754;&#23545;&#40784;&#20551;&#35774;&#30340;&#36136;&#30097;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13703v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#23558;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36716;&#21270;&#20026;&#38596;&#36777;&#32780;&#26377;&#29992;&#30340;&#21161;&#25163;&#23545;&#20419;&#36827;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#22320;&#21306;&#30340;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#31934;&#31070;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23545;&#36328;&#22810;&#31181;&#21360;&#27431;&#35821;&#35328;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#30340;&#30740;&#31350;&#32773;&#65292;&#26088;&#22312;&#30740;&#31350;&#22810;&#35821;&#27169;&#22411;&#22312;&#36873;&#25321;&#30340;&#26368;&#24120;&#29992;&#30340;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24182;&#34892;&#12289;&#22810;&#36718;&#25945;&#23398;&#35843;&#25972;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#35821;&#35328;&#21644;&#25945;&#23398;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#20013;&#22411;&#22810;&#35821;&#35328;LLM&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#24182;&#34892;&#25945;&#23398;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25945;&#23398;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24182;&#34892;&#25945;&#23398;&#35843;&#25972;&#32780;&#19981;&#26159;&#21333;&#35821;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#25945;&#23398;&#35843;&#25972;&#21487;&#20197;&#20351;&#36328;&#35821;&#35328;&#36981;&#24490;&#33021;&#21147;&#25552;&#39640;&#22810;&#36798;4.6%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#34920;&#38754;&#23545;&#40784;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#25152;&#35843;&#26597;&#30340;&#22810;&#35821;7B&#21442;&#25968;&#27169;&#22411;&#26159;&#19968;&#20010;&#21453;&#20363;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#25945;&#23398;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13703v1 Announce Type: new  Abstract: The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11161</link><description>&lt;p&gt;
PANDA&#65288;Pedantic ANswer-correctness Determination and Adjudication&#65289;&#65306;&#25913;&#36827;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;QA&#65289;&#21482;&#26377;&#22312;&#25105;&#20204;&#30693;&#36947;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#26102;&#25165;&#33021;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#26377;&#36259;&#30340;QA&#31034;&#20363;&#65292;&#24403;&#21069;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#65288;AC&#65289;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20887;&#38271;&#12289;&#33258;&#30001;&#26684;&#24335;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#32570;&#20047;&#25968;&#25454;&#21644;&#27169;&#22411;&#36807;&#22823;&#12290;&#22522;&#20110;LLM&#30340;&#35780;&#20998;&#22120;&#19982;&#20154;&#31867;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20294;&#36825;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#20165;&#22312;&#26377;&#38480;&#30340;QA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#28165;&#26224;&#30340;&#25351;&#21335;&#26469;&#35780;&#20272;&#20174;&#20154;&#31867;QA&#27604;&#36187;&#20013;&#37319;&#32435;&#30340;&#26426;&#22120;QA&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#30830;&#23450;&#21644;&#35009;&#20915;&#65288;Precise ANswer correctness Determination and Adjudication&#65292;PANDA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23567;&#24039;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;AC&#20998;&#31867;&#22120;&#65288;812 KB&#65289;&#65292;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11161v1 Announce Type: cross  Abstract: Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big. LLM based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing clear guidelines for evaluating machine QA adopted from human QA contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.
&lt;/p&gt;</description></item><item><title>Paramanu&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#21333;&#20010;GPU&#19978;&#36827;&#34892;&#20102;&#20174;&#22836;&#39044;&#35757;&#32451;&#12290;&#23427;&#36824;&#21253;&#25324;&#19968;&#20010;&#20808;&#36827;&#30340;&#21360;&#24230;&#20998;&#35789;&#22120;&#20197;&#21450;&#36991;&#20813;&#22810;&#35821;&#35328;&#35781;&#21650;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.18034</link><description>&lt;p&gt;
Paramanu: &#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18034
&lt;/p&gt;
&lt;p&gt;
Paramanu&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#21333;&#20010;GPU&#19978;&#36827;&#34892;&#20102;&#20174;&#22836;&#39044;&#35757;&#32451;&#12290;&#23427;&#36824;&#21253;&#25324;&#19968;&#20010;&#20808;&#36827;&#30340;&#21360;&#24230;&#20998;&#35789;&#22120;&#20197;&#21450;&#36991;&#20813;&#22810;&#35821;&#35328;&#35781;&#21650;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Gyan AI Paramanu&#65288;&#8220;&#21407;&#23376;&#8221;&#65289;&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#23427;&#26159;&#19968;&#20010;&#22312;&#21333;&#20010;GPU&#19978;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#30340;&#21253;&#21547;&#21333;&#35821;&#12289;&#21452;&#35821;&#21644;&#22810;&#35821;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#28085;&#30422;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#65288;&#38463;&#33832;&#22982;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#24247;&#22350;&#23612;&#35821;&#12289;&#36808;&#33922;&#21033;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#22885;&#36842;&#20122;&#35821;&#12289;&#26805;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#20197;&#21450;5&#31181;&#19981;&#21516;&#22823;&#23567;&#30340;&#23383;&#27597;&#34920;&#65288;&#23391;&#21152;&#25289;&#35821;&#12289;&#22825;&#22478;&#20307;&#12289;&#22885;&#36842;&#20122;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;1024&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#22312;&#21333;&#20010;GPU&#19978;&#39044;&#35757;&#32451;&#65292;&#38750;&#24120;&#39640;&#25928;&#12289;&#23567;&#24039;&#12289;&#24555;&#36895;&#19988;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20808;&#36827;&#30340;&#21360;&#24230;&#35821;&#20998;&#35789;&#22120;&#65292;&#29978;&#33267;&#21487;&#20197;&#26631;&#35760;&#26410;&#30693;&#35821;&#35328;&#12290;&#20026;&#20102;&#36991;&#20813;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;mParamanu&#27169;&#22411;&#20013;&#30340;&#8220;&#22810;&#35821;&#35328;&#35781;&#21650;&#8221;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#23383;&#27597;&#34920;&#25353;&#35821;&#35328;&#31867;&#22411;&#36827;&#34892;&#20102;&#21487;&#27604;&#36739;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#35780;&#20272;&#25351;&#26631;&#21253;&#25324;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Gyan AI Paramanu ("atom"), a family of novel language models for Indian languages. It is a collection of auto-regressive monolingual, bilingual, and multilingual Indic language models pretrained from scratch on a single GPU for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are pretrained with a context size of 1024 on a single GPU. The models are very efficient, small, fast, and powerful. We have also developed an efficient most advanced Indic tokenizer that can even tokenize unseen languages. In order to avoid the "curse of multi-linguality" in our multilingual mParamanu model, we pretrained on comparable corpora by typological grouping using the same script. We performed human evaluation of our pretrained models for open end text generation on grammar, coherence, creativity, and factuality metrics fo
&lt;/p&gt;</description></item><item><title>LLaMP&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#20197;&#21450;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2401.17244</link><description>&lt;p&gt;
LLaMP: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#20445;&#30495;&#26448;&#26009;&#30693;&#35782;&#26816;&#32034;&#21644;&#25552;&#28860;&#20013;&#30340;&#24378;&#22823;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17244
&lt;/p&gt;
&lt;p&gt;
LLaMP&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#20197;&#21450;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38169;&#35823;&#20449;&#24687;&#23545;&#20110;&#31185;&#23398;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;LLM&#22825;&#29983;&#32570;&#20047;&#38271;&#26399;&#35760;&#24518;&#65292;&#22240;&#27492;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25991;&#29486;&#21644;&#25968;&#25454;&#19978;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#12289;&#20020;&#26102;&#30340;&#21644;&#19981;&#21487;&#36991;&#20813;&#20855;&#26377;&#20559;&#35265;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LLaMP&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#65292;&#30001;&#22810;&#20010;&#25968;&#25454;&#24863;&#30693;&#30340;&#25512;&#29702;&#19982;&#34892;&#21160;&#65288;ReAct&#65289;&#26234;&#33021;&#20307;&#21160;&#24577;&#19982;Materials Project (MP)&#19978;&#30340;&#35745;&#31639;&#21644;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#12290;&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;LLaMP&#23637;&#31034;&#20102;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26041;&#24335;&#30340;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#21363;&#26102;&#33719;&#21462;&#30456;&#20851;&#25968;&#25454;&#23384;&#20648;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#65288;&#22914;&#26230;&#20307;&#32467;&#26500;&#21644;&#24377;&#24615;&#24352;&#37327;&#65289;&#65292;&#24182;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#30340;&#22810;&#27493;&#39588;&#36807;&#31243;&#12290;&#25105;&#20204;&#35777;&#26126;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#65292;&#23558;&#39057;&#32321;&#35760;&#24405;&#30340;&#33021;&#24102;&#38388;&#38553;MAPE&#38477;&#20302;&#20102;5.21%&#65292;&#23558;&#26174;&#33879;&#30340;&#38169;&#35823;&#38477;&#20302;&#20102;1103.54%
&lt;/p&gt;
&lt;p&gt;
Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences where reproducibility is crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of multiple data-aware reasoning-and-acting (ReAct) agents that dynamically interact with computational and experimental data on Materials Project (MP). Without fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structures and elastic tensors), and summarize multi-step procedures for solid-state synthesis. We show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge, reducing a 5.21% MAPE on frequently-documented bandgaps and a significant 1103.54%
&lt;/p&gt;</description></item><item><title>TAP4LLM&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#34920;&#26684;&#25552;&#31034;&#30340;&#22810;&#21151;&#33021;&#39044;&#22788;&#29702;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#37319;&#26679;&#12289;&#22686;&#34917;&#21644;&#25171;&#21253;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#21644;&#22823;&#22411;&#34920;&#26684;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.09039</link><description>&lt;p&gt;
TAP4LLM&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#34920;&#26684;&#25552;&#20379;&#32773;&#22312;&#23545;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#37319;&#26679;&#12289;&#22686;&#34917;&#21644;&#25171;&#21253;
&lt;/p&gt;
&lt;p&gt;
TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09039
&lt;/p&gt;
&lt;p&gt;
TAP4LLM&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#34920;&#26684;&#25552;&#31034;&#30340;&#22810;&#21151;&#33021;&#39044;&#22788;&#29702;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#37319;&#26679;&#12289;&#22686;&#34917;&#21644;&#25171;&#21253;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#21644;&#22823;&#22411;&#34920;&#26684;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#30340;&#25512;&#29702;&#22312;&#32467;&#21512;&#28145;&#24230;&#27169;&#22411;&#21644;&#31163;&#25955;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#38656;&#35201;&#23545;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#38382;&#39064;&#21644;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#34920;&#26684;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#21482;&#32771;&#34385;&#23567;&#22411;&#34920;&#26684;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#26356;&#22823;&#34920;&#26684;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#25512;&#29702;&#22797;&#26434;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#22522;&#26412;&#20449;&#24687;&#25110;&#20998;&#25955;&#22312;&#19981;&#21516;&#20301;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TAP4LLM&#20316;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#39044;&#22788;&#29702;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#24179;&#34913;&#26631;&#35760;&#20998;&#37197;&#26435;&#34913;&#26469;&#29983;&#25104;&#34920;&#26684;&#25552;&#31034;&#65292;&#23454;&#29616;(1) &#34920;&#26684;&#37319;&#26679;&#65292;(2) &#34920;&#26684;&#22686;&#34917;&#21644;(3) &#34920;&#26684;&#25171;&#21253;&#12290;&#22312;&#27599;&#20010;&#27169;&#22359;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#21644;&#35774;&#35745;&#20102;&#20960;&#31181;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#20351;&#29992;&#30340;&#24120;&#35265;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;&#36895;&#24230;&#19982;&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#65289;&#12290;&#25105;&#20204;&#36824;&#23545;T&#20869;&#37096;&#27599;&#20010;&#32452;&#20214;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09039v2 Announce Type: replace-cross  Abstract: Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and semi-structured tabular data. However, previous table reasoning solutions only consider small-sized tables and exhibit limitations in handling larger tables. In addition, most existing methods struggle to reason over complex questions since they lack essential information or they are scattered in different places. To alleviate these challenges, we propose TAP4LLM as a versatile pre-processing toolbox to generate table prompts through (1) table sampling, (2) table augmentation, and (3) table packing while balancing the token allocation trade-off. In each module, we collect and design several common methods for usage in various scenarios (e.g., speed over accuracy). We also provide a comprehensive evaluation on performance of each components inside T
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10248</link><description>&lt;p&gt;
&#28608;&#27963;&#28155;&#21152;: &#26080;&#38656;&#20248;&#21270;&#21363;&#21487;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#22320;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#26377;&#30417;&#30563;&#24494;&#35843;&#12289;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12289;&#25552;&#31034;&#24037;&#31243;&#21644;&#24341;&#23548;&#35299;&#30721;&#12290;&#25105;&#20204;&#30456;&#21453;&#65292;&#30740;&#31350;&#20102;&#28608;&#27963;&#24037;&#31243;&#65306;&#22312;&#25512;&#29702;&#26102;&#20462;&#25913;&#28608;&#27963;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38544;&#24335;&#25351;&#23450;&#20102;&#19968;&#20010;&#28155;&#21152;&#30340;&#8220;&#23548;&#21521;&#21521;&#37327;&#8221;&#26469;&#20559;&#32622;&#21069;&#21521;&#20256;&#25773;&#12290;&#19982;&#20197;&#21069;&#23398;&#20064;&#36825;&#20123;&#23548;&#21521;&#21521;&#37327;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#28608;&#27963;&#28155;&#21152;&#65288;ActAdd&#65289;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#26469;&#33258;&#25552;&#31034;&#23545;&#30340;&#28608;&#27963;&#24046;&#24322;&#26469;&#35745;&#31639;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;OpenWebText&#21644;ConceptNet&#19978;&#23637;&#31034;&#20102;ActAdd&#22312;GPT-2&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25512;&#29702;&#26102;&#26041;&#27861;&#25511;&#21046;&#20102;&#36755;&#20986;&#30340;&#39640;&#32423;&#23646;&#24615;&#24182;&#20445;&#25345;&#20102;&#38750;&#30446;&#26631;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23427;&#25152;&#38656;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#24037;&#20316;&#27604;&#24494;&#35843;&#35201;&#23569;&#24471;&#22810;&#65292;&#20801;&#35768;&#29992;&#25143;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#30340;&#35268;&#33539;&#65292;&#24182;&#19988;&#20854;&#24320;&#38144;&#19982;&#27169;&#22411;&#35268;&#27169;&#33258;&#28982;&#22320;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.  Unlike past work which learned these steering vectors, our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with m
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#65292;&#20173;&#28982;&#20855;&#26377;&#22270;&#28789;&#23436;&#22791;&#24615;&#65292;&#20854;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.17026</link><description>&lt;p&gt;
&#35770;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Computational Power of Decoder-Only Transformer Language Models. (arXiv:2305.17026v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#65292;&#20173;&#28982;&#20855;&#26377;&#22270;&#28789;&#23436;&#22791;&#24615;&#65292;&#20854;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#23545;&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;Transformer&#27169;&#22411;&#30340;&#29702;&#35770;&#25991;&#29486;&#65292;&#24182;&#34920;&#26126;&#20165;&#20351;&#29992;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#30340;&#35299;&#30721;&#22120;Transformer&#32467;&#26500;&#65292;&#22312;&#21512;&#29702;&#20551;&#35774;&#19979;&#20855;&#22791;&#22270;&#28789;&#23436;&#22791;&#24615;&#12290;&#20174;&#29702;&#35770;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#22270;&#28789;&#23436;&#22791;&#24615;&#25104;&#31435;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a theoretical evaluation of the computational universality of decoder-only transformer models. We extend the theoretical literature on transformer models and show that decoder-only transformer architectures (even with only a single layer and single attention head) are Turing complete under reasonable assumptions. From the theoretical analysis, we show sparsity/compressibility of the word embedding to be a necessary condition for Turing completeness to hold.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#31867;&#22312;&#23398;&#20064;&#21644;&#25512;&#24191;&#19981;&#21516;&#32467;&#26500;&#31243;&#24230;&#30340;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#31995;&#32479;&#21270;&#27010;&#25324;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#23545;&#20110;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#21644;&#36827;&#21270;&#26500;&#25104;&#20102;&#19968;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.12239</link><description>&lt;p&gt;
&#32534;&#31243;&#20160;&#20040;&#20351;&#19968;&#31181;&#35821;&#35328;&#26131;&#20110;&#28145;&#24230;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Makes a Language Easy to Deep-Learn?. (arXiv:2302.12239v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#31867;&#22312;&#23398;&#20064;&#21644;&#25512;&#24191;&#19981;&#21516;&#32467;&#26500;&#31243;&#24230;&#30340;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#31995;&#32479;&#21270;&#27010;&#25324;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#23545;&#20110;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#21644;&#36827;&#21270;&#26500;&#25104;&#20102;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25104;&#21151;&#12290;&#35821;&#35328;&#30340;&#19968;&#20010;&#22522;&#26412;&#23646;&#24615;&#26159;&#20854;&#32452;&#25104;&#32467;&#26500;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#31995;&#32479;&#22320;&#20135;&#29983;&#26032;&#30340;&#24847;&#20041;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#31995;&#32479;&#21270;&#27010;&#25324;&#26041;&#38754;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#26032;&#20852;&#36890;&#20449;&#27169;&#25311;&#20013;&#19981;&#19968;&#23450;&#21463;&#30410;&#20110;&#32452;&#25104;&#32467;&#26500;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#21644;&#36827;&#21270;&#26500;&#25104;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#26263;&#31034;&#20102;&#19981;&#21516;&#23398;&#20064;&#31995;&#32479;&#30340;&#20559;&#35265;&#30340;&#20851;&#38190;&#24046;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30452;&#25509;&#27979;&#35797;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#21644;&#27010;&#25324;&#19981;&#21516;&#36755;&#20837;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#35821;&#35328;&#22312;&#20854;&#32467;&#26500;&#31243;&#24230;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-3.5&#65288;&#31867;&#20284;&#20110;&#25104;&#24180;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#65289;&#21644;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;&#31867;&#20284;&#20110;&#20799;&#31461;&#31532;&#19968;&#35821;&#35328;&#23398;&#20064;&#32773;&#65289;&#30340;&#35760;&#24518;&#21644;&#27010;&#25324;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#20196;&#20154;&#38663;&#24778;&#30340;
&lt;/p&gt;
&lt;p&gt;
Neural networks drive the success of natural language processing. A fundamental property of language is its compositional structure, allowing humans to produce forms for new meanings systematically. However, unlike humans, neural networks notoriously struggle with systematic generalization, and do not necessarily benefit from compositional structure in emergent communication simulations. This poses a problem for using neural networks to simulate human language learning and evolution, and suggests crucial differences in the biases of the different learning systems. Here, we directly test how neural networks compare to humans in learning and generalizing different input languages that vary in their degree of structure. We evaluate the memorization and generalization capabilities of a pre-trained language model GPT-3.5 (analagous to an adult second language learner) and recurrent neural networks trained from scratch (analaogous to a child first language learner). Our results show striking
&lt;/p&gt;</description></item></channel></rss>