<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#35299;&#20026;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#31572;&#26696;&#30340;&#24544;&#23454;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02655</link><description>&lt;p&gt;
&#36890;&#36807;&#35825;&#23548;&#24544;&#23454;&#24615;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Calibrating the Confidence of Large Language Models by Eliciting Fidelity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#35299;&#20026;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#31572;&#26696;&#30340;&#24544;&#23454;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;RLHF&#31561;&#25216;&#26415;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23545;&#40784;&#65292;&#26082;&#26377;&#24110;&#21161;&#24615;&#21448;&#26080;&#23475;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#40784;&#20043;&#21518;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#65292;&#34920;&#36798;&#30340;&#32622;&#20449;&#24230;&#24182;&#19981;&#20934;&#30830;&#22320;&#19982;&#20854;&#27491;&#30830;&#29575;&#26657;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#35299;&#20026;&#20851;&#20110;&#38382;&#39064;&#30340;\textit{&#19981;&#30830;&#23450;&#24615;}&#21644;&#23545;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;\textit{&#24544;&#23454;&#24615;}&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#12290;&#36890;&#36807;&#22312;&#22235;&#20010;MCQA&#25968;&#25454;&#38598;&#19978;&#23545;6&#20010;RLHF-LMs&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;IPR&#21644;CE&#65292;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#23545;\textit{&#30495;&#27491;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;}&#36827;&#34892;&#20102;&#35814;&#32454;&#35752;&#35770;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#22522;&#32447;&#65292;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#25552;&#20379;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02655v1 Announce Type: new  Abstract: Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \textit{Uncertainty} about the question and the \textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \textit{Truly Well-Calibrated Confidence}. Our method could serve as a strong baseline, and we hope that this work will provide some insights into
&lt;/p&gt;</description></item><item><title>UniMEEC&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24773;&#32490;&#35782;&#21035;&#21644;&#24773;&#32490;-&#21407;&#22240;&#20998;&#26512;&#26694;&#26550;&#65292;&#23558;MERC&#21644;MECPE&#37325;&#26032;&#23450;&#20041;&#20026;&#20004;&#20010;&#25513;&#30721;&#39044;&#27979;&#38382;&#39064;&#65292;&#20197;&#22686;&#24378;&#24773;&#32490;&#21644;&#21407;&#22240;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.00403</link><description>&lt;p&gt;
UniMEEC:&#36208;&#21521;&#32479;&#19968;&#30340;&#22810;&#27169;&#24773;&#32490;&#35782;&#21035;&#19982;&#24773;&#32490;&#22240;&#26524;
&lt;/p&gt;
&lt;p&gt;
UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00403
&lt;/p&gt;
&lt;p&gt;
UniMEEC&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24773;&#32490;&#35782;&#21035;&#21644;&#24773;&#32490;-&#21407;&#22240;&#20998;&#26512;&#26694;&#26550;&#65292;&#23558;MERC&#21644;MECPE&#37325;&#26032;&#23450;&#20041;&#20026;&#20004;&#20010;&#25513;&#30721;&#39044;&#27979;&#38382;&#39064;&#65292;&#20197;&#22686;&#24378;&#24773;&#32490;&#21644;&#21407;&#22240;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#35805;&#20013;&#30340;&#22810;&#27169;&#24773;&#32490;&#35782;&#21035;&#65288;MERC&#65289;&#21644;&#22810;&#27169;&#24773;&#32490;-&#21407;&#22240;&#23545;&#25552;&#21462;&#65288;MECPE&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24773;&#32490;&#26159;&#24773;&#24863;&#25110;&#24863;&#21463;&#30340;&#34920;&#36798;&#65307;&#23545;&#29305;&#23450;&#20107;&#20214;&#12289;&#24819;&#27861;&#25110;&#24773;&#20917;&#30340;&#21709;&#24212;&#34987;&#31216;&#20026;&#24773;&#32490;&#21407;&#22240;&#12290;&#23427;&#20204;&#22914;&#21516;&#19968;&#26522;&#30828;&#24065;&#30340;&#20004;&#38754;&#65292;&#20849;&#21516;&#25551;&#36848;&#20102;&#20154;&#31867;&#34892;&#20026;&#21644;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#23558;MERC&#21644;MECPE&#35270;&#20026;&#29420;&#31435;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#25972;&#21512;&#24773;&#32490;&#21644;&#21407;&#22240;&#21040;&#29616;&#23454;&#24212;&#29992;&#20013;&#23384;&#22312;&#28508;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24773;&#32490;&#35782;&#21035;&#21644;&#24773;&#32490;-&#21407;&#22240;&#20998;&#26512;&#26694;&#26550;&#65288;UniMEEC&#65289;&#65292;&#20197;&#25506;&#32034;&#24773;&#32490;&#21644;&#24773;&#32490;&#21407;&#22240;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#20114;&#34917;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;UniMEEC&#23558;MERC&#21644;MECPE&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20004;&#20010;&#25513;&#30721;&#39044;&#27979;&#38382;&#39064;&#65292;&#22686;&#24378;&#20102;&#24773;&#32490;&#21644;&#21407;&#22240;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;UniMEEC&#22312;&#21508;&#27169;&#24577;&#20043;&#38388;&#20849;&#20139;&#36805;&#36895;&#23398;&#20064;&#20197;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00403v1 Announce Type: new  Abstract: Multimodal emotion recognition in conversation (MERC) and multimodal emotion-cause pair extraction (MECPE) has recently garnered significant attention. Emotions are the expression of affect or feelings; responses to specific events, thoughts, or situations are known as emotion causes. Both are like two sides of a coin, collectively describing human behaviors and intents. However, most existing works treat MERC and MECPE as separate tasks, which may result in potential challenges in integrating emotion and cause in real-world applications. In this paper, we propose a Unified Multimodal Emotion recognition and Emotion-Cause analysis framework (UniMEEC) to explore the causality and complementarity between emotion and emotion cause. Concretely, UniMEEC reformulates the MERC and MECPE tasks as two mask prediction problems, enhancing the interaction between emotion and cause. Meanwhile, UniMEEC shares the prompt learning among modalities for p
&lt;/p&gt;</description></item><item><title>&#20174;&#35821;&#35328;&#36827;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#26032;&#20852;&#27807;&#36890;&#25991;&#29486;&#65292;&#21457;&#29616;&#20854;&#22312;&#35774;&#35745;&#21644;&#35843;&#25972;&#27169;&#22411;&#20197;&#24674;&#22797;&#33258;&#28982;&#35821;&#35328;&#20013;&#21021;&#22987;&#32570;&#22833;&#30340;&#35821;&#35328;&#29616;&#35937;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#25581;&#31034;&#20102;&#20851;&#38190;&#21387;&#21147;&#20419;&#20351;&#24674;&#22797;&#26368;&#21021;&#19981;&#26174;&#29616;&#30340;&#20154;&#31867;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.14427</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32039;&#24613;&#27807;&#36890;&#21644;&#23398;&#20064;&#21387;&#21147;&#65306;&#35821;&#35328;&#36827;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Emergent communication and learning pressures in language models: a language evolution perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14427
&lt;/p&gt;
&lt;p&gt;
&#20174;&#35821;&#35328;&#36827;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#26032;&#20852;&#27807;&#36890;&#25991;&#29486;&#65292;&#21457;&#29616;&#20854;&#22312;&#35774;&#35745;&#21644;&#35843;&#25972;&#27169;&#22411;&#20197;&#24674;&#22797;&#33258;&#28982;&#35821;&#35328;&#20013;&#21021;&#22987;&#32570;&#22833;&#30340;&#35821;&#35328;&#29616;&#35937;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#25581;&#31034;&#20102;&#20851;&#38190;&#21387;&#21147;&#20419;&#20351;&#24674;&#22797;&#26368;&#21021;&#19981;&#26174;&#29616;&#30340;&#20154;&#31867;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#26159;&#20004;&#31181;&#23398;&#20064;&#31995;&#32479;&#12290;&#21457;&#29616;&#25110;&#20419;&#36827;&#20108;&#32773;&#20043;&#38388;&#30340;&#20849;&#21516;&#28857;&#21487;&#33021;&#20250;&#22312;&#25105;&#20204;&#29702;&#35299;&#35821;&#35328;&#30340;&#20064;&#24471;&#21644;&#28436;&#21270;&#26041;&#38754;&#21462;&#24471;&#37325;&#22823;&#31361;&#30772;&#12290;&#35768;&#22810;&#35821;&#35328;&#36827;&#21270;&#29702;&#35770;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#23398;&#20064;&#20559;&#22909;&#21644;&#23398;&#20064;&#21387;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23398;&#20064;&#21387;&#21147;&#23384;&#22312;&#30528;&#37325;&#22823;&#24046;&#24322;&#65292;&#23545;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#21542;&#36275;&#20197;&#21551;&#21457;&#27934;&#35265;&#24182;&#20540;&#24471;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#19968;&#36215;&#36827;&#34892;&#27979;&#35797;&#26159;&#20540;&#24471;&#24576;&#30097;&#30340;&#12290;&#26412;&#25991;&#20174;&#35821;&#35328;&#36827;&#21270;&#30340;&#35282;&#24230;&#23457;&#35270;&#20102;&#26032;&#20852;&#27807;&#36890;&#25991;&#29486;&#65292;&#36825;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26032;&#20852;&#27807;&#36890;&#25991;&#29486;&#22312;&#35774;&#35745;&#21644;&#35843;&#25972;&#27169;&#22411;&#20197;&#24674;&#22797;&#33258;&#28982;&#35821;&#35328;&#30340;&#26368;&#21021;&#19981;&#26174;&#29616;&#30340;&#35821;&#35328;&#29616;&#35937;&#26041;&#38754;&#26377;&#26480;&#20986;&#34920;&#29616;&#12290;&#26681;&#25454;&#23545;&#25991;&#29486;&#30340;&#31616;&#35201;&#22238;&#39038;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#22312;&#26032;&#20852;&#27807;&#36890;&#20013;&#24674;&#22797;&#26368;&#21021;&#19981;&#26174;&#29616;&#30340;&#20154;&#31867;&#27169;&#24335;&#30340;&#20851;&#38190;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14427v1 Announce Type: new  Abstract: Language models and humans are two types of learning systems. Finding or facilitating commonalities could enable major breakthroughs in our understanding of the acquisition and evolution of language. Many theories of language evolution rely heavily on learning biases and learning pressures. Yet due to substantial differences in learning pressures, it is questionable whether the similarity between humans and machines is sufficient for insights to carry over and to be worth testing with human participants. Here, we review the emergent communication literature, a subfield of multi-agent reinforcement learning, from a language evolution perspective. We find that the emergent communication literature excels at designing and adapting models to recover initially absent linguistic phenomena of natural languages. Based on a short literature review, we identify key pressures that have recovered initially absent human patterns in emergent communica
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26102;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#26681;&#25454;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#26469;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2403.09559</link><description>&lt;p&gt;
&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#23545;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Less is More: Data Value Estimation for Visual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09559
&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26102;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#26681;&#25454;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#26469;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26159;&#26500;&#24314;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLLMs&#20027;&#35201;&#20381;&#36182;&#20110;&#22810;&#20010;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#28151;&#21512;&#35757;&#32451;&#65288;&#29978;&#33267;&#36229;&#36807;&#19968;&#30334;&#19975;&#26465;&#25351;&#23548;&#65289;&#65292;&#36825;&#21487;&#33021;&#24341;&#20837;&#25968;&#25454;&#20887;&#20313;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#20869;&#23384;&#22312;&#26174;&#33879;&#20887;&#20313;&#65292;&#24182;&#26174;&#31034;&#22823;&#22823;&#20943;&#23569;&#20960;&#20010;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#29978;&#33267;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#20197;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;TIVE&#39318;&#20808;&#26681;&#25454;&#35745;&#31639;&#30340;&#26799;&#24230;&#20272;&#35745;&#35270;&#35273;&#25351;&#23548;&#30340;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#20272;&#35745;&#30340;&#20215;&#20540;&#65292;TIVE&#30830;&#23450;&#20102;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#25351;&#23548;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09559v1 Announce Type: new  Abstract: Visual instruction tuning is the key to building multimodal large language models (MLLMs), which greatly improves the reasoning capabilities of large language models (LLMs) in vision scenario. However, existing MLLMs mostly rely on a mixture of multiple highly diverse visual instruction datasets for training (even more than a million instructions), which may introduce data redundancy. To investigate this issue, we conduct a series of empirical studies, which reveal a significant redundancy within the visual instruction datasets, and show that greatly reducing the amount of several instruction dataset even do not affect the performance. Based on the findings, we propose a new data selection approach TIVE, to eliminate redundancy within visual instruction data. TIVE first estimates the task-level and instance-level value of the visual instructions based on computed gradients. Then, according to the estimated values, TIVE determines the tas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#36755;&#20837;&#32763;&#35793;&#20026;&#22810;&#31181;&#35821;&#35328;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22810;&#35821;&#35328;&#24179;&#34892;&#36755;&#20837;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#35328;&#36755;&#20837;&#21487;&#20197;&#36229;&#36234;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#21453;&#30452;&#35273;&#29616;&#35937;</title><link>https://arxiv.org/abs/2403.09073</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24182;&#34892;&#22810;&#35821;&#35328;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Parallel Multilingual Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09073
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36755;&#20837;&#32763;&#35793;&#20026;&#22810;&#31181;&#35821;&#35328;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22810;&#35821;&#35328;&#24179;&#34892;&#36755;&#20837;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#35328;&#36755;&#20837;&#21487;&#20197;&#36229;&#36234;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#21453;&#30452;&#35273;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65306;&#36890;&#36807;&#23558;&#36755;&#20837;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#25105;&#20204;&#20026;LLMs&#25552;&#20379;&#20102;&#22810;&#35821;&#35328;&#24179;&#34892;&#36755;&#20837;&#65288;PiM&#65289;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#27979;&#35797;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21253;&#25324;8&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#12289;7&#31181;&#35821;&#35328;&#21644;8&#31181;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;LLMs&#22312;&#20869;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#32467;&#26524;&#26174;&#31034;&#65292;&#65288;1&#65289;&#25972;&#21512;&#26356;&#22810;&#35821;&#35328;&#21487;&#20197;&#24110;&#21161;PiM&#36827;&#19968;&#27493;&#36229;&#36234;&#20256;&#32479;&#30340;ICL&#65307;&#65288;2&#65289;&#21363;&#20351;&#19982;&#22522;&#20934;&#24615;&#33021;&#20302;&#21155;&#30340;&#32763;&#35793;&#32467;&#21512;&#20063;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26816;&#26597;LLMs&#20013;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24847;&#22806;&#20294;&#26377;&#36259;&#30340;&#29616;&#35937;&#12290;&#19982;&#24120;&#35265;&#35266;&#28857;&#30456;&#21453;&#65292;PiM&#24182;&#19981;&#20250;&#28608;&#27963;&#27604;&#21333;&#35821;&#36755;&#20837;&#26356;&#22810;&#30340;&#31070;&#32463;&#20803;&#26469;&#21033;&#29992;&#20174;&#22810;&#31181;&#35821;&#35328;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#65292;&#32780;&#23454;&#38469;&#19978;&#26159;&#25233;&#21046;&#31070;&#32463;&#20803;&#24182;&#20419;&#36827;&#26356;&#31934;&#30830;&#30340;&#31070;&#32463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09073v1 Announce Type: new  Abstract: In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs. Experimental results show that (1) incorporating more languages help PiM surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#24314;&#20102;ROME&#65292;&#25552;&#20379;&#20102;&#26356;&#31283;&#23450;&#30340;r-ROME&#23454;&#29616;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07175</link><description>&lt;p&gt;
&#37325;&#24314;ROME: &#35299;&#20915;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#24314;&#20102;ROME&#65292;&#25552;&#20379;&#20102;&#26356;&#31283;&#23450;&#30340;r-ROME&#23454;&#29616;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;Rank-One Model Editing (ROME)&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26377;&#19968;&#20123;&#20107;&#23454;&#34920;&#26126;&#35813;&#31639;&#27861;&#26080;&#27861;&#36827;&#34892;&#32534;&#36753;&#32780;&#19981;&#30772;&#22351;&#27169;&#22411;&#12290;&#36825;&#20123;&#32534;&#36753;&#20197;&#21069;&#34987;&#31216;&#20026;&#31105;&#29992;&#32534;&#36753;&#12290;&#36825;&#20123;&#31105;&#29992;&#32534;&#36753;&#20250;&#23548;&#33268;&#31435;&#21363;&#27169;&#22411;&#23849;&#28291;&#65292;&#24182;&#38480;&#21046;&#20102;ROME&#29992;&#20110;&#39034;&#24207;&#32534;&#36753;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20351;&#29992;CounterFact&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#36753;&#26102;&#65292;ROME&#20165;&#22312;&#27492;&#26102;&#21457;&#29983;&#27169;&#22411;&#23849;&#28291;&#65292;&#24182;&#22312;&#20351;&#29992;zsRE&#25968;&#25454;&#38598;&#26102;&#19981;&#20250;&#21457;&#29983;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#31105;&#29992;&#32534;&#36753;&#26159;ROME&#21407;&#22987;&#23454;&#29616;&#30340;&#20135;&#29289;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#31283;&#23450;&#30340;&#23454;&#29616;ROME&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;r-ROME&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#22312;&#20351;&#29992;ROME&#36827;&#34892;&#22823;&#35268;&#27169;&#39034;&#24207;&#32534;&#36753;&#26102;&#19981;&#20877;&#35266;&#23519;&#21040;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07175v1 Announce Type: cross  Abstract: Recent work on model editing using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we make two main contributions. Firstly, we show that model collapse with ROME only happens when making edits using the CounterFact dataset and does not happen when using the zsRE dataset. Secondly, we find that disabling edits are an artifact of the original implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that we no longer observe model collapse when making large scale sequential edits with ROME.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02966</link><description>&lt;p&gt;
&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#29992;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;-shot&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#24615;&#33021;&#65292;&#28982;&#32780;&#32467;&#26500;&#21270;&#30340;KG&#24418;&#24335;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#19977;&#20803;&#32452;&#24418;&#24335;&#25110;&#19977;&#20803;&#32452;&#20107;&#23454;&#30340;&#33258;&#30001;&#25991;&#26412;&#36716;&#25442;&#65292;&#36935;&#21040;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#30001;&#20110;&#37325;&#22797;&#23454;&#20307;&#25110;&#20851;&#31995;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#23494;&#24230;&#38477;&#20302;&#65292;&#20197;&#21450;&#30001;&#20110;&#26080;&#27861;&#24378;&#35843;&#20851;&#38190;&#35777;&#25454;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#28165;&#26224;&#24230;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EFSum&#65292;&#19968;&#20010;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;LLMs&#22686;&#24378;QA&#12290;&#25105;&#20204;&#36890;&#36807;&#33976;&#39311;&#21644;&#20559;&#22909;&#23545;&#40784;&#26469;&#20248;&#21270;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#20316;&#20026;&#20107;&#23454;&#25688;&#35201;&#22120;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;EFSum&#25552;&#39640;&#20102;LLM&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#30830;&#20445;&#25688;&#35201;&#30340;&#21516;&#26102;&#26377;&#30410;&#21644;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
&lt;/p&gt;</description></item><item><title>&#25513;&#30422;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#31181;&#35821;&#35328;&#20013;&#30340;&#23569;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#32988;&#36807;LLM&#25552;&#31034;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.12801</link><description>&lt;p&gt;
&#19977;&#31181;&#35821;&#35328;&#20013;&#30340;&#23569;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#65306;&#25513;&#30422;&#35821;&#35328;&#27169;&#22411;&#32988;&#36807;LLM&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12801
&lt;/p&gt;
&lt;p&gt;
&#25513;&#30422;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#31181;&#35821;&#35328;&#20013;&#30340;&#23569;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#32988;&#36807;LLM&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#20154;&#20204;&#26399;&#26395;&#23427;&#20204;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#33021;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;8&#20010;&#39046;&#22495;&#20869;&#65288;&#20020;&#24202;&#65289;&#21644;6&#20010;&#39046;&#22495;&#22806;&#30340;&#40644;&#37329;&#26631;&#20934;&#35821;&#26009;&#24211;&#65292;&#35780;&#20272;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;10&#20010;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;16&#20010;&#29992;&#20110;&#25991;&#26412;&#32534;&#30721;&#30340;&#25513;&#30422;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;BiLSTM-CRF&#30417;&#30563;&#26631;&#27880;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#38480;&#21046;&#21487;&#29992;&#30340;&#24102;&#26631;&#27880;&#25968;&#25454;&#37327;&#20026;100&#20010;&#21477;&#23376;&#26469;&#21019;&#24314;&#19968;&#20010;&#23569;&#26679;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#26356;&#22823;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#24448;&#24448;&#22312;&#20020;&#24202;&#39046;&#22495;&#20043;&#22806;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;F-measure&#65292;&#20294;&#36825;&#31181;&#24615;&#33021;&#27700;&#24179;&#24182;&#26410;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12801v1 Announce Type: new  Abstract: Large Language Models are becoming the go-to solution for many natural language processing tasks, including in specialized domains where their few-shot capacities are expected to yield high performance in low-resource settings. Herein, we aim to assess the performance of Large Language Models for few shot clinical entity recognition in multiple languages. We evaluate named entity recognition in English, French and Spanish using 8 in-domain (clinical) and 6 out-domain gold standard corpora. We assess the performance of 10 auto-regressive language models using prompting and 16 masked language models used for text encoding in a biLSTM-CRF supervised tagger. We create a few-shot set-up by limiting the amount of annotated data available to 100 sentences. Our experiments show that although larger prompt-based models tend to achieve competitive F-measure for named entity recognition outside the clinical domain, this level of performance does no
&lt;/p&gt;</description></item><item><title>FormulaQA&#26159;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#20197;&#21450;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#24212;&#23545;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#30340;&#28508;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.12692</link><description>&lt;p&gt;
FormulaQA&#65306;&#19968;&#20010;&#22522;&#20110;&#20844;&#24335;&#30340;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12692
&lt;/p&gt;
&lt;p&gt;
FormulaQA&#26159;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#20197;&#21450;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#24212;&#23545;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#30340;&#28508;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#20844;&#24335;&#26159;&#20154;&#31867;&#22312;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#20540;&#25512;&#29702;&#25968;&#25454;&#38598;&#24456;&#23569;&#26126;&#30830;&#25351;&#20986;&#25512;&#29702;&#27493;&#39588;&#20013;&#20351;&#29992;&#30340;&#20844;&#24335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;FormulaQA&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22823;&#23567;&#20174;7B&#21040;&#36229;&#36807;100B&#21442;&#25968;&#30340;LLMs&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24605;&#32500;&#38142;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#24182;&#25506;&#32034;&#20102;&#22312;&#25552;&#20379;&#22806;&#37096;&#20844;&#24335;&#25968;&#25454;&#24211;&#26102;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23545;&#22823;&#23567;&#19981;&#36229;&#36807;2B&#30340;&#36739;&#23567;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#24378;&#35843;&#20102;&#24403;&#24212;&#29992;&#20110;&#25105;&#20204;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#25913;&#36827;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12692v1 Announce Type: new  Abstract: The application of formulas is a fundamental ability of humans when addressing numerical reasoning problems. However, existing numerical reasoning datasets seldom explicitly indicate the formulas employed during the reasoning steps. To bridge this gap, we propose a question answering dataset for formula-based numerical reasoning called FormulaQA, from junior high school physics examinations. We further conduct evaluations on LLMs with size ranging from 7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thoughts methods and we explored the approach of using retrieval-augmented LLMs when providing an external formula database. We also fine-tune on smaller models with size not exceeding 2B. Our empirical findings underscore the significant potential for improvement in existing models when applied to our complex, formula-driven FormulaQA.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLMs&#33258;&#21160;&#35780;&#20272;&#24515;&#29702;&#21672;&#35810;&#23545;&#35805;&#20013;&#30340;&#24037;&#20316;&#32852;&#30431;&#65292;&#32467;&#26524;&#26174;&#31034;&#19982;&#20154;&#24037;&#35780;&#20272;&#39640;&#24230;&#19968;&#33268;&#65292;&#24182;&#25552;&#20379;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.11958</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#33258;&#21160;&#35780;&#20272;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;
&lt;/p&gt;
&lt;p&gt;
Automatic Evaluation for Mental Health Counseling using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11958
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLMs&#33258;&#21160;&#35780;&#20272;&#24515;&#29702;&#21672;&#35810;&#23545;&#35805;&#20013;&#30340;&#24037;&#20316;&#32852;&#30431;&#65292;&#32467;&#26524;&#26174;&#31034;&#19982;&#20154;&#24037;&#35780;&#20272;&#39640;&#24230;&#19968;&#33268;&#65292;&#24182;&#25552;&#20379;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#24515;&#29702;&#21672;&#35810;&#23545;&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#65292;&#21450;&#26102;&#35780;&#20272;&#23545;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#21672;&#35810;&#20250;&#35805;&#33719;&#21462;&#19987;&#19994;&#35780;&#20272;&#26082;&#26114;&#36149;&#21448;&#20855;&#25361;&#25112;&#24615;&#12290;&#20381;&#36182;&#33258;&#25105;&#25110;&#31532;&#19977;&#26041;&#25163;&#21160;&#25253;&#21578;&#26469;&#35780;&#20272;&#21672;&#35810;&#36136;&#37327;&#30340;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#20027;&#35266;&#20559;&#35265;&#21644;&#32791;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#39640;&#25928;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35780;&#20272;&#21672;&#35810;&#23545;&#35805;&#20013;&#30340;&#24037;&#20316;&#32852;&#30431;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#21672;&#35810;&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#27835;&#30103;&#20851;&#31995;&#29702;&#35770;&#36827;&#34892;&#20102;&#22810;&#26041;&#35780;&#20272;&#12290;&#25105;&#20204;&#22522;&#20110;LLMs&#30340;&#35780;&#20272;&#32467;&#21512;&#25105;&#20204;&#30340;&#25351;&#21335;&#65292;&#19982;&#20154;&#24037;&#35780;&#20272;&#39640;&#24230;&#19968;&#33268;&#65292;&#24182;&#20026;&#21672;&#35810;&#33050;&#26412;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#36825;&#31361;&#26174;&#20102;LLMs&#20316;&#20026;&#30417;&#30563;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11958v1 Announce Type: new  Abstract: High-quality psychological counseling is crucial for mental health worldwide, and timely evaluation is vital for ensuring its effectiveness. However, obtaining professional evaluation for each counseling session is expensive and challenging. Existing methods that rely on self or third-party manual reports to assess the quality of counseling suffer from subjective biases and limitations of time-consuming.   To address above challenges, this paper proposes an innovative and efficient automatic approach using large language models (LLMs) to evaluate the working alliance in counseling conversations. We collected a comprehensive counseling dataset and conducted multiple third-party evaluations based on therapeutic relationship theory. Our LLM-based evaluation, combined with our guidelines, shows high agreement with human evaluations and provides valuable insights into counseling scripts. This highlights the potential of LLMs as supervisory to
&lt;/p&gt;</description></item><item><title>&#20998;&#38548;&#31526;&#30340;&#24341;&#20837;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.10645</link><description>&lt;p&gt;
&#20998;&#38548;&#31526;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#25928;&#26524;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Separators Improve Chain-of-Thought Prompting?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10645
&lt;/p&gt;
&lt;p&gt;
&#20998;&#38548;&#31526;&#30340;&#24341;&#20837;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;CoT&#30340;&#22522;&#26412;&#29702;&#24565;&#26159;&#36890;&#36807;&#23558;&#31034;&#20363;&#25918;&#22312;&#36755;&#20837;&#25552;&#31034;&#20013;&#65292;&#35753;LLMs&#36880;&#27493;&#25286;&#35299;&#20182;&#20204;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;CoT&#25552;&#31034;&#30340;&#23494;&#38598;&#32467;&#26500;&#21487;&#33021;&#23548;&#33268;LLMs&#30340;&#35748;&#30693;&#36127;&#33655;&#36807;&#37325;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoT-Sep&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;CoT&#25552;&#31034;&#20013;&#27599;&#20010;&#31034;&#20363;&#30340;&#26411;&#23614;&#31574;&#30053;&#24615;&#22320;&#24212;&#29992;&#20998;&#38548;&#31526;&#12290;&#36825;&#20123;&#20998;&#38548;&#31526;&#26088;&#22312;&#24110;&#21161;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#29702;&#35299;&#20182;&#20204;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#19981;&#20351;&#29992;&#20998;&#38548;&#31526;&#30340;&#26222;&#36890;CoT&#30456;&#27604;&#65292;CoT-Sep&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;GSM-8K&#12289;AQuA&#12289;CSQA&#65289;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#20998;&#38548;&#31526;&#23545;&#22810;&#20010;LLMs&#65288;&#21253;&#25324;GPT-3.5-Turbo&#12289;GPT-4&#21644;LLaMA-27&#65289;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10645v1 Announce Type: cross  Abstract: Chain-of-thought (CoT) prompting is a simple and effective method for improving the reasoning capabilities of Large language models (LLMs). The basic idea of CoT is to let LLMs break down their thought processes step-by-step by putting exemplars in the input prompt. However, the densely structured prompt exemplars of CoT may cause the cognitive overload of LLMs. Inspired by human cognition, we introduce CoT-Sep, a novel method that strategically employs separators at the end of each exemplar in CoT prompting. These separators are designed to help the LLMs understand their thought processes better while reasoning. It turns out that CoT-Sep significantly improves the LLMs' performances on complex reasoning tasks (e.g., GSM-8K, AQuA, CSQA), compared with the vanilla CoT, which does not use separators. We also study the effects of the type and the location of separators tested on multiple LLMs, including GPT-3.5-Turbo, GPT-4, and LLaMA-2 7
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TEAROOM&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#22522;&#20110;&#26641;&#29366;&#30828;&#27880;&#24847;&#21147;&#21644;&#33258;&#25105;&#28608;&#21169;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20998;&#23618;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#26426;&#21046;&#20351;&#27169;&#22411;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#19982;&#29305;&#23450;&#20219;&#21153;&#30456;&#20851;&#30340;&#21494;&#23376;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.08874</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#29366;&#30828;&#27880;&#24847;&#21147;&#21644;&#33258;&#25105;&#28608;&#21169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tree-Based Hard Attention with Self-Motivation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08874
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TEAROOM&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#22522;&#20110;&#26641;&#29366;&#30828;&#27880;&#24847;&#21147;&#21644;&#33258;&#25105;&#28608;&#21169;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20998;&#23618;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#26426;&#21046;&#20351;&#27169;&#22411;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#19982;&#29305;&#23450;&#20219;&#21153;&#30456;&#20851;&#30340;&#21494;&#23376;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#32431;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#19987;&#38376;&#35774;&#35745;&#26469;&#22788;&#29702;&#20998;&#23618;&#25991;&#26412;&#32467;&#26500;&#12290;&#20174;&#23427;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#22238;&#22797;&#20013;&#25552;&#21462;&#20219;&#21153;&#25152;&#38656;&#30340;&#23646;&#24615;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#22788;&#29702;&#27493;&#39588;&#12290;&#20107;&#23454;&#19978;&#65292;&#36873;&#25321;&#24615;&#22320;&#29702;&#35299;&#22823;&#35268;&#27169;&#25991;&#26412;&#30340;&#23618;&#27425;&#32467;&#26500;&#23545;&#20110;&#29702;&#35299;&#20854;&#23454;&#36136;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#25552;&#31034;&#23558;LLM&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;&#20998;&#31867;&#25110;&#22238;&#24402;&#20540;&#26356;&#32039;&#23494;&#22320;&#23545;&#40784;&#20063;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Tree-Based Hard Attention with Self-Motivation for Large Language Models&#65288;TEAROOM&#65289;&#12290;TEAROOM&#23558;&#26641;&#29366;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#32435;&#20837;LLM&#20013;&#65292;&#20197;&#22788;&#29702;&#20998;&#23618;&#32467;&#26500;&#30340;&#25991;&#26412;&#36755;&#20837;&#12290;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#26426;&#21046;&#65292;&#23427;&#20351;&#20923;&#32467;&#30340;LLM&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#19982;&#26681;&#33410;&#28857;&#30456;&#20851;&#30340;&#21494;&#23376;&#33410;&#28857;&#65292;&#29983;&#25104;&#19968;&#20010;&#23450;&#21046;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08874v1 Announce Type: new Abstract: While large language models (LLMs) excel at understanding and generating plain text, they are not specifically tailored to handle hierarchical text structures. Extracting the task-desired property from their natural language responses typically necessitates additional processing steps. In fact, selectively comprehending the hierarchical structure of large-scale text is pivotal to understanding its substance. Aligning LLMs more closely with the classification or regression values of specific task through prompting also remains challenging. To this end, we propose a novel framework called Tree-Based Hard Attention with Self-Motivation for Large Language Models (TEAROOM). TEAROOM incorporates a tree-based hard attention mechanism for LLMs to process hierarchically structured text inputs. By leveraging prompting, it enables a frozen LLM to selectively focus on relevant leaves in relation to the root, generating a tailored symbolic representat
&lt;/p&gt;</description></item><item><title>&#22312;LLMs&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#32508;&#21512;&#30740;&#31350;&#20102;&#21508;&#31181;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#21644;&#35299;&#30721;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#20219;&#21153;&#30456;&#20851;&#65292;&#21463;&#21040;&#23545;&#40784;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#37327;&#21270;&#31561;&#22240;&#32032;&#24433;&#21709;&#65307;&#26576;&#20123;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26435;&#34913;&#21462;&#33293;&#12290;</title><link>https://arxiv.org/abs/2402.06925</link><description>&lt;p&gt;
LLM&#26102;&#20195;&#35299;&#30721;&#26041;&#27861;&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Thorough Examination of Decoding Methods in the Era of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06925
&lt;/p&gt;
&lt;p&gt;
&#22312;LLMs&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#32508;&#21512;&#30740;&#31350;&#20102;&#21508;&#31181;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#21644;&#35299;&#30721;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#20219;&#21153;&#30456;&#20851;&#65292;&#21463;&#21040;&#23545;&#40784;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#37327;&#21270;&#31561;&#22240;&#32032;&#24433;&#21709;&#65307;&#26576;&#20123;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26435;&#34913;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#26041;&#27861;&#22312;&#23558;&#35821;&#35328;&#27169;&#22411;&#20174;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#36716;&#25442;&#20026;&#23454;&#38469;&#20219;&#21153;&#35299;&#20915;&#22120;&#20013;&#36215;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#20316;&#29992;&#12290;&#20197;&#24448;&#20851;&#20110;&#35299;&#30721;&#26041;&#27861;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#19978;&#65292;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#24403;&#21069;&#36890;&#29992;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26102;&#20195;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#35299;&#30721;&#31574;&#30053;&#30340;&#28044;&#20837;&#36827;&#19968;&#27493;&#22797;&#26434;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#22312;LLMs&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#21508;&#31181;&#35299;&#30721;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#32780;&#22810;&#26041;&#20301;&#30340;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#37096;&#32626;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12289;&#23545;&#36229;&#21442;&#25968;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#35299;&#30721;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#26126;&#26174;&#19982;&#20219;&#21153;&#30456;&#20851;&#65292;&#24182;&#21463;&#21040;&#23545;&#40784;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#37327;&#21270;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25935;&#24863;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#26576;&#20123;&#26041;&#27861;&#22312;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#21069;&#25552;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#31361;&#20986;&#20102;&#22312;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining opt
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20462;&#25913;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#38480;&#21046;&#21069;K&#20010;softmax&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#33021;&#22815;&#26377;&#25928;&#25269;&#24481;&#24120;&#35265;&#30340;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.03627</link><description>&lt;p&gt;
&#36817;&#20284;&#30340;&#20013;&#24515;&#21270;softmax&#25439;&#22833;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Partially Recentralization Softmax Loss for Vision-Language Models Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20462;&#25913;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#38480;&#21046;&#21069;K&#20010;softmax&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#33021;&#22815;&#26377;&#25928;&#25269;&#24481;&#24120;&#35265;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#31361;&#30772;&#65292;&#22810;&#27169;&#24577;&#25216;&#26415;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#21363;&#27169;&#22411;&#30340;&#36755;&#20986;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#32780;&#21457;&#29983;&#24040;&#22823;&#21464;&#21270;&#12290;&#34429;&#28982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#38450;&#24481;&#25216;&#26415;&#65292;&#20294;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36824;&#27809;&#26377;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#20462;&#25913;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38480;&#21046;&#21069;K&#20010;softmax&#36755;&#20986;&#26469;&#25552;&#20379;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#35780;&#20272;&#21644;&#35780;&#20998;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#65292;&#23545;&#25239;&#24120;&#35265;&#30340;&#25915;&#20987;&#26377;&#25928;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#24212;&#35813;&#25506;&#32034;&#36825;&#31867;&#25439;&#22833;&#20989;&#25968;&#30340;&#36755;&#20986;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;&#20043;&#21518;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models make a breakthrough in natural language processing tasks (NLP), multimodal technique becomes extremely popular. However, it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input. While several defense techniques have been proposed both in computer vision and NLP models, the multimodal robustness of models have not been fully explored. In this paper, we study the adversarial robustness provided by modifying loss function of pre-trained multimodal models, by restricting top K softmax outputs. Based on the evaluation and scoring, our experiments show that after a fine-tuning, adversarial robustness of pre-trained models can be significantly improved, against popular attacks. Further research should be studying, such as output diversity, generalization and the robustness-performance trade-off of this kind of loss functions. Our code will be available after th
&lt;/p&gt;</description></item><item><title>SWAG&#26159;&#19968;&#31181;&#26032;&#30340;&#25925;&#20107;&#35762;&#36848;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25925;&#20107;&#20889;&#20316;&#31616;&#21270;&#20026;&#25628;&#32034;&#38382;&#39064;&#65292;&#20351;&#29992;&#20004;&#20010;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#26469;&#25351;&#23548;&#25925;&#20107;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#22312;GPT-4&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;SWAG&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#20351;&#29992;&#20165;&#24320;&#28304;&#27169;&#22411;&#30340;SWAG&#27969;&#31243;&#36229;&#36807;&#20102;GPT-3.5-Turbo&#12290;</title><link>https://arxiv.org/abs/2402.03483</link><description>&lt;p&gt;
SWAG: &#24102;&#26377;&#34892;&#21160;&#25351;&#23548;&#30340;&#25925;&#20107;&#35762;&#36848;
&lt;/p&gt;
&lt;p&gt;
SWAG: Storytelling With Action Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03483
&lt;/p&gt;
&lt;p&gt;
SWAG&#26159;&#19968;&#31181;&#26032;&#30340;&#25925;&#20107;&#35762;&#36848;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25925;&#20107;&#20889;&#20316;&#31616;&#21270;&#20026;&#25628;&#32034;&#38382;&#39064;&#65292;&#20351;&#29992;&#20004;&#20010;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#26469;&#25351;&#23548;&#25925;&#20107;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#22312;GPT-4&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;SWAG&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#20351;&#29992;&#20165;&#24320;&#28304;&#27169;&#22411;&#30340;SWAG&#27969;&#31243;&#36229;&#36807;&#20102;GPT-3.5-Turbo&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#38271;&#31687;&#25925;&#20107;&#29983;&#25104;&#36890;&#24120;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19968;&#27425;&#24615;&#21019;&#24314;&#65292;&#23427;&#21487;&#20197;&#20135;&#29983;&#36830;&#36143;&#20294;&#19981;&#19968;&#23450;&#24341;&#20154;&#20837;&#32988;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#26377;&#34892;&#21160;&#25351;&#23548;&#30340;&#25925;&#20107;&#35762;&#36848;&#65288;SWAG&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#23558;&#25925;&#20107;&#20889;&#20316;&#31616;&#21270;&#20026;&#19968;&#20010;&#25628;&#32034;&#38382;&#39064;&#65306;&#19968;&#20010;LLM&#29983;&#25104;&#25925;&#20107;&#20869;&#23481;&#65292;&#21478;&#19968;&#20010;&#36741;&#21161;LLM&#29992;&#20110;&#36873;&#25321;&#19979;&#19968;&#20010;&#26368;&#20339;&#30340;&#8220;&#34892;&#21160;&#8221;&#65292;&#20197;&#24341;&#23548;&#25925;&#20107;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;GPT-4&#21644;&#20154;&#24037;&#35780;&#20272;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;SWAG&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#30340;&#31471;&#21040;&#31471;&#25925;&#20107;&#29983;&#25104;&#25216;&#26415;&#65292;&#24182;&#19988;&#25105;&#20204;&#21482;&#20351;&#29992;&#24320;&#28304;&#27169;&#22411;&#30340;SWAG&#27969;&#31243;&#36229;&#36234;&#20102;GPT-3.5-Turbo&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated long-form story generation typically employs long-context large language models (LLMs) for one-shot creation, which can produce cohesive but not necessarily engaging content. We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach reduces story writing to a search problem through a two-model feedback loop: one LLM generates story content, and another auxiliary LLM is used to choose the next best "action" to steer the story's future direction. Our results show that SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by GPT-4 and through human evaluation, and our SWAG pipeline using only open-source models surpasses GPT-3.5-Turbo.
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#20174;&#20559;&#22909;&#27604;&#36739;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#26469;&#23545;&#30446;&#26631;&#32467;&#26524;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#31639;&#27861;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01920</link><description>&lt;p&gt;
&#23545;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#30340;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Preference Poisoning Attacks on Reward Model Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01920
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20174;&#20559;&#22909;&#27604;&#36739;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#26469;&#23545;&#30446;&#26631;&#32467;&#26524;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#31639;&#27861;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20004;&#20004;&#27604;&#36739;&#20013;&#23398;&#20064;&#25928;&#29992;&#25110;&#22870;&#21169;&#27169;&#22411;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#26041;&#27861;&#20174;&#26412;&#36136;&#19978;&#38656;&#35201;&#20174;&#20154;&#20204;&#37027;&#37324;&#25910;&#38598;&#20559;&#22909;&#20449;&#24687;&#65292;&#32780;&#21453;&#39304;&#36890;&#24120;&#26159;&#21311;&#21517;&#25552;&#20379;&#30340;&#12290;&#30001;&#20110;&#20559;&#22909;&#26159;&#20027;&#35266;&#30340;&#65292;&#27809;&#26377;&#21487;&#20197;&#27604;&#36739;&#30340;&#40644;&#37329;&#26631;&#20934;&#65307;&#28982;&#32780;&#65292;&#23545;&#20559;&#22909;&#23398;&#20064;&#30340;&#39640;&#24433;&#21709;&#31995;&#32479;&#30340;&#20381;&#36182;&#24615;&#20026;&#24694;&#24847;&#34892;&#20026;&#32773;&#20542;&#21521;&#20110;&#25197;&#26354;&#20197;&#36798;&#21040;&#20854;&#30446;&#30340;&#32780;&#37319;&#38598;&#30340;&#25968;&#25454;&#21019;&#36896;&#20102;&#24378;&#28872;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#31181;&#23041;&#32961;&#27169;&#22411;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#36825;&#31181;&#28431;&#27934;&#30340;&#24615;&#36136;&#21644;&#31243;&#24230;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#21487;&#20197;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#65292;&#20197;&#20419;&#36827;&#25110;&#36140;&#20302;&#30446;&#26631;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#29992;&#20110;&#36825;&#20123;&#25915;&#20987;&#30340;&#31639;&#27861;&#26041;&#27861;&#65306;&#22522;&#20110;&#21407;&#21017;&#30340;&#26799;&#24230;&#26694;&#26550;&#21644;&#20960;&#31181;&#21464;&#31181;&#30340;&#25353;&#36317;&#31163;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31867;&#26368;&#20339;&#25915;&#20987;&#22312;&#25104;&#21151;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning utility, or reward, models from pairwise comparisons is a fundamental component in a number of application domains. These approaches inherently entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability systematically by considering a threat model in which an attacker can flip a small subset of preference comparisons with the goal of either promoting or demoting a target outcome. First, we propose two classes of algorithmic approaches for these attacks: a principled gradient-based framework, and several variants of rank-by-distance methods. Next, we demonstrate the efficacy of best attacks in both these classes in successfully achieving malicio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#24635;&#32467;&#20102;&#26368;&#36817;&#22312;&#20167;&#24680;&#35328;&#35770;&#23457;&#26680;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#21548;&#35273;&#20803;&#32032;&#22312;&#20256;&#25773;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#22823;&#22411;&#27169;&#22411;&#23545;&#23457;&#26680;&#33021;&#21147;&#30340;&#37325;&#26032;&#23450;&#20041;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#20102;&#22312;&#23569;&#25968;&#35821;&#35328;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#30740;&#31350;&#24046;&#36317;&#21644;&#22788;&#29702;&#20302;&#36164;&#28304;&#29615;&#22659;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.16727</link><description>&lt;p&gt;
&#26368;&#36817;&#22312;&#20167;&#24680;&#35328;&#35770;&#23457;&#26680;&#26041;&#38754;&#30340;&#36827;&#23637;&#65306;&#22810;&#27169;&#24577;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large Models. (arXiv:2401.16727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16727
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#24635;&#32467;&#20102;&#26368;&#36817;&#22312;&#20167;&#24680;&#35328;&#35770;&#23457;&#26680;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#21548;&#35273;&#20803;&#32032;&#22312;&#20256;&#25773;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#22823;&#22411;&#27169;&#22411;&#23545;&#23457;&#26680;&#33021;&#21147;&#30340;&#37325;&#26032;&#23450;&#20041;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#20102;&#22312;&#23569;&#25968;&#35821;&#35328;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#30740;&#31350;&#24046;&#36317;&#21644;&#22788;&#29702;&#20302;&#36164;&#28304;&#29615;&#22659;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#20132;&#27969;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#23457;&#26680;&#20167;&#24680;&#35328;&#35770;&#65288;HS&#65289;&#38754;&#20020;&#30528;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#30001;&#25968;&#23383;&#20869;&#23481;&#30340;&#22810;&#27169;&#24577;&#29305;&#24615;&#25152;&#24102;&#26469;&#30340;&#12290;&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#28145;&#20837;&#30740;&#31350;&#20102;HS&#23457;&#26680;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#23835;&#36215;&#35282;&#33394;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#23545;&#24403;&#21069;&#25991;&#29486;&#30340;&#20840;&#38754;&#20998;&#26512;&#24320;&#22987;&#65292;&#25581;&#31034;&#20102;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#21548;&#35273;&#20803;&#32032;&#22312;&#20256;&#25773;HS&#20013;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#36235;&#21183;&#65292;&#21363;&#23558;&#36825;&#20123;&#27169;&#24577;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;HS&#30340;&#20256;&#25773;&#20855;&#26377;&#22797;&#26434;&#24615;&#21644;&#24494;&#22937;&#24615;&#12290;&#23545;&#20110;&#30001;LLMs&#21644;LMMs&#24102;&#26469;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20102;&#20854;&#23545;&#26816;&#27979;&#21644;&#23457;&#26680;&#33021;&#21147;&#36793;&#30028;&#30340;&#37325;&#26032;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#29616;&#26377;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#25968;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#20197;&#21450;&#22312;&#22788;&#29702;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#38656;&#35201;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving landscape of online communication, moderating hate speech (HS) presents an intricate challenge, compounded by the multimodal nature of digital content. This comprehensive survey delves into the recent strides in HS moderation, spotlighting the burgeoning role of large language models (LLMs) and large multimodal models (LMMs). Our exploration begins with a thorough analysis of current literature, revealing the nuanced interplay between textual, visual, and auditory elements in propagating HS. We uncover a notable trend towards integrating these modalities, primarily due to the complexity and subtlety with which HS is disseminated. A significant emphasis is placed on the advances facilitated by LLMs and LMMs, which have begun to redefine the boundaries of detection and moderation capabilities. We identify existing gaps in research, particularly in the context of underrepresented languages and cultures, and the need for solutions to handle low-resource settings. The survey
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;X-ELM&#30340;&#36328;&#35821;&#35328;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#31435;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23376;&#38598;&#26469;&#20943;&#36731;&#22810;&#35821;&#35328;&#31454;&#20105;&#65292;&#20026;&#22810;&#35821;&#35328;&#22788;&#29702;&#24102;&#26469;&#25552;&#21319;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;X-ELM&#22312;&#21508;&#31181;&#35821;&#35328;&#19978;&#20248;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#26032;&#35821;&#35328;&#30340;&#36845;&#20195;&#28155;&#21152;&#12290;</title><link>http://arxiv.org/abs/2401.10440</link><description>&lt;p&gt;
&#29992;&#36328;&#35821;&#35328;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#31361;&#30772;&#22810;&#35821;&#35328;&#29615;&#22659;&#30340;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models. (arXiv:2401.10440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;X-ELM&#30340;&#36328;&#35821;&#35328;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#31435;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23376;&#38598;&#26469;&#20943;&#36731;&#22810;&#35821;&#35328;&#31454;&#20105;&#65292;&#20026;&#22810;&#35821;&#35328;&#22788;&#29702;&#24102;&#26469;&#25552;&#21319;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;X-ELM&#22312;&#21508;&#31181;&#35821;&#35328;&#19978;&#20248;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#26032;&#35821;&#35328;&#30340;&#36845;&#20195;&#28155;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#33521;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#31454;&#20105;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#19981;&#21450;&#21333;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65288;X-ELM&#65289;&#65292;&#36890;&#36807;&#23545;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#30340;&#23376;&#38598;&#36827;&#34892;&#29420;&#31435;&#35757;&#32451;&#65292;&#26469;&#20943;&#36731;&#36825;&#31181;&#31454;&#20105;&#12290;&#36825;&#20010;&#36807;&#31243;&#20351;X-ELM&#38024;&#23545;&#19981;&#21516;&#35821;&#35328;&#36827;&#34892;&#19987;&#38376;&#35757;&#32451;&#65292;&#21516;&#26102;&#20316;&#20026;&#19968;&#20010;&#22810;&#35821;&#35328;&#38598;&#21512;&#20445;&#25345;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#32473;&#23450;&#30456;&#21516;&#35745;&#31639;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;X-ELM&#22312;&#25152;&#26377;&#32771;&#34385;&#30340;&#35821;&#35328;&#19978;&#20248;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#36825;&#20123;&#25910;&#30410;&#21487;&#20197;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;X-ELM&#22312;&#24615;&#33021;&#25913;&#36827;&#26041;&#38754;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#22909;&#22788;&#65306;&#21487;&#20197;&#36845;&#20195;&#22320;&#28155;&#21152;&#26032;&#30340;&#19987;&#23478;&#65292;&#36866;&#24212;&#26032;&#35821;&#35328;&#32780;&#19981;&#20250;&#20135;&#29983;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#26159;&#24322;&#27493;&#36827;&#34892;&#30340;&#65292;&#20943;&#23569;&#20102;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#30828;&#20214;&#35201;&#27714;&#65292;&#23454;&#29616;&#22810;&#35821;&#35328;&#24314;&#27169;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their popularity in non-English NLP, multilingual language models often underperform monolingual ones due to inter-language competition for model parameters. We propose Cross-lingual Expert Language Models (X-ELM), which mitigate this competition by independently training language models on subsets of the multilingual corpus. This process specializes X-ELMs to different languages while remaining effective as a multilingual ensemble. Our experiments show that when given the same compute budget, X-ELM outperforms jointly trained multilingual models across all considered languages and that these gains transfer to downstream tasks. X-ELM provides additional benefits over performance improvements: new experts can be iteratively added, adapting X-ELM to new languages without catastrophic forgetting. Furthermore, training is asynchronous, reducing the hardware requirements for multilingual training and democratizing multilingual modeling.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Axis Tour&#65292;&#29992;&#20110;&#30830;&#23450;ICA&#36716;&#25442;&#23884;&#20837;&#20013;&#36724;&#30340;&#39034;&#24207;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#35821;&#20041;&#36830;&#32493;&#24615;&#26469;&#25552;&#39640;&#35789;&#23884;&#20837;&#31354;&#38388;&#30340;&#28165;&#26224;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Axis Tour&#26500;&#24314;&#30340;&#20302;&#32500;&#23884;&#20837;&#27604;PCA&#21644;ICA&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.06112</link><description>&lt;p&gt;
Axis Tour: Word Tour &#30830;&#23450;ICA&#36716;&#25442;&#23884;&#20837;&#20013;&#36724;&#30340;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;
Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings. (arXiv:2401.06112v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Axis Tour&#65292;&#29992;&#20110;&#30830;&#23450;ICA&#36716;&#25442;&#23884;&#20837;&#20013;&#36724;&#30340;&#39034;&#24207;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#35821;&#20041;&#36830;&#32493;&#24615;&#26469;&#25552;&#39640;&#35789;&#23884;&#20837;&#31354;&#38388;&#30340;&#28165;&#26224;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Axis Tour&#26500;&#24314;&#30340;&#20302;&#32500;&#23884;&#20837;&#27604;PCA&#21644;ICA&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#23884;&#20837;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#65292;&#20294;&#35299;&#37322;&#39640;&#32500;&#23884;&#20837;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#34987;&#30830;&#23450;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;ICA&#36716;&#25442;&#30340;&#35789;&#23884;&#20837;&#25581;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#36724;&#65292;&#20294;&#36825;&#20123;&#36724;&#30340;&#39034;&#24207;&#26159;&#20219;&#24847;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#20851;&#27880;&#36825;&#20010;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Axis Tour&#65292;&#23427;&#20248;&#21270;&#20102;&#36724;&#30340;&#39034;&#24207;&#12290;&#21463;&#21040;&#19968;&#32500;&#35789;&#23884;&#20837;&#26041;&#27861;Word Tour&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#26368;&#22823;&#21270;&#36724;&#30340;&#35821;&#20041;&#36830;&#32493;&#24615;&#26469;&#25552;&#39640;&#35789;&#23884;&#20837;&#31354;&#38388;&#30340;&#28165;&#26224;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;PCA&#21644;ICA&#30456;&#27604;&#65292;Axis Tour&#26500;&#24314;&#20102;&#26356;&#22909;&#30340;&#20302;&#32500;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word embedding is one of the most important components in natural language processing, but interpreting high-dimensional embeddings remains a challenging problem. To address this problem, Independent Component Analysis (ICA) is identified as an effective solution. ICA-transformed word embeddings reveal interpretable semantic axes; however, the order of these axes are arbitrary. In this study, we focus on this property and propose a novel method, Axis Tour, which optimizes the order of the axes. Inspired by Word Tour, a one-dimensional word embedding method, we aim to improve the clarity of the word embedding space by maximizing the semantic continuity of the axes. Furthermore, we show through experiments on downstream tasks that Axis Tour constructs better low-dimensional embeddings compared to both PCA and ICA.
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;HPSG&#30340;Supertagging&#65292;&#22312;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#26641;&#24211;&#21644;&#22810;&#26679;&#21270;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;SVM&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#20934;&#30830;&#29575;&#12290;&#30456;&#20851;&#25968;&#25454;&#38598;&#24050;&#25972;&#29702;&#20026;&#26631;&#35760;&#20998;&#31867;&#24418;&#24335;&#65292;&#21487;&#20026;&#29616;&#20195;HPSG&#35299;&#26512;&#22120;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2309.07590</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;HPSG&#30340;Supertagging
&lt;/p&gt;
&lt;p&gt;
Revisiting Supertagging for HPSG. (arXiv:2309.07590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07590
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;HPSG&#30340;Supertagging&#65292;&#22312;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#26641;&#24211;&#21644;&#22810;&#26679;&#21270;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;SVM&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#20934;&#30830;&#29575;&#12290;&#30456;&#20851;&#25968;&#25454;&#38598;&#24050;&#25972;&#29702;&#20026;&#26631;&#35760;&#20998;&#31867;&#24418;&#24335;&#65292;&#21487;&#20026;&#29616;&#20195;HPSG&#35299;&#26512;&#22120;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;HPSG&#26641;&#24211;&#35757;&#32451;&#30340;&#26032;&#22411;supertagger&#12290;&#36825;&#20123;&#26641;&#24211;&#22522;&#20110;&#19968;&#20010;&#25104;&#29087;&#30340;&#35821;&#35328;&#23398;&#29702;&#35770;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;&#20016;&#23500;&#22810;&#26679;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#36229;&#20986;&#20102;&#36890;&#24120;&#30340;WSJ&#31532;23&#33410;&#21644;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#12290;&#20043;&#21069;&#30340;HPSG supertagging&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;MaxEnt&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;SVM&#21644;&#22522;&#20110;&#31070;&#32463;CRF&#21644;BERT&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20986;SVM&#21644;&#31070;&#32463;supertagger&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#24494;&#35843;&#30340;BERT-based tagger&#22312;&#26469;&#33258;WSJ23&#30340;1000&#20010;&#21477;&#23376;&#19978;&#36798;&#21040;&#20102;97.26%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#23436;&#20840;&#19981;&#21516;&#39046;&#22495;&#30340;"The Cathedral and the Bazaar"&#19978;&#36798;&#21040;&#20102;93.88%&#30340;&#20934;&#30830;&#29575;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23558;&#36825;&#20123;&#26032;&#30340;supertagger&#38598;&#25104;&#21040;&#29616;&#20195;HPSG&#35299;&#26512;&#22120;&#20013;&#26159;&#26377;&#24847;&#20041;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#20063;&#24076;&#26395;&#25105;&#20204;&#22312;&#36825;&#37324;&#20351;&#29992;&#30340;&#22810;&#26679;&#19988;&#38590;&#30340;&#25968;&#25454;&#38598;&#22312;&#35813;&#39046;&#22495;&#20013;&#33719;&#24471;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#37325;&#26032;&#26684;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31867;&#30340;&#23436;&#25972;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new supertaggers trained on HPSG-based treebanks. These treebanks feature high-quality annotation based on a well-developed linguistic theory and include diverse and challenging test datasets, beyond the usual WSJ section 23 and Wikipedia data. HPSG supertagging has previously relied on MaxEnt-based models. We use SVM and neural CRF- and BERT-based methods and show that both SVM and neural supertaggers achieve considerably higher accuracy compared to the baseline. Our fine-tuned BERT-based tagger achieves 97.26% accuracy on 1000 sentences from WSJ23 and 93.88% on the completely out-of-domain The Cathedral and the Bazaar (cb)). We conclude that it therefore makes sense to integrate these new supertaggers into modern HPSG parsers, and we also hope that the diverse and difficult datasets we used here will gain more popularity in the field. We contribute the complete dataset reformatted for token classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05516</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;LLMs&#37327;&#21270;&#20013;&#30340;&#26435;&#37325;&#33293;&#20837;
&lt;/p&gt;
&lt;p&gt;
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25191;&#34892;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#38543;&#30528;&#20301;&#25968;&#30340;&#20943;&#23569;&#65292;&#37327;&#21270;&#32593;&#26684;&#21464;&#24471;&#26356;&#21152;&#23485;&#27867;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19978;&#19979;&#33293;&#20837;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#32454;&#35843;&#19978;&#19979;&#33293;&#20837;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21046;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#31934;&#30830;&#19988;&#26377;&#38480;&#30340;&#36793;&#30028;&#65292;&#21482;&#26377;&#25913;&#21464;&#33293;&#20837;&#20540;&#30340;&#38408;&#20540;&#25165;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#39640;&#25928;&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;SignRound&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#30340;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;HC3 Plus&#65292;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#35821;&#26009;&#24211;&#32771;&#34385;&#20102;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#20013;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#21644;Tk-instruct&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.02731</link><description>&lt;p&gt;
HC3 Plus&#65306;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus. (arXiv:2309.02731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HC3 Plus&#65292;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#35821;&#26009;&#24211;&#32771;&#34385;&#20102;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#20013;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#21644;Tk-instruct&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22240;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#28508;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#26816;&#27979;&#36234;&#26469;&#36234;&#20851;&#27880;&#65292;&#36825;&#23545;&#26410;&#32463;&#35757;&#32451;&#30340;&#20154;&#31867;&#26469;&#35828;&#24448;&#24448;&#24456;&#38590;&#35782;&#21035;&#12290;&#30446;&#21069;&#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#38382;&#31572;&#26041;&#38754;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#20855;&#26377;&#35821;&#20041;&#19981;&#21464;&#24615;&#30340;&#20219;&#21153;&#65292;&#22914;&#25688;&#35201;&#12289;&#32763;&#35793;&#21644;&#25913;&#20889;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#19978;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#12289;&#26356;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#32463;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#20197;&#21069;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25351;&#23548;&#24494;&#35843;&#20102;Tk-instruct&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has gained significant interest due to its impressive performance, but people are increasingly concerned about its potential risks, particularly around the detection of AI-generated content (AIGC), which is often difficult for untrained humans to identify. Current datasets utilized for detecting ChatGPT-generated text primarily center around question-answering, yet they tend to disregard tasks that possess semantic-invariant properties, such as summarization, translation, and paraphrasing. Our primary studies demonstrate that detecting model-generated text on semantic-invariant tasks is more difficult. To fill this gap, we introduce a more extensive and comprehensive dataset that considers more types of tasks than previous work, including semantic-invariant tasks. In addition, the model after a large number of task instruction fine-tuning shows a strong powerful performance. Owing to its previous success, we further instruct fine-tuning Tk-instruct and built a more powerful det
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25490;&#21015;&#38382;&#39064;&#12289;&#35828;&#35805;&#20154;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#21644;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10652</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#35821;&#38899;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25490;&#21015;&#38382;&#39064;&#12289;&#35828;&#35805;&#20154;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#21644;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#35821;&#38899;&#20998;&#31163;&#30340;&#26368;&#20808;&#36827;&#24037;&#20855;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#24517;&#39035;&#22788;&#29702;&#25490;&#21015;&#38382;&#39064;&#65292;&#23427;&#20204;&#21463;&#21040;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#35828;&#35805;&#32773;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#23384;&#22312;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#35821;&#38899;&#20998;&#31163;&#25216;&#26415;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#24314;&#31435;&#24103;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#30340;&#28145;&#24230;&#27169;&#22359;&#21270;&#20219;&#21153;&#20013;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35821;&#38899;&#20998;&#31163;&#20013;&#65292;&#35828;&#35805;&#20154;&#30340;&#19981;&#21516;&#24103;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#32473;&#23450;&#37027;&#20010;&#35828;&#35805;&#20154;&#30340;&#38544;&#21547;&#26631;&#20934;&#24103;&#30340;&#22686;&#24378;&#29256;&#12290;&#35828;&#35805;&#20154;&#30340;&#24103;&#21253;&#21547;&#36275;&#22815;&#30340;&#38901;&#24459;&#20449;&#24687;&#37325;&#21472;&#65292;&#36825;&#26159;&#35821;&#38899;&#20998;&#31163;&#30340;&#20851;&#38190;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23398;&#20064;&#32553;&#23567;&#24103;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current monaural state of the art tools for speech separation relies on supervised learning. This means that they must deal with permutation problem, they are impacted by the mismatch on the number of speakers used in training and inference. Moreover, their performance heavily relies on the presence of high-quality labelled data. These problems can be effectively addressed by employing a fully unsupervised technique for speech separation. In this paper, we use contrastive learning to establish the representations of frames then use the learned representations in the downstream deep modularization task. Concretely, we demonstrate experimentally that in speech separation, different frames of a speaker can be viewed as augmentations of a given hidden standard frame of that speaker. The frames of a speaker contain enough prosodic information overlap which is key in speech separation. Based on this, we implement a self-supervised learning to learn to minimize the distance between frames
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07865</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25512;&#25991;/&#29992;&#25143;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22788;&#29702;&#25991;&#26412;&#22823;&#25968;&#25454;&#22320;&#29702;&#26631;&#35760;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#20272;&#35745;&#22352;&#26631;&#23545;&#65288;&#32463;&#24230;&#65292;&#32428;&#24230;&#65289;&#21644;&#20108;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#33539;&#22260;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#12290;&#24615;&#33021;&#25351;&#26631;&#34920;&#26126;&#65292;&#23545;&#20110;&#22312;&#25512;&#25991;&#20869;&#23481;&#21644;&#20803;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;30&#20844;&#37324;&#65292;&#32654;&#22269;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;15&#20844;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
&lt;/p&gt;</description></item></channel></rss>