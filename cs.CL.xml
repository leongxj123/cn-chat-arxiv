<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>Bonito&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#19987;&#23646;&#25968;&#25454;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18334</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#29992;&#20110;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18334
&lt;/p&gt;
&lt;p&gt;
Bonito&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#19987;&#23646;&#25968;&#25454;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bonito&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#27169;&#22411;&#65292;&#29992;&#20110;&#26465;&#20214;&#20219;&#21153;&#29983;&#25104;&#65306;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29992;&#20110;&#25351;&#20196;&#35843;&#20248;&#30340;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#29992;&#25143;&#19987;&#38376;&#30340;&#31169;&#20154;&#25968;&#25454;&#19978;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;1.65M&#20010;&#31034;&#20363;&#30340;&#26032;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;Bonito&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#37325;&#26032;&#28151;&#21512;&#25104;&#20803;&#27169;&#26495;&#32780;&#21019;&#24314;&#30340;&#12290;&#25968;&#25454;&#38598;&#30340;&#20803;&#27169;&#26495;&#20135;&#29983;&#35757;&#32451;&#31034;&#20363;&#65292;&#20854;&#20013;&#36755;&#20837;&#26159;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#21644;&#20219;&#21153;&#23646;&#24615;&#65292;&#36755;&#20986;&#21253;&#25324;&#25351;&#20196;&#21644;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;Bonito&#20026;&#19971;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#21512;&#25104;&#20219;&#21153;&#65292;&#36328;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411; -- &#26159;&#38750;&#38382;&#31572;&#12289;&#25277;&#21462;&#24335;&#38382;&#31572;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702; -- &#24182;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Bonito&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18334v1 Announce Type: new  Abstract: We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned mode
&lt;/p&gt;</description></item><item><title>MathGenie&#36890;&#36807;&#38382;&#39064;&#21453;&#21521;&#32763;&#35793;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21019;&#36896;&#20102;&#19968;&#20010;&#23478;&#26063;&#21270;&#30340;&#27169;&#22411;&#31995;&#21015;MathGenieLM&#12290;</title><link>https://arxiv.org/abs/2402.16352</link><description>&lt;p&gt;
MathGenie: &#20351;&#29992;&#38382;&#39064;&#21453;&#21521;&#32763;&#35793;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16352
&lt;/p&gt;
&lt;p&gt;
MathGenie&#36890;&#36807;&#38382;&#39064;&#21453;&#21521;&#32763;&#35793;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21019;&#36896;&#20102;&#19968;&#20010;&#23478;&#26063;&#21270;&#30340;&#27169;&#22411;&#31995;&#21015;MathGenieLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24320;&#28304;&#27169;&#22411;&#21644;GPT-4&#31561;&#38381;&#28304;&#27169;&#22411;&#20043;&#38388;&#22312;&#36825;&#19968;&#39046;&#22495;&#20173;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;MathGenie&#65292;&#29992;&#20110;&#20174;&#23567;&#35268;&#27169;&#38382;&#39064;-&#35299;&#20915;&#26041;&#26696;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#31181;&#23376;&#25968;&#25454;&#65289;&#20013;&#29983;&#25104;&#22810;&#26679;&#19988;&#21487;&#38752;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#20805;&#20102;&#31181;&#23376;&#25968;&#25454;&#30340;&#30495;&#23454;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#21453;&#21521;&#32763;&#35793;&#27169;&#22411;&#65292;&#23558;&#25193;&#20805;&#30340;&#35299;&#20915;&#26041;&#26696;&#32763;&#35793;&#22238;&#26032;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20026;&#26032;&#38382;&#39064;&#29983;&#25104;&#20102;&#38598;&#25104;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#30830;&#20445;&#38598;&#25104;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21407;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#39564;&#35777;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#26032;&#31579;&#36873;&#30340;&#25968;&#25454;&#19978;&#23545;&#20174;7B&#21040;70B&#19981;&#31561;&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#27979;&#35797;&#25152;&#25552;&#20986;&#30340;&#22686;&#24378;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#31216;&#20026;MathGenieLM&#30340;&#27169;&#22411;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16352v1 Announce Type: cross  Abstract: Large language models (LLMs) have exhibited great potential in mathematical reasoning. However, there remains a performance gap in this area between existing open-source models and closed-source models such as GPT-4. In this paper, we introduce MathGenie, a novel method for generating diverse and reliable math problems from a small-scale problem-solution dataset (denoted as seed data). We augment the ground-truth solutions of our seed data and train a back-translation model to translate the augmented solutions back into new questions. Subsequently, we generate code-integrated solutions for the new questions. To ensure the correctness of the code-integrated solutions, we employ rationale-based strategy for solution verification. Various pretrained models, ranging from 7B to 70B, are trained on the newly curated data to test the effectiveness of the proposed augmentation technique, resulting in a family of models known as MathGenieLM. Th
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.13764</link><description>&lt;p&gt;
CriticBench: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#35770;&#23478;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Evaluating Large Language Models as Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13764
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102; CriticBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22235;&#20010;&#20851;&#38190;&#35780;&#35770;&#33021;&#21147;&#32500;&#24230;&#65288;&#21453;&#39304;&#12289;&#27604;&#36739;&#12289;&#25913;&#36827;&#21644;&#20803;&#21453;&#39304;&#65289;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;CriticBench&#21253;&#21547;&#20061;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#36136;&#37327;&#32454;&#31890;&#24230;&#27700;&#24179;&#19978;&#35780;&#35770;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#25581;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#26377;&#36259;&#30340;&#20851;&#31995;&#12290;CriticBench&#30340;&#25968;&#25454;&#38598;&#12289;&#36164;&#28304;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#23558;&#22312;https://github.com/gmftbyGMFTBY/Cri&#19978;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13764v1 Announce Type: cross  Abstract: Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/Cri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010; WEBLINX &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#37327;&#20449;&#24687;&#30340;&#22788;&#29702;&#29942;&#39048;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05930</link><description>&lt;p&gt;
WebLINX: &#22810;&#36718;&#23545;&#35805;&#19979;&#30340;&#30495;&#23454;&#19990;&#30028;&#32593;&#31449;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
WebLINX: Real-World Website Navigation with Multi-Turn Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010; WEBLINX &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#37327;&#20449;&#24687;&#30340;&#22788;&#29702;&#29942;&#39048;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#23383;&#20195;&#29702;&#25511;&#21046;&#30528;&#19968;&#20010;&#32593;&#39029;&#27983;&#35272;&#22120;&#65292;&#24182;&#25353;&#29031;&#29992;&#25143;&#30340;&#25351;&#20196;&#20197;&#22810;&#36718;&#23545;&#35805;&#30340;&#26041;&#24335;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; WEBLINX - &#19968;&#20010;100K&#20132;&#20114;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#22312;2300&#20010;&#19987;&#23478;&#28436;&#31034;&#20013;&#36827;&#34892;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#28085;&#30422;&#20102;150&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#32593;&#31449;&#19978;&#30340;&#24191;&#27867;&#27169;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#20449;&#24687;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#23454;&#26102;&#22788;&#29702;&#25972;&#20010;&#32593;&#39029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#25490;&#21517;&#30456;&#20851;&#20803;&#32032;&#26469;&#39640;&#25928;&#22320;&#20462;&#21098; HTML &#39029;&#38754;&#12290;&#25105;&#20204;&#20351;&#29992;&#36873;&#23450;&#30340;&#20803;&#32032;&#65292;&#20197;&#21450;&#23631;&#24149;&#25130;&#22270;&#21644;&#25805;&#20316;&#21382;&#21490;&#35760;&#24405;&#65292;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#22312;&#23548;&#33322;&#32593;&#39029;&#26102;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20174;&#23567;&#22411;&#32431;&#25991;&#26412;&#27169;&#22411;&#21040;&#19987;&#26377;&#30340;&#22810;&#27169;&#24577; LLMs &#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We fi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24178;&#39044;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00711</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Explaining Text Classifiers with Counterfactual Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24178;&#39044;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#26041;&#27861;&#21487;&#20197;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20854;&#20013;&#21453;&#20107;&#23454;&#26159;&#25351;&#38500;&#20102;&#19968;&#20010;&#20998;&#31867;&#29305;&#24449;&#20043;&#22806;&#65292;&#19982;&#30495;&#23454;&#35266;&#23519;&#23436;&#20840;&#30456;&#21516;&#30340;&#20551;&#35774;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#26412;&#39046;&#22495;&#26500;&#24314;&#36825;&#31181;&#21453;&#20107;&#23454;&#23384;&#22312;&#29305;&#23450;&#25361;&#25112;&#65292;&#22240;&#20026;&#26576;&#20123;&#23646;&#24615;&#20540;&#21487;&#33021;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#20107;&#20214;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#24178;&#39044;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#24178;&#39044;&#26041;&#27861;&#26159;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;Pearl&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#20013;&#23450;&#20041;&#30340;&#21453;&#20107;&#23454;&#26159;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22522;&#20110;&#30495;&#23454;&#21453;&#20107;&#23454;&#65288;&#36890;&#36807;&#26126;&#30830;&#30340;&#25991;&#26412;&#24178;&#39044;&#33719;&#24471;&#65289;&#21644;&#25105;&#20204;&#30340;&#21453;&#20107;&#23454;&#65288;&#36890;&#36807;&#23545;&#25991;&#26412;&#34920;&#31034;&#30340;&#24178;&#39044;&#24471;&#21040;&#65289;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the r
&lt;/p&gt;</description></item><item><title>Temp-Lora&#26041;&#27861;&#36890;&#36807;&#22312;&#38271;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#36880;&#27493;&#35757;&#32451;&#20020;&#26102;Lora&#27169;&#22359;&#65292;&#26377;&#25928;&#20445;&#30041;&#19978;&#19979;&#25991;&#30693;&#35782;&#24182;&#36991;&#20813;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#27704;&#20037;&#24615;&#25913;&#21464;&#12290;</title><link>https://arxiv.org/abs/2401.11504</link><description>&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#37327;&#22686;&#21152;&#65292;&#25512;&#26029;&#35757;&#32451;&#26377;&#21161;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11504
&lt;/p&gt;
&lt;p&gt;
Temp-Lora&#26041;&#27861;&#36890;&#36807;&#22312;&#38271;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#36880;&#27493;&#35757;&#32451;&#20020;&#26102;Lora&#27169;&#22359;&#65292;&#26377;&#25928;&#20445;&#30041;&#19978;&#19979;&#25991;&#30693;&#35782;&#24182;&#36991;&#20813;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#27704;&#20037;&#24615;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#25991;&#26412;&#29983;&#25104;&#65292;&#22914;&#23567;&#35828;&#21019;&#20316;&#21644;&#20855;&#26377;&#26497;&#38271;&#19978;&#19979;&#25991;&#30340;&#31687;&#31456;&#32423;&#32763;&#35793;&#65292;&#23545;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#38271;&#24230;&#22806;&#25512;&#31561;&#31574;&#30053;&#25193;&#23637;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;/&#25110;&#25512;&#26029;&#38454;&#27573;&#35201;&#27714;&#22823;&#37327;&#30828;&#20214;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Temp-Lora&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#27010;&#24565;&#12290;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;KV&#32531;&#23384;&#23384;&#20648;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#26159;&#23558;&#36825;&#20123;&#20449;&#24687;&#30452;&#25509;&#23884;&#20837;&#20020;&#26102;Lora&#27169;&#22359;&#20013;&#12290;&#22312;&#38271;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36825;&#20010;&#27169;&#22359;&#20250;&#38543;&#30528;&#20808;&#21069;&#29983;&#25104;&#30340;&#25991;&#26412;&#36880;&#28176;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#22320;&#20445;&#30041;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#36824;&#38450;&#27490;&#20102;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#20219;&#20309;&#27704;&#20037;&#24615;&#25913;&#21464;&#65292;&#22240;&#20026;&#27169;&#22359;&#22312;&#29983;&#25104;&#21518;&#34987;&#20002;&#24323;&#12290;&#22312;PG19&#35821;&#35328;&#24314;&#27169;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11504v2 Announce Type: replace-cross  Abstract: Long text generation, such as novel writing and discourse-level translation with extremely long contexts, presents significant challenges to current language models. Existing methods mainly focus on extending the model's context window through strategies like length extrapolation. However, these approaches demand substantial hardware resources during the training and/or inference phases. Our proposed method, Temp-Lora, introduces an alternative concept. Instead of relying on the KV cache to store all context information, we embeds this information directly into a temporary Lora module. In the process of long text generation, this module is progressively trained with text generated previously. This approach not only efficiently preserves contextual knowledge but also prevents any permanent alteration to the model's parameters given that the module is discarded post-generation. Extensive experiments on the PG19 language modeling 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#21644;&#35782;&#21035;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#65292;&#27169;&#22411;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#35299;&#37322;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.04916</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explainable Identification of Hate Speech towards Islam using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04916
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#21644;&#35782;&#21035;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#65292;&#27169;&#22411;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#35299;&#37322;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#22312;&#22312;&#32447;&#31038;&#20132;&#20114;&#21160;&#24179;&#21488;&#19978;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#35782;&#21035;&#21644;&#28040;&#38500;&#36825;&#31181;&#20167;&#24680;&#26159;&#36808;&#21521;&#21644;&#35856;&#19982;&#21644;&#24179;&#26410;&#26469;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#21644;&#35299;&#37322;&#38024;&#23545;&#20234;&#26031;&#20848;&#25945;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#12289;&#25552;&#21462;&#24182;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#33021;&#22815;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#23545;&#28508;&#22312;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04916v2 Announce Type: cross  Abstract: Islamophobic language is a prevalent challenge on online social interaction platforms. Identifying and eliminating such hatred is a crucial step towards a future of harmony and peace. This study presents a novel paradigm for identifying and explaining hate speech towards Islam using graph neural networks. Utilizing the intrinsic ability of graph neural networks to find, extract, and use relationships across disparate data points, our model consistently achieves outstanding performance while offering explanations for the underlying correlations and causation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.06118</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#36890;&#36807;&#21152;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#25216;&#26415;&#30340;&#31454;&#36187;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#26368;&#32456;&#29992;&#25143;&#35774;&#22791;&#19978;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22810;&#30721;&#26412;&#37327;&#21270;(MCQ)&#30340;&#32463;&#20856;&#26041;&#27861;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#8220;&#26497;&#31471;&#8221;LLM&#21387;&#32553;&#30340;&#38382;&#39064;&#65292;&#21363;&#38024;&#23545;&#38750;&#24120;&#20302;&#30340;&#20301;&#25968;&#65292;&#20363;&#22914;&#27599;&#20010;&#21442;&#25968;2&#21040;3&#20301;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#21152;&#24615;&#37327;&#21270;&#36825;&#19968;&#32463;&#20856;&#31639;&#27861;&#20043;&#19978;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#31639;&#27861;&#22312;LLM&#21387;&#32553;&#26041;&#38754;&#25512;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#20197;&#32473;&#23450;&#21387;&#32553;&#39044;&#31639;&#30340;&#20934;&#30830;&#24615;&#32780;&#35328;&#65292;&#20248;&#20110;&#25152;&#26377;&#26368;&#36817;&#25552;&#20986;&#30340;&#25216;&#26415;&#12290;&#20363;&#22914;&#65292;&#24403;&#23558;Llama 2&#27169;&#22411;&#21387;&#32553;&#21040;&#27599;&#20010;&#21442;&#25968;2&#20301;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;7B&#27169;&#22411;&#37327;&#21270;&#20026;6.93&#22256;&#24785;&#24230;(&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20339;&#24037;&#20316;&#25913;&#36827;1.29&#65292;&#30456;&#23545;&#20110;FP16&#25913;&#36827;1.81)&#65292;13B&#27169;&#22411;&#37327;&#21270;&#20026;5.70&#22256;&#24785;&#24230;(&#25913;&#36827;0.36)&#65292;70B&#27169;&#22411;&#37327;&#21270;&#20026;3.94&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#36807;&#31243;&#20013;&#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2312.10813</link><description>&lt;p&gt;
&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65306;&#22312;0.5K&#21442;&#25968;&#20869;&#25512;&#24191;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters. (arXiv:2312.10813v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#36807;&#31243;&#20013;&#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#12290;&#26368;&#36817;&#65292;&#25552;&#31034;&#35843;&#20248;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#35843;&#25972;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#20923;&#32467;&#39592;&#24178;&#37096;&#20998;&#30340;&#21442;&#25968;&#65292;&#21482;&#35774;&#35745;&#21644;&#35843;&#25972;&#25552;&#31034;&#12290;&#19968;&#26041;&#38754;&#65292;&#25552;&#31034;&#35843;&#20248;&#30340;&#31934;&#24515;&#35774;&#35745;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#26356;&#26032;&#35268;&#21017;&#22823;&#22823;&#22686;&#21152;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#21463;&#21040;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#28436;&#21464;&#27169;&#24335;&#19982;&#36866;&#24212;&#36807;&#31243;&#20013;&#25552;&#31034;&#30697;&#38453;&#31209;&#21464;&#21270;&#36235;&#21183;&#30340;&#35843;&#21644;&#19968;&#33268;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#25552;&#31034;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22823;&#22823;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of large pre-trained vision-language models, how to effectively transfer the knowledge of such foundational models to downstream tasks becomes a hot topic, especially in a data-deficient scenario. Recently, prompt tuning has become a popular solution. When adapting the vision-language models, researchers freeze the parameters in the backbone and only design and tune the prompts. On the one hand, the delicate design of prompt tuning exhibits strong performance. On the other hand, complicated structures and update rules largely increase the computation and storage cost. Motivated by the observation that the evolution pattern of the generalization capability in visual-language models aligns harmoniously with the trend of rank variations in the prompt matrix during adaptation, we design a new type of prompt, Re-parameterized Low-rank Prompt (RLP), for both efficient and effective adaptation. Our method could largely reduce the number of tunable parameters and storage s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#21457;&#29616;&#20102;&#35813;&#22240;&#23376;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.11616</link><description>&lt;p&gt;
&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;&#65306;&#19968;&#31181;&#24515;&#29702;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach. (arXiv:2310.11616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#21457;&#29616;&#20102;&#35813;&#22240;&#23376;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#29702;&#35770;&#22312;&#20154;&#31867;&#21644;&#26576;&#20123;&#21160;&#29289;&#29289;&#31181;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;Open LLM Leaderboard&#65288;&#21253;&#21547;1,232&#20010;&#27169;&#22411;&#65289;&#21644;General Language Understanding Evaluation&#65288;GLUE&#65289;Leaderboard&#65288;&#21253;&#21547;88&#20010;&#27169;&#22411;&#65289;&#36827;&#34892;&#22240;&#23376;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20855;&#26377;&#19968;&#32500;&#24615;&#21644;&#39640;&#24230;&#31283;&#23450;&#24615;&#30340;g&#22240;&#23376;&#65292;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;g&#20043;&#38388;&#30340;&#20013;&#24230;&#30456;&#20851;&#24615;&#20026;0.48&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;g&#22240;&#23376;&#20026;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#65292;&#20026;&#26356;&#24378;&#22823;&#12289;&#22522;&#20110;g&#22240;&#23376;&#30340;&#27169;&#22411;&#33021;&#21147;&#35780;&#20272;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#20174;&#24515;&#29702;&#27979;&#37327;&#30340;&#35282;&#24230;&#29702;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#23545;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study uncovers the factor of general intelligence, or g, in language models, extending the psychometric theory traditionally applied to humans and certain animal species. Utilizing factor analysis on two extensive datasets Open LLM Leaderboard with 1,232 models and General Language Understanding Evaluation (GLUE) Leaderboard with 88 models - we find compelling evidence for a unidimensional, highly stable g factor that accounts for 85% of the variance in model performance. The study also finds a moderate correlation of .48 between model size and g. The discovery of g in language models offers a unified metric for model evaluation and opens new avenues for more robust, g-based model ability assessment. These findings lay the foundation for understanding and future research on artificial general intelligence from a psychometric perspective and have practical implications for model evaluation and development.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#30740;&#31350;11&#31181;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;surprisal&#19982;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#27979;&#35797;&#20102;Surprisal&#29702;&#35770;&#30340;&#19977;&#20010;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#20182;&#35821;&#35328;&#29305;&#24449;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.03667</link><description>&lt;p&gt;
&#22312;11&#31181;&#35821;&#35328;&#20013;&#27979;&#35797;Surprisal&#29702;&#35770;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Testing the Predictions of Surprisal Theory in 11 Languages. (arXiv:2307.03667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#30740;&#31350;11&#31181;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;surprisal&#19982;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#27979;&#35797;&#20102;Surprisal&#29702;&#35770;&#30340;&#19977;&#20010;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#20182;&#35821;&#35328;&#29305;&#24449;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#35821;&#35328;&#23398;&#30340;&#19968;&#20010;&#22522;&#26412;&#32467;&#26524;&#26159;&#65292;&#21487;&#39044;&#27979;&#24615;&#36739;&#20302;&#30340;&#35789;&#35821;&#38656;&#35201;&#26356;&#38271;&#26102;&#38388;&#26469;&#22788;&#29702;&#12290;Surprisal&#29702;&#35770;&#65288;Hale, 2001; Levy, 2008&#65289;&#26159;&#23545;&#36825;&#19968;&#21457;&#29616;&#30340;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#23427;&#23558;&#19968;&#20010;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#37327;&#21270;&#20026;&#20854;surprisal&#65292;&#21363;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#36127;&#23545;&#25968;&#27010;&#29575;&#12290;&#34429;&#28982;&#26377;&#22823;&#37327;&#30340;&#35777;&#25454;&#25903;&#25345;Surprisal&#29702;&#35770;&#30340;&#39044;&#27979;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#19968;&#20010;&#38750;&#24120;&#26377;&#38480;&#30340;&#25968;&#25454;&#33539;&#22260;&#20869;&#65292;&#21363;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#20154;&#38405;&#35835;&#33521;&#35821;&#25991;&#26412;&#12290;&#20107;&#23454;&#19978;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#22312;&#20116;&#20010;&#35821;&#35328;&#23478;&#26063;&#20013;&#20998;&#24067;&#30340;&#21313;&#19968;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;surprisal&#19982;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#22635;&#34917;&#24403;&#21069;&#25991;&#29486;&#20013;&#30340;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#20174;&#21333;&#35821;&#21644;&#22810;&#35821;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#23548;&#20272;&#35745;&#20540;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19982;surprisal&#29702;&#35770;&#30456;&#20851;&#30340;&#19977;&#20010;&#39044;&#27979;&#65306;(i) surprisal&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#38405;&#35835;&#26102;&#38388;&#65307;(ii) &#39044;&#26399;surprisal&#65292;&#21363;&#19978;&#19979;&#25991;&#29109;&#65292;&#26159;&#21542;&#24433;&#21709;&#38405;&#35835;&#26102;&#38388;&#65307;(iii) &#19982;surprisal&#30456;&#20851;&#30340;&#20854;&#20182;&#35821;&#35328;&#29305;&#24449;&#26159;&#21542;&#21487;&#20197;&#35299;&#37322;&#38405;&#35835;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental result in psycholinguistics is that less predictable words take a longer time to process. One theoretical explanation for this finding is Surprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's predictability as its surprisal, i.e. its negative log-probability given a context. While evidence supporting the predictions of Surprisal Theory have been replicated widely, most have focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times; (ii) whether expected surprisal, i.e. contextual entropy, i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21475;&#36848;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#36328;&#35821;&#35328;&#35270;&#35282;&#12290;&#36890;&#36807;&#20351;&#29992;&#33655;&#20848;&#35821;&#12289;&#33521;&#35821;&#21644;&#24503;&#35821;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#21450;&#31649;&#36947;&#21644;&#31471;&#21040;&#31471;&#26041;&#26696;&#65292;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#20266;&#26631;&#27880;&#25968;&#25454;&#38598;&#21644;Wav2Vec2-XLS-R&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20960;&#31181;&#36866;&#24212;&#36328;&#35821;&#35328;&#31995;&#32479;&#30340;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#31471;&#21040;&#31471;&#21475;&#36848;NER&#22312;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20248;&#20110;&#31649;&#36947;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#24503;&#35821;&#21040;&#33655;&#20848;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20102;&#33655;&#20848;&#35821;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01310</link><description>&lt;p&gt;
&#25506;&#32034;&#21475;&#36848;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#36328;&#35821;&#35328;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Spoken Named Entity Recognition: A Cross-Lingual Perspective. (arXiv:2307.01310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21475;&#36848;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#36328;&#35821;&#35328;&#35270;&#35282;&#12290;&#36890;&#36807;&#20351;&#29992;&#33655;&#20848;&#35821;&#12289;&#33521;&#35821;&#21644;&#24503;&#35821;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#21450;&#31649;&#36947;&#21644;&#31471;&#21040;&#31471;&#26041;&#26696;&#65292;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#20266;&#26631;&#27880;&#25968;&#25454;&#38598;&#21644;Wav2Vec2-XLS-R&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20960;&#31181;&#36866;&#24212;&#36328;&#35821;&#35328;&#31995;&#32479;&#30340;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#31471;&#21040;&#31471;&#21475;&#36848;NER&#22312;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20248;&#20110;&#31649;&#36947;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#24503;&#35821;&#21040;&#33655;&#20848;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20102;&#33655;&#20848;&#35821;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#25968;&#25454;&#20013;&#23454;&#20307;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21475;&#36848;NER&#20316;&#20026;&#21475;&#36848;&#25991;&#26723;&#26816;&#32034;&#30340;&#19987;&#38376;&#39046;&#22495;&#65292;&#30001;&#20110;&#30740;&#31350;&#26377;&#38480;&#21644;&#25968;&#25454;&#31232;&#32570;&#32780;&#28382;&#21518;&#12290;&#27492;&#22806;&#65292;&#21475;&#36848;NER&#20013;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#21033;&#29992;&#33655;&#20848;&#35821;&#12289;&#33521;&#35821;&#21644;&#24503;&#35821;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#20351;&#29992;&#31649;&#36947;&#21644;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#26041;&#26696;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#20266;&#26631;&#27880;&#25968;&#25454;&#38598;&#20351;&#29992;Wav2Vec2-XLS-R&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#20960;&#31181;&#36866;&#24212;&#36328;&#35821;&#35328;&#31995;&#32479;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31471;&#21040;&#31471;&#21475;&#36848;NER&#22312;&#25105;&#20204;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#19978;&#20248;&#20110;&#22522;&#20110;&#31649;&#36947;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#24503;&#35821;&#21040;&#33655;&#20848;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#36229;&#36807;&#20102;&#33655;&#20848;&#35821;E2E&#31995;&#32479;7%&#21644;&#33655;&#20848;&#35821;&#31649;&#36947;&#31995;&#32479;4%&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#31361;&#20986;&#20102;&#21475;&#36848;NER&#20013;&#36801;&#31227;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#19988;&#20026;&#26410;&#26469;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Named Entity Recognition (NER) have significantly improved the identification of entities in textual data. However, spoken NER, a specialized field of spoken document retrieval, lags behind due to its limited research and scarce datasets. Moreover, cross-lingual transfer learning in spoken NER has remained unexplored. This paper utilizes transfer learning across Dutch, English, and German using pipeline and End-to-End (E2E) schemes. We employ Wav2Vec2-XLS-R models on custom pseudo-annotated datasets and investigate several architectures for the adaptability of cross-lingual systems. Our results demonstrate that End-to-End spoken NER outperforms pipeline-based alternatives over our limited annotations. Notably, transfer learning from German to Dutch surpasses the Dutch E2E system by 7% and the Dutch pipeline system by 4%. This study not only underscores the feasibility of transfer learning in spoken NER but also sets promising outcomes for future evaluations, hint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;RRWKV&#26550;&#26500;&#65292;&#23427;&#22312;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21152;&#20837;&#22238;&#39038;&#33021;&#21147;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.05176</link><description>&lt;p&gt;
RRWKV&#65306;&#22312;RWKV&#20013;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
RRWKV: Capturing Long-range Dependencies in RWKV. (arXiv:2306.05176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;RRWKV&#26550;&#26500;&#65292;&#23427;&#22312;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21152;&#20837;&#22238;&#39038;&#33021;&#21147;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;Transformer&#24778;&#20154;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#26550;&#26500;&#12290;&#26368;&#36817;&#65292;Receptance Weighted Key Value&#65288;RWKV&#65289;&#26550;&#26500;&#36981;&#24490;&#38750;Transformer&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#32570;&#28857;&#65292;&#20854;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#21576;&#20108;&#27425;&#25193;&#23637;&#12290;&#23613;&#31649;RWKV&#21033;&#29992;&#20102;&#32447;&#24615;&#24352;&#37327;&#31215;&#27880;&#24847;&#26426;&#21046;&#24182;&#36890;&#36807;&#37096;&#32626;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#23454;&#29616;&#20102;&#24182;&#34892;&#35745;&#31639;&#65292;&#20294;&#19982;&#26631;&#20934;Transformer&#20013;&#30452;&#25509;&#20132;&#20114;&#33719;&#24471;&#30340;&#23436;&#25972;&#20449;&#24687;&#30456;&#27604;&#65292;&#23427;&#26080;&#27861;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#20026;&#20854;&#21463;&#38480;&#20110;&#21521;&#21518;&#26597;&#30475;&#20808;&#21069;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#23558;&#22238;&#39038;&#33021;&#21147;&#32435;&#20837;RWKV&#20013;&#26469;&#35774;&#35745;Retrospected Receptance Weighted Key Value&#65288;RRWKV&#65289;&#26550;&#26500;&#65292;&#20197;&#26377;&#25928;&#22320;&#21560;&#25910;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to the impressive dot-product attention, the Transformers have been the dominant architectures in various natural language processing (NLP) tasks. Recently, the Receptance Weighted Key Value (RWKV) architecture follows a non-transformer architecture to eliminate the drawbacks of dot-product attention, where memory and computational complexity exhibits quadratic scaling with sequence length. Although RWKV has exploited a linearly tensor-product attention mechanism and achieved parallelized computations by deploying the time-sequential mode, it fails to capture long-range dependencies because of its limitation on looking back at previous information, compared with full information obtained by direct interactions in the standard transformer. Therefore, the paper devises the Retrospected Receptance Weighted Key Value (RRWKV) architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#24615;&#33021;&#12289;&#24230;&#37327;&#26631;&#20934;&#12289;&#40065;&#26834;&#24615;&#21644;&#38169;&#35823;&#31867;&#22411;&#22235;&#20010;&#26041;&#38754;&#35780;&#20272;ChatGPT&#30340;&#20449;&#24687;&#25277;&#21462;&#33021;&#21147;&#65292;&#21457;&#29616;&#20102;ChatGPT&#19982;SOTA&#32467;&#26524;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21305;&#37197;&#31574;&#30053;&#20197;&#26356;&#20934;&#30830;&#22320;&#21453;&#26144;ChatGPT&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14450</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#35299;&#20915;&#20102;&#20449;&#24687;&#25277;&#21462;&#38382;&#39064;&#65311;&#24615;&#33021;&#12289;&#24230;&#37327;&#26631;&#20934;&#12289;&#40065;&#26834;&#24615;&#21644;&#38169;&#35823;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. (arXiv:2305.14450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#24615;&#33021;&#12289;&#24230;&#37327;&#26631;&#20934;&#12289;&#40065;&#26834;&#24615;&#21644;&#38169;&#35823;&#31867;&#22411;&#22235;&#20010;&#26041;&#38754;&#35780;&#20272;ChatGPT&#30340;&#20449;&#24687;&#25277;&#21462;&#33021;&#21147;&#65292;&#21457;&#29616;&#20102;ChatGPT&#19982;SOTA&#32467;&#26524;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21305;&#37197;&#31574;&#30053;&#20197;&#26356;&#20934;&#30830;&#22320;&#21453;&#26144;ChatGPT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#28608;&#21457;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28526;&#12290;&#26412;&#25991;&#20174;&#24615;&#33021;&#12289;&#24230;&#37327;&#26631;&#20934;&#12289;&#40065;&#26834;&#24615;&#21644;&#38169;&#35823;&#31867;&#22411;&#22235;&#20010;&#26041;&#38754;&#35780;&#20272;&#20102;ChatGPT&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#12289;&#23567;&#26679;&#26412;&#21644;&#24605;&#32771;&#20018;&#32852;&#31561;&#22330;&#26223;&#19979;&#23545;17&#20010;&#25968;&#25454;&#38598;&#30340;14&#20010;IE&#23376;&#20219;&#21153;&#35780;&#20272;&#20102;ChatGPT&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;ChatGPT&#19982;SOTA&#32467;&#26524;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#36825;&#31181;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#36719;&#21305;&#37197;&#31574;&#30053;&#20197;&#26356;&#20934;&#30830;&#22320;&#21453;&#26144;ChatGPT&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ChatGPT&#22312;14&#20010;IE&#23376;&#20219;&#21153;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#65306;1&#65289;ChatGPT&#24456;&#23569;&#36755;&#20986;&#26080;&#25928;&#30340;&#21709;&#24212;&#65307;2&#65289;&#19981;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21644;&#38271;&#23614;&#30446;&#26631;&#31867;&#22411;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;ChatGPT&#30340;&#24615;&#33021;&#65307;3&#65289;ChatGPT&#26080;&#27861;&#24456;&#22909;&#22320;&#29702;&#35299;RE&#20219;&#21153;&#20013;&#30340;&#20027;&#23458;&#20307;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ChatGPT&#30340;&#38169;&#35823;&#65292;&#24182;&#21457;&#29616;&#8220;&#26410;&#27880;&#37322;&#30340;&#36328;&#24230;&#8221;&#26159;&#26368;&#20027;&#35201;&#30340;&#38169;&#35823;&#31867;&#22411;&#12290;&#36825;&#24341;&#36215;&#20102;&#26377;&#20851;&#29616;&#23454;&#22330;&#26223;&#19979;&#20449;&#24687;&#25552;&#21462;&#24615;&#33021;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has stimulated the research boom in the field of large language models. In this paper, we assess the capabilities of ChatGPT from four perspectives including Performance, Evaluation Criteria, Robustness and Error Types. Specifically, we first evaluate ChatGPT's performance on 17 datasets with 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought scenarios, and find a huge performance gap between ChatGPT and SOTA results. Next, we rethink this gap and propose a soft-matching strategy for evaluation to more accurately reflect ChatGPT's performance. Then, we analyze the robustness of ChatGPT on 14 IE sub-tasks, and find that: 1) ChatGPT rarely outputs invalid responses; 2) Irrelevant context and long-tail target types greatly affect ChatGPT's performance; 3) ChatGPT cannot understand well the subject-object relationships in RE task. Finally, we analyze the errors of ChatGPT, and find that "unannotated spans" is the most dominant error type. This raises concerns about 
&lt;/p&gt;</description></item></channel></rss>