<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#32423;&#24635;&#32467;&#27861;&#23545;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;F1&#20998;&#25968;&#30340;&#26174;&#33879;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.13107</link><description>&lt;p&gt;
&#38754;&#21521;&#27861;&#24459;&#25991;&#26412;&#30340;&#22810;&#32423;&#24635;&#32467;&#26080;&#30417;&#30563;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Unsupervised Question Answering System with Multi-level Summarization for Legal Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#32423;&#24635;&#32467;&#27861;&#23545;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;F1&#20998;&#25968;&#30340;&#26174;&#33879;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#22242;&#38431;SCaLAR&#22312;SemEval-2024&#20219;&#21153;5&#19978;&#30340;&#24037;&#20316;&#65306;&#27665;&#20107;&#31243;&#24207;&#20013;&#30340;&#27861;&#24459;&#35770;&#35777;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#30001;&#20110;&#28041;&#21450;&#21040;&#30340;&#27861;&#24459;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#32780;&#20196;&#20154;&#26395;&#32780;&#21364;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21448;&#26032;&#39062;&#30340;&#22522;&#20110;&#30456;&#20284;&#24230;&#21644;&#36317;&#31163;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#29983;&#25104;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#38598;&#25104;&#29305;&#24449;&#65288;&#21253;&#25324;CNN&#12289;GRU&#21644;LSTM&#65289;&#30340;&#22810;&#32423;Legal-Bert&#23884;&#20837;&#30340;&#34701;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#27861;&#24459;&#35299;&#37322;&#30340;&#20887;&#38271;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;T5&#30340;&#20998;&#27573;&#25688;&#35201;&#65292;&#25104;&#21151;&#22320;&#20445;&#30041;&#20102;&#20851;&#38190;&#20449;&#24687;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#31995;&#32479;&#22312;&#24320;&#21457;&#38598;&#19978;&#35265;&#35777;&#20102;macro F1&#20998;&#25968;&#22686;&#21152;&#20102;20&#20010;&#30334;&#20998;&#28857;&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#22686;&#21152;&#20102;10&#20010;&#30334;&#20998;&#28857;&#65292;&#32771;&#34385;&#21040;&#20854;&#31616;&#21333;&#30340;&#26550;&#26500;&#65292;&#36825;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13107v1 Announce Type: new  Abstract: This paper summarizes Team SCaLAR's work on SemEval-2024 Task 5: Legal Argument Reasoning in Civil Procedure. To address this Binary Classification task, which was daunting due to the complexity of the Legal Texts involved, we propose a simple yet novel similarity and distance-based unsupervised approach to generate labels. Further, we explore the Multi-level fusion of Legal-Bert embeddings using ensemble features, including CNN, GRU, and LSTM. To address the lengthy nature of Legal explanation in the dataset, we introduce T5-based segment-wise summarization, which successfully retained crucial information, enhancing the model's performance. Our unsupervised system witnessed a 20-point increase in macro F1-score on the development set and a 10-point increase on the test set, which is promising given its uncomplicated architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861; GORA &#21644;&#19968;&#31181;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861; SORA&#65292;&#29992;&#20197;&#35299;&#20915;&#27169;&#22411;&#32534;&#36753;&#20013;&#30340;&#38544;&#34255;&#31354;&#38388;&#20013;&#30340;&#28063;&#28458;&#25928;&#24212;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07825</link><description>&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#20013;&#30340;&#36951;&#28431;&#20043;&#22788;&#65306;&#28145;&#20837;&#25506;&#35752;&#27169;&#22411;&#32534;&#36753;&#24102;&#26469;&#30340;&#38544;&#34255;&#25439;&#23475;
&lt;/p&gt;
&lt;p&gt;
The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861; GORA &#21644;&#19968;&#31181;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861; SORA&#65292;&#29992;&#20197;&#35299;&#20915;&#27169;&#22411;&#32534;&#36753;&#20013;&#30340;&#38544;&#34255;&#31354;&#38388;&#20013;&#30340;&#28063;&#28458;&#25928;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20854;&#21331;&#36234;&#30340;&#25928;&#26524;&#24443;&#24213;&#25913;&#21464;&#20102;&#35768;&#22810;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#20197;&#20462;&#25913;&#36807;&#26102;&#25110;&#38169;&#35823;&#20449;&#24687;&#30340;&#20851;&#38190;&#24615;&#24037;&#20316;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#19968;&#20010;&#31216;&#20026;&#8220;&#38544;&#34255;&#31354;&#38388;&#20013;&#30340;&#28063;&#28458;&#25928;&#24212;&#8221;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#36825;&#31181;&#25928;&#24212;&#34429;&#28982;&#38590;&#20197;&#26816;&#27979;&#65292;&#20294;&#21364;&#20250;&#26174;&#33879;&#38459;&#30861;&#27169;&#22411;&#32534;&#36753;&#20219;&#21153;&#30340;&#25928;&#26524;&#65292;&#24182;&#24694;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#22522;&#20110;&#22270;&#24418;&#29305;&#24322;&#20540;&#20851;&#31995;&#30340;&#35780;&#20272;(GORA)&#65292;&#26469;&#24212;&#23545;&#36825;&#19968;&#31185;&#23398;&#25361;&#25112;&#65292;&#37327;&#21270;&#35780;&#20272;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#21644;&#32534;&#36753;&#30340;&#21518;&#32493;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26088;&#22312;&#20943;&#36731;&#36825;&#31181;&#28063;&#28458;&#25928;&#24212;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#8212;&#8212;&#36873;&#25321;&#24615;&#24322;&#24120;&#20540;&#37325;&#26032;&#32534;&#36753;&#26041;&#27861;(SORA)&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#25581;&#31034;&#20102;&#38544;&#34255;&#31354;&#38388;&#20013;&#30340;&#28063;&#28458;&#25928;&#24212;&#22312;&#25152;&#26377;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20013;&#37117;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;G
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07825v1 Announce Type: new  Abstract: Large Language Models have revolutionized numerous tasks with their remarkable efficacy.However, the editing of these models, crucial for rectifying outdated or erroneous information, often leads to a complex issue known as the ripple effect in the hidden space. This effect, while difficult to detect, can significantly impede the efficacy of model editing tasks and deteriorate model performance.This paper addresses this scientific challenge by proposing a novel evaluation methodology, Graphical Outlier Relation based Assessment(GORA), which quantitatively evaluates the adaptations of the model and the subsequent impact of editing. Furthermore, we introduce the Selective Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate this ripple effect. Our comprehensive evaluations reveal that the ripple effect in the hidden space is a significant issue in all current model editing methods. However, our proposed methods, G
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#65292;&#21253;&#25324;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#12289;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#12289;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12289;&#24635;&#32467;&#23545;&#35805;&#20197;&#21450;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#30340;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#35780;&#20215;&#25351;&#26631;PREFS&#12290;</title><link>https://arxiv.org/abs/2403.03823</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Modular Approach for Multimodal Summarization of TV Shows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03823
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#65292;&#21253;&#25324;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#12289;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#12289;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12289;&#24635;&#32467;&#23545;&#35805;&#20197;&#21450;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#30340;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#35780;&#20215;&#25351;&#26631;PREFS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#21040;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65306;&#22797;&#26434;&#25512;&#29702;&#12289;&#22810;&#27169;&#24577;&#21644;&#38271;&#31687;&#21465;&#20107;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#21508;&#20010;&#32452;&#20214;&#25191;&#34892;&#19987;&#38376;&#30340;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#35748;&#20026;&#19982;&#31471;&#21040;&#31471;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#28041;&#21450;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#65292;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#20197;&#23613;&#37327;&#20943;&#23569;&#19981;&#21516;&#20107;&#20214;&#20043;&#38388;&#30340;&#20999;&#25442;&#27425;&#25968;&#65292;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#24635;&#32467;&#27599;&#20010;&#22330;&#26223;&#20013;&#30340;&#23545;&#35805;&#65292;&#24182;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#25104;&#25972;&#38598;&#30340;&#26368;&#32456;&#25688;&#35201;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;PREFS&#65288;&#25688;&#35201;&#20107;&#23454;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#65289;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#25688;&#35201;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#20026;&#21407;&#23376;&#20107;&#23454;&#12290;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;SummScreen3D&#25968;&#25454;&#38598;Papalampidi&#21644;Lapata&#65288;2023&#65289;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03823v1 Announce Type: new  Abstract: In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PREFS (\textbf{P}recision and \textbf{R}ecall \textbf{E}valuation of Summary \textbf{F}act\textbf{s}), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset Papalampidi and Lapata (2023), our method produces hi
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.02990</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#30340;&#25968;&#25454;&#22686;&#24378;&#65306;&#25968;&#25454;&#35270;&#35282;&#12289;&#23398;&#20064;&#33539;&#24335;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02990
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#35757;&#32451;&#26679;&#26412;&#22810;&#26679;&#21270;&#32780;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21450;&#20854;&#20182;&#39046;&#22495;&#20013;&#23427;&#20204;&#25552;&#20379;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#20174;&#25968;&#25454;&#35270;&#35282;&#21644;&#23398;&#20064;&#35270;&#35282;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20174;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#31561;&#12290;&#26412;&#35843;&#26597;&#31361;&#26174;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#26088;&#22312;&#20316;&#20026;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02990v1 Announce Type: cross  Abstract: In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a 
&lt;/p&gt;</description></item><item><title>Proc2PDDL&#26159;&#21253;&#21547;&#24320;&#25918;&#39046;&#22495;&#31243;&#24207;&#24615;&#25991;&#26412;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#23545;&#20110;&#21160;&#20316;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#25928;&#26524;&#30340;&#23450;&#20041;&#22312;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#20026;&#26410;&#26469;&#25972;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#35268;&#21010;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.00092</link><description>&lt;p&gt;
PROC2PDDL&#65306;&#26469;&#33258;&#25991;&#26412;&#30340;&#24320;&#25918;&#39046;&#22495;&#35268;&#21010;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
PROC2PDDL: Open-Domain Planning Representations from Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00092
&lt;/p&gt;
&lt;p&gt;
Proc2PDDL&#26159;&#21253;&#21547;&#24320;&#25918;&#39046;&#22495;&#31243;&#24207;&#24615;&#25991;&#26412;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#23545;&#20110;&#21160;&#20316;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#25928;&#26524;&#30340;&#23450;&#20041;&#22312;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#20026;&#26410;&#26469;&#25972;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#35268;&#21010;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#35268;&#21010;&#20173;&#28982;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#39044;&#27979;&#35268;&#21010;&#39046;&#22495;&#23450;&#20041;&#65288;&#20363;&#22914;&#65292;PDDL&#65289;&#65292;&#20294;&#20165;&#22312;&#23553;&#38381;&#39046;&#22495;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Proc2PDDL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#24320;&#25918;&#39046;&#22495;&#31243;&#24207;&#24615;&#25991;&#26412;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;PDDL&#34920;&#31034;&#30340;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#23450;&#20041;&#21160;&#20316;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#25928;&#26524;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;Proc2PDDL&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;GPT-3.5 &#30340;&#25104;&#21151;&#29575;&#25509;&#36817;&#20110;0%&#65292;&#32780; GPT-4 &#30340;&#25104;&#21151;&#29575;&#32422;&#20026;35%&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#20986;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;&#31243;&#24207;&#21644;&#25512;&#29702;&#20107;&#20214;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#20998;&#26512;&#21644;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#26410;&#26469;&#22312;&#25972;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#35268;&#21010;&#30340;&#26368;&#20339;&#26041;&#38754;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00092v1 Announce Type: new  Abstract: Planning in a text-based environment continues to be a major challenge for AI systems. Recent approaches have used language models to predict a planning domain definition (e.g., PDDL) but have only been evaluated in closed-domain simulated environments. To address this, we present Proc2PDDL , the first dataset containing open-domain procedural texts paired with expert-annotated PDDL representations. Using this dataset, we evaluate state-of-the-art models on defining the preconditions and effects of actions. We show that Proc2PDDL is highly challenging, with GPT-3.5's success rate close to 0% and GPT-4's around 35%. Our analysis shows both syntactic and semantic errors, indicating LMs' deficiency in both generating domain-specific prgorams and reasoning about events. We hope this analysis and dataset helps future progress towards integrating the best of LMs and formal planning.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24335;&#23567;&#23398;&#25968;&#23398;&#25968;&#25454;&#38598;&#65288;GSM-Plus&#65289;&#65292;&#35780;&#20272;&#20102;25&#20010;LLMs&#21644;4&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#24191;&#27867;&#30340;&#38382;&#39064;&#21464;&#21270;&#20013;&#23637;&#31034;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#36828;&#38750;&#31283;&#20581;&#12290;</title><link>https://arxiv.org/abs/2402.19255</link><description>&lt;p&gt;
GSM-Plus&#65306;&#35780;&#20272;LLMs&#20316;&#20026;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#32773;&#30340;&#31283;&#20581;&#24615;&#30340;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19255
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24335;&#23567;&#23398;&#25968;&#23398;&#25968;&#25454;&#38598;&#65288;GSM-Plus&#65289;&#65292;&#35780;&#20272;&#20102;25&#20010;LLMs&#21644;4&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#24191;&#27867;&#30340;&#38382;&#39064;&#21464;&#21270;&#20013;&#23637;&#31034;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#36828;&#38750;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#24182;&#24212;&#29992;&#25968;&#23398;&#30693;&#35782;&#65292;&#36824;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#25463;&#24452;&#65292;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20105;&#35770;&#12290;&#19968;&#20010;&#22522;&#26412;&#19988;&#32463;&#24120;&#21457;&#29983;&#30340;&#35777;&#25454;&#26159;&#65292;&#24403;&#25968;&#23398;&#38382;&#39064;&#31245;&#20316;&#26356;&#25913;&#26102;&#65292;LLMs&#21487;&#33021;&#20250;&#20986;&#29616;&#38169;&#35823;&#34892;&#20026;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#38382;&#39064;&#21464;&#21270;&#26469;&#35780;&#20272;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25239;&#24335;&#23567;&#23398;&#25968;&#23398;&#65288;\datasetname&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#23545;GSM8K&#30340;&#25193;&#23637;&#65292;&#24182;&#28155;&#21152;&#20102;&#21508;&#31181;&#25968;&#23398;&#25200;&#21160;&#12290;&#25105;&#20204;&#23545;25&#20010;LLMs&#21644;4&#31181;&#25552;&#31034;&#25216;&#26415;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#34429;&#28982;LLMs&#23637;&#29616;&#20986;&#19981;&#21516;&#27700;&#24179;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#34920;&#29616;&#36828;&#38750;&#31283;&#20581;&#12290;&#29305;&#21035;&#26159;&#65292;&#21363;&#20351;&#26159;&#22312;GSM8K&#20013;&#24050;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;LLMs&#20063;&#21487;&#33021;&#20986;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19255v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (\datasetname) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25925;&#20107;&#35762;&#36848;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#25945;&#32946;&#26041;&#27861;&#65292;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#23398;&#20064;&#22797;&#26434;&#30340;&#27861;&#24459;&#27010;&#24565;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#27861;&#24459;&#25925;&#20107;&#21644;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17019</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35762;&#25925;&#20107;&#23398;&#20064;&#22797;&#26434;&#27861;&#24459;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17019
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25925;&#20107;&#35762;&#36848;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#25945;&#32946;&#26041;&#27861;&#65292;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#23398;&#20064;&#22797;&#26434;&#30340;&#27861;&#24459;&#27010;&#24565;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#27861;&#24459;&#25925;&#20107;&#21644;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27861;&#24459;&#30693;&#35782;&#21464;&#24471;&#26356;&#23481;&#26131;&#29702;&#35299;&#23545;&#20110;&#25552;&#21319;&#26222;&#36890;&#27861;&#24459;&#32032;&#20859;&#21644;&#40723;&#21169;&#20844;&#27665;&#21442;&#19982;&#27665;&#20027;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#27809;&#26377;&#27861;&#24459;&#32972;&#26223;&#30340;&#20154;&#26469;&#35828;&#65292;&#27861;&#24459;&#25991;&#20214;&#36890;&#24120;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27861;&#24459;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#65292;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#36890;&#36807;&#35762;&#25925;&#20107;&#23398;&#20064;&#22797;&#26434;&#30340;&#27861;&#24459;&#27010;&#24565;&#65292;&#35762;&#25925;&#20107;&#26159;&#20256;&#36798;&#22797;&#26434;&#21644;&#25277;&#35937;&#27010;&#24565;&#30340;&#26377;&#25928;&#25945;&#23398;&#24037;&#20855;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LegalStories&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;295&#20010;&#22797;&#26434;&#30340;&#27861;&#24459;&#21407;&#21017;&#65292;&#27599;&#20010;&#21407;&#21017;&#37117;&#38468;&#26377;&#19968;&#20010;&#25925;&#20107;&#21644;&#19968;&#32452;&#30001;LLMs&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#20026;&#20102;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#21508;&#31181;LLMs&#29983;&#25104;&#35299;&#37322;&#36825;&#20123;&#27010;&#24565;&#30340;&#27861;&#24459;&#25925;&#20107;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#23478;&#21442;&#19982;&#30340;&#26041;&#27861;&#26469;&#36845;&#20195;&#35774;&#35745;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17019v1 Announce Type: new  Abstract: Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 295 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop method to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through an 
&lt;/p&gt;</description></item><item><title>GraphWiz&#26159;&#19968;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#35299;&#20915;&#21508;&#31181;&#22270;&#38382;&#39064;&#31867;&#22411;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#36229;&#36807;&#20102;GPT-4&#30340;43.8%&#12290;</title><link>https://arxiv.org/abs/2402.16029</link><description>&lt;p&gt;
GraphWiz&#65306;&#29992;&#20110;&#22270;&#38382;&#39064;&#30340;&#25351;&#20196;&#36319;&#38543;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphWiz: An Instruction-Following Language Model for Graph Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16029
&lt;/p&gt;
&lt;p&gt;
GraphWiz&#26159;&#19968;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#35299;&#20915;&#21508;&#31181;&#22270;&#38382;&#39064;&#31867;&#22411;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#36229;&#36807;&#20102;GPT-4&#30340;43.8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#29702;&#35299;&#21644;&#35299;&#20915;&#22797;&#26434;&#22270;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GraphInstruct&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22788;&#29702;&#21508;&#31181;&#22270;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#26126;&#30830;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;&#21033;&#29992;GraphInstruct&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;GraphWiz&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#22270;&#38382;&#39064;&#31867;&#22411;&#24182;&#29983;&#25104;&#28165;&#26224;&#25512;&#29702;&#36807;&#31243;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26694;&#26550;&#32435;&#20837;&#22270;&#38382;&#39064;&#27714;&#35299;&#29615;&#22659;&#20013;&#12290;&#22686;&#24378;&#27169;&#22411;GraphWiz-DPO&#22312;&#20061;&#20010;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24615;&#27700;&#24179;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;65%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;43.8%&#30340;GPT-4&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16029v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#21512;&#25104;&#25351;&#21335;&#26367;&#25442;&#30495;&#23454;&#25351;&#21335;&#20197;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#36890;&#36807;&#31169;&#23494;&#24494;&#35843;&#29983;&#25104;&#22120;&#29983;&#25104;&#27492;&#31867;&#21512;&#25104;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#36807;&#28388;&#31639;&#27861;&#20351;&#21512;&#25104;&#25351;&#21335;&#30340;&#20998;&#24067;&#19982;&#30495;&#23454;&#25351;&#21335;&#19968;&#33268;&#65292;&#23637;&#31034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#39640;&#25928;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13659</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#38544;&#31169;&#20445;&#25252;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Instructions for Aligning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13659
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#21512;&#25104;&#25351;&#21335;&#26367;&#25442;&#30495;&#23454;&#25351;&#21335;&#20197;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#36890;&#36807;&#31169;&#23494;&#24494;&#35843;&#29983;&#25104;&#22120;&#29983;&#25104;&#27492;&#31867;&#21512;&#25104;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#36807;&#28388;&#31639;&#27861;&#20351;&#21512;&#25104;&#25351;&#21335;&#30340;&#20998;&#24067;&#19982;&#30495;&#23454;&#25351;&#21335;&#19968;&#33268;&#65292;&#23637;&#31034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#39640;&#25928;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#30340;&#26381;&#21153;&#25552;&#20379;&#21830;&#22312;&#37326;&#22806;&#25910;&#38598;&#29992;&#25143;&#25351;&#21335;&#65292;&#24182;&#22312;&#36827;&#19968;&#27493;&#23545;&#40784;LLM&#19982;&#29992;&#25143;&#24847;&#22270;&#20013;&#20351;&#29992;&#36825;&#20123;&#25351;&#21335;&#12290;&#36825;&#20123;&#28508;&#22312;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#25351;&#21335;&#22312;&#27969;&#31243;&#20013;&#30001;&#20154;&#24037;&#24037;&#20316;&#32773;&#26631;&#27880;&#12290;&#36825;&#24102;&#26469;&#20102;&#26032;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#32780;Typical Private Optimization&#27809;&#26377;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#21512;&#25104;&#25351;&#21335;&#26367;&#25442;&#25968;&#25454;&#26631;&#27880;&#21644;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#30495;&#23454;&#25351;&#21335;&#12290;&#36890;&#36807;&#20351;&#29992;&#31169;&#23494;&#24494;&#35843;&#29983;&#25104;&#22120;&#29983;&#25104;&#36825;&#20123;&#21512;&#25104;&#25351;&#21335;&#65292;&#21487;&#20197;&#30830;&#20445;&#24418;&#24335;&#24046;&#24322;&#38544;&#31169;&#12290;&#22312;&#23454;&#29616;&#25152;&#38656;&#25928;&#29992;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#25105;&#20204;&#30340;&#26032;&#39062;&#36807;&#28388;&#31639;&#27861;&#65292;&#23558;&#21512;&#25104;&#25351;&#21335;&#30340;&#20998;&#24067;&#19982;&#23454;&#38469;&#25351;&#21335;&#30340;&#20998;&#24067;&#36827;&#34892;&#21305;&#37197;&#12290;&#22312;&#26377;&#20154;&#21453;&#39304;&#30340;&#21463;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#23637;&#31034;&#21512;&#25104;&#25351;&#21335;&#30340;&#26368;&#32456;&#38598;&#21512;&#30340;&#39640;&#25928;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13659v1 Announce Type: cross  Abstract: Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by sho
&lt;/p&gt;</description></item><item><title>StyleDubber&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30005;&#24433;&#37197;&#38899;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38899;&#32032;&#32423;&#21035;&#36827;&#34892;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069; V2C &#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#38899;&#32032;&#21457;&#38899;&#19981;&#23436;&#25972;&#21644;&#36523;&#20221;&#31283;&#23450;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12636</link><description>&lt;p&gt;
StyleDubber: &#38754;&#21521;&#30005;&#24433;&#37197;&#38899;&#30340;&#22810;&#23610;&#24230;&#39118;&#26684;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12636
&lt;/p&gt;
&lt;p&gt;
StyleDubber&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30005;&#24433;&#37197;&#38899;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38899;&#32032;&#32423;&#21035;&#36827;&#34892;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069; V2C &#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#38899;&#32032;&#21457;&#38899;&#19981;&#23436;&#25972;&#21644;&#36523;&#20221;&#31283;&#23450;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20221;&#21095;&#26412;&#65292;&#22312;&#30005;&#24433;&#37197;&#38899;&#65288;&#35270;&#35273;&#35821;&#38899;&#20811;&#38534;&#65292;V2C&#65289;&#20013;&#30340;&#25361;&#25112;&#26159;&#26681;&#25454;&#21442;&#32771;&#38899;&#36712;&#30340;&#35821;&#27668;&#65292;&#29983;&#25104;&#19982;&#35270;&#39057;&#22312;&#26102;&#38388;&#21644;&#24773;&#32490;&#19978;&#37117;&#33391;&#22909;&#23545;&#40784;&#30340;&#35821;&#38899;&#12290;&#29616;&#26377;&#30340; V2C &#27169;&#22411;&#26681;&#25454;&#35270;&#39057;&#24103;&#38388;&#30340;&#38388;&#38548;&#23383;&#26029;&#20998;&#21106;&#21095;&#26412;&#30340;&#38899;&#32032;&#65292;&#36825;&#35299;&#20915;&#20102;&#26102;&#38388;&#23545;&#40784;&#38382;&#39064;&#65292;&#20294;&#23548;&#33268;&#38899;&#32032;&#21457;&#38899;&#19981;&#23436;&#25972;&#21644;&#36523;&#20221;&#31283;&#23450;&#24615;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986; StyleDubber&#65292;&#23427;&#23558;&#37197;&#38899;&#23398;&#20064;&#20174;&#24103;&#32423;&#21035;&#36716;&#20026;&#38899;&#32032;&#32423;&#21035;&#12290;&#23427;&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22810;&#27169;&#24577;&#39118;&#26684;&#36866;&#37197;&#22120;&#65292;&#20197;&#38899;&#32032;&#32423;&#21035;&#25805;&#20316;&#65292;&#20174;&#21442;&#32771;&#38899;&#39057;&#20013;&#23398;&#20064;&#21457;&#38899;&#39118;&#26684;&#65292;&#24182;&#29983;&#25104;&#21463;&#35270;&#39057;&#20013;&#21576;&#29616;&#30340;&#38754;&#37096;&#24773;&#32490;&#24433;&#21709;&#30340;&#20013;&#38388;&#34920;&#31034;&#65307;&#65288;2&#65289;&#19968;&#20010;&#20197;&#35821;&#21477;&#32423;&#21035;&#39118;&#26684;&#23398;&#20064;&#27169;&#22359;&#65292;&#24341;&#23548;&#20013;&#38388;&#34920;&#29616;&#30340; mel-spectrogram &#35299;&#30721;&#21644;&#32454;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12636v1 Announce Type: new  Abstract: Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track. Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability. To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level. It contains three main components: (1) A multimodal style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate e
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#24895;&#26223;&#65292;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20026;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.12327</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#20132;&#27969;&#21527;&#65306;&#25506;&#32034;&#31454;&#20105;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21457;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#24895;&#26223;&#65292;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20026;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#20195;&#29702;&#20855;&#26377;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21644;&#31038;&#20250;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#30740;&#31350;LLM&#20195;&#29702;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#33258;&#21457;&#24314;&#31435;&#21512;&#20316;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#19981;&#20165;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#31454;&#20105;&#19982;&#21512;&#20316;&#30340;&#33021;&#21147;&#65292;&#20063;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#24895;&#26223;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#37027;&#20123;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#31038;&#20250;&#29616;&#35937;&#30340;&#27934;&#23519;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/wuzengqing001225/SABM_ShallWe &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12327v1 Announce Type: new  Abstract: Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics. However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied. To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings. This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science. Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena. The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#19982;&#27169;&#22411;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#25552;&#21462;&#19981;&#21516;&#29305;&#24449;&#26469;&#39044;&#27979;Transformer&#25991;&#26412;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11469</link><description>&lt;p&gt;
&#22312;&#25628;&#32034;&#35757;&#32451;&#25968;&#25454;&#19982;Transformer&#25991;&#26412;&#27169;&#22411;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26102;&#30340;&#19968;&#20010;&#26377;&#36259;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#19982;&#27169;&#22411;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#25552;&#21462;&#19981;&#21516;&#29305;&#24449;&#26469;&#39044;&#27979;Transformer&#25991;&#26412;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#25991;&#26412;Transformer&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25991;&#26412;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;&#23545;&#25239;&#24615;&#35780;&#20272;&#36890;&#24120;&#22312;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20043;&#21518;&#25165;&#36827;&#34892;&#65292;&#24573;&#30053;&#20102;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#26088;&#22312;&#35777;&#26126;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#20043;&#38388;&#20063;&#23384;&#22312;&#30528;&#24378;&#20851;&#32852;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20195;&#34920;&#24191;&#27867;&#36755;&#20837;&#24494;&#35843;&#35821;&#26009;&#24211;&#23646;&#24615;&#30340;13&#31181;&#19981;&#21516;&#29305;&#24449;&#65292;&#24182;&#29992;&#23427;&#20204;&#26469;&#39044;&#27979;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20165;&#32534;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;BERT&#21644;RoBERTa&#65292;&#24182;&#38468;&#21152;&#20102;BART&#12289;ELECTRA&#21644;GPT2&#30340;&#20854;&#20182;&#32467;&#26524;&#65292;&#20026;&#25105;&#20204;&#30340;&#35770;&#28857;&#25552;&#20379;&#22810;&#26679;&#30340;&#35777;&#25454;&#12290;&#39318;&#20808;&#65292;&#32463;&#39564;&#35777;&#26126;&#65292;(a)&#25552;&#21462;&#30340;&#29305;&#24449;&#21487;&#19982;&#36731;&#37327;&#32423;&#20998;&#31867;&#22120;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#65289;&#19968;&#36215;&#26377;&#25928;&#22320;&#39044;&#27979;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11469v1 Announce Type: cross  Abstract: Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations. Traditional adversarial evaluation is often done \textit{only after} fine-tuning the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models. Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#33258;&#21160;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19979;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#24040;&#22823;&#21464;&#21270;&#65292;&#19988;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2402.10770</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#22312;&#38754;&#21521;&#25351;&#20196;&#30340;LLM&#20013;&#26377;&#22810;&#21487;&#38752;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#33258;&#21160;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19979;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#24040;&#22823;&#21464;&#21270;&#65292;&#19988;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#25991;&#26412;&#37325;&#21472;&#21644;LLM&#21028;&#26029;&#30340;&#33258;&#21160;&#26041;&#27861;&#20316;&#20026;&#20154;&#24037;&#35780;&#20272;&#30340;&#25104;&#26412;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#21644;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20219;&#21153;&#31867;&#22411;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#21160;&#26041;&#27861;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#26174;&#33879;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;ROUGE-L&#24230;&#37327;&#22312;&#30701;&#31572;&#26696;&#33521;&#35821;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21028;&#26029;&#24378;&#30456;&#20851;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#19981;&#21487;&#38752;&#12290;&#20351;&#29992;GPT-4&#20316;&#20026;&#35780;&#20272;&#21592;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#22312;&#35201;&#27714;&#35780;&#20272;&#26102;&#21253;&#21547;&#21442;&#32771;&#31572;&#26696;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#35780;&#20272;&#36807;&#20110;&#20005;&#26684;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21487;&#20197;&#36817;&#20284;&#20154;&#31867;&#21028;&#26029;&#65292;&#20294;&#20854;&#20934;&#30830;&#24615;&#21487;&#33021;&#22240;&#20219;&#21153;&#31867;&#22411;&#21644;&#35780;&#20272;&#35774;&#32622;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10770v1 Announce Type: cross  Abstract: Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we study the reliability of such methods across a broad range of tasks and in a cross-lingual setting. In contrast to previous findings, we observe considerable variability in correlations between automatic methods and human evaluators when scores are differentiated by task type. Specifically, the widely-used ROUGE-L metric strongly correlates with human judgments for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of GPT-4 as an evaluator depends on including reference answers when prompting for assessments, which can lead to overly strict evaluations in free-form generation tasks. In summary, we find that, while automatic evaluation methods can approximate human judgements und
&lt;/p&gt;</description></item><item><title>HGOT&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#20998;&#23618;&#24605;&#32500;&#22270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#24605;&#32500;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#26469;&#25552;&#39640;&#30456;&#20851;&#27573;&#33853;&#30340;&#26816;&#32034;&#21644;&#31572;&#26696;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.09390</link><description>&lt;p&gt;
HGOT: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#20998;&#23618;&#24605;&#32500;&#22270;
&lt;/p&gt;
&lt;p&gt;
HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09390
&lt;/p&gt;
&lt;p&gt;
HGOT&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#20998;&#23618;&#24605;&#32500;&#22270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#24605;&#32500;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#26469;&#25552;&#39640;&#30456;&#20851;&#27573;&#33853;&#30340;&#26816;&#32034;&#21644;&#31572;&#26696;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20107;&#23454;&#24615;&#21644;&#24187;&#35273;&#30340;&#20542;&#21521;&#24341;&#21457;&#20102;&#37325;&#22823;&#20851;&#20999;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26816;&#32034;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#24605;&#32500;&#22270;&#65288;HGOT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#12289;&#22810;&#23618;&#27425;&#30340;&#22270;&#24418;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#20013;&#30456;&#20851;&#27573;&#33853;&#30340;&#26816;&#32034;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;LLMs&#30340;&#36880;&#28176;&#35268;&#21010;&#33021;&#21147;&#65292;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#23558;&#22797;&#26434;&#26597;&#35810;&#20998;&#35299;&#20026;&#21487;&#22788;&#29702;&#30340;&#23376;&#26597;&#35810;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#26368;&#36817;&#25552;&#20986;&#30340;&#24341;&#25991;&#22238;&#24518;&#21644;&#31934;&#30830;&#24230;&#25351;&#26631;&#26469;&#35780;&#20272;&#24605;&#32500;&#36136;&#37327;&#65292;&#23558;&#31572;&#26696;&#30340;&#21487;&#20449;&#24230;&#19982;&#24605;&#32500;&#30340;&#36136;&#37327;&#20869;&#22312;&#22320;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#33258;&#27965;&#24615;&#22810;&#25968;&#25237;&#31080;&#30340;&#31572;&#26696;&#36873;&#25321;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#21152;&#26435;&#31995;&#32479;&#65292;&#22312;&#22810;&#25968;&#25237;&#31080;&#20013;&#20248;&#20808;&#32771;&#34385;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09390v1 Announce Type: new Abstract: With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns. To address this issue, particularly in retrieval-augmented in-context learning, we introduce the hierarchical graph of thoughts (HGOT), a structured, multi-layered graph approach designed to enhance the retrieval of pertinent passages during in-context learning. The framework utilizes the emergent planning capabilities of LLMs, employing the divide-and-conquer strategy to break down complex queries into manageable sub-queries. It refines self-consistency majority voting for answer selection, which incorporates the recently proposed citation recall and precision metrics to assess the quality of thoughts, linking an answer's credibility intrinsically to the thought's quality. This methodology introduces a weighted system in majority voting, prioritizing answers 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.08787</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Rethinking Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65288;MU&#65289;&#65292;&#31216;&#20026;LLM&#28040;&#38500;&#25216;&#26415;&#12290;&#36825;&#20010;&#30740;&#31350;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#25935;&#24863;&#25110;&#38750;&#27861;&#20449;&#24687;&#65289;&#20197;&#21450;&#30456;&#20851;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#22522;&#26412;&#30340;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#24182;&#19981;&#24433;&#21709;&#22240;&#26524;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35774;&#24819;LLM&#28040;&#38500;&#25216;&#26415;&#23558;&#25104;&#20026;LLM&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#21487;&#33021;&#25104;&#20026;&#24320;&#21457;&#26082;&#23433;&#20840;&#12289;&#21487;&#38752;&#21448;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#20840;&#37325;&#35757;&#32451;&#12290;&#25105;&#20204;&#20174;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#24212;&#29992;&#31561;&#26041;&#38754;&#25506;&#32034;&#20102;LLM&#28040;&#38500;&#25216;&#26415;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#29616;&#26377;LLM&#28040;&#38500;&#25216;&#26415;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#20363;&#22914;&#28040;&#38500;&#33539;&#22260;&#12289;&#25968;&#25454;&#27169;&#22411;&#20132;&#20114;&#21644;&#22810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#25351;&#23548;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#37096;&#20998;&#22240;&#26524;&#20851;&#31995;&#30340;&#20107;&#20214;&#27010;&#24565;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#22635;&#34917;&#30693;&#35782;&#31354;&#30333;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2401.07237</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distilling Event Sequence Knowledge From Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#25351;&#23548;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#37096;&#20998;&#22240;&#26524;&#20851;&#31995;&#30340;&#20107;&#20214;&#27010;&#24565;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#22635;&#34917;&#30693;&#35782;&#31354;&#30333;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#22312;&#20107;&#20214;&#30340;&#20998;&#26512;&#21644;&#39044;&#27979;&#20013;&#34987;&#21457;&#29616;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;&#24314;&#31435;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#20016;&#23500;&#30340;&#39640;&#36136;&#37327;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#24178;&#20928;&#30340;&#32467;&#26500;&#21270;&#20107;&#20214;&#24207;&#21015;&#19981;&#21487;&#29992;&#65292;&#33258;&#21160;&#21270;&#24207;&#21015;&#25552;&#21462;&#23548;&#33268;&#30340;&#25968;&#25454;&#22826;&#22024;&#26434;&#21644;&#19981;&#23436;&#25972;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#27010;&#29575;&#20107;&#20214;&#27169;&#22411;&#26500;&#24314;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#20174;LLMs&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#30340;&#19968;&#31181;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#20855;&#26377;&#37096;&#20998;&#22240;&#26524;&#20851;&#31995;&#30340;&#20107;&#20214;&#27010;&#24565;&#30340;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#26469;&#25351;&#23548;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22240;&#26524;&#20107;&#20214;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;&#36755;&#20837;KG&#20013;&#30340;&#30693;&#35782;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#30340;&#24207;&#21015;&#26469;&#21457;&#29616;&#26356;&#26377;&#29992;&#21644;&#26356;&#22797;&#26434;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#30524;&#21160;&#36861;&#36394;&#25216;&#26415;&#30740;&#31350;&#20102;&#27597;&#35821;&#20026;&#27721;&#35821;&#30340;&#20154;&#23545;&#35821;&#35328;&#26223;&#35266;&#30340;&#27880;&#24847;&#28857;&#65292;&#21457;&#29616;&#20854;&#20851;&#27880;&#31243;&#24230;&#39640;&#20110;&#19968;&#33324;&#26223;&#35266;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#35821;&#35328;&#26223;&#35266;&#30340;&#20449;&#24687;&#23494;&#24230;&#36739;&#39640;&#12290;</title><link>https://arxiv.org/abs/2312.08906</link><description>&lt;p&gt;
&#20351;&#29992;&#30524;&#21160;&#36861;&#36394;&#30740;&#31350;&#27597;&#35821;&#20026;&#27721;&#35821;&#30340;&#20154;&#23545;&#35821;&#35328;&#26223;&#35266;&#22270;&#20687;&#30340;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Using eye tracking to investigate what native Chinese speakers notice about linguistic landscape images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#30524;&#21160;&#36861;&#36394;&#25216;&#26415;&#30740;&#31350;&#20102;&#27597;&#35821;&#20026;&#27721;&#35821;&#30340;&#20154;&#23545;&#35821;&#35328;&#26223;&#35266;&#30340;&#27880;&#24847;&#28857;&#65292;&#21457;&#29616;&#20854;&#20851;&#27880;&#31243;&#24230;&#39640;&#20110;&#19968;&#33324;&#26223;&#35266;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#35821;&#35328;&#26223;&#35266;&#30340;&#20449;&#24687;&#23494;&#24230;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26223;&#35266;&#26159;&#31038;&#20250;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#30524;&#21160;&#36861;&#36394;&#25216;&#26415;&#26159;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#25216;&#26415;&#12290;&#22312;&#23545;&#35821;&#35328;&#26223;&#35266;&#30340;&#30740;&#31350;&#20013;&#65292;&#24456;&#23569;&#20351;&#29992;&#30524;&#21160;&#36861;&#36394;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#21033;&#29992;&#30524;&#21160;&#36861;&#36394;&#25216;&#26415;&#30740;&#31350;&#20102;&#27597;&#35821;&#20026;&#27721;&#35821;&#30340;&#20154;&#23545;&#35821;&#35328;&#26223;&#35266;&#30340;&#23454;&#38469;&#27880;&#24847;&#28857;&#65292;&#24182;&#21457;&#29616;&#22312;&#20957;&#35270;&#26102;&#38388;&#21644;&#20957;&#35270;&#27425;&#25968;&#36825;&#20004;&#20010;&#32500;&#24230;&#19978;&#65292;&#27597;&#35821;&#20026;&#27721;&#35821;&#30340;&#20154;&#23545;&#35821;&#35328;&#26223;&#35266;&#30340;&#20851;&#27880;&#31243;&#24230;&#39640;&#20110;&#19968;&#33324;&#26223;&#35266;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#35821;&#35328;&#26223;&#35266;&#30340;&#20449;&#24687;&#23494;&#24230;&#36739;&#39640;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#36825;&#19968;&#29616;&#35937;&#30340;&#20854;&#20182;&#21487;&#33021;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linguistic landscape is an important field in sociolinguistic research. Eye tracking technology is a common technology in psychological research. There are few cases of using eye movement to study linguistic landscape. This paper uses eye tracking technology to study the actual fixation of the linguistic landscape and finds that in the two dimensions of fixation time and fixation times, the fixation of native Chinese speakers to the linguistic landscape is higher than that of the general landscape. This paper argues that this phenomenon is due to the higher information density of linguistic landscapes. At the same time, the article also discusses other possible reasons for this phenomenon.
&lt;/p&gt;</description></item><item><title>AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.12963</link><description>&lt;p&gt;
AutoRT&#65306;&#22823;&#35268;&#27169;&#32534;&#25490;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12963
&lt;/p&gt;
&lt;p&gt;
AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#34892;&#21160;&#31561;&#21151;&#33021;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#21033;&#29992;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#26469;&#25512;&#29702;&#26377;&#29992;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32570;&#20047;&#22522;&#20110;&#29289;&#29702;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoRT&#65292;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#23436;&#20840;&#26410;&#30693;&#22330;&#26223;&#20013;&#25805;&#20316;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#24037;&#30417;&#30563;&#12290;AutoRT&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#23454;&#29616;&#22330;&#26223;&#29702;&#35299;&#21644;&#22522;&#30784;&#32465;&#23450;&#65292;&#24182;&#36827;&#19968;&#27493;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#20379;&#19968;&#32452;&#26426;&#22120;&#20154;&#25191;&#34892;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#25351;&#23548;&#25968;&#25454;&#25910;&#38598;&#65292;AutoRT&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#26174;&#33879;&#25193;&#22823;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;AutoRT&#21521;20&#22810;&#20010;&#26426;&#22120;&#20154;&#25552;&#35758;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#36755;&#20986;&#24182;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#65292;&#21033;&#29992;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#36873;&#25321;&#26368;&#20248;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2309.17272</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#20013;&#30340;&#33021;&#21147;&#36890;&#36807;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency. (arXiv:2309.17272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#36755;&#20986;&#24182;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#65292;&#21033;&#29992;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#36873;&#25321;&#26368;&#20248;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#22914;&#20195;&#30721;&#29983;&#25104;&#20013;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#22312;&#19968;&#27425;&#23581;&#35797;&#20013;&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#36755;&#20986;&#65292;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#27809;&#26377;&#20840;&#38754;&#22320;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25429;&#25417;&#36825;&#31181;&#19968;&#33268;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#26694;&#26550;&#30340;&#26032;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;LLM&#65292;&#23427;&#23558;&#26469;&#33258;&#22810;&#20010;&#35282;&#24230;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#21333;&#20010;&#35282;&#24230;&#20869;&#30340;&#20869;&#19968;&#33268;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35201;&#27714;LLMs&#23545;&#32473;&#23450;&#26597;&#35810;&#20174;&#21508;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#12290;&#36890;&#36807;&#20004;&#20010;&#39044;&#23450;&#20041;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#25105;&#20204;&#23558;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#12290;&#26368;&#20339;&#36873;&#25321;&#26159;&#26681;&#25454;&#36825;&#20123;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#36873;&#25321;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable ability in textual generation. However, in complex reasoning tasks such as code generation, generating the correct answer in a single attempt remains a formidable challenge for LLMs. Previous research has explored solutions by aggregating multiple outputs, leveraging the consistency among them. However, none of them have comprehensively captured this consistency from different perspectives. In this paper, we propose the Multi-Perspective Self-Consistency (MPSC) framework, a novel decoding strategy for LLM that incorporates both inter-consistency across outputs from multiple perspectives and intra-consistency within a single perspective. Specifically, we ask LLMs to sample multiple diverse outputs from various perspectives for a given query and then construct a multipartite graph based on them. With two predefined measures of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice is th
&lt;/p&gt;</description></item><item><title>STANCE-C3&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21453;&#20107;&#23454;&#29983;&#25104;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#25512;&#26029;&#20154;&#20204;&#23545;&#20110;&#26222;&#36941;&#25110;&#26377;&#20105;&#35758;&#35805;&#39064;&#30340;&#35266;&#28857;&#12290;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#21644;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#26631;&#27880;&#25968;&#25454;&#30340;&#25361;&#25112;&#19978;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.15176</link><description>&lt;p&gt;
STANCE-C3: &#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21453;&#20107;&#23454;&#29983;&#25104;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
STANCE-C3: Domain-adaptive Cross-target Stance Detection via Contrastive Learning and Counterfactual Generation. (arXiv:2309.15176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15176
&lt;/p&gt;
&lt;p&gt;
STANCE-C3&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21453;&#20107;&#23454;&#29983;&#25104;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#25512;&#26029;&#20154;&#20204;&#23545;&#20110;&#26222;&#36941;&#25110;&#26377;&#20105;&#35758;&#35805;&#39064;&#30340;&#35266;&#28857;&#12290;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#21644;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#26631;&#27880;&#25968;&#25454;&#30340;&#25361;&#25112;&#19978;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#26159;&#36890;&#36807;&#25512;&#26029;&#19968;&#20010;&#20154;&#22312;&#29305;&#23450;&#38382;&#39064;&#19978;&#30340;&#31435;&#22330;&#25110;&#35266;&#28857;&#65292;&#20197;&#25512;&#26029;&#23545;&#20110;&#26222;&#36941;&#25110;&#26377;&#20105;&#35758;&#30340;&#35805;&#39064;&#30340;&#26222;&#36941;&#30475;&#27861;&#65292;&#20363;&#22914;COVID-19&#30123;&#24773;&#26399;&#38388;&#30340;&#20581;&#24247;&#25919;&#31574;&#12290;&#29616;&#26377;&#30340;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#22312;&#35757;&#32451;&#26102;&#24448;&#24448;&#22312;&#21333;&#20010;&#39046;&#22495;&#65288;&#20363;&#22914;COVID-19&#65289;&#21644;&#29305;&#23450;&#30446;&#26631;&#35805;&#39064;&#65288;&#20363;&#22914;&#21475;&#32617;&#35268;&#23450;&#65289;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20854;&#20182;&#39046;&#22495;&#25110;&#30446;&#26631;&#20013;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#39046;&#22495;&#29305;&#23450;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#20851;&#30340;&#24050;&#26631;&#27880;&#25968;&#25454;&#65292;&#20294;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#19981;&#23481;&#26131;&#33719;&#21462;&#12290;&#36825;&#23601;&#38754;&#20020;&#30528;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#26631;&#27880;&#25968;&#25454;&#30340;&#36807;&#31243;&#20195;&#20215;&#39640;&#26114;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#65292;&#31216;&#20026;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21453;&#20107;&#23454;&#29983;&#25104;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#65288;STANCE-C3&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stance detection is the process of inferring a person's position or standpoint on a specific issue to deduce prevailing perceptions toward topics of general or controversial interest, such as health policies during the COVID-19 pandemic. Existing models for stance detection are trained to perform well for a single domain (e.g., COVID-19) and a specific target topic (e.g., masking protocols), but are generally ineffectual in other domains or targets due to distributional shifts in the data. However, constructing high-performing, domain-specific stance detection models requires an extensive corpus of labeled data relevant to the targeted domain, yet such datasets are not readily available. This poses a challenge as the process of annotating data is costly and time-consuming. To address these challenges, we introduce a novel stance detection model coined domain-adaptive Cross-target STANCE detection via Contrastive learning and Counterfactual generation (STANCE-C3) that uses counterfactua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#33258;&#36523;&#30693;&#35782;&#30340;&#29702;&#35299;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#35299;&#20915;&#8220;&#24050;&#30693;-&#26410;&#30693;&#8221;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#35821;&#20041;&#35780;&#20272;&#26041;&#27861;&#37327;&#21270;&#20102;&#27169;&#22411;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13712</link><description>&lt;p&gt;
&#30693;&#35782;&#30340;&#30693;&#35782;&#65306;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;-&#24050;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models. (arXiv:2305.13712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#33258;&#36523;&#30693;&#35782;&#30340;&#29702;&#35299;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#35299;&#20915;&#8220;&#24050;&#30693;-&#26410;&#30693;&#8221;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#35821;&#20041;&#35780;&#20272;&#26041;&#27861;&#37327;&#21270;&#20102;&#27169;&#22411;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29702;&#35299;&#33258;&#36523;&#30693;&#35782;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#32531;&#35299;&#34394;&#26500;&#29616;&#35937;&#12290;&#25105;&#20204;&#19987;&#38376;&#20851;&#27880;&#35299;&#20915;&#8220;&#24050;&#30693;-&#26410;&#30693;&#8221;&#38382;&#39064;&#65292;&#36825;&#31181;&#38382;&#39064;&#30001;&#20110;&#32570;&#20047;&#30830;&#23450;&#30340;&#31572;&#26696;&#32780;&#20855;&#26377;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#24050;&#30693;-&#26410;&#30693;&#38382;&#39064;&#65288;KUQ&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#26469;&#38416;&#26126;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;LLM&#21306;&#20998;&#24050;&#30693;&#21644;&#26410;&#30693;&#38382;&#39064;&#20197;&#21450;&#30456;&#24212;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#24320;&#25918;&#24335;QA&#29615;&#22659;&#20013;&#35780;&#20272;LLM&#30340;&#31572;&#26696;&#36136;&#37327;&#12290;&#20026;&#20102;&#37327;&#21270;&#31572;&#26696;&#20013;&#34920;&#36798;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#35821;&#20041;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#27169;&#22411;&#22312;&#34920;&#36798;&#24050;&#30693;vs&#26410;&#30693;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their own knowledge and measuring their uncertainty. We argue this is an important feature for mitigating hallucinations. Specifically, we focus on addressing \textit{known-unknown} questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a dataset with new Known-Unknown Questions (KUQ) and propose a novel categorization scheme to elucidate the sources of uncertainty. Subsequently, we assess the LLMs' ability to differentiate between known and unknown questions and classify them accordingly. Moreover, we evaluate the quality of their answers in an Open-Ended QA setting. To quantify the uncertainty expressed in the answers, we create a semantic evaluation method that measures the model's accuracy in expressing uncertainty between known vs unknown questions.
&lt;/p&gt;</description></item></channel></rss>