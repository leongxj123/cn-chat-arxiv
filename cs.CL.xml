<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;RoleInteract&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#31038;&#20132;&#24615;&#30340;&#22522;&#20934;&#65292;&#35206;&#30422;&#20102;500&#20010;&#35282;&#33394;&#12289;6000&#22810;&#20010;&#38382;&#39064;&#25552;&#31034;&#21644;30800&#20010;&#23545;&#35805;&#35805;&#35821;&#12290;</title><link>https://arxiv.org/abs/2403.13679</link><description>&lt;p&gt;
RoleInteract&#65306;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#20195;&#29702;&#30340;&#31038;&#20132;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
RoleInteract: Evaluating the Social Interaction of Role-Playing Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13679
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;RoleInteract&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#31038;&#20132;&#24615;&#30340;&#22522;&#20934;&#65292;&#35206;&#30422;&#20102;500&#20010;&#35282;&#33394;&#12289;6000&#22810;&#20010;&#38382;&#39064;&#25552;&#31034;&#21644;30800&#20010;&#23545;&#35805;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#20102;&#21508;&#31181;AI&#23545;&#35805;&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#27169;&#20223;&#19981;&#21516;&#35282;&#33394;&#21644;&#20154;&#31867;&#34892;&#20026;&#30340;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;RoleInteract&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#22312;&#31038;&#20132;&#26041;&#38754;&#34920;&#29616;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#20174;&#21508;&#31181;&#26469;&#28304;&#26500;&#24314;&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;500&#20010;&#35282;&#33394;&#12289;6000&#22810;&#20010;&#38382;&#39064;&#25552;&#31034;&#21644;30800&#20010;&#22810;&#36718;&#35282;&#33394;&#25198;&#28436;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13679v1 Announce Type: new  Abstract: Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs. We find that agents excelling in in
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#65292;&#22312;&#21508;&#31181;&#32422;&#26463;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.13244</link><description>&lt;p&gt;
&#20351;&#29992;&#24072;&#29983;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13244
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#65292;&#22312;&#21508;&#31181;&#32422;&#26463;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#27169;&#22411;&#21644;&#35745;&#31639;&#24037;&#20855;&#29992;&#20110;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#20998;&#26512;&#65292;&#20294;&#29983;&#25104;&#31526;&#21512;&#25152;&#26377;&#26399;&#26395;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#20998;&#23376;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#31867;&#20284;&#20110;&#23398;&#29983;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#26469;&#33258;&#21508;&#31181;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#65288;&#21363;&#8220;&#32769;&#24072;&#8221;&#65289;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35757;&#32451;TSMMG&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#36825;&#20123;&#8216;&#32769;&#24072;&#8217;&#20013;&#25552;&#21462;&#30340;&#20998;&#23376;&#30693;&#35782;&#26500;&#24314;&#20102;&#22823;&#37327;&#25991;&#26412;-&#20998;&#23376;&#23545;&#65292;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#21508;&#31181;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;TSMMG&#22312;&#29983;&#25104;&#31526;&#21512;&#22797;&#26434;&#12289;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20004;&#12289;&#19977;&#21644;&#22235;&#32422;&#26463;&#20219;&#21153;&#30340;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24179;&#22343;&#20998;&#23376;&#26377;&#25928;&#24615;&#36229;&#36807;99&#65285;&#65292;&#25104;&#21151;&#29575;&#20998;&#21035;&#20026;88.08&#65285;&#12289;65.27&#65285;&#21644;61.44&#65285;&#12290;&#35813;&#27169;&#22411;&#36824;ex
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13244v1 Announce Type: new  Abstract: While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge. Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'. To train TSMMG, we construct a large set of text-molecule pairs by extracting molecular knowledge from these 'teachers', enabling it to generate novel molecules that conform to the descriptions through various text prompts. We experimentally show that TSMMG remarkably performs in generating molecules meeting complex, natural language-described property requirements across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 88.08%, 65.27%, and 61.44%, respectively. The model also ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30828;&#36127;&#26679;&#26412;&#25913;&#36827;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#27010;&#24565;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#39068;&#33394;&#12289;&#23545;&#35937;&#21644;&#22823;&#23567;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.02875</link><description>&lt;p&gt;
&#36890;&#36807;&#30828;&#36127;&#26679;&#26412;&#22686;&#24378;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#27010;&#24565;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30828;&#36127;&#26679;&#26412;&#25913;&#36827;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#27010;&#24565;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#39068;&#33394;&#12289;&#23545;&#35937;&#21644;&#22823;&#23567;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#21457;&#23637;&#31934;&#32454;&#30340;&#27010;&#24565;&#29702;&#35299;&#26041;&#38754;&#36890;&#24120;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#38543;&#26426;&#36127;&#26679;&#26412;&#65292;&#23548;&#33268;&#20960;&#20046;&#21482;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#27010;&#24565;&#36827;&#34892;&#25439;&#22833;&#20989;&#25968;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#32454;&#31890;&#24230;&#35821;&#20041;&#24046;&#24322;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21512;&#25104;&#30340;&#30828;&#36127;&#25991;&#26412;&#31034;&#20363;&#12290;&#36825;&#20123;&#30828;&#36127;&#26679;&#26412;&#23545;&#24212;&#20110;&#35270;&#35273;&#27010;&#24565;&#30340;&#25490;&#21015;&#65292;&#23548;&#33268;&#26356;&#31934;&#32454;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#27010;&#24565;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InpaintCOCO&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#39068;&#33394;&#12289;&#23545;&#35937;&#21644;&#22823;&#23567;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#26032;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;COCO&#22270;&#20687;&#29983;&#25104;&#30340;&#20449;&#24687;&#22635;&#20805;&#26469;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25913;&#21464;&#35270;&#35273;&#27010;&#24565;&#65292;&#20351;&#22270;&#20687;&#19981;&#20877;&#19982;&#20854;&#21407;&#22987;&#26631;&#39064;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02875v1 Announce Type: cross  Abstract: Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show sig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;InjecAgent&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#24037;&#20855;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#23545;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;30&#31181;LLM&#20195;&#29702;&#65292;&#21457;&#29616;&#36825;&#20123;&#20195;&#29702;&#23384;&#22312;&#28431;&#27934;</title><link>https://arxiv.org/abs/2403.02691</link><description>&lt;p&gt;
InjecAgent&#65306;&#22522;&#20110;&#24037;&#20855;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#20013;&#30340;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;InjecAgent&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#24037;&#20855;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#23545;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;30&#31181;LLM&#20195;&#29702;&#65292;&#21457;&#29616;&#36825;&#20123;&#20195;&#29702;&#23384;&#22312;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;LLMs&#20316;&#20026;&#20195;&#29702;&#20307;&#29616;&#20986;&#26469;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#35775;&#38382;&#24037;&#20855;&#65292;&#25191;&#34892;&#25805;&#20316;&#65292;&#24182;&#19982;&#22806;&#37096;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#30005;&#23376;&#37038;&#20214;&#25110;&#32593;&#31449;&#65289;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22806;&#37096;&#20869;&#23481;&#24341;&#20837;&#20102;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#65288;IPI&#65289;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24694;&#24847;&#25351;&#20196;&#34987;&#23884;&#20837;LLMs&#22788;&#29702;&#30340;&#20869;&#23481;&#20013;&#65292;&#26088;&#22312;&#25805;&#32437;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#23545;&#29992;&#25143;&#26377;&#23475;&#30340;&#25805;&#20316;&#12290;&#32771;&#34385;&#21040;&#36825;&#31867;&#25915;&#20987;&#30340;&#28508;&#22312;&#20005;&#37325;&#21518;&#26524;&#65292;&#24314;&#31435;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#22522;&#20934;&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InjecAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#24037;&#20855;&#38598;&#25104;&#30340;LLM&#20195;&#29702;&#23545;IPI&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;InjecAgent&#21253;&#25324;1,054&#20010;&#27979;&#35797;&#29992;&#20363;&#65292;&#28085;&#30422;17&#31181;&#19981;&#21516;&#30340;&#29992;&#25143;&#24037;&#20855;&#21644;62&#31181;&#25915;&#20987;&#32773;&#24037;&#20855;&#12290;&#25105;&#20204;&#23558;&#25915;&#20987;&#24847;&#22270;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31867;&#22411;&#65306;&#23545;&#29992;&#25143;&#36896;&#25104;&#30452;&#25509;&#20260;&#23475;&#21644;&#31363;&#21462;&#31169;&#20154;&#25968;&#25454;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;30&#31181;&#19981;&#21516;&#30340;LLM&#20195;&#29702;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#20195;&#29702;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02691v1 Announce Type: new  Abstract: Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative.   In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#33258;&#28982;&#31354;&#38388;&#25551;&#36848;&#36827;&#34892;&#22810;&#23610;&#24230;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#36890;&#36807;&#33719;&#30693;&#22320;&#22270;&#30693;&#35782;&#24471;&#21040;&#30340;&#25551;&#36848;&#33021;&#22815;&#25552;&#20379;&#29615;&#22659;&#30340;&#25972;&#20307;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.16364</link><description>&lt;p&gt;
&#20174;&#21738;&#37324;&#20986;&#21457;&#65311;&#26469;&#33258;&#33258;&#28982;&#31354;&#38388;&#25551;&#36848;&#20013;&#30340;&#22810;&#23610;&#24230;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16364
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#33258;&#28982;&#31354;&#38388;&#25551;&#36848;&#36827;&#34892;&#22810;&#23610;&#24230;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#36890;&#36807;&#33719;&#30693;&#22320;&#22270;&#30693;&#35782;&#24471;&#21040;&#30340;&#25551;&#36848;&#33021;&#22815;&#25552;&#20379;&#29615;&#22659;&#30340;&#25972;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29992;&#33258;&#28982;&#35821;&#35328;&#20256;&#36798;&#36335;&#32447;&#26102;&#65292;&#8220;&#33719;&#24471;&#30340;&#31354;&#38388;&#30693;&#35782;&#8221;&#27010;&#24565;&#23545;&#22320;&#29702;&#20449;&#24687;&#26816;&#32034;&#65288;GIR&#65289;&#21644;&#31354;&#38388;&#35748;&#30693;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23548;&#33322;&#30740;&#31350;&#32463;&#24120;&#24573;&#35270;&#36825;&#31181;&#33719;&#24471;&#30693;&#35782;&#23545;&#25991;&#26412;&#25551;&#36848;&#30340;&#24433;&#21709;&#12290;&#24403;&#21069;&#23548;&#33322;&#30740;&#31350;&#38598;&#20013;&#22312;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#26412;&#22320;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#8220;&#23427;&#23558;&#22312;&#24744;&#30340;&#21491;&#36793;&#8221;&#65289;&#65292;&#36825;&#20123;&#25551;&#36848;&#38656;&#35201;&#23545;&#20195;&#29702;&#20154;&#30340;&#26412;&#22320;&#30693;&#35273;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#22320;&#22270;&#33719;&#24471;&#30340;&#30693;&#35782;&#22522;&#30784;&#19978;&#30340;&#25551;&#36848;&#25552;&#20379;&#20102;&#29615;&#22659;&#30340;&#25972;&#20307;&#35270;&#22270;&#65292;&#24182;&#25429;&#25417;&#20102;&#20854;&#24635;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16364v1 Announce Type: new  Abstract: When communicating routes in natural language, the concept of {\em acquired spatial knowledge} is crucial for geographic information retrieval (GIR) and in spatial cognitive research. However, NLP navigation studies often overlook the impact of such acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., `it will be on your right') that require reasoning over the agent's local perception. These instructions are typically given as a sequence of steps, with each action-step explicitly mentioning and being followed by a landmark that the agent can use to verify they are on the right path (e.g., `turn right and then you will see...'). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure. These instructions (e.g., `it is south of Central Park and a block north of a police station') are 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#8220;CosmoAgent&#8221;&#65292;&#21033;&#29992;LLM&#27169;&#25311;&#20154;&#31867;&#21644;&#22806;&#26143;&#25991;&#26126;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#35780;&#20272;&#21644;&#24179;&#20849;&#23384;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#37327;&#21270;&#35780;&#20272;&#25991;&#26126;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#21516;&#26102;&#32771;&#34385;&#19981;&#21516;&#25991;&#26126;&#20043;&#38388;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13184</link><description>&lt;p&gt;
&#22914;&#26524;LLM&#20855;&#26377;&#19981;&#21516;&#30340;&#19990;&#30028;&#35266;&#65306;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#27169;&#25311;&#22806;&#26143;&#25991;&#26126;
&lt;/p&gt;
&lt;p&gt;
What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13184
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#8220;CosmoAgent&#8221;&#65292;&#21033;&#29992;LLM&#27169;&#25311;&#20154;&#31867;&#21644;&#22806;&#26143;&#25991;&#26126;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#35780;&#20272;&#21644;&#24179;&#20849;&#23384;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#37327;&#21270;&#35780;&#20272;&#25991;&#26126;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#21516;&#26102;&#32771;&#34385;&#19981;&#21516;&#25991;&#26126;&#20043;&#38388;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;CosmoAgent&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#27169;&#25311;&#20154;&#31867;&#19982;&#22806;&#26143;&#25991;&#26126;&#20043;&#38388;&#22797;&#26434;&#30340;&#20132;&#20114;&#65292;&#29305;&#21035;&#24378;&#35843;&#21490;&#33922;&#33452;&#183;&#38669;&#37329;&#20851;&#20110;&#19981;&#35201;&#38543;&#24847;&#21521;&#23431;&#23449;&#21457;&#36865;&#26080;&#32447;&#30005;&#20449;&#21495;&#30340;&#35880;&#24910;&#24314;&#35758;&#12290;&#35813;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#21644;&#24179;&#20849;&#23384;&#30340;&#21487;&#34892;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#21487;&#33021;&#23041;&#32961;&#21892;&#24847;&#25991;&#26126;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#36890;&#36807;&#37319;&#29992;&#25968;&#23398;&#27169;&#22411;&#21644;&#29366;&#24577;&#36716;&#25442;&#30697;&#38453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23450;&#37327;&#35780;&#20272;&#25991;&#26126;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#20026;&#22312;&#20851;&#38190;&#22686;&#38271;&#21644;&#39281;&#21644;&#28857;&#20570;&#20986;&#26410;&#26469;&#20915;&#31574;&#25552;&#20379;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25215;&#35748;&#23431;&#23449;&#20013;&#28508;&#22312;&#29983;&#27963;&#26465;&#20214;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#21487;&#33021;&#20250;&#20419;&#36827;&#19981;&#21516;&#25991;&#26126;&#20043;&#38388;&#29420;&#29305;&#30340;&#23431;&#23449;&#35266;&#12289;&#36947;&#24503;&#20934;&#21017;&#21644;&#19990;&#30028;&#35266;&#12290;&#35748;&#35782;&#21040;&#22320;&#29699;&#19978;--
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13184v1 Announce Type: new  Abstract: In this study, we introduce "CosmoAgent," an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2402.12198</link><description>&lt;p&gt;
&#38646;-shot &#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#30446;&#26631;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Zero shot VLMs for hate meme detection: Are we there yet?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#20854;&#20013;&#27169;&#22240;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#24418;&#24335;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19968;&#20123;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#27169;&#22240;&#38024;&#23545;&#20010;&#20154;&#25110;&#26131;&#21463;&#25915;&#20987;&#30340;&#31038;&#21306;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#35782;&#21035;&#21644;&#35299;&#20915;&#27492;&#31867;&#24694;&#24847;&#27169;&#22240;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#33879;&#23616;&#38480;&#24615;&#26159;&#38656;&#35201;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#25165;&#33021;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#30028;&#35265;&#35777;&#20102;&#20960;&#31181;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#36825;&#20123;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#35832;&#22914;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#31561;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#32622;&#26469;&#19987;&#27880;&#20110;&#23545;&#24694;&#24847;/&#26377;&#23475;&#27169;&#22240;&#30340;&#38646;-shot &#20998;&#31867;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12198v1 Announce Type: new  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we o
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#30340;&#23433;&#20840;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#25193;&#23637;&#20102;&#20004;&#31181;&#22330;&#26223;&#29992;&#20110;&#35782;&#21035;&#26377;&#39118;&#38505;&#25552;&#31034;&#25298;&#32477;&#30340;&#34394;&#38420;&#36127;&#38754;&#21644;&#38169;&#35823;&#32943;&#23450;&#31034;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12193</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23433;&#20840;&#26426;&#21046;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Chinese Dataset for Evaluating the Safeguards in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12193
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#30340;&#23433;&#20840;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#25193;&#23637;&#20102;&#20004;&#31181;&#22330;&#26223;&#29992;&#20110;&#35782;&#21035;&#26377;&#39118;&#38505;&#25552;&#31034;&#25298;&#32477;&#30340;&#34394;&#38420;&#36127;&#38754;&#21644;&#38169;&#35823;&#32943;&#23450;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20135;&#29983;&#26377;&#23475;&#21709;&#24212;&#65292;&#22312;LLMs&#37096;&#32626;&#26102;&#20351;&#29992;&#25143;&#38754;&#20020;&#24847;&#22806;&#39118;&#38505;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20851;&#20110;LLMs&#24341;&#21457;&#39118;&#38505;&#30340;&#32508;&#21512;&#20998;&#31867;&#27861;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#25552;&#31034;&#65292;&#21487;&#29992;&#20110;&#26816;&#26597;LLMs&#30340;&#23433;&#20840;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#20854;&#20182;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#23558;&#20854;&#25193;&#23637;&#21040;&#21478;&#22806;&#20004;&#31181;&#24773;&#26223;&#65292;&#21487;&#29992;&#20110;&#26356;&#22909;&#22320;&#35782;&#21035;&#20851;&#20110;&#26377;&#39118;&#38505;&#25552;&#31034;&#25298;&#32477;&#30340;&#34394;&#38420;&#36127;&#38754;&#21644;&#38169;&#35823;&#32943;&#23450;&#31034;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#38024;&#23545;&#27599;&#31181;&#39118;&#38505;&#31867;&#22411;&#25552;&#20986;&#19968;&#32452;&#32454;&#31890;&#24230;&#30340;&#23433;&#20840;&#35780;&#20272;&#26631;&#20934;&#65292;&#20419;&#36827;&#20154;&#24037;&#26631;&#27880;&#21644;&#33258;&#21160;&#35780;&#20272;LLM&#21709;&#24212;&#26377;&#23475;&#24615;&#12290;&#25105;&#20204;&#23545;&#20116;&#20010;LLM&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29305;&#23450;&#20110;&#22320;&#21306;&#30340;&#39118;&#38505;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12193v1 Announce Type: new  Abstract: Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by LLMs, as well as corresponding prompts that can be used to examine the safety mechanisms of LLMs. However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese LLMs, and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments on five LLMs show that region-specific risks a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#38598;ECInstruct&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65288;eCeLLM&#65289;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08831</link><description>&lt;p&gt;
eCeLLM&#65306;&#20174;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#20013;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#24191;&#21040;&#30005;&#23376;&#21830;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#38598;ECInstruct&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65288;eCeLLM&#65289;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#30005;&#23376;&#21830;&#21153;&#27169;&#22411;&#26041;&#38754;&#20570;&#20986;&#24040;&#22823;&#21162;&#21147;&#65292;&#20256;&#32479;&#30340;&#30005;&#23376;&#21830;&#21153;&#27169;&#22411;&#22312;&#36890;&#29992;&#30005;&#23376;&#21830;&#21153;&#24314;&#27169;&#19978;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#24182;&#19988;&#22312;&#26032;&#29992;&#25143;&#21644;&#26032;&#20135;&#21697;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#8212;&#8212;&#36825;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#39046;&#22495;&#22806;&#27867;&#21270;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#36890;&#29992;&#24314;&#27169;&#21644;&#39046;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;ECInstruct&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#21830;&#21153;&#30340;&#24320;&#28304;&#12289;&#22823;&#35268;&#27169;&#21644;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;ECInstruct&#65292;&#25105;&#20204;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65292;&#31216;&#20026;eCeLLM&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#21644;&#35780;&#20272;&#34920;&#26126;&#65292;eCeLLM&#27169;&#22411;&#22312;&#20869;&#37096;&#29615;&#22659;&#20013;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;GPT-4&#21644;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08831v1 Announce Type: cross Abstract: With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain ev
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#35299;&#20915;&#22312;&#32447;&#31038;&#21306;&#20013;&#25925;&#20107;&#26816;&#27979;&#22256;&#38590;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;StorySeeker&#24037;&#20855;&#21253;&#65292;&#21253;&#25324;&#35814;&#32454;&#27880;&#37322;&#30340;Reddit&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#31361;&#20986;&#20102;&#22312;&#32447;&#21465;&#20107;&#30340;&#25991;&#26412;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#21465;&#20107;&#36328;&#24230;&#26816;&#27979;&#20316;&#20026;&#19968;&#20010;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2311.09675</link><description>&lt;p&gt;
&#20154;&#20204;&#22312;&#21738;&#37324;&#22312;&#32447;&#35762;&#25925;&#20107;&#65311;&#36328;&#22312;&#32447;&#31038;&#21306;&#30340;&#25925;&#20107;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Where Do People Tell Stories Online? Story Detection Across Online Communities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09675
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#35299;&#20915;&#22312;&#32447;&#31038;&#21306;&#20013;&#25925;&#20107;&#26816;&#27979;&#22256;&#38590;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;StorySeeker&#24037;&#20855;&#21253;&#65292;&#21253;&#25324;&#35814;&#32454;&#27880;&#37322;&#30340;Reddit&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#31361;&#20986;&#20102;&#22312;&#32447;&#21465;&#20107;&#30340;&#25991;&#26412;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#21465;&#20107;&#36328;&#24230;&#26816;&#27979;&#20316;&#20026;&#19968;&#20010;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#21306;&#20013;&#30340;&#25925;&#20107;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#25925;&#20107;&#20998;&#25955;&#22312;&#31038;&#21306;&#20013;&#65292;&#24182;&#19988;&#19982;&#21333;&#20010;&#25991;&#26412;&#20013;&#30340;&#38750;&#21465;&#20107;&#37096;&#20998;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#21644;&#21457;&#24067;StorySeeker&#24037;&#20855;&#21253;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#21253;&#21547;502&#20010;Reddit&#24086;&#23376;&#21644;&#35780;&#35770;&#30340;&#20016;&#23500;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#36866;&#24212;&#31038;&#20132;&#23186;&#20307;&#32972;&#26223;&#30340;&#35814;&#32454;&#30340;&#20195;&#30721;&#20070;&#65292;&#20197;&#21450;&#29992;&#20110;&#22312;&#25991;&#26723;&#21644;&#36328;&#24230;&#32423;&#21035;&#39044;&#27979;&#21465;&#20107;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#20174;&#25968;&#30334;&#20010;&#27969;&#34892;&#30340;&#33521;&#35821;Reddit&#31038;&#21306;&#20013;&#25277;&#26679;&#32780;&#26469;&#65292;&#28085;&#30422;&#20102;33&#20010;&#20027;&#39064;&#31867;&#21035;&#65292;&#23427;&#21253;&#21547;&#20102;&#32454;&#31890;&#24230;&#30340;&#19987;&#23478;&#27880;&#37322;&#65292;&#21253;&#25324;&#20108;&#20803;&#25925;&#20107;&#26631;&#31614;&#65292;&#25925;&#20107;&#36328;&#24230;&#21644;&#20107;&#20214;&#36328;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#32447;&#21465;&#20107;&#30340;&#29420;&#29305;&#25991;&#26412;&#29305;&#24449;&#65292;&#37325;&#28857;&#20851;&#27880;&#21465;&#20107;&#36328;&#24230;&#26816;&#27979;&#65292;&#36825;&#26159;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#20010;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#22823;&#35268;&#27169;&#21465;&#20107;&#30340;&#20998;&#24067;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09675v2 Announce Type: replace  Abstract: Story detection in online communities is a challenging task as stories are scattered across communities and interwoven with non-storytelling spans within a single text. We address this challenge by building and releasing the StorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts and comments, a detailed codebook adapted to the social media context, and models to predict storytelling at the document and span level. Our dataset is sampled from hundreds of popular English-language Reddit communities ranging across 33 topic categories, and it contains fine-grained expert annotations, including binary story labels, story spans, and event spans. We evaluate a range of detection methods using our data, and we identify the distinctive textual features of online storytelling, focusing on storytelling span detection, which we introduce as a new task. We illuminate distributional characteristics of storytelling on a large
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38142;&#24335;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#12289;&#20010;&#24615;&#21270;&#25351;&#23548;&#21644;&#28789;&#27963;&#27979;&#39564;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2309.08112</link><description>&lt;p&gt;
&#36890;&#36807;&#38142;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31169;&#20154;&#36741;&#23548;
&lt;/p&gt;
&lt;p&gt;
Empowering Private Tutoring by Chaining Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08112
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38142;&#24335;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#12289;&#20010;&#24615;&#21270;&#25351;&#23548;&#21644;&#28789;&#27963;&#27979;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#34987;&#24212;&#29992;&#20110;&#22312;&#32447;&#25945;&#32946;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20197;&#20419;&#36827;&#25945;&#23398;&#21644;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#23436;&#25972;&#30340;AI&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#30001;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#28085;&#30422;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#21644;&#35843;&#25972;&#12289;&#23450;&#21046;&#25351;&#23548;&#20197;&#21450;&#28789;&#27963;&#30340;&#27979;&#39564;&#35780;&#20272;&#12290;&#20026;&#20102;&#20351;&#31995;&#32479;&#33021;&#22815;&#32463;&#21463;&#20303;&#38271;&#26102;&#38388;&#20132;&#20114;&#24182;&#28385;&#36275;&#20010;&#24615;&#21270;&#25945;&#32946;&#30340;&#38656;&#27714;&#65292;&#31995;&#32479;&#34987;&#20998;&#35299;&#20026;&#19977;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#26680;&#24515;&#27969;&#31243;-&#20132;&#20114;&#12289;&#21453;&#24605;&#21644;&#21453;&#24212;&#12290;&#27599;&#20010;&#27969;&#31243;&#37117;&#36890;&#36807;&#38142;&#25509;LLM&#39537;&#21160;&#30340;&#24037;&#20855;&#20197;&#21450;&#21160;&#24577;&#26356;&#26032;&#30340;&#35760;&#24518;&#27169;&#22359;&#26469;&#23454;&#29616;&#12290;&#24037;&#20855;&#26159;LLMs&#65292;&#34987;&#25552;&#31034;&#25191;&#34892;&#19968;&#39033;&#29305;&#23450;&#20219;&#21153;&#65292;&#32780;&#35760;&#24518;&#26159;&#22312;&#25945;&#32946;&#36807;&#31243;&#20013;&#26356;&#26032;&#30340;&#25968;&#25454;&#23384;&#20648;&#12290;&#23398;&#20064;&#26085;&#24535;&#20013;&#30340;&#32479;&#35745;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.08112v1 Announce Type: cross  Abstract: Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches has been made toward a complete AI-powered tutoring system. In this work, we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs), covering automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation. To make the system robust to prolonged interaction and cater to individualized education, the system is decomposed into three inter-connected core processes-interaction, reflection, and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. Tools are LLMs prompted to execute one specific task at a time, while memories are data storage that gets updated during education process. Statistical results from learning logs demonstrate the effec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16458</link><description>&lt;p&gt;
&#20449;&#29992;&#39118;&#38505;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65306;&#20174;P2P&#20511;&#36151;&#30340;&#36151;&#27454;&#25551;&#36848;&#20013;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending. (arXiv:2401.16458v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
P2P&#20511;&#36151;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#30340;&#34701;&#36164;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#32447;&#24179;&#21488;&#23558;&#20511;&#27454;&#20154;&#19982;&#25918;&#27454;&#20154;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;P2P&#20511;&#36151;&#38754;&#20020;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25918;&#27454;&#20154;&#24448;&#24448;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#20511;&#27454;&#20154;&#30340;&#20449;&#29992;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#20511;&#27454;&#20154;&#22312;&#36151;&#27454;&#30003;&#35831;&#36807;&#31243;&#20013;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22788;&#29702;&#36825;&#20123;&#25991;&#26412;&#25551;&#36848;&#65292;LLM&#26159;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#27169;&#24335;&#21644;&#35821;&#20041;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#23558;LLM&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;Lending Club&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;BERT&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#26174;&#33879;&#25552;&#39640;&#20102;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#22266;&#26377;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#20197;&#21450;&#28508;&#22312;&#20559;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism, linking borrowers with lenders through online platforms. However, P2P lending faces the challenge of information asymmetry, as lenders often lack sufficient data to assess the creditworthiness of borrowers. This paper proposes a novel approach to address this issue by leveraging the textual descriptions provided by borrowers during the loan application process. Our methodology involves processing these textual descriptions using a Large Language Model (LLM), a powerful tool capable of discerning patterns and semantics within the text. Transfer learning is applied to adapt the LLM to the specific task at hand.  Our results derived from the analysis of the Lending Club dataset show that the risk score generated by BERT, a widely used LLM, significantly improves the performance of credit risk classifiers. However, the inherent opacity of LLM-based systems, coupled with uncertainties about potential biases, unders
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#65292;&#24341;&#20837;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#35780;&#20272;&#35282;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19987;&#38376;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#24314;&#31435;&#20102;&#22522;&#30784;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.09002</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models. (arXiv:2401.09002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#65292;&#24341;&#20837;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#35780;&#20272;&#35282;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19987;&#38376;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#24314;&#31435;&#20102;&#22522;&#30784;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21019;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#30340;&#26032;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#20581;&#22766;&#24615;&#35780;&#20272;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;&#31895;&#31890;&#24230;&#35780;&#20272;&#21644;&#32454;&#31890;&#24230;&#35780;&#20272;&#12290;&#27599;&#20010;&#26694;&#26550;&#37117;&#20351;&#29992;&#20174;0&#21040;1&#30340;&#35780;&#20998;&#33539;&#22260;&#65292;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#21644;&#32454;&#33268;&#22320;&#35780;&#20272;&#25915;&#20987;&#25928;&#26524;&#65292;&#24182;&#24110;&#21161;&#25915;&#20987;&#32773;&#26356;&#22909;&#22320;&#20248;&#21270;&#25915;&#20987;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#36234;&#29425;&#20219;&#21153;&#30340;&#20840;&#38754;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#19981;&#20165;&#26159;&#25105;&#20204;&#24403;&#21069;&#30740;&#31350;&#30340;&#20851;&#38190;&#22522;&#20934;&#65292;&#20063;&#20026;&#26410;&#26469;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#30784;&#36164;&#28304;&#65292;&#21487;&#20197;&#22312;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#36827;&#34892;&#19968;&#33268;&#21644;&#27604;&#36739;&#30340;&#20998;&#26512;&#12290;&#36890;&#36807;&#19982;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#30340;&#31934;&#24515;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#19982;&#20043;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our research, we pioneer a novel approach to evaluate the effectiveness of jailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2, diverging from traditional robustness-focused binary evaluations. Our study introduces two distinct evaluation frameworks: a coarse-grained evaluation and a fine-grained evaluation. Each framework, using a scoring range from 0 to 1, offers a unique perspective, enabling a more comprehensive and nuanced evaluation of attack effectiveness and empowering attackers to refine their attack prompts with greater understanding. Furthermore, we have developed a comprehensive ground truth dataset specifically tailored for jailbreak tasks. This dataset not only serves as a crucial benchmark for our current study but also establishes a foundational resource for future research, enabling consistent and comparative analyses in this evolving field. Upon meticulous comparison with traditional evaluation methods, we discovered that our evaluation alig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#20102;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#30340;&#35266;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#20154;&#24037;&#21512;&#25104;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.06416</link><description>&lt;p&gt;
&#19981;&#21487;&#33021;&#20219;&#21153;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mission: Impossible Language Models. (arXiv:2401.06416v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20102;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#30340;&#35266;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#20154;&#24037;&#21512;&#25104;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chomsky&#21644;&#20854;&#20182;&#20154;&#30452;&#25509;&#22768;&#31216;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#20154;&#31867;&#26080;&#27861;&#23398;&#20064;&#30340;&#21487;&#33021;&#21644;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#21457;&#34920;&#30340;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#36825;&#26679;&#30340;&#35828;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#25913;&#21464;&#33521;&#25991;&#25968;&#25454;&#30340;&#35789;&#24207;&#21644;&#35821;&#27861;&#35268;&#21017;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#19981;&#21487;&#33021;&#30340;&#21512;&#25104;&#35821;&#35328;&#65292;&#27599;&#31181;&#35821;&#35328;&#30340;&#22797;&#26434;&#31243;&#24230;&#19981;&#21516;&#12290;&#36825;&#20123;&#35821;&#35328;&#20301;&#20110;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#36830;&#32493;&#20307;&#19978;&#65306;&#19968;&#31471;&#26159;&#26412;&#36136;&#19978;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#65292;&#20363;&#22914;&#33521;&#25991;&#21333;&#35789;&#30340;&#38543;&#26426;&#21644;&#19981;&#21487;&#36870;&#30340;&#27927;&#29260;&#65292;&#32780;&#21478;&#19968;&#31471;&#26159;&#22312;&#35821;&#35328;&#23398;&#19978;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#35745;&#31639;&#35789;&#20301;&#32622;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#26469;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#23398;&#20064;&#36825;&#20123;&#26080;&#21487;&#20105;&#35758;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#36825;&#20123;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25972;&#21512;&#20102;&#21307;&#29983;&#30340;&#35786;&#26029;&#36923;&#36753;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20559;&#22909;&#23398;&#20064;&#20174;&#36807;&#31243;&#21453;&#39304;&#65288;PLPF&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#22312;&#21307;&#30103;&#23545;&#35805;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05695</link><description>&lt;p&gt;
&#23558;&#21307;&#29983;&#30340;&#35786;&#26029;&#36923;&#36753;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65306;&#20174;&#36807;&#31243;&#21453;&#39304;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback. (arXiv:2401.05695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05695
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25972;&#21512;&#20102;&#21307;&#29983;&#30340;&#35786;&#26029;&#36923;&#36753;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20559;&#22909;&#23398;&#20064;&#20174;&#36807;&#31243;&#21453;&#39304;&#65288;PLPF&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#22312;&#21307;&#30103;&#23545;&#35805;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#24341;&#36215;&#20102;&#37325;&#35270;&#65292;&#33268;&#21147;&#20110;&#25913;&#21892;&#21709;&#24212;&#36136;&#37327;&#21644;&#27969;&#30021;&#24615;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#21333;&#36718;&#21307;&#30103;&#38382;&#31572;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26377;&#24517;&#35201;&#22686;&#24378;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#36991;&#20813;&#36923;&#36753;&#19981;&#19968;&#33268;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20174;&#36807;&#31243;&#21453;&#39304;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PLPF&#65289;&#65292;&#23558;&#21307;&#29983;&#30340;&#35786;&#26029;&#36923;&#36753;&#25972;&#21512;&#21040;LLM&#20013;&#12290;PLPF&#21253;&#25324;&#35268;&#21017;&#24314;&#27169;&#12289;&#20559;&#22909;&#25968;&#25454;&#29983;&#25104;&#21644;&#20559;&#22909;&#23545;&#40784;&#65292;&#20197;&#35757;&#32451;&#27169;&#22411;&#36981;&#24490;&#35786;&#26029;&#36807;&#31243;&#12290;&#20351;&#29992;&#26631;&#20934;&#21270;&#24739;&#32773;&#27979;&#35797;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PLPF&#23558;&#21307;&#30103;&#23545;&#35805;&#20013;&#22522;&#20934;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;17.6&#65285;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;PLPF&#22312;&#22810;&#36718;&#21644;&#21333;&#36718;&#23545;&#35805;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of large language models in medical dialogue generation has garnered significant attention, with a focus on improving response quality and fluency. While previous studies have made progress in optimizing model performance for single-round medical Q&amp;A tasks, there is a need to enhance the model's capability for multi-round conversations to avoid logical inconsistencies. To address this, we propose an approach called preference learning from process feedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF involves rule modeling, preference data generation, and preference alignment to train the model to adhere to the diagnostic process. Experimental results using Standardized Patient Testing show that PLPF enhances the diagnostic accuracy of the baseline model in medical conversations by 17.6%, outperforming traditional reinforcement learning from human feedback. Additionally, PLPF demonstrates effectiveness in both multi-round and single-round dialogue task
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.05631</link><description>&lt;p&gt;
DrawTalking&#65306;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05631
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;DrawTalking&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#12290;&#23427;&#24378;&#35843;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#31867;&#20284;&#32534;&#31243;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;iPad&#19978;&#23454;&#29616;&#20102;&#23427;&#12290;&#19968;&#39033;&#24320;&#25918;&#24335;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#35768;&#22810;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#29992;&#20363;&#30456;&#22865;&#21512;&#21644;&#36866;&#29992;&#12290;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#21644;&#25351;&#23548;&#26410;&#26469;&#33258;&#28982;&#29992;&#25143;&#20013;&#24515;&#30028;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#22810;&#38454;&#27573;&#21327;&#20316;&#33976;&#39311;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.08640</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#30693;&#35782;&#33976;&#39311;&#22312;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation. (arXiv:2311.08640v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08640
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#22810;&#38454;&#27573;&#21327;&#20316;&#33976;&#39311;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#36825;&#31181;&#20219;&#21153;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#22826;&#23569;&#20197;&#33267;&#20110;&#26080;&#27861;&#26377;&#25928;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#24615;&#33021;&#20063;&#19981;&#22815;&#29702;&#24819;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19968;&#20123;&#26114;&#36149;&#19988;&#23545;&#39044;&#35757;&#32451;&#30340; LLM &#19981;&#29087;&#24713;&#30340;&#20219;&#21153;&#65292;&#22914;&#35299;&#26512;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340; LLM &#33976;&#39311;&#20986;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#36890;&#24120;&#27604;&#20854;&#25945;&#24072;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; - &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#30693;&#35782;&#33976;&#39311; (MCKD) - &#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#12290;MCKD &#39318;&#20808;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#35753;LLM&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#22312;&#27599;&#20010;&#20013;&#38388;&#30693;&#35782;&#33976;&#39311; (KD) &#38454;&#27573;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#25968;&#25454;&#30340;&#19981;&#37325;&#21472;&#20998;&#21306;&#26469;&#35757;&#32451;&#19968;&#23545;&#26032;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#23398;&#29983;&#27169;&#22411;&#20026;&#20854;&#26410;&#35265;&#20998;&#21306;&#29983;&#25104;&#26032;&#30340;&#21644;&#25913;&#36827;&#30340;&#20266;&#26631;&#31614;&#65292;&#22312;&#19979;&#19968;&#20010;&#33976;&#39311;&#38454;&#27573;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study semi-supervised sequence generation tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we present a new method -multistage collaborative knowledge distillation from an LLM (MCKD) -- for such tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. At each intermediate knowledge distillation (KD) stage, a new pair of students is trained on disjoint partitions of the pseudolabeled data. Each student then produces new and improved pseudolabels for its unseen partition to be used in the next stage of distillation. We demonstrate the advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#24320;&#21457;&#20102;&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#32455;&#21322;&#32467;&#26500;&#21270;&#30340;&#20250;&#35758;&#25968;&#25454;&#24182;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#35299;&#20915;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#33719;&#21462;&#20934;&#30830;&#12289;&#26368;&#26032;&#20449;&#24687;&#26102;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.13028</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable Academic Conference Question Answering: A Study Based on Large Language Model. (arXiv:2310.13028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#24320;&#21457;&#20102;&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#32455;&#21322;&#32467;&#26500;&#21270;&#30340;&#20250;&#35758;&#25968;&#25454;&#24182;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#35299;&#20915;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#33719;&#21462;&#20934;&#30830;&#12289;&#26368;&#26032;&#20449;&#24687;&#26102;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#23398;&#26415;&#20250;&#35758;&#19978;&#30340;&#30740;&#31350;&#22823;&#37327;&#22686;&#21152;&#65292;&#20419;&#36827;&#20102;&#20840;&#29699;&#23398;&#26415;&#20132;&#27969;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#20010;&#38454;&#27573;&#37117;&#25345;&#32493;&#23547;&#27714;&#20851;&#20110;&#36825;&#20123;&#20107;&#20214;&#30340;&#20934;&#30830;&#12289;&#26368;&#26032;&#20449;&#24687;&#12290;&#36825;&#31181;&#25968;&#25454;&#29190;&#21457;&#38656;&#35201;&#19968;&#20010;&#26234;&#33021;&#30340;&#38382;&#31572;&#31995;&#32479;&#26469;&#39640;&#25928;&#35299;&#20915;&#30740;&#31350;&#20154;&#21592;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#23545;&#26368;&#26032;&#36827;&#23637;&#30340;&#20102;&#35299;&#12290;&#20250;&#35758;&#20449;&#24687;&#36890;&#24120;&#22312;&#23448;&#26041;&#32593;&#31449;&#19978;&#21457;&#24067;&#65292;&#20197;&#21322;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#32452;&#32455;&#65292;&#24182;&#21253;&#21547;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ConferenceQA&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;7&#20010;&#19981;&#21516;&#23398;&#26415;&#20250;&#35758;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#26631;&#27880;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#25163;&#21160;&#21644;&#33258;&#21160;&#26041;&#27861;&#30340;&#32452;&#21512;&#65292;&#20197;&#21322;&#32467;&#26500;&#21270;&#30340;JSON&#26684;&#24335;&#32452;&#32455;&#23398;&#26415;&#20250;&#35758;&#25968;&#25454;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#20250;&#35758;&#27880;&#37322;&#20102;&#36817;100&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#27599;&#20010;&#23545;&#24212;&#23545;&#24212;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#32500;&#24230;&#20998;&#31867;&#12290;&#20026;&#20102;&#30830;&#20445;&#25968;&#25454;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25163;&#21160;&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of computer science has led to a proliferation of research presented at academic conferences, fostering global scholarly communication. Researchers consistently seek accurate, current information about these events at all stages. This data surge necessitates an intelligent question-answering system to efficiently address researchers' queries and ensure awareness of the latest advancements. The information of conferences is usually published on their official website, organized in a semi-structured way with a lot of text. To address this need, we have developed the ConferenceQA dataset for 7 diverse academic conferences with human annotations. Firstly, we employ a combination of manual and automated methods to organize academic conference data in a semi-structured JSON format. Subsequently, we annotate nearly 100 question-answer pairs for each conference. Each pair is classified into four different dimensions. To ensure the reliability of the data, we manually annotate 
&lt;/p&gt;</description></item><item><title>LLM&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#26041;&#24335;&#30340;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#20316;&#20026;&#24187;&#35273;&#25915;&#20987;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.01469</link><description>&lt;p&gt;
LLM&#35854;&#35328;: &#24187;&#35273;&#19981;&#26159;&#28431;&#27934;&#65292;&#32780;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. (arXiv:2310.01469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01469
&lt;/p&gt;
&lt;p&gt;
LLM&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#26041;&#24335;&#30340;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#20316;&#20026;&#24187;&#35273;&#25915;&#20987;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21253;&#25324;GPT-3.5&#12289;LLaMA&#21644;PaLM&#65292;&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;LLM&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#32780;&#19981;&#34987;&#23519;&#35273;&#12290;&#24187;&#35273;&#23384;&#22312;&#30340;&#21407;&#22240;&#21644;&#26222;&#36941;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#12290;&#36825;&#20010;&#29616;&#35937;&#36843;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#24187;&#35273;&#21487;&#33021;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#19988;&#23427;&#19982;&#24120;&#35268;&#30340;&#23545;&#25239;&#26679;&#26412;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24449;&#65292;&#20316;&#20026;LLM&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20197;&#23545;&#25239;&#30340;&#26041;&#24335;&#23558;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#24418;&#24335;&#21270;&#20026;&#24187;&#35273;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#34987;&#25915;&#20987;&#30340;&#23545;&#25239;&#25552;&#31034;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception. And the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. This phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of LLMs. Therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. Finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#23545;&#35805;&#25968;&#25454;&#24320;&#21457;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#35813;&#26426;&#22120;&#20154;&#32467;&#21512;&#32842;&#22825;&#26426;&#22120;&#20154;&#20114;&#21160;&#29305;&#24449;&#21644;&#33521;&#35821;&#25945;&#31185;&#20070;&#30340;&#31995;&#32479;&#26448;&#26009;&#65292;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#21475;&#35821;&#25216;&#24039;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#25945;&#31185;&#20070;&#20027;&#39064;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#20851;&#23545;&#35805;&#65292;&#36890;&#36807;&#24494;&#35843;&#26426;&#22120;&#20154;&#30340;&#23545;&#35805;&#25968;&#25454;&#21019;&#24314;&#19968;&#20010;&#35838;&#31243;&#39537;&#21160;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#24341;&#39046;&#22522;&#20110;&#35838;&#31243;&#30340;&#23545;&#35805;&#21644;&#36866;&#24212;&#29992;&#25143;&#33521;&#35821;&#27700;&#24179;&#26041;&#38754;&#20248;&#20110;ChatGPT&#12290;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#25945;&#31185;&#20070;&#26041;&#27861;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#35838;&#31243;&#30456;&#21305;&#37197;&#24182;&#25552;&#20379;&#29992;&#25143;&#20010;&#24615;&#21270;&#23545;&#35805;&#32451;&#20064;&#30340;&#20114;&#21160;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.16804</link><description>&lt;p&gt;
&#35838;&#31243;&#39537;&#21160;&#30340;&#25945;&#32946;&#26426;&#22120;&#20154;&#65306;&#36890;&#36807;&#32508;&#21512;&#23545;&#35805;&#25968;&#25454;&#24320;&#21457;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data. (arXiv:2309.16804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#23545;&#35805;&#25968;&#25454;&#24320;&#21457;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#35813;&#26426;&#22120;&#20154;&#32467;&#21512;&#32842;&#22825;&#26426;&#22120;&#20154;&#20114;&#21160;&#29305;&#24449;&#21644;&#33521;&#35821;&#25945;&#31185;&#20070;&#30340;&#31995;&#32479;&#26448;&#26009;&#65292;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#21475;&#35821;&#25216;&#24039;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#25945;&#31185;&#20070;&#20027;&#39064;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#20851;&#23545;&#35805;&#65292;&#36890;&#36807;&#24494;&#35843;&#26426;&#22120;&#20154;&#30340;&#23545;&#35805;&#25968;&#25454;&#21019;&#24314;&#19968;&#20010;&#35838;&#31243;&#39537;&#21160;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#24341;&#39046;&#22522;&#20110;&#35838;&#31243;&#30340;&#23545;&#35805;&#21644;&#36866;&#24212;&#29992;&#25143;&#33521;&#35821;&#27700;&#24179;&#26041;&#38754;&#20248;&#20110;ChatGPT&#12290;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#25945;&#31185;&#20070;&#26041;&#27861;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#35838;&#31243;&#30456;&#21305;&#37197;&#24182;&#25552;&#20379;&#29992;&#25143;&#20010;&#24615;&#21270;&#23545;&#35805;&#32451;&#20064;&#30340;&#20114;&#21160;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25945;&#32946;&#22330;&#26223;&#20013;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#25913;&#21464;&#20102;&#23398;&#29983;&#19982;&#35838;&#31243;&#20114;&#21160;&#21644;&#25945;&#24072;&#25480;&#35838;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35838;&#31243;&#39537;&#21160;&#30340;&#25945;&#32946;&#26426;&#22120;&#20154;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20114;&#21160;&#29305;&#24449;&#19982;&#33521;&#35821;&#25945;&#31185;&#20070;&#30340;&#31995;&#32479;&#26448;&#26009;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#21475;&#35821;&#25216;&#24039;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#25945;&#31185;&#20070;&#20013;&#25552;&#21462;&#30456;&#20851;&#20027;&#39064;&#65292;&#28982;&#21518;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#36825;&#20123;&#20027;&#39064;&#30456;&#20851;&#30340;&#23545;&#35805;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#23545;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#21019;&#24314;&#25105;&#20204;&#30340;&#35838;&#31243;&#39537;&#21160;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#24341;&#39046;&#22522;&#20110;&#35838;&#31243;&#30340;&#23545;&#35805;&#21644;&#36866;&#24212;&#29992;&#25143;&#33521;&#35821;&#27700;&#24179;&#26041;&#38754;&#20248;&#20110;ChatGPT&#12290;&#36890;&#36807;&#23558;&#20256;&#32479;&#25945;&#31185;&#20070;&#26041;&#27861;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#35838;&#31243;&#30456;&#21305;&#37197;&#24182;&#25552;&#20379;&#29992;&#25143;&#20010;&#24615;&#21270;&#23545;&#35805;&#32451;&#20064;&#30340;&#20114;&#21160;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots have become popular in educational settings, revolutionizing how students interact with material and how teachers teach. We present Curriculum-Driven EduBot, a framework for developing a chatbot that combines the interactive features of chatbots with the systematic material of English textbooks to assist students in enhancing their conversational skills. We begin by extracting pertinent topics from textbooks and then using large language models to generate dialogues related to these topics. We then fine-tune an open-source LLM using our generated conversational data to create our curriculum-driven chatbot. User studies demonstrate that our chatbot outperforms ChatGPT in leading curriculum-based dialogues and adapting its dialogue to match the user's English proficiency level. By combining traditional textbook methodologies with conversational AI, our approach offers learners an interactive tool that aligns with their curriculum and provides user-tailored conversation practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20851;&#31995;&#24314;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#29983;&#25104;&#36830;&#36143;&#30340;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#21453;&#21521;&#20851;&#31995;&#21019;&#24314;&#23545;&#31216;&#22270;&#26469;&#25552;&#20379;&#39069;&#22806;&#30340;&#26631;&#31614;&#21644;&#34917;&#20805;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.14770</link><description>&lt;p&gt;
KERMIT: &#24102;&#26377;&#21453;&#36716;&#21464;&#25442;&#30340;&#22686;&#24378;&#20851;&#31995;&#24314;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse Transformation. (arXiv:2309.14770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20851;&#31995;&#24314;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#29983;&#25104;&#36830;&#36143;&#30340;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#21453;&#21521;&#20851;&#31995;&#21019;&#24314;&#23545;&#31216;&#22270;&#26469;&#25552;&#20379;&#39069;&#22806;&#30340;&#26631;&#31614;&#21644;&#34917;&#20805;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26159;&#19968;&#39033;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#21487;&#29992;&#20449;&#24687;&#22635;&#20805;&#32570;&#22833;&#19977;&#20803;&#32452;&#30340;&#20219;&#21153;&#12290;&#22312;&#24403;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#19977;&#20803;&#32452;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24314;&#27169;&#26041;&#27861;&#21487;&#33021;&#36935;&#21040;&#19968;&#20123;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#24403;&#25551;&#36848;&#19981;&#33021;&#20934;&#30830;&#20805;&#20998;&#22320;&#34920;&#36798;&#39044;&#26399;&#21547;&#20041;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#20010;&#39069;&#22806;&#26426;&#21046;&#26469;&#22686;&#21152;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#22806;&#37096;&#30693;&#35782;&#24211;&#65292;&#29983;&#25104;&#36830;&#36143;&#30340;&#25551;&#36848;&#20197;&#24357;&#34917;&#26597;&#35810;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#21521;&#20851;&#31995;&#21019;&#24314;&#23545;&#31216;&#22270;&#65292;&#20174;&#32780;&#20026;&#38142;&#25509;&#39044;&#27979;&#25552;&#20379;&#39069;&#22806;&#30340;&#26631;&#31614;&#21644;&#34917;&#20805;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20851;&#31995;&#23454;&#20307;&#20043;&#38388;&#39069;&#22806;&#30340;&#27934;&#23519;&#21147;&#12290;&#36890;&#36807;&#36825;&#20123;&#21162;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion is a task that revolves around filling in missing triples based on the information available in a knowledge graph. Among the current studies, text-based methods complete the task by utilizing textual descriptions of triples. However, this modeling approach may encounter limitations, particularly when the description fails to accurately and adequately express the intended meaning. To overcome these challenges, we propose the augmentation of data through two additional mechanisms. Firstly, we employ ChatGPT as an external knowledge base to generate coherent descriptions to bridge the semantic gap between the queries and answers. Secondly, we leverage inverse relations to create a symmetric graph, thereby creating extra labeling and providing supplementary information for link prediction. This approach offers additional insights into the relationships between entities. Through these efforts, we have observed significant improvements in knowledge graph completion
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35782;&#21035;&#23545;&#35805;&#20013;&#20154;&#31867;&#24773;&#24863;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#24320;&#25918;&#39046;&#22495;&#38386;&#32842;&#23545;&#35805;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12881</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Affect Recognition in Conversations Using Large Language Models. (arXiv:2309.12881v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35782;&#21035;&#23545;&#35805;&#20013;&#20154;&#31867;&#24773;&#24863;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#24320;&#25918;&#39046;&#22495;&#38386;&#32842;&#23545;&#35805;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#22312;&#20154;&#31867;&#20132;&#27969;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28085;&#30422;&#24773;&#32490;&#12289;&#24515;&#24773;&#21644;&#24863;&#21463;&#12290;&#22312;&#20250;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35782;&#21035;&#21644;&#22238;&#24212;&#20154;&#31867;&#24773;&#24863;&#32447;&#32034;&#30340;&#33021;&#21147;&#23545;&#20110;&#21019;&#24314;&#24341;&#20154;&#20837;&#32988;&#19988;&#23500;&#26377;&#21516;&#29702;&#24515;&#30340;&#20114;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#20013;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#24320;&#25918;&#39046;&#22495;&#38386;&#32842;&#23545;&#35805;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12290;&#21033;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;IEMOCAP&#12289;EmoWOZ&#21644;DAIC-WOZ&#65292;&#28085;&#30422;&#20102;&#20174;&#26085;&#24120;&#23545;&#35805;&#21040;&#20020;&#24202;&#38754;&#35797;&#30340;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#65292;&#25105;&#20204;&#35780;&#20272;&#24182;&#27604;&#36739;&#20102;LLMs&#22312;&#24773;&#24863;&#35782;&#21035;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20197;&#21450;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#26469;&#25552;&#39640;&#27169;&#22411;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#32771;&#34385;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affect recognition, encompassing emotions, moods, and feelings, plays a pivotal role in human communication. In the realm of conversational artificial intelligence (AI), the ability to discern and respond to human affective cues is a critical factor for creating engaging and empathetic interactions. This study delves into the capacity of large language models (LLMs) to recognise human affect in conversations, with a focus on both open-domain chit-chat dialogues and task-oriented dialogues. Leveraging three diverse datasets, namely IEMOCAP, EmoWOZ, and DAIC-WOZ, covering a spectrum of dialogues from casual conversations to clinical interviews, we evaluated and compared LLMs' performance in affect recognition. Our investigation explores the zero-shot and few-shot capabilities of LLMs through in-context learning (ICL) as well as their model capacities through task-specific fine-tuning. Additionally, this study takes into account the potential impact of automatic speech recognition (ASR) e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25805;&#32437;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#12290;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#23545;&#25239;&#25552;&#31034;&#19982;&#29992;&#25143;&#26597;&#35810;&#32467;&#21512;&#65292;&#21487;&#20197;&#25200;&#20081;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23548;&#33268;&#24847;&#22806;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#20026;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35786;&#26029;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.01446</link><description>&lt;p&gt;
&#24320;&#38376;&#21543;&#65281;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#40657;&#30418;&#30772;&#35299;
&lt;/p&gt;
&lt;p&gt;
Open Sesame! Universal Black Box Jailbreaking of Large Language Models. (arXiv:2309.01446v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25805;&#32437;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#12290;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#23545;&#25239;&#25552;&#31034;&#19982;&#29992;&#25143;&#26597;&#35810;&#32467;&#21512;&#65292;&#21487;&#20197;&#25200;&#20081;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23548;&#33268;&#24847;&#22806;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#20026;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26088;&#22312;&#25552;&#20379;&#26377;&#24110;&#21161;&#21644;&#23433;&#20840;&#30340;&#22238;&#22797;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#40784;&#25216;&#26415;&#19982;&#29992;&#25143;&#24847;&#22270;&#21644;&#31038;&#20250;&#25351;&#21335;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23545;&#40784;&#21487;&#33021;&#20250;&#34987;&#24694;&#24847;&#34892;&#20026;&#32773;&#21033;&#29992;&#65292;&#20197;&#29992;&#20110;&#24847;&#24819;&#19981;&#21040;&#30340;&#30446;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#22312;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#19981;&#21487;&#35775;&#38382;&#26102;&#25805;&#32437;LLMs&#12290;GA&#25915;&#20987;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#23545;&#25239;&#25552;&#31034;&#19982;&#29992;&#25143;&#26597;&#35810;&#32467;&#21512;&#65292;&#25200;&#20081;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23548;&#33268;&#24847;&#22806;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#26041;&#27861;&#36890;&#36807;&#25581;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#31995;&#32479;&#22320;&#25581;&#31034;&#20102;&#20854;&#21709;&#24212;&#19982;&#39044;&#26399;&#34892;&#20026;&#19981;&#31526;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#20026;&#20851;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#30340;&#35752;&#35770;&#25552;&#20379;&#20102;&#19968;&#31181;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM's outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user's query -- disrupts the attacked model's alignment, resulting in unintended and potentially harmful outputs. Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. Through extensive experiments we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible AI development by providing a diagnostic tool 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#25972;&#21512;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#21644;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16143</link><description>&lt;p&gt;
&#20026;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#32780;&#36827;&#34892;&#30340;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications. (arXiv:2306.16143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#25972;&#21512;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#21644;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#20307;&#39564;&#65288;UX&#65289;&#26159;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#19987;&#27880;&#20110;&#25552;&#39640;&#31995;&#32479;&#29992;&#25143;&#30340;&#30452;&#35266;&#24615;&#12289;&#36879;&#26126;&#24230;&#12289;&#31616;&#27905;&#24615;&#21644;&#20449;&#20219;&#24230;&#12290;&#22823;&#22810;&#25968;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;UX&#30740;&#31350;&#37117;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21363;&#27809;&#26377;&#20851;&#27880;&#29992;&#25143;&#38656;&#27714;&#65292;&#24182;&#20165;&#20165;&#23558;&#39046;&#22495;&#29992;&#25143;&#29992;&#20110;&#21487;&#29992;&#24615;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#26356;&#20856;&#22411;&#30340;UX&#26041;&#27861;&#26159;&#20808;&#38024;&#23545;&#29992;&#25143;&#30340;&#21487;&#29992;&#24615;&#36827;&#34892;&#23450;&#21046;&#65292;&#32780;&#19981;&#26159;&#39318;&#20808;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;UX&#30740;&#31350;&#25972;&#21512;&#21040;&#24320;&#21457;&#39046;&#22495;NLP&#24212;&#29992;&#20013;&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#24335;UX&#30740;&#31350;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#21021;&#22987;&#38454;&#27573;&#65292;&#21363;&#26500;&#24605;&#21644;&#27010;&#24565;&#35780;&#20272;&#38454;&#27573;&#65292;&#20197;&#21450;&#26368;&#21518;&#19968;&#38454;&#27573;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#38024;&#23545;&#36807;&#31243;&#24037;&#19994;&#20013;&#26085;&#24120;&#25805;&#20316;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#20041;&#25628;&#32034;&#30340;&#23436;&#25972;&#21407;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
User experience (UX) is a part of human-computer interaction (HCI) research and focuses on increasing intuitiveness, transparency, simplicity, and trust for system users. Most of the UX research for machine learning (ML) or natural language processing (NLP) focuses on a data-driven methodology, i.e., it fails to focus on users' requirements, and engages domain users mainly for usability evaluation. Moreover, more typical UX methods tailor the systems towards user usability, unlike learning about the user needs first. The paper proposes a methodology for integrating generative UX research into developing domain NLP applications. Generative UX research employs domain users at the initial stages of prototype development, i.e., ideation and concept evaluation, and the last stage for evaluating the change in user value. In the case study, we report the full-cycle prototype development of a domain-specific semantic search for daily operations in the process industry. Our case study shows tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#24050;&#25104;&#20026;&#32452;&#32455;&#21307;&#23398;&#30693;&#35782;&#30340;&#26377;&#32467;&#26500;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21307;&#23398;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#24378;&#35843;&#20102;&#22312;HKG&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#32508;&#36848;&#26159;HKG&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;HKG&#26500;&#24314;&#30340;&#27969;&#31243;&#21644;&#20851;&#38190;&#25216;&#26415;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#21644;&#36890;&#36807;&#38598;&#25104;&#65289;&#65292;&#20197;&#21450;&#24120;&#35265;&#30340;&#21033;&#29992;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#22522;&#20110;&#27169;&#22411;&#65289;&#12290;&#20026;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#25429;&#33719;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65288;&#35813;&#36164;&#28304;&#23384;&#20648;&#20110;https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase&#65289;&#32452;&#32455;&#20102;&#29616;&#26377;&#30340;HKG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#24212;&#29992;&#37096;&#20998;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#65292;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.11169</link><description>&lt;p&gt;
&#22312;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#35821;&#20041;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Meaning in Language Models Trained on Programs. (arXiv:2305.11169v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#65292;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#35757;&#32451;&#21482;&#26159;&#25191;&#34892;&#25991;&#26412;&#19978;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#31243;&#24207;&#35821;&#26009;&#24211;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#27599;&#20010;&#31243;&#24207;&#37117;&#20197;&#65288;&#25991;&#26412;&#65289;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#30340;&#24418;&#24335;&#20316;&#20026;&#35268;&#33539;&#12290;&#19982;&#31243;&#24207;&#19968;&#36215;&#24037;&#20316;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#23450;&#20041;&#19982;&#35821;&#35328;&#20013;&#26377;&#20851;&#21547;&#20041;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#27491;&#30830;&#24615;&#21644;&#35821;&#20041;&#65289;&#65292;&#20351;&#24471;&#31243;&#24207;&#32508;&#21512;&#25104;&#20026;&#19968;&#20010;&#20013;&#38388;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#34920;&#24449;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21547;&#20041;&#30340;&#23384;&#22312;&#65288;&#25110;&#19981;&#23384;&#22312;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;Transformer&#27169;&#22411;&#65292;&#28982;&#21518;&#25506;&#26597;&#20102;&#24050;&#32463;&#23436;&#25104;&#35268;&#33539;&#30340;&#31243;&#24207;&#26102;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#23613;&#31649;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#24403;&#21069;&#21644;&#26410;&#26469;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#12290;&#27492;&#22806;&#65292;&#32447;&#24615;&#25506;&#27979;&#22120;&#30340;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#24378;&#26377;&#21147;&#12289;&#32479;&#35745;&#23398;&#26174;&#33879;&#22320;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present evidence that language models can learn meaning despite being trained only to perform next token prediction on text, specifically a corpus of programs. Each program is preceded by a specification in the form of (textual) input-output examples. Working with programs enables us to precisely define concepts relevant to meaning in language (e.g., correctness and semantics), making program synthesis well-suited as an intermediate testbed for characterizing the presence (or absence) of meaning in language models.  We first train a Transformer model on the corpus of programs, then probe the trained model's hidden states as it completes a program given a specification. Despite providing no inductive bias toward learning the semantics of the language, we find that a linear probe is able to extract abstractions of both current and future program states from the model states. Moreover, there is a strong, statistically significant correlation between the accuracy of the probe and the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#35789;&#27719;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#21387;&#21147;&#20419;&#36827;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#30340;&#20986;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25509;&#25910;&#32773;&#26080;&#27861;&#22788;&#29702;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#36865;&#32773;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#30340;&#21046;&#32422;&#22240;&#32032;&#23454;&#29616;&#27807;&#36890;&#12290;</title><link>http://arxiv.org/abs/2305.05821</link><description>&lt;p&gt;
&#29615;&#22659;&#32422;&#26463;&#19979;&#30340;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Context-dependent communication under environmental constraints. (arXiv:2305.05821v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#35789;&#27719;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#21387;&#21147;&#20419;&#36827;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#30340;&#20986;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25509;&#25910;&#32773;&#26080;&#27861;&#22788;&#29702;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#36865;&#32773;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#30340;&#21046;&#32422;&#22240;&#32032;&#23454;&#29616;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#27807;&#36890;&#19981;&#33021;&#31616;&#21333;&#22320;&#36890;&#36807;&#21457;&#36865;&#20855;&#26377;&#29420;&#31435;&#20110;&#24773;&#22659;&#24847;&#20041;&#30340;&#20449;&#21495;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#20197;&#32463;&#20856;&#30340;Lewis(1969)&#20449;&#21495;&#27169;&#22411;&#30340;&#21464;&#20307;&#20026;&#22522;&#30784;&#65292;&#25506;&#35752;&#22312;&#24773;&#22659;&#21270;&#22330;&#26223;&#19979;&#20135;&#29983;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#30340;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26368;&#23567;&#21270;&#35789;&#27719;&#37327;&#30340;&#21387;&#21147;&#19979;&#65292;&#36825;&#31181;&#27807;&#36890;&#30340;&#20986;&#29616;&#26159;&#36275;&#22815;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#33021;&#20351;&#31526;&#21495;&#21547;&#20041;&#24471;&#21040;&#24773;&#22659;&#21306;&#20998;&#30340;&#29615;&#22659;&#26465;&#20214;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25509;&#21463;&#32773;&#30340;&#25351;&#20195;&#36873;&#25321;&#21463;&#21040;&#29615;&#22659;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#36865;&#32773;&#21487;&#20197;&#21333;&#26041;&#38754;&#22320;&#21033;&#29992;&#36825;&#20123;&#38480;&#21046;&#65292;&#32780;&#26080;&#38656;&#25509;&#25910;&#32773;&#20855;&#26377;&#28548;&#28165;&#27495;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#24120;&#35265;&#30340;&#20551;&#35774;&#19968;&#33268;&#65292;&#21457;&#36865;&#32773;&#23545;&#24773;&#22659;&#30340;&#24847;&#35782;&#20284;&#20046;&#26159;&#38656;&#35201;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#26159;&#19968;&#31181;&#22810;&#23618;&#27425;&#30340;&#24773;&#22659;&#21270;&#29616;&#35937;&#65292;&#20854;&#21463;&#29615;&#22659;&#29305;&#24615;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is significant evidence that real-world communication cannot be reduced to sending signals with context-independent meaning. In this work, based on a variant of the classical Lewis (1969) signaling model, we explore the conditions for the emergence of context-dependent communication in a situated scenario. In particular, we demonstrate that pressure to minimise the vocabulary size is sufficient for such emergence. At the same time, we study the environmental conditions and cognitive capabilities that enable contextual disambiguation of symbol meanings. We show that environmental constraints on the receiver's referent choice can be unilaterally exploited by the sender, without disambiguation capabilities on the receiver's end. Consistent with common assumptions, the sender's awareness of the context appears to be required for contextual communication. We suggest that context-dependent communication is a situated multilayered phenomenon, crucially influenced by environment properti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GenExpan&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#21069;&#32512;&#26641;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03531</link><description>&lt;p&gt;
&#20174;&#26816;&#32034;&#21040;&#29983;&#25104;&#65306;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Retrieval to Generation: Efficient and Effective Entity Set Expansion. (arXiv:2304.03531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GenExpan&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#21069;&#32512;&#26641;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38598;&#25193;&#23637;&#65288;ESE&#65289;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#25193;&#23637;&#30001;&#23567;&#30340;&#31181;&#23376;&#23454;&#20307;&#38598;&#25551;&#36848;&#30340;&#30446;&#26631;&#35821;&#20041;&#31867;&#30340;&#23454;&#20307;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;ESE&#26041;&#27861;&#26159;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#38656;&#35201;&#25552;&#21462;&#23454;&#20307;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#35745;&#31639;&#31181;&#23376;&#23454;&#20307;&#21644;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20004;&#20010;&#30446;&#30340;&#65292;&#23427;&#20204;&#24517;&#39035;&#36845;&#20195;&#22320;&#36941;&#21382;&#35821;&#26009;&#24211;&#21644;&#25968;&#25454;&#38598;&#20013;&#25552;&#20379;&#30340;&#23454;&#20307;&#35789;&#27719;&#65292;&#23548;&#33268;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#26816;&#32034;&#30340;ESE&#26041;&#27861;&#28040;&#32791;&#30340;&#26102;&#38388;&#19982;&#23454;&#20307;&#35789;&#27719;&#21644;&#35821;&#26009;&#24211;&#30340;&#22823;&#23567;&#25104;&#32447;&#24615;&#22686;&#38271;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;ESE&#26694;&#26550;&#65292;Generative Entity Set Expansion (GenExpan)&#65292;&#23427;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;ESE&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#21069;&#32512;&#26641;&#26469;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Set Expansion (ESE) is a critical task aiming to expand entities of the target semantic class described by a small seed entity set. Most existing ESE methods are retrieval-based frameworks that need to extract the contextual features of entities and calculate the similarity between seed entities and candidate entities. To achieve the two purposes, they should iteratively traverse the corpus and the entity vocabulary provided in the datasets, resulting in poor efficiency and scalability. The experimental results indicate that the time consumed by the retrieval-based ESE methods increases linearly with entity vocabulary and corpus size. In this paper, we firstly propose a generative ESE framework, Generative Entity Set Expansion (GenExpan), which utilizes a generative pre-trained language model to accomplish ESE task. Specifically, a prefix tree is employed to guarantee the validity of entity generation, and automatically generated class names are adopted to guide the model to gen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.07088</link><description>&lt;p&gt;
&#35270;&#35273;&#23398;&#20064;&#32773;&#36935;&#35265;Web&#22270;&#20687;-&#25991;&#26412;&#23545;
&lt;/p&gt;
&lt;p&gt;
Vision Learners Meet Web Image-Text Pairs. (arXiv:2301.07088v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#22312;&#32500;&#25252;&#33391;&#22909;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#32771;&#34385;&#21040;&#32593;&#32476;&#25968;&#25454;&#30340;&#20986;&#33394;&#21487;&#20280;&#32553;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#24212;&#35813;&#22522;&#20110;&#22024;&#26434;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#37197;&#23545;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#22914;&#27492;&#35774;&#32622;&#19979;&#65292;&#23545;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#20195;&#34920;&#24615;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#34987;&#23631;&#34109;&#30340;&#35757;&#32451;&#30446;&#26631;&#30340;&#21333;&#27169;&#24335;&#26041;&#27861;&#21644;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#35757;&#32451;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#24182;&#19981;&#27604;&#21333;&#27169;&#24577;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#35270;&#35282;&#26469;&#35299;&#37322;&#36825;&#20123;&#22522;&#20934;&#32467;&#26524;&#65292;&#36825;&#25552;&#20379;&#20102;&#22914;&#20309;&#35774;&#35745;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;&#30340;&#35265;&#35299;&#12290;&#21463;&#21040;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#22810;&#27169;&#24335;&#29983;&#25104;&#22120;&#65288;MUG&#65289;&#65292;&#23427;&#20174;&#21487;&#20280;&#32553;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;MUG&#22312;&#20960;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CIFAR-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;3.4&#65285;&#65292;&#22312;STL-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;2.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most recent self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data. MUG ach
&lt;/p&gt;</description></item></channel></rss>