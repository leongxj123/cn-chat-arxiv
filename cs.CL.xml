<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#22235;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#22312;&#25163;&#26426;GPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#22312;&#31227;&#21160;&#25512;&#26029;&#24341;&#25806;Transformer-Lite&#20013;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#36895;&#24230;&#21644;&#38477;&#20302;&#20102;&#25163;&#26426;&#28382;&#21518;&#65292;&#20174;&#32780;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.20041</link><description>&lt;p&gt;
Transformer-Lite: &#22312;&#25163;&#26426;GPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22235;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#22312;&#25163;&#26426;GPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#22312;&#31227;&#21160;&#25512;&#26029;&#24341;&#25806;Transformer-Lite&#20013;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#36895;&#24230;&#21644;&#38477;&#20302;&#20102;&#25163;&#26426;&#28382;&#21518;&#65292;&#20174;&#32780;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26234;&#33021;&#21161;&#25163;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#32763;&#35793;&#21644;&#25163;&#26426;&#19978;&#30340;&#22810;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35774;&#22791;&#19978;LLM&#37096;&#32626;&#26041;&#27861;&#36895;&#24230;&#36739;&#24930;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19981;&#20339;&#12290;&#20026;&#20102;&#23454;&#29616;&#22312;&#35774;&#22791;GPU&#19978;&#39640;&#25928;&#37096;&#32626;LLM&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#20248;&#21270;&#25216;&#26415;&#65306;&#65288;a&#65289;&#22522;&#20110;&#31526;&#21495;&#34920;&#36798;&#30340;&#26041;&#27861;&#25903;&#25345;&#21160;&#24577;&#24418;&#29366;&#27169;&#22411;&#25512;&#26029;&#65307;&#65288;b&#65289;&#25805;&#20316;&#20248;&#21270;&#21644;&#25191;&#34892;&#20248;&#20808;&#32423;&#35774;&#32622;&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#24182;&#20943;&#23569;&#25163;&#26426;&#28382;&#21518;&#65307;&#65288;c&#65289;&#19968;&#31181;&#21517;&#20026;M0E4&#30340;FP4&#37327;&#21270;&#26041;&#27861;&#20197;&#20943;&#23569;&#21435;&#37327;&#21270;&#24320;&#38144;&#65307;&#65288;d&#65289;&#19968;&#31181;&#22522;&#20110;&#23376;&#24352;&#37327;&#30340;&#25216;&#26415;&#26469;&#22312;LLM&#25512;&#26029;&#21518;&#28040;&#38500;&#22797;&#21046;KV&#32531;&#23384;&#30340;&#38656;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#31227;&#21160;&#25512;&#26029;&#24341;&#25806;Transformer-Lite&#20013;&#23454;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#35813;&#24341;&#25806;&#19982;&#39640;&#36890;&#21644;MTK&#22788;&#29702;&#22120;&#20860;&#23481;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;Transformer-Lite&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20041v1 Announce Type: new  Abstract: The Large Language Model (LLM) is widely employed for tasks such as intelligent assistants, text summarization, translation, and multi-modality on mobile phones. However, the current methods for on-device LLM deployment maintain slow inference speed, which causes poor user experience. To facilitate high-efficiency LLM deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 quantization method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference. Furthermore, we implement these methods in our mobile inference engine, Transformer-Lite, which is compatible with both Qualcomm and MTK processors. We evaluated Transformer-Lite's performance u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#21487;&#24402;&#23646;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#20808;&#22686;&#21152;&#23646;&#24615;&#65292;&#28982;&#21518;&#29983;&#25104;&#8221;&#30340;&#26041;&#24335;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#20026;&#20869;&#23481;&#36873;&#25321;&#12289;&#21477;&#23376;&#35268;&#21010;&#21644;&#24207;&#21015;&#21477;&#23376;&#29983;&#25104;&#19977;&#20010;&#27493;&#39588;&#65292;&#20197;&#31616;&#21270;&#24341;&#29992;&#39564;&#35777;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.17104</link><description>&lt;p&gt;
&#39318;&#20808;&#22686;&#21152;&#23646;&#24615;&#65292;&#28982;&#21518;&#29983;&#25104;&#65306;&#23616;&#37096;&#21487;&#24402;&#23646;&#30340;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Attribute First, then Generate: Locally-attributable Grounded Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#21487;&#24402;&#23646;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#20808;&#22686;&#21152;&#23646;&#24615;&#65292;&#28982;&#21518;&#29983;&#25104;&#8221;&#30340;&#26041;&#24335;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#20026;&#20869;&#23481;&#36873;&#25321;&#12289;&#21477;&#23376;&#35268;&#21010;&#21644;&#24207;&#21015;&#21477;&#23376;&#29983;&#25104;&#19977;&#20010;&#27493;&#39588;&#65292;&#20197;&#31616;&#21270;&#24341;&#29992;&#39564;&#35777;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#24187;&#35273;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#23646;&#24615;&#25991;&#26412;&#29983;&#25104;&#19978;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#29992;&#25903;&#25345;&#28304;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#21152;&#20837;&#25903;&#25345;&#25991;&#26412;&#20197;&#36827;&#34892;&#20107;&#21518;&#20107;&#23454;&#26680;&#26597;&#21644;&#26356;&#27491;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24341;&#29992;&#36890;&#24120;&#25351;&#21521;&#25972;&#20010;&#25991;&#26723;&#25110;&#27573;&#33853;&#65292;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#32321;&#37325;&#30340;&#39564;&#35777;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23616;&#37096;&#21487;&#24402;&#23646;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#37325;&#28857;&#25918;&#22312;&#31616;&#27905;&#30340;&#23646;&#24615;&#19978;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;&#8220;&#20808;&#22686;&#21152;&#23646;&#24615;&#65292;&#28982;&#21518;&#29983;&#25104;&#8221;&#65292;&#23558;&#20256;&#32479;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#19977;&#20010;&#30452;&#35266;&#30340;&#27493;&#39588;&#65306;&#20869;&#23481;&#36873;&#25321;&#12289;&#21477;&#23376;&#35268;&#21010;&#21644;&#24207;&#21015;&#21477;&#23376;&#29983;&#25104;&#12290;&#36890;&#36807;&#39318;&#20808;&#35782;&#21035;&#30456;&#20851;&#26469;&#28304;&#37096;&#20998;&#65288;&#8220;&#20808;&#36873;&#25321;&#8221;&#65289;&#65292;&#28982;&#21518;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23545;&#23427;&#20204;&#36827;&#34892;&#26465;&#20214;&#21270;&#65288;&#8220;&#28982;&#21518;&#29983;&#25104;&#8221;&#65289;&#65292;&#25105;&#20204;&#30830;&#20445;&#36825;&#20123;&#37096;&#20998;&#20063;&#20316;&#20026;&#36755;&#20986;&#30340;&#32454;&#31890;&#24230;&#23646;&#24615;&#65288;&#8220;&#36873;&#25321;&#8221;&#21464;&#20026;&#8220;&#23646;&#24615;&#8221;&#65289;&#12290; &#22312;Mu&#19978;&#32463;&#36807;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17104v1 Announce Type: new  Abstract: Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections. Yet, these citations often point to entire documents or paragraphs, burdening users with extensive verification work. In this paper, we introduce a locally-attributable text generation approach, prioritizing concise attributions. Our method, named ``Attribute First, then Generate'', breaks down the conventional end-to-end generation process into three intuitive steps: content selection, sentence planning, and sequential sentence generation. By initially identifying relevant source segments (``select first'') and then conditioning the generation process on them (``then generate''), we ensure these segments also act as the output's fine-grained attributions (``select'' becomes ``attribute''). Tested on Mu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.13213</link><description>&lt;p&gt;
&#20174;&#34920;&#29616;&#24615;&#20260;&#23475;&#21040;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;:&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#27493;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20854;&#23545;&#24050;&#32463;&#36793;&#32536;&#21270;&#20154;&#32676;&#30340;&#19981;&#21033;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20943;&#36731;&#25514;&#26045;&#26469;&#24320;&#21457;&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#65292;&#27604;&#22914;&#30417;&#30563;&#24335;&#30340;&#23433;&#20840;&#23450;&#21521;&#24494;&#35843;&#21644;&#21033;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#20869;&#22312;&#20559;&#35265;&#20173;&#23384;&#22312;&#22810;&#37325;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#20026;&#20102;&#23433;&#20840;&#32780;&#20248;&#21270;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23637;&#31034;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#65292;&#27604;&#22914;&#20986;&#20110;&#39044;&#38450;&#25514;&#26045;&#32780;&#20542;&#21521;&#20110;&#19981;&#22238;&#24212;&#26576;&#20123;&#35831;&#27714;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#35760;&#24405;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26126;&#26174;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13213v1 Announce Type: cross  Abstract: Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluatin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.06725</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#37325;&#35201;&#24615;&#26426;&#21046;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#22522;&#20110;&#23398;&#29983;&#30340;&#21382;&#21490;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#30693;&#35782;&#25484;&#25569;&#31243;&#24230;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;KT&#65288;DLKT&#65289;&#26041;&#27861;&#22312;KT&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#22914;&#39044;&#31639;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35266;&#23519;&#21040;&#30340;&#20114;&#21160;&#38750;&#24120;&#26377;&#38480;&#65292;&#21363;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#12290;&#30452;&#25509;&#22312;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DLKT&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#24456;&#38590;&#36873;&#25321;&#36866;&#24403;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;KT&#26694;&#26550;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;&#21463;&#30427;&#34892;&#30340;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
&lt;/p&gt;</description></item><item><title>CLIcK&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;1,995&#20010;&#38382;&#31572;&#23545;&#30340;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#38889;&#35821;&#22522;&#20934;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#32780;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.06412</link><description>&lt;p&gt;
CLIcK&#65306;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06412
&lt;/p&gt;
&lt;p&gt;
CLIcK&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;1,995&#20010;&#38382;&#31572;&#23545;&#30340;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#38889;&#35821;&#22522;&#20934;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#32780;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38024;&#23545;&#38889;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#32570;&#20047;&#27979;&#35797;&#24517;&#35201;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#30693;&#35782;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#30340;&#35768;&#22810;&#38889;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#32763;&#35793;&#20174;&#33521;&#35821;&#23545;&#24212;&#25968;&#25454;&#38598;&#20013;&#34893;&#29983;&#20986;&#26469;&#30340;&#65292;&#23427;&#20204;&#36890;&#24120;&#24573;&#35270;&#19981;&#21516;&#30340;&#25991;&#21270;&#32972;&#26223;&#12290;&#20165;&#26377;&#23569;&#25968;&#20174;&#38889;&#22269;&#25968;&#25454;&#28304;&#25429;&#25417;&#25991;&#21270;&#30693;&#35782;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#30340;&#20165;&#26377;&#20559;&#35265;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#29421;&#31364;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CLIcK&#30340;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1,995&#20010;&#38382;&#31572;&#23545;&#12290;CLIcK&#23558;&#20854;&#25968;&#25454;&#26469;&#28304;&#20110;&#38889;&#22269;&#23448;&#26041;&#32771;&#35797;&#21644;&#25945;&#31185;&#20070;&#65292;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#31867;&#21035;&#65288;&#35821;&#35328;&#21644;&#25991;&#21270;&#65289;&#19979;&#30340;11&#20010;&#31867;&#21035;&#12290;&#23545;&#20110;CLIcK&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#21738;&#20123;&#25991;&#21270;&#21644;&#35821;&#35328;&#30693;&#35782;&#30340;&#32454;&#31890;&#24230;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06412v1 Announce Type: new  Abstract: Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as bias and hate speech detection are offered. To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in CLIcK, we provide fine-grained annotation of which cultural and linguistic knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;KG-Rank&#26694;&#26550;&#65292;&#21033;&#29992;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#25490;&#21517;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05881</link><description>&lt;p&gt;
KG-Rank: &#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#25490;&#21517;&#25216;&#26415;&#22686;&#24378;&#21307;&#23398;&#38382;&#31572;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;KG-Rank&#26694;&#26550;&#65292;&#21033;&#29992;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#25490;&#21517;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25512;&#36827;&#20102;&#21307;&#30103;&#20445;&#20581;&#21019;&#26032;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#20559;&#31163;&#21307;&#30103;&#20107;&#23454;&#21644;&#22266;&#26377;&#20559;&#35265;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#20020;&#24202;&#35774;&#32622;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22686;&#24378;&#22411;LLM&#26694;&#26550;KG-Rank&#65292;&#21033;&#29992;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19982;&#25490;&#21517;&#21644;&#37325;&#26032;&#25490;&#21517;&#25216;&#26415;&#65292;&#26088;&#22312;&#25913;&#36827;&#21307;&#23398;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#38382;&#31572;&#65288;QA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#25910;&#21040;&#38382;&#39064;&#21518;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#21307;&#23398;KG&#20013;&#26816;&#32034;&#19977;&#20803;&#32452;&#20197;&#25910;&#38598;&#20107;&#23454;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#24212;&#29992;&#25490;&#21517;&#26041;&#27861;&#26469;&#31934;&#32454;&#35843;&#25972;&#36825;&#20123;&#19977;&#20803;&#32452;&#30340;&#39034;&#24207;&#65292;&#26088;&#22312;&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;KG-Rank&#26159;&#39318;&#20010;&#23558;&#25490;&#21517;&#27169;&#22411;&#19982;KG&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#38271;&#31572;&#26696;&#30340;&#21307;&#23398;&#38382;&#31572;&#24212;&#29992;&#12290;&#23545;&#22235;&#20010;&#36873;&#23450;&#30340;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;KG-Rank&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05881v1 Announce Type: new  Abstract: Large Language Models (LLMs) have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases. In this work, we develop an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph (KG) with ranking and re-ranking techniques, aiming to improve free-text question-answering (QA) in the medical domain. Specifically, upon receiving a question, we initially retrieve triplets from a medical KG to gather factual information. Subsequently, we innovatively apply ranking methods to refine the ordering of these triplets, aiming to yield more precise answers. To the best of our knowledge, KG-Rank is the first application of ranking models combined with KG in medical QA specifically for generating long answers. Evaluation of four selected medical QA datasets shows that KG-Rank achieves a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22810;&#27169;&#24577;&#23545;&#20107;&#23454;&#25512;&#29702;&#24773;&#24863;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20928;&#21270;&#21644;&#32531;&#35299;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05023</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#24046;&#20928;&#21270;&#23454;&#29616;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Multimodal Sentiment Analysis Debiasing via Bias Purification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05023
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22810;&#27169;&#24577;&#23545;&#20107;&#23454;&#25512;&#29702;&#24773;&#24863;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20928;&#21270;&#21644;&#32531;&#35299;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65288;MSA&#65289;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#65288;&#22914;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#38899;&#39057;&#65289;&#30340;&#19982;&#24773;&#24863;&#30456;&#20851;&#32447;&#32034;&#26469;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;MSA&#20219;&#21153;&#26222;&#36941;&#21463;&#21040;&#26410;&#32463;&#35745;&#21010;&#30340;&#25968;&#25454;&#38598;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22810;&#27169;&#24577;&#35805;&#35821;&#32423;&#26631;&#31614;&#20559;&#35265;&#21644;&#21333;&#35789;&#32423;&#19978;&#19979;&#25991;&#20559;&#35265;&#12290;&#36825;&#20123;&#26377;&#23475;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#35823;&#23548;&#27169;&#22411;&#19987;&#27880;&#20110;&#32479;&#35745;&#25463;&#24452;&#21644;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#32780;&#38750;&#20256;&#32479;&#20284;&#28982;&#24615;&#30340;&#22810;&#27169;&#24577;&#23545;&#20107;&#23454;&#25512;&#29702;&#24773;&#24863;&#65288;MCIS&#65289;&#20998;&#26512;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#19968;&#20010;&#22240;&#26524;&#22270;&#26469;&#21457;&#29616;&#24050;&#35757;&#32451;&#30340;&#22522;&#20934;&#27169;&#22411;&#20013;&#30340;&#26377;&#23475;&#20559;&#35265;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#32473;&#23450;&#19968;&#20010;&#20107;&#23454;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;MCIS&#24819;&#35937;&#20004;&#31181;&#23545;&#20107;&#23454;&#24773;&#24418;&#65292;&#20197;&#20928;&#21270;&#21644;&#32531;&#35299;&#36825;&#20123;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;MCIS&#21487;&#20197;&#20174;&#20559;&#24046;&#20013;&#20570;&#20986;&#19981;&#24102;&#20559;&#35265;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05023v1 Announce Type: new  Abstract: Multimodal Sentiment Analysis (MSA) aims to understand human intentions by integrating emotion-related clues from diverse modalities, such as visual, language, and audio. Unfortunately, the current MSA task invariably suffers from unplanned dataset biases, particularly multimodal utterance-level label bias and word-level context bias. These harmful biases potentially mislead models to focus on statistical shortcuts and spurious correlations, causing severe performance bottlenecks. To alleviate these issues, we present a Multimodal Counterfactual Inference Sentiment (MCIS) analysis framework based on causality rather than conventional likelihood. Concretely, we first formulate a causal graph to discover harmful biases from already-trained vanilla models. In the inference phase, given a factual multimodal input, MCIS imagines two counterfactual scenarios to purify and mitigate these biases. Then, MCIS can make unbiased decisions from biase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3.5&#21644;GPT-4&#20174;&#20135;&#21697;&#26631;&#39064;&#21644;&#25551;&#36848;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#23646;&#24615;&#20540;&#30340;&#28508;&#21147;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;WDC PAVE&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.02130</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#20135;&#21697;&#23646;&#24615;&#20540;
&lt;/p&gt;
&lt;p&gt;
Using LLMs for the Extraction and Normalization of Product Attribute Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3.5&#21644;GPT-4&#20174;&#20135;&#21697;&#26631;&#39064;&#21644;&#25551;&#36848;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#23646;&#24615;&#20540;&#30340;&#28508;&#21147;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;WDC PAVE&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;&#20135;&#21697;&#25552;&#20379;&#36890;&#24120;&#21253;&#25324;&#25991;&#26412;&#20135;&#21697;&#26631;&#39064;&#21644;&#25991;&#26412;&#20135;&#21697;&#25551;&#36848;&#12290;&#20026;&#20102;&#25552;&#20379;&#35832;&#22914;&#20998;&#38754;&#20135;&#21697;&#36807;&#28388;&#25110;&#22522;&#20110;&#20869;&#23481;&#30340;&#20135;&#21697;&#25512;&#33616;&#31561;&#21151;&#33021;&#65292;&#32593;&#31449;&#38656;&#35201;&#20174;&#38750;&#32467;&#26500;&#21270;&#20135;&#21697;&#25551;&#36848;&#20013;&#25552;&#21462;&#23646;&#24615;&#20540;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#65292;&#20174;&#20135;&#21697;&#26631;&#39064;&#21644;&#20135;&#21697;&#25551;&#36848;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#23646;&#24615;&#20540;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WDC&#20135;&#21697;&#23646;&#24615;-&#20540;&#25552;&#21462;&#65288;WDC PAVE&#65289;&#25968;&#25454;&#38598;&#12290;WDC PAVE&#21253;&#21547;&#26469;&#33258;&#25552;&#20379;schema.org&#27880;&#37322;&#30340;87&#20010;&#32593;&#31449;&#30340;&#20135;&#21697;&#25552;&#20379;&#12290;&#36825;&#20123;&#25552;&#20379;&#23646;&#20110;&#20116;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#27599;&#20010;&#31867;&#21035;&#37117;&#20855;&#26377;&#19968;&#32452;&#29305;&#23450;&#30340;&#23646;&#24615;&#12290;&#35813;&#25968;&#25454;&#38598;&#20197;&#20004;&#31181;&#24418;&#24335;&#25552;&#20379;&#25163;&#21160;&#39564;&#35777;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65306;&#65288;i&#65289;&#30452;&#25509;&#25552;&#21462;&#30340;&#20540;&#21644;&#65288;ii&#65289;&#35268;&#33539;&#21270;&#30340;&#23646;&#24615;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02130v1 Announce Type: new  Abstract: Product offers on e-commerce websites often consist of a textual product title and a textual product description. In order to provide features such as faceted product filtering or content-based product recommendation, the websites need to extract attribute-value pairs from the unstructured product descriptions. This paper explores the potential of using large language models (LLMs), such as OpenAI's GPT-3.5 and GPT-4, to extract and normalize attribute values from product titles and product descriptions. For our experiments, we introduce the WDC Product Attribute-Value Extraction (WDC PAVE) dataset. WDC PAVE consists of product offers from 87 websites that provide schema.org annotations. The offers belong to five different categories, each featuring a specific set of attributes. The dataset provides manually verified attribute-value pairs in two forms: (i) directly extracted values and (ii) normalized attribute values. The normalization 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00791</link><description>&lt;p&gt;
$\textit{L+M-24}$&#65306;&#22312;ACL 2024&#24180;&#20026;&#35821;&#35328;+&#20998;&#23376;&#26500;&#24314;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
$\textit{L+M-24}$: Building a Dataset for Language + Molecules @ ACL 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00791
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;-&#20998;&#23376;&#27169;&#22411;&#24050;&#25104;&#20026;&#20998;&#23376;&#21457;&#29616;&#21644;&#29702;&#35299;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#23376;-&#35821;&#35328;&#23545;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#24050;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26377;&#20197;&#19979;&#20960;&#31181;&#31867;&#22411;&#65306;1) &#23567;&#35268;&#27169;&#19988;&#20174;&#29616;&#26377;&#25968;&#25454;&#24211;&#20013;&#25235;&#21462;&#65292;2) &#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#19988;&#36890;&#36807;&#22312;&#31185;&#23398;&#25991;&#29486;&#19978;&#25191;&#34892;&#23454;&#20307;&#38142;&#25509;&#26469;&#26500;&#24314;&#65292;3) &#36890;&#36807;&#23558;&#23646;&#24615;&#39044;&#27979;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20351;&#29992;&#27169;&#26495;&#32780;&#26500;&#24314;&#12290;&#22312;&#26412;&#25991;&#26723;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#21019;&#24314;&#30340;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#12290;&#29305;&#21035;&#22320;&#65292;$\textit{L+M-24}$&#26088;&#22312;&#38598;&#20013;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#39033;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00791v1 Announce Type: cross  Abstract: Language-molecule models have emerged as an exciting direction for molecular discovery and understanding. However, training these models is challenging due to the scarcity of molecule-language pair datasets. At this point, datasets have been released which are 1) small and scraped from existing databases, 2) large but noisy and constructed by performing entity linking on the scientific literature, and 3) built by converting property prediction datasets to natural language using templates. In this document, we detail the $\textit{L+M-24}$ dataset, which has been created for the Language + Molecules Workshop shared task at ACL 2024. In particular, $\textit{L+M-24}$ is designed to focus on three key benefits of natural language in molecule design: compositionality, functionality, and abstraction.
&lt;/p&gt;</description></item><item><title>&#31532;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#19981;&#19968;&#23450;&#20195;&#34920;&#26368;&#32456;&#25991;&#26412;&#36755;&#20986;&#65292;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#19982;&#29992;&#25143;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2402.14499</link><description>&lt;p&gt;
"&#25105;&#30340;&#31572;&#26696;&#26159;C": &#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31532;&#19968;&#20010;&#20196;&#29260;&#27010;&#29575;&#19982;&#25991;&#26412;&#31572;&#26696;&#19981;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
"My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14499
&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#19981;&#19968;&#23450;&#20195;&#34920;&#26368;&#32456;&#25991;&#26412;&#36755;&#20986;&#65292;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#19982;&#29992;&#25143;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14499v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#35821;&#35328;&#29983;&#25104;&#30340;&#24320;&#25918;&#24615;&#36136;&#20351;&#24471;&#35780;&#20272;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#35780;&#20272;&#26041;&#27861;&#26159;&#20351;&#29992;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#38480;&#21046;&#21709;&#24212;&#31354;&#38388;&#12290;&#28982;&#21518;&#36890;&#36807;&#25490;&#21517;&#20505;&#36873;&#31572;&#26696;&#30340;&#31532;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#30340;&#23545;&#25968;&#27010;&#29575;&#26469;&#35780;&#20272;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#31532;&#19968;&#20010;&#20196;&#29260;&#21487;&#33021;&#19981;&#19968;&#33268;&#22320;&#21453;&#26144;&#26368;&#32456;&#30340;&#21709;&#24212;&#36755;&#20986;&#65292;&#22240;&#20026;&#27169;&#22411;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#21709;&#24212;&#39118;&#26684;&#65292;&#20363;&#22914;&#20197;"&#30830;&#23450;"&#24320;&#22836;&#25110;&#25298;&#32477;&#22238;&#31572;&#12290;&#22240;&#27492;&#65292;MCQ&#35780;&#20272;&#26080;&#27861;&#34920;&#26126;&#27169;&#22411;&#19982;&#29992;&#25143;&#20114;&#21160;&#26102;&#30340;&#34892;&#20026;&#12290;&#20294;&#24046;&#36317;&#26377;&#22810;&#22823;&#21602;&#65311;&#25105;&#20204;&#35780;&#20272;&#20102;&#31532;&#19968;&#20010;&#20196;&#29260;&#35780;&#20272;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#19982;&#25991;&#26412;&#36755;&#20986;&#30340;&#19968;&#33268;&#24615;&#65292;&#21363;&#26368;&#32456;&#36873;&#39033;&#36873;&#25321;&#12289;&#25298;&#32477;&#29575;&#12289;&#36873;&#25321;&#20998;&#24067;&#21644;&#22312;&#25552;&#31034;&#25200;&#21160;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#20005;&#37325;&#19981;&#19968;&#33268;&#65292;&#36798;&#21040;60%&#20197;&#19978;&#30340;&#19981;&#21305;&#37197;&#29575;&#12290;&#27169;&#22411;&#38750;&#24120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14499v1 Announce Type: new  Abstract: The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with "Sure" or refusing to answer. Consequently, MCQ evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%. Models heavil
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#35780;&#20272;LLM&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#30701;&#36890;&#29992;&#30701;&#35821;&#21487;&#20197;&#27450;&#39575;LLMs&#25552;&#20379;&#39640;&#35780;&#20998;&#65292;&#36825;&#31181;&#25915;&#20987;&#23545;&#20110;&#20174;&#31616;&#21333;&#30340;&#20018;&#32852;&#25915;&#20987;&#21040;&#36716;&#31227;&#23398;&#20064;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.14016</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#26159;&#21542;&#31283;&#20581;&#65311;&#30740;&#31350;&#36890;&#29992;&#23545;&#25239;&#25915;&#20987;&#23545;&#38646;&#26679;&#28857;LLM&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#35780;&#20272;LLM&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#30701;&#36890;&#29992;&#30701;&#35821;&#21487;&#20197;&#27450;&#39575;LLMs&#25552;&#20379;&#39640;&#35780;&#20998;&#65292;&#36825;&#31181;&#25915;&#20987;&#23545;&#20110;&#20174;&#31616;&#21333;&#30340;&#20018;&#32852;&#25915;&#20987;&#21040;&#36716;&#31227;&#23398;&#20064;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#24378;&#22823;&#30340;&#38646;&#26679;&#28857;&#35780;&#20272;&#32773;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#31508;&#35797;&#25110;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#31561;&#24773;&#22659;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#20998;&#26512;&#23545;&#25239;&#35797;&#22270;&#25805;&#32437;&#36755;&#20986;&#30340;&#35780;&#21028;LLMs&#30340;&#33030;&#24369;&#24615;&#30340;&#24037;&#20316;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;&#35780;&#20272;LLMs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#23547;&#25214;&#30701;&#36890;&#29992;&#30701;&#35821;&#65292;&#24403;&#38468;&#21152;&#21040;&#25991;&#26412;&#26102;&#21487;&#20197;&#27450;&#39575;LLMs&#25552;&#20379;&#39640;&#35780;&#20998;&#12290;&#22312;SummEval&#21644;TopicalChat&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#35780;&#20998;&#21644;&#20004;&#20004;LLM&#27604;&#36739;&#35780;&#20272;&#37117;&#23481;&#26131;&#21463;&#21040;&#31616;&#21333;&#30340;&#20018;&#32852;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;LLM&#35780;&#20998;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#21487;&#20197;&#20135;&#29983;&#26368;&#39640;&#35780;&#20998;&#65292;&#32780;&#19981;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#21487;&#20256;&#36882;&#30340;&#65292;&#23398;&#21040;&#30340;&#30701;&#35821;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#22823;&#30340;&#23553;&#38381;&#28304;&#27169;&#22411;&#65292;&#22914;GPT3.5
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14016v1 Announce Type: new  Abstract: Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems. Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs. This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores. Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality. Interestingly, such attacks are transferable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5
&lt;/p&gt;</description></item><item><title>LoRA+&#36890;&#36807;&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#25913;&#36827;&#21407;&#22987;LoRA&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#24494;&#35843;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12354</link><description>&lt;p&gt;
LoRA+: &#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#39640;&#25928;&#20302;&#31209;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoRA+: Efficient Low Rank Adaptation of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12354
&lt;/p&gt;
&lt;p&gt;
LoRA+&#36890;&#36807;&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#25913;&#36827;&#21407;&#22987;LoRA&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#24494;&#35843;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26368;&#21021;&#30001;&#32993;&#31561;&#20154;&#65288;2021&#24180;&#65289;&#24341;&#20837;&#65292;&#23548;&#33268;&#23545;&#20855;&#26377;&#22823;&#23485;&#24230;&#65288;&#23884;&#20837;&#32500;&#24230;&#65289;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#34920;&#29616;&#20122;&#20248;&#12290;&#36825;&#26159;&#22240;&#20026;LoRA&#20013;&#30340;&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#29575;&#36827;&#34892;&#26356;&#26032;&#12290;&#36890;&#36807;&#23545;&#22823;&#23485;&#24230;&#32593;&#32476;&#36827;&#34892;&#32553;&#25918;&#21442;&#25968;&#30340;&#35770;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#29575;&#19981;&#21033;&#20110;&#26377;&#25928;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;LoRA&#30340;&#36825;&#31181;&#27425;&#20248;&#24615;&#21487;&#20197;&#31616;&#21333;&#22320;&#36890;&#36807;&#20026;LoRA&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#20197;&#21450;&#19968;&#20010;&#31934;&#24515;&#36873;&#25321;&#30340;&#27604;&#29575;&#26469;&#36827;&#34892;&#26657;&#27491;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25552;&#20986;&#30340;&#31639;&#27861;&#31216;&#20026;LoRA$+$&#12290;&#22312;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;LoRA$+$&#22312;&#30456;&#21516;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#39640;&#20102;&#24615;&#33021;&#65288;1-2&#65285;&#30340;&#25913;&#36827;&#65289;&#21644;&#24494;&#35843;&#36895;&#24230;&#65288;&#26368;&#22810;&#25552;&#36895;&#32422;2&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12354v1 Announce Type: cross  Abstract: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#65292;&#38381;&#28304;&#27169;&#22411;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#19982;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.11997</link><description>&lt;p&gt;
&#22238;&#24518;&#37027;&#19968;&#24180;&#21457;&#29983;&#30340;&#20107;&#20214;&#65311;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11997
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#65292;&#38381;&#28304;&#27169;&#22411;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#19982;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#25512;&#29702;&#21644;&#20445;&#30041;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#20107;&#20214;&#30340;&#39034;&#24207;&#24615;&#23545;&#20851;&#38190;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#25968;&#25454;&#38598;\textbf{TempUN}&#19978;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26102;&#38388;&#20445;&#30041;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#33879;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38381;&#28304;&#27169;&#22411;&#26356;&#39057;&#32321;&#22320;&#26174;&#31034;&#20986;&#30693;&#35782;&#24046;&#36317;&#65292;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#21644;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;&#24182;&#27809;&#26377;&#24102;&#26469;&#20027;&#35201;&#24615;&#33021;&#25913;&#36827;&#12290;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#33719;&#24471;&#65288;https://github.com/lingoiitgn/TempUN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11997v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various fine-tuning approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32534;&#36753;HotpotQA&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;LLM MHQA&#35780;&#20272;&#22522;&#20934;&#65292;&#21516;&#26102;&#27880;&#37322;&#21644;&#35780;&#20272;&#20102;&#25512;&#29702;&#38142;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;MHQA&#22522;&#20934;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.11924</link><description>&lt;p&gt;
MRKE&#65306;&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23545;LLMs&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11924
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32534;&#36753;HotpotQA&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;LLM MHQA&#35780;&#20272;&#22522;&#20934;&#65292;&#21516;&#26102;&#27880;&#37322;&#21644;&#35780;&#20272;&#20102;&#25512;&#29702;&#38142;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;MHQA&#22522;&#20934;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65288;MHQA&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30495;&#27491;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24453;&#25506;&#35752;&#12290;&#30446;&#21069;&#30340;LLM QA&#35780;&#20272;&#22522;&#20934;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;1&#65289;&#25968;&#25454;&#27745;&#26579;&#65292;&#35780;&#20272;&#25968;&#25454;&#21487;&#33021;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#32473;LLMs&#65307;&#20197;&#21450;2&#65289;&#24573;&#35270;&#25512;&#29702;&#38142;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;LLM MHQA&#35780;&#20272;&#22522;&#20934;&#65292;&#36825;&#26159;&#22522;&#20110;&#32534;&#36753;&#29616;&#25104;HotpotQA&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#12289;&#21069;&#25152;&#26410;&#26377;&#30340;&#30693;&#35782;&#30340;&#31532;&#19968;&#20010;QA&#22522;&#20934;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27880;&#37322;&#21644;&#35780;&#20272;&#20102;&#25512;&#29702;&#38142;&#65292;&#20197;&#23376;&#38382;&#39064;&#21644;&#20013;&#38388;&#31572;&#26696;&#30340;&#24418;&#24335;&#23545;&#24212;&#20110;&#22810;&#36339;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#65292;1&#65289;LLMs&#22312;&#21407;&#22987;HotpotQA&#21644;&#25105;&#20204;&#32534;&#36753;&#30340;&#25968;&#25454;&#20043;&#38388;&#26174;&#31034;&#24615;&#33021;&#24046;&#36317;&#65292;&#35748;&#20026;&#24403;&#21069;&#30340;MHQA&#22522;&#20934;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#38590;&#20197;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11924v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) have shown strong performance in Multi-hop Question Answering (MHQA) tasks, their real reasoning ability remains exploration. Current LLM QA evaluation benchmarks have shown limitations, including 1) data contamination, the evaluation data are potentially exposed to LLMs during the pretraining stage; and 2) ignoration of the reasoning chain evaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QA benchmark based on the new, unprecedented knowledge by editing the off-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the reasoning chain in the form of sub-questions and intermediate answers corresponding to the multi-hop questions. Specifically, based on the observation, 1) LLMs show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA benchmarks have the potential risk of data contamination that hard to evaluate LLMs' perfor
&lt;/p&gt;</description></item><item><title>DELL&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;LLMs&#25972;&#21512;&#21040;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#31649;&#36947;&#20013;&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#38395;&#21453;&#24212;&#21644;&#35299;&#37322;&#26469;&#25552;&#21319;&#23545;&#26032;&#38395;&#25991;&#31456;&#30495;&#23454;&#24615;&#30340;&#21028;&#26029;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10426</link><description>&lt;p&gt;
DELL: &#22522;&#20110;LLM&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#29983;&#25104;&#21453;&#24212;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10426
&lt;/p&gt;
&lt;p&gt;
DELL&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;LLMs&#25972;&#21512;&#21040;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#31649;&#36947;&#20013;&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#38395;&#21453;&#24212;&#21644;&#35299;&#37322;&#26469;&#25552;&#21319;&#23545;&#26032;&#38395;&#25991;&#31456;&#30495;&#23454;&#24615;&#30340;&#21028;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21463;&#20107;&#23454;&#24615;&#21644;&#24187;&#35273;&#26041;&#38754;&#30340;&#25361;&#25112;&#25152;&#38480;&#65292;&#22240;&#27492;&#26080;&#27861;&#30452;&#25509;&#29992;&#20110;&#26032;&#38395;&#25991;&#31456;&#30495;&#23454;&#24615;&#30340;&#21028;&#26029;&#65292;&#32780;&#20107;&#23454;&#20934;&#30830;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DELL&#65292;&#23427;&#30830;&#23450;&#20102;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20013;LLM&#21487;&#20197;&#20316;&#20026;&#31649;&#36947;&#30340;&#19968;&#37096;&#20998;&#30340;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#65306;1&#65289;LLM&#21487;&#20197;&#29983;&#25104;&#26032;&#38395;&#21453;&#24212;&#26469;&#20195;&#34920;&#19981;&#21516;&#35270;&#35282;&#24182;&#27169;&#25311;&#29992;&#25143;-&#26032;&#38395;&#20132;&#20114;&#32593;&#32476;&#65307;2&#65289;LLM&#21487;&#20197;&#20026;&#20195;&#29702;&#20219;&#21153;&#65288;&#22914;&#24773;&#24863;&#12289;&#31435;&#22330;&#65289;&#29983;&#25104;&#35299;&#37322;&#65292;&#20197;&#20016;&#23500;&#26032;&#38395;&#25991;&#31456;&#30340;&#32972;&#26223;&#24182;&#20135;&#29983;&#19987;&#38376;&#30740;&#31350;&#26032;&#38395;&#19981;&#21516;&#26041;&#38754;&#30340;&#19987;&#23478;&#65307;3&#65289;LLM&#21487;&#20197;&#21512;&#24182;&#20219;&#21153;&#29305;&#23450;&#30340;&#19987;&#23478;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#19987;&#23478;&#30340;&#39044;&#27979;&#21644;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#25552;&#20379;&#25972;&#20307;&#39044;&#27979;&#12290;&#23545;&#19971;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;DELL&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10426v1 Announce Type: new  Abstract: Large language models are limited by challenges in factuality and hallucinations to be directly employed off-the-shelf for judging the veracity of news articles, where factual accuracy is paramount. In this work, we propose DELL that identifies three key stages in misinformation detection where LLMs could be incorporated as part of the pipeline: 1) LLMs could \emph{generate news reactions} to represent diverse perspectives and simulate user-news interaction networks; 2) LLMs could \emph{generate explanations} for proxy tasks (e.g., sentiment, stance) to enrich the contexts of news articles and produce experts specializing in various aspects of news understanding; 3) LLMs could \emph{merge task-specific experts} and provide an overall prediction by incorporating the predictions and confidence scores of varying experts. Extensive experiments on seven datasets with three LLMs demonstrate that DELL outperforms state-of-the-art baselines by u
&lt;/p&gt;</description></item><item><title>RareBench&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#32597;&#35265;&#30149;&#39046;&#22495;&#30340;&#35786;&#26029;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#22823;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.06341</link><description>&lt;p&gt;
RareBench&#65306;LLMs&#33021;&#21542;&#25285;&#20219;&#32597;&#35265;&#30149;&#19987;&#23478;&#65311;
&lt;/p&gt;
&lt;p&gt;
RareBench: Can LLMs Serve as Rare Diseases Specialists?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06341
&lt;/p&gt;
&lt;p&gt;
RareBench&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#32597;&#35265;&#30149;&#39046;&#22495;&#30340;&#35786;&#26029;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#22823;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#65292;&#22312;&#21253;&#25324;&#21307;&#23398;&#35786;&#26029;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#12290;&#32597;&#35265;&#30149;&#65292;&#24433;&#21709;&#20840;&#29699;&#32422;3&#20159;&#20154;&#65292;&#24448;&#24448;&#30001;&#20110;&#32570;&#20047;&#32463;&#39564;&#20016;&#23500;&#30340;&#21307;&#29983;&#21644;&#38590;&#20197;&#21306;&#20998;&#20247;&#22810;&#32597;&#35265;&#30149;&#30340;&#22797;&#26434;&#24615;&#32780;&#23548;&#33268;&#20020;&#24202;&#35786;&#26029;&#29575;&#19981;&#23613;&#20154;&#24847;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#36817;&#30340;&#26032;&#38395;&#22914;"ChatGPT&#22312;17&#21517;&#21307;&#29983;&#22833;&#36133;&#21518;&#27491;&#30830;&#35786;&#26029;&#20986;&#20102;&#19968;&#20301;4&#23681;&#23401;&#23376;&#30340;&#32597;&#35265;&#30149;"&#24378;&#35843;&#20102;LLMs&#22312;&#20020;&#24202;&#35786;&#26029;&#32597;&#35265;&#30149;&#20013;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#36825;&#20010;&#35282;&#33394;&#22312;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;RareBench&#65292;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;LLMs&#22312;&#32597;&#35265;&#30149;&#39046;&#22495;&#20869;&#30340;4&#20010;&#20851;&#38190;&#32500;&#24230;&#19978;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#26368;&#22823;&#30340;&#32597;&#35265;&#30149;&#24739;&#32773;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;&#20026;&#20102;&#20419;&#36827;&#32597;&#35265;&#30149;&#30340;&#24046;&#24322;&#35786;&#26029;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21160;&#24577;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis. Rare diseases, affecting approximately 300 million people worldwide, often have unsatisfactory clinical diagnosis rates primarily due to a lack of experienced physicians and the complexity of differentiating among many rare diseases. In this context, recent news such as "ChatGPT correctly diagnosed a 4-year-old's rare disease after 17 doctors failed" underscore LLMs' potential, yet underexplored, role in clinically diagnosing rare diseases. To bridge this research gap, we introduce RareBench, a pioneering benchmark designed to systematically evaluate the capabilities of LLMs on 4 critical dimensions within the realm of rare diseases. Meanwhile, we have compiled the largest open-source dataset on rare disease patients, establishing a benchmark for future studies in this domain. To facilitate differential diagnosis of rare diseases, we develop a dynamic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;</title><link>https://arxiv.org/abs/2402.04854</link><description>&lt;p&gt;
&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#23398;&#26415;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32570;&#20047;&#30740;&#31350;&#22521;&#35757;&#30340;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#30740;&#31350;&#35843;&#26597;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#30740;&#31350;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#24456;&#38590;&#29702;&#35299;&#20182;&#20204;&#30740;&#31350;&#20027;&#39064;&#20869;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#21457;&#29616;&#26032;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#20026;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#25552;&#20379;&#30452;&#35266;&#30340;&#24110;&#21161;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#25552;&#20379;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#24182;&#25512;&#33616;&#30456;&#20851;&#30340;&#23398;&#26415;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#20027;&#35201;&#20381;&#36182;&#20110;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#23383;&#65292;&#24120;&#24120;&#26080;&#27861;&#28165;&#26970;&#22320;&#21576;&#29616;&#22810;&#20010;&#30456;&#20851;&#35770;&#25991;&#20043;&#38388;&#30340;&#36923;&#36753;&#23618;&#27425;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20165;&#20165;&#20381;&#36182;&#20110;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#35753;&#30740;&#31350;&#20154;&#21592;&#22256;&#24785;&#20026;&#20160;&#20040;&#25512;&#33616;&#20102;&#29305;&#23450;&#30340;&#25991;&#31456;&#12290;&#20182;&#20204;&#21487;&#33021;&#32570;&#20047;&#20102;&#35299;&#20851;&#20110;&#20182;&#20204;&#24076;&#26395;&#33719;&#24471;&#30340;"&#38382;&#39064;&#35299;&#20915;"&#21644;"&#38382;&#39064;&#21457;&#29616;"&#20043;&#38388;&#30340;&#35265;&#35299;&#36830;&#25509;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25903;&#25345;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EffiBench&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4-turbo&#29983;&#25104;&#30340;&#20195;&#30721;&#26368;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.02037</link><description>&lt;p&gt;
EffiBench:&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#30340;&#25928;&#29575;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EffiBench: Benchmarking the Efficiency of Automatically Generated Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EffiBench&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4-turbo&#29983;&#25104;&#30340;&#20195;&#30721;&#26368;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#22312;&#36741;&#21161;&#36719;&#20214;&#24320;&#21457;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#23436;&#25104;&#20195;&#30721;&#34917;&#20840;&#12289;&#35843;&#35797;&#21644;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#28145;&#20837;&#30740;&#31350;&#20102;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#27491;&#30830;&#24615;&#65292;&#20294;&#29983;&#25104;&#20195;&#30721;&#30340;&#25928;&#29575;&#36825;&#19968;&#37325;&#35201;&#26041;&#38754;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EffiBench&#65292;&#19968;&#20010;&#21253;&#21547;1,000&#20010;&#25928;&#29575;&#20851;&#38190;&#30340;&#32534;&#30721;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25928;&#29575;&#12290;EffiBench&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;LeetCode&#32534;&#30721;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#19968;&#20010;&#21487;&#25191;&#34892;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#20856;&#22411;&#35299;&#20915;&#26041;&#26696;&#37197;&#23545;&#12290;&#36890;&#36807;EffiBench&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#32771;&#23519;&#20102;21&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20854;&#20013;13&#31181;&#26159;&#24320;&#28304;&#30340;&#65292;8&#31181;&#26159;&#38381;&#28304;&#30340;&#65289;&#22312;&#29983;&#25104;&#39640;&#25928;&#20195;&#30721;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4-turbo&#29983;&#25104;&#30340;&#20195;&#30721;&#26368;&#39640;&#25928;&#65292;&#26126;&#26174;&#20248;&#20110;Palm-2-chat-bison&#12289;Claude-instant-1&#12289;Gemini-pro&#12289;GPT-4&#21644;GPT-3.5&#12290;
&lt;/p&gt;
&lt;p&gt;
Code generation models have increasingly become integral to aiding software development, offering assistance in tasks such as code completion, debugging, and code translation. Although current research has thoroughly examined the correctness of code produced by code generation models, a vital aspect, i.e., the efficiency of the generated code, has often been neglected. This paper presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems for assessing the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution. With EffiBench, we empirically examine the capability of 21 Large Language Models (13 open-sourced and 8 closed-sourced) in generating efficient code. The results demonstrate that GPT-4-turbo generates the most efficient code, significantly outperforming Palm-2-chat-bison, Claude-instant-1, Gemini-pro, GPT-4, and GPT-3.5. Ne
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.01695</link><description>&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language-Guided World Models: A Model-Based Approach to AI Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01695
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#23433;&#35013;&#21040;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20013;&#65292;&#20026;&#20154;&#31867;&#19982;&#36825;&#20123;&#20195;&#29702;&#27807;&#36890;&#21644;&#25511;&#21046;&#25171;&#24320;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#28192;&#36947;&#12290;&#38500;&#20102;&#26356;&#26032;&#20195;&#29702;&#31574;&#30053;&#65292;&#20154;&#31867;&#36824;&#21487;&#20197;&#20462;&#25913;&#20182;&#20204;&#30340;&#20869;&#37096;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#24433;&#21709;&#20195;&#29702;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#38590;&#20197;&#36866;&#24212;&#20154;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#33258;&#28982;&#30340;&#36890;&#20449;&#30028;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;&#23427;&#20204;&#36824;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#26368;&#21021;&#29992;&#20110;&#25351;&#23548;&#20154;&#31867;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#33258;&#25105;&#23398;&#20064;&#12290;&#20026;&#20102;&#20419;&#36827;LWMs&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;MESSENGER&#28216;&#25103;&#65288;Hanjie&#31561;&#20154;&#65292;2021&#65289;&#30340;&#25361;&#25112;&#22522;&#20934;&#65292;&#38656;&#35201;&#23545;&#26032;&#22330;&#26223;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#30340;&#26426;&#36935;&#21644;&#39118;&#38505;&#12290;&#36890;&#36807;&#25552;&#20986;&#28151;&#21512;&#24322;&#36136;&#19987;&#23478;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;LLM&#26426;&#22120;&#20154;&#26816;&#27979;&#22120;&#65292;&#24182;&#21457;&#29616;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#31034;&#20363;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#21363;&#21487;&#21462;&#24471;&#36229;&#36807;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;LLM&#24341;&#23548;&#30340;&#25805;&#32437;&#31574;&#30053;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#29616;&#26377;&#26426;&#22120;&#20154;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00371</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#26816;&#27979;&#20013;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#20250;&#19982;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#30340;&#26426;&#36935;&#21644;&#39118;&#38505;&#12290;&#36890;&#36807;&#25552;&#20986;&#28151;&#21512;&#24322;&#36136;&#19987;&#23478;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;LLM&#26426;&#22120;&#20154;&#26816;&#27979;&#22120;&#65292;&#24182;&#21457;&#29616;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#31034;&#20363;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#21363;&#21487;&#21462;&#24471;&#36229;&#36807;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;LLM&#24341;&#23548;&#30340;&#25805;&#32437;&#31574;&#30053;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#29616;&#26377;&#26426;&#22120;&#20154;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#26816;&#27979;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#26426;&#22120;&#20154;&#26816;&#27979;&#22120;&#21644;&#23545;&#25239;&#26426;&#22120;&#20154;&#31574;&#30053;&#20043;&#38388;&#30340;&#19968;&#22330;&#20891;&#22791;&#31454;&#36187;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#26368;&#26032;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;&#65292;&#23558;&#36825;&#22330;&#20891;&#22791;&#31454;&#36187;&#25552;&#21319;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#27700;&#24179;&#12290;&#20026;&#20102;&#25506;&#32034;&#26426;&#20250;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;LLM&#30340;&#26032;&#39062;&#26426;&#22120;&#20154;&#26816;&#27979;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#24322;&#36136;&#19987;&#23478;&#26694;&#26550;&#65292;&#23545;&#19981;&#21516;&#30340;&#29992;&#25143;&#20449;&#24687;&#27169;&#24577;&#36827;&#34892;&#21010;&#20998;&#21644;&#24449;&#26381;&#12290;&#20026;&#20102;&#25581;&#31034;&#39118;&#38505;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;LLM&#24341;&#23548;&#29992;&#25143;&#25991;&#26412;&#21644;&#32467;&#26500;&#21270;&#20449;&#24687;&#25805;&#32437;&#26469;&#36867;&#36991;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#23545;1,000&#20010;&#27880;&#37322;&#31034;&#20363;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#23601;&#21487;&#20197;&#20135;&#29983;&#19987;&#19994;&#30340;LLM&#65292;&#20854;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#39640;&#36798;9.1%&#65292;&#32780;LLM&#24341;&#23548;&#30340;&#25805;&#32437;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#29616;&#26377;&#26426;&#22120;&#20154;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;GPT-4&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#23384;&#22312;&#22522;&#26412;&#38169;&#35823;&#65292;&#24182;&#19988;&#21363;&#20351;&#26159;GPT-4&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2401.17169</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Conditional and Modal Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;GPT-4&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#23384;&#22312;&#22522;&#26412;&#38169;&#35823;&#65292;&#24182;&#19988;&#21363;&#20351;&#26159;GPT-4&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#30740;&#31350;&#27491;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#35748;&#30693;&#31185;&#23398;&#39046;&#22495;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21313;&#20960;&#20010;LLM&#33021;&#21542;&#21306;&#20998;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#25512;&#35770;&#21644;&#36923;&#36753;&#19978;&#33618;&#35884;&#30340;&#25512;&#35770;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#28041;&#21450;&#26465;&#20214;&#21477;&#65288;&#20363;&#22914;&#65292;&#8220;&#22914;&#26524;&#23433;&#26377;&#19968;&#20010;&#30343;&#21518;&#65292;&#37027;&#20040;&#40077;&#21187;&#26377;&#19968;&#20010;J&#29260;&#8221;&#65289;&#21644;&#35748;&#35782;&#24773;&#24577;&#65288;&#20363;&#22914;&#65292;&#8220;&#23433;&#21487;&#33021;&#26377;&#19968;&#20010;A&#29260;&#8221;&#65292;&#8220;&#40077;&#21187;&#24517;&#39035;&#26377;&#19968;&#20010;K&#29260;&#8221;&#65289;&#30340;&#25512;&#29702;&#27169;&#24335;&#12290;&#36825;&#20123;&#25512;&#29702;&#27169;&#24335;&#23545;&#20110;&#36923;&#36753;&#23398;&#23478;&#12289;&#21746;&#23398;&#23478;&#21644;&#35821;&#35328;&#23398;&#23478;&#26469;&#35828;&#20855;&#26377;&#29305;&#27530;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#22312;&#20154;&#31867;&#25512;&#29702;&#20013;&#25198;&#28436;&#19968;&#20010;&#26680;&#24515;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;LLM&#22312;&#36825;&#20123;&#25512;&#29702;&#27169;&#24335;&#19978;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#30456;&#21305;&#37197;&#26159;&#38750;&#24120;&#30456;&#20851;&#30340;&#12290;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;LLM&#20013;&#65292;&#38500;&#20102;GPT-4&#65292;&#20854;&#20182;&#37117;&#24120;&#24120;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#29359;&#22522;&#26412;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;GPT-4&#65292;&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science. In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning. Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.
&lt;/p&gt;</description></item><item><title>PythonSaga&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;Python&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;,&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#23384;&#22312;&#30340;&#32534;&#31243;&#27010;&#24565;&#20559;&#35265;&#21644;&#31616;&#21333;&#20219;&#21153;&#26222;&#36941;&#24615;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2401.03855</link><description>&lt;p&gt;
PythonSaga&#65306;&#37325;&#26032;&#23450;&#20041;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;LLM&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03855
&lt;/p&gt;
&lt;p&gt;
PythonSaga&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;Python&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;,&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#23384;&#22312;&#30340;&#32534;&#31243;&#27010;&#24565;&#20559;&#35265;&#21644;&#31616;&#21333;&#20219;&#21153;&#26222;&#36941;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#20195;&#30721;&#28608;&#22686;&#30340;&#25512;&#21160;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;LLMs&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#23545;HumanEval&#21644;MBPP&#20004;&#20010;&#27969;&#34892;&#30340;Python&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#22810;&#26679;&#24615;&#21644;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#32534;&#31243;&#27010;&#24565;&#23384;&#22312;&#20005;&#37325;&#20559;&#35265;&#65292;&#23436;&#20840;&#24573;&#35270;&#20102;&#22823;&#22810;&#25968;&#20854;&#20182;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22823;&#37327;&#31616;&#21333;&#20219;&#21153;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#21487;&#33021;&#22840;&#22823;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;PythonSaga&#65292;&#21253;&#21547;&#20102;185&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#65292;&#28085;&#30422;&#20102;38&#20010;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#32534;&#31243;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03855v2 Announce Type: replace-cross  Abstract: Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of HumanEval and MBPP, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks, potentially inflating model performance estimations. To address these limitations, we propose a novel benchmark, PythonSaga, featuring 185 hand-crafted prompts on a balanced representation of 38 programming concepts across diverse difficulty levels.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Contrastive Activation Addition&#65288;CAA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#65292;&#26174;&#33879;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#24182;&#22312;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#39069;&#22806;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.06681</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#28608;&#27963;&#21152;&#27861;&#25351;&#23548;Llama 2
&lt;/p&gt;
&lt;p&gt;
Steering Llama 2 via Contrastive Activation Addition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06681
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Contrastive Activation Addition&#65288;CAA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#65292;&#26174;&#33879;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#24182;&#22312;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#39069;&#22806;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;Contrastive Activation Addition&#65288;CAA&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#20462;&#25913;&#20854;&#28608;&#27963;&#26469;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#12290;CAA&#36890;&#36807;&#23545;&#26576;&#31181;&#34892;&#20026;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#31034;&#20363;&#20043;&#38388;&#27531;&#24046;&#27969;&#28608;&#27963;&#30340;&#24046;&#24322;&#27714;&#24179;&#22343;&#65292;&#35745;&#31639;&#20986;&#8220;&#25351;&#23548;&#21521;&#37327;&#8221;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22312;&#29992;&#25143;&#25552;&#31034;&#21518;&#30340;&#25152;&#26377;token&#20301;&#32622;&#19978;&#20197;&#27491;&#36127;&#31995;&#25968;&#28155;&#21152;&#36825;&#20123;&#25351;&#23548;&#21521;&#37327;&#65292;&#20174;&#32780;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#36873;&#25321;&#34892;&#20026;&#38382;&#39064;&#25968;&#25454;&#38598;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#22312;Llama 2 Chat&#19978;&#35780;&#20272;&#20102;CAA&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;CAA&#26174;&#30528;&#25913;&#21464;&#20102;&#27169;&#22411;&#34892;&#20026;&#65292;&#19981;&#20165;&#22312;&#20256;&#32479;&#26041;&#27861;&#22914;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#26377;&#25928;&#65292;&#32780;&#19988;&#26368;&#23567;&#31243;&#24230;&#22320;&#38477;&#20302;&#20102;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#34892;&#20026;&#20570;&#20986;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06681v3 Announce Type: replace-cross  Abstract: We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25552;&#31034;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;GPT4 APO&#22312;&#26631;&#20934;&#21270;&#25552;&#21319;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#19987;&#23478;&#23450;&#21046;&#21270;&#23545;&#20110;&#20869;&#23481;&#36136;&#37327;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.09684</link><description>&lt;p&gt;
&#21307;&#29983;&#20204;&#30693;&#36947;&#22914;&#20309;&#25552;&#37266;&#21527;&#65311;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#24110;&#21161;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25552;&#31034;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;GPT4 APO&#22312;&#26631;&#20934;&#21270;&#25552;&#21319;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#19987;&#23478;&#23450;&#21046;&#21270;&#23545;&#20110;&#20869;&#23481;&#36136;&#37327;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#25552;&#31034;&#24037;&#31243;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#26469;&#25913;&#36827;&#21021;&#22987;&#25552;&#31034;&#65292;&#24182;&#27604;&#36739;&#20102;&#21307;&#23398;&#19987;&#23478;&#12289;&#38750;&#21307;&#23398;&#19987;&#23478;&#20197;&#21450;&#32463;&#36807;APO&#22686;&#24378;&#30340;GPT3.5&#21644;GPT4&#30340;&#36755;&#20986;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;GPT4 APO&#22312;&#26631;&#20934;&#21270;&#20020;&#24202;&#31508;&#35760;&#21508;&#33410;&#25552;&#31034;&#36136;&#37327;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#20154;&#22312;&#29615;&#20013;&#26041;&#27861;&#26174;&#31034;&#65292;&#19987;&#23478;&#22312;APO&#21518;&#20445;&#25345;&#20869;&#23481;&#36136;&#37327;&#65292;&#20294;&#26356;&#20559;&#22909;&#33258;&#24049;&#30340;&#20462;&#25913;&#65292;&#34920;&#26126;&#20102;&#19987;&#23478;&#23450;&#21046;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#20004;&#38454;&#27573;&#20248;&#21270;&#36807;&#31243;&#65292;&#21033;&#29992;APO-GPT4&#30830;&#20445;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;&#19987;&#23478;&#36755;&#20837;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09684v2 Announce Type: replace-cross  Abstract: This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the outputs of medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in standardizing prompt quality across clinical note sections. A human-in-the-loop approach shows that experts maintain content quality post-APO, with a preference for their own modifications, suggesting the value of expert customization. We recommend a two-phase optimization process, leveraging APO-GPT4 for consistency and expert input for personalization.
&lt;/p&gt;</description></item><item><title>AI&#24605;&#24819;&#23545;&#20010;&#20307;&#21019;&#36896;&#21147;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13481</link><description>&lt;p&gt;
AI&#24605;&#24819;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24605;&#24819;&#30340;&#21019;&#36896;&#21147;&#12289;&#22810;&#26679;&#24615;&#21644;&#36827;&#21270;&#65306;&#26469;&#33258;&#19968;&#20010;&#22823;&#35268;&#27169;&#21160;&#24577;&#23454;&#39564;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment. (arXiv:2401.13481v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13481
&lt;/p&gt;
&lt;p&gt;
AI&#24605;&#24819;&#23545;&#20010;&#20307;&#21019;&#36896;&#21147;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#25509;&#35302;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#12290;&#35266;&#30475;&#21040;AI&#29983;&#25104;&#30340;&#24605;&#24819;&#23558;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24605;&#24819;&#65311;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23454;&#39564;&#65288;800+&#21442;&#19982;&#32773;&#65292;40+&#20010;&#22269;&#23478;&#65289;&#65292;&#21442;&#19982;&#32773;&#35266;&#30475;&#20102;&#26469;&#33258;ChatGPT&#25110;&#20043;&#21069;&#23454;&#39564;&#21442;&#19982;&#32773;&#30340;&#21019;&#24847;&#24605;&#24819;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#33258;&#24049;&#30340;&#21019;&#24847;&#24605;&#32771;&#12290;&#25105;&#20204;&#21464;&#21270;&#20102;AI&#29983;&#25104;&#31034;&#20363;&#30340;&#25968;&#37327;&#65288;&#26080;&#12289;&#20302;&#12289;&#39640;&#26333;&#20809;&#65289;&#20197;&#21450;&#31034;&#20363;&#26159;&#21542;&#26631;&#35760;&#20026;&#8220;AI&#8221;&#65288;&#25259;&#38706;&#65289;&#12290;&#25105;&#20204;&#30340;&#21160;&#24577;&#23454;&#39564;&#35774;&#35745; - &#22312;&#21516;&#19968;&#23454;&#39564;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;&#20043;&#21069;&#21442;&#19982;&#32773;&#30340;&#24605;&#24819;&#20316;&#20026;&#26410;&#26469;&#21442;&#19982;&#32773;&#30340;&#21050;&#28608; - &#27169;&#25311;&#20102;&#25991;&#21270;&#21019;&#36896;&#30340;&#30456;&#20114;&#20381;&#36182;&#36807;&#31243;&#65306;&#21019;&#36896;&#24615;&#24605;&#24819;&#24314;&#31435;&#22312;&#20043;&#21069;&#30340;&#24605;&#24819;&#22522;&#30784;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25429;&#25417;&#21040;&#20102;LLM&#8220;&#22312;&#25991;&#21270;&#24490;&#29615;&#20013;&#8221;&#30340;&#22797;&#21512;&#25928;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;AI&#26333;&#20809;&#65288;&#20294;&#19981;&#26159;&#20302;AI&#26333;&#20809;&#65289;&#24182;&#27809;&#26377;&#24433;&#21709;&#20010;&#20154;&#24605;&#24819;&#30340;&#21019;&#36896;&#21147;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#24179;&#22343;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;AI&#20351;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#32047;&#31215;&#25928;&#24212;&#22686;&#24378;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- mimics the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04444</link><description>&lt;p&gt;
&#39764;&#27861;&#35789;&#26159;&#20160;&#20040;&#65311;LLM&#25552;&#31034;&#30340;&#25511;&#21046;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;LLM&#30340;&#37096;&#32626;&#20013;&#26159;&#26377;&#25928;&#21644;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#25968;&#23398;&#19978;&#29702;&#35299;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35748;&#20026;&#26159;&#35843;&#33410;LLM&#36755;&#20986;&#20998;&#24067;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;token&#24207;&#21015;&#65292;&#26159;&#21542;&#24635;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#21487;&#20197;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65311;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26368;&#20248;&#25552;&#31034;&#31216;&#20026;&#39764;&#27861;&#35789;&#65292;&#22240;&#20026;&#28155;&#21152;&#25552;&#31034;&#20250;&#23548;&#33268;LLM&#36755;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#23384;&#22312;&#39764;&#27861;&#35789;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#23427;&#20204;&#65311;&#22914;&#26524;&#21487;&#20197;&#65292;&#23427;&#20204;&#30340;&#29305;&#24615;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#25511;&#21046;&#29702;&#35770;&#24212;&#29992;&#20110;&#33258;&#27880;&#24847;&#21147;&#22836;&#30340;&#20998;&#26512;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20989;&#25968;&#20026;&#21487;&#25511;&#21046;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20511;&#37492;&#25511;&#21046;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;$k-\epsilon$&#21487;&#25511;&#21046;&#24615;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;Logical-GLM&#65292;&#29992;&#20110;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#27491;&#30830;&#36923;&#36753;&#30340;&#25991;&#26412;&#65292;&#24182;&#20197;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Logical-GLM&#22312;&#20351;&#29992;&#36739;&#23569;&#25968;&#25454;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.13782</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#36923;&#36753;&#22270;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#29983;&#25104;&#30340;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning with Logical Graph-based Language Model for Instruction Generation. (arXiv:2308.13782v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;Logical-GLM&#65292;&#29992;&#20110;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#27491;&#30830;&#36923;&#36753;&#30340;&#25991;&#26412;&#65292;&#24182;&#20197;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Logical-GLM&#22312;&#20351;&#29992;&#36739;&#23569;&#25968;&#25454;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#31070;&#32463;&#27169;&#22411;&#38590;&#20197;&#20174;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#20013;&#25429;&#25417;&#21040;&#38544;&#21547;&#30340;&#35268;&#21017;&#65292;&#22240;&#27492;&#24456;&#38590;&#29983;&#25104;&#20855;&#26377;&#27491;&#30830;&#36923;&#36753;&#30340;&#25991;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;Logical-GLM&#65292;&#23558;&#36923;&#36753;&#27880;&#20837;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#26356;&#26377;&#25928;&#30340;&#25991;&#26412;&#29983;&#25104;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#26500;&#24314;&#36890;&#24120;&#25551;&#36848;&#39046;&#22495;&#30340;&#36923;&#36753;&#36125;&#21494;&#26031;&#22270;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#29983;&#25104;&#36923;&#36753;&#39592;&#26550;&#20197;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20132;&#26367;&#20248;&#21270;&#22270;&#30340;&#25628;&#32034;&#31574;&#30053;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#30452;&#33267;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Logical-GLM&#19982;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#23613;&#31649;&#20351;&#29992;&#35268;&#27169;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#65292;&#20173;&#28982;&#20855;&#26377;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26377;&#25928;&#30340;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the superior performance of large language models to generate natural language texts, it is hard to generate texts with correct logic according to a given task, due to the difficulties for neural models to capture implied rules from free-form texts. In this paper, we propose a novel graph-based language model, Logical-GLM, to infuse logic into language models for more valid text generation and interpretability. Specifically, we first capture information from natural language instructions and construct logical bayes graphs that generally describe domains. Next, we generate logical skeletons to guide language model training, infusing domain knowledge into language models. Finally, we alternately optimize the searching policy of graphs and language models until convergence. The experimental results show that Logical-GLM is both effective and efficient compared with traditional language models, despite using smaller-scale training data and fewer parameters. Our approach can generat
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2308.13191</link><description>&lt;p&gt;
Chunk, Align, Select: &#19968;&#31181;&#31616;&#21333;&#30340;&#29992;&#20110;transformer&#30340;&#38271;&#24207;&#21015;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers. (arXiv:2308.13191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13191
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#30528;&#38271;&#24207;&#21015;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;transformer&#20013;&#33258;&#27880;&#24847;&#25805;&#20316;&#30340;&#35745;&#31639;&#25104;&#26412;&#38543;&#30528;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#38271;&#24207;&#21015;&#22788;&#29702;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;transformer&#33021;&#22815;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#38271;&#24207;&#21015;&#36755;&#20837;&#21010;&#20998;&#20026;&#19968;&#25209;chunk&#65292;&#28982;&#21518;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23545;chunk&#20043;&#38388;&#30340;&#20449;&#24687;&#36827;&#34892;&#23545;&#40784;&#65292;&#26368;&#21518;&#20174;&#32534;&#30721;&#22120;&#20013;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#35299;&#30721;&#12290;&#20026;&#20102;&#25552;&#21462;chunk&#20043;&#38388;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#32534;&#30721;transformer&#22359;&#20013;&#23545;chunk&#20043;&#38388;&#30340;&#36215;&#22987;&#21644;&#32467;&#26463;token&#36827;&#34892;&#23545;&#40784;&#12290;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#26377;&#25928;&#30340;&#38544;&#34255;&#29366;&#24577;&#36873;&#25321;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#37325;&#26356;&#26032;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although dominant in natural language processing, transformer-based models remain challenged by the task of long-sequence processing, because the computational cost of self-attention operations in transformers swells quadratically with the input sequence length. To alleviate the complexity of long-sequence processing, we propose a simple framework to enable the offthe-shelf pre-trained transformers to process much longer sequences, while the computation and memory costs remain growing linearly with the input sequence lengths. More specifically, our method divides each long-sequence input into a batch of chunks, then aligns the interchunk information during the encoding steps, and finally selects the most representative hidden states from the encoder for the decoding process. To extract inter-chunk semantic information, we align the start and end token embeddings among chunks in each encoding transformer block. To learn an effective hidden selection policy, we design a dual updating sch
&lt;/p&gt;</description></item><item><title>mPLM-Sim&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#20174;mPLMs&#20013;&#24341;&#23548;&#20986;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21487;&#29992;&#20110;&#36873;&#25321;&#28304;&#35821;&#35328;&#20197;&#22686;&#24378;&#36328;&#35821;&#35328;&#36801;&#31227;&#65292;&#20855;&#26377;&#20013;&#31561;&#31243;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#19981;&#21516;&#30340;mPLMs&#21644;&#23618;&#20135;&#29983;&#19981;&#21516;&#30340;&#30456;&#20284;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13684</link><description>&lt;p&gt;
mPLM-Sim: &#25581;&#31034;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26356;&#22909;&#30340;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#21644;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
mPLM-Sim: Unveiling Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models. (arXiv:2305.13684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13684
&lt;/p&gt;
&lt;p&gt;
mPLM-Sim&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#20174;mPLMs&#20013;&#24341;&#23548;&#20986;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21487;&#29992;&#20110;&#36873;&#25321;&#28304;&#35821;&#35328;&#20197;&#22686;&#24378;&#36328;&#35821;&#35328;&#36801;&#31227;&#65292;&#20855;&#26377;&#20013;&#31561;&#31243;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#19981;&#21516;&#30340;mPLMs&#21644;&#23618;&#20135;&#29983;&#19981;&#21516;&#30340;&#30456;&#20284;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;mPLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#29305;&#23450;&#35821;&#35328;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#24182;&#27809;&#26377;&#34987;&#26126;&#30830;&#25552;&#20379;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#23558;mPLMs&#29992;&#20110;&#27979;&#37327;&#35821;&#35328;&#30456;&#20284;&#24615;&#65292;&#24182;&#38543;&#21518;&#20351;&#29992;&#30456;&#20284;&#24615;&#32467;&#26524;&#36873;&#25321;&#28304;&#35821;&#35328;&#20197;&#22686;&#24378;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;mPLM-Sim&#65292;&#23427;&#21033;&#29992;&#22810;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#20174;mPLMs&#20013;&#24341;&#23548;&#20986;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;mPLM-Sim&#19982;&#35789;&#27719;&#32479;&#35745;&#12289;&#35821;&#31995;&#21644;&#22320;&#29702;&#21306;&#22495;&#31561;&#35821;&#35328;&#30456;&#20284;&#24230;&#27979;&#37327;&#20855;&#26377;&#20013;&#31561;&#31243;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#35821;&#35328;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#35266;&#23519;&#21040;mPLM-Sim&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#30456;&#20284;&#24615;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#20284;&#24615;&#32467;&#26524;&#22240;&#19981;&#21516;&#30340;mPLMs&#21644;mPLM&#20013;&#30340;&#19981;&#21516;&#23618;&#32780;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35843;&#26597;&#20102;mPLMs&#23545;&#35821;&#35328;&#36801;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent multilingual pretrained language models (mPLMs) have been shown to encode strong language-specific signals, which are not explicitly provided during pretraining. It remains an open question whether it is feasible to employ mPLMs to measure language similarity, and subsequently use the similarity results to select source languages for boosting cross-lingual transfer. To investigate this, we propose mPLM-Sim, a new language similarity measure that induces the similarities across languages from mPLMs using multi-parallel corpora. Our study shows that mPLM-Sim exhibits moderately high correlations with linguistic similarity measures, such as lexicostatistics, genealogical language family, and geographical sprachbund. We also conduct a case study on languages with low correlation and observe that mPLM-Sim yields more accurate similarity results. Additionally, we find that similarity results vary across different mPLMs and different layers within an mPLM. We further investigate whethe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13338</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#22240;&#38598;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
Gene Set Summarization using Large Language Models. (arXiv:2305.13338v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#29983;&#29289;&#23398;&#23478;&#32463;&#24120;&#35299;&#37322;&#20174;&#39640;&#36890;&#37327;&#23454;&#39564;&#21644;&#35745;&#31639;&#20998;&#26512;&#20013;&#33719;&#24471;&#30340;&#22522;&#22240;&#21015;&#34920;&#12290;&#36825;&#36890;&#24120;&#26159;&#36890;&#36807;&#32479;&#35745;&#23500;&#38598;&#20998;&#26512;&#26469;&#23436;&#25104;&#30340;&#65292;&#35813;&#20998;&#26512;&#27979;&#37327;&#19982;&#22522;&#22240;&#25110;&#20854;&#23646;&#24615;&#30456;&#20851;&#30340;&#29983;&#29289;&#21151;&#33021;&#26415;&#35821;&#30340;&#36807;&#24230;&#25110;&#27424;&#34920;&#31034;&#31243;&#24230;&#65292;&#22522;&#20110;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#65288;&#20363;&#22914;Gene Ontology&#65288;GO&#65289;&#65289;&#20013;&#30340;&#32534;&#35793;&#26029;&#35328;&#12290;&#35299;&#37322;&#22522;&#22240;&#21015;&#34920;&#20063;&#21487;&#20197;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#25991;&#26412;&#27010;&#25324;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#65292;&#21487;&#33021;&#30452;&#25509;&#21033;&#29992;&#31185;&#23398;&#25991;&#26412;&#24182;&#36991;&#20813;&#20381;&#36182;KB&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;SPINDOCTOR&#65288;&#31283;&#23450;&#30340;&#25552;&#31034;&#25554;&#20540;&#30340;&#21463;&#25511;&#26415;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#27169;&#26495;&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;GPT&#27169;&#22411;&#25191;&#34892;&#22522;&#22240;&#38598;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#26631;&#20934;&#23500;&#38598;&#20998;&#26512;&#30340;&#34917;&#20805;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#22240;&#21151;&#33021;&#20449;&#24687;&#26469;&#28304;&#65306;&#65288;1&#65289;&#20174;&#37492;&#23450;&#30340;&#26412;&#20307;KB&#27880;&#37322;&#20013;&#33719;&#24471;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#65288;2&#65289;&#20174;&#25991;&#26412;&#25366;&#25496;&#20013;&#25512;&#26029;&#30340;&#26412;&#20307;&#26415;&#35821;&#65292;&#20197;&#21450;&#65288;3&#65289;&#30452;&#25509;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#24471;&#30340;&#26415;&#35821;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;1813&#20010;&#22522;&#22240;&#38598;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SPINDOCTOR&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;GPT&#27169;&#22411;&#26174;&#33879;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#30340;&#22522;&#22240;&#21151;&#33021;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular biologists frequently interpret gene lists derived from high-throughput experiments and computational analysis. This is typically done as a statistical enrichment analysis that measures the over- or under-representation of biological function terms associated with genes or their properties, based on curated assertions from a knowledge base (KB) such as the Gene Ontology (GO). Interpreting gene lists can also be framed as a textual summarization task, enabling the use of Large Language Models (LLMs), potentially utilizing scientific texts directly and avoiding reliance on a KB.  We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language Descriptions of Controlled Terms for Ontology Reporting), a method that uses GPT models to perform gene set function summarization as a complement to standard enrichment analysis. This method can use different sources of gene functional information: (1) structured text derived from curated ontological KB annotations, (2) ontol
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22797;&#26434;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#24182;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#12290;&#35768;&#22810;&#30495;&#23454;&#30340;&#25991;&#26412;&#20998;&#31867;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#19968;&#20010;&#22270;&#12290;&#26412;&#32508;&#36848;&#35206;&#30422;&#21040;2023&#24180;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35814;&#32454;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#22270;&#26500;&#24314;&#26426;&#21046;&#21644;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#28085;&#30422;&#20102;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#23454;&#39564;&#35774;&#35745;&#65292;&#24182;&#24635;&#32467;&#20102;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11534</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#20998;&#31867;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Text Classification: A Survey. (arXiv:2304.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22797;&#26434;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#24182;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#12290;&#35768;&#22810;&#30495;&#23454;&#30340;&#25991;&#26412;&#20998;&#31867;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#19968;&#20010;&#22270;&#12290;&#26412;&#32508;&#36848;&#35206;&#30422;&#21040;2023&#24180;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35814;&#32454;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#22270;&#26500;&#24314;&#26426;&#21046;&#21644;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#28085;&#30422;&#20102;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#23454;&#39564;&#35774;&#35745;&#65292;&#24182;&#24635;&#32467;&#20102;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#22522;&#26412;&#21644;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#35768;&#22810;&#26368;&#36817;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#37319;&#29992;&#20102;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#22797;&#26434;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#24182;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#12290;&#35768;&#22810;&#30495;&#23454;&#30340;&#25991;&#26412;&#20998;&#31867;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#19968;&#20010;&#22270;&#65292;&#20854;&#20013;&#25429;&#33719;&#20102;&#21333;&#35789;&#12289;&#25991;&#26723;&#21644;&#35821;&#26009;&#24211;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;&#26412;&#32508;&#36848;&#23558;&#35206;&#30422;&#21040;2023&#24180;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#26500;&#24314;&#26426;&#21046;&#21644;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#38500;&#20102;&#25216;&#26415;&#32508;&#36848;&#65292;&#25105;&#20204;&#36824;&#20851;&#27880;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#38382;&#39064;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#23454;&#39564;&#35774;&#35745;&#65292;&#24182;&#21576;&#29616;&#20102;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#24067;&#30340;&#24615;&#33021;&#24635;&#32467;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text Classification is the most essential and fundamental problem in Natural Language Processing. While numerous recent text classification models applied the sequential deep learning technique, graph neural network-based models can directly deal with complex structured text data and exploit global information. Many real text classification applications can be naturally cast into a graph, which captures words, documents, and corpus global features. In this survey, we bring the coverage of methods up to 2023, including corpus-level and document-level graph neural networks. We discuss each of these methods in detail, dealing with the graph construction mechanisms and the graph-based learning process. As well as the technological survey, we look at issues behind and future directions addressed in text classification using graph neural networks. We also cover datasets, evaluation metrics, and experiment design and present a summary of published performance on the publicly available benchma
&lt;/p&gt;</description></item><item><title>&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16166</link><description>&lt;p&gt;
&#27809;&#26377;&#27491;&#30830;&#24615;&#30340;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#37325;&#35201;&#65306;&#22312;NLP&#39046;&#22495;&#20013;&#27979;&#35797;&#20195;&#30721;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16166
&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20854;&#22312;&#30740;&#31350;&#23454;&#39564;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20195;&#30721;&#27491;&#30830;&#24615;&#24448;&#24448;&#20165;&#22522;&#20110;&#32467;&#26524;&#30340;&#24863;&#30693;&#36136;&#37327;&#32780;&#34987;&#20551;&#23450;&#12290;&#36825;&#24102;&#26469;&#20102;&#38169;&#35823;&#32467;&#26524;&#21644;&#28508;&#22312;&#35823;&#23548;&#24615;&#21457;&#29616;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#20851;&#27880;&#32467;&#26524;&#37325;&#29616;&#24212;&#35813;&#19982;&#24378;&#35843;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#25903;&#25345;&#25105;&#20204;&#21521;NLP&#31038;&#21306;&#21457;&#20986;&#30340;&#21495;&#21484;&#65292;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#24182;&#32416;&#27491;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;Conformer&#26550;&#26500;&#30340;&#24320;&#28304;&#23454;&#29616;&#20013;&#30340;&#19977;&#20010;Bug&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Bug&#30340;&#23384;&#22312;&#24182;&#19981;&#20250;&#22952;&#30861;&#33719;&#24471;&#33391;&#22909;&#30340;&#21644;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#65292;&#21453;&#32780;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#33021;&#25552;&#20379;&#38169;&#35823;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#39033;&#30740;&#31350;&#21628;&#21505;&#37319;&#29992;&#26088;&#22312;&#20419;&#36827;NLP&#30740;&#31350;&#20013;&#27491;&#30830;&#24615;&#30340;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
&lt;/p&gt;</description></item></channel></rss>