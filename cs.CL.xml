<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#30340;&#26041;&#24335;&#23545;&#24515;&#20869;&#30005;&#22270;&#36827;&#34892;&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#12290;&#30456;&#27604;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25151;&#39076;&#20998;&#31867;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01115</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#35299;&#35835;&#24515;&#20869;&#30005;&#22270;
&lt;/p&gt;
&lt;p&gt;
Interpretation of Intracardiac Electrograms Through Textual Representations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#30340;&#26041;&#24335;&#23545;&#24515;&#20869;&#30005;&#22270;&#36827;&#34892;&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#12290;&#30456;&#27604;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25151;&#39076;&#20998;&#31867;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#25151;&#39076;(AFib)&#30340;&#19981;&#35268;&#21017;&#30005;&#27963;&#21160;&#19968;&#30452;&#26159;&#24515;&#30005;&#22270;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#23545;&#20110;&#20005;&#37325;&#30340;&#25151;&#39076;&#30149;&#20363;&#65292;&#36827;&#34892;&#23548;&#31649;&#28040;&#34701;&#20197;&#33719;&#21462;&#24515;&#20869;&#30005;&#22270;(EGMs)&#12290;EGMs&#25552;&#20379;&#20102;&#24515;&#33039;&#30005;&#27963;&#21160;&#30340;&#22797;&#26434;&#32454;&#33410;&#21644;&#23616;&#37096;&#21270;&#20449;&#24687;&#65292;&#26159;&#21487;&#35299;&#37322;&#30340;&#24515;&#33039;&#30740;&#31350;&#30340;&#29702;&#24819;&#27169;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#36827;&#23637;&#20351;&#24471;&#19968;&#20123;&#30740;&#31350;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#37322;&#25151;&#39076;&#20013;&#30340;EGMs&#12290;&#27492;&#22806;&#65292;&#35821;&#35328;&#27169;&#22411;(LMs)&#22312;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LMs&#26469;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#23545;EGM&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23558;EGM&#24418;&#24335;&#21270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#19982;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#25151;&#39076;&#20998;&#31867;&#26041;&#38754;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the irregular electrical activity of atrial fibrillation (AFib) has been a key challenge in electrocardiography. For serious cases of AFib, catheter ablations are performed to collect intracardiac electrograms (EGMs). EGMs offer intricately detailed and localized electrical activity of the heart and are an ideal modality for interpretable cardiac studies. Recent advancements in artificial intelligence (AI) has allowed some works to utilize deep learning frameworks to interpret EGMs during AFib. Additionally, language models (LMs) have shown exceptional performance in being able to generalize to unseen domains, especially in healthcare. In this study, we are the first to leverage pretrained LMs for finetuning of EGM interpolation and AFib classification via masked language modeling. We formulate the EGM as a textual sequence and present competitive performances on AFib classification compared against other representations. Lastly, we provide a comprehensive interpretabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.19181</link><description>&lt;p&gt;
&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#25490;&#21517;&#22120;
&lt;/p&gt;
&lt;p&gt;
Make Large Language Model a Better Ranker
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#26174;&#33879;&#22686;&#24378;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25512;&#33616;&#31995;&#32479;&#65288;RSs&#65289;&#27010;&#24565;&#21644;&#24320;&#21457;&#26041;&#24335;&#21457;&#29983;&#20102;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#23545;&#28857;&#21644;&#25104;&#23545;&#25512;&#33616;&#33539;&#24335;&#19978;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#22120;&#20013;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#19968;&#20123;&#30740;&#31350;&#34429;&#28982;&#28145;&#20837;&#30740;&#31350;&#20102;&#21015;&#34920;&#22411;&#26041;&#27861;&#65292;&#20294;&#22312;&#25490;&#21517;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#19981;&#36275;&#24402;&#22240;&#20110;&#25490;&#21517;&#21644;&#35821;&#35328;&#29983;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#12290;ALRO&#26088;&#22312;&#24357;&#21512;LLMs&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#24494;&#22937;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;ALRO&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#24341;&#20837;&#20102;&#36719;lambda&#20540;lo
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19181v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda lo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24605;&#21644;&#25552;&#20379;&#22810;&#20010;&#20505;&#36873;&#31572;&#26696;&#30340;&#29702;&#30001;&#26469;&#35299;&#20915;&#23545;&#19981;&#27491;&#30830;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.09972</link><description>&lt;p&gt;
&#22312;&#25215;&#35834;&#20043;&#21069;&#19977;&#24605;&#65306;&#36890;&#36807;&#21453;&#24605;&#22810;&#20010;&#31572;&#26696;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24605;&#21644;&#25552;&#20379;&#22810;&#20010;&#20505;&#36873;&#31572;&#26696;&#30340;&#29702;&#30001;&#26469;&#35299;&#20915;&#23545;&#19981;&#27491;&#30830;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32622;&#20449;&#24230;&#20272;&#35745;&#26088;&#22312;&#35780;&#20272;&#36755;&#20986;&#30340;&#21487;&#20449;&#24230;&#65292;&#22312;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#40657;&#30418;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#22312;&#29983;&#25104;&#19981;&#27491;&#30830;&#31572;&#26696;&#26102;&#30340;&#36807;&#24230;&#33258;&#20449;&#65292;&#29616;&#26377;&#23545;LLM&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#36890;&#24120;&#19981;&#21487;&#26657;&#20934;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21463;&#21040;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#30340;&#38459;&#30861;&#65292;&#21363;&#23427;&#20204;&#20165;&#32771;&#34385;LLM&#29983;&#25104;&#30340;&#19968;&#20010;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#33539;&#24335;&#65292;&#24443;&#24213;&#35780;&#20272;&#22810;&#20010;&#20505;&#36873;&#31572;&#26696;&#30340;&#21487;&#20449;&#24230;&#65292;&#20197;&#20943;&#36731;&#23545;&#19981;&#27491;&#30830;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;&#22522;&#20110;&#36825;&#19968;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#27493;&#26694;&#26550;&#65292;&#39318;&#20808;&#25351;&#23548;LLM&#21453;&#24605;&#24182;&#20026;&#27599;&#20010;&#31572;&#26696;&#25552;&#20379;&#29702;&#30001;&#65292;&#28982;&#21518;&#27719;&#24635;&#36825;&#20123;&#29702;&#30001;&#36827;&#34892;&#32508;&#21512;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;&#36825;&#19968;&#26694;&#26550;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09972v1 Announce Type: new  Abstract: Confidence estimation aiming to evaluate output trustability is crucial for the application of large language models (LLM), especially the black-box ones. Existing confidence estimation of LLM is typically not calibrated due to the overconfidence of LLM on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM. To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trustability of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#25105;&#35780;&#20272;&#20013;&#21033;&#29992;&#29627;&#29827;&#31665;&#29305;&#24449;&#30340;&#23454;&#29992;&#24615;&#65292;&#21457;&#29616;softmax&#20998;&#24067;&#22312;&#36136;&#37327;&#35780;&#20272;&#20013;&#21487;&#38752;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#29305;&#24449;&#22686;&#24378;&#35780;&#20272;&#30340;&#31574;&#30053;&#65292;&#24182;&#39564;&#35777;&#20102;&#20351;&#29992;&#29627;&#29827;&#31665;&#29305;&#24449;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#35780;&#20272;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04222</link><description>&lt;p&gt;
&#22522;&#20110;&#29627;&#29827;&#31665;&#29305;&#24449;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Self-Evaluation of Large Language Model based on Glass-box Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04222
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#25105;&#35780;&#20272;&#20013;&#21033;&#29992;&#29627;&#29827;&#31665;&#29305;&#24449;&#30340;&#23454;&#29992;&#24615;&#65292;&#21457;&#29616;softmax&#20998;&#24067;&#22312;&#36136;&#37327;&#35780;&#20272;&#20013;&#21487;&#38752;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#29305;&#24449;&#22686;&#24378;&#35780;&#20272;&#30340;&#31574;&#30053;&#65292;&#24182;&#39564;&#35777;&#20102;&#20351;&#29992;&#29627;&#29827;&#31665;&#29305;&#24449;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#35780;&#20272;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04222v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34028;&#21187;&#21457;&#23637;&#20984;&#26174;&#20102;&#23545;&#35780;&#20272;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20381;&#36182;&#20110;&#22806;&#37096;&#35780;&#20272;&#32773;&#65292;&#20391;&#37325;&#20110;&#35757;&#32451;&#21644;&#25552;&#31034;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#8212;&#8212;&#27169;&#22411;&#24863;&#30693;&#30340;&#29627;&#29827;&#31665;&#29305;&#24449;&#8212;&#8212;&#34987;&#24573;&#35270;&#20102;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#33258;&#25105;&#35780;&#20272;&#24773;&#22659;&#19979;&#20351;&#29992;&#29627;&#29827;&#31665;&#29305;&#24449;&#30340;&#25928;&#29992;&#65292;&#21363;&#24212;&#29992;LLM&#35780;&#20272;&#20854;&#33258;&#36523;&#36755;&#20986;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#29627;&#29827;&#31665;&#29305;&#24449;&#32452;&#65292;&#24182;&#21457;&#29616;softmax&#20998;&#24067;&#20316;&#20026;&#36136;&#37327;&#35780;&#20272;&#30340;&#21487;&#38752;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#21512;&#24182;&#20174;&#21442;&#32771;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#26469;&#22686;&#24378;&#35780;&#20272;&#30340;&#20004;&#31181;&#31574;&#30053;&#12290;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#20351;&#29992;&#29627;&#29827;&#31665;&#29305;&#24449;&#36827;&#34892;LLMs&#30340;&#33258;&#25105;&#35780;&#20272;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04222v1 Announce Type: new  Abstract: The proliferation of open-source Large Language Models (LLMs) underscores the pressing need for evaluation methods. Existing works primarily rely on external evaluators, focusing on training and prompting strategies. However, a crucial aspect - model-aware glass-box features - is overlooked. In this study, we explore the utility of glass-box features under the scenario of self-evaluation, namely applying an LLM to evaluate its own output. We investigate various glass-box feature groups and discovered that the softmax distribution serves as a reliable indicator for quality evaluation. Furthermore, we propose two strategies to enhance the evaluation by incorporating features derived from references. Experimental results on public benchmarks validate the feasibility of self-evaluation of LLMs using glass-box features.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20004;&#31181;&#26041;&#27861;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#23454;&#20307;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24494;&#35843;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#23454;&#20307;&#30340;&#24615;&#33021;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#21017;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01432</link><description>&lt;p&gt;
&#24494;&#35843;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#29992;&#20110;&#19981;&#22826;&#27969;&#34892;&#30693;&#35782;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20004;&#31181;&#26041;&#27861;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#23454;&#20307;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24494;&#35843;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#23454;&#20307;&#30340;&#24615;&#33021;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#21017;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35760;&#24518;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#24403;&#22788;&#29702;&#19981;&#22826;&#27969;&#34892;&#25110;&#20302;&#39057;&#27010;&#24565;&#21644;&#23454;&#20307;&#26102;&#65292;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#20363;&#22914;&#22312;&#39046;&#22495;&#29305;&#23450;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#25506;&#35752;&#21644;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65288;FT&#65289;&#23545;&#23450;&#21046;LLMs&#22788;&#29702;&#20302;&#39057;&#23454;&#20307;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FT&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#23454;&#20307;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#21463;&#27426;&#36814;&#21644;&#26368;&#19981;&#21463;&#27426;&#36814;&#30340;&#32676;&#20307;&#20013;&#65292;&#32780;RAG&#36229;&#36234;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;&#21478;&#22806;&#65292;&#26816;&#32034;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#36827;&#27493;&#21152;&#24378;&#20102;RAG&#21644;FT&#26041;&#27861;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01432v1 Announce Type: new  Abstract: Large language models (LLMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LLMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LLMs in handling low-frequency entities on question answering task. Our findings indicate that FT significantly boosts the performance across entities of varying popularity, especially in the most and least popular groups, while RAG surpasses other methods. Additionally, the success of both RAG and FT approaches is amplified by advancements in retrieval and data augmentation techniques. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SKT5SciSumm&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#24341;&#25991;&#20449;&#24687;&#30340;&#21464;&#25442;&#22120;&#21644;T5&#31995;&#21015;&#27169;&#22411;&#65292;&#22312;&#22810;&#25991;&#26723;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17311</link><description>&lt;p&gt;
SKT5SciSumm - &#19968;&#31181;&#29992;&#20110;&#22810;&#25991;&#26723;&#31185;&#23398;&#25688;&#35201;&#30340;&#28151;&#21512;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SKT5SciSumm - A Hybrid Generative Approach for Multi-Document Scientific Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17311
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SKT5SciSumm&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#24341;&#25991;&#20449;&#24687;&#30340;&#21464;&#25442;&#22120;&#21644;T5&#31995;&#21015;&#27169;&#22411;&#65292;&#22312;&#22810;&#25991;&#26723;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17311v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#31185;&#23398;&#25991;&#26412;&#25688;&#35201;&#23545;&#20110;&#30740;&#31350;&#30028;&#21644;&#20154;&#31867;&#31038;&#20250;&#37117;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;&#32771;&#34385;&#21040;&#31185;&#23398;&#25991;&#26412;&#30340;&#29305;&#27530;&#24615;&#20197;&#21450;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#30340;&#36755;&#20837;&#23454;&#36136;&#19978;&#24456;&#38271;&#65292;&#35813;&#20219;&#21153;&#38656;&#35201;&#36275;&#22815;&#30340;&#23884;&#20837;&#29983;&#25104;&#21644;&#25991;&#26412;&#25130;&#26029;&#65292;&#21516;&#26102;&#21448;&#19981;&#33021;&#20002;&#22833;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SKT5SciSumm - &#19968;&#31181;&#29992;&#20110;&#22810;&#25991;&#26723;&#31185;&#23398;&#25688;&#35201;&#30340;&#28151;&#21512;&#26694;&#26550;&#65288;MDSS&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#24341;&#25991;&#20449;&#24687;&#30340;&#21464;&#25442;&#22120;(SPECTER)&#30340;&#31185;&#23398;&#25991;&#29486;&#23884;&#20837;&#30340;&#21477;&#23376;-&#21464;&#25442;&#22120;&#29256;&#26412;&#26469;&#32534;&#30721;&#21644;&#34920;&#31034;&#25991;&#26412;&#21477;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#20351;&#29992;k-means&#32858;&#31867;&#36827;&#34892;&#39640;&#25928;&#25688;&#35201;&#25552;&#21462;&#12290;&#25105;&#20204;&#20351;&#29992;T5&#31995;&#21015;&#27169;&#22411;&#20351;&#29992;&#25552;&#21462;&#30340;&#21477;&#23376;&#29983;&#25104;&#25277;&#35937;&#25688;&#35201;&#12290;SKT5SciSumm&#22312;Multi-XScience&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17311v1 Announce Type: new  Abstract: Summarization for scientific text has shown significant benefits both for the research community and human society. Given the fact that the nature of scientific text is distinctive and the input of the multi-document summarization task is substantially long, the task requires sufficient embedding generation and text truncation without losing important information. To tackle these issues, in this paper, we propose SKT5SciSumm - a hybrid framework for multi-document scientific summarization (MDSS). We leverage the Sentence-Transformer version of Scientific Paper Embeddings using Citation-Informed Transformers (SPECTER) to encode and represent textual sentences, allowing for efficient extractive summarization using k-means clustering. We employ the T5 family of models to generate abstractive summaries using extracted sentences. SKT5SciSumm achieves state-of-the-art performance on the Multi-XScience dataset. Through extensive experiments and
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;MATHWELL&#27169;&#22411;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#65292;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#32467;&#26524;&#26174;&#31034;&#65292;MATHWELL&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#24182;&#31526;&#21512;&#25152;&#26377;&#26631;&#20934;&#30340;&#20221;&#39069;&#27604;&#20854;&#20182;&#36873;&#25321;&#39640;&#20986;40%&#65292;&#20854;&#20013;74%&#30340;&#21487;&#35299;&#20915;&#38382;&#39064;&#21516;&#26102;&#20570;&#21040;&#20102;&#20934;&#30830;&#21644;&#36866;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.15861</link><description>&lt;p&gt;
MATHWELL: &#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#25945;&#32946;&#25968;&#23398;&#24212;&#29992;&#39064;
&lt;/p&gt;
&lt;p&gt;
MATHWELL: Generating Educational Math Word Problems at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15861
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;MATHWELL&#27169;&#22411;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#65292;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#32467;&#26524;&#26174;&#31034;&#65292;MATHWELL&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#24182;&#31526;&#21512;&#25152;&#26377;&#26631;&#20934;&#30340;&#20221;&#39069;&#27604;&#20854;&#20182;&#36873;&#25321;&#39640;&#20986;40%&#65292;&#20854;&#20013;74%&#30340;&#21487;&#35299;&#20915;&#38382;&#39064;&#21516;&#26102;&#20570;&#21040;&#20102;&#20934;&#30830;&#21644;&#36866;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#24212;&#29992;&#39064;&#22312;K-8&#25945;&#32946;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#32534;&#20889;&#23427;&#20204;&#32791;&#26102;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#35268;&#27169;&#21270;&#38382;&#39064;&#26469;&#25903;&#25345;K-8&#25968;&#23398;&#25945;&#32946;&#12290;&#20026;&#20102;&#25945;&#32946;&#24615;&#65292;&#29983;&#25104;&#30340;&#38382;&#39064;&#24517;&#39035;&#26159;1&#65289;&#21487;&#35299;&#20915;&#30340;&#65292;2&#65289;&#20934;&#30830;&#30340;&#65292;3&#65289;&#36866;&#24403;&#30340;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#26410;&#26631;&#35760;&#36825;&#20123;&#26631;&#20934;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#35757;&#32451;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MATHWELL&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#24494;&#35843;&#30340;70B Llama-2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;K-8&#25968;&#23398;&#24212;&#29992;&#39064;&#12290;&#20511;&#21161;MATHWELL&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#12290;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#30340;3,484&#20010;&#38382;&#39064;&#21457;&#29616;&#65292;MATHWELL&#25317;&#26377;&#27604;&#20854;&#20182;&#36873;&#25321;&#26356;&#39640;&#30340;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#21644;&#28385;&#36275;&#25152;&#26377;&#26631;&#20934;&#30340;&#38382;&#39064;&#20221;&#39069;&#39640;&#20986;40&#65285;&#65292;&#20854;&#20013;74&#65285;&#30340;&#38382;&#39064;&#20855;&#26377;&#21487;&#35299;&#30340;&#12289;&#20934;&#30830;&#30340;&#21644;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15861v1 Announce Type: new  Abstract: Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise. We suggest that language models can support K-8 math education by automatically generating problems at scale. To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate. Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators. We introduce MATHWELL, a Llama-2 (70B) model iteratively finetuned to generate K-8 math word problems using data from expert annotation. Using MATHWELL, we generate the largest English word problem dataset to date, containing 20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12842</link><description>&lt;p&gt;
PromptKD&#65306;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#20026;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12842
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#23545;&#25512;&#29702;&#25104;&#26412;&#30340;&#25285;&#24551;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#38024;&#23545;LLMs&#36825;&#26679;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;KD&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#27169;&#22411;&#30340;KD&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptKD&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972; - &#22312;KD&#20013;&#39318;&#27425;&#20986;&#29616; - &#20351;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20256;&#36882;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#12290;&#19982;&#20808;&#21069;&#20998;&#31867;&#24037;&#20316;&#19981;&#21516;&#65292;&#20808;&#21069;&#37027;&#20123;&#38656;&#35201;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#20197;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#65292;PromptKD&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#20165;&#36890;&#36807;&#23398;&#29983;&#25351;&#23548;&#35843;&#25972;&#25552;&#31034;&#26469;&#36798;&#21040;&#31867;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12842v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07309</link><description>&lt;p&gt;
HyperBERT:&#23558;&#28151;&#21512;&#36229;&#22270;&#24863;&#30693;&#23618;&#19982;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#36890;&#36807;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#26631;&#35760;&#65292;&#34920;&#36798;&#22810;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;&#36229;&#36793;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#36229;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#21516;&#26102;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#20840;&#37096;&#20869;&#23481;&#21644;&#33410;&#28857;&#23646;&#24615;&#20013;&#30340;&#20016;&#23500;&#35821;&#35328;&#23646;&#24615;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20026;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#36827;&#19968;&#27493;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#24341;&#20837;&#19987;&#38376;&#30340;&#36229;&#22270;&#24863;&#30693;&#23618;&#12290;&#36825;&#20123;&#23618;&#23558;&#39640;&#38454;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#24341;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#21033;&#29992;&#36229;&#22270;&#32467;&#26500;&#20013;&#30340;&#39640;&#38454;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#25991;&#26412;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
&lt;/p&gt;</description></item><item><title>SEER&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#26469;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13246</link><description>&lt;p&gt;
SEER: &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning. (arXiv:2401.13246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13246
&lt;/p&gt;
&lt;p&gt;
SEER&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#26469;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38416;&#26126;&#20174;&#38382;&#39064;&#21040;&#31572;&#26696;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#35299;&#37322;&#26159;&#26681;&#26412;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#26174;&#33879;&#22686;&#24378;&#20102;&#38382;&#31572;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#32467;&#26500;&#21270;&#35299;&#37322;&#35201;&#27714;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#25512;&#29702;&#65292;&#36825;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38598;&#20013;&#22312;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#21333;&#27493;&#25512;&#29702;&#65292;&#24573;&#35270;&#27493;&#39588;&#20043;&#38388;&#30340;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#32467;&#26500;&#21270;&#20851;&#31995;&#65292;&#38459;&#30861;&#20102;RL&#22312;&#32467;&#26500;&#21270;&#25512;&#29702;&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEER&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#65292;&#20197;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#20934;&#30830;&#25551;&#36848;&#20102;&#32467;&#26500;&#21270;&#25512;&#29702;&#20013;&#22266;&#26377;&#30340;&#20998;&#23618;&#21644;&#20998;&#25903;&#32467;&#26500;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#29366;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Elucidating the reasoning process with structured explanations from question to answer is fundamentally crucial, as it significantly enhances the interpretability and trustworthiness of question-answering (QA) systems. However, structured explanations demand models to perform intricate structured reasoning, which poses great challenges. Most existing methods focus on single-step reasoning through supervised learning, ignoring logical dependencies between steps. Meanwhile, existing reinforcement learning (RL)-based methods overlook the structured relationships, impeding RL's potential in structured reasoning. In this paper, we propose SEER, a novel method that maximizes a structure-based return to facilitate structured reasoning and explanation. Our proposed structure-based return precisely describes the hierarchical and branching structure inherent in structured reasoning, effectively capturing the intricate relationships between states. We also introduce a fine-grained reward function
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20020;&#24202;&#25688;&#35201;&#20013;&#20351;&#29992;&#21477;&#23376;&#32423;&#35268;&#21010;&#24182;&#36890;&#36807;&#23884;&#20837;&#24335;&#23454;&#20307;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02369</link><description>&lt;p&gt;
SPEER: Embedded Entity Retrieval&#19979;&#30340;&#38271;&#20020;&#24202;&#25688;&#35201;&#21477;&#23376;&#32423;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval. (arXiv:2401.02369v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20020;&#24202;&#25688;&#35201;&#20013;&#20351;&#29992;&#21477;&#23376;&#32423;&#35268;&#21010;&#24182;&#36890;&#36807;&#23884;&#20837;&#24335;&#23454;&#20307;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#21307;&#29983;&#22312;&#27599;&#27425;&#30149;&#20154;&#20986;&#38498;&#26102;&#24517;&#39035;&#20889;&#19968;&#20221;&#20887;&#38271;&#30340;&#25688;&#35201;&#12290;&#30001;&#20110;&#28085;&#30422;&#30340;&#20020;&#24202;&#27010;&#24565;&#25968;&#37327;&#24222;&#22823;&#65292;&#36825;&#39033;&#20219;&#21153;&#38750;&#24120;&#32791;&#26102;&#12290;&#35782;&#21035;&#21644;&#28085;&#30422;&#26174;&#33879;&#23454;&#20307;&#23545;&#20110;&#25688;&#35201;&#30340;&#20020;&#24202;&#23454;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#35813;&#20219;&#21153;&#19978;&#24494;&#35843;&#20102;&#24320;&#28304;&#30340;LLM&#27169;&#22411;&#65288;Mistral-7B-Instruct&#21644;Zephyr-7B-&#951;&#65289;&#65292;&#21457;&#29616;&#23427;&#20204;&#29983;&#25104;&#30340;&#25688;&#35201;&#19981;&#23436;&#25972;&#19988;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#22686;&#21152;&#23454;&#20307;&#35206;&#30422;&#33539;&#22260;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#36739;&#23567;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#39044;&#27979;&#26174;&#33879;&#23454;&#20307;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20869;&#23481;&#35745;&#21010;&#26469;&#25351;&#23548;LLM&#12290;&#20026;&#20102;&#40723;&#21169;LLM&#20851;&#27880;&#28304;&#31508;&#35760;&#20013;&#30340;&#29305;&#23450;&#25552;&#21450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPEER&#65306;Embedded Entity Retrieval&#19979;&#30340;&#21477;&#23376;&#32423;&#35268;&#21010;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#29305;&#27530;&#30340;"{{ }}"&#36793;&#30028;&#26631;&#31614;&#26631;&#35760;&#27599;&#20010;&#26174;&#33879;&#23454;&#20307;&#36328;&#24230;&#65292;&#24182;&#35201;&#27714;LLM&#22312;&#29983;&#25104;&#27599;&#20010;&#21477;&#23376;&#20043;&#21069;&#26816;&#32034;&#26631;&#35760;&#30340;&#36328;&#24230;&#12290;&#21477;&#23376;&#32423;&#35268;&#21010;&#30456;&#24403;&#20110;&#19968;&#31181;&#29366;&#24577;&#36861;&#36394;&#65292;&#27169;&#22411;&#26126;&#30830;&#35760;&#24405;&#19979;&#27599;&#20010;&#21477;&#23376;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinician must write a lengthy summary each time a patient is discharged from the hospital. This task is time-consuming due to the sheer number of unique clinical concepts covered in the admission. Identifying and covering salient entities is vital for the summary to be clinically useful. We fine-tune open-source LLMs (Mistral-7B-Instruct and Zephyr-7B-\b{eta}) on the task and find that they generate incomplete and unfaithful summaries. To increase entity coverage, we train a smaller, encoder-only model to predict salient entities, which are treated as content-plans to guide the LLM. To encourage the LLM to focus on specific mentions in the source notes, we propose SPEER: Sentence-level Planning via Embedded Entity Retrieval. Specifically, we mark each salient entity span with special "{{ }}" boundary tags and instruct the LLM to retrieve marked spans before generating each sentence. Sentence-level planning acts as a form of state tracking in that the model is explicitly recording the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;ICL&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26032;&#33539;&#24335;&#65292;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#39640;&#32423;&#25216;&#26415;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2301.00234</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on In-context Learning. (arXiv:2301.00234v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;ICL&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26032;&#33539;&#24335;&#65292;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#39640;&#32423;&#25216;&#26415;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#22312;&#20854;&#20013;LLM&#20165;&#22522;&#20110;&#21152;&#20837;&#23569;&#37327;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#39044;&#27979;&#12290;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;LLM&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;&#21644;&#24635;&#32467;ICL&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#28548;&#28165;&#20854;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32452;&#32455;&#21644;&#35752;&#35770;&#39640;&#32423;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#31574;&#30053;&#12289;&#28436;&#31034;&#35774;&#35745;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#40723;&#21169;&#26356;&#22810;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;ICL&#30340;&#24037;&#20316;&#21407;&#29702;&#24182;&#25913;&#36827;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.
&lt;/p&gt;</description></item><item><title>DICTDIS&#26159;&#19968;&#31181;&#26032;&#39062;&#26377;&#35789;&#20856;&#32422;&#26463;&#30340;NMT&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#22810;&#20010;&#23383;&#20856;&#20505;&#36873;&#39033;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20174;&#22810;&#20041;&#35789;&#20013;&#28040;&#38500;&#32763;&#35793;&#27495;&#20041;&#30340;&#30446;&#30340;&#65292;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.06996</link><description>&lt;p&gt;
DICTDIS&#65306;&#22522;&#20110;&#35789;&#20856;&#32422;&#26463;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#28040;&#27495;&#26041;&#27861;&#23545; NMT &#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
DICTDIS: Dictionary Constrained Disambiguation for Improved NMT. (arXiv:2210.06996v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06996
&lt;/p&gt;
&lt;p&gt;
DICTDIS&#26159;&#19968;&#31181;&#26032;&#39062;&#26377;&#35789;&#20856;&#32422;&#26463;&#30340;NMT&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#22810;&#20010;&#23383;&#20856;&#20505;&#36873;&#39033;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20174;&#22810;&#20041;&#35789;&#20013;&#28040;&#38500;&#32763;&#35793;&#27495;&#20041;&#30340;&#30446;&#30340;&#65292;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65288;&#20363;&#22914;&#25945;&#32946;&#24212;&#29992;&#31243;&#24207;&#65289;&#22312;&#22810;&#35821;&#35328;&#31038;&#20250;&#20013;&#24110;&#21161;&#20351;&#20449;&#24687;&#23545;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#21487;&#35775;&#38382;&#26159;&#20855;&#26377;&#31038;&#20250;&#24847;&#20041;&#30340;&#12290;&#36825;&#31181; NMT &#31995;&#32479;&#24212;&#35813;&#20855;&#26377;&#35789;&#27719;&#32422;&#26463;&#24182;&#20174;&#39046;&#22495;&#29305;&#23450;&#30340;&#35789;&#20856;&#20013;&#27762;&#21462;&#12290;&#30001;&#20110;&#21333;&#35789;&#30340;&#22810;&#20041;&#24615;&#65292;&#35789;&#20856;&#20013;&#21487;&#33021;&#20250;&#20026;&#28304;&#21333;&#35789;&#25110;&#30701;&#35821;&#21576;&#29616;&#22810;&#20010;&#20505;&#36873;&#32763;&#35793;&#12290;&#36825;&#26102;&#65292;NMT &#27169;&#22411;&#38656;&#35201;&#36873;&#25321;&#19982;&#35821;&#22659;&#26368;&#30456;&#20851;&#30340;&#20505;&#36873;&#32763;&#35793;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#24573;&#30053;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#20391;&#37325;&#20110;&#21333;&#20010;&#20505;&#36873;&#32422;&#26463;&#35774;&#32622;&#65292;&#20854;&#20013;&#30446;&#26631;&#35789;&#25110;&#30701;&#35821;&#34987;&#21333;&#20010;&#32422;&#26463;&#26367;&#25442;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DICTDIS&#30340;&#35789;&#20856;&#32422;&#26463; NMT &#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#28040;&#38500;&#20102;&#20174;&#23383;&#20856;&#20013;&#24471;&#20986;&#30340;&#22810;&#20010;&#20505;&#36873;&#32763;&#35793;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35757;&#32451;&#25968;&#25454;&#19982;&#22810;&#20010;&#23383;&#20856;&#20505;&#36873;&#39033;&#36827;&#34892;&#22686;&#37327;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#31215;&#26497;&#40723;&#21169;&#28040;&#38500;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-specific neural machine translation (NMT) systems (\eg, in educational applications) are socially significant with the potential to help make information accessible to a diverse set of users in multilingual societies. It is desirable that such NMT systems be lexically constrained and draw from domain-specific dictionaries. Dictionaries could present multiple candidate translations for a source word/phrase due to the polysemous nature of words. The onus is then on the NMT model to choose the contextually most appropriate candidate. Prior work has largely ignored this problem and focused on the single candidate constraint setting wherein the target word or phrase is replaced by a single constraint. In this work we present \dictdis, a lexically constrained NMT system that disambiguates between multiple candidate translations derived from dictionaries. We achieve this by augmenting training data with multiple dictionary candidates to actively encourage disambiguation during training
&lt;/p&gt;</description></item></channel></rss>