<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#12289;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#24110;&#21161;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23884;&#20837;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.00458</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#20992;&#20999;&#65306;&#22810;&#39046;&#22495;&#12289;&#22810;&#20219;&#21153;&#26694;&#26550;&#29992;&#20110;&#23884;&#20837;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for Embedding Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00458
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#12289;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#24110;&#21161;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23884;&#20837;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#24110;&#21161;&#36873;&#25321;&#36866;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#26368;&#26377;&#25928;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19987;&#26377;&#21644;&#24320;&#28304;&#32534;&#30721;&#22120;&#27169;&#22411;&#22823;&#37327;&#22686;&#21152;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00458v1 Announce Type: new  Abstract: This position paper proposes a systematic approach towards developing a framework to help select the most effective embedding models for natural language processing (NLP) tasks, addressing the challenge posed by the proliferation of both proprietary and open-source encoder models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#20351;&#29992;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12212</link><description>&lt;p&gt;
&#35780;&#20272;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#27604;&#36739;&#20998;&#26512;&#24052;&#35199;&#20844;&#21496;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#19978;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#20351;&#29992;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#31181;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20851;&#20110;NER&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#33521;&#35821;&#25991;&#26723;&#19978;&#65292;&#23548;&#33268;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36130;&#21153;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#37329;&#34701;&#39046;&#22495;&#20869;NER&#38656;&#27714;&#65292;&#24182;&#20391;&#37325;&#20110;&#20174;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#25552;&#21462;&#30340;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#12290;&#36890;&#36807;&#25972;&#29702;&#21253;&#25324;384&#20010;&#36716;&#24405;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#36827;&#34892;&#27880;&#37322;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#33889;&#33796;&#29273;&#35821;&#65288;BERTimbau&#21644;PTT5&#65289;&#35757;&#32451;&#30340;&#21333;&#35821;&#27169;&#22411;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;mBERT&#21644;mT5&#65289;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;T5&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#35780;&#20272;&#12290;&#22312;&#27169;&#22411;&#24494;&#35843;&#20043;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12212v1 Announce Type: cross  Abstract: Named Entity Recognition (NER) is a Natural Language Processing technique for extracting information from textual documents. However, much of the existing research on NER has been centered around English-language documents, leaving a gap in the availability of datasets tailored to the financial domain in Portuguese. This study addresses the need for NER within the financial domain, focusing on Portuguese-language texts extracted from earnings call transcriptions of Brazilian banks. By curating a comprehensive dataset comprising 384 transcriptions and leveraging weak supervision techniques for annotation, we evaluate the performance of monolingual models trained on Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5). Notably, we introduce a novel approach that reframes the token classification task as a text generation problem, enabling fine-tuning and evaluation of T5 models. Following the fine-tuning of the models,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;&#65288;LLMMs&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20020;&#24202;&#31508;&#35760;&#21644;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#29992;&#20110;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.04785</link><description>&lt;p&gt;
&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#39044;&#27979;5&#24180;&#24930;&#24615;&#30142;&#30149;&#38431;&#21015;&#30340;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Multimodal Models for 5-Year Chronic Disease Cohort Prediction Using EHR Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;&#65288;LLMMs&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20020;&#24202;&#31508;&#35760;&#21644;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#29992;&#20110;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24930;&#24615;&#30142;&#30149;&#22914;&#31958;&#23615;&#30149;&#26159;&#20840;&#29699;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26412;&#30740;&#31350;&#20174;&#21488;&#28286;&#21307;&#38498;&#25968;&#25454;&#24211;&#25910;&#38598;&#20102;&#20116;&#24180;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#21253;&#25324;1,420,596&#20221;&#20020;&#24202;&#31508;&#35760;&#12289;387,392&#20221;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#20197;&#21450;&#36229;&#36807;1,505&#31181;&#23454;&#39564;&#23460;&#26816;&#39564;&#39033;&#30446;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#29992;&#20110;&#30740;&#31350;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;&#65288;LLMMs&#65289;&#26694;&#26550;&#65292;&#23558;&#20020;&#24202;&#31508;&#35760;&#21644;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#25991;&#26412;&#23884;&#20837;&#32534;&#30721;&#22120;&#21644;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#26469;&#23398;&#20064;&#23454;&#39564;&#23460;&#26816;&#39564;&#25968;&#20540;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22359;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04785v1 Announce Type: cross  Abstract: Chronic diseases such as diabetes are the leading causes of morbidity and mortality worldwide. Numerous research studies have been attempted with various deep learning models in diagnosis. However, most previous studies had certain limitations, including using publicly available datasets (e.g. MIMIC), and imbalanced data. In this study, we collected five-year electronic health records (EHRs) from the Taiwan hospital database, including 1,420,596 clinical notes, 387,392 laboratory test results, and more than 1,505 laboratory test items, focusing on research pre-training large language models. We proposed a novel Large Language Multimodal Models (LLMMs) framework incorporating multimodal data from clinical notes and laboratory test results for the prediction of chronic disease risk. Our method combined a text embedding encoder and multi-head attention layer to learn laboratory test values, utilizing a deep neural network (DNN) module to 
&lt;/p&gt;</description></item><item><title>&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04261</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#21306;&#25361;&#25112;&#25512;&#21160;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advancing Biomedical Text Mining with Community Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04261
&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#39046;&#22495;&#31215;&#32047;&#20102;&#22823;&#37327;&#26469;&#33258;&#31185;&#23398;&#25991;&#29486;&#12289;&#30005;&#23376;&#30149;&#21382;&#12289;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#21508;&#26041;&#38754;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#28982;&#32780;&#25163;&#21160;&#22788;&#29702;&#21644;&#20998;&#26512;&#36825;&#20123;&#24222;&#22823;&#19988;&#22797;&#26434;&#30340;&#36164;&#28304;&#26159;&#32791;&#26102;&#19988;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#65292;&#20063;&#31216;&#20026;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#22791;&#21463;&#20851;&#27880;&#12290;&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#25361;&#25112;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#24320;&#21457;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#25366;&#25496;&#21644;&#20449;&#24687;&#22788;&#29702;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#30340;&#24179;&#21488;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19982;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#26377;&#20851;&#30340;&#26368;&#26032;&#31038;&#21306;&#25361;&#25112;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04261v1 Announce Type: new  Abstract: The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention. Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research. These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining. Firs
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SimuCourt&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#21496;&#27861;&#25991;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#21496;&#27861;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.02959</link><description>&lt;p&gt;
SimuCourt: &#21033;&#29992;&#30495;&#23454;&#21496;&#27861;&#21028;&#20915;&#25991;&#20214;&#26500;&#24314;&#21496;&#27861;&#20915;&#31574;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02959
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SimuCourt&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#21496;&#27861;&#25991;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#21496;&#27861;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20256;&#32479;&#21496;&#27861;&#34892;&#19994;&#21508;&#20010;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#20010;&#21035;&#21496;&#27861;&#38454;&#27573;&#65292;&#24573;&#35270;&#20102;&#36328;&#38454;&#27573;&#30340;&#21327;&#20316;&#12290;&#38543;&#30528;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#20195;&#29702;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26234;&#33021;&#65292;&#24182;&#33021;&#20570;&#20986;&#22797;&#26434;&#20915;&#31574;&#65292;&#20026;&#21496;&#27861;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SimuCourt&#65292;&#19968;&#20010;&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#30340;420&#20221;&#21028;&#20915;&#25991;&#20214;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#26368;&#24120;&#35265;&#31867;&#22411;&#30340;&#21496;&#27861;&#26696;&#20363;&#65292;&#20197;&#21450;&#19968;&#20010;&#26032;&#39062;&#20219;&#21153;&#21496;&#27861;&#20915;&#31574;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#21496;&#27861;&#30693;&#35782;&#24211;&#65292;JudicialKB&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#31181;&#27861;&#24459;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;AgentsCourt
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02959v1 Announce Type: cross  Abstract: With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32676;&#32452;&#21644;&#23545;&#31216;&#24615;&#21407;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#30740;&#31350;&#22235;&#20010;&#32676;&#32452;&#23646;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#20445;&#25345;&#32676;&#32452;&#23646;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.06120</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32676;&#32452;&#21644;&#23545;&#31216;&#24615;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
Exploring Group and Symmetry Principles in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32676;&#32452;&#21644;&#23545;&#31216;&#24615;&#21407;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#30740;&#31350;&#22235;&#20010;&#32676;&#32452;&#23646;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#20445;&#25345;&#32676;&#32452;&#23646;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#35780;&#20272;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;&#32676;&#32452;&#21644;&#23545;&#31216;&#24615;&#21407;&#29702;&#20026;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#36825;&#20123;&#21407;&#29702;&#22312;&#29289;&#29702;&#23398;&#21644;&#25968;&#23398;&#31561;&#39046;&#22495;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#35780;&#20272;&#23427;&#20204;&#33021;&#21147;&#30340;&#26041;&#24335;&#12290;&#34429;&#28982;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#20026;&#20102;&#23637;&#31034;&#20351;&#29992;&#36825;&#20123;&#23646;&#24615;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#20851;&#27880;&#31639;&#26415;&#25512;&#29702;&#65292;&#24182;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#32676;&#32452;&#23646;&#24615;&#65288;&#23553;&#38381;&#24615;&#12289;&#24658;&#31561;&#24615;&#12289;&#36870;&#24615;&#21644;&#32467;&#21512;&#24615;&#65289;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#30740;&#31350;&#30340;LLM&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#26041;&#26696;&#20013;&#38590;&#20197;&#20445;&#25345;&#32676;&#32452;&#23646;&#24615;&#12290;&#22312;&#23553;&#38381;&#24615;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23545;&#29305;&#23450;&#36755;&#20986;&#30340;&#20559;&#35265;&#65292;&#24182;&#22312;&#29305;&#23450;&#30340;&#24207;&#21015;&#38271;&#24230;&#21518;&#20174;100&#65285;&#30340;&#24615;&#33021;&#36805;&#36895;&#19979;&#38477;&#21040;0&#65285;&#12290;&#23427;&#20204;&#22312;&#24658;&#31561;&#24615;&#27979;&#35797;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#20195;&#34920;&#20102;&#30456;&#21152;&#24471;&#21040;&#21407;&#25968;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive performance across a wide range of applications; however, assessing their reasoning capabilities remains a significant challenge. In this paper, we introduce a framework grounded in group and symmetry principles, which have played a crucial role in fields such as physics and mathematics, and offer another way to evaluate their capabilities. While the proposed framework is general, to showcase the benefits of employing these properties, we focus on arithmetic reasoning and investigate the performance of these models on four group properties: closure, identity, inverse, and associativity. Our findings reveal that LLMs studied in this work struggle to preserve group properties across different test regimes. In the closure test, we observe biases towards specific outputs and an abrupt degradation in their performance from 100% to 0% after a specific sequence length. They also perform poorly in the identity test, which represents add
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20440;&#33719;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65292;&#19981;&#20165;&#25972;&#20307;&#20934;&#30830;&#29575;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.01676</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#22312;&#20851;&#38190;&#35821;&#27861;&#32467;&#26500;&#19978;&#30340;&#21028;&#26029;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Language models align with human judgments on key grammatical constructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20440;&#33719;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65292;&#19981;&#20165;&#25972;&#20307;&#20934;&#30830;&#29575;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#35821;&#35328;&#26222;&#36941;&#24615;&#65311;Dentella&#31561;&#20154;&#65288;2023&#24180;&#65307;&#8220;DGL&#8221;&#65289;&#20351;&#29992;&#22810;&#20010;LLMs&#25552;&#31034;&#35821;&#27861;&#27491;&#30830;&#24615;&#38382;&#39064;&#65292;&#20197;&#33719;&#21462;80&#20010;&#33521;&#35821;&#21477;&#23376;&#30340;&#35821;&#27861;&#21477;&#23376;&#21028;&#26029;&#65292;&#24471;&#20986;LLMs&#23384;&#22312;&#8220;&#26159;&#8221;&#20559;&#21521;&#21644;&#8220;&#19981;&#33021;&#21306;&#20998;&#35821;&#27861;&#21644;&#38750;&#35821;&#27861;&#21477;&#23376;&#8221;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26082;&#23450;&#30340;&#23454;&#36341;&#26041;&#27861;&#37325;&#26032;&#35780;&#20272;LLM&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;DGL&#30340;&#25968;&#25454;&#23454;&#38469;&#19978;&#35777;&#26126;&#20102;LLM&#22914;&#20309;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#34892;&#20026;&#12290;&#27169;&#22411;&#19981;&#20165;&#25972;&#20307;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#65292;&#36824;&#25429;&#25417;&#21040;&#20102;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#30340;&#32454;&#24494;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models (LLMs) make human-like linguistic generalizations? Dentella et al. (2023; "DGL") prompt several LLMs ("Is the following sentence grammatically correct in English?") to elicit grammaticality judgments of 80 English sentences, concluding that LLMs demonstrate a "yes-response bias" and a "failure to distinguish grammatical from ungrammatical sentences". We re-evaluate LLM performance using well-established practices and find that DGL's data in fact provide evidence for just how well LLMs capture human behaviors. Models not only achieve high accuracy overall, but also capture fine-grained variation in human linguistic judgments.
&lt;/p&gt;</description></item><item><title>Loop Copilot&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;AI&#38899;&#20048;&#21512;&#22863;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#20132;&#20114;&#24335;&#22810;&#36718;&#23545;&#35805;&#30028;&#38754;&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#38899;&#20048;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;AI&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#20010;&#38598;&#20013;&#30340;&#34920;&#20013;&#20445;&#25345;&#20851;&#38190;&#23646;&#24615;&#20197;&#30830;&#20445;&#38899;&#20048;&#30340;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12404</link><description>&lt;p&gt;
Loop Copilot: &#29992;&#20110;&#38899;&#20048;&#29983;&#25104;&#21644;&#36845;&#20195;&#32534;&#36753;&#30340;AI&#21512;&#22863;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing. (arXiv:2310.12404v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12404
&lt;/p&gt;
&lt;p&gt;
Loop Copilot&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;AI&#38899;&#20048;&#21512;&#22863;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#20132;&#20114;&#24335;&#22810;&#36718;&#23545;&#35805;&#30028;&#38754;&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#38899;&#20048;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;AI&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#20010;&#38598;&#20013;&#30340;&#34920;&#20013;&#20445;&#25345;&#20851;&#38190;&#23646;&#24615;&#20197;&#30830;&#20445;&#38899;&#20048;&#30340;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#38899;&#20048;&#26159;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#27599;&#20010;&#38454;&#27573;&#37117;&#38656;&#35201;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;AI&#38899;&#20048;&#31995;&#32479;&#22312;&#32452;&#32455;&#22810;&#20010;&#23376;&#31995;&#32479;&#20197;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Loop Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#20132;&#20114;&#24335;&#12289;&#22810;&#36718;&#23545;&#35805;&#30028;&#38754;&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#38899;&#20048;&#30340;&#26032;&#22411;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#37322;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#36873;&#25321;&#36866;&#24403;&#30340;AI&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#12290;&#27599;&#20010;&#21518;&#31471;&#27169;&#22411;&#37117;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#36755;&#20986;&#32858;&#21512;&#36215;&#26469;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#30830;&#20445;&#38899;&#20048;&#30340;&#36830;&#36143;&#24615;&#65292;&#20851;&#38190;&#23646;&#24615;&#34987;&#20445;&#30041;&#22312;&#19968;&#20010;&#38598;&#20013;&#30340;&#34920;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#21322;&#32467;&#26500;&#21270;&#30340;&#35775;&#35848;&#21644;&#38382;&#21367;&#35843;&#26597;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#20102;&#23427;&#22312;&#20419;&#36827;&#38899;&#20048;&#21019;&#20316;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating music is iterative, requiring varied methods at each stage. However, existing AI music systems fall short in orchestrating multiple subsystems for diverse needs. To address this gap, we introduce Loop Copilot, a novel system that enables users to generate and iteratively refine music through an interactive, multi-round dialogue interface. The system uses a large language model to interpret user intentions and select appropriate AI models for task execution. Each backend model is specialized for a specific task, and their outputs are aggregated to meet the user's requirements. To ensure musical coherence, essential attributes are maintained in a centralized table. We evaluate the effectiveness of the proposed system through semi-structured interviews and questionnaires, highlighting its utility not only in facilitating music creation but also its potential for broader applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#20204;&#22914;&#20309;&#23558;&#36523;&#20221;&#26631;&#31614;&#24212;&#29992;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#20197;&#21450;&#37327;&#21270;&#31361;&#20986;&#30340;&#31038;&#20250;&#32500;&#24230;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09548</link><description>&lt;p&gt;
&#20351;&#29992;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#26469;&#34913;&#37327;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Measuring Stereotypes using Entity-Centric Data. (arXiv:2305.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#20204;&#22914;&#20309;&#23558;&#36523;&#20221;&#26631;&#31614;&#24212;&#29992;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#20197;&#21450;&#37327;&#21270;&#31361;&#20986;&#30340;&#31038;&#20250;&#32500;&#24230;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21051;&#26495;&#21360;&#35937;&#24433;&#21709;&#25105;&#20204;&#22914;&#20309;&#23637;&#31034;&#33258;&#24049;&#21644;&#20182;&#20154;&#65292;&#20174;&#32780;&#24433;&#21709;&#25105;&#20204;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#34913;&#37327;&#21051;&#26495;&#21360;&#35937;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#65288;DSM&#65289;&#65288;&#22914;BERT&#65289;&#20013;&#23884;&#20837;&#30340;&#25237;&#24433;&#26469;&#36827;&#34892;&#36825;&#20123;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;DSMs&#25429;&#25417;&#21040;&#30340;&#35748;&#30693;&#32852;&#24819;&#19981;&#19968;&#23450;&#19982;&#21051;&#26495;&#21360;&#35937;&#30340;&#20154;&#38469;&#24615;&#36136;&#30456;&#20851;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20174;Twitter&#21644;Wikipedia&#20256;&#35760;&#20013;&#23398;&#20064;&#21051;&#26495;&#21360;&#35937;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#30701;&#35821;&#24212;&#29992;&#20110;&#21516;&#19968;&#20010;&#20154;&#30340;&#20107;&#23454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25193;&#22823;&#20102;&#23398;&#20064;&#32852;&#24819;&#30340;&#20154;&#26412;&#36523;&#20013;&#24515;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#20204;&#22914;&#20309;&#23558;&#36523;&#20221;&#26631;&#31614;&#24212;&#29992;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#20197;&#21450;&#37327;&#21270;&#31361;&#20986;&#30340;&#31038;&#20250;&#32500;&#24230;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#23545;&#26410;&#26469;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stereotypes inform how we present ourselves and others, and in turn how we behave. They are thus important to measure. Recent work has used projections of embeddings from Distributional Semantic Models (DSMs), such as BERT, to perform these measurements. However, DSMs capture cognitive associations that are not necessarily relevant to the interpersonal nature of stereotyping. Here, we propose and evaluate three novel, entity-centric methods for learning stereotypes from Twitter and Wikipedia biographies. Models are trained by leveraging the fact that multiple phrases are applied to the same person, magnifying the person-centric nature of the learned associations. We show that these models outperform existing approaches to stereotype measurement with respect to 1) predicting which identities people apply to themselves and others, and 2) quantifying stereotypes on salient social dimensions (e.g. gender). Via a case study, we also show the utility of these models for future questions in c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;CLIP&#30340;&#32452;&#21512;&#24615;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2212.10537</link><description>&lt;p&gt;
CLIP&#26159;&#21542;&#25414;&#32465;&#27010;&#24565;&#65311;&#25506;&#32034;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does CLIP Bind Concepts? Probing Compositionality in Large Image Models. (arXiv:2212.10537v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;CLIP&#30340;&#32452;&#21512;&#24615;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#32534;&#30721;&#20102;&#23427;&#20204;&#25805;&#20316;&#30340;&#27010;&#24565;&#30340;&#32452;&#25104;&#24615;&#34920;&#31034;&#65292;&#22914;&#36890;&#36807;&#23545;&#8220;&#32418;&#33394;&#31435;&#26041;&#20307;&#8221;&#36827;&#34892;&#25512;&#29702;&#20197;&#27491;&#30830;&#35782;&#21035;&#8220;&#32418;&#33394;&#8221;&#21644;&#8220;&#31435;&#26041;&#20307;&#8221;&#36825;&#20123;&#25104;&#20998;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#32534;&#30721;&#32452;&#21512;&#27010;&#24565;&#30340;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#21306;&#20998;&#8220;&#31435;&#26041;&#20307;&#22312;&#29699;&#20307;&#21518;&#38754;&#8221;&#21644;&#8220;&#29699;&#20307;&#22312;&#31435;&#26041;&#20307;&#21518;&#38754;&#8221;&#65289;&#12290;&#20026;&#20102;&#26816;&#26597;CLIP&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#35768;&#22810;&#26469;&#33258;&#32452;&#21512;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#65288;CDSMs&#65289;&#30340;&#26550;&#26500;&#65292;&#36825;&#26159;&#19968;&#31181;&#35797;&#22270;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23454;&#29616;&#20256;&#32479;&#32452;&#21512;&#35821;&#35328;&#32467;&#26500;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#21457;&#29616;CLIP&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20984;&#26174;&#20102;&#35780;&#20272;&#22823;&#22411;&#27169;&#22411;&#32452;&#21512;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ''red cube'' by reasoning over the constituents ''red'' and ''cube''. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ''cube behind sphere'' from ''sphere behind cube''). In order to inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance
&lt;/p&gt;</description></item><item><title>MelHuBERT&#26159;&#22522;&#20110;Mel&#39057;&#35889;&#22270;&#30340;&#31616;&#21270;&#29256;HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#34920;&#31034;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#34920;&#29616;&#65292;&#33410;&#30465;&#20102;31.2%&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#21644;33.5%&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2211.09944</link><description>&lt;p&gt;
MelHuBERT: &#19968;&#31181;&#22522;&#20110;Mel&#39057;&#35889;&#22270;&#30340;&#31616;&#21270;HuBERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MelHuBERT: A simplified HuBERT on Mel spectrograms. (arXiv:2211.09944v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09944
&lt;/p&gt;
&lt;p&gt;
MelHuBERT&#26159;&#22522;&#20110;Mel&#39057;&#35889;&#22270;&#30340;&#31616;&#21270;&#29256;HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#34920;&#31034;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#34920;&#29616;&#65292;&#33410;&#30465;&#20102;31.2%&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#21644;33.5%&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#22810;&#20010;GPU&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20005;&#37325;&#38480;&#21046;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#20943;&#23569;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;HuBERT&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#25104;&#21151;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#25913;&#36827;&#24182;&#31616;&#21270;&#20102;&#20960;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#34920;&#31034;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MelHuBERT&#22312;&#38899;&#32032;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#22343;&#33021;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#33410;&#30465;&#20102;31.2%&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#65292;&#25110;&#31561;&#25928;&#22320;&#27599;&#31186;&#35821;&#38899;&#33410;&#30465;&#20102;33.5%&#30340;MACs&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/nervjack2/MelHuBERT&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.
&lt;/p&gt;</description></item></channel></rss>