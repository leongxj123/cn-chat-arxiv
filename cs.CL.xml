<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#26367;&#25442;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#20026;&#21516;&#20041;&#35789;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2404.01569</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#31181;&#23454;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models Using Contrast Sets: An Experimental Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01569
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#26367;&#25442;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#20026;&#21516;&#20041;&#35789;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;NLI&#65289;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#22810;&#20010;&#36755;&#20837;&#25991;&#26412;&#20998;&#31867;&#30340;&#20219;&#21153;&#20013;&#65292;&#20132;&#21449;&#29109;&#25439;&#22833;&#24230;&#37327;&#34987;&#24191;&#27867;&#24212;&#29992;&#20316;&#20026;&#38169;&#35823;&#24230;&#37327;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#35813;&#24230;&#37327;&#22312;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#29702;&#35299;&#35821;&#21477;&#34164;&#28085;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#28041;&#21450;&#33258;&#21160;&#23558;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#26367;&#25442;&#20026;&#23427;&#20204;&#30340;&#21516;&#20041;&#35789;&#65292;&#20197;&#20445;&#30041;&#21477;&#23376;&#30340;&#21407;&#22987;&#21547;&#20041;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;&#25105;&#20204;&#20351;&#29992;ELECTRA-small&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#35813;&#27169;&#22411;&#22312;&#20256;&#32479;&#30340;SNLI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;89.9%&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#23545;&#27604;&#38598;&#19978;&#26174;&#31034;&#20986;&#20102;72.5%&#30340;&#20934;&#30830;&#24230;&#65292;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01569v1 Announce Type: cross  Abstract: In the domain of Natural Language Inference (NLI), especially in tasks involving the classification of multiple input texts, the Cross-Entropy Loss metric is widely employed as a standard for error measurement. However, this metric falls short in effectively evaluating a model's capacity to understand language entailments. In this study, we introduce an innovative technique for generating a contrast set for the Stanford Natural Language Inference (SNLI) dataset. Our strategy involves the automated substitution of verbs, adverbs, and adjectives with their synonyms to preserve the original meaning of sentences. This method aims to assess whether a model's performance is based on genuine language comprehension or simply on pattern recognition. We conducted our analysis using the ELECTRA-small model. The model achieved an accuracy of 89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5% on our contrast set, indicati
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.19085</link><description>&lt;p&gt;
&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65306;&#26397;&#30528;&#21487;&#25511;&#22810;&#30446;&#26631;&#23545;&#40784;&#26041;&#21521;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19085
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23545;&#40784;&#24037;&#20316;&#26088;&#22312;&#36861;&#27714;&#27169;&#22411;&#21709;&#24212;&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#23545;&#40784;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#31526;&#21512;&#21508;&#31181;&#20559;&#22909;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19085v1 Announce Type: new  Abstract: Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the "alignment tax" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18659</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#28216;&#25103;&#65306;&#35843;&#30740;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Games: A Survey and Roadmap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#24613;&#21095;&#22686;&#21152;&#65292;&#24182;&#20276;&#38543;&#30528;&#20844;&#20247;&#23545;&#35813;&#20027;&#39064;&#30340;&#21442;&#19982;&#12290;&#23613;&#31649;&#36215;&#21021;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;LLMs&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#21253;&#25324;&#28216;&#25103;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21450;&#20026;&#28216;&#25103;&#25552;&#20379;&#25903;&#25345;&#30340;&#21508;&#31181;&#24212;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#26126;&#30830;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21487;&#20197;&#25198;&#28436;&#30340;&#19981;&#21516;&#35282;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23578;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#21644;LLMs&#22312;&#28216;&#25103;&#20013;&#26410;&#26469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;LLMs&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#20316;&#20026;LLMs&#21644;&#28216;&#25103;&#20132;&#21449;&#39046;&#22495;&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#21644;&#36335;&#32447;&#22270;&#65292;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#25104;&#20026;&#36825;&#19968;&#28608;&#21160;&#20154;&#24515;&#30340;&#26032;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#21644;&#21019;&#26032;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18659v1 Announce Type: cross  Abstract: Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#35266;&#24565;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#65292;&#24182;&#25581;&#31034;&#20102;&#20215;&#20540;&#23545;&#40784;&#33021;&#21147;&#21487;&#20197;&#34987;&#36328;&#35821;&#35328;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18120</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25506;&#32034;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#35266;&#24565;&#65306;&#20215;&#20540;&#35266;&#40784;&#25972;&#24615;&#12289;&#36328;&#35821;&#35328;&#36716;&#31227;&#24615;&#21644;&#21487;&#25511;&#24615;&#26159;&#21542;&#20855;&#26377;&#19968;&#33268;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18120
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#35266;&#24565;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#65292;&#24182;&#25581;&#31034;&#20102;&#20215;&#20540;&#23545;&#40784;&#33021;&#21147;&#21487;&#20197;&#34987;&#36328;&#35821;&#35328;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#22312;&#34920;&#31034;&#24037;&#31243;&#39046;&#22495;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#20854;&#34920;&#31034;&#31354;&#38388;&#20013;&#32534;&#30721;&#27010;&#24565;&#65292;&#20027;&#35201;&#22260;&#32469;&#33521;&#35821;&#23637;&#24320;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#29702;&#24565;&#25193;&#23637;&#21040;&#22810;&#35821;&#22659;&#22330;&#26223;&#65292;&#28145;&#20837;&#25506;&#35752;LLMs&#20013;&#30340;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#27010;&#24565;&#12290;&#36890;&#36807;&#25105;&#20204;&#23545;7&#31181;&#20154;&#31867;&#20215;&#20540;&#12289;16&#31181;&#35821;&#35328;&#20197;&#21450;3&#20010;&#20855;&#26377;&#26126;&#26174;&#22810;&#35821;&#29305;&#24615;&#30340;LLM&#31995;&#21015;&#36827;&#34892;&#30340;&#20840;&#38754;&#25506;&#32034;&#65292;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#35777;&#23454;&#20102;LLMs&#20013;&#23384;&#22312;&#22810;&#35821;&#35328;&#20154;&#31867;&#20215;&#20540;&#35266;&#24565;&#12290;&#23545;&#36825;&#20123;&#27010;&#24565;&#30340;&#36328;&#35821;&#35328;&#20998;&#26512;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#30001;&#20110;&#35821;&#35328;&#36164;&#28304;&#24046;&#24322;&#32780;&#20135;&#29983;&#30340;3&#20010;&#29305;&#24449;&#65306;&#36328;&#35821;&#35328;&#19981;&#19968;&#33268;&#24615;&#12289;&#25197;&#26354;&#30340;&#35821;&#35328;&#20851;&#31995;&#20197;&#21450;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#22312;&#20154;&#31867;&#20215;&#20540;&#27010;&#24565;&#26041;&#38754;&#30340;&#21333;&#21521;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36890;&#36807;&#21033;&#29992;&#20027;&#23548;&#35821;&#35328;&#20316;&#20026;&#20449;&#24687;&#28304;&#23454;&#29616;&#23545;LLMs&#20215;&#20540;&#35266;&#40784;&#25972;&#24615;&#30340;&#36328;&#35821;&#35328;&#25511;&#21046;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18120v1 Announce Type: new  Abstract: Prior research in representation engineering has revealed that LLMs encode concepts within their representation spaces, predominantly centered around English. In this study, we extend this philosophy to a multilingual scenario, delving into multilingual human value concepts in LLMs. Through our comprehensive exploration covering 7 types of human values, 16 languages and 3 LLM series with distinct multilinguality, we empirically substantiate the existence of multilingual human values in LLMs. Further cross-lingual analysis on these concepts discloses 3 traits arising from language resource disparities: cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of human value concepts. Additionally, we validate the feasibility of cross-lingual control over value alignment capabilities of LLMs, leveraging the dominant language as a source 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#30340;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#21512;&#25104;&#30417;&#30563;&#20449;&#21495;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16508</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#21512;&#25104;&#30417;&#30563;&#36827;&#34892;&#36328;&#35821;&#35328;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#30340;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#21512;&#25104;&#30417;&#30563;&#20449;&#21495;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#38382;&#31572;&#65288;CLQA&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20174;&#22810;&#35821;&#35328;&#30693;&#35782;&#24211;&#36827;&#34892;&#36328;&#35821;&#35328;&#26816;&#32034;&#65292;&#28982;&#21518;&#22312;&#33521;&#35821;&#25110;&#26597;&#35810;&#35821;&#35328;&#20013;&#29983;&#25104;&#31572;&#26696;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;CLQA&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38142;&#25509;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#26469;&#21512;&#25104;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#36890;&#36807;&#19968;&#31181;&#22635;&#31354;&#26597;&#35810;&#24418;&#24335;&#29983;&#25104;&#26356;&#33258;&#28982;&#30340;&#26597;&#35810;&#20197;&#30417;&#30563;&#31572;&#26696;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CLASS&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#38646;-shot&#24773;&#20917;&#19979;&#22343;&#20248;&#20110;&#21487;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16508v1 Announce Type: new  Abstract: Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation. Together, we show our approach, \texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot lan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25298;&#32477;&#22238;&#31572;&#24182;&#35299;&#37322;&#26410;&#30693;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.15062</link><description>&lt;p&gt;
&#21035;&#32781;&#33457;&#25307;&#65281;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#35843;&#25972;&#20197;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15062
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25298;&#32477;&#22238;&#31572;&#24182;&#35299;&#37322;&#26410;&#30693;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#22238;&#31572;&#38382;&#39064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#38382;&#39064;&#27809;&#26377;&#26126;&#30830;&#31572;&#26696;&#26102;&#24448;&#24448;&#34920;&#29616;&#20986;&#30456;&#24403;&#31243;&#24230;&#30340;&#33258;&#20449;&#36807;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#21521;&#36825;&#20123;&#26410;&#30693;&#38382;&#39064;&#25552;&#20379;&#34394;&#26500;&#31572;&#26696;&#65292;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#25506;&#35752;&#25298;&#32477;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#33258;&#25105;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#26412;&#36523;&#26469;&#22686;&#24378;&#20854;&#23545;&#19981;&#21516;&#31867;&#22411;&#26410;&#30693;&#38382;&#39064;&#30340;&#22238;&#24212;&#33021;&#21147;&#65292;&#19981;&#20165;&#33021;&#22815;&#25298;&#32477;&#22238;&#31572;&#65292;&#36824;&#33021;&#22815;&#35299;&#37322;&#26410;&#30693;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#21407;&#22240;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Self-Align&#26041;&#27861;&#39318;&#20808;&#37319;&#29992;&#20004;&#38454;&#27573;&#31867;&#24863;&#30693;&#33258;&#25105;&#22686;&#24378;&#26041;&#27861;&#29983;&#25104;&#22823;&#37327;&#26410;&#30693;&#38382;&#39064;-&#22238;&#24212;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#24046;&#24322;&#39537;&#21160;&#30340;&#33258;&#25105;&#25972;&#29702;&#65292;&#36873;&#25321;&#21512;&#26684;&#25968;&#25454;&#23545;LLM&#26412;&#36523;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35843;&#25972;&#23545;&#26410;&#30693;&#38382;&#39064;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15062v1 Announce Type: new  Abstract: Despite the remarkable abilities of Large Language Models (LLMs) to answer questions, they often display a considerable level of overconfidence even when the question does not have a definitive answer. To avoid providing hallucinated answers to these unknown questions, existing studies typically investigate approaches to refusing to answer these questions. In this work, we propose a novel and scalable self-alignment method to utilize the LLM itself to enhance its response-ability to different types of unknown questions, being capable of not only refusing to answer but also providing explanation to the unanswerability of unknown questions. Specifically, the Self-Align method first employ a two-stage class-aware self-augmentation approach to generate a large amount of unknown question-response data. Then we conduct disparity-driven self-curation to select qualified data for fine-tuning the LLM itself for aligning the responses to unknown q
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;LLMs&#22312;&#35848;&#21028;&#23545;&#35805;&#20013;&#30340;&#22810;&#26041;&#38754;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#35848;&#21028;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.13550</link><description>&lt;p&gt;
LLM&#20204;&#26159;&#26377;&#25928;&#30340;&#35848;&#21028;&#32773;&#21527;&#65311;&#23545;LLM&#22312;&#35848;&#21028;&#23545;&#35805;&#20013;&#22810;&#26041;&#38754;&#33021;&#21147;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;LLMs&#22312;&#35848;&#21028;&#23545;&#35805;&#20013;&#30340;&#22810;&#26041;&#38754;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#35848;&#21028;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27425;&#25104;&#21151;&#30340;&#35848;&#21028;&#38656;&#35201;&#23545;&#35848;&#35805;&#32972;&#26223;&#26377;&#28145;&#21051;&#29702;&#35299;&#65292;&#20855;&#22791;&#25512;&#26029;&#23545;&#26041;&#21160;&#26426;&#30340;&#24515;&#29702;&#29702;&#35770;&#25216;&#33021;&#65292;&#20197;&#21450;&#25112;&#30053;&#25512;&#29702;&#21644;&#26377;&#25928;&#27807;&#36890;&#65292;&#36825;&#20351;&#24471;&#33258;&#21160;&#21270;&#31995;&#32479;&#38754;&#20020;&#25361;&#25112;&#12290;&#37492;&#20110;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;LLMs&#22914;&#20309;&#25512;&#21160;&#35848;&#21028;&#30740;&#31350;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21253;&#25324;&#35774;&#35745;&#23545;&#35805;&#31995;&#32479;&#12289;&#25552;&#20379;&#25945;&#23398;&#21453;&#39304;&#21644;&#25193;&#22823;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20998;&#26512;LLMs&#22312;&#21508;&#31181;&#23545;&#35805;&#24773;&#26223;&#20013;&#30340;&#22810;&#26041;&#38754;&#33021;&#21147;&#65292;&#28085;&#30422;&#20856;&#22411;&#35848;&#21028;&#20114;&#21160;&#30340;&#25152;&#26377;&#26102;&#38388;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;GPT-4&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;LLMs&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#20173;&#28982;&#22256;&#38590;&#30340;&#32454;&#33410;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13550v1 Announce Type: cross  Abstract: A successful negotiation demands a deep comprehension of the conversation context, Theory-of-Mind (ToM) skills to infer the partner's motives, as well as strategic reasoning and effective communication, making it challenging for automated systems. Given the remarkable performance of LLMs across a variety of NLP tasks, in this work, we aim to understand how LLMs can advance different aspects of negotiation research, ranging from designing dialogue systems to providing pedagogical feedback and scaling up data collection practices. To this end, we devise a methodology to analyze the multifaceted capabilities of LLMs across diverse dialogue scenarios covering all the time stages of a typical negotiation interaction. Our analysis adds to the increasing evidence for the superiority of GPT-4 across various tasks while also providing insights into specific tasks that remain difficult for LLMs. For instance, the models correlate poorly with hum
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#24335;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#65292;&#21253;&#25324;&#35757;&#32451;&#27169;&#22411;&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#21644;&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.11176</link><description>&lt;p&gt;
KnowTuning&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
KnowTuning: Knowledge-aware Fine-tuning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#24335;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#65292;&#21253;&#25324;&#35757;&#32451;&#27169;&#22411;&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#21644;&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#36827;&#34892;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#34920;&#29616;&#20986;&#29983;&#25104;&#19981;&#23436;&#25972;&#12289;&#38750;&#20107;&#23454;&#24615;&#25110;&#19981;&#21512;&#36923;&#36753;&#30340;&#31572;&#26696;&#31561;&#38480;&#21046;&#12290;&#36825;&#20123;&#38480;&#21046;&#28304;&#20110;LLMs&#22312;&#26222;&#36890;&#24494;&#35843;&#26399;&#38388;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#65288;KnowTuning&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#21644;&#38544;&#24335;&#22320;&#25913;&#21892;LLMs&#30340;&#30693;&#35782;&#35748;&#35782;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24335;&#30693;&#35782;&#24863;&#30693;&#29983;&#25104;&#38454;&#27573;&#65292;&#35757;&#32451;LLMs&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38544;&#24335;&#30693;&#35782;&#24863;&#30693;&#27604;&#36739;&#38454;&#27573;&#65292;&#35757;&#32451;LLMs&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#21253;&#25324;&#23436;&#25972;&#24615;&#12289;&#20107;&#23454;&#24615;&#21644;&#36923;&#36753;&#24615;&#19977;&#20010;&#26041;&#38754;&#12290;&#23545;&#36890;&#29992;&#21644;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11176v1 Announce Type: cross  Abstract: Despite their success at many natural language processing (NLP) tasks, large language models (LLMs) still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to explicitly and implicitly improve the knowledge awareness of LLMs. We devise an explicit knowledge-aware generation stage to train LLMs to explicitly identify knowledge triples in answers. We also propose an implicit knowledge-aware comparison stage to train LLMs to implicitly distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#33258;&#21160;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19979;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#24040;&#22823;&#21464;&#21270;&#65292;&#19988;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2402.10770</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#22312;&#38754;&#21521;&#25351;&#20196;&#30340;LLM&#20013;&#26377;&#22810;&#21487;&#38752;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#33258;&#21160;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19979;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#24040;&#22823;&#21464;&#21270;&#65292;&#19988;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#25991;&#26412;&#37325;&#21472;&#21644;LLM&#21028;&#26029;&#30340;&#33258;&#21160;&#26041;&#27861;&#20316;&#20026;&#20154;&#24037;&#35780;&#20272;&#30340;&#25104;&#26412;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#21644;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20219;&#21153;&#31867;&#22411;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#21160;&#26041;&#27861;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#26174;&#33879;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;ROUGE-L&#24230;&#37327;&#22312;&#30701;&#31572;&#26696;&#33521;&#35821;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21028;&#26029;&#24378;&#30456;&#20851;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#19981;&#21487;&#38752;&#12290;&#20351;&#29992;GPT-4&#20316;&#20026;&#35780;&#20272;&#21592;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#22312;&#35201;&#27714;&#35780;&#20272;&#26102;&#21253;&#21547;&#21442;&#32771;&#31572;&#26696;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#35780;&#20272;&#36807;&#20110;&#20005;&#26684;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21487;&#20197;&#36817;&#20284;&#20154;&#31867;&#21028;&#26029;&#65292;&#20294;&#20854;&#20934;&#30830;&#24615;&#21487;&#33021;&#22240;&#20219;&#21153;&#31867;&#22411;&#21644;&#35780;&#20272;&#35774;&#32622;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10770v1 Announce Type: cross  Abstract: Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we study the reliability of such methods across a broad range of tasks and in a cross-lingual setting. In contrast to previous findings, we observe considerable variability in correlations between automatic methods and human evaluators when scores are differentiated by task type. Specifically, the widely-used ROUGE-L metric strongly correlates with human judgments for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of GPT-4 as an evaluator depends on including reference answers when prompting for assessments, which can lead to overly strict evaluations in free-form generation tasks. In summary, we find that, while automatic evaluation methods can approximate human judgements und
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38382;&#31572;&#26469;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09177</link><description>&lt;p&gt;
&#20511;&#21161;&#22810;&#36718;&#20132;&#20114;&#21033;&#29992;&#19978;&#19979;&#25991;&#36827;&#34892;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38382;&#31572;&#26469;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36234;&#29425;&#25915;&#20987;&#36890;&#36807;&#24494;&#22937;&#22320;&#20462;&#25913;&#25915;&#20987;&#26597;&#35810;&#26469;&#25552;&#21462;&#26377;&#23475;&#20449;&#24687;&#12290;&#38543;&#30528;&#38450;&#24481;&#26426;&#21046;&#30340;&#36827;&#21270;&#65292;&#36234;&#29425;&#25915;&#20987;&#30452;&#25509;&#33719;&#21462;&#26377;&#23475;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#20154;&#31867;&#38388;&#25509;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#30340;&#23454;&#36341;&#21551;&#21457;&#65292;&#38024;&#23545;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;LLMs&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#33258;&#22238;&#24402;&#24615;&#36136;&#12290;&#25105;&#20204;&#35748;&#20026;&#20808;&#21069;&#30340;&#19978;&#19979;&#25991;&#8212;&#8212;&#25915;&#20987;&#26597;&#35810;&#20043;&#21069;&#30340;&#20449;&#24687;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#36234;&#29425;&#25915;&#20987;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21021;&#27493;&#38382;&#31572;&#23545;&#19982;LLMs&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#30340;&#22238;&#31572;&#26397;&#30528;&#25581;&#31034;&#8220;&#26399;&#26395;&#30340;&#8221;&#26377;&#23475;&#20449;&#24687;&#30340;&#26041;&#21521;&#21457;&#23637;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09177v1 Announce Type: cross Abstract: Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query. As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks. In this work, inspired by human practices of indirect context to elicit harmful information, we focus on a new attack form called Contextual Interaction Attack. The idea relies on the autoregressive nature of the generation process in LLMs. We contend that the prior context--the information preceding the attack query--plays a pivotal role in enabling potent Jailbreaking attacks. Specifically, we propose an approach that leverages preliminary question-answer pairs to interact with the LLM. By doing so, we guide the responses of the model toward revealing the 'desired' harmful information. We conduct experiments on four different LLMs and demonstrate the efficacy of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.04437</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Structured Entity Extraction Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#24433;&#21709;&#20102;&#20449;&#24687;&#25552;&#21462;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#21644;&#35268;&#33539;&#21270;&#20102;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#65288;SEE&#65289;&#20219;&#21153;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#36817;&#20284;&#23454;&#20307;&#38598;&#37325;&#21472;&#65288;AESOP&#65289;&#24230;&#37327;&#65292;&#20197;&#36866;&#24403;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#24182;&#34892;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#39046;&#22495;&#30340;&#26410;&#26469;&#36827;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35843;&#26597;&#20102;&#21253;&#21547;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#20027;&#24352;&#30340;NLP&#30740;&#31350;&#65292;&#21457;&#29616;&#19981;&#21516;&#35770;&#25991;&#23545;&#20110;&#36825;&#19968;&#27010;&#24565;&#30340;&#23450;&#20041;&#21644;&#26631;&#20934;&#21508;&#19981;&#30456;&#21516;&#65292;&#24341;&#20837;&#20102;&#22810;&#20010;&#32500;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#35821;&#35328;&#36873;&#25321;&#23384;&#22312;&#30340;&#20559;&#21521;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.04222</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is 'Typological Diversity' in NLP?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35843;&#26597;&#20102;&#21253;&#21547;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#20027;&#24352;&#30340;NLP&#30740;&#31350;&#65292;&#21457;&#29616;&#19981;&#21516;&#35770;&#25991;&#23545;&#20110;&#36825;&#19968;&#27010;&#24565;&#30340;&#23450;&#20041;&#21644;&#26631;&#20934;&#21508;&#19981;&#30456;&#21516;&#65292;&#24341;&#20837;&#20102;&#22810;&#20010;&#32500;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#35821;&#35328;&#36873;&#25321;&#23384;&#22312;&#30340;&#20559;&#21521;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#30740;&#31350;&#30028;&#23545;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#25237;&#20837;&#26356;&#22810;&#20851;&#27880;&#65292;&#20174;&#32780;&#22312;&#22810;&#35821;&#35328;NLP&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25913;&#36827;&#21482;&#36866;&#29992;&#20110;&#19990;&#30028;&#35821;&#35328;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#20026;&#20102;&#25193;&#23637;&#36825;&#19968;&#33539;&#22260;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35770;&#25991;&#33268;&#21147;&#20110;&#25552;&#39640;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#35821;&#35328;&#31867;&#22411;&#23398;&#24120;&#34987;&#29992;&#26469;&#36873;&#25321;&#35821;&#35328;&#65292;&#22522;&#20110;&#24191;&#27867;&#30340;&#35821;&#35328;&#31867;&#22411;&#26679;&#26412;&#24212;&#33021;&#24102;&#26469;&#23545;&#22810;&#31181;&#35821;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#20123;&#36873;&#25321;&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#21253;&#21547;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#20027;&#24352;&#30340;NLP&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#30830;&#20999;&#30340;&#23450;&#20041;&#25110;&#26631;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#32500;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#36817;&#20284;&#35821;&#35328;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#21457;&#29616;&#32467;&#26524;&#22312;&#19981;&#21516;&#35770;&#25991;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#35328;&#36873;&#25321;&#30340;&#20559;&#21521;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP. However, these improvements only apply to a small subset of the world's languages. Aiming to extend this, an increasing number of papers aspires to enhance generalizable multilingual performance across languages. To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages. These selections are often described as being 'typologically diverse'. In this work, we systematically investigate NLP research that includes claims regarding 'typological diversity'. We find there are no set definitions or criteria for such claims. We introduce metrics to approximate the diversity of language selection along several axes and find that the results vary considerably across papers. Furthermore, we show that skewed language sele
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Leeroo&#32534;&#25490;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#32534;&#25490;&#22120;&#22312;&#24615;&#33021;&#19978;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#25104;&#26412;&#21482;&#26377;&#20854;&#19977;&#20998;&#20043;&#20108;&#12290;&#24403;&#20801;&#35768;&#26356;&#39640;&#30340;&#25104;&#26412;&#26102;&#65292;Leeroo&#32534;&#25490;&#22120;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;Mixtral&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#38598;&#25104;GPT4&#26102;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13979</link><description>&lt;p&gt;
Leeroo Orchestrator: &#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration. (arXiv:2401.13979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Leeroo&#32534;&#25490;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#32534;&#25490;&#22120;&#22312;&#24615;&#33021;&#19978;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#25104;&#26412;&#21482;&#26377;&#20854;&#19977;&#20998;&#20043;&#20108;&#12290;&#24403;&#20801;&#35768;&#26356;&#39640;&#30340;&#25104;&#26412;&#26102;&#65292;Leeroo&#32534;&#25490;&#22120;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;Mixtral&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#38598;&#25104;GPT4&#26102;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#30340;&#38598;&#20307;&#30693;&#35782;&#65292;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32534;&#25490;&#22120;&#65292;&#33021;&#22815;&#36873;&#25321;&#26368;&#20339;&#30340;&#24213;&#23618;LLM&#19987;&#23478;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#12290;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#25105;&#23545;&#24328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26597;&#35810;&#29983;&#25104;&#12289;&#32534;&#25490;&#21644;&#35780;&#20272;&#30340;&#24490;&#29615;&#65292;&#20026;&#32534;&#25490;&#22120;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20027;&#35201;&#38024;&#23545;MMLU&#22522;&#20934;&#65292;&#22312;Hugging Face&#19978;&#20351;&#29992;&#20102;&#20855;&#26377;7B&#12289;13B&#21644;34B&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;Leeroo&#32534;&#25490;&#22120;&#23454;&#29616;&#20102;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#21482;&#20135;&#29983;&#20102;&#20854;&#25104;&#26412;&#30340;&#19977;&#20998;&#20043;&#20108;&#12290;&#27492;&#22806;&#65292;&#22686;&#21152;&#20801;&#35768;&#30340;&#25104;&#26412;&#36229;&#36807;&#20102;Mixtral&#30340;&#20934;&#30830;&#24615;&#65292;&#36798;&#21040;&#20102;75.9%&#30340;&#20934;&#30830;&#24615;&#12290;&#24403;&#23558;GPT4&#38598;&#25104;&#21040;&#24213;&#23618;&#27169;&#22411;&#27744;&#20013;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20063;&#24471;&#21040;&#20102;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an architecture to harness the collective knowledge of multiple trained LLMs to create a new state-of-the-art. At the core of this framework is a LLM-based orchestrator that is adept at picking the right underlying LLM experts for optimal task execution. Inspired by self-play in reinforcement learning, we created a loop of query generation, orchestration, and evaluation to generate training data for the orchestrator. Our evaluation focused on the MMLU benchmark, employing models with 7B, 13B, and 34B parameters available on Hugging Face. The results demonstrate new state-of-the-art open-source models: Our Leeroo orchestrator achieves performance on par with the Mixtral model while incurring only two-thirds of its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by over 5% at the same cost level, reaching an accuracy of 75.9%. Further enhancements were observed when integrating GPT4 into the underlying model pool. The Leeroo orchestrator
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#65292;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#20851;&#31995;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#27867;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05967</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20998;&#22359;&#23545;&#35282;&#27491;&#20132;&#20851;&#31995;&#21644;&#30697;&#38453;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding. (arXiv:2401.05967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#65292;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#20851;&#31995;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#27867;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20302;&#32500;&#34920;&#31034;&#20197;&#39044;&#27979;&#32570;&#22833;&#30340;&#20107;&#23454;&#12290;&#26059;&#36716;-based&#26041;&#27861;&#22914;RotatE&#21644;QuatE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#26377;&#38480;&#65292;&#38656;&#35201;&#19982;&#23454;&#20307;&#32500;&#24230;&#25104;&#27604;&#20363;&#22320;&#22686;&#21152;&#20851;&#31995;&#22823;&#23567;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#26059;&#36716;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OrthogonalE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#19982;Riemannian&#20248;&#21270;&#34920;&#31034;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#26082;&#20855;&#26377;&#24191;&#27867;&#24615;&#21448;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#20851;&#31995;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary aim of Knowledge Graph embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts. While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations. To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations. This approach enhances the generality and flexibility of KGE models. The experimental results indicate that our new KGE model, OrthogonalE, is both general and flexible, significantly outperforming state-of-the-art KGE models while substantially reducing the number of relation parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11085</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25512;&#26029;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#35201;&#27714;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#25110;&#25512;&#26029;&#23427;&#20204;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22122;&#22768;&#65292;(2)&#23427;&#20204;&#38656;&#35201;&#20154;&#24037;&#23545;&#25991;&#26723;&#36827;&#34892;&#27880;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#30340;&#30740;&#31350;&#32773;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#22312;&#28040;&#38500;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#24615;&#30340;&#20248;&#21183;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;DocRED&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#24863;&#30693;&#30340;&#28145;&#20266;&#25991;&#26412;&#20316;&#32773;&#35782;&#21035;&#26041;&#27861;TopRoBERTa&#65292;&#36890;&#36807;&#25429;&#25417;&#28145;&#20266;&#25991;&#26412;&#20013;&#30340;&#26356;&#22810;&#35821;&#35328;&#27169;&#24335;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#20316;&#32773;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.12934</link><description>&lt;p&gt;
TopRoBERTa&#65306;&#25299;&#25169;&#24863;&#30693;&#30340;&#28145;&#20266;&#25991;&#26412;&#20316;&#32773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
TopRoBERTa: Topology-Aware Authorship Attribution of Deepfake Texts. (arXiv:2309.12934v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#24863;&#30693;&#30340;&#28145;&#20266;&#25991;&#26412;&#20316;&#32773;&#35782;&#21035;&#26041;&#27861;TopRoBERTa&#65292;&#36890;&#36807;&#25429;&#25417;&#28145;&#20266;&#25991;&#26412;&#20013;&#30340;&#26356;&#22810;&#35821;&#35328;&#27169;&#24335;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#20316;&#32773;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#24320;&#25918;&#24615;&#12289;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#20123;&#25991;&#26412;&#24456;&#38590;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#21306;&#20998;&#24320;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#31216;&#20026;&#8220;&#28145;&#20266;&#25991;&#26412;&#8221;&#12290;&#30446;&#21069;&#65292;huggingface&#27169;&#22411;&#23384;&#20648;&#24211;&#20013;&#26377;&#36229;&#36807;11K&#20010;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#24694;&#24847;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;&#36825;&#20123;&#24320;&#28304;&#30340;LLM&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#26377;&#23475;&#25991;&#26412;&#21644;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24076;&#26395;&#26377;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#33021;&#22815;&#30830;&#23450;&#32473;&#23450;&#30340;&#25991;&#26412;&#26159;&#21542;&#20026;&#28145;&#20266;&#25991;&#26412;&#65292;&#21363;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#65288;TT&#65289;&#26469;&#21028;&#26029;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26356;&#19968;&#33324;&#29256;&#26412;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#22810;&#31867;&#21035;&#35774;&#32622;&#19979;&#30340;&#8220;&#20316;&#32773;&#35782;&#21035;&#65288;AA&#65289;&#8221;&#65292;&#21363;&#19981;&#20165;&#30830;&#23450;&#32473;&#23450;&#30340;&#25991;&#26412;&#26159;&#21542;&#20026;&#28145;&#20266;&#25991;&#26412;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#30830;&#23450;&#21738;&#20010;LLM&#26159;&#20316;&#32773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TopRoBERTa&#65292;&#36890;&#36807;&#21253;&#21547;&#26356;&#22810;&#28145;&#20266;&#25991;&#26412;&#20013;&#30340;&#35821;&#35328;&#27169;&#24335;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;AA&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Large Language Models (LLMs) have enabled the generation of open-ended high-quality texts, that are non-trivial to distinguish from human-written texts. We refer to such LLM-generated texts as \emph{deepfake texts}. There are currently over 11K text generation models in the huggingface model repo. As such, users with malicious intent can easily use these open-sourced LLMs to generate harmful texts and misinformation at scale. To mitigate this problem, a computational method to determine if a given text is a deepfake text or not is desired--i.e., Turing Test (TT). In particular, in this work, we investigate the more general version of the problem, known as \emph{Authorship Attribution (AA)}, in a multi-class setting--i.e., not only determining if a given text is a deepfake text or not but also being able to pinpoint which LLM is the author. We propose \textbf{TopRoBERTa} to improve existing AA solutions by capturing more linguistic patterns in deepfake texts by includ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#38750;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#29992;&#20110;&#23545;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#20854;&#20855;&#26377;&#20301;&#32622;&#24863;&#30693;&#33258;&#35843;&#33410;&#21644;&#20381;&#36182;&#25509;&#21475;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#19982;&#20854;&#20182;&#20027;&#27969;&#27169;&#22411;&#30456;&#27604;&#26356;&#24555;&#30340;&#35299;&#30721;&#26102;&#38388;&#20869;&#33719;&#24471;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#28508;&#22312;&#25554;&#20540;&#31561;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03977</link><description>&lt;p&gt;
&#19968;&#31181;&#24102;&#26377;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#25991;&#26412;&#29983;&#25104;&#23545;&#25239;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Non-Autoregressive Model for Text Generation with Incomplete Information. (arXiv:2305.03977v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#38750;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#29992;&#20110;&#23545;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#20854;&#20855;&#26377;&#20301;&#32622;&#24863;&#30693;&#33258;&#35843;&#33410;&#21644;&#20381;&#36182;&#25509;&#21475;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#19982;&#20854;&#20182;&#20027;&#27969;&#27169;&#22411;&#30456;&#27604;&#26356;&#24555;&#30340;&#35299;&#30721;&#26102;&#38388;&#20869;&#33719;&#24471;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#28508;&#22312;&#25554;&#20540;&#31561;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#23436;&#25972;&#20449;&#24687;&#24773;&#20917;&#65288;CIS&#65289;&#19979;&#24050;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#27169;&#22411;&#20855;&#26377;&#23436;&#25972;&#30340;&#36755;&#20837;&#20449;&#24687;&#26469;&#33719;&#21462;&#30456;&#24212;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19981;&#23436;&#25972;&#20449;&#24687;&#24773;&#20917;&#65288;IIS&#65289;&#19979;&#30340;&#25506;&#32034;&#26497;&#20026;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;IIS&#20013;&#19981;&#23436;&#25972;&#30340;&#36755;&#20837;&#20449;&#24687;&#23558;&#22686;&#21152;&#22312;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#19979;&#35757;&#32451;&#30340;&#29616;&#26377;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;IIS&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#38750;&#33258;&#22238;&#24402;Transformer &#65288;ANT&#65289;&#27169;&#22411;&#65292;&#20855;&#26377;&#20004;&#20010;&#26032;&#29305;&#24615;&#65306;1&#65289;&#20301;&#32622;&#24863;&#30693;&#33258;&#35843;&#33410;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#21512;&#29702;&#30340;&#38544;&#34255;&#34920;&#31034;&#65307;2&#65289;&#20381;&#36182;&#24615;&#21069;&#39304;&#32593;&#32476;&#65292;&#21487;&#20197;&#22686;&#24378;&#20854;&#20381;&#36182;&#24615;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;ANT&#19982;IIS&#20013;&#30340;&#20854;&#20182;&#20027;&#27969;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;ANT&#21487;&#20197;&#23454;&#29616;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#24555;&#22320;&#36827;&#34892;&#35299;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ANT&#22312;&#28508;&#22312;&#25554;&#20540;&#31561;&#21508;&#31181;&#24212;&#29992;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive models have been widely studied in the Complete Information Scenario (CIS), in which the models have complete input information to obtain corresponding output. However, their explorations in the Incomplete Information Scenario (IIS) are extremely limited. Our analyses reveal that the IIS's incomplete input information will augment the inherent limitations of existing non-autoregressive models trained under Maximum Likelihood Estimation. In this paper, we propose for the IIS an Adversarial Non-autoregressive Transformer (ANT) which has two novel features: 1) Position Aware Self-Modulation to provide more reasonable hidden representations, and 2) Dependency Feed Forward Network to strengthen its capacity in dependency modeling. We compare ANT with other mainstream models in the IIS and demonstrate that ANT can achieve comparable performance with much fewer decoding iterations. Furthermore, we show its great potential in various applications like latent interpolation an
&lt;/p&gt;</description></item></channel></rss>