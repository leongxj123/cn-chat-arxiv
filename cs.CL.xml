<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01109</link><description>&lt;p&gt;
&#30123;&#33495;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Vaccine: Perturbation-aware Alignment for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01109
&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#21363;&#26381;&#21153;&#33539; paradigm&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#19968;&#23567;&#37096;&#20998;&#26377;&#23475;&#25968;&#25454;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23481;&#26131;&#27450;&#39575;&#24494;&#35843;&#36807;&#31243;&#20174;&#32780;&#20135;&#29983;&#23545;&#40784;&#22833;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#33021;&#23548;&#33268;&#23545;&#40784;&#22833;&#25928;&#30340;&#26377;&#23475;&#23884;&#20837;&#28418;&#31227;&#29616;&#35937;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30123;&#33495; (Vaccine) &#65292;&#19968;&#31181;&#38024;&#23545;&#24178;&#25200;&#24863;&#30693;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#29992;&#25143;&#24494;&#35843;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30123;&#33495;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#23545;&#40784;&#38454;&#27573;&#36880;&#28176;&#28155;&#21152;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#65292;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#20174;&#32780;&#20351;&#23884;&#20837;&#33021;&#22815;&#25269;&#24481;&#26469;&#33258;&#26410;&#32463;&#28040;&#27602;&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;&#20027;&#27969;LLM&#65288;&#22914;Llama2&#65292;Opt&#65292;Vicuna&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30123;&#33495;&#33021;&#22815;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.15371</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#32034;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models explore in-context?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15371
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36827;&#34892;&#25506;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#25105;&#20204;&#20851;&#27880;&#29616;&#26377;LLMs&#30340;&#21407;&#29983;&#24615;&#33021;&#65292;&#27809;&#26377;&#36827;&#34892;&#35757;&#32451;&#24178;&#39044;&#12290;&#25105;&#20204;&#23558;LLMs&#37096;&#32626;&#20026;&#31616;&#21333;&#22810;&#33218;&#32769;&#34382;&#26426;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#24182;&#23436;&#20840;&#22312;&#19978;&#19979;&#25991;&#20013;&#25351;&#23450;&#29615;&#22659;&#25551;&#36848;&#21644;&#20132;&#20114;&#21382;&#21490;&#65292;&#21363;&#22312;LLM&#25552;&#31034;&#20869;&#37096;&#36827;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#23545;GPT-3.5&#12289;GPT-4&#21644;Llama2&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24182;&#27809;&#26377;&#31283;&#20581;&#22320;&#36827;&#34892;&#25506;&#32034;&#65306;i&#65289;&#22312;&#25105;&#20204;&#30340;&#25152;&#26377;&#23454;&#39564;&#20013;&#65292;&#21482;&#26377;&#19968;&#20010;&#37197;&#32622;&#23548;&#33268;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#65306;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#22806;&#37096;&#24635;&#32467;&#30340;&#20132;&#20114;&#21382;&#21490;&#30340;GPT-4&#65292;&#36825;&#20123;&#34987;&#21576;&#29616;&#20026;&#20805;&#20998;&#32479;&#35745;&#30340;&#24773;&#20917;&#65307;ii&#65289;&#25152;&#26377;&#20854;&#20182;&#37197;&#32622;&#37117;&#27809;&#26377;&#20135;&#29983;&#31283;&#20581;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#21253;&#25324;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#20854;&#20182;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15371v1 Announce Type: cross  Abstract: We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#31034;&#20363;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#20316;&#32773;&#39564;&#35777;&#20219;&#21153;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.11265</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#21892;&#20316;&#32773;&#39564;&#35777;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11265
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#31034;&#20363;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#20316;&#32773;&#39564;&#35777;&#20219;&#21153;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#39564;&#35777;&#65288;AV&#65289;&#26159;&#19968;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#20851;&#27880;&#30340;&#26159;&#25512;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#30001;&#19968;&#20010;&#29305;&#23450;&#20316;&#32773;&#25776;&#20889;&#36824;&#26159;&#30001;&#20854;&#20182;&#20154;&#25776;&#20889;&#12290;&#24050;&#32463;&#26174;&#31034;&#35768;&#22810;AV&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#24694;&#24847;&#20316;&#32773;&#31215;&#26497;&#23581;&#35797;&#27450;&#39575;&#20998;&#31867;&#22120;&#65292;&#26041;&#27861;&#26159;&#38544;&#34255;&#20182;&#20204;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#25110;&#32773;&#27169;&#20223;&#21478;&#19968;&#20301;&#20316;&#32773;&#30340;&#39118;&#26684;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#20998;&#31867;&#22120;&#35757;&#32451;&#38598;&#19982;&#65288;&#36127;&#38754;&#30340;&#65289;&#21512;&#25104;&#31034;&#20363;&#36827;&#34892;&#22686;&#24378;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#36825;&#20123;&#21512;&#25104;&#31034;&#20363;&#26159;&#20026;&#20102;&#27169;&#20223;&#24863;&#20852;&#36259;&#30340;&#20316;&#32773;&#30340;&#39118;&#26684;&#32780;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#22686;&#24378;&#23545;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#30340;AV&#20219;&#21153;&#20013;&#24102;&#26469;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#25913;&#36827;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65288;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#23567;&#35268;&#27169;transformers&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#27969;&#34892;&#30340;GPT&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11265v1 Announce Type: cross  Abstract: Authorship Verification (AV) is a text classification task concerned with inferring whether a candidate text has been written by one specific author or by someone else. It has been shown that many AV systems are vulnerable to adversarial attacks, where a malicious author actively tries to fool the classifier by either concealing their writing style, or by imitating the style of another author. In this paper, we investigate the potential benefits of augmenting the classifier training set with (negative) synthetic examples. These synthetic examples are generated to imitate the style of the author of interest. We analyze the improvements in classifier prediction that this augmentation brings to bear in the task of AV in an adversarial setting. In particular, we experiment with three different generator architectures (one based on Recurrent Neural Networks, another based on small-scale transformers, and another based on the popular GPT mod
&lt;/p&gt;</description></item><item><title>KEBench&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#26032;&#22686;&#21152;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#31227;&#26893;&#24615;&#65289;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.07350</link><description>&lt;p&gt;
KEBench: &#29992;&#20110;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07350
&lt;/p&gt;
&lt;p&gt;
KEBench&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#26032;&#22686;&#21152;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#31227;&#26893;&#24615;&#65289;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07350v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#30446;&#21069;&#65292;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#30693;&#35782;&#32534;&#36753;&#30740;&#31350;&#24456;&#23569;&#12290;&#32534;&#36753;LVLMs&#38754;&#20020;&#30528;&#26377;&#25928;&#25972;&#21512;&#22810;&#31181;&#27169;&#24577;&#65288;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#30830;&#20445;&#20462;&#25913;&#36830;&#36143;&#19988;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#12290;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#38752;&#24615;&#12289;&#23616;&#37096;&#24615;&#21644;&#19968;&#33324;&#24615;&#65289;&#29992;&#20110;&#34913;&#37327;LVLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#22312;&#35780;&#20272;&#20013;&#20351;&#29992;&#30340;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#19981;&#36275;&#65292;&#24182;&#19988;&#26080;&#27861;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#22320;&#21033;&#29992;&#19982;&#30456;&#20851;&#20869;&#23481;&#30456;&#20851;&#30340;&#32534;&#36753;&#30693;&#35782;&#12290;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;$\textbf{KEBench}$&#65292;&#24182;&#25193;&#23637;&#20102;&#26032;&#24230;&#37327;&#26631;&#20934;(&#21487;&#31227;&#26893;&#24615;)&#20197;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#20511;&#21161;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65292;&#25105;&#20204;&#30340;&#22270;&#20687;&#25968;&#25454;&#21576;&#29616;&#20986;&#26126;&#30830;&#30340;&#32473;&#23454;&#20307;&#26041;&#21521;&#24615;&#12290;&#36825;&#31181;&#26041;&#21521;&#24615;&#21487;&#20197;&#36827;&#19968;&#27493;&#29992;&#20110;&#25552;&#21462;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#30693;&#35782;&#21644;&#36827;&#34892;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07350v1 Announce Type: cross  Abstract: Currently, little research has been done on knowledge editing for Large Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications. An existing benchmark has three metrics (Reliability, Locality and Generality) to measure knowledge editing for LVLMs. However, the benchmark falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited knowledge in relation to the associated content. We adopt different data collection methods to construct a new benchmark, $\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation. Leveraging a multimodal knowledge graph, our image data exhibits clear directionality towards entities. This directional aspect can be further utilized to extract entity-related knowledge and form editing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DIVERSE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;173,000&#26465;YouTube&#35270;&#39057;&#35780;&#35770;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#21477;&#23376;&#20013;&#30340;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.03334</link><description>&lt;p&gt;
DIVERSE&#65306;&#36890;&#36807;&#35270;&#39057;&#35780;&#35770;&#24577;&#24230;&#20998;&#26512;&#35299;&#35835;&#20114;&#32852;&#32593;&#23545;&#32654;&#22269;&#20891;&#20107;&#30340;&#30475;&#27861;&#65292;&#19968;&#20010;&#29992;&#20110;&#31435;&#22330;&#20998;&#31867;&#30340;&#26032;&#39062;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DIVERSE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;173,000&#26465;YouTube&#35270;&#39057;&#35780;&#35770;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#21477;&#23376;&#20013;&#30340;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#31435;&#22330;&#26816;&#27979;&#26159;&#28041;&#21450;&#35782;&#21035;&#22312;&#26377;&#20105;&#35758;&#20027;&#39064;&#19978;&#25317;&#26377;&#30456;&#21453;&#35266;&#28857;&#30340;&#29992;&#25143;&#32676;&#32452;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#30123;&#33495;&#25509;&#31181;&#21644;&#20105;&#35770;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#31435;&#22330;&#25552;&#20379;&#20102;&#23545;&#23454;&#20307;&#31435;&#22330;&#30340;&#25351;&#31034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DIVERSE&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#23545;&#36229;&#36807;173,000&#20010;YouTube&#35270;&#39057;&#35780;&#35770;&#36827;&#34892;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#20110;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#12290;&#36825;&#20123;&#31435;&#22330;&#36890;&#36807;&#19968;&#31181;&#30001;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#36827;&#34892;&#26631;&#27880;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#21477;&#23376;&#20013;&#34164;&#21547;&#30340;&#35821;&#27668;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#65292;&#32780;&#38750;&#20351;&#29992;&#20154;&#31867;&#25163;&#21160;&#27880;&#37322;&#12290;&#36825;&#20123;&#24369;&#20449;&#21495;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#21644;&#35773;&#21050;&#30340;&#23384;&#22312;&#65292;&#29305;&#23450;&#20851;&#38190;&#35789;&#30340;&#23384;&#22312;&#65292;&#25991;&#26412;&#30340;&#24773;&#24863;&#20197;&#21450;&#20174;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#30340;&#31435;&#22330;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#35780;&#35770;&#34987;&#27880;&#37322;&#20043;&#21069;&#65292;&#36825;&#20123;&#24369;&#20449;&#21495;&#20351;&#29992;&#25968;&#25454;&#32534;&#31243;&#27169;&#22411;&#36827;&#34892; consol
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03334v1 Announce Type: cross  Abstract: Stance detection of social media text is a key component of downstream tasks involving the identification of groups of users with opposing opinions on contested topics such as vaccination and within arguments. In particular, stance provides an indication of an opinion towards an entity. This paper introduces DIVERSE, a dataset of over 173,000 YouTube video comments annotated for their stance towards videos of the U.S. military. The stance is annotated through a human-guided, machine-assisted labeling methodology that makes use of weak signals of tone within the sentence as supporting indicators, as opposed to using manual annotations by humans. These weak signals consist of the presence of hate speech and sarcasm, the presence of specific keywords, the sentiment of the text, and the stance inference from two Large Language Models. The weak signals are then consolidated using a data programming model before each comment is annotated wit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#31572;&#26696;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;DACO&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#28608;&#21457;&#26410;&#26469;&#23545;&#25968;&#25454;&#20998;&#26512;&#36825;&#19968;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.02528</link><description>&lt;p&gt;
DACO: &#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#23454;&#29616;&#24212;&#29992;&#39537;&#21160;&#21644;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#31572;&#26696;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;DACO&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#28608;&#21457;&#26410;&#26469;&#23545;&#25968;&#25454;&#20998;&#26512;&#36825;&#19968;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20998;&#26512;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#20998;&#26512;&#36807;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#28145;&#20837;&#30740;&#31350;&#21644;&#32467;&#35770;&#24615;&#35265;&#35299;&#65292;&#20840;&#38754;&#22238;&#31572;&#32473;&#23450;&#29992;&#25143;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#26597;&#35810;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#26032;&#30340;&#36164;&#28304;&#21644;&#22522;&#20934;&#65292;&#28608;&#21457;&#26410;&#26469;&#23545;&#36825;&#19968;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26410;&#20805;&#20998;&#25366;&#25496;&#30340;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21644;&#22810;&#36718;&#25552;&#31034;&#25216;&#26415;&#33258;&#21160;&#20135;&#29983;&#39640;&#36136;&#37327;&#31572;&#26696;&#27880;&#37322;&#65292;&#26500;&#24314;&#20102;DACO&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;440&#20010;&#26469;&#33258;&#30495;&#23454;&#22330;&#26223;&#30340;&#25968;&#25454;&#24211;&#65288;&#34920;&#26684;&#25968;&#25454;&#65289;&#65292;&#32422;2k&#20010;&#26597;&#35810;-&#31572;&#26696;&#23545;&#21487;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#24369;&#30417;&#30563;&#65292;&#20197;&#21450;&#19968;&#20010;&#20154;&#24037;&#31934;&#32454;&#35843;&#25972;&#30340;&#26631;&#27880;&#30340;&#32039;&#20945;&#20294;&#39640;&#36136;&#37327;&#27979;&#35797;&#38598;&#65292;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02528v1 Announce Type: cross  Abstract: Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique. We construct the DACO dataset, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. We train a 6B supervised fine-tuning (SFT) model on DACO datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#30693;&#35782;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#21442;&#32771;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.17010</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#22238;&#24518;&#21442;&#32771;&#20301;&#32622;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Recall Reference Location Like Humans?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#30693;&#35782;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#21442;&#32771;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23436;&#25104;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26102;&#65292;&#20154;&#31867;&#26377;&#26102;&#19981;&#20165;&#38656;&#35201;&#19968;&#20010;&#31572;&#26696;&#65292;&#36824;&#38656;&#35201;&#30456;&#24212;&#30340;&#21442;&#32771;&#27573;&#33853;&#20379;&#36741;&#21161;&#38405;&#35835;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#39069;&#22806;&#30340;&#26816;&#32034;&#27169;&#22411;&#33719;&#21462;&#39044;&#20998;&#27573;&#30340;&#25991;&#31456;&#22359;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#65292;&#29420;&#31435;&#20110;&#20219;&#20309;&#36215;&#22987;&#20301;&#32622;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#26131;&#34987;&#36951;&#24536;&#21442;&#32771;&#30340;&#24773;&#26223;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;LLM&#34987;&#25552;&#31034;&#22238;&#24518;&#25991;&#26723;&#26631;&#39064;&#26631;&#35782;&#31526;&#20197;&#33719;&#21462;&#31895;&#31890;&#24230;&#25991;&#26723;&#38598;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#33719;&#24471;&#30340;&#31895;&#31890;&#24230;&#25991;&#26723;&#38598;&#65292;&#23427;&#22238;&#24518;&#32454;&#31890;&#24230;&#27573;&#33853;&#12290;&#22312;&#20004;&#38454;&#27573;&#22238;&#24518;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32422;&#26463;&#35299;&#30721;&#26469;&#30830;&#20445;&#19981;&#29983;&#25104;&#23384;&#20648;&#25991;&#26723;&#20043;&#22806;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22686;&#21152;&#36895;&#24230;&#65292;&#25105;&#20204;&#21482;&#22238;&#24518;&#30701;&#21069;&#32512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17010v1 Announce Type: cross  Abstract: When completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#20013;&#30340;&#20215;&#20540;&#20559;&#22909;&#20272;&#35745;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#21442;&#19982;&#32773;&#20114;&#21160;&#35299;&#20915;&#20102;&#36873;&#25321;&#19982;&#21160;&#26426;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#24182;&#37325;&#28857;&#27604;&#36739;&#20102;&#20174;&#21160;&#26426;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#19982;&#20165;&#20174;&#36873;&#25321;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.16751</link><description>&lt;p&gt;
&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#20013;&#30340;&#20215;&#20540;&#20559;&#22909;&#20272;&#35745;&#21644;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#20013;&#30340;&#20215;&#20540;&#20559;&#22909;&#20272;&#35745;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#21442;&#19982;&#32773;&#20114;&#21160;&#35299;&#20915;&#20102;&#36873;&#25321;&#19982;&#21160;&#26426;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#24182;&#37325;&#28857;&#27604;&#36739;&#20102;&#20174;&#21160;&#26426;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#19982;&#20165;&#20174;&#36873;&#25321;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#20013;&#29702;&#35299;&#20844;&#27665;&#30340;&#20215;&#20540;&#35266;&#23545;&#20110;&#20197;&#20844;&#27665;&#20026;&#20013;&#24515;&#30340;&#25919;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#20010;&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#65292;&#22312;&#36825;&#20010;&#31995;&#32479;&#20013;&#65292;&#21442;&#19982;&#32773;&#20570;&#20986;&#36873;&#25321;&#24182;&#25552;&#20379;&#36873;&#25321;&#30340;&#21160;&#26426;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#36807;&#19982;&#20182;&#20204;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#20215;&#20540;&#20559;&#22909;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#21442;&#19982;&#32773;&#30340;&#36873;&#25321;&#21644;&#21160;&#26426;&#20043;&#38388;&#26816;&#27979;&#21040;&#20914;&#31361;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#20272;&#35745;&#20215;&#20540;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#36890;&#36807;&#19982;&#21442;&#19982;&#32773;&#20114;&#21160;&#26469;&#35299;&#20915;&#26816;&#27979;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23558;&#8220;&#29645;&#35270;&#26159;&#32463;&#36807;&#28145;&#24605;&#29087;&#34385;&#30340;&#26377;&#24847;&#20041;&#34892;&#20026;&#8221;&#36825;&#19968;&#21746;&#23398;&#31435;&#22330;&#25805;&#20316;&#21270;&#12290;&#20063;&#23601;&#26159;&#22914;&#26524;&#21442;&#19982;&#32773;&#30340;&#36873;&#25321;&#26159;&#22522;&#20110;&#23545;&#20215;&#20540;&#20559;&#22909;&#30340;&#28145;&#24605;&#29087;&#34385;&#65292;&#37027;&#20040;&#21487;&#20197;&#22312;&#21442;&#19982;&#32773;&#20026;&#36873;&#25321;&#25552;&#20379;&#30340;&#21160;&#26426;&#20013;&#35266;&#23519;&#21040;&#20215;&#20540;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#20248;&#20808;&#32771;&#34385;&#20174;&#21160;&#26426;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#32780;&#19981;&#26159;&#20165;&#20174;&#36873;&#25321;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#30340;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16751v1 Announce Type: cross  Abstract: Understanding citizens' values in participatory systems is crucial for citizen-centric policy-making. We envision a hybrid participatory system where participants make choices and provide motivations for those choices, and AI agents estimate their value preferences by interacting with them. We focus on situations where a conflict is detected between participants' choices and motivations, and propose methods for estimating value preferences while addressing detected inconsistencies by interacting with the participants. We operationalize the philosophical stance that "valuing is deliberatively consequential." That is, if a participant's choice is based on a deliberation of value preferences, the value preferences can be observed in the motivation the participant provides for the choice. Thus, we propose and compare value estimation methods that prioritize the values estimated from motivations over the values estimated from choices alone.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#22810;&#31181;&#39118;&#26684;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#22810;&#37325;&#22870;&#21169;&#65292;&#23454;&#29616;&#20102;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#21516;&#26102;&#25511;&#21046;&#22810;&#31181;&#39118;&#26684;&#12290;</title><link>https://arxiv.org/abs/2402.14146</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#22810;&#37325;&#22870;&#21169;&#21152;&#26435;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22810;&#26679;&#24335;&#21487;&#25511;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#22810;&#31181;&#39118;&#26684;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#22810;&#37325;&#22870;&#21169;&#65292;&#23454;&#29616;&#20102;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#21516;&#26102;&#25511;&#21046;&#22810;&#31181;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#26159;&#34920;&#36798;&#21508;&#31181;&#20449;&#24687;&#30340;&#25991;&#26412;&#20013;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#20154;&#38469;&#21160;&#24577;&#65288;&#20363;&#22914;&#27491;&#24335;&#24615;&#65289;&#21644;&#20316;&#32773;&#30340;&#24773;&#32490;&#25110;&#24577;&#24230;&#65288;&#20363;&#22914;&#21388;&#24694;&#65289;&#12290;&#20154;&#31867;&#32463;&#24120;&#21516;&#26102;&#37319;&#29992;&#22810;&#31181;&#39118;&#26684;&#12290;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#26126;&#30830;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#32534;&#32455;&#30446;&#26631;&#39118;&#26684;&#65306;&#20363;&#22914;&#65292;&#29983;&#25104;&#26082;&#28040;&#26497;&#21448;&#26080;&#27602;&#30340;&#25991;&#26412;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#23545;&#21333;&#19968;&#39118;&#26684;&#30340;&#25511;&#21046;&#29983;&#25104;&#65292;&#25110;&#32773;&#23545;&#39118;&#26684;&#21644;&#20854;&#20182;&#23646;&#24615;&#30340;&#25511;&#21046;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#25193;&#23637;&#21040;&#21516;&#26102;&#25511;&#21046;&#22810;&#31181;&#39118;&#26684;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#21463;&#25511;&#22810;&#26679;&#24335;&#29983;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#30340;&#22810;&#31181;&#39118;&#26684;&#22870;&#21169;&#30340;&#21508;&#31181;&#20844;&#24335;&#12290;&#36825;&#20123;&#22870;&#21169;&#20844;&#24335;&#21253;&#25324;&#26469;&#33258;&#37492;&#21035;&#22120;&#30340;&#26657;&#20934;&#36755;&#20986;&#20197;&#21450;&#36890;&#36807;&#37492;&#21035;&#22120;&#26799;&#24230;&#24133;&#24230;&#36827;&#34892;&#21160;&#24577;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14146v1 Announce Type: new  Abstract: Style is an integral component of text that expresses a diverse set of information, including interpersonal dynamics (e.g. formality) and the author's emotions or attitudes (e.g. disgust). Humans often employ multiple styles simultaneously. An open question is how large language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. Previous work investigates the controlled generation of a single style, or else controlled generation of a style and other attributes. In this paper, we expand this into controlling multiple styles simultaneously. Specifically, we investigate various formulations of multiple style rewards for a reinforcement learning (RL) approach to controlled multi-style generation. These reward formulations include calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. W
&lt;/p&gt;</description></item><item><title>KARL&#26159;&#19968;&#31181;&#22522;&#20110;DKT&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#21033;&#29992;&#26816;&#32034;&#21644;BERT&#23884;&#20837;&#26469;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23398;&#29983;&#35760;&#24518;&#39044;&#27979;&#65292;&#22312;AUC&#21644;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.12291</link><description>&lt;p&gt;
KARL: &#30693;&#35782;&#24863;&#30693;&#26816;&#32034;&#21644;&#34920;&#31034;&#24110;&#21161;&#23398;&#29983;&#20445;&#25345;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12291
&lt;/p&gt;
&lt;p&gt;
KARL&#26159;&#19968;&#31181;&#22522;&#20110;DKT&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#21033;&#29992;&#26816;&#32034;&#21644;BERT&#23884;&#20837;&#26469;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23398;&#29983;&#35760;&#24518;&#39044;&#27979;&#65292;&#22312;AUC&#21644;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Flashcard&#35843;&#24230;&#22120;&#26159;&#20381;&#36182;&#20110;&#23398;&#29983;&#27169;&#22411;&#26469;&#39044;&#27979;&#23398;&#29983;&#25484;&#25569;&#30340;&#21333;&#35789;&#21345;&#65292;&#24182;&#20351;&#29992;&#25945;&#23398;&#31574;&#30053;&#26681;&#25454;&#36825;&#20123;&#39044;&#27979;&#23433;&#25490;&#35789;&#21345;&#30340;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#23398;&#29983;&#27169;&#22411;&#20165;&#20351;&#29992;&#21333;&#35789;&#21345;&#32423;&#21035;&#30340;&#29305;&#24449;&#65292;&#27604;&#22914;&#23398;&#29983;&#30340;&#36807;&#21435;&#22238;&#31572;&#65292;&#24573;&#30053;&#20102;&#21333;&#35789;&#21345;&#20043;&#38388;&#30340;&#35821;&#20041;&#32852;&#31995;&#12290;&#28145;&#24230;&#30693;&#35782;&#36319;&#36394;&#65288;DKT&#65289;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#35821;&#20041;&#20851;&#31995;&#65292;&#20294;&#25928;&#29575;&#20302;&#19979;&#65292;&#32570;&#20047;&#20869;&#23481;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#65292;&#24182;&#38656;&#35201;&#31283;&#20581;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;KARL&#65292;&#36825;&#26159;&#21463;DKT&#21551;&#21457;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#21033;&#29992;&#26816;&#32034;&#21644;BERT&#23884;&#20837;&#20197;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23398;&#29983;&#35760;&#24518;&#39044;&#27979;&#12290;&#20026;&#20102;&#27979;&#35797;KARL&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#24191;&#27867;&#23398;&#20064;&#21382;&#21490;&#20851;&#20110;&#29712;&#20107;&#38382;&#39064;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;KARL&#22312;AUC&#21644;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25945;&#23398;&#31574;&#30053;&#65292;&#21033;&#29992;DKT&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#22312;&#32447;&#37096;&#32626;KARL&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12291v1 Announce Type: new  Abstract: Flashcard schedulers are tools that rely on 1) student models to predict the flashcards a student knows; and 2) teaching policies to schedule cards based on these predictions. Existing student models, however, only use flashcard-level features, like the student's past responses, ignoring the semantic ties of flashcards. Deep Knowledge Tracing (DKT) models can capture semantic relations with language models, but are inefficient, lack content-rich datasets for evaluation, and require robust teaching policies. To address these issues, we design KARL, a DKT-inspired student model that uses retrieval and BERT embeddings for efficient and accurate student recall predictions. To test KARL, we collect a new dataset of diverse study history on trivia questions. KARL bests existing student models in AUC and calibration error. Finally, we propose a novel teaching policy that exploits the predictive power of DKT models to deploy KARL online. Based o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35843;&#26597;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#38754;&#20020;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#33324;&#26694;&#26550;&#21644;&#19981;&#21516;&#24418;&#24335;&#30340;&#21518;&#38376;&#25915;&#20987;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.11208</link><description>&lt;p&gt;
&#35686;&#24789;&#24744;&#30340;&#20195;&#29702;&#20154;&#65281;&#35843;&#26597;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#21518;&#38376;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11208
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35843;&#26597;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#38754;&#20020;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#33324;&#26694;&#26550;&#21644;&#19981;&#21516;&#24418;&#24335;&#30340;&#21518;&#38376;&#25915;&#20987;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLM&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#65288;&#21253;&#25324;&#37329;&#34701;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#36141;&#29289;&#31561;&#65289;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#12290;&#22312;&#24212;&#29992;&#36807;&#31243;&#20013;&#30830;&#20445;LLM&#20195;&#29702;&#20154;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLM&#20195;&#29702;&#20154;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#24037;&#20316;&#39318;&#27425;&#25506;&#35752;&#20102;&#20856;&#22411;&#23433;&#20840;&#23041;&#32961;&#20043;&#19968;&#65292;&#21363;&#23545;LLM&#20195;&#29702;&#20154;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20154;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#28982;&#21518;&#23545;&#19981;&#21516;&#24418;&#24335;&#30340;&#20195;&#29702;&#20154;&#21518;&#38376;&#25915;&#20987;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20174;&#26368;&#32456;&#25915;&#20987;&#32467;&#26524;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36873;&#25321;&#25805;&#32437;&#26368;&#32456;&#36755;&#20986;&#20998;&#24067;&#65292;&#25110;&#32773;&#20165;&#22312;&#20013;&#38388;&#25512;&#29702;&#36807;&#31243;&#20013;&#24341;&#20837;&#24694;&#24847;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#32456;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#21069;&#19968;&#31867;&#21487;&#20197;&#20998;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11208v1 Announce Type: cross  Abstract: Leveraging the rapid development of Large Language Models LLMs, LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate reasoning process, while keeping the final output correct. Furthermore, the former category can be divided
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.10946</link><description>&lt;p&gt;
&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CultureLLM: Incorporating Cultural Differences into Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25253;&#36947;&#20559;&#21521;&#20110;&#26576;&#20123;&#25991;&#21270;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20027;&#35201;&#26469;&#33258;&#33521;&#35821;&#35821;&#26009;&#24211;&#12290;&#30001;&#20110;&#22810;&#35821;&#31181;&#25991;&#21270;&#25968;&#25454;&#36890;&#24120;&#36739;&#38590;&#25910;&#38598;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25110;&#29305;&#23450;&#25991;&#21270;&#30340;&#39044;&#35757;&#32451;&#26469;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#24573;&#35270;&#20102;&#20302;&#36164;&#28304;&#25991;&#21270;&#30340;&#30693;&#35782;&#32570;&#20047;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CultureLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;LLMs&#20013;&#12290;CultureLLM&#37319;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#35821;&#20041;&#31561;&#25928;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20165;&#20351;&#29992;&#26469;&#33258;WVS&#30340;50&#20010;&#31181;&#23376;&#26679;&#26412;&#21644;&#22686;&#24378;&#25968;&#25454;&#65292;&#25105;&#20204;&#23545;9&#31181;&#21253;&#25324;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#21270;&#29305;&#23450;LLMs&#21644;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;60&#20010;&#19982;&#25991;&#21270;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CultureLLM&#22312;&#22686;&#24378;LLM&#30340;&#25991;&#21270;&#29305;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10946v1 Announce Type: cross  Abstract: Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM signif
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07625</link><description>&lt;p&gt;
AutoMathText&#65306;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#12290;&#19982;&#20256;&#32479;&#30340;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#30417;&#30563;&#24494;&#35843;&#25110;&#35757;&#32451;&#36807;&#30340;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#39564;&#35777;&#22120;&#65292;&#33258;&#20027;&#35780;&#20272;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#20869;&#23481;&#65292;&#24182;&#21457;&#24067;&#20102;&#32463;&#36807;&#31574;&#21010;&#30340;&#24320;&#28304;AutoMathText&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;200GB&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;AutoMathText&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;7B&#21442;&#25968;&#30340;Mistral&#35821;&#35328;&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#32780;&#20196;&#29260;&#25968;&#37327;&#27604;&#20043;&#21069;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#24037;&#20316;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20934;&#30340;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26399;&#26395;&#65292;&#26088;&#22312;&#35299;&#20915;&#27169;&#22411;&#23545;&#19978;&#19979;&#25991;&#30340;&#20851;&#27880;&#19981;&#36275;&#12289;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#36739;&#20302;&#20197;&#21450;&#22238;&#31572;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23545;15&#20010;QA&#31995;&#32479;&#22312;5&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#26032;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2401.18001</link><description>&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#19978;&#19979;&#25991;&#20351;&#29992;&#30340;&#26399;&#26395;
&lt;/p&gt;
&lt;p&gt;
Desiderata for the Context Use of Question Answering Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26399;&#26395;&#65292;&#26088;&#22312;&#35299;&#20915;&#27169;&#22411;&#23545;&#19978;&#19979;&#25991;&#30340;&#20851;&#27880;&#19981;&#36275;&#12289;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#36739;&#20302;&#20197;&#21450;&#22238;&#31572;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23545;15&#20010;QA&#31995;&#32479;&#22312;5&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#26032;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#29616;&#26377;&#20808;&#36827;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#19968;&#31995;&#21015;&#20849;&#21516;&#38382;&#39064;&#65306;&#24403;&#19978;&#19979;&#25991;&#19982;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#20914;&#31361;&#26102;&#65292;&#32570;&#20047;&#23545;&#19978;&#19979;&#25991;&#30340;&#20851;&#27880;&#65292;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#24456;&#23567;&#65292;&#24182;&#19988;&#22238;&#31572;&#30340;&#19968;&#33268;&#24615;&#19981;&#36275;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#21333;&#29420;&#35299;&#20915;&#20854;&#20013;&#19968;&#20004;&#20010;&#38382;&#39064;&#19978;&#65292;&#36825;&#20351;&#24471;&#24456;&#38590;&#30475;&#21040;&#23427;&#20204;&#20043;&#38388;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#39318;&#20808;&#27010;&#36848;QA&#27169;&#22411;&#30340;&#19968;&#31995;&#21015; - &#20808;&#21069;&#35752;&#35770;&#36807;&#30340;&#21644;&#26032;&#30340; - &#26399;&#26395;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#30456;&#20851;&#30340;&#20998;&#26512;&#21644;&#26041;&#27861;&#35770;&#25991;&#65292;&#25552;&#20379;&#39046;&#22495;&#29616;&#29366;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#31532;&#20108;&#37096;&#20998;&#23637;&#31034;&#20102;&#23454;&#39564;&#65292;&#22312;5&#20010;&#25968;&#25454;&#38598;&#19978;&#21516;&#26102;&#25353;&#29031;&#25152;&#26377;&#26399;&#26395;&#35780;&#20272;&#20102;15&#20010;QA&#31995;&#32479;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#35768;&#22810;&#26032;&#30340;&#36235;&#21183;&#65292;&#21253;&#25324;&#65288;1&#65289;&#23545;&#22122;&#22768;&#36739;&#19981;&#25935;&#24863;&#30340;&#31995;&#32479;&#22312;&#25552;&#20379;&#26080;&#20851;&#19978;&#19979;&#25991;&#26102;&#19981;&#19968;&#23450;&#26356;&#19968;&#33268;&#22320;&#22238;&#31572;&#38382;&#39064;&#65307;&#65288;2&#65289;&#22823;&#22810;&#25968;&#23545;&#22122;&#22768;&#25935;&#24863;&#30340;&#31995;&#32479;...
&lt;/p&gt;
&lt;p&gt;
Prior work has uncovered a set of common problems in state-of-the-art context-based question answering (QA) systems: a lack of attention to the context when the latter conflicts with a model's parametric knowledge, little robustness to noise, and a lack of consistency with their answers. However, most prior work focus on one or two of those problems in isolation, which makes it difficult to see trends across them. We aim to close this gap, by first outlining a set of -- previously discussed as well as novel -- desiderata for QA models. We then survey relevant analysis and methods papers to provide an overview of the state of the field. The second part of our work presents experiments where we evaluate 15 QA systems on 5 datasets according to all desiderata at once. We find many novel trends, including (1) systems that are less susceptible to noise are not necessarily more consistent with their answers when given irrelevant context; (2) most systems that are more susceptible to noise ar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#36817;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#27169;&#22411;&#25512;&#26029;&#20986;&#20195;&#29702;&#21464;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.06687</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#36817;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Proximal Causal Inference With Text Data. (arXiv:2401.06687v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#36817;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#27169;&#22411;&#25512;&#26029;&#20986;&#20195;&#29702;&#21464;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#22240;&#26524;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#20542;&#21521;&#20110;&#21253;&#21547;&#37096;&#20998;&#25110;&#19981;&#23436;&#20840;&#27979;&#37327;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#20195;&#29702;&#26469;&#20943;&#36731;&#28151;&#28102;&#20559;&#24046;&#12290;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#20998;&#26512;&#20154;&#21592;&#22312;&#19968;&#37096;&#20998;&#23454;&#20363;&#30340;&#25991;&#26412;&#20013;&#20855;&#26377;&#26377;&#30417;&#30563;&#30340;&#28151;&#28102;&#21464;&#37327;&#26631;&#31614;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#25104;&#26412;&#65292;&#36825;&#31181;&#32422;&#26463;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#23558;&#22788;&#29702;&#21069;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#38646;&#26679;&#26412;&#27169;&#22411;&#20174;&#20998;&#21106;&#30340;&#20004;&#20010;&#37096;&#20998;&#25512;&#26029;&#20986;&#20004;&#20010;&#20195;&#29702;&#65292;&#24182;&#23558;&#36825;&#20123;&#20195;&#29702;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#30340;&#20195;&#29702;&#26041;&#27861;&#28385;&#36275;&#36817;&#37051; g-formula&#25152;&#38656;&#30340;&#35782;&#21035;&#26465;&#20214;&#65292;&#32780;&#20854;&#20182;&#30475;&#20284;&#21512;&#29702;&#30340;&#25552;&#35758;&#21017;&#19981;&#28385;&#36275;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-based causal methods attempt to mitigate confounding bias by including unstructured text data as proxies of confounding variables that are partially or imperfectly measured. These approaches assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is not always feasible due to data privacy or cost. Here, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that splits pre-treatment text data, infers two proxies from two zero-shot models on the separate splits, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. This combination of proximal causal inference and zero-sh
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#36825;&#19977;&#20010;&#32452;&#20214;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#26045;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#28508;&#22312;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2307.08925</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#20010;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08925
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#36825;&#19977;&#20010;&#32452;&#20214;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#26045;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#28508;&#22312;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#33719;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#65292;&#20294;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#24320;&#21457;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#21294;&#20047;&#20197;&#21450;&#23545;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#39033;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#20986;&#29616;&#20102;&#65292;&#23427;&#33021;&#22815;&#22312;&#20445;&#25345;&#20998;&#25955;&#25968;&#25454;&#30340;&#21516;&#26102;&#23454;&#29616;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;LLM&#30340;&#27010;&#24565;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#32852;&#37030;&#24335;LLM&#39044;&#35757;&#32451;&#12289;&#32852;&#37030;&#24335;LLM&#24494;&#35843;&#21644;&#32852;&#37030;&#24335;LLM&#25552;&#31034;&#24037;&#31243;&#12290;&#23545;&#20110;&#27599;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#30456;&#23545;&#20110;&#20256;&#32479;LLM&#35757;&#32451;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#24037;&#31243;&#31574;&#30053;&#26469;&#23454;&#26045;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#20998;&#26512;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#30830;&#23450;&#21487;&#33021;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles fac
&lt;/p&gt;</description></item></channel></rss>