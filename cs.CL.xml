<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20351;&#24471;&#22270;&#20687;&#26816;&#32034;&#21487;&#20197;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.19651</link><description>&lt;p&gt;
MagicLens&#65306;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#19982;&#24320;&#25918;&#24335;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20351;&#24471;&#22270;&#20687;&#26816;&#32034;&#21487;&#20197;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#26816;&#32034;&#65292;&#21363;&#26681;&#25454;&#21442;&#32771;&#22270;&#20687;&#26597;&#25214;&#25152;&#38656;&#22270;&#20687;&#65292;&#22266;&#26377;&#22320;&#21253;&#21547;&#38590;&#20197;&#20165;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#24230;&#37327;&#25429;&#25417;&#21040;&#30340;&#20016;&#23500;&#12289;&#22810;&#26041;&#38754;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20801;&#35768;&#29992;&#25143;&#26356;&#33258;&#30001;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#37027;&#20123;&#35270;&#35273;&#19978;&#30456;&#20284;&#21644;/&#25110;&#21487;&#20197;&#29992;&#19968;&#23567;&#32452;&#39044;&#23450;&#20041;&#20851;&#31995;&#26469;&#34920;&#24449;&#30340;&#22270;&#20687;&#23545;&#19978;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#35770;&#28857;&#26159;&#25991;&#26412;&#25351;&#20196;&#21487;&#20197;&#20351;&#22270;&#20687;&#26816;&#32034;&#33021;&#22815;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#12290;MagicLens&#24314;&#31435;&#22312;&#19968;&#20010;&#37325;&#35201;&#30340;&#26032;&#39062;&#35265;&#35299;&#19978;&#65306;&#33258;&#28982;&#21457;&#29983;&#22312;&#21516;&#19968;&#32593;&#39029;&#19978;&#30340;&#22270;&#20687;&#23545;&#21253;&#21547;&#30528;&#22823;&#37327;&#38544;&#24335;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#20869;&#37096;&#35270;&#22270;&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#32508;&#21512;&#25351;&#20196;&#23558;&#36825;&#20123;&#38544;&#24335;&#20851;&#31995;&#21464;&#20026;&#26174;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19651v1 Announce Type: cross  Abstract: Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent work leverages text instructions to allow users to more freely express their search intents. However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16512</link><description>&lt;p&gt;
LLMs&#26159;&#23569;&#26679;&#26412;&#24773;&#22659;&#20302;&#36164;&#28304;&#35821;&#35328;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMs Are Few-Shot In-Context Low-Resource Language Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#25903;&#25345;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#21033;&#29992;&#30701;&#26102;&#30340;&#24773;&#22659;&#20449;&#24687;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#36825;&#20026;&#32553;&#23567;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#25552;&#20379;&#20102;&#37325;&#35201;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#38598;&#20013;&#22312;&#30456;&#23545;&#39640;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#27604;&#22914;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;ICL&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#65288;X-ICL&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19981;&#20165;&#35780;&#20272;&#20102;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#21457;&#29616;&#20102;&#24773;&#22659;&#26631;&#31614;&#23545;&#40784;&#30340;&#32570;&#38519;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#26597;&#35810;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24635;&#32467;&#20102;&#23569;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16512v1 Announce Type: cross  Abstract: In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-cont
&lt;/p&gt;</description></item><item><title>&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#22312;&#30005;&#20449;&#39046;&#22495;&#23637;&#31034;&#20986;&#19982;&#22823;&#22411;&#23545;&#24212;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#25552;&#21319;&#20102;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04666</link><description>&lt;p&gt;
&#30005;&#20449;&#35821;&#35328;&#27169;&#22411;&#65306;&#23427;&#20204;&#24517;&#39035;&#24222;&#22823;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Telecom Language Models: Must They Be Large?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04666
&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#22312;&#30005;&#20449;&#39046;&#22495;&#23637;&#31034;&#20986;&#19982;&#22823;&#22411;&#23545;&#24212;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#25552;&#21319;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#37096;&#38376;&#23545;&#24222;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26085;&#30410;&#20851;&#27880;&#20984;&#26174;&#20102;&#23427;&#20204;&#22312;&#25913;&#21464;&#36816;&#33829;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#24448;&#24448;&#21463;&#21040;&#20854;&#24040;&#22823;&#20307;&#31215;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#21487;&#34892;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#20986;&#29616;&#20102;&#19968;&#25209;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#19982;&#20854;&#36739;&#22823;&#23545;&#24212;&#29289;&#30456;&#24403;&#65292;&#27604;&#22914;&#32534;&#30721;&#21644;&#24120;&#35782;&#25512;&#29702;&#12290;Phi-2&#26159;&#19968;&#31181;&#32039;&#20945;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#20307;&#29616;&#20102;&#36825;&#19968;&#31995;&#21015;&#39640;&#25928;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#28010;&#28526;&#12290;&#26412;&#25991;&#23545;Phi-2&#22312;&#30005;&#20449;&#39046;&#22495;&#20869;&#22312;&#26412;&#36136;&#19978;&#30340;&#29702;&#35299;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#37492;&#20110;&#35268;&#27169;&#30456;&#20851;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#65292;&#31934;&#24515;&#22686;&#24378;&#20102;Phi-2&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04666v1 Announce Type: new  Abstract: The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2's intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2's capabilities through a Retrieval-Augmented Generation approach, meticulously i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65292;&#20174;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#35282;&#24230;&#36830;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.19350</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65292;&#20174;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#35282;&#24230;&#36830;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21033;&#29992;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#21644;&#25512;&#26029;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#36339;QA&#26041;&#38754;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;PLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#24515;&#29702;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#65292;&#36755;&#20837;&#25991;&#27573;&#20013;&#30340;&#26174;&#24335;&#20449;&#24687;&#19982;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#26410;&#33021;&#20805;&#20998;&#20851;&#27880;&#20174;&#20154;&#31867;&#35748;&#30693;&#30740;&#31350;&#30340;&#35282;&#24230;&#38142;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#22522;&#20110;PLMs&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#65288;PEI&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#25552;&#31034;&#36830;&#25509;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#65292;&#19982;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#23545;&#40784;&#65292;&#29992;&#20110;&#22810;&#36339;QA&#12290;&#25105;&#20204;&#23558;&#36755;&#20837;&#25991;&#27573;&#35270;&#20026;&#26174;&#24335;&#30693;&#35782;&#65292;&#21033;&#29992;&#23427;&#20204;&#36890;&#36807;&#32479;&#19968;&#25552;&#31034;&#25512;&#23548;&#38544;&#24335;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19350v1 Announce Type: new  Abstract: Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to simulate human reasoning and inference processes, achieving proficient performance in multi-hop QA. However, a gap persists between PLMs' reasoning abilities and those of humans when tackling complex problems. Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading. Nevertheless, current research has given insufficient attention to linking input passages and PLMs' pre-training-based knowledge from the perspective of human cognition studies. In this study, we introduce a \textbf{P}rompting \textbf{E}xplicit and \textbf{I}mplicit knowledge (PEI) framework, which uses prompts to connect explicit and implicit knowledge, aligning with human reading process for multi-hop QA. We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified prompt reason
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#21644;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#26377;&#25928;&#20943;&#23569;&#24187;&#35273;&#30340;&#29983;&#25104;&#65292;&#24182;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.15422</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24544;&#23454;&#19988;&#39640;&#36136;&#37327;&#30340;&#30149;&#20154;&#24635;&#32467;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#21644;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#26377;&#25928;&#20943;&#23569;&#24187;&#35273;&#30340;&#29983;&#25104;&#65292;&#24182;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#32463;&#24120;&#38754;&#20020;&#38590;&#20197;&#29702;&#35299;&#20854;&#20303;&#38498;&#24773;&#20917;&#30340;&#22256;&#38590;&#65292;&#32780;&#21307;&#25252;&#20154;&#21592;&#36164;&#28304;&#26377;&#38480;&#20197;&#25552;&#20379;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#23545;&#29983;&#25104;&#24635;&#32467;&#30340;&#24544;&#23454;&#24615;&#21644;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#29992;&#20110;&#24187;&#35273;&#65292;&#35753;&#20004;&#20301;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#20102;100&#20010;&#30495;&#23454;&#24635;&#32467;&#21644;100&#20010;&#29983;&#25104;&#30340;&#24635;&#32467;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;Llama 2&#27599;&#20010;&#24635;&#32467;&#30340;&#24187;&#35273;&#20174;2.60&#38477;&#20302;&#21040;1.55&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;&#34429;&#28982;&#25928;&#26524;&#20173;&#28982;&#23384;&#22312;&#65292;&#20294;&#24403;&#20351;&#29992;&#20116;&#20010;&#20363;&#23376;&#25552;&#31034;GPT-4&#26102;&#65292;&#35813;&#25928;&#26524;&#35201;&#23567;&#24471;&#22810;&#65288;0.70&#38477;&#33267;0.40&#65289;&#12290;&#25105;&#20204;&#36824;&#23545;&#26080;&#24187;&#35273;&#21644;&#25913;&#36827;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#12290;&#21363;&#20351;&#22312;&#24187;&#35273;&#33258;&#30001;&#25968;&#25454;&#19979;&#65292;GPT-4&#20063;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15422v1 Announce Type: cross  Abstract: Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we develop a rigorous labeling protocol for hallucinations, and have two medical experts annotate 100 real-world summaries and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. Although the effect is still present, it is much smaller for GPT-4 when prompted with five examples (0.70 to 0.40). We also conduct a qualitative evaluation using hallucination-free and improved training data. GPT-4 shows very good results even in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MT-Bench-101&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#33021;&#21147;&#65292;&#26500;&#24314;&#20102;&#21253;&#21547;4208&#36718;&#23545;&#35805;&#25968;&#25454;&#30340;&#19977;&#32423;&#20998;&#23618;&#33021;&#21147;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#20102;21&#31181;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#19981;&#21516;&#23545;&#35805;&#36718;&#27425;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.14762</link><description>&lt;p&gt;
MT-Bench-101: &#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14762
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MT-Bench-101&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#33021;&#21147;&#65292;&#26500;&#24314;&#20102;&#21253;&#21547;4208&#36718;&#23545;&#35805;&#25968;&#25454;&#30340;&#19977;&#32423;&#20998;&#23618;&#33021;&#21147;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#20102;21&#31181;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#19981;&#21516;&#23545;&#35805;&#36718;&#27425;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22823;&#22823;&#22686;&#24378;&#20102;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#36718;&#23545;&#35805;&#25110;&#32773;&#25552;&#20379;&#31895;&#31890;&#24230;&#21644;&#19981;&#23436;&#25972;&#30340;&#22810;&#36718;&#23545;&#35805;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#30495;&#23454;&#23545;&#35805;&#30340;&#22797;&#26434;&#24615;&#21644;&#32454;&#24494;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MT-Bench-101&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#22810;&#36718;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;13&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;1388&#20010;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;4208&#36718;&#30340;&#19977;&#32423;&#20998;&#23618;&#33021;&#21147;&#20998;&#31867;&#12290;&#28982;&#21518;&#25105;&#20204;&#22522;&#20110;MT-Bench-101&#35780;&#20272;&#20102;21&#20010;&#27969;&#34892;&#30340;LLMs&#65292;&#20174;&#33021;&#21147;&#21644;&#20219;&#21153;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#35266;&#23519;&#21040;LLMs&#22312;&#23545;&#35805;&#36718;&#27425;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14762v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns with
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550; LlmCorr&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;LLM&#21487;&#20197;&#20316;&#20026;&#20107;&#21518;&#26657;&#27491;&#22120;&#65292;&#20026;&#20219;&#24847;ML&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20986;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.13414</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#20107;&#21518;&#26657;&#27491;&#22120;
&lt;/p&gt;
&lt;p&gt;
Harnessing Large Language Models as Post-hoc Correctors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13414
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550; LlmCorr&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;LLM&#21487;&#20197;&#20316;&#20026;&#20107;&#21518;&#26657;&#27491;&#22120;&#65292;&#20026;&#20219;&#24847;ML&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20986;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#38271;&#24182;&#38656;&#27714;&#26356;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#19982;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21644;&#24494;&#35843;&#30456;&#20851;&#30340;&#36153;&#29992;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#12290;&#21463;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#30340;&#20196;&#20154;&#30633;&#30446;&#25104;&#23601;&#21551;&#21457;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;LLMs&#33021;&#21542;&#20197;&#26497;&#20302;&#25104;&#26412;&#26377;&#25928;&#22320;&#25913;&#21892;ML&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550; LlmCorr&#65292;&#19968;&#20010;LLM&#21487;&#20197;&#20316;&#20026;&#20107;&#21518;&#26657;&#27491;&#22120;&#65292;&#20026;&#20219;&#24847;ML&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20986;&#20462;&#27491;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#25972;&#21512;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#20449;&#24687;&#21644;ML&#27169;&#22411;&#23545;&#39564;&#35777;&#38598;&#30340;&#39044;&#27979;&#26469;&#24418;&#25104;&#19968;&#20010;&#19978;&#19979;&#25991;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#35201;&#27714;LLM&#24635;&#32467;ML&#27169;&#22411;&#29359;&#38169;&#35823;&#30340;&#23454;&#20363;&#20197;&#21450;&#20027;&#35201;&#39044;&#27979;&#19982;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#38543;&#21518;&#65292;LLM&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13414v1 Announce Type: cross  Abstract: As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can tr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25351;&#20196;&#38142;&#65288;CoI&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#26469;&#22788;&#29702;&#30001;&#22810;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#25351;&#20196;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#22810;&#35821;&#35328;&#25688;&#35201;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11532</link><description>&lt;p&gt;
&#25351;&#20196;&#38142;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25351;&#20196;&#38142;&#65288;CoI&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#26469;&#22788;&#29702;&#30001;&#22810;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#25351;&#20196;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#22810;&#35821;&#35328;&#25688;&#35201;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19968;&#31995;&#21015;&#22823;&#22411;&#21644;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#23545;&#26410;&#26366;&#35265;&#36807;&#30340;&#20219;&#21153;&#20063;&#36866;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25351;&#20196;&#38142;&#65288;CoI&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#20854;&#20013;&#19968;&#20010;&#25351;&#20196;&#30340;&#36755;&#20986;&#25104;&#20026;&#19979;&#19968;&#20010;&#25351;&#20196;&#30340;&#36755;&#20837;&#65292;&#23601;&#20687;&#19968;&#26465;&#38142;&#26465;&#12290;&#19982;&#35299;&#20915;&#21333;&#19968;&#25351;&#20196;&#20219;&#21153;&#30340;&#20256;&#32479;&#20570;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#40723;&#21169;&#27169;&#22411;&#36880;&#27493;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#30452;&#33267;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;CoI&#35843;&#25972;&#65288;&#21363;&#20351;&#29992;CoI&#25351;&#20196;&#36827;&#34892;&#24494;&#35843;&#65289;&#25552;&#39640;&#20102;&#27169;&#22411;&#22788;&#29702;&#30001;&#22810;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#25351;&#20196;&#33021;&#21147;&#12290;&#32463;CoI&#35843;&#25972;&#30340;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#25688;&#35201;&#19978;&#20063;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#35777;&#26126;....
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11532v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with a collection of large and diverse instructions has improved the model's generalization to different tasks, even for unseen tasks. However, most existing instruction datasets include only single instructions, and they struggle to follow complex instructions composed of multiple subtasks (Wang et al., 2023a). In this work, we propose a novel concept of compositional instructions called chain-of-instructions (CoI), where the output of one instruction becomes an input for the next like a chain. Unlike the conventional practice of solving single instruction tasks, our proposed method encourages a model to solve each subtask step by step until the final answer is reached. CoI-tuning (i.e., fine-tuning with CoI instructions) improves the model's ability to handle instructions composed of multiple subtasks. CoI-tuned models also outperformed baseline models on multilingual summarization, demonstratin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11253</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25919;&#31574;&#30340;&#33258;&#25105;&#21028;&#26029;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models by On-Policy Self-Judgment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#21033;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#25191;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#35201;&#20040;&#36890;&#36807;&#25918;&#24323;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#21644;&#23545;&#29420;&#31435;RM&#30340;&#38656;&#27714;&#31616;&#21270;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#23427;&#26082;&#26159;(1) &#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#21448;&#26159;(2) &#21442;&#25968;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;RM&#26469;&#35780;&#20272;&#26679;&#26412;&#36827;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#20316;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#19968;&#23545;&#19968;&#21028;&#26029;&#20219;&#21153;&#35270;&#20026;&#25351;&#23548;&#24335;&#20219;&#21153;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#21709;&#24212;&#23545;&#20013;&#36873;&#25321;&#26356;&#22909;&#30340;&#21709;&#24212;&#12290;&#22240;&#27492;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#35780;&#21028;&#24403;&#21069;&#31574;&#30053;&#30340;&#21363;&#26102;&#21709;&#24212;&#20559;&#22909;&#65292;&#20174;&#33258;&#36523;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;SELF-JUDGE&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11253v1 Announce Type: cross  Abstract: To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Stepwise ORMs (SORMs)&#65292;&#23427;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20197;&#36817;&#20284;&#39044;&#27979;&#26368;&#20248;&#31574;&#30053;&#30340;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;</title><link>https://arxiv.org/abs/2402.10963</link><description>&lt;p&gt;
GLoRe: &#20309;&#26102;&#12289;&#20309;&#22320;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#25913;&#36827;&#26469;&#25552;&#39640;LLM&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10963
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Stepwise ORMs (SORMs)&#65292;&#23427;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20197;&#36817;&#20284;&#39044;&#27979;&#26368;&#20248;&#31574;&#30053;&#30340;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#12289;&#31185;&#23398;&#25110;&#32534;&#30721;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#25913;&#36827;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#22909;&#30340;&#27169;&#22411;&#20063;&#24456;&#38590;&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#20309;&#26102;&#20309;&#22320;&#36827;&#34892;&#25913;&#36827;&#12290;&#22522;&#20110;&#32467;&#26524;&#30340;&#22870;&#21169;&#27169;&#22411;(ORMs)&#65292;&#34987;&#35757;&#32451;&#26469;&#39044;&#27979;&#26368;&#32456;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#65292;&#25351;&#31034;&#20309;&#26102;&#36827;&#34892;&#25913;&#36827;&#65292;&#20026;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#25913;&#36827;&#25552;&#20379;&#20102;&#19968;&#31181;&#20415;&#21033;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#36807;&#31243;&#30340;&#22870;&#21169;&#27169;&#22411;(PRMs)&#21463;&#36807;&#35757;&#32451;&#65292;&#29992;&#20197;&#39044;&#27979;&#20013;&#38388;&#27493;&#39588;&#30340;&#27491;&#30830;&#24615;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#26469;&#25351;&#31034;&#20309;&#22788;&#36827;&#34892;&#25913;&#36827;&#12290;&#20294;&#23427;&#20204;&#24456;&#26114;&#36149;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36880;&#27493;ORMs(SORMs)&#65292;&#23427;&#20204;&#21482;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#21463;&#36807;&#35757;&#32451;&#65292;&#20197;&#36817;&#20284;&#39044;&#27979;&#26368;&#20248;&#31574;&#30053;&#25110;$V^{\star}$&#30340;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;SORMs&#21463;&#35757;&#32451;&#26469;&#39044;&#27979;&#24403;&#21462;&#26679;&#26102;&#26368;&#32456;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10963v1 Announce Type: new  Abstract: State-of-the-art language models can exhibit impressive reasoning refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify \textit{when and where to refine} without access to external feedback. Outcome-based Reward Models (\textbf{ORMs}), trained to predict correctness of the final answer indicating when to refine, offer one convenient solution for deciding when to refine. Process Based Reward Models (\textbf{PRMs}), trained to predict correctness of intermediate steps, can then be used to indicate where to refine. But they are expensive to train, requiring extensive human annotations. In this paper, we propose Stepwise ORMs (\textbf{SORMs}) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or $V^{\star}$. More specifically, SORMs are trained to predict the correctness of the final answer when samplin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10882</link><description>&lt;p&gt;
&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#29992;&#20110;&#23433;&#20840;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Optimizer for Safe Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#26681;&#25454;&#25991;&#23383;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#19981;&#23433;&#20840;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#65292;&#22914;&#33394;&#24773;&#12289;&#39578;&#25200;&#21644;&#38750;&#27861;&#27963;&#21160;&#22270;&#20687;&#12290;&#22522;&#20110;&#22270;&#20687;&#26816;&#26597;&#22120;&#12289;&#27169;&#22411;&#24494;&#35843;&#21644;&#23884;&#20837;&#24335;&#38459;&#27490;&#30340;&#29616;&#26377;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840; T2I &#29983;&#25104;&#30340;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10882v1 Announce Type: cross  Abstract: Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, \textit{we propose the first universal prompt optimizer for safe T2I generation in black-box scenario}. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating in
&lt;/p&gt;</description></item><item><title>DE-COP&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#29256;&#26435;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#25506;&#27979;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27169;&#22411;&#35757;&#32451;&#25991;&#26412;&#20013;&#21487;&#33021;&#21253;&#21547;&#30340;&#29256;&#26435;&#20869;&#23481;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#30340;&#36923;&#36753;&#21487;&#29992;&#26102;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;9.6%&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#23436;&#20840;&#40657;&#30418;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;72%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.09910</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#26816;&#27979;&#29256;&#26435;&#20869;&#23481;&#30340;&#26041;&#27861;&#65306;DE-COP
&lt;/p&gt;
&lt;p&gt;
DE-COP: Detecting Copyrighted Content in Language Models Training Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09910
&lt;/p&gt;
&lt;p&gt;
DE-COP&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#29256;&#26435;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#25506;&#27979;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27169;&#22411;&#35757;&#32451;&#25991;&#26412;&#20013;&#21487;&#33021;&#21253;&#21547;&#30340;&#29256;&#26435;&#20869;&#23481;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#30340;&#36923;&#36753;&#21487;&#29992;&#26102;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;9.6%&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#23436;&#20840;&#40657;&#30418;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;72%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#21040;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26159;&#20445;&#23494;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22914;&#20309;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#20351;&#29992;&#20102;&#29256;&#26435;&#20869;&#23481;&#65311;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#22522;&#20110;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#24456;&#21487;&#33021;&#33021;&#22815;&#35782;&#21035;&#20986;&#20854;&#35757;&#32451;&#25991;&#26412;&#20013;&#30340;&#29420;&#25991;&#25688;&#24405;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DE-COP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#26159;&#21542;&#22312;&#35757;&#32451;&#20013;&#21253;&#21547;&#20102;&#19968;&#27573;&#29256;&#26435;&#20869;&#23481;&#12290;DE-COP&#30340;&#26680;&#24515;&#26041;&#27861;&#26159;&#36890;&#36807;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25506;&#27979;&#65292;&#36873;&#25321;&#39033;&#21253;&#25324;&#29420;&#25991;&#26412;&#21644;&#23427;&#20204;&#30340;&#37322;&#20041;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;BookTection&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22312;&#27169;&#22411;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21069;&#21644;&#20043;&#21518;&#20986;&#29256;&#30340;165&#26412;&#20070;&#30340;&#25688;&#24405;&#20197;&#21450;&#23427;&#20204;&#30340;&#37322;&#20041;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DE-COP&#22312;&#27169;&#22411;&#30340;&#36923;&#36753;&#21487;&#29992;&#26102;&#65292;&#26816;&#27979;&#24615;&#33021;&#65288;AUC&#65289;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20339;&#26041;&#27861;9.6%&#12290;&#27492;&#22806;&#65292;DE-COP&#22312;&#23436;&#20840;&#40657;&#30418;&#27169;&#22411;&#19978;&#26816;&#27979;&#21487;&#30097;&#20070;&#31821;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;72%&#65292;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#21482;&#26377;$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09910v1 Announce Type: new  Abstract: How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give $
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09631</link><description>&lt;p&gt;
MiMiC&#65306;&#34920;&#31034;&#31354;&#38388;&#20013;&#26368;&#23567;&#20462;&#25913;&#30340;&#23545;&#25239;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
MiMiC: Minimally Modified Counterfactuals in the Representation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#23398;&#31185; &#31616;&#20171;&#65306;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#33391;&#34892;&#20026;&#65292;&#22914;&#24615;&#21035;&#20559;&#35265;&#25110;&#26377;&#27602;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#24178;&#39044;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20004;&#31181;&#24120;&#35265;&#30340;&#24178;&#39044;&#25216;&#26415;&#65292;&#21363;&#32447;&#24615;&#25830;&#38500;&#21644;&#23450;&#21521;&#21521;&#37327;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#39640;&#24230;&#21487;&#25511;&#21644;&#34920;&#36798;&#20016;&#23500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24178;&#39044;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20351;&#28304;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#19982;&#30446;&#26631;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#38750;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#30456;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#20551;&#35774;&#19979;&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 Announce Type: cross  Abstract: Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that ena
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#23884;&#20837;&#26412;&#20307;
&lt;/p&gt;
&lt;p&gt;
Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21253;&#21547;&#39046;&#22495;&#20869;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#31867;&#21035;&#65292;&#21363;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22806;&#24310;&#30693;&#35782;&#25552;&#20379;&#20851;&#20110;&#26412;&#20307;&#20013;&#29305;&#23450;&#27010;&#24565;&#25152;&#23646;&#30340;&#20855;&#20307;&#23454;&#20363;&#30340;&#20449;&#24687;&#65292;&#32780;&#20869;&#28085;&#30693;&#35782;&#35814;&#32454;&#25551;&#36848;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#20869;&#22312;&#23646;&#24615;&#12289;&#29305;&#24449;&#21644;&#35821;&#20041;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#26410;&#33021;&#21516;&#26102;&#20805;&#20998;&#32771;&#34385;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EIKE&#65288;Extensional and Intensional Knowledge Embedding&#65289;&#30340;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#12290;EIKE&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#23454;&#20363;&#12289;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#23884;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#23545;&#22806;&#24310;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20869;&#28085;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can captur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#65288;EDiSC&#65289;&#65292;&#32467;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;DiSC&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;EDiSC&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00541</link><description>&lt;p&gt;
&#19968;&#20010;&#24102;&#26377;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#30340;&#35770;&#25991;&#19982;&#19968;&#20010;&#20851;&#20110;&#21476;&#24076;&#33098;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#65288;EDiSC&#65289;&#65292;&#32467;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;DiSC&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;EDiSC&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#30340;&#24847;&#20041;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#21464;&#21270;&#65292;&#35789;&#20041;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20250;&#28436;&#21464;&#12289;&#20986;&#29616;&#25110;&#28040;&#22833;&#12290;&#23545;&#20110;&#21476;&#20195;&#35821;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#35821;&#26009;&#24211;&#36890;&#24120;&#36739;&#23567;&#12289;&#31232;&#30095;&#19988;&#22024;&#26434;&#65292;&#20934;&#30830;&#24314;&#27169;&#36825;&#31181;&#21464;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#27492;&#23545;&#20110;&#24847;&#20041;&#21464;&#21270;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#21464;&#24471;&#37325;&#35201;&#12290;GASC&#21644;DiSC&#26159;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#32463;&#34987;&#29992;&#26469;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#65292;&#20351;&#29992;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#24182;&#27809;&#26377;&#20511;&#21161;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#24110;&#21161;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#32473;&#23450;&#30446;&#26631;&#35789;&#27719;&#65288;&#22914;"kosmos"&#65292;&#24847;&#20026;&#35013;&#39280;&#12289;&#31209;&#24207;&#25110;&#19990;&#30028;&#65289;&#30340;&#24847;&#20041;&#34920;&#31034;&#20026;&#19978;&#19979;&#25991;&#35789;&#27719;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#24847;&#20041;&#30340;&#26222;&#36941;&#24615;&#34920;&#31034;&#20026;&#24847;&#20041;&#30340;&#20998;&#24067;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36827;&#34892;&#25311;&#21512;&#65292;&#20197;&#27979;&#37327;&#36825;&#20123;&#34920;&#31034;&#20013;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EDiSC&#65292;&#36825;&#26159;DiSC&#30340;&#23884;&#20837;&#29256;&#26412;&#65292;&#23427;&#23558;&#35789;&#23884;&#20837;&#19982;DiSC&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#26356;&#20248;&#31168;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;EDiSC&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small, sparse and noisy, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC and DiSC are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as "kosmos" (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using MCMC methods to measure temporal changes in these representations. In this paper, we introduce EDiSC, an embedded version of DiSC, which combines word embeddings with DiSC to provide superior model performance. We show empirically that EDiSC offers improved p
&lt;/p&gt;</description></item><item><title>NExT-GPT&#26159;&#19968;&#20010;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25509;&#21463;&#21644;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2309.05519</link><description>&lt;p&gt;
NExT-GPT: &#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NExT-GPT: Any-to-Any Multimodal LLM. (arXiv:2309.05519v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05519
&lt;/p&gt;
&lt;p&gt;
NExT-GPT&#26159;&#19968;&#20010;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25509;&#21463;&#21644;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLM&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#25391;&#22859;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23384;&#22312;&#19968;&#20010;&#38480;&#21046;&#65292;&#21363;&#21482;&#33021;&#22312;&#36755;&#20837;&#31471;&#36827;&#34892;&#22810;&#27169;&#24577;&#29702;&#35299;&#65292;&#26080;&#27861;&#20197;&#22810;&#31181;&#27169;&#24335;&#29983;&#25104;&#20869;&#23481;&#12290;&#30001;&#20110;&#25105;&#20204;&#20154;&#31867;&#24635;&#26159;&#36890;&#36807;&#21508;&#31181;&#27169;&#24577;&#24863;&#30693;&#19990;&#30028;&#21644;&#19982;&#20154;&#20132;&#27969;&#65292;&#22240;&#27492;&#24320;&#21457;&#33021;&#22815;&#25509;&#21463;&#21644;&#20256;&#36882;&#20219;&#20309;&#27169;&#24577;&#20869;&#23481;&#30340;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;MM-LLM&#31995;&#32479;&#23545;&#20110;&#23454;&#29616;&#20154;&#32423;AI&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;NExT-GPT&#12290;&#25105;&#20204;&#36890;&#36807;&#36830;&#25509;&#19968;&#20010;&#21547;&#26377;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#30340;LLM&#65292;&#20351;&#24471;NExT-GPT&#33021;&#22815;&#20197;&#20219;&#24847;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#36827;&#34892;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#35757;&#32451;&#26377;&#32032;&#30340;&#39640;&#24615;&#33021;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;NExT-GPT&#20165;&#36890;&#36807;&#35843;&#25972;&#26576;&#20123;&#25237;&#24433;&#23618;&#30340;&#23569;&#37327;&#21442;&#25968;&#65288;1%&#65289;&#36827;&#34892;&#35843;&#20248;&#65292;&#36825;&#19981;&#20165;&#26377;&#21033;&#20110;&#20302;&#25104;&#26412;&#35757;&#32451;&#65292;&#36824;&#26377;&#21161;&#20110;&#26041;&#20415;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#25991;&#26412;&#34920;&#26684;&#38382;&#31572;&#26694;&#26550;S3HQA&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#35757;&#32451;&#31934;&#32454;&#30340;&#26816;&#32034;&#22120;&#26469;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#37319;&#29992;&#28151;&#21512;&#36873;&#25321;&#22120;&#26469;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#23454;&#38469;&#30693;&#35782;&#24182;&#37319;&#29992;&#22522;&#20110;&#29983;&#25104;&#30340;&#25512;&#29702;&#22120;&#26469;&#33719;&#21462;&#31572;&#26696;&#12290;&#22312;WikiTableQuestions&#21644;ComplexWebQuestions&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11725</link><description>&lt;p&gt;
S3HQA&#65306;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;-&#34920;&#26684;&#28151;&#21512;&#38382;&#31572;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering. (arXiv:2305.11725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#25991;&#26412;&#34920;&#26684;&#38382;&#31572;&#26694;&#26550;S3HQA&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#35757;&#32451;&#31934;&#32454;&#30340;&#26816;&#32034;&#22120;&#26469;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#37319;&#29992;&#28151;&#21512;&#36873;&#25321;&#22120;&#26469;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#23454;&#38469;&#30693;&#35782;&#24182;&#37319;&#29992;&#22522;&#20110;&#29983;&#25104;&#30340;&#25512;&#29702;&#22120;&#26469;&#33719;&#21462;&#31572;&#26696;&#12290;&#22312;WikiTableQuestions&#21644;ComplexWebQuestions&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#28041;&#21450;&#25991;&#26412;&#21644;&#34920;&#26684;&#28151;&#21512;&#20107;&#23454;&#30693;&#35782;&#30340;&#22810;&#36339;&#38382;&#39064;(TextTableQA)&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#27169;&#22411;&#20027;&#35201;&#37319;&#29992;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#26694;&#26550;&#65292;&#23384;&#22312;&#20960;&#20010;&#38382;&#39064;&#65292;&#22914;&#35757;&#32451;&#26816;&#32034;&#22120;&#30340;&#22024;&#26434;&#26631;&#31614;&#12289;&#23545;&#25991;&#26412;&#21644;&#34920;&#26684;&#30340;&#24322;&#26500;&#20449;&#24687;&#21033;&#29992;&#19981;&#36275;&#20197;&#21450;&#19981;&#21516;&#25512;&#29702;&#25805;&#20316;&#33021;&#21147;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#25991;&#26412;&#34920;&#26684;&#38382;&#31572;&#26694;&#26550;S3HQA&#65292;&#21253;&#25324;&#26816;&#32034;&#22120;&#12289;&#36873;&#25321;&#22120;&#21644;&#25512;&#29702;&#22120;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#35757;&#32451;&#31934;&#32454;&#30340;&#26816;&#32034;&#22120;&#26469;&#35299;&#20915;&#22024;&#26434;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#19968;&#20010;&#28151;&#21512;&#36873;&#25321;&#22120;&#26469;&#32771;&#34385;&#24322;&#26500;&#25968;&#25454;&#20043;&#38388;&#30340;&#38142;&#25509;&#20851;&#31995;&#65292;&#20197;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#23454;&#38469;&#30693;&#35782;&#12290;&#22312;&#26368;&#21518;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#30340;&#25512;&#29702;&#22120;&#26469;&#33719;&#21462;&#31572;&#26696;&#65292;&#32780;&#19981;&#26159;&#20687;&#20043;&#21069;&#30340;&#26041;&#27861;&#19968;&#26679;&#20351;&#29992;&#38405;&#35835;&#29702;&#35299;&#27169;&#22359;&#12290;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#25353;&#34892;&#29983;&#25104;&#22120;&#65292;&#19968;&#31181;&#26159;LLM&#25552;&#31034;&#29983;&#25104;&#22120;&#65288;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#31532;&#19968;&#27425;&#20351;&#29992;&#65289;&#12290;&#22312;WikiTableQuestions&#21644;ComplexWebQuestions&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;S3HQA&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterogeneous information over text and table, and deficient ability for different reasoning operations. In this paper, we propose a three-stage TextTableQA framework S3HQA, which comprises of retriever, selector, and reasoner. We use a retriever with refinement training to solve the noisy labeling problem. Then, a hybrid selector considers the linked relationships between heterogeneous data to select the most relevant factual knowledge. For the final stage, instead of adapting a reading comprehension module like in previous methods, we employ a generation-based reasoner to obtain answers. This includes two approaches: a row-wise generator and an LLM prompting generator~(first time used in this tas
&lt;/p&gt;</description></item></channel></rss>