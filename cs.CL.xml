<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;CoBSAT&#12290;&#30740;&#31350;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01293</link><description>&lt;p&gt;
MLLMs&#33021;&#21542;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can MLLMs Perform Text-to-Image In-Context Learning?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;CoBSAT&#12290;&#30740;&#31350;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21457;&#23637;&#21040;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#25512;&#21160;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25193;&#23637;&#21040;&#22810;&#27169;&#24335;&#30340;&#30740;&#31350;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;ICL&#19978;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;ICL&#65288;T2I-ICL&#65289;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24615;&#21644;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#20294;&#20173;&#28982;&#23569;&#26377;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;T2I-ICL&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;CoBSAT&#65292;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#20219;&#21153;&#30340;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20845;&#20010;&#26368;&#20808;&#36827;&#30340;MLLMs&#65292;&#25105;&#20204;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation. To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements. Our code and dataset are available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#21019;&#24314;&#20102;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;</title><link>https://arxiv.org/abs/2404.01461</link><description>&lt;p&gt;
&#35831;&#30495;&#27491;&#30340;&#29747;&#36798;&#31449;&#20986;&#26469;...&#38754;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#65311;&#22312;LLMs&#20013;&#23457;&#35270;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;
&lt;/p&gt;
&lt;p&gt;
Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#21019;&#24314;&#20102;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#23637;&#29616;&#20986;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLMs&#21487;&#33021;&#20250;&#23481;&#26131;&#21463;&#21040;&#20154;&#31867;&#20915;&#31574;&#20013;&#30340;&#19968;&#31181;&#24120;&#35265;&#35748;&#30693;&#38519;&#38449;&#24433;&#21709;&#65292;&#21363;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#12290;&#36825;&#26159;&#24515;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#27010;&#24565;&#65292;&#25351;&#30340;&#26159;&#26681;&#25454;&#20107;&#20214;&#19982;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#21407;&#22411;&#25110;&#20856;&#22411;&#20363;&#23376;&#30340;&#30456;&#20284;&#31243;&#24230;&#26469;&#21028;&#26029;&#20107;&#20214;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#20107;&#23454;&#25110;&#32479;&#35745;&#35777;&#25454;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#23545;LLM&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;REHEAT&#65288;Representativeness Heuristic AI Testing&#65289;&#65292;&#19968;&#20010;&#21253;&#21547;&#28085;&#30422;&#20845;&#31181;&#24120;&#35265;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#31867;&#22411;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#26174;&#31034;&#65292;&#24212;&#29992;&#20110;REHEAT&#30340;&#22235;&#20010;LLMs&#37117;&#34920;&#29616;&#20986;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#27493;&#39588;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01461v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.20262</link><description>&lt;p&gt;
ELITR-Bench: &#38754;&#21521;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#20250;&#35758;&#21161;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20027;&#35201;&#33268;&#21147;&#20110;&#25193;&#23637;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#38271;&#25991;&#26723;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#38271;&#36317;&#31163;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#20294;&#29616;&#26377;&#30340;&#21162;&#21147;&#20027;&#35201;&#32771;&#34385;&#30340;&#26159;&#19981;&#19968;&#23450;&#19982;&#29616;&#23454;&#24212;&#29992;&#30456;&#20851;&#30340;&#36890;&#29992;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#30340;&#26032;&#22522;&#20934;&#12290;&#22312;&#36825;&#31181;&#24773;&#26223;&#19979;&#65292;&#38271;&#19978;&#19979;&#25991;&#30001;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#33719;&#24471;&#30340;&#36716;&#24405;&#32452;&#25104;&#65292;&#30001;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#22266;&#26377;&#22024;&#26434;&#24615;&#21644;&#21475;&#35821;&#29305;&#24615;&#65292;&#36825;&#20026;LLMs&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;ELITR-Bench&#65292;&#36890;&#36807;271&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21450;&#20854;&#30495;&#23454;&#31572;&#26696;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;ELITR&#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#12290;&#25105;&#20204;&#22312;ELITR-Bench&#19978;&#23545;&#26368;&#26032;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20262v1 Announce Type: cross  Abstract: Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, our work proposes a new benchmark for long-context LLMs focused on a practical meeting assistant scenario. In this scenario, the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271 manually crafted questions and their ground-truth answers. Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20004;&#20010;LLM&#30340;&#21103;&#26412;&#19982;&#39564;&#35777;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#27491;&#24335;&#35268;&#33539;&#20043;&#38388;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2403.18327</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#36827;&#34892;&#27491;&#24335;&#23545;&#35805;&#21527;&#65311;&#33258;&#21160;&#35780;&#20272;LLMs&#22312;&#36716;&#25442;&#21644;&#35299;&#37322;&#27491;&#24335;&#35268;&#33539;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20004;&#20010;LLM&#30340;&#21103;&#26412;&#19982;&#39564;&#35777;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#27491;&#24335;&#35268;&#33539;&#20043;&#38388;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#30410;&#30456;&#20851;&#32773;&#32463;&#24120;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#31995;&#32479;&#38656;&#27714;&#65292;&#28982;&#21518;&#30001;&#39046;&#22495;&#19987;&#23478;&#23558;&#20854;&#36716;&#25442;&#20026;&#24418;&#24335;&#21270;&#35821;&#27861;&#65292;&#20174;&#32780;&#22686;&#21152;&#35774;&#35745;&#25104;&#26412;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#27491;&#24335;&#35268;&#33539;&#20043;&#38388;&#36716;&#25442;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20004;&#20010;LLM&#30340;&#21103;&#26412;&#19982;&#29616;&#25104;&#30340;&#39564;&#35777;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#20154;&#24037;&#36755;&#20837;&#23601;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#20854;&#32763;&#35793;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#35821;&#35328;&#35821;&#27861;&#29983;&#25104;&#24418;&#24335;&#21270;&#35821;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#20197;&#34913;&#37327;&#36825;&#31181;&#32763;&#35793;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18327v1 Announce Type: cross  Abstract: Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets. We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;FEEL&#65292;&#29992;&#20110;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38750;&#20154;&#24037;&#26041;&#27861;&#22312;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#37319;&#29992;&#20102;&#27010;&#29575;&#20998;&#24067;&#26041;&#27861;&#21644;&#38598;&#25104;&#23398;&#20064;&#20197;&#33719;&#24471;&#26356;&#31283;&#23450;&#21644;&#20840;&#38754;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15699</link><description>&lt;p&gt;
FEEL&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;FEEL&#65292;&#29992;&#20110;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38750;&#20154;&#24037;&#26041;&#27861;&#22312;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#37319;&#29992;&#20102;&#27010;&#29575;&#20998;&#24067;&#26041;&#27861;&#21644;&#38598;&#25104;&#23398;&#20064;&#20197;&#33719;&#24471;&#26356;&#31283;&#23450;&#21644;&#20840;&#38754;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#65288;ESC&#65289;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#23545;&#35805;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24110;&#21161;&#29992;&#25143;&#32531;&#35299;&#24773;&#24863;&#21387;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24773;&#24863;&#20998;&#26512;&#20013;&#28041;&#21450;&#22266;&#26377;&#20027;&#35266;&#24615;&#65292;&#24403;&#21069;&#38750;&#20154;&#24037;&#26041;&#27861;&#22312;&#26377;&#25928;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#24456;&#20302;&#30340;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#25163;&#21160;&#35780;&#20272;&#26041;&#27861;&#23558;&#23548;&#33268;&#24456;&#39640;&#30340;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#27169;&#22411;FEEL&#65288;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#30340;&#26694;&#26550;&#65289;&#65292;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35780;&#20272;&#32773;&#26469;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#21608;&#23494;&#32771;&#34385;ESC&#30340;&#21508;&#31181;&#35780;&#20272;&#26041;&#38754;&#65292;&#24212;&#29992;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;ESC&#35780;&#20272;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23427;&#37319;&#29992;&#27010;&#29575;&#20998;&#24067;&#26041;&#27861;&#20197;&#33719;&#24471;&#26356;&#31283;&#23450;&#30340;&#32467;&#26524;&#65292;&#24182;&#38598;&#25104;&#20102;&#38598;&#25104;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15699v1 Announce Type: new  Abstract: Emotional Support Conversation (ESC) is a typical dialogue that can effec-tively assist the user in mitigating emotional pressures. However, owing to the inherent subjectivity involved in analyzing emotions, current non-artificial methodologies face challenges in effectively appraising the emo-tional support capability. These metrics exhibit a low correlation with human judgments. Concurrently, manual evaluation methods extremely will cause high costs. To solve these problems, we propose a novel model FEEL (Framework for Evaluating Emotional Support Capability with Large Lan-guage Models), employing Large Language Models (LLMs) as evaluators to assess emotional support capabilities. The model meticulously considers var-ious evaluative aspects of ESC to apply a more comprehensive and accurate evaluation method for ESC. Additionally, it employs a probability distribu-tion approach for a more stable result and integrates an ensemble learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25991;&#26412;&#25968;&#25454;&#24773;&#24863;&#20998;&#31867;&#20013;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#25216;&#26415;&#22914;&#21435;&#38500;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#36825;&#20123;&#20803;&#32032;&#20173;&#28982;&#33021;&#22815;&#20256;&#36798;&#24773;&#24863;&#25110;&#24378;&#35843;&#65292;&#32780;Transformer&#30340;&#20248;&#21183;&#22312;&#20110;&#29702;&#35299;&#25991;&#26412;&#20869;&#30340;&#35821;&#22659;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.15454</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#36827;&#34892;&#24773;&#24863;&#26816;&#27979;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emotion Detection with Transformers: A Comparative Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25991;&#26412;&#25968;&#25454;&#24773;&#24863;&#20998;&#31867;&#20013;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#25216;&#26415;&#22914;&#21435;&#38500;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#36825;&#20123;&#20803;&#32032;&#20173;&#28982;&#33021;&#22815;&#20256;&#36798;&#24773;&#24863;&#25110;&#24378;&#35843;&#65292;&#32780;Transformer&#30340;&#20248;&#21183;&#22312;&#20110;&#29702;&#35299;&#25991;&#26412;&#20869;&#30340;&#35821;&#22659;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#25991;&#26412;&#25968;&#25454;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#21464;&#20307;&#30340;Transformer&#23545;Emotion&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#35770;&#25991;&#36824;&#20998;&#26512;&#20102;&#19968;&#20123;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#27604;&#22914;Transformer&#23618;&#30340;&#24494;&#35843;&#12289;&#23618;&#30340;&#21487;&#35757;&#32451;&#24615;&#20197;&#21450;&#25991;&#26412;&#25968;&#25454;&#30340;&#39044;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24120;&#29992;&#25216;&#26415;&#22914;&#21435;&#38500;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;Transformer&#30340;&#20248;&#21183;&#22312;&#20110;&#29702;&#35299;&#25991;&#26412;&#20869;&#30340;&#35821;&#22659;&#20851;&#31995;&#12290;&#20687;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#36825;&#26679;&#30340;&#20803;&#32032;&#20173;&#28982;&#21487;&#20197;&#20256;&#36798;&#24773;&#24863;&#25110;&#24378;&#35843;&#65292;&#21435;&#38500;&#23427;&#20204;&#21487;&#33021;&#20250;&#30772;&#22351;&#36825;&#31181;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15454v1 Announce Type: new  Abstract: In this study, we explore the application of transformer-based models for emotion classification on text data. We train and evaluate several pre-trained transformer models, on the Emotion dataset using different variants of transformers. The paper also analyzes some factors that in-fluence the performance of the model, such as the fine-tuning of the transformer layer, the trainability of the layer, and the preprocessing of the text data. Our analysis reveals that commonly applied techniques like removing punctuation and stop words can hinder model performance. This might be because transformers strength lies in understanding contextual relationships within text. Elements like punctuation and stop words can still convey sentiment or emphasis and removing them might disrupt this context.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#33258;&#21160;&#20114;&#21160;&#35780;&#20272;&#65288;AIE&#65289;&#26694;&#26550;&#21644;&#29366;&#24577;&#24863;&#30693;&#30149;&#20154;&#27169;&#25311;&#22120;&#65288;SAPS&#65289;&#65292;&#20197;&#21160;&#24577;&#12289;&#30495;&#23454;&#30340;&#24179;&#21488;&#35780;&#20272;LLMs&#65292;&#24357;&#34917;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#20020;&#24202;&#20219;&#21153;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.08495</link><description>&lt;p&gt;
&#20855;&#26377;&#29366;&#24577;&#24863;&#30693;&#30149;&#20154;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20114;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08495
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#33258;&#21160;&#20114;&#21160;&#35780;&#20272;&#65288;AIE&#65289;&#26694;&#26550;&#21644;&#29366;&#24577;&#24863;&#30693;&#30149;&#20154;&#27169;&#25311;&#22120;&#65288;SAPS&#65289;&#65292;&#20197;&#21160;&#24577;&#12289;&#30495;&#23454;&#30340;&#24179;&#21488;&#35780;&#20272;LLMs&#65292;&#24357;&#34917;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#20020;&#24202;&#20219;&#21153;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20154;&#26426;&#20114;&#21160;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#33258;&#21160;&#20114;&#21160;&#35780;&#20272;&#65288;AIE&#65289;&#26694;&#26550;&#21644;&#29366;&#24577;&#24863;&#30693;&#30149;&#20154;&#27169;&#25311;&#22120;&#65288;SAPS&#65289;&#65292;&#26088;&#22312;&#24357;&#34917;&#20256;&#32479;LLM&#35780;&#20272;&#19982;&#20020;&#24202;&#23454;&#36341;&#30340;&#24494;&#22937;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08495v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in human interactions, yet their application within the medical field remains insufficiently explored. Previous works mainly focus on the performance of medical knowledge with examinations, which is far from the realistic scenarios, falling short in assessing the abilities of LLMs on clinical tasks. In the quest to enhance the application of Large Language Models (LLMs) in healthcare, this paper introduces the Automated Interactive Evaluation (AIE) framework and the State-Aware Patient Simulator (SAPS), targeting the gap between traditional LLM evaluations and the nuanced demands of clinical practice. Unlike prior methods that rely on static medical knowledge assessments, AIE and SAPS provide a dynamic, realistic platform for assessing LLMs through multi-turn doctor-patient simulations. This approach offers a closer approximation to real clinical scenarios and allows f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35775;&#38382;&#20869;&#23384;&#26102;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.07805</link><description>&lt;p&gt;
&#36229;&#36234;&#27515;&#35760;&#30828;&#32972;&#65306;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Beyond Memorization: The Challenge of Random Memory Access in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35775;&#38382;&#20869;&#23384;&#26102;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#21442;&#25968;&#20869;&#37096;&#30340;&#30693;&#35782;&#23384;&#20648;&#21644;&#20869;&#23384;&#35775;&#38382;&#26426;&#21046;&#20173;&#28982;&#20196;&#20154;&#36153;&#35299;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-2&#65289;&#26159;&#21542;&#33021;&#22815;&#39034;&#24207;&#25110;&#38543;&#26426;&#22320;&#35775;&#38382;&#20854;&#20869;&#23384;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20840;&#38754;&#32972;&#35829;&#12289;&#36873;&#25321;&#24615;&#32972;&#35829;&#21644;&#22522;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#24773;&#26223;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LMs&#33021;&#22815;&#39034;&#24207;&#35775;&#38382;&#20854;&#20869;&#23384;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#35775;&#38382;&#24050;&#35760;&#24518;&#20869;&#23481;&#26102;&#36935;&#21040;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;LMs&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#36825;&#31181;&#24178;&#39044;&#24212;&#29992;&#20110;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36890;&#36807;&#32972;&#35829;&#26469;&#22686;&#24378;&#38543;&#26426;&#35775;&#38382;&#25216;&#26415;&#23545;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07805v1 Announce Type: cross  Abstract: Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in questi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#30456;&#21516;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#26597;&#35810;&#32452;&#21512;&#20026;&#21333;&#20010;&#25552;&#31034;&#65292;&#20197;&#26368;&#23567;&#21270;&#37325;&#22797;&#35843;&#29992;&#26469;&#20248;&#21270;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.00067</link><description>&lt;p&gt;
Query-OPT&#65306;&#36890;&#36807;&#22810;&#26597;&#35810;&#25351;&#20196;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#30456;&#21516;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#26597;&#35810;&#32452;&#21512;&#20026;&#21333;&#20010;&#25552;&#31034;&#65292;&#20197;&#26368;&#23567;&#21270;&#37325;&#22797;&#35843;&#29992;&#26469;&#20248;&#21270;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20851;&#27880;&#22522;&#20110;&#26597;&#35810;&#30340;&#20250;&#35758;&#25688;&#35201;&#20219;&#21153;&#65292;&#22312;&#27492;&#20219;&#21153;&#20013;&#65292;&#38024;&#23545;&#29305;&#23450;&#26597;&#35810;&#23545;&#19978;&#19979;&#25991;&#65288;&#20250;&#35758;&#35760;&#24405;&#65289;&#29983;&#25104;&#25688;&#35201;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#27492;&#20219;&#21153;&#26102;&#65292;&#21363;&#20351;&#19978;&#19979;&#25991;&#20445;&#25345;&#19981;&#21464;&#65292;&#27599;&#20010;&#26032;&#26597;&#35810;&#20063;&#38656;&#35201;&#23545;LLM&#25512;&#29702;&#31471;&#28857;/API&#36827;&#34892;&#19968;&#27425;&#26032;&#35843;&#29992;&#12290;&#28982;&#32780;&#65292;&#21453;&#22797;&#35843;&#29992;LLM&#25512;&#29702;&#31471;&#28857;&#20250;&#26174;&#33879;&#22686;&#21152;&#22312;&#29983;&#20135;&#20013;&#20351;&#29992;&#23427;&#20204;&#30340;&#25104;&#26412;&#65292;&#36825;&#20351;&#24471;&#35768;&#22810;&#23454;&#38469;&#29992;&#20363;&#20013;LLMs&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#25104;&#21151;&#22320;&#23558;&#30456;&#21516;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#26597;&#35810;&#32452;&#21512;&#20026;&#21333;&#20010;&#25552;&#31034;&#20197;&#26368;&#23567;&#21270;&#37325;&#22797;&#35843;&#29992;&#65292;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#21508;&#31181;&#27969;&#34892;&#30340;LLM&#65288;GPT-4&#12289;PaLM-2&#12289;LLaMA-2&#12289;Mistral&#21644;FLAN-T5&#65289;&#22312;&#21333;&#26597;&#35810;&#21644;&#22810;&#26597;&#35810;&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00067v1 Announce Type: new  Abstract: This work focuses on the task of query-based meeting summarization in which the summary of a context (meeting transcript) is generated in response to a specific query. When using Large Language Models (LLMs) for this task, a new call to the LLM inference endpoint/API is required for each new query even if the context stays the same. However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases. To address this problem, in this paper, we investigate whether combining the queries for the same input context in a single prompt to minimize repeated calls can be successfully used in meeting summarization. In this regard, we conduct extensive experiments by comparing the performance of various popular LLMs: GPT-4, PaLM-2, LLaMA-2, Mistral, and FLAN-T5 in single-query and multi-query settings. We observe that while most LLMs tend to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23884;&#20837;&#26041;&#27861;MetaEOL&#65292;&#36890;&#36807;&#20803;&#20219;&#21153;&#25552;&#31034;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21477;&#23376;&#23884;&#20837;&#65292;&#26080;&#38656;&#27169;&#22411;&#24494;&#35843;&#25110;&#29305;&#23450;&#20219;&#21153;&#24037;&#31243;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#27979;&#35797;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;</title><link>https://arxiv.org/abs/2402.18458</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#20219;&#21153;&#25552;&#31034;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Meta-Task Prompting Elicits Embedding from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18458
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23884;&#20837;&#26041;&#27861;MetaEOL&#65292;&#36890;&#36807;&#20803;&#20219;&#21153;&#25552;&#31034;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21477;&#23376;&#23884;&#20837;&#65292;&#26080;&#38656;&#27169;&#22411;&#24494;&#35843;&#25110;&#29305;&#23450;&#20219;&#21153;&#24037;&#31243;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#27979;&#35797;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23884;&#20837;&#26041;&#27861;&#65292;&#21363;&#24102;&#26174;&#24335;&#21333;&#35789;&#38480;&#21046;&#30340;&#20803;&#20219;&#21153;&#25552;&#31034;&#65288;MetaEOL&#65289;&#65292;&#29992;&#20110;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25110;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;&#20803;&#20219;&#21153;&#25552;&#31034;&#65292;MetaEOL&#24341;&#23548;LLMs&#36890;&#36807;&#19968;&#31995;&#21015;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#29983;&#25104;&#23884;&#20837;&#65292;&#36825;&#20123;&#25552;&#31034;&#28085;&#30422;&#20102;&#22810;&#20010;&#34920;&#31034;&#26041;&#38754;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20174;&#21508;&#31181;&#20803;&#20219;&#21153;&#24179;&#22343;&#24471;&#21040;&#30340;&#23884;&#20837;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#65292;&#36229;&#36234;&#20102;&#23545;&#27604;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#29983;&#25104;&#30340;&#26032;&#30340;&#25193;&#23637;&#23450;&#24459;&#65292;&#20026;&#36328;&#22810;&#31181;&#20197;&#21477;&#23376;&#20026;&#20013;&#24515;&#30340;&#22330;&#26223;&#20013;&#30340;&#23884;&#20837;&#25552;&#21462;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#12289;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18458v1 Announce Type: new  Abstract: In this work, we introduce a new unsupervised embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning or task-specific engineering. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law for embedding generation, offering a versatile, resource-efficient approach for embedding extraction across diverse sentence-centric scenarios.
&lt;/p&gt;</description></item><item><title>OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17553</link><description>&lt;p&gt;
OmniACT&#65306;&#29992;&#20110;&#21551;&#29992;&#26700;&#38754;&#21644;Web&#22810;&#27169;&#24335;&#36890;&#29992;&#20027;&#21160;&#26234;&#33021;&#20307;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17553
&lt;/p&gt;
&lt;p&gt;
OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#26469;&#65292;&#20154;&#26426;&#20132;&#20114;&#20174;&#26681;&#26412;&#19978;&#19968;&#30452;&#26159;&#25163;&#21160;&#30340;&#12290;&#21363;&#20351;&#22312;&#20170;&#22825;&#65292;&#20960;&#20046;&#25152;&#26377;&#22312;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39640;&#25928;&#24037;&#20316;&#37117;&#38656;&#35201;&#20154;&#31867;&#22312;&#27599;&#19968;&#27493;&#37117;&#25552;&#20379;&#36755;&#20837;&#12290;&#34394;&#25311;&#20027;&#21160;&#26234;&#33021;&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#35768;&#22810;&#36825;&#20123;&#29712;&#30862;&#20219;&#21153;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#27493;&#39588;&#12290;&#34394;&#25311;&#20195;&#29702;&#23558;&#20351;&#25216;&#26415;&#33021;&#21147;&#26377;&#38480;&#30340;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#22320;&#31616;&#21270;&#35768;&#22810;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#20174;&#26085;&#21382;&#31649;&#29702;&#21040;&#22797;&#26434;&#30340;&#26053;&#34892;&#39044;&#35746;&#65292;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; OmniACT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#26469;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#33539;&#22260;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#35832;&#22914;"&#25773;&#25918;&#19979;&#19968;&#39318;&#27468;"&#20043;&#31867;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20197;&#21450;&#26356;&#20026;&#38271;&#26399;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17553v1 Announce Type: new  Abstract: For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such
&lt;/p&gt;</description></item><item><title>MLLMs&#36890;&#36807;&#24494;&#35843;&#33719;&#24471;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#20294;&#25237;&#24433;&#24182;&#26410;&#25552;&#21462;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16832</link><description>&lt;p&gt;
&#31070;&#31192;&#30340;&#25237;&#24433;&#65306;&#22810;&#27169;&#24577;LLMs&#22312;&#27809;&#26377;&#26356;&#20016;&#23500;&#30340;&#36328;&#27169;&#24577;&#25237;&#24433;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16832
&lt;/p&gt;
&lt;p&gt;
MLLMs&#36890;&#36807;&#24494;&#35843;&#33719;&#24471;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#20294;&#25237;&#24433;&#24182;&#26410;&#25552;&#21462;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22914;LLaVA&#21644;GPT-4(V)&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#20851;&#20110;&#22270;&#20687;&#30340;&#36890;&#29992;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#29616;&#25104;&#30340;MLLMs&#21487;&#33021;&#22312;&#35832;&#22914;&#30382;&#32932;&#30149;&#23398;&#21644;&#20892;&#19994;&#31561;&#39046;&#22495;&#30340;&#22270;&#20687;&#19978;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#24517;&#39035;&#36827;&#34892;&#24494;&#35843;&#20197;&#35299;&#38145;&#29305;&#23450;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;4&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#22312;&#20004;&#31181;&#24494;&#35843;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#30528;MLLM&#30340;&#24494;&#35843;&#65292;&#23427;&#30830;&#23454;&#33719;&#24471;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26356;&#26032;&#24182;&#27809;&#26377;&#23548;&#33268;&#25237;&#24433;&#25552;&#21462;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16832v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable general-purpose conversations about images with the language modality. As off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications. The prevalent architecture of current open-source MLLMs comprises two major modules: an image-language (cross-modal) projection network and a large language model. It is desirable to understand the roles of these two modules in modeling domain-specific visual attributes to inform the design of future models and streamline the interpretability efforts on the current models. To this end, via experiments on 4 datasets and under 2 fine-tuning settings, we find that as the MLLM is fine-tuned, it indeed gains domain-specific visual capabilities, but the updates do not lead to the projection extracting relevant domain-specific visual
&lt;/p&gt;</description></item><item><title>Rainbow Teaming&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#25918;&#24335;&#25628;&#32034;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#39046;&#22495;&#30340;&#27169;&#22411;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.16822</link><description>&lt;p&gt;
&#24425;&#34425;&#22242;&#38431;&#65306;&#22810;&#26679;&#21270;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16822
&lt;/p&gt;
&lt;p&gt;
Rainbow Teaming&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#25918;&#24335;&#25628;&#32034;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#39046;&#22495;&#30340;&#27169;&#22411;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#29702;&#35299;&#21644;&#22686;&#24378;&#23427;&#20204;&#23545;&#29992;&#25143;&#36755;&#20837;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35782;&#21035;&#25932;&#23545;&#25552;&#31034;&#30340;&#26041;&#27861;&#24448;&#24448;&#19987;&#27880;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#25110;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24425;&#34425;&#22242;&#38431;&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#21270;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#24425;&#34425;&#22242;&#38431;&#23558;&#23545;&#25239;&#24615;&#25552;&#31034;&#29983;&#25104;&#35270;&#20026;&#19968;&#20010;&#36136;&#37327; - &#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24320;&#25918;&#24335;&#25628;&#32034;&#26469;&#29983;&#25104;&#26082;&#26377;&#25928;&#21448;&#22810;&#26679;&#30340;&#25552;&#31034;&#12290;&#23427;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#22312;&#24191;&#27867;&#39046;&#22495;&#20869;&#30340;&#33030;&#24369;&#24615;&#65292;&#21253;&#25324;&#26412;&#25991;&#20013;&#30340;&#23433;&#20840;&#24615;&#12289;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23545;&#30001;&#24425;&#34425;&#22242;&#38431;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;LLMs&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#25439;&#23475;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16822v1 Announce Type: new  Abstract: As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to user inputs is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem, and uses open-ended search to generate prompts that are both effective and diverse. It can uncover a model's vulnerabilities across a broad range of domains including, in this paper, safety, question answering, and cybersecurity. We also demonstrate that fine-tuning on synthetic data generated by Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting their general capabilities 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;TutorEval&#21644;TutorChat&#65292;&#36890;&#36807;TutorEval&#22522;&#20934;&#21487;&#20197;&#34913;&#37327;LMs&#20316;&#20026;&#31185;&#23398;&#21161;&#25163;&#30340;&#23454;&#38469;&#21487;&#29992;&#24615;&#65292;TutorChat&#25968;&#25454;&#38598;&#29992;&#20110;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11111</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31185;&#23398;&#23548;&#24072;
&lt;/p&gt;
&lt;p&gt;
Language Models as Science Tutors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11111
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;TutorEval&#21644;TutorChat&#65292;&#36890;&#36807;TutorEval&#22522;&#20934;&#21487;&#20197;&#34913;&#37327;LMs&#20316;&#20026;&#31185;&#23398;&#21161;&#25163;&#30340;&#23454;&#38469;&#21487;&#29992;&#24615;&#65292;TutorChat&#25968;&#25454;&#38598;&#29992;&#20110;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#26368;&#36817;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#65292;&#26397;&#30528;&#35757;&#32451;&#20855;&#26377;&#36739;&#24378;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#26041;&#21521;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#21457;&#23637;&#24182;&#27809;&#26377;&#19987;&#27880;&#20110;LMs&#22312;&#31185;&#23398;&#25945;&#32946;&#20013;&#30340;&#23454;&#38469;&#29992;&#20363;&#65292;&#21253;&#25324;&#38656;&#35201;&#22788;&#29702;&#38271;&#31687;&#31185;&#23398;&#25991;&#26723;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TutorEval&#21644;TutorChat&#12290;TutorEval&#26159;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#38382;&#31572;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#26377;&#20851;STEM&#25945;&#31185;&#20070;&#38271;&#31687;&#31456;&#33410;&#30340;&#38382;&#39064;&#65292;&#30001;&#19987;&#23478;&#32534;&#20889;&#12290;TutorEval&#26377;&#21161;&#20110;&#34913;&#37327;LMs&#20316;&#20026;&#31185;&#23398;&#21161;&#25163;&#30340;&#23454;&#38469;&#21487;&#29992;&#24615;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#38271;&#19978;&#19979;&#25991;&#12289;&#33258;&#30001;&#29983;&#25104;&#21644;&#22810;&#23398;&#31185;&#31185;&#23398;&#30693;&#35782;&#30340;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#29616;&#26377;&#23545;&#35805;&#25968;&#25454;&#38598;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20250;&#23548;&#33268;TutorEval&#24615;&#33021;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;TutorChat&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;80,000&#20010;&#20851;&#20110;&#25945;&#31185;&#20070;&#30340;&#38271;&#21512;&#25104;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;TutorChat&#26469;&#24494;&#35843;Llemma&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11111v1 Announce Type: cross  Abstract: NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models wit
&lt;/p&gt;</description></item><item><title>ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.09727</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#20154;&#24037;&#26234;&#33021;&#38405;&#35835;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09727
&lt;/p&gt;
&lt;p&gt;
ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#38480;&#21046;&#22312;&#26576;&#20010;&#26368;&#22823;&#19978;&#19979;&#25991;&#38271;&#24230;&#20869;&#65292;&#32780;&#19988;&#26080;&#27861;&#31283;&#23450;&#22320;&#22788;&#29702;&#38271;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReadAgent&#65292;&#19968;&#20010;&#22686;&#21152;&#20102;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#31995;&#32479;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#21487;&#20197;&#36798;&#21040;20&#20493;&#12290;&#21463;&#21040;&#20154;&#31867;&#20132;&#20114;&#24335;&#38405;&#35835;&#38271;&#25991;&#26723;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;ReadAgent&#23454;&#29616;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#21033;&#29992;LLM&#30340;&#39640;&#32423;&#35821;&#35328;&#33021;&#21147;&#26469;&#65306;&#65288;1&#65289;&#20915;&#23450;&#23558;&#21738;&#20123;&#20869;&#23481;&#23384;&#20648;&#22312;&#19968;&#20010;&#35760;&#24518;&#29255;&#27573;&#20013;&#65292;&#65288;2&#65289;&#23558;&#36825;&#20123;&#35760;&#24518;&#29255;&#27573;&#21387;&#32553;&#25104;&#20026;&#31216;&#20026;&#27010;&#35201;&#35760;&#24518;&#30340;&#30701;&#26102;&#35760;&#24518;&#65292;&#65288;3&#65289;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#21407;&#22987;&#25991;&#26412;&#26597;&#25214;&#27573;&#33853;&#26469;&#25552;&#37266;&#33258;&#24049;&#30456;&#20851;&#32454;&#33410;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#12289;&#20351;&#29992;&#21407;&#22987;&#38271;&#19978;&#19979;&#25991;&#20197;&#21450;&#20351;&#29992;&#27010;&#35201;&#35760;&#24518;&#26469;&#35780;&#20272;ReadAgent&#19982;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#35780;&#20272;&#26159;&#22312;&#19977;&#20010;&#38271;&#25991;&#26723;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09727v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension task
&lt;/p&gt;</description></item><item><title>MAPLE&#36890;&#36807;&#22312;&#20004;&#20010;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#23545;LLama-2-7B&#21644;Mistral-7B&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20845;&#39033;&#28085;&#30422;40&#31181;&#35821;&#35328;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#24494;&#35843;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2401.07598</link><description>&lt;p&gt;
MAPLE: &#22810;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07598
&lt;/p&gt;
&lt;p&gt;
MAPLE&#36890;&#36807;&#22312;&#20004;&#20010;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#23545;LLama-2-7B&#21644;Mistral-7B&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20845;&#39033;&#28085;&#30422;40&#31181;&#35821;&#35328;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#24494;&#35843;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Finetuning (PEFT)&#24050;&#32463;&#25104;&#20026;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#36164;&#28304;&#21644;&#35745;&#31639;&#12290;&#26412;&#25991;&#22312;&#20004;&#20010;&#21512;&#25104;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;LLama-2-7B&#21644;Mistral-7B&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#20854;&#23545;&#20845;&#20010;&#28085;&#30422;&#22235;&#21313;&#31181;&#35821;&#35328;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#19981;&#21516;&#30340;&#21442;&#25968;&#65292;&#20363;&#22914;&#29992;&#20110;&#20302;&#31209;&#36866;&#24212;&#30340;&#31209;&#21644;&#37327;&#21270;&#20540;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#19979;&#28216;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26356;&#39640;&#30340;&#31209;&#21644;&#26356;&#39640;&#30340;hig
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07598v2 Announce Type: replace  Abstract: Parameter Efficient Finetuning (PEFT) has emerged as a viable solution for improving the performance of Large Language Models (LLMs) without requiring massive resources and compute. Prior work on multilingual evaluation has shown that there is a large gap between the performance of LLMs on English and other languages. Further, there is also a large gap between the performance of smaller open-source models and larger LLMs. Finetuning can be an effective way to bridge this gap and make language models more equitable. In this work, we finetune the LLama-2-7B and Mistral-7B models on two synthetic multilingual instruction tuning datasets to determine its effect on model performance on six downstream tasks covering forty languages in all. Additionally, we experiment with various parameters, such as rank for low-rank adaptation and values of quantisation to determine their effects on downstream performance and find that higher rank and hig
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.15964</link><description>&lt;p&gt;
&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Pre-training for Localized Instruction Generation of Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35270;&#39057;&#23637;&#31034;&#20102;&#35832;&#22914;&#39135;&#35889;&#20934;&#22791;&#31561;&#20219;&#21153;&#30340;&#36880;&#27493;&#28436;&#31034;&#12290;&#29702;&#35299;&#27492;&#31867;&#35270;&#39057;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#23545;&#27493;&#39588;&#36827;&#34892;&#31934;&#30830;&#23450;&#20301;&#24182;&#29983;&#25104;&#25991;&#23383;&#35828;&#26126;&#12290;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#24182;&#32534;&#20889;&#35828;&#26126;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#38480;&#21046;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#24182;&#38459;&#30861;&#20102;&#26377;&#25928;&#23398;&#20064;&#12290;&#21033;&#29992;&#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#30340;&#35270;&#39057;-&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#36716;&#24405;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#65292;&#19982;&#20154;&#31867;&#27880;&#37322;&#21592;&#32534;&#20889;&#30340;&#35828;&#26126;&#30456;&#27604;&#23384;&#22312;&#39118;&#26684;&#21464;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;Sieve-&amp;-Swap&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#21644;&#20351;&#29992;&#25991;&#26412;&#39135;&#35889;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#33258;&#21160;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#20197;&#22686;&#24378;&#25991;&#23383;&#25351;&#20196;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&amp;-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#36923;&#36753;&#22238;&#24402;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33609;&#22270;&#24341;&#23548;&#32422;&#26463;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#36741;&#21161;&#27169;&#22411;&#20248;&#21270;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#21021;&#27493;&#36755;&#20986;&#20316;&#20026;&#36827;&#19968;&#27493;&#25193;&#23637;&#30340; "&#33609;&#22270;"&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26377;&#38480;&#32422;&#26463;&#35299;&#30721;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09967</link><description>&lt;p&gt;
&#26080;&#38656;&#35775;&#38382;&#36923;&#36753;&#22238;&#24402;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33609;&#22270;&#24341;&#23548;&#32422;&#26463;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access. (arXiv:2401.09967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#36923;&#36753;&#22238;&#24402;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33609;&#22270;&#24341;&#23548;&#32422;&#26463;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#36741;&#21161;&#27169;&#22411;&#20248;&#21270;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#21021;&#27493;&#36755;&#20986;&#20316;&#20026;&#36827;&#19968;&#27493;&#25193;&#23637;&#30340; "&#33609;&#22270;"&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26377;&#38480;&#32422;&#26463;&#35299;&#30721;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#32422;&#26463;&#22312;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#25511;&#21046;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#26550;&#26500;&#20462;&#25913;&#30340;&#26041;&#24335;&#65292;&#20294;&#36890;&#24120;&#21482;&#36866;&#29992;&#20110;&#25317;&#26377;&#36923;&#36753;&#22238;&#24402;&#35775;&#38382;&#26435;&#38480;&#30340;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33609;&#22270;&#24341;&#23548;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32422;&#26463;&#35299;&#30721;&#65288;SGCD&#65289;&#26041;&#27861;&#65292;&#26080;&#38656;&#35775;&#38382;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#22238;&#24402;&#12290;SGCD&#21033;&#29992;&#26412;&#22320;&#36741;&#21161;&#27169;&#22411;&#26469;&#20248;&#21270;&#26080;&#32422;&#26463;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23558;&#20854;&#20316;&#20026;&#36827;&#19968;&#27493;&#25193;&#23637;&#30340;&#8220;&#33609;&#22270;&#8221;&#12290;&#27492;&#26041;&#27861;&#21487;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#36923;&#36753;&#22238;&#24402;&#30340;&#25216;&#26415;&#30456;&#20114;&#34917;&#20805;&#65292;&#20351;&#26377;&#38480;&#32422;&#26463;&#35299;&#30721;&#22312;&#26080;&#27861;&#23436;&#20840;&#36879;&#26126;&#30340;&#27169;&#22411;&#29615;&#22659;&#20013;&#24212;&#29992;&#12290;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;SGCD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constrained decoding, a technique for enforcing constraints on language model outputs, offers a way to control text generation without retraining or architectural modifications. Its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a novel approach to constrained decoding for blackbox LLMs, which operates without access to the logits of the blackbox LLM. SGCD utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, effectively treating this initial output as a "sketch" for further elaboration. This approach is complementary to traditional logit-based techniques and enables the application of constrained decoding in settings where full model transparency is unavailable. We demonstrate the efficacy of SGCD through experiments in cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#65288;AlignInstruct&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#35789;&#23545;&#40784;&#26500;&#24314;&#30340;&#36328;&#35821;&#35328;&#37492;&#21035;&#22120;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30417;&#30563;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#23558;&#25903;&#25345;&#30340;&#35821;&#35328;&#25193;&#23637;&#21040;&#26410;&#30693;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25968;&#25454;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#36890;&#36807;MTInstruct&#21487;&#20197;&#26377;&#25928;&#22320;&#32763;&#35793;&#26410;&#30693;&#35821;&#35328;&#65292;&#24182;&#19988;&#20351;&#29992;AlignInstruct&#22312;&#28041;&#21450;&#33521;&#35821;&#30340;48&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#12290;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#25351;&#20196;&#20248;&#20110;&#29983;&#25104;&#22411;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2401.05811</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26410;&#30693;&#12289;&#20302;&#36164;&#28304;&#35821;&#35328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages. (arXiv:2401.05811v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#65288;AlignInstruct&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#35789;&#23545;&#40784;&#26500;&#24314;&#30340;&#36328;&#35821;&#35328;&#37492;&#21035;&#22120;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30417;&#30563;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#23558;&#25903;&#25345;&#30340;&#35821;&#35328;&#25193;&#23637;&#21040;&#26410;&#30693;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25968;&#25454;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#36890;&#36807;MTInstruct&#21487;&#20197;&#26377;&#25928;&#22320;&#32763;&#35793;&#26410;&#30693;&#35821;&#35328;&#65292;&#24182;&#19988;&#20351;&#29992;AlignInstruct&#22312;&#28041;&#21450;&#33521;&#35821;&#30340;48&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#12290;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#25351;&#20196;&#20248;&#20110;&#29983;&#25104;&#22411;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#65288;AlignInstruct&#65289;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#20010;&#26159;&#23558;&#25903;&#25345;&#30340;&#35821;&#35328;&#25193;&#23637;&#21040;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#12290;&#31532;&#20108;&#20010;&#19982;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#32570;&#20047;&#25968;&#25454;&#26377;&#20851;&#12290;&#36890;&#36807;MT&#25351;&#20196;&#65288;MTInstruct&#65289;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26159;&#24212;&#23545;&#31532;&#19968;&#20010;&#25361;&#25112;&#30340;&#19968;&#31181;&#30452;&#25509;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;MTInstruct&#21463;&#21040;&#31532;&#20108;&#20010;&#25361;&#25112;&#20013;&#22266;&#26377;&#30340;&#24369;&#35821;&#35328;&#36328;&#24230;&#20449;&#21495;&#30340;&#38480;&#21046;&#12290;AlignInstruct&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32479;&#35745;&#35789;&#23545;&#40784;&#26500;&#24314;&#30340;&#36328;&#35821;&#35328;&#37492;&#21035;&#22120;&#26469;&#24378;&#35843;&#36328;&#35821;&#35328;&#30417;&#30563;&#12290;&#25105;&#20204;&#22522;&#20110;&#22312;&#22810;&#36798;24&#31181;&#26410;&#30693;&#35821;&#35328;&#19978;&#23545;BLOOMZ&#27169;&#22411;&#65288;1b1&#12289;3b&#21644;7b1&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;LLMs&#21487;&#20197;&#20351;&#29992;MTInstruct&#26377;&#25928;&#22320;&#32763;&#35793;&#26410;&#30693;&#35821;&#35328;&#65307;&#65288;2&#65289;AlignInstruct&#22312;&#28041;&#21450;&#33521;&#35821;&#30340;48&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;3&#65289;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#25351;&#20196;&#20248;&#20110;&#29983;&#25104;&#22411;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces contrastive alignment instructions (AlignInstruct) to address two challenges in machine translation (MT) on large language models (LLMs). One is the expansion of supported languages to previously unseen ones. The second relates to the lack of data in low-resource languages. Model fine-tuning through MT instructions (MTInstruct) is a straightforward approach to the first challenge. However, MTInstruct is limited by weak cross-lingual signals inherent in the second challenge. AlignInstruct emphasizes cross-lingual supervision via a cross-lingual discriminator built using statistical word alignments. Our results based on fine-tuning the BLOOMZ models (1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can effectively translate unseen languages using MTInstruct; (2) AlignInstruct led to consistent improvements in translation quality across 48 translation directions involving English; (3) Discriminator-based instructions outperformed their generativ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Cherry&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25214;&#20986;&#30446;&#26631;&#26032;&#38395;&#25253;&#36947;&#20013;&#32570;&#22833;&#30340;&#37325;&#35201;&#38472;&#36848;&#26469;&#33258;&#21160;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#25688;&#36873;&#38472;&#36848;&#12290;Cherry&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#26032;&#38395;&#25253;&#36947;&#20998;&#26512;&#26469;&#35782;&#21035;&#25688;&#36873;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2401.05650</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#25688;&#36873;
&lt;/p&gt;
&lt;p&gt;
On Detecting Cherry-picking in News Coverage Using Large Language Models. (arXiv:2401.05650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Cherry&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25214;&#20986;&#30446;&#26631;&#26032;&#38395;&#25253;&#36947;&#20013;&#32570;&#22833;&#30340;&#37325;&#35201;&#38472;&#36848;&#26469;&#33258;&#21160;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#25688;&#36873;&#38472;&#36848;&#12290;Cherry&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#26032;&#38395;&#25253;&#36947;&#20998;&#26512;&#26469;&#35782;&#21035;&#25688;&#36873;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#36873;&#26159;&#25351;&#26377;&#24847;&#36873;&#25321;&#26377;&#21033;&#20110;&#29305;&#23450;&#35266;&#28857;&#30340;&#35777;&#25454;&#25110;&#20107;&#23454;&#65292;&#21516;&#26102;&#24573;&#35270;&#25110;&#25197;&#26354;&#25903;&#25345;&#30456;&#21453;&#35266;&#28857;&#30340;&#35777;&#25454;&#12290;&#22312;&#26032;&#38395;&#25253;&#36947;&#20013;&#25163;&#21160;&#35782;&#21035;&#25688;&#36873;&#38472;&#36848;&#21487;&#33021;&#20250;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#30456;&#21453;&#35266;&#28857;&#30340;&#25253;&#36947;&#32570;&#22833;&#26102;&#12290;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Cherry&#65292;&#29992;&#20110;&#36890;&#36807;&#25214;&#20986;&#30446;&#26631;&#26032;&#38395;&#25253;&#36947;&#20013;&#32570;&#22833;&#30340;&#37325;&#35201;&#38472;&#36848;&#26469;&#33258;&#21160;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#25688;&#36873;&#38472;&#36848;&#12290;Cherry&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#26032;&#38395;&#25253;&#36947;&#20998;&#26512;&#26469;&#35782;&#21035;&#25688;&#36873;&#23454;&#20363;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#32771;&#34385;&#26469;&#33258;&#20854;&#20182;&#26032;&#38395;&#26469;&#28304;&#30340;&#35821;&#22659;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#38472;&#36848;&#23545;&#30446;&#26631;&#26032;&#38395;&#25253;&#36947;&#25152;&#28085;&#30422;&#20107;&#20214;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#25688;&#36873;&#26816;&#27979;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cherry-picking refers to the deliberate selection of evidence or facts that favor a particular viewpoint while ignoring or distorting evidence that supports an opposing perspective. Manually identifying instances of cherry-picked statements in news stories can be challenging, particularly when the opposing viewpoint's story is absent. This study introduces Cherry, an innovative approach for automatically detecting cherry-picked statements in news articles by finding missing important statements in the target news story. Cherry utilizes the analysis of news coverage from multiple sources to identify instances of cherry-picking. Our approach relies on language models that consider contextual information from other news sources to classify statements based on their importance to the event covered in the target news story. Furthermore, this research introduces a novel dataset specifically designed for cherry-picking detection, which was used to train and evaluate the performance of the mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#36890;&#36807;&#19982;SOT&#32467;&#21512;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#27604;SIMO&#22522;&#20934;&#27169;&#22411;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20998;&#21035;&#38477;&#20302;&#20102;8%&#21644;10%&#12290;</title><link>http://arxiv.org/abs/2401.04152</link><description>&lt;p&gt;
&#36328;&#35828;&#35805;&#20154;&#32534;&#30721;&#32593;&#32476;&#29992;&#20110;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Speaker Encoding Network for Multi-Talker Speech Recognition. (arXiv:2401.04152v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#36890;&#36807;&#19982;SOT&#32467;&#21512;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#27604;SIMO&#22522;&#20934;&#27169;&#22411;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20998;&#21035;&#38477;&#20302;&#20102;8%&#21644;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#20316;&#20026;&#19968;&#31181;&#30452;&#25509;&#36716;&#24405;&#22810;&#20010;&#35828;&#35805;&#20154;&#37325;&#21472;&#35821;&#38899;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;1&#65289;&#24102;&#26377;&#20998;&#25903;&#32534;&#30721;&#22120;&#30340;&#21333;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;SIMO&#65289;&#27169;&#22411;&#65292;&#25110;&#32773;2&#65289;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#65288;SOT&#65289;&#30340;&#21333;&#36755;&#20837;&#21333;&#36755;&#20986;&#65288;SISO&#65289;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#26469;&#35299;&#20915;SIMO&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;CSE&#27169;&#22411;&#19982;SOT&#30456;&#32467;&#21512;&#65292;&#26082;&#21457;&#25381;&#20102;SIMO&#21644;SISO&#30340;&#20248;&#21183;&#65292;&#21448;&#32531;&#35299;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#35813;&#24037;&#20316;&#20195;&#34920;&#20102;&#23558;SIMO&#21644;SISO&#38598;&#25104;&#21040;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#26089;&#26399;&#24037;&#20316;&#12290;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;LibrispeechMix&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CES&#27169;&#22411;&#30456;&#27604;&#20110;SIMO&#22522;&#20934;&#27169;&#22411;&#23558;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#20102;8%&#12290;CSE-SOT&#27169;&#22411;&#23558;WER&#38477;&#20302;&#20102;10%
&lt;/p&gt;
&lt;p&gt;
End-to-end multi-talker speech recognition has garnered great interest as an effective approach to directly transcribe overlapped speech from multiple speakers. Current methods typically adopt either 1) single-input multiple-output (SIMO) models with a branched encoder, or 2) single-input single-output (SISO) models based on attention-based encoder-decoder architecture with serialized output training (SOT). In this work, we propose a Cross-Speaker Encoding (CSE) network to address the limitations of SIMO models by aggregating cross-speaker representations. Furthermore, the CSE model is integrated with SOT to leverage both the advantages of SIMO and SISO while mitigating their drawbacks. To the best of our knowledge, this work represents an early effort to integrate SIMO and SISO for multi-talker speech recognition. Experiments on the two-speaker LibrispeechMix dataset show that the CES model reduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model reduces WER by 10
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#31215;&#31639;&#27861;(ADMM)&#30340;&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#23618;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#65292;&#32467;&#21512;&#31616;&#21333;&#30340;&#36845;&#20195;&#20462;&#21098;&#25513;&#30721;&#36873;&#25321;&#65292;&#22312;&#24191;&#27867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02938</link><description>&lt;p&gt;
&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast and Optimal Weight Update for Pruned Large Language Models. (arXiv:2401.02938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#31215;&#31639;&#27861;(ADMM)&#30340;&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#23618;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#65292;&#32467;&#21512;&#31616;&#21333;&#30340;&#36845;&#20195;&#20462;&#21098;&#25513;&#30721;&#36873;&#25321;&#65292;&#22312;&#24191;&#27867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35268;&#27169;&#24222;&#22823;&#12290;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#24494;&#35843;&#65292;&#36825;&#26159;&#20026;&#20102;&#24674;&#22797;&#22240;&#21024;&#38500;&#26435;&#37325;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#35201;&#20040;&#23436;&#20840;&#24573;&#30053;&#20102;&#24494;&#35843;&#65292;&#19987;&#27880;&#20110;&#39640;&#25928;&#30340;&#20462;&#21098;&#26631;&#20934;&#65292;&#35201;&#20040;&#23581;&#35797;&#36880;&#23618;&#26435;&#37325;&#26356;&#26032;&#65292;&#20445;&#25345;&#27599;&#20010;&#23618;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#36880;&#23618;&#26435;&#37325;&#26356;&#26032;&#23545;LLMs&#26469;&#35828;&#20063;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#24471;&#19981;&#37319;&#29992;&#21508;&#31181;&#36817;&#20284;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#31215;&#31639;&#27861;(ADMM)&#30340;&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#23618;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#12290;&#32467;&#21512;&#31616;&#21333;&#30340;&#36845;&#20195;&#20462;&#21098;&#25513;&#30721;&#36873;&#25321;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24191;&#27867;&#30340;LLMs&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/fmfi-compbio/admm-pruning&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations.  In our paper, we propose a fast and optimal weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). Coupled with a simple iterative pruning mask selection, our algorithm achieves state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#21253;&#25324;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.17432</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Video Understanding with Large Language Models: A Survey. (arXiv:2312.17432v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17432
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#21253;&#25324;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#35270;&#39057;&#24179;&#21488;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#19981;&#26029;&#22686;&#22810;&#65292;&#23545;&#29087;&#32451;&#30340;&#35270;&#39057;&#29702;&#35299;&#24037;&#20855;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#21152;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#25216;&#26415;&#36827;&#34892;&#35270;&#39057;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#20196;&#20154;&#24778;&#35766;&#65292;&#23588;&#20854;&#26159;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#26412;&#35843;&#26597;&#23545;Vid-LLMs&#30340;&#29420;&#29305;&#29305;&#28857;&#21644;&#33021;&#21147;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20998;&#20026;&#22235;&#31181;&#20027;&#35201;&#31867;&#22411;&#65306;&#22522;&#20110;LLM&#30340;&#35270;&#39057;&#20195;&#29702;&#12289;Vid-LLMs&#30340;&#39044;&#35757;&#32451;&#12289;Vid-LLMs&#30340;&#25351;&#20196;&#35843;&#25972;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#26597;&#23545;Vid-LLMs&#30340;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#21478;&#22806;&#65292;&#23427;&#36824;&#25506;&#35752;&#20102;Vid-LLMs&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of Large Language Models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of the recent advancements in video understanding harnessing the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into four main types: LLM-based Video Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods. Furthermore, this survey presents a comprehensive study of the tasks, datasets, and evaluation methodologies for Vid-LLMs. Additionally, it explores 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20204</link><description>&lt;p&gt;
&#21033;&#29992;&#36817;&#26080;&#38480;&#21382;&#21490;&#30340;&#36890;&#29992;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History. (arXiv:2310.20204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20204
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#24320;&#21457;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#27515;&#20129;&#39044;&#27979;&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#19987;&#23478;&#24847;&#35265;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#35843;&#25972;&#35266;&#27979;&#31383;&#21475;&#22823;&#23567;&#12290;&#36825;&#32473;&#19987;&#23478;&#24102;&#26469;&#36127;&#25285;&#24182;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#36896;&#25104;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65288;REMed&#65289;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;REMed&#21487;&#20197;&#22522;&#26412;&#35780;&#20272;&#26080;&#38480;&#37327;&#30340;&#20020;&#24202;&#20107;&#20214;&#65292;&#36873;&#25321;&#30456;&#20851;&#30340;&#20107;&#20214;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#23454;&#26102;&#35266;&#23519;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;27&#20010;&#20020;&#24202;&#20219;&#21153;&#21644;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;EHR&#25968;&#25454;&#38598;&#30340;&#29420;&#31435;&#38431;&#21015;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#29305;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;REMed&#20248;&#20110;&#20854;&#20182;&#29616;&#20195;&#26550;&#26500;&#65292;&#23427;&#20204;&#26088;&#22312;&#22788;&#29702;&#23613;&#21487;&#33021;&#22810;&#30340;&#20107;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;REMed&#30340;&#20559;&#22909;&#19982;&#21307;&#23398;&#19987;&#23478;&#30340;&#20559;&#22909;&#23494;&#20999;&#30456;&#20284;&#12290;&#25105;&#20204;&#26399;&#26395;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#26174;&#33879;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.13447</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Multiscale Superpixel Structured Difference Graph Convolutional Network for VL Representation. (arXiv:2310.13447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20013;&#65292;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20851;&#38190;&#22312;&#20110;&#24314;&#31435;&#19968;&#20010;&#33391;&#22909;&#30340;&#23545;&#40784;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35821;&#20041;&#34920;&#24449;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#24403;&#21069;&#22522;&#20110;&#20687;&#32032;&#25110;&#22359;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#25552;&#21462;&#22797;&#26434;&#22330;&#26223;&#36793;&#30028;&#26041;&#38754;&#23384;&#22312;&#31354;&#38388;&#35821;&#20041;&#36830;&#36143;&#24615;&#19981;&#36275;&#21644;&#23545;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#23558;&#36229;&#20687;&#32032;&#20316;&#20026;&#21487;&#23398;&#20064;&#22270;&#20687;&#25968;&#25454;&#30340;&#32508;&#21512;&#32039;&#20945;&#34920;&#24449;&#65292;&#36890;&#36807;&#23545;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#36827;&#34892;&#32858;&#31867;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#12290;&#20026;&#20102;&#25366;&#25496;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#12290;&#23427;&#23558;&#25972;&#20010;&#22270;&#20687;&#35299;&#26512;&#20026;&#32454;&#21040;&#31895;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25972;&#20010;&#22270;&#20687;&#30340;&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the multimodal field, the key to integrating vision and language lies in establishing a good alignment strategy. Recently, benefiting from the success of self-supervised learning, significant progress has been made in multimodal semantic representation based on pre-trained models for vision and language. However, there is still room for improvement in visual semantic representation. The lack of spatial semantic coherence and vulnerability to noise makes it challenging for current pixel or patch-based methods to accurately extract complex scene boundaries. To this end, this paper develops superpixel as a comprehensive compact representation of learnable image data, which effectively reduces the number of visual primitives for subsequent processing by clustering perceptually similar pixels. To mine more precise topological relations, we propose a Multiscale Difference Graph Convolutional Network (MDGCN). It parses the entire image as a fine-to-coarse hierarchical structure of cons
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#35780;&#20998;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#20154;&#31867;&#24494;&#22937;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;LLMs&#23545;&#38544;&#21947;&#29702;&#35299;&#33021;&#21147;&#26377;&#25152;&#25913;&#21892;&#65292;&#20294;&#23545;&#35773;&#21050;&#29702;&#35299;&#33021;&#21147;&#30340;&#25913;&#36827;&#24182;&#26410;&#35266;&#23519;&#21040;&#12290;</title><link>http://arxiv.org/abs/2309.10744</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#31579;&#36873;&#27979;&#35797;&#29702;&#35299;&#38544;&#21947;&#21644;&#35773;&#21050;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language models' ability to understand metaphor and sarcasm using a screening test for Asperger syndrome. (arXiv:2309.10744v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#35780;&#20998;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#20154;&#31867;&#24494;&#22937;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;LLMs&#23545;&#38544;&#21947;&#29702;&#35299;&#33021;&#21147;&#26377;&#25152;&#25913;&#21892;&#65292;&#20294;&#23545;&#35773;&#21050;&#29702;&#35299;&#33021;&#21147;&#30340;&#25913;&#36827;&#24182;&#26410;&#35266;&#23519;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#21644;&#35773;&#21050;&#26159;&#25105;&#20204;&#39640;&#24230;&#36827;&#21270;&#30340;&#31038;&#20132;&#27807;&#36890;&#25216;&#24039;&#30340;&#29645;&#36149;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#30340;&#20799;&#31461;&#20247;&#25152;&#21608;&#30693;&#22312;&#29702;&#35299;&#35773;&#21050;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#20182;&#20204;&#20855;&#26377;&#36275;&#22815;&#29702;&#35299;&#38544;&#21947;&#30340;&#21475;&#35821;&#26234;&#21830;&#27700;&#24179;&#12290;&#37492;&#20110;&#27492;&#65292;&#24050;&#32463;&#20351;&#29992;&#20102;&#19968;&#20010;&#35780;&#20998;&#27979;&#35797;&#26469;&#35780;&#20272;&#29702;&#35299;&#38544;&#21947;&#21644;&#35773;&#21050;&#30340;&#33021;&#21147;&#65292;&#20197;&#21306;&#20998;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#21644;&#20854;&#20182;&#34920;&#29616;&#30456;&#20284;&#22806;&#37096;&#34892;&#20026;&#30340;&#30151;&#29366;&#65288;&#20363;&#22914;&#27880;&#24847;&#21147;&#32570;&#38519;/&#22810;&#21160;&#38556;&#30861;&#65289;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26631;&#20934;&#21270;&#27979;&#35797;&#26469;&#30740;&#31350;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#20154;&#31867;&#24494;&#22937;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#29702;&#35299;&#38544;&#21947;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#20294;&#24182;&#27809;&#26377;&#35266;&#23519;&#21040;&#23545;&#35773;&#21050;&#29702;&#35299;&#30340;&#25913;&#36827;&#12290;&#36825;&#24847;&#21619;&#30528;&#26377;&#24517;&#35201;&#37319;&#21462;&#20854;&#20182;&#26041;&#27861;&#26469;&#20351;LLMs&#20855;&#22791;&#29702;&#35299;&#35773;&#21050;&#30340;&#33021;&#21147;&#65292;&#36825;&#24050;&#19982;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaphors and sarcasm are precious fruits of our highly-evolved social communication skills. However, children with Asperger syndrome are known to have difficulties in comprehending sarcasm, even if they possess a certain level of verbal IQ sufficient for understanding metaphors. Given that, a screening test that scores the ability to understand metaphor and sarcasm has been used to differentiate Asperger syndrome from other symptoms exhibiting akin external behaviors (e.g., attention-deficit/hyperactivity disorder). This study uses the standardized test to examine the capability of recent large language models (LLMs) in understanding human nuanced communication. The results divulged that, whereas their ability to comprehend metaphors has been improved with the increase of the number of model parameters, the improvement in sarcasm understanding was not observed. This implies that an alternative approach is imperative to imbue LLMs with the capacity to grasp sarcasm, which has been asso
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.00378</link><description>&lt;p&gt;
&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Long-Term Memorability On Advertisements. (arXiv:2309.00378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#33457;&#36153;&#25968;&#21313;&#20159;&#32654;&#20803;&#22312;&#24191;&#21578;&#19978;&#65292;&#20294;&#26159;&#25237;&#20837;&#21040;&#24191;&#21578;&#19978;&#30340;&#37329;&#38065;&#33021;&#36215;&#22810;&#22823;&#20316;&#29992;&#21602;&#65311;&#24403;&#39038;&#23458;&#22312;&#36141;&#20080;&#26102;&#26080;&#27861;&#36776;&#35748;&#20986;&#20182;&#20204;&#30475;&#36807;&#30340;&#21697;&#29260;&#30340;&#35805;&#65292;&#33457;&#22312;&#24191;&#21578;&#19978;&#30340;&#38065;&#22522;&#26412;&#19978;&#23601;&#34987;&#28010;&#36153;&#20102;&#12290;&#23613;&#31649;&#22312;&#33829;&#38144;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#20851;&#20110;&#24191;&#21578;&#35760;&#24518;&#21147;&#30340;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#23545;&#29305;&#23450;&#20869;&#23481;&#31867;&#22411;&#65288;&#22914;&#29289;&#20307;&#21644;&#21160;&#20316;&#35270;&#39057;&#65289;&#36827;&#34892;&#30701;&#26399;&#22238;&#24518;&#65288;&lt;5&#20998;&#38047;&#65289;&#30340;&#30740;&#31350;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24191;&#21578;&#34892;&#19994;&#21482;&#20851;&#24515;&#38271;&#26399;&#35760;&#24518;&#65288;&#20960;&#20010;&#23567;&#26102;&#25110;&#26356;&#38271;&#26102;&#38388;&#65289;&#65292;&#32780;&#19988;&#24191;&#21578;&#20960;&#20046;&#24635;&#26159;&#39640;&#24230;&#22810;&#27169;&#24335;&#21270;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#24418;&#24335;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26469;&#35762;&#25925;&#20107;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#26426;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#20010;&#22823;&#35268;&#27169;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#20849;&#26377;1203&#21517;&#21442;&#19982;&#32773;&#21644;2205&#20010;&#24191;&#21578;&#28085;&#30422;&#20102;276&#20010;&#21697;&#29260;&#12290;&#22312;&#19981;&#21516;&#21442;&#19982;&#32773;&#23376;&#32676;&#20307;&#21644;&#24191;&#21578;&#31867;&#22411;&#19978;&#36827;&#34892;&#32479;&#35745;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#35768;&#22810;&#26377;&#20851;&#20160;&#20040;&#20351;&#24191;&#21578;&#38590;&#24536;&#30340;&#26377;&#36259;&#35265;&#35299;-&#26080;&#35770;&#26159;&#20869;&#23481;&#36824;&#26159;
&lt;/p&gt;
&lt;p&gt;
Marketers spend billions of dollars on advertisements but to what end? At the purchase time, if customers cannot recognize a brand for which they saw an ad, the money spent on the ad is essentially wasted. Despite its importance in marketing, until now, there has been no study on the memorability of ads in the ML literature. Most studies have been conducted on short-term recall (&lt;5 mins) on specific content types like object and action videos. On the other hand, the advertising industry only cares about long-term memorability (a few hours or longer), and advertisements are almost always highly multimodal, depicting a story through its different modalities (text, images, and videos). With this motivation, we conduct the first large scale memorability study consisting of 1203 participants and 2205 ads covering 276 brands. Running statistical tests over different participant subpopulations and ad-types, we find many interesting insights into what makes an ad memorable - both content and h
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Retrieval-Pretrained Transformer&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#32852;&#21512;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22120;&#26469;&#27169;&#25311;&#38271;&#25991;&#26412;&#12290;&#27169;&#22411;&#21487;&#20197;&#35745;&#31639;&#25991;&#26412;&#22359;&#30340;&#26597;&#35810;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26816;&#32034;&#21069;&#38754;&#30340;&#22359;&#65292;&#20174;&#32780;&#34701;&#21512;&#20449;&#24687;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#30446;&#26631;&#22359;&#12290;&#26816;&#32034;&#22120;&#20351;&#29992;&#19968;&#20010;&#35821;&#20041;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#30446;&#26631;&#26159;&#26816;&#32034;&#37027;&#20123;&#22686;&#21152;&#19979;&#19968;&#20010;&#22359;&#27010;&#29575;&#30340;&#22359;&#12290;</title><link>http://arxiv.org/abs/2306.13421</link><description>&lt;p&gt;
&#33258;&#26816;&#32034;&#30340;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Long-range Language Modeling with Self-retrieval. (arXiv:2306.13421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Retrieval-Pretrained Transformer&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#32852;&#21512;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22120;&#26469;&#27169;&#25311;&#38271;&#25991;&#26412;&#12290;&#27169;&#22411;&#21487;&#20197;&#35745;&#31639;&#25991;&#26412;&#22359;&#30340;&#26597;&#35810;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26816;&#32034;&#21069;&#38754;&#30340;&#22359;&#65292;&#20174;&#32780;&#34701;&#21512;&#20449;&#24687;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#30446;&#26631;&#22359;&#12290;&#26816;&#32034;&#22120;&#20351;&#29992;&#19968;&#20010;&#35821;&#20041;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#30446;&#26631;&#26159;&#26816;&#32034;&#37027;&#20123;&#22686;&#21152;&#19979;&#19968;&#20010;&#22359;&#27010;&#29575;&#30340;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22522;&#20110;&#26816;&#32034;&#36741;&#21161;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20294;&#26159;&#65292;&#36890;&#24120;&#26816;&#32034;&#22120;&#24182;&#19981;&#26159;&#20316;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#22320;&#32452;&#20214;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#30340;&#65292;&#32780;&#26159;&#34987;&#28155;&#21152;&#21040;&#24050;&#32463;&#39044;&#35757;&#32451;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36825;&#38480;&#21046;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22120;&#30456;&#20114;&#36866;&#24212;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Retrieval-Pretrained Transformer (RPT)&#65292;&#19968;&#31181;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#26816;&#32034;&#36741;&#21161;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#38271;&#25991;&#26412;&#12290;&#32473;&#23450;&#19968;&#20010;&#26368;&#36817;&#22312;&#38271;&#25991;&#26723;&#20013;&#29983;&#25104;&#30340;&#25991;&#26412;&#22359;&#65292;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#26597;&#35810;&#34920;&#31034;&#65292;&#28982;&#21518;&#29992;&#23427;&#26469;&#26816;&#32034;&#25991;&#26723;&#20013;&#26356;&#26089;&#30340;&#22359;&#65292;&#36825;&#20123;&#22359;&#21487;&#33021;&#36328;&#36234;&#25968;&#19975;&#20010;&#26631;&#35760;&#12290;&#26816;&#32034;&#21040;&#30340;&#22359;&#20013;&#30340;&#20449;&#24687;&#34987;&#34701;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#20013;&#65292;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#30446;&#26631;&#22359;&#12290;&#25105;&#20204;&#29992;&#19968;&#20010;&#35821;&#20041;&#30446;&#26631;&#26469;&#35757;&#32451;&#26816;&#32034;&#22120;&#32452;&#20214;&#65292;&#35813;&#30446;&#26631;&#30340;&#30446;&#30340;&#26159;&#26816;&#32034;&#22686;&#21152;&#19979;&#19968;&#20010;&#22359;&#27010;&#29575;&#30340;&#22359;&#65292;&#26681;&#25454;&#21442;&#32771;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (LMs) have received much attention recently. However, typically the retriever is not trained jointly as a native component of the LM, but added to an already-pretrained LM, which limits the ability of the LM and the retriever to adapt to one another. In this work, we propose the Retrieval-Pretrained Transformer (RPT), an architecture and training procedure for jointly training a retrieval-augmented LM from scratch for the task of modeling long texts. Given a recently generated text chunk in a long document, the LM computes query representations, which are then used to retrieve earlier chunks in the document, located potentially tens of thousands of tokens before. Information from retrieved chunks is fused into the LM representations to predict the next target chunk. We train the retriever component with a semantic objective, where the goal is to retrieve chunks that increase the probability of the next chunk, according to a reference LM. We evaluate 
&lt;/p&gt;</description></item><item><title>BiomedGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#22312;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;16&#20010;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#21253;&#25324;&#36229;&#36807;&#20102;OpenAI&#30340;GPT-4V&#21644;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#21516;&#26102;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.17100</link><description>&lt;p&gt;
BiomedGPT&#65306;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#32479;&#19968;&#19988;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks. (arXiv:2305.17100v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17100
&lt;/p&gt;
&lt;p&gt;
BiomedGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#22312;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;16&#20010;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#21253;&#25324;&#36229;&#36807;&#20102;OpenAI&#30340;GPT-4V&#21644;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#21516;&#26102;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#21644;&#32500;&#25252;&#20013;&#19981;&#22815;&#28789;&#27963;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#32467;&#21512;&#29616;&#20195;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#20026;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26377;&#28508;&#21147;&#35299;&#37322;&#19981;&#21516;&#30340;&#21307;&#30103;&#27169;&#24577;&#65292;&#24182;&#20135;&#29983;&#22914;&#33258;&#30001;&#25991;&#26412;&#25253;&#21578;&#25110;&#30142;&#30149;&#35786;&#26029;&#31561;&#34920;&#36798;&#24615;&#36755;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BiomedGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#22810;&#26679;&#21270;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#24320;&#28304;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;BiomedGPT&#22312;26&#20010;&#25968;&#25454;&#38598;&#30340;&#20116;&#20010;&#20020;&#24202;&#37325;&#35201;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;16&#20010;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25918;&#23556;&#23398;&#20154;&#21592;&#35780;&#20272;&#20013;&#65292;&#23427;&#36229;&#36234;&#20102;OpenAI&#30340;GPT-4 with vision&#65288;GPT-4V&#65289;&#65292;&#24182;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#36229;&#36807;&#20102;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#27492;&#22806;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional task- and modality-specific artificial intelligence (AI) models are inflexible in real-world deployment and maintenance for biomedicine. At the same time, the growing availability of biomedical data, coupled with the advancements in modern multi-modal multi-task AI techniques, has paved the way for the emergence of generalist biomedical AI solutions. These solutions hold the potential to interpret different medical modalities and produce expressive outputs such as free-text reports or disease diagnosis. Here, we propose BiomedGPT, the first open-source and generalist visual language AI for diverse biomedical tasks. BiomedGPT achieved 16 state-of-the-art results across five clinically significant tasks on 26 datasets. Notably, it outperformed OpenAI's GPT-4 with vision (GPT-4V) in radiology human evaluation and surpassed Google's Med-PaLM M (12B) in breast cancer diagnosis and medical visual question answering. Moreover, BiomedGPT facilitates zero-shot transfer learning, gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07687</link><description>&lt;p&gt;
MLRegTest&#65306;&#26426;&#22120;&#23398;&#20064;&#27491;&#21017;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MLRegTest: A Benchmark for the Machine Learning of Regular Languages. (arXiv:2304.07687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#24050;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#20801;&#35768;&#32454;&#33268;&#22320;&#26816;&#26597;&#23427;&#20204;&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#27169;&#24335;&#65292;&#24182;&#22312;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#26410;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#26102;&#24314;&#31435;&#20449;&#24515;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#30340;&#24207;&#21015;&#20998;&#31867;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#35757;&#32451;&#12289;&#24320;&#21457;&#21644;&#27979;&#35797;&#38598;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#24418;&#24335;&#35821;&#35328;&#20195;&#34920;&#30528;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#65292;&#24182;&#27491;&#30830;&#22320;&#35782;&#21035;&#24207;&#21015;&#20013;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26159;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25104;&#21151;&#27867;&#21270;&#30340;&#24050;&#30693;&#25361;&#25112;&#12290;MLRegTest&#26681;&#25454;&#23427;&#20204;&#30340;&#36923;&#36753;&#22797;&#26434;&#24230;&#65288;&#21333;&#35843;&#20108;&#38454;&#65292;&#19968;&#38454;&#65292;&#21629;&#39064;&#25110;&#21333;&#39033;&#24335;&#34920;&#36798;&#24335;&#65289;&#21644;&#36923;&#36753;&#25991;&#23383;&#30340;&#31181;&#31867;&#65288;&#23383;&#31526;&#20018;&#65292;&#23450;&#32423;&#23383;&#31526;&#20018;&#65292;&#23376;&#24207;&#21015;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#65289;&#32452;&#32455;&#20854;&#35821;&#35328;&#12290;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#25991;&#23383;&#30340;&#36873;&#25321;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#29702;&#35299;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#23427;&#20204;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating machine learning (ML) systems on their ability to learn known classifiers allows fine-grained examination of the patterns they can learn, which builds confidence when they are applied to the learning of unknown classifiers. This article presents a new benchmark for ML systems on sequence classification called MLRegTest, which contains training, development, and test sets from 1,800 regular languages.  Different kinds of formal languages represent different kinds of long-distance dependencies, and correctly identifying long-distance dependencies in sequences is a known challenge for ML systems to generalize successfully. MLRegTest organizes its languages according to their logical complexity (monadic second order, first order, propositional, or monomial expressions) and the kind of logical literals (string, tier-string, subsequence, or combinations thereof). The logical complexity and choice of literal provides a systematic way to understand different kinds of long-distance d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06671</link><description>&lt;p&gt;
&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;&#21644;&#36845;&#20195;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#25511;&#21046;&#26159;&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#22312;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20855;&#26377;&#31867;&#20284;&#31354;&#38388;&#37197;&#32622;&#30340;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#38598;&#19978;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#20219;&#24847;&#19981;&#30830;&#23450;&#30340;&#24067;&#23616;&#30340;&#31163;&#32447;&#20998;&#24067;&#26679;&#26412;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#29616;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayoutBench&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35786;&#26029;&#30340;&#22522;&#20934;&#65292;&#23427;&#26816;&#26597;&#20102;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#65306;&#25968;&#37327;&#65292;&#20301;&#32622;&#65292;&#22823;&#23567;&#21644;&#24418;&#29366;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#26368;&#36817;&#20195;&#34920;&#24615;&#30340;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#33391;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#36793;&#30028;&#19978;&#30340;&#23545;&#35937;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#65292;&#23427;&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#23637;&#31034;&#20986;&#22312;LayoutBench&#30340;OOD&#24067;&#23616;&#19978;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#34920;&#26126;IterInpaint&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#22810;&#26679;&#21644;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#22270;&#20687;&#21644;&#21487;&#25511;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Active-Prompt&#65292;&#23427;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#31034;&#20363;&#25552;&#31034;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#20219;&#21153;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#19982;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.12246</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#20027;&#21160;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Active Prompting with Chain-of-Thought for Large Language Models. (arXiv:2302.12246v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Active-Prompt&#65292;&#23427;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#31034;&#20363;&#25552;&#31034;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#20219;&#21153;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#19982;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#26085;&#30410;&#22686;&#22823;&#65292;&#20026;&#21508;&#31181;&#38656;&#35201;&#25512;&#29702;&#30340;&#22797;&#26434;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#65289;&#24102;&#26469;&#20102;&#26032;&#30340;&#33021;&#21147;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#30340;&#26377;&#25928;&#35774;&#35745;&#23545;LLMs&#20135;&#29983;&#39640;&#36136;&#37327;&#31572;&#26696;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#31034;&#20363;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#23548;&#25552;&#31034;&#65292;&#23427;&#22823;&#22823;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#24403;&#21069;&#30340;CoT&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#22266;&#23450;&#30340;&#20154;&#31867;&#27880;&#37322;&#31034;&#20363;&#65292;&#36825;&#20123;&#31034;&#20363;&#19981;&#19968;&#23450;&#26159;&#19981;&#21516;&#20219;&#21153;&#30340;&#26368;&#26377;&#25928;&#31034;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Active-Prompt&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#31034;&#20363;&#25552;&#31034;&#65288;&#20154;&#20026;&#35774;&#35745;&#30340;CoT&#25512;&#29702;&#27880;&#37322;&#65289;&#26469;&#36866;&#24212;LLMs&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#30830;&#23450;&#21738;&#20123;&#38382;&#39064;&#20174;&#20219;&#21153;&#29305;&#23450;&#26597;&#35810;&#27744;&#20013;&#27880;&#37322;&#26368;&#37325;&#35201;&#21644;&#26377;&#29992;&#12290;&#36890;&#36807;&#20511;&#37492;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20027;&#21160;&#25552;&#31034;(Acitve-Prompt)&#30340;&#26041;&#27861;&#65292;&#23558;&#26368;&#30456;&#20851;&#30340;&#38382;&#39064;&#20316;&#20026;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#28155;&#21152;&#32473;LLMs&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#25351;&#23548;&#25163;&#20876;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#29289;&#20307;&#19982;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04449</link><description>&lt;p&gt;
&#38405;&#35835;&#24182;&#33719;&#24471;&#22238;&#25253;&#65306;&#22312;&#19982;&#25351;&#23548;&#25163;&#20876;&#30340;&#24110;&#21161;&#19979;&#23398;&#20064;&#29609;Atari&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#25351;&#23548;&#25163;&#20876;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#29289;&#20307;&#19982;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#19968;&#30452;&#26159;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#19981;&#20165;&#20165;&#26159;&#36890;&#36807;&#20132;&#20114;&#25110;&#28436;&#31034;&#65292;&#36824;&#21253;&#25324;&#38405;&#35835;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25991;&#26723;&#65292;&#20363;&#22914;&#25351;&#23548;&#25163;&#20876;&#12290;&#25351;&#23548;&#25163;&#20876;&#21644;&#32500;&#22522;&#39029;&#38754;&#26159;&#26368;&#20016;&#23500;&#30340;&#25968;&#25454;&#20043;&#19968;&#65292;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#23453;&#36149;&#29305;&#24449;&#12289;&#31574;&#30053;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#25105;&#20204;&#20551;&#35774;&#21033;&#29992;&#20154;&#20889;&#30340;&#25351;&#23548;&#25163;&#20876;&#26469;&#24110;&#21161;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#31574;&#30053;&#23558;&#23548;&#33268;&#26356;&#39640;&#25928;&#21644;&#26356;&#20248;&#31168;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#12290;&#38405;&#35835;&#24182;&#22870;&#21169;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#26469;&#21152;&#36895;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#24635;&#32467;&#25351;&#23548;&#25163;&#20876;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#26681;&#25454;&#25351;&#23548;&#25163;&#20876;&#20013;&#30340;&#20449;&#24687;&#35780;&#20272;&#29289;&#20307;-&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;&#19968;&#20010;&#36741;&#21161;&#30340;&#21453;&#39304;&#26426;&#21046;&#21487;&#20197;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#34701;&#20837;&#21040;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#35774;&#35745;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#32452;&#20214;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#36890;&#36807;&#25193;&#23637;XAI&#38382;&#39064;&#24211;&#24182;&#25552;&#20379;&#35299;&#37322;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30495;&#27491;&#33258;&#28982;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2209.02552</link><description>&lt;p&gt;
&#33258;&#28982;&#23545;&#35805;&#20013;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#36208;&#21521;&#23545;&#35805;&#24335;XAI&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explaining Machine Learning Models in Natural Conversations: Towards a Conversational XAI Agent. (arXiv:2209.02552v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#34701;&#20837;&#21040;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#35774;&#35745;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#32452;&#20214;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#36890;&#36807;&#25193;&#23637;XAI&#38382;&#39064;&#24211;&#24182;&#25552;&#20379;&#35299;&#37322;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30495;&#27491;&#33258;&#28982;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#26041;&#27861;&#26469;&#25581;&#31034;&#40657;&#30418;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20197;&#20415;&#21521;&#20154;&#31867;&#35299;&#37322;&#12290;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#25351;&#20986;&#65292;&#36825;&#26679;&#30340;&#35299;&#37322;&#24212;&#35813;&#26159;&#23545;&#35805;&#24335;&#30340;&#65292;&#31867;&#20284;&#20110;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;XAI&#34701;&#20837;&#21040;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#32452;&#20214;&#30340;&#26631;&#20934;&#35774;&#35745;&#12290;&#25105;&#20204;&#26681;&#25454;&#36136;&#25511;&#30340;&#37322;&#20041;&#37325;&#36848;&#25193;&#23637;&#20102;&#19968;&#20010;XAI&#38382;&#39064;&#24211;&#65292;&#20197;&#29702;&#35299;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#36866;&#21512;&#25552;&#20379;&#31572;&#26696;&#20449;&#24687;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#25991;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24314;&#35758;&#21015;&#34920;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#23454;&#29616;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30495;&#27491;&#33258;&#28982;&#23545;&#35805;&#30340;&#31532;&#19968;&#27493;&#65292;&#19982;&#19968;&#20010;&#35299;&#37322;&#20195;&#29702;&#26377;&#20851;&#30340;&#20840;&#38754;&#30340;XAI&#38382;&#39064;&#21015;&#34920;&#21644;&#30456;&#24212;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Explainable AI (XAI) is to design methods to provide insights into the reasoning process of black-box models, such as deep neural networks, in order to explain them to humans. Social science research states that such explanations should be conversational, similar to human-to-human explanations. In this work, we show how to incorporate XAI in a conversational agent, using a standard design for the agent comprising natural language understanding and generation components. We build upon an XAI question bank which we extend by quality-controlled paraphrases to understand the user's information needs. We further systematically survey the literature for suitable explanation methods that provide the information to answer those questions, and present a comprehensive list of suggestions. Our work is the first step towards truly natural conversations about machine learning models with an explanation agent. The comprehensive list of XAI questions and the corresponding explanation meth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#28040;&#36153;&#32773;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21382;&#21490;&#26159;&#30740;&#31350;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#21160;&#26426;&#30340;&#19968;&#31181;&#34987;&#20302;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#36890;&#36807;&#23545;&#24086;&#23376;&#21382;&#21490;&#25552;&#21462;&#30340;&#25991;&#26412;&#32447;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#20998;&#20139;&#32773;&#22312;&#35328;&#36766;&#19978;&#26356;&#22810;&#28041;&#21450;&#24868;&#24594;&#12289;&#23447;&#25945;&#21644;&#26435;&#21147;&#12290;&#24182;&#19988;&#65292;&#36890;&#36807;&#23558;&#24086;&#23376;&#21382;&#21490;&#20013;&#30340;&#25991;&#26412;&#32447;&#32034;&#21152;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28608;&#27963;&#23447;&#25945;&#20215;&#20540;&#35266;&#21644;&#20943;&#23569;&#24868;&#24594;&#65292;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#21644;&#26356;&#24191;&#27867;&#30340;&#20998;&#20139;&#12290;</title><link>http://arxiv.org/abs/2203.10560</link><description>&lt;p&gt;
&#25552;&#21319;&#34394;&#20551;&#26032;&#38395;&#32531;&#35299;&#65306;&#26469;&#33258;&#20998;&#20139;&#32773;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21382;&#21490;&#30340;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Empowering Fake-News Mitigation: Insights from Sharers' Social Media Post-Histories. (arXiv:2203.10560v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#28040;&#36153;&#32773;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21382;&#21490;&#26159;&#30740;&#31350;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#21160;&#26426;&#30340;&#19968;&#31181;&#34987;&#20302;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#36890;&#36807;&#23545;&#24086;&#23376;&#21382;&#21490;&#25552;&#21462;&#30340;&#25991;&#26412;&#32447;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#20998;&#20139;&#32773;&#22312;&#35328;&#36766;&#19978;&#26356;&#22810;&#28041;&#21450;&#24868;&#24594;&#12289;&#23447;&#25945;&#21644;&#26435;&#21147;&#12290;&#24182;&#19988;&#65292;&#36890;&#36807;&#23558;&#24086;&#23376;&#21382;&#21490;&#20013;&#30340;&#25991;&#26412;&#32447;&#32034;&#21152;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28608;&#27963;&#23447;&#25945;&#20215;&#20540;&#35266;&#21644;&#20943;&#23569;&#24868;&#24594;&#65292;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#21644;&#26356;&#24191;&#27867;&#30340;&#20998;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#26159;&#19968;&#20010;&#20840;&#29699;&#24615;&#38382;&#39064;&#65292;&#38480;&#21046;&#20854;&#20256;&#25773;&#23545;&#20445;&#25252;&#27665;&#20027;&#12289;&#20844;&#20849;&#21355;&#29983;&#21644;&#28040;&#36153;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35748;&#20026;&#28040;&#36153;&#32773;&#33258;&#24049;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21382;&#21490;&#26159;&#19968;&#20010;&#34987;&#20302;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#29992;&#20110;&#30740;&#31350;&#26159;&#20160;&#20040;&#23548;&#33268;&#20182;&#20204;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#38142;&#25509;&#12290;&#22312;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#24086;&#23376;&#21382;&#21490;&#20013;&#25552;&#21462;&#30340;&#25991;&#26412;&#32447;&#32034;&#22914;&#20309;&#21306;&#20998;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#32773;&#21644;&#38543;&#26426;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#20197;&#21450;&#20854;&#20182;&#22312;&#35823;&#23548;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#20154;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#32773;&#20351;&#29992;&#26356;&#22810;&#19982;&#24868;&#24594;&#12289;&#23447;&#25945;&#21644;&#26435;&#21147;&#30456;&#20851;&#30340;&#35789;&#27719;&#12290;&#22312;&#31532;&#20108;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#24086;&#23376;&#21382;&#21490;&#20013;&#28155;&#21152;&#25991;&#26412;&#32447;&#32034;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#35841;&#26377;&#21487;&#33021;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#31532;&#19977;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20174;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#25512;&#23548;&#20986;&#30340;&#20004;&#31181;&#32531;&#35299;&#31574;&#30053;&#36827;&#34892;&#20102;&#21021;&#27493;&#27979;&#35797;&#65292;&#21363;&#28608;&#27963;&#23447;&#25945;&#20215;&#20540;&#35266;&#21644;&#20943;&#23569;&#24868;&#24594;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#21644;&#26356;&#24191;&#27867;&#30340;&#20998;&#20139;&#12290;&#22312;&#31532;&#22235;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#35843;&#26597;&#32467;&#26524;&#19982;&#29992;&#25143;&#30340;&#39564;&#35777;&#25512;&#29305;&#32080;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation is a global concern and limiting its spread is critical for protecting democracy, public health, and consumers. We propose that consumers' own social media post-histories are an underutilized data source to study what leads them to share links to fake-news. In Study 1, we explore how textual cues extracted from post-histories distinguish fake-news sharers from random social media users and others in the misinformation ecosystem. Among other results, we find across two datasets that fake-news sharers use more words related to anger, religion and power. In Study 2, we show that adding textual cues from post-histories improves the accuracy of models to predict who is likely to share fake-news. In Study 3, we provide a preliminary test of two mitigation strategies deduced from Study 1 - activating religious values and reducing anger - and find that they reduce fake-news sharing and sharing more generally. In Study 4, we combine survey responses with users' verified Twitter p
&lt;/p&gt;</description></item></channel></rss>