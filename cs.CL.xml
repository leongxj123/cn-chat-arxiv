<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#65292;&#36890;&#36807;Transformer-based&#22270;&#20687;&#32534;&#30721;&#22120;&#38544;&#24335;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14270</link><description>&lt;p&gt;
&#22330;&#26223;&#22270;ViT&#65306;&#31471;&#21040;&#31471;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#65292;&#36890;&#36807;Transformer-based&#22270;&#20687;&#32534;&#30721;&#22120;&#38544;&#24335;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#29616;&#26377;&#30446;&#26631;&#26816;&#27979;&#26550;&#26500;&#20013;&#28155;&#21152;&#21333;&#29420;&#30340;&#20851;&#31995;&#27169;&#22359;&#25110;&#35299;&#30721;&#22120;&#26469;&#22788;&#29702;&#27492;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#31163;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#38459;&#30861;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#22522;&#20110;Transformer&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#32452;&#25104;&#65292;&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#26631;&#35760;&#65292;&#24182;&#38544;&#21547;&#22320;&#24314;&#27169;&#23427;&#20204;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36873;&#25321;&#21487;&#33021;&#24418;&#25104;&#20851;&#31995;&#30340;&#23545;&#35937;&#23545;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21333;&#38454;&#27573;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#23545;&#35937;&#21644;&#20851;&#31995;&#26816;&#27979;&#25968;&#25454;&#19978;&#35757;&#32451;&#27492;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Visual Genome&#21644;&#22823;&#35789;&#27719;GQA&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#65292;&#21487;&#23454;&#29616;&#23454;&#26102;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14270v1 Announce Type: cross  Abstract: Visual relationship detection aims to identify objects and their relationships in images. Prior methods approach this task by adding separate relationship modules or decoders to existing object detection architectures. This separation increases complexity and hinders end-to-end training, which limits performance. We propose a simple and highly efficient decoder-free architecture for open-vocabulary visual relationship detection. Our model consists of a Transformer-based image encoder that represents objects as tokens and models their relationships implicitly. To extract relationship information, we introduce an attention mechanism that selects object pairs likely to form a relationship. We provide a single-stage recipe to train this model on a mixture of object and relationship detection data. Our approach achieves state-of-the-art relationship detection performance on Visual Genome and on the large-vocabulary GQA benchmark at real-tim
&lt;/p&gt;</description></item><item><title>MoralBERT &#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#25991;&#26412;&#20013;&#36947;&#24503;&#24494;&#22937;&#20043;&#22788;&#30340;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;Twitter&#12289;Reddit&#21644;Facebook&#30340;&#25968;&#25454;&#65292;&#25193;&#22823;&#20102;&#27169;&#22411;&#29702;&#35299;&#36947;&#24503;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07678</link><description>&lt;p&gt;
MoralBERT&#65306;&#26816;&#27979;&#31038;&#20250;&#35805;&#35821;&#20013;&#30340;&#36947;&#24503;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
MoralBERT: Detecting Moral Values in Social Discourse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07678
&lt;/p&gt;
&lt;p&gt;
MoralBERT &#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#25991;&#26412;&#20013;&#36947;&#24503;&#24494;&#22937;&#20043;&#22788;&#30340;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;Twitter&#12289;Reddit&#21644;Facebook&#30340;&#25968;&#25454;&#65292;&#25193;&#22823;&#20102;&#27169;&#22411;&#29702;&#35299;&#36947;&#24503;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#24503;&#22312;&#25105;&#20204;&#24863;&#30693;&#20449;&#24687;&#12289;&#24433;&#21709;&#20915;&#31574;&#21644;&#21028;&#26029;&#36807;&#31243;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#21253;&#25324;&#30123;&#33495;&#25509;&#31181;&#12289;&#22549;&#32974;&#12289;&#31181;&#26063;&#20027;&#20041;&#21644;&#24615;&#21462;&#21521;&#22312;&#20869;&#30340;&#26377;&#20105;&#35758;&#35805;&#39064;&#24448;&#24448;&#24341;&#21457;&#30340;&#24847;&#35265;&#21644;&#24577;&#24230;&#24182;&#38750;&#20165;&#22522;&#20110;&#35777;&#25454;&#65292;&#32780;&#26356;&#22810;&#21453;&#26144;&#20102;&#36947;&#24503;&#19990;&#30028;&#35266;&#12290;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#36947;&#24503;&#20215;&#20540;&#21487;&#20197;&#20174;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#20869;&#23481;&#20013;&#24471;&#21040;&#21028;&#26029;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#25429;&#25417;&#25991;&#26412;&#20013;&#36947;&#24503;&#24494;&#22937;&#20043;&#22788;&#30340;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;MoralBERT&#12290;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#26469;&#28304;&#65288;Twitter&#12289;Reddit&#21644;Facebook&#65289;&#30340;&#24102;&#26377;&#27880;&#37322;&#30340;&#36947;&#24503;&#25968;&#25454;&#65292;&#28085;&#30422;&#21508;&#31181;&#31038;&#20250;&#30456;&#20851;&#20027;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#22823;&#20102;&#35821;&#35328;&#22810;&#26679;&#24615;&#65292;&#21487;&#33021;&#22686;&#24378;&#27169;&#22411;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#29702;&#35299;&#36947;&#24503;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#24182;&#23558;&#20854;&#19982;&#26631;&#20934;&#30340;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07678v1 Announce Type: new  Abstract: Morality plays a fundamental role in how we perceive information while greatly influencing our decisions and judgements. Controversial topics, including vaccination, abortion, racism, and sexuality, often elicit opinions and attitudes that are not solely based on evidence but rather reflect moral worldviews. Recent advances in natural language processing have demonstrated that moral values can be gauged in human-generated textual content. Here, we design a range of language representation models fine-tuned to capture exactly the moral nuances in text, called MoralBERT. We leverage annotated moral data from three distinct sources: Twitter, Reddit, and Facebook user-generated content covering various socially relevant topics. This approach broadens linguistic diversity and potentially enhances the models' ability to comprehend morality in various contexts. We also explore a domain adaptation technique and compare it to the standard fine-tu
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.02504</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02504
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20195;&#34920;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#12290;&#35813;&#33539;&#24335;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21306;&#21035;&#20110;&#20247;&#65292;&#23637;&#31034;&#20102;&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#20063;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#29575;&#12290;&#36825;&#31181;&#25928;&#29575;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#29305;&#21035;&#26377;&#30410;&#65292;&#22240;&#20026;&#27880;&#37322;&#26679;&#26412;&#30340;&#25968;&#37327;&#36890;&#24120;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#25945;&#31243;&#20840;&#38754;&#20171;&#32461;&#20102;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#23454;&#38469;&#24212;&#29992;&#30340;&#26696;&#20363;&#32451;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#33539;&#24335;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#31867;&#21035;&#20998;&#31867;&#21644;&#22238;&#24402;&#12290;&#24378;&#35843;&#20854;&#39640;&#25928;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#65292;&#35813;&#25945;&#31243;&#26088;&#22312;&#40723;&#21169;&#26356;&#24191;&#27867;&#22320;&#37319;&#32435;&#36825;&#31181;&#33539;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#30340;&#24320;&#25918;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02504v1 Announce Type: cross  Abstract: The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BiasBuster&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#21644;&#27979;&#35797;&#22810;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;LLMs&#33258;&#36523;&#26469;&#28040;&#38500;&#20854;&#25552;&#31034;&#20013;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00811</link><description>&lt;p&gt;
LLM&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Cognitive Bias in High-Stakes Decision-Making with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00811
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BiasBuster&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#21644;&#27979;&#35797;&#22810;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;LLMs&#33258;&#36523;&#26469;&#28040;&#38500;&#20854;&#25552;&#31034;&#20013;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25903;&#25345;&#26085;&#30410;&#25193;&#22823;&#30340;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#20154;&#31867;(&#21019;&#36896;&#30340;)&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;LLMs&#21487;&#33021;&#20250;&#32487;&#25215;&#38024;&#23545;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20063;&#21487;&#33021;&#21463;&#21040;&#35748;&#30693;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#22952;&#30861;&#21033;&#29992;LLM&#21327;&#21161;&#20570;&#20986;&#20844;&#24179;&#21644;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;BiasBuster&#65292;&#19968;&#20010;&#26088;&#22312;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#12290;&#21463;&#24515;&#29702;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#20808;&#21069;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#35748;&#30693;&#20559;&#35265;(&#20363;&#22914;&#65292;&#25552;&#31034;&#35825;&#23548;&#12289;&#39034;&#24207;&#12289;&#22266;&#26377;)&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#26469;&#28040;&#38500;&#23427;&#20204;&#33258;&#24049;&#30340;&#25552;&#31034;&#20013;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#39046;&#22495;&#35748;&#30693;&#20559;&#35265;&#23384;&#22312;&#21644;&#24433;&#21709;&#30340;&#20840;&#38754;&#22270;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00811v1 Announce Type: new  Abstract: Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. However, given their training on human (created) data, LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive sciences, we develop a dataset containing 16,800 prompts to evaluate different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, amidst proposing a novel method using LLMs to debias their own prompts. Our analysis provides a comprehensive picture on the presence and effects of cognitive bias across diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#30456;&#21516;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#26597;&#35810;&#32452;&#21512;&#20026;&#21333;&#20010;&#25552;&#31034;&#65292;&#20197;&#26368;&#23567;&#21270;&#37325;&#22797;&#35843;&#29992;&#26469;&#20248;&#21270;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.00067</link><description>&lt;p&gt;
Query-OPT&#65306;&#36890;&#36807;&#22810;&#26597;&#35810;&#25351;&#20196;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#30456;&#21516;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#26597;&#35810;&#32452;&#21512;&#20026;&#21333;&#20010;&#25552;&#31034;&#65292;&#20197;&#26368;&#23567;&#21270;&#37325;&#22797;&#35843;&#29992;&#26469;&#20248;&#21270;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20851;&#27880;&#22522;&#20110;&#26597;&#35810;&#30340;&#20250;&#35758;&#25688;&#35201;&#20219;&#21153;&#65292;&#22312;&#27492;&#20219;&#21153;&#20013;&#65292;&#38024;&#23545;&#29305;&#23450;&#26597;&#35810;&#23545;&#19978;&#19979;&#25991;&#65288;&#20250;&#35758;&#35760;&#24405;&#65289;&#29983;&#25104;&#25688;&#35201;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#27492;&#20219;&#21153;&#26102;&#65292;&#21363;&#20351;&#19978;&#19979;&#25991;&#20445;&#25345;&#19981;&#21464;&#65292;&#27599;&#20010;&#26032;&#26597;&#35810;&#20063;&#38656;&#35201;&#23545;LLM&#25512;&#29702;&#31471;&#28857;/API&#36827;&#34892;&#19968;&#27425;&#26032;&#35843;&#29992;&#12290;&#28982;&#32780;&#65292;&#21453;&#22797;&#35843;&#29992;LLM&#25512;&#29702;&#31471;&#28857;&#20250;&#26174;&#33879;&#22686;&#21152;&#22312;&#29983;&#20135;&#20013;&#20351;&#29992;&#23427;&#20204;&#30340;&#25104;&#26412;&#65292;&#36825;&#20351;&#24471;&#35768;&#22810;&#23454;&#38469;&#29992;&#20363;&#20013;LLMs&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#25104;&#21151;&#22320;&#23558;&#30456;&#21516;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#26597;&#35810;&#32452;&#21512;&#20026;&#21333;&#20010;&#25552;&#31034;&#20197;&#26368;&#23567;&#21270;&#37325;&#22797;&#35843;&#29992;&#65292;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#21508;&#31181;&#27969;&#34892;&#30340;LLM&#65288;GPT-4&#12289;PaLM-2&#12289;LLaMA-2&#12289;Mistral&#21644;FLAN-T5&#65289;&#22312;&#21333;&#26597;&#35810;&#21644;&#22810;&#26597;&#35810;&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00067v1 Announce Type: new  Abstract: This work focuses on the task of query-based meeting summarization in which the summary of a context (meeting transcript) is generated in response to a specific query. When using Large Language Models (LLMs) for this task, a new call to the LLM inference endpoint/API is required for each new query even if the context stays the same. However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases. To address this problem, in this paper, we investigate whether combining the queries for the same input context in a single prompt to minimize repeated calls can be successfully used in meeting summarization. In this regard, we conduct extensive experiments by comparing the performance of various popular LLMs: GPT-4, PaLM-2, LLaMA-2, Mistral, and FLAN-T5 in single-query and multi-query settings. We observe that while most LLMs tend to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#21160;&#24335;KBQA&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30693;&#35782;&#24211;&#20114;&#21160;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#65292;&#24182;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.15131</link><description>&lt;p&gt;
&#20114;&#21160;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36718;&#20132;&#20114;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#21160;&#24335;KBQA&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30693;&#35782;&#24211;&#20114;&#21160;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#65292;&#24182;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#30340;&#39046;&#22495;&#12290;KBQA&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#23558;&#22797;&#26434;&#38382;&#39064;&#35299;&#26512;&#20026;&#21487;&#25191;&#34892;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#65288;SP&#65289;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#21160;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#26223;&#19979;&#20805;&#20998;&#21033;&#29992;LLMs&#23558;&#38382;&#39064;&#35299;&#26512;&#20026;&#36923;&#36753;&#24418;&#24335;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20114;&#21160;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;Interactive-KBQA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#19982;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#30452;&#25509;&#20114;&#21160;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#12290;&#23545;&#20110;&#27599;&#31181;&#22797;&#26434;&#38382;&#39064;&#31867;&#21035;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;LLMs&#23436;&#25104;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15131v1 Announce Type: cross  Abstract: This study explores the realm of knowledge-base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. Yet, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results o
&lt;/p&gt;</description></item><item><title>SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09025</link><description>&lt;p&gt;
SLEB: &#36890;&#36807;&#20887;&#20313;&#39564;&#35777;&#21644;&#28040;&#38500;Transformer&#22359;&#20248;&#21270;LLM&#30340;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09025
&lt;/p&gt;
&lt;p&gt;
SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#32473;&#23454;&#38469;&#37096;&#32626;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#31934;&#31616;&#65292;&#19968;&#31181;&#26088;&#22312;&#20943;&#23567;LLM&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#20887;&#20313;&#32452;&#20214;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#31934;&#31616;&#26377;&#24076;&#26395;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31471;&#21040;&#31471;LLM&#25512;&#29702;&#21152;&#36895;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLEB&#65292;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;Transformer&#22359;&#20316;&#20026;&#31934;&#31616;&#30340;&#22522;&#26412;&#21333;&#20301;&#65292;&#22240;&#20026;LLM&#22312;&#30456;&#37051;&#22359;&#30340;&#36755;&#20986;&#20043;&#38388;&#20855;&#26377;&#22359;&#32423;&#21035;&#30340;&#20887;&#20313;&#21644;&#39640;&#30456;&#20284;&#24615;&#12290;&#36825;&#20010;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22686;&#24378;LLM&#30340;&#22788;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SLEB&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09025v1 Announce Type: new Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#20102;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#22870;&#21169;&#27169;&#22411;&#30340;&#36873;&#25321;&#20197;&#21450;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#30340;&#33258;&#28982;&#21464;&#25442;&#36873;&#25321;&#65292;&#35813;&#21464;&#25442;&#24378;&#35843;&#25913;&#21892;&#34920;&#29616;&#19981;&#20339;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#27424;&#25311;&#21512;&#21644;&#22870;&#21169;&#27450;&#39575;&#12290;</title><link>https://arxiv.org/abs/2402.00742</link><description>&lt;p&gt;
&#25913;&#21464;&#21644;&#32452;&#21512;&#22870;&#21169;&#20197;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transforming and Combining Rewards for Aligning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#20102;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#22870;&#21169;&#27169;&#22411;&#30340;&#36873;&#25321;&#20197;&#21450;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#30340;&#33258;&#28982;&#21464;&#25442;&#36873;&#25321;&#65292;&#35813;&#21464;&#25442;&#24378;&#35843;&#25913;&#21892;&#34920;&#29616;&#19981;&#20339;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#27424;&#25311;&#21512;&#21644;&#22870;&#21169;&#27450;&#39575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#39318;&#20808;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#22870;&#21169;&#27169;&#22411;&#26469;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#20004;&#20010;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22870;&#21169;&#27169;&#22411;&#30340;&#20219;&#20309;&#21333;&#35843;&#21464;&#25442;&#37117;&#20445;&#25345;&#20559;&#22909;&#25490;&#21517;&#65307;&#26159;&#21542;&#26377;&#19968;&#31181;&#27604;&#20854;&#20182;&#36873;&#25321;&#8220;&#26356;&#22909;&#8221;&#30340;&#36873;&#25321;&#65311;&#20854;&#27425;&#65292;&#25105;&#20204;&#32463;&#24120;&#24076;&#26395;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#20010;&#29305;&#24615;&#23545;&#40784;&#65306;&#25105;&#20204;&#22914;&#20309;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#65311;&#36890;&#36807;&#23545;&#40784;&#36807;&#31243;&#30340;&#27010;&#29575;&#35299;&#37322;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20174;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#23398;&#20064;&#30340;&#22870;&#21169;&#65288;&#24120;&#35265;&#24773;&#20917;&#65289;&#30340;&#33258;&#28982;&#21464;&#25442;&#36873;&#25321;&#12290;&#36825;&#20010;&#27966;&#29983;&#30340;&#21464;&#25442;&#20855;&#26377;&#20004;&#20010;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#24378;&#35843;&#25913;&#36827;&#34920;&#29616;&#19981;&#20339;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#24050;&#32463;&#24471;&#20998;&#33391;&#22909;&#30340;&#36755;&#20986;&#12290;&#36825;&#26082;&#20943;&#36731;&#20102;&#27424;&#25311;&#21512;&#65288;&#20854;&#20013;&#19968;&#20123;&#25552;&#31034;&#27809;&#26377;&#24471;&#21040;&#25913;&#36827;&#65289;&#65292;&#21448;&#20943;&#23569;&#20102;&#22870;&#21169;&#27450;&#39575;&#65288;&#27169;&#22411;&#23398;&#20064;&#21033;&#29992;&#38169;&#35823;&#25351;&#23450;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12492</link><description>&lt;p&gt;
&#27604;&#36739;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35821;&#35328;&#24314;&#27169;&#65306;&#27169;&#25311;&#32676;&#20307;&#12289;&#20010;&#20307;&#29305;&#28857;&#36824;&#26159;&#20004;&#32773;&#20860;&#39038;&#65311;
&lt;/p&gt;
&lt;p&gt;
Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#23558;&#20154;&#30340;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#65288;&#22914;45&#23681;&#20197;&#19978;&#30340;&#20154;&#32676;&#65289;&#36824;&#26159;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#26356;&#26377;&#25928;&#30340;&#38382;&#39064;&#23578;&#26410;&#30830;&#23450;&#12290;&#32676;&#20307;&#23646;&#24615;&#22312;&#25216;&#26415;&#19978;&#26356;&#23481;&#26131;&#23454;&#29616;&#65292;&#20294;&#26159;&#36807;&#20110;&#31895;&#31961;&#65306;&#24182;&#38750;&#25152;&#26377;45&#23681;&#20197;&#19978;&#30340;&#20154;&#37117;&#20197;&#30456;&#21516;&#30340;&#26041;&#24335;&#20070;&#20889;&#12290;&#30456;&#21453;&#65292;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#33021;&#22815;&#25429;&#25417;&#27599;&#20010;&#20154;&#36523;&#20221;&#30340;&#22797;&#26434;&#24615;&#65292;&#20801;&#35768;&#26356;&#20010;&#24615;&#21270;&#30340;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#27169;&#25311;&#26080;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#24182;&#19988;&#38656;&#35201;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36890;&#36807;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#23558;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#30340;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#65288;&#22914;&#24180;&#40836;&#20272;&#35745;&#25110;&#20010;&#24615;&#35780;&#20272;&#65289;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#31435;&#22330;&#21644;&#20027;&#39064;&#26816;&#27979;&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.04131</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#23450;&#20301;&#36328;&#20219;&#21153;&#24207;&#21015;&#32487;&#32493;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04131
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;Transformer&#27169;&#22411;&#36824;&#21407;&#20026;&#21487;&#35835;&#30340;&#30005;&#36335;&#34920;&#31034;&#65292;&#29992;&#20110;&#23454;&#29616;&#31639;&#27861;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#26469;&#25193;&#23637;&#36825;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#23383;&#12289;&#25968;&#23383;&#35789;&#21644;&#26376;&#20221;&#30340;&#36882;&#22686;&#24207;&#21015;&#12290;&#36890;&#36807;&#24212;&#29992;&#30005;&#36335;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36127;&#36131;&#26816;&#27979;&#24207;&#21015;&#25104;&#21592;&#21644;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#25104;&#21592;&#30340;&#20851;&#38190;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35821;&#20041;&#30456;&#20851;&#24207;&#21015;&#20381;&#36182;&#20110;&#20855;&#26377;&#31867;&#20284;&#20316;&#29992;&#30340;&#20849;&#20139;&#30005;&#36335;&#23376;&#22270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35760;&#24405;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#32534;&#36753;&#36807;&#31243;&#12290;&#36825;&#31181;&#23545;Transformer&#30340;&#26426;&#26800;&#29702;&#35299;&#26159;&#26500;&#24314;&#26356;&#20581;&#22766;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#26356;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;WavCaps&#65292;&#21547;&#32422;40&#19975;&#26465;&#24102;&#26377;&#37197;&#23545;&#23383;&#24149;&#30340;&#38899;&#39057;&#21098;&#36753;&#12290;&#20026;&#20811;&#26381;&#22122;&#22768;&#26631;&#27880;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ChatGPT&#30340;&#19977;&#38454;&#27573;&#23383;&#24149;&#29983;&#25104;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.17395</link><description>&lt;p&gt;
WavCaps: &#19968;&#31181;ChatGPT&#36741;&#21161;&#30340;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35821;&#35328;&#22810;&#27169;&#24577;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research. (arXiv:2303.17395v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;WavCaps&#65292;&#21547;&#32422;40&#19975;&#26465;&#24102;&#26377;&#37197;&#23545;&#23383;&#24149;&#30340;&#38899;&#39057;&#21098;&#36753;&#12290;&#20026;&#20811;&#26381;&#22122;&#22768;&#26631;&#27880;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ChatGPT&#30340;&#19977;&#38454;&#27573;&#23383;&#24149;&#29983;&#25104;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38899;&#39057;-&#35821;&#35328;&#65288;AL&#65289;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#30340;&#21457;&#23637;&#38750;&#24120;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;AL&#25968;&#25454;&#38598;&#25910;&#38598;&#36807;&#31243;&#26114;&#36149;&#36153;&#26102;&#65292;&#35268;&#27169;&#26377;&#38480;&#65292;&#32473;&#30740;&#31350;&#32773;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;WavCaps&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#22823;&#32422;40&#19975;&#26465;&#24102;&#26377;&#37197;&#23545;&#23383;&#24149;&#30340;&#22823;&#35268;&#27169;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20174;Web&#36164;&#28304;&#21644;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#38899;&#39057;&#21098;&#36753;&#21450;&#21407;&#22987;&#25551;&#36848;&#12290;&#20294;&#26159;&#65292;&#22312;&#32447;&#25910;&#38598;&#21040;&#30340;&#21407;&#22987;&#25551;&#36848;&#38750;&#24120;&#22024;&#26434;&#65292;&#19981;&#36866;&#21512;&#29992;&#20110;&#33258;&#21160;&#21270;&#38899;&#39057;&#23383;&#24149;&#31561;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#22788;&#29702;&#27969;&#31243;&#65292;&#20197;&#36807;&#28388;&#22024;&#26434;&#25968;&#25454;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#23383;&#24149;&#65292;&#22312;&#20854;&#20013;&#21033;&#29992;&#20102;ChatGPT&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26469;&#33258;&#21160;&#36807;&#28388;&#21644;&#36716;&#25442;&#21407;&#22987;&#25551;&#36848;&#12290;&#25105;&#20204;&#23545;WavCaps&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07865</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25512;&#25991;/&#29992;&#25143;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22788;&#29702;&#25991;&#26412;&#22823;&#25968;&#25454;&#22320;&#29702;&#26631;&#35760;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#20272;&#35745;&#22352;&#26631;&#23545;&#65288;&#32463;&#24230;&#65292;&#32428;&#24230;&#65289;&#21644;&#20108;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#33539;&#22260;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#12290;&#24615;&#33021;&#25351;&#26631;&#34920;&#26126;&#65292;&#23545;&#20110;&#22312;&#25512;&#25991;&#20869;&#23481;&#21644;&#20803;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;30&#20844;&#37324;&#65292;&#32654;&#22269;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;15&#20844;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
&lt;/p&gt;</description></item></channel></rss>