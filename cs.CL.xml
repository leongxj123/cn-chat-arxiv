<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;PairEval&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#23558;&#22238;&#22797;&#30340;&#36136;&#37327;&#19982;&#19981;&#21516;&#23545;&#35805;&#20013;&#30340;&#22238;&#22797;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01015</link><description>&lt;p&gt;
PairEval&#65306;&#20351;&#29992;&#20004;&#20004;&#27604;&#36739;&#36827;&#34892;&#24320;&#25918;&#22495;&#23545;&#35805;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01015
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PairEval&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#23558;&#22238;&#22797;&#30340;&#36136;&#37327;&#19982;&#19981;&#21516;&#23545;&#35805;&#20013;&#30340;&#22238;&#22797;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01015v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#24314;&#31435;&#21487;&#38752;&#19988;&#33258;&#21160;&#21270;&#30340;&#35780;&#20272;&#25351;&#26631;&#26159;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#24517;&#19981;&#21487;&#23569;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#32771;&#34385;&#29983;&#25104;&#30340;&#22238;&#22797;&#19982;&#20043;&#21069;&#30340;&#23545;&#35805;&#21382;&#21490;&#30340;&#30456;&#20851;&#24615;&#26469;&#35780;&#20272;&#36825;&#20123;&#22238;&#22797;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#25351;&#26631;&#30452;&#25509;&#35780;&#20272;&#21333;&#20010;&#22238;&#22797;&#65292;&#32780;&#26410;&#32771;&#34385;&#20854;&#30456;&#23545;&#36136;&#37327;&#19982;&#20854;&#20182;&#22238;&#22797;&#30456;&#27604;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PairEval&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#23558;&#22238;&#22797;&#30340;&#36136;&#37327;&#19982;&#19981;&#21516;&#23545;&#35805;&#20013;&#30340;&#22238;&#22797;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#12290;PairEval&#24314;&#31435;&#22312;&#24320;&#28304;&#21644;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#65292;&#24182;&#20351;&#20854;&#19987;&#38376;&#21270;&#20110;&#23545;&#35805;&#22238;&#22797;&#20043;&#38388;&#30340;&#20004;&#20004;&#27604;&#36739;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#21576;&#29616;&#20986;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#36229;&#36807;&#22522;&#32447;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25152;&#25552;&#20986;&#30340;&#27604;&#36739;&#24615;&#25351;&#26631;&#22312;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01015v1 Announce Type: new  Abstract: Building a reliable and automated evaluation metric is a necessary but challenging problem for open-domain dialogue systems. Recent studies proposed evaluation metrics that assess generated responses by considering their relevance to previous dialogue histories. Although effective, these metrics evaluate individual responses directly rather than considering their relative quality compared to other responses. To handle this, we propose PairEval, a novel dialogue evaluation metric for assessing responses by comparing their quality against responses in different conversations. PairEval is built on top of open-sourced and moderate-size language models, and we make them specialized in pairwise comparison between dialogue responses. Extensive experiments on multiple benchmarks demonstrate that our metric exhibits a higher correlation with human judgments than baseline metrics. We also find that the proposed comparative metric is more robust in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#20462;&#27491;&#25512;&#29702;&#26694;&#26550;LeCo&#65292;&#26080;&#38656;&#20154;&#31867;&#21453;&#39304;&#12289;&#22806;&#37096;&#24037;&#20855;&#25110;&#25163;&#21160;&#25552;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#24182;&#22522;&#20110;&#29983;&#25104;logits&#26469;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19094</link><description>&lt;p&gt;
&#27809;&#26377;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27491;&#30830;&#24615;&#20351;LLM&#25104;&#20026;&#39640;&#25928;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Learning From Correctness Without Prompting Makes LLM Efficient Reasoner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#20462;&#27491;&#25512;&#29702;&#26694;&#26550;LeCo&#65292;&#26080;&#38656;&#20154;&#31867;&#21453;&#39304;&#12289;&#22806;&#37096;&#24037;&#20855;&#25110;&#25163;&#21160;&#25552;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#24182;&#22522;&#20110;&#29983;&#25104;logits&#26469;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#12289;&#19981;&#24544;&#23454;&#30340;&#25512;&#29702;&#21644;&#26377;&#27602;&#20869;&#23481;&#31561;&#23616;&#38480;&#24615;&#12290;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#20010;&#28508;&#22312;&#26041;&#27861;&#26159;&#20174;&#20154;&#31867;&#25110;&#22806;&#37096;&#21453;&#39304;&#65288;&#20363;&#22914;&#24037;&#20855;&#65289;&#20013;&#23398;&#20064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#20869;&#22312;&#33258;&#25105;&#20462;&#27491;&#25512;&#29702;&#26694;&#26550;&#65292;&#28040;&#38500;&#20102;&#20154;&#31867;&#21453;&#39304;&#12289;&#22806;&#37096;&#24037;&#20855;&#21644;&#25163;&#24037;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#22522;&#20110;&#19968;&#31181;&#22810;&#27493;&#25512;&#29702;&#33539;&#24335;Learning from Correctness (LeCo)&#65292;&#22312;&#19981;&#38656;&#35201;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25512;&#29702;&#24615;&#33021;&#12290;&#35813;&#33539;&#24335;&#20248;&#20808;&#23398;&#20064;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#22522;&#20110;&#29983;&#25104;logits&#26469;&#34913;&#37327;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#21508;&#31181;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25913;&#21892;&#25512;&#29702;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19094v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content. One potential approach to mitigate these issues is learning from human or external feedback (e.g. tools). In this paper, we introduce an intrinsic self-correct reasoning framework for LLMs that eliminates the need for human feedback, external tools, and handcraft prompts. The proposed framework, based on a multi-step reasoning paradigm \textbf{Le}arning from \textbf{Co}rrectness (\textsc{LeCo}), improves reasoning performance without needing to learn from errors. This paradigm prioritizes learning from correct reasoning steps, and a unique method to measure confidence for each reasoning step based on generation logits. Experimental results across various multi-step reasoning tasks demonstrate the effectiveness of the framework in improving reasoning
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#25968;&#25454;&#38598;BIMCV-R&#65292;&#21253;&#21547;8,069&#20010;3D CT&#20307;&#31215;&#21644;&#20854;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#26816;&#32034;&#31574;&#30053;MedFinder&#65292;&#20026;3D&#21307;&#23398;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#25552;&#20379;&#20102;&#37325;&#35201;&#36129;&#29486;</title><link>https://arxiv.org/abs/2403.15992</link><description>&lt;p&gt;
BIMCV-R&#65306;&#29992;&#20110;3D CT&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#30340;&#37324;&#31243;&#30865;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#25968;&#25454;&#38598;BIMCV-R&#65292;&#21253;&#21547;8,069&#20010;3D CT&#20307;&#31215;&#21644;&#20854;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#26816;&#32034;&#31574;&#30053;MedFinder&#65292;&#20026;3D&#21307;&#23398;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#25552;&#20379;&#20102;&#37325;&#35201;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15992v1 &#21457;&#24067;&#31867;&#22411;: &#36328;&#36234;  &#25688;&#35201;: &#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#19982;&#21307;&#30103;&#20445;&#20581;&#30340;&#34701;&#21512;&#19981;&#26029;&#22686;&#21152;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#24037;&#20316;&#37327;&#12290;&#20026;&#20102;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#35786;&#26029;&#36807;&#31243;&#20013;&#65292;&#20943;&#36731;&#20854;&#24037;&#20316;&#37327;&#65292;&#24320;&#21457;&#19968;&#20010;&#21487;&#38752;&#30340;&#26816;&#32034;&#30456;&#20284;&#30149;&#20363;&#30740;&#31350;&#30340;&#31995;&#32479;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#36825;&#19968;&#27010;&#24565;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#30446;&#21069;3D&#21307;&#23398;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#21463;&#38480;&#20110;&#32570;&#20047;&#20581;&#20840;&#30340;&#35780;&#20272;&#22522;&#20934;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;BIMCV-R&#65288;&#27492;&#25968;&#25454;&#38598;&#23558;&#22312;&#25509;&#21463;&#21518;&#21457;&#24067;&#12290;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;8,069&#20010;3D CT&#20307;&#31215;&#30340;&#24191;&#27867;&#25910;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;200&#19975;&#24352;&#20999;&#29255;&#65292;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#22312;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25299;&#23637;&#20102;&#19968;&#31181;&#26816;&#32034;&#31574;&#30053;&#65292;MedFinder&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#21452;&#27969;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#22823;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15992v1 Announce Type: cross  Abstract: The burgeoning integration of 3D medical imaging into healthcare has led to a substantial increase in the workload of medical professionals. To assist clinicians in their diagnostic processes and alleviate their workload, the development of a robust system for retrieving similar case studies presents a viable solution. While the concept holds great promise, the field of 3D medical text-image retrieval is currently limited by the absence of robust evaluation benchmarks and curated datasets. To remedy this, our study presents a groundbreaking dataset, BIMCV-R (This dataset will be released upon acceptance.), which includes an extensive collection of 8,069 3D CT volumes, encompassing over 2 million slices, paired with their respective radiological reports. Expanding upon the foundational work of our dataset, we craft a retrieval strategy, MedFinder. This approach employs a dual-stream network architecture, harnessing the potential of larg
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09635</link><description>&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09635
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;transformer&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#28145;&#24230;&#26041;&#38754;&#20173;&#28982;&#24456;&#38590;&#25193;&#23637;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#21069;&#21521;&#21644;&#21453;&#21521;&#20449;&#21495;&#30697;&#30340;&#20844;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#29702;&#35299;&#21644;&#32531;&#35299;&#19982;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#30456;&#20851;&#30340;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#31209;&#22349;&#32553;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;DeepScaleLM&#65292;&#19968;&#31181;&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#36890;&#36807;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#27169;&#22411;&#20013;&#20445;&#25345;&#21333;&#20301;&#36755;&#20986;/&#26799;&#24230;&#30697;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#20855;&#26377;100&#22810;&#23618;&#30340;&#38750;&#24120;&#28145;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;transformer&#27169;&#22411;&#21487;&#20197;&#26356;&#28145; - &#25105;&#20204;&#30340;&#28145;&#23618;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#20165;&#32534;&#30721;&#22120;&#12289;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#20307;&#65292;&#36866;&#29992;&#20110;Pre-LN&#21644;Post-LN transformers&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09635v1 Announce Type: cross  Abstract: In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These imp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MultiQ&#22522;&#20934;&#65292;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;LLMs&#22312;&#20854;&#39044;&#26399;&#20351;&#29992;&#33539;&#22260;&#20043;&#22806;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#33267;&#23569;&#26576;&#20123;&#35821;&#35328;&#33021;&#22815;&#24544;&#23454;&#21644;&#20934;&#30830;&#22320;&#36827;&#34892;&#22238;&#31572;&#12290;</title><link>https://arxiv.org/abs/2403.03814</link><description>&lt;p&gt;
&#29992;MultiQ&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MultiQ&#22522;&#20934;&#65292;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;LLMs&#22312;&#20854;&#39044;&#26399;&#20351;&#29992;&#33539;&#22260;&#20043;&#22806;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#33267;&#23569;&#26576;&#20123;&#35821;&#35328;&#33021;&#22815;&#24544;&#23454;&#21644;&#20934;&#30830;&#22320;&#36827;&#34892;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#20026;&#20840;&#29699;&#22823;&#22810;&#25968;&#38750;&#33521;&#35821;&#20351;&#29992;&#32773;&#25552;&#20379;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;LLMs&#20170;&#22825;&#65292;&#29305;&#21035;&#26159;&#24320;&#25918;&#30340;LLMs&#65292;&#36890;&#24120;&#20165;&#24847;&#20026;&#22312;&#33521;&#35821;&#65288;&#20363;&#22914;Llama2&#12289;Mistral&#65289;&#25110;&#23569;&#25968;&#20960;&#31181;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;&#20363;&#22914;Mixtral&#12289;Qwen&#65289;&#20013;&#20351;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#23384;&#22312;&#20351;&#29992;&#19978;&#30340;&#38480;&#21046;&#65292;&#20154;&#20204;&#20250;&#29992;&#35768;&#22810;&#19981;&#21516;&#30340;&#35821;&#35328;&#25552;&#31034;LLMs&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;LLMs&#22312;&#20854;&#39044;&#26399;&#20351;&#29992;&#33539;&#22260;&#20043;&#22806;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MultiQ&#65292;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#22522;&#26412;&#24320;&#25918;&#24335;&#38382;&#31572;&#30340;&#38134;&#26631;&#20934;&#22522;&#20934;&#65292;&#28085;&#30422;137&#31181;&#35821;&#35328;&#30340;27.4k&#20010;&#27979;&#35797;&#38382;&#39064;&#12290;&#36890;&#36807;MultiQ&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35821;&#35328;&#24544;&#23454;&#24230;&#65292;&#21363;&#27169;&#22411;&#26159;&#21542;&#20197;&#25552;&#31034;&#30340;&#35821;&#35328;&#22238;&#22797;&#65292;&#20197;&#21450;&#38382;&#39064;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#27979;&#35797;&#30340;&#25152;&#26377;LLMs&#23545;&#33267;&#23569;&#26576;&#20123;&#35821;&#35328;&#21709;&#24212;&#24471;&#24544;&#23454;&#21644;/&#25110;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03814v1 Announce Type: cross  Abstract: Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers. However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages. Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use. For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e.\ whether models respond in the prompted language, and question answering accuracy. All LLMs we test respond faithfully and/or accurately for at least some languages be
&lt;/p&gt;</description></item><item><title>&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;</title><link>https://arxiv.org/abs/2402.19088</link><description>&lt;p&gt;
&#23545;&#35821;&#20041;&#21464;&#21270;&#29305;&#24449;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey in Characterization of Semantic Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19088
&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#35821;&#35328;&#19981;&#26029;&#21457;&#23637;&#65292;&#20197;&#21560;&#32435;&#20154;&#31867;&#31038;&#20250;&#30340;&#25991;&#21270;&#21464;&#21270;&#12290;&#36825;&#31181;&#28436;&#21464;&#36890;&#36807;&#26032;&#35789;&#35821;&#65288;&#26032;&#21333;&#35789;&#65289;&#25110;&#21333;&#35789;&#30340;&#35821;&#20041;&#21464;&#21270;&#65288;&#36171;&#20104;&#24050;&#26377;&#21333;&#35789;&#26032;&#30340;&#21547;&#20041;&#65289;&#26469;&#20307;&#29616;&#12290;&#29702;&#35299;&#21333;&#35789;&#30340;&#21547;&#20041;&#23545;&#35299;&#37322;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#65288;&#22320;&#26041;&#29992;&#35821;&#25110;&#20442;&#35821;&#65289;&#12289;&#39046;&#22495;&#65288;&#20363;&#22914;&#25216;&#26415;&#26415;&#35821;&#65289;&#25110;&#26102;&#20195;&#30340;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#65292;&#36825;&#20123;&#21333;&#35789;&#19982;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30456;&#20851;&#65292;&#20363;&#22914;&#32763;&#35793;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#31561;&#12290;&#35821;&#20041;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#21644;&#24418;&#24335;&#21270;&#34920;&#24449;&#36825;&#20123;&#21464;&#21270;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#30740;&#31350;&#36825;&#31181;&#24433;&#21709;&#26159;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#36817;&#26399;&#24341;&#36215;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20960;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#26816;&#27979;&#35821;&#20041;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#21162;&#21147;&#26469;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19088v1 Announce Type: cross  Abstract: Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to charact
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24544;&#23454;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;FRODO&#26694;&#26550;&#26469;&#25913;&#36827;&#29983;&#25104;&#25512;&#29702;&#27493;&#39588;&#21644;&#22362;&#22266;&#25512;&#29702;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.13950</link><description>&lt;p&gt;
&#20351;&#25512;&#29702;&#21464;&#24471;&#37325;&#35201;&#65306;&#34913;&#37327;&#21644;&#25552;&#39640;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#30340;&#24544;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24544;&#23454;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;FRODO&#26694;&#26550;&#26469;&#25913;&#36827;&#29983;&#25104;&#25512;&#29702;&#27493;&#39588;&#21644;&#22362;&#22266;&#25512;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#32463;&#36807;&#36880;&#27493;&#25512;&#29702;&#24050;&#34987;&#35777;&#26126;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#26368;&#32456;&#31572;&#26696;&#19982;&#25152;&#36848;&#25512;&#29702;&#27493;&#39588;&#30340;&#24544;&#23454;&#31243;&#24230;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#23545;&#21313;&#20108;&#20010;LLMs&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#20197;&#26816;&#39564;LLM&#29983;&#25104;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;LLMs&#22312;&#29983;&#25104;&#31572;&#26696;&#26102;&#24182;&#19981;&#21487;&#38752;&#22320;&#20351;&#29992;&#20854;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FRODO&#65292;&#19968;&#20010;&#26088;&#22312;&#23450;&#21046;&#23567;&#22411;LM&#20197;&#29983;&#25104;&#27491;&#30830;&#25512;&#29702;&#27493;&#39588;&#24182;&#22312;&#36825;&#20123;&#27493;&#39588;&#19978;&#36827;&#34892;&#22362;&#22266;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;FRODO&#21253;&#25324;&#19968;&#20010;&#25512;&#26029;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#20351;&#29992;&#38544;&#24335;&#22240;&#26524;&#22870;&#21169;&#20989;&#25968;&#29983;&#25104;&#27491;&#30830;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#19988;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#20351;&#29992;&#21453;&#20107;&#23454;&#21644;&#22240;&#26524;&#20559;&#22909;&#30446;&#26631;&#22312;&#36825;&#20123;&#20013;&#38388;&#25512;&#29702;&#19978;&#24544;&#23454;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;F
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13950v1 Announce Type: new  Abstract: Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that F
&lt;/p&gt;</description></item><item><title>QuRating&#26159;&#19968;&#31181;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24179;&#34913;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.09739</link><description>&lt;p&gt;
&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;QuRating&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QuRating: Selecting High-Quality Data for Training Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09739
&lt;/p&gt;
&lt;p&gt;
QuRating&#26159;&#19968;&#31181;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24179;&#34913;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#21019;&#24314;&#33021;&#21147;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#24456;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QuRating&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22235;&#20010;&#29305;&#24449; - &#20889;&#20316;&#39118;&#26684;&#12289;&#25152;&#38656;&#19987;&#19994;&#30693;&#35782;&#12289;&#20107;&#23454;&#21644;&#29712;&#20107;&#20197;&#21450;&#25945;&#32946;&#20215;&#20540;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36776;&#21035;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#36827;&#34892;&#25991;&#26412;&#30340;&#37197;&#23545;&#21028;&#26029;&#26041;&#38754;&#27604;&#30452;&#25509;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#26356;&#22909;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;QuRater&#27169;&#22411;&#65292;&#20174;&#37197;&#23545;&#21028;&#26029;&#20013;&#23398;&#20064;&#26631;&#37327;&#35780;&#20998;&#65292;&#24182;&#20351;&#29992;&#23427;&#20026;260B&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#27599;&#20010;&#26631;&#20934;&#36827;&#34892;&#36136;&#37327;&#35780;&#32423;&#27880;&#37322;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#36136;&#37327;&#35780;&#32423;&#36873;&#25321;&#20102;30B&#20010;&#20196;&#29260;&#65292;&#24182;&#22312;&#25152;&#36873;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;13&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09739v1 Announce Type: new  Abstract: Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive. In this paper, we investigate four qualities - writing style, required expertise, facts &amp; trivia, and educational value. We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#24314;&#27169;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65292;&#36890;&#36807;&#25193;&#23637;&#21040;Deepfake Detection VQA&#20219;&#21153;&#26469;&#27169;&#25311;&#20154;&#31867;&#30452;&#35273;&#65292;&#35299;&#37322;&#26631;&#35760;&#22270;&#20687;&#20026;&#30495;&#23454;&#25110;&#20266;&#36896;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.00126</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Common Sense Reasoning for Deep Fake Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#24314;&#27169;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65292;&#36890;&#36807;&#25193;&#23637;&#21040;Deepfake Detection VQA&#20219;&#21153;&#26469;&#27169;&#25311;&#20154;&#31867;&#30452;&#35273;&#65292;&#35299;&#37322;&#26631;&#35760;&#22270;&#20687;&#20026;&#30495;&#23454;&#25110;&#20266;&#36896;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#29305;&#24449;&#36827;&#34892;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20108;&#20998;&#31867;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#30417;&#30563;&#35757;&#32451;&#19979;&#25552;&#21462;&#20102;&#21487;&#33021;&#30340;&#20266;&#36896;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#34920;&#31034;&#19981;&#33258;&#28982;&#30340;&#8220;&#38750;&#29289;&#29702;&#8221;&#35821;&#20041;&#38754;&#37096;&#23646;&#24615; - &#27169;&#31946;&#30340;&#21457;&#38469;&#32447;&#12289;&#21452;&#30473;&#27611;&#12289;&#20725;&#30828;&#30340;&#30643;&#23380;&#25110;&#19981;&#33258;&#28982;&#30340;&#30382;&#32932;&#30528;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#38754;&#37096;&#23646;&#24615;&#36890;&#24120;&#36890;&#36807;&#24120;&#35782;&#25512;&#29702;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#24863;&#30693;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26174;&#33879;&#24615;&#22270;&#25552;&#20379;&#35270;&#35273;&#35299;&#37322;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21487;&#33021;&#24456;&#38590;&#34987;&#20154;&#31867;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#24314;&#27169;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;Deepfake Detection VQA&#65288;DD-VQA&#65289;&#20219;&#21153;&#65292;&#30446;&#30340;&#26159;&#27169;&#25311;&#20154;&#31867;&#30452;&#35273;&#26469;&#35299;&#37322;&#26631;&#35760;&#22270;&#20687;&#20026;&#30495;&#23454;&#25110;&#20266;&#36896;&#30340;&#21407;&#22240;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#19982;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30456;&#20851;&#30340;&#38382;&#39064;&#25552;&#20379;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art approaches rely on image-based features extracted via neural networks for the deepfake detection binary classification. While these approaches trained in the supervised sense extract likely fake features, they may fall short in representing unnatural `non-physical' semantic facial attributes -- blurry hairlines, double eyebrows, rigid eye pupils, or unnatural skin shading. However, such facial attributes are generally easily perceived by humans via common sense reasoning. Furthermore, image-based feature extraction methods that provide visual explanation via saliency maps can be hard to be interpreted by humans. To address these challenges, we propose the use of common sense reasoning to model deepfake detection, and extend it to the Deepfake Detection VQA (DD-VQA) task with the aim to model human intuition in explaining the reason behind labeling an image as either real or fake. To this end, we introduce a new dataset that provides answers to the questions related to 
&lt;/p&gt;</description></item><item><title>&#22810;Agent&#36777;&#35770;&#65288;MAD&#65289;&#20316;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30495;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#23545;&#20110;&#35299;&#20915;&#30830;&#20445;&#29983;&#25104;&#20195;&#29702;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#31572;&#26696;&#30340;&#25361;&#25112;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#24418;&#24335;&#19979;&#30340;&#22810;Agent&#36777;&#35770;&#31995;&#32479;&#22312;&#21487;&#38752;&#24615;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2311.17371</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#30127;&#29378;&#21527;&#65311;&#22810;Agent&#36777;&#35770;&#31574;&#30053;&#23545;LLMs&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17371
&lt;/p&gt;
&lt;p&gt;
&#22810;Agent&#36777;&#35770;&#65288;MAD&#65289;&#20316;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30495;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#23545;&#20110;&#35299;&#20915;&#30830;&#20445;&#29983;&#25104;&#20195;&#29702;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#31572;&#26696;&#30340;&#25361;&#25112;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#24418;&#24335;&#19979;&#30340;&#22810;Agent&#36777;&#35770;&#31995;&#32479;&#22312;&#21487;&#38752;&#24615;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#22238;&#31572;&#21508;&#31181;&#39046;&#22495;&#38382;&#39064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#29983;&#25104;&#20195;&#29702;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#30340;&#31572;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22810;Agent&#36777;&#35770;&#65288;MAD&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;LLMs&#30495;&#23454;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#36777;&#35770;&#21644;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#35752;&#25104;&#26412;&#12289;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#30446;&#21069;&#24418;&#24335;&#19979;&#30340;&#22810;Agent&#36777;&#35770;&#31995;&#32479;&#22312;&#21487;&#38752;&#24615;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#20854;&#20182;&#24314;&#35758;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#22914;&#33258;&#19968;&#33268;&#24615;&#21644;&#20351;&#29992;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#36827;&#34892;&#38598;&#25104;&#12290;&#20294;&#26159;&#65292;&#22312;&#25191;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#26102;&#65292;&#19968;&#20123;MAD&#31995;&#32479;&#65292;&#22914;Multi-Persona&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#34920;&#26126;MAD&#21327;&#35758;&#21487;&#33021;&#24182;&#19981;&#20250;&#27604;&#20854;&#20182;&#26041;&#27861;&#22825;&#28982;&#26356;&#24046;&#65292;&#32780;&#26159;&#26356;&#23481;&#26131;&#21463;&#21040;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17371v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models (LLMs) underscore their potential for responding to inquiries in various domains. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a promising strategy for enhancing the truthfulness of LLMs. We benchmark a range of debating and prompting strategies to explore the trade-offs between cost, time, and accuracy. Importantly, we find that multi-agent debating systems, in their current form, do not reliably outperform other proposed prompting strategies, such as self-consistency and ensembling using multiple reasoning paths. However, when performing hyperparameter tuning, several MAD systems, such as Multi-Persona, perform better. This suggests that MAD protocols might not be inherently worse than other approaches, but that they are more sensitive to different hyperparameter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24615;&#25512;&#29702;&#21644;&#38382;&#39064;&#29983;&#25104;&#65292;&#23558;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMM)&#36171;&#20104;&#20102;&#26174;&#24615;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25512;&#29702;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10005</link><description>&lt;p&gt;
&#20197;&#26174;&#24615;&#25512;&#29702;&#38142;&#21644;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#25512;&#36827;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation. (arXiv:2401.10005v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24615;&#25512;&#29702;&#21644;&#38382;&#39064;&#29983;&#25104;&#65292;&#23558;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMM)&#36171;&#20104;&#20102;&#26174;&#24615;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25512;&#29702;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#33021;&#22815;&#35299;&#37322;&#21644;&#25512;&#29702;&#35270;&#35273;&#20869;&#23481;&#30340;&#26234;&#33021;&#31995;&#32479;&#38656;&#27714;&#36234;&#26469;&#36234;&#39640;&#65292;&#38656;&#35201;&#24320;&#21457;&#19981;&#20165;&#20934;&#30830;&#32780;&#19988;&#20855;&#26377;&#26174;&#24615;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#26174;&#24615;&#25512;&#29702;&#33021;&#21147;&#36171;&#20104;LMMs&#65292;&#22522;&#20110;&#35270;&#35273;&#20869;&#23481;&#21644;&#25991;&#26412;&#25351;&#23548;&#36827;&#34892;&#26174;&#24615;&#25512;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21487;&#20197;&#25552;&#38382;&#20197;&#33719;&#21462;&#24517;&#35201;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#25512;&#29702;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#30340;&#24320;&#21457;&#65292;&#26088;&#22312;&#20419;&#36827;&#24605;&#32500;&#38142;&#25512;&#29702;&#19982;&#25552;&#38382;&#26426;&#21046;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#24230;&#20855;&#26377;&#21306;&#22495;&#24847;&#35782;&#30340;LMM&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#30340;&#22797;&#26434;&#38656;&#27714;&#12290;&#35813;&#27169;&#22411;&#32463;&#21382;&#20102;&#19977;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#39318;&#20808;&#26159;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#65292;&#25509;&#19979;&#26469;&#26159;&#36890;&#36807;&#26174;&#24335;&#25512;&#29702;&#30340;&#38382;&#39064;&#29983;&#25104;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of Large Multi-Modal Models (LMMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to imbue an LMM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process. Our method comprises the development of a novel dataset generated by a Large Language Model (LLM), designed to promote chain-of-thought reasoning combined with a question-asking mechanism. We designed an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment. The model undergoes a three-stage training phase, starting with large-scale image-text alignment using a large-scale datasets, followed 
&lt;/p&gt;</description></item><item><title>AWQ&#26159;&#19968;&#31181;&#28608;&#27963;&#24863;&#30693;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25252;&#23569;&#37327;&#26174;&#33879;&#26435;&#37325;&#26469;&#38477;&#20302;&#37327;&#21270;&#35823;&#24046;&#65292;&#19981;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#25110;&#37325;&#26500;&#65292;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00978</link><description>&lt;p&gt;
AWQ&#65306;LLM&#21387;&#32553;&#19982;&#21152;&#36895;&#30340;&#28608;&#27963;&#24863;&#30693;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. (arXiv:2306.00978v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00978
&lt;/p&gt;
&lt;p&gt;
AWQ&#26159;&#19968;&#31181;&#28608;&#27963;&#24863;&#30693;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25252;&#23569;&#37327;&#26174;&#33879;&#26435;&#37325;&#26469;&#38477;&#20302;&#37327;&#21270;&#35823;&#24046;&#65292;&#19981;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#25110;&#37325;&#26500;&#65292;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#25552;&#39640;&#20102;&#20026;&#26381;&#21153;(&#20869;&#23384;&#22823;&#23567;)&#24102;&#26469;&#30340;&#30828;&#20214;&#38556;&#30861;&#65292;&#24182;&#38477;&#20302;&#20102;&#20196;&#29260;&#29983;&#25104;&#36895;&#24230;(&#20869;&#23384;&#24102;&#23485;)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28608;&#27963;&#24863;&#30693;&#26435;&#37325;&#37327;&#21270;(AWQ)&#30340;&#30828;&#20214;&#21451;&#22909;&#26041;&#27861;&#65292;&#29992;&#20110;LLM&#20302;&#27604;&#29305;&#26435;&#37325;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#26435;&#37325;&#24182;&#19981;&#26159;&#31561;&#37325;&#35201;&#30340;&#65307;&#20165;&#20445;&#25252;1%&#30340;&#26174;&#33879;&#26435;&#37325;&#23601;&#33021;&#22823;&#22823;&#38477;&#20302;&#37327;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#23547;&#25214;&#36890;&#36807;&#35266;&#23519;&#28608;&#27963;&#20540;&#32780;&#19981;&#26159;&#26435;&#37325;&#26469;&#20445;&#25252;&#26174;&#33879;&#26435;&#37325;&#30340;&#26368;&#20339;&#25353;&#36890;&#36947;&#32553;&#25918;&#26041;&#27861;&#12290;AWQ&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#21453;&#21521;&#20256;&#25773;&#25110;&#37325;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#24456;&#22909;&#22320;&#20445;&#25345;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24335;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#20250;&#36807;&#24230;&#25311;&#21512;&#26657;&#20934;&#38598;&#12290;AWQ&#22312;&#21508;&#31181;&#35821;&#35328;&#24314;&#27169;&#21644;&#39046;&#22495;&#29305;&#23450;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#30001;&#20110;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23427;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#37327;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth). In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. Our method is based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error. We then propose to search for the optimal per-channel scaling that protects the salient weights by observing the activation, not weights. AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on different domains and modalities, without overfitting to the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks. Thanks to better generalization, it achieves excellent quantiz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.11525</link><description>&lt;p&gt;
SIFT: &#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#20197;&#26368;&#22823;&#38480;&#24230;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#25928;&#29575;&#65288;&#19982;&#35757;&#32451;FLOPS&#30456;&#20851;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#65289;&#12290; &#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;FLOP&#65292;&#20294;&#20351;&#29992;&#31232;&#30095;&#26435;&#37325;&#36827;&#34892;&#35757;&#32451;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25439;&#22833;&#25110;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#21608;&#26399;&#65292;&#20351;&#24471;&#32467;&#26524;&#30340;&#35757;&#32451;&#25928;&#29575;&#19981;&#22815;&#28165;&#26224;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#31232;&#30095;&#24615;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#21516;&#30340;FLOPS&#65292;&#24182;&#36890;&#36807;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#23637;&#31034;&#35757;&#32451;&#25928;&#29575;&#25552;&#39640;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SIFT&#65292;&#19968;&#32452;&#29992;&#20316;&#23494;&#38598;&#23618;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#26469;&#25552;&#39640;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;FLOP&#25928;&#29575;&#30340;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#12290; &#27599;&#20010;&#36716;&#25442;&#37117;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#65288;&#31232;&#30095;&#32423;&#21035;&#65289;&#21442;&#25968;&#21270;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31232;&#30095;&#25513;&#33180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2207.07051</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#23545;&#25512;&#29702;&#20219;&#21153;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25512;&#29702;&#26159;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39640;&#20110;&#38543;&#26426;&#30340;&#24615;&#33021;&#65292;&#20294;&#23384;&#22312;&#35768;&#22810;&#19981;&#23436;&#21892;&#20043;&#22788;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#25277;&#35937;&#25512;&#29702;&#20063;&#26159;&#19981;&#23436;&#32654;&#30340;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25512;&#29702;&#21463;&#21040;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#21644;&#20449;&#24565;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#8220;&#20869;&#23481;&#25928;&#24212;&#8221;&#65307;&#24403;&#38382;&#39064;&#30340;&#35821;&#20041;&#20869;&#23481;&#25903;&#25345;&#27491;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#26102;&#65292;&#20154;&#31867;&#26356;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#20869;&#23481;&#32416;&#32544;&#30340;&#25512;&#29702;&#27169;&#24335;&#22312;&#20851;&#20110;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#24615;&#36136;&#30340;&#20105;&#35770;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28151;&#20837;&#20869;&#23481;&#26469;&#22238;&#31572;&#36923;&#36753;&#38382;&#39064;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#25429;&#25417;&#20102;&#19968;&#20123;&#20154;&#31867;&#30693;&#35782;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#21028;&#26029;&#19977;&#27573;&#35770;&#30340;&#36923;&#36753;&#26377;&#25928;&#24615;&#21644;Wason&#36873;&#25321;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
&lt;/p&gt;</description></item></channel></rss>