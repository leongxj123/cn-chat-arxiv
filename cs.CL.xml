<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01343</link><description>&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01343
&lt;/p&gt;
&lt;p&gt;
CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#21644;&#36719;&#20214;&#24179;&#21488;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#20351;&#29992;&#20687;GPT-3.5&#12289;GPT-4&#12289;GLM-3&#21644;LLaMa-2&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23458;&#25143;&#26381;&#21153;&#30340;&#32842;&#22825;&#36741;&#21161;&#25110;&#25512;&#29702;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#23458;&#25143;&#26381;&#21153;&#27169;&#22411;&#22312;&#19982;&#23458;&#25143;&#37197;&#32622;&#25991;&#20214;&#30340;&#38598;&#25104;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#24182;&#19988;&#32570;&#20047;&#26377;&#25928;&#26381;&#21153;&#25152;&#38656;&#30340;&#25805;&#20316;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#65288;CHat with custOmer Profile in existing System&#65289;&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#65306;&#65288;1&#65289;&#39640;&#25928;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#20197;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#25110;&#25353;&#29031;&#29616;&#26377;&#25351;&#21335;&#19982;&#36825;&#20123;&#31995;&#32479;&#20132;&#20114;&#65307;&#65288;2&#65289;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#22312;&#31995;&#32479;&#20013;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#65307;&#65288;3&#65289;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01343v1 Announce Type: cross  Abstract: Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.15744</link><description>&lt;p&gt;
&#35770;&#20027;&#21160;&#23398;&#20064;&#32773;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Fragility of Active Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23454;&#20363;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#26631;&#27880;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#65288;&#20363;&#22914;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20998;&#31867;&#22120;&#65289;&#65292;&#23427;&#20204;&#30340;&#30410;&#22788;&#24182;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#30340;&#32452;&#21512;&#22914;&#20309;&#21487;&#33021;&#25513;&#30422;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#30340;&#20219;&#20309;&#25910;&#30410;&#12290;&#19987;&#27880;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;AL&#25216;&#26415;&#65292;&#36825;&#20123;&#23454;&#39564;&#22312;&#25968;&#25454;&#38598;&#12289;&#25209;&#22823;&#23567;&#12289;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#26041;&#38754;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;AL&#21482;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#22659;&#20013;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20351;&#29992;&#19982;&#29616;&#23454;&#19990;&#30028;&#26399;&#26395;&#26356;&#22909;&#23545;&#40784;&#30340;&#24230;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24433;&#21709;&#22312;&#20110;&#23545;&#20174;&#19994;&#32773;&#30340;&#27934;&#23519;&#65306;(a) &#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#19982;AL&#25216;&#26415;&#30340;&#36873;&#25321;&#19968;&#26679;&#37325;&#35201;&#65292;(b) &#36873;&#25321;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15744v1 Announce Type: cross  Abstract: Active learning (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique.   Focusing on text classification, we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, text representation and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations.   The impact of this study is in its insights for a practitioner: (a) the choice of text representation and classifier is as important as that of an AL technique, (b) choice of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#25506;&#27979;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#30693;&#35782;&#65292;&#21457;&#29616;&#39318;&#20010;&#20196;&#29260;&#30340;logit&#20998;&#24067;&#21253;&#21547;&#36275;&#22815;&#20449;&#24687;&#65292;&#21487;&#20197;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#30340;&#35270;&#35273;&#38382;&#39064;&#12289;&#38450;&#33539;&#22810;&#27169;&#24577;&#36234;&#29425;&#25915;&#20987;&#20197;&#21450;&#35782;&#21035;&#27450;&#39575;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#20197;&#26377;&#25928;&#25913;&#21892;&#29983;&#25104;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.09037</link><description>&lt;p&gt;
&#31532;&#19968;&#20010;&#30693;&#36947;&#65306;&#20196;&#29260;&#20998;&#24067;&#22914;&#20309;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#30693;&#35782;&#65311;
&lt;/p&gt;
&lt;p&gt;
The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#25506;&#27979;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#30693;&#35782;&#65292;&#21457;&#29616;&#39318;&#20010;&#20196;&#29260;&#30340;logit&#20998;&#24067;&#21253;&#21547;&#36275;&#22815;&#20449;&#24687;&#65292;&#21487;&#20197;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#30340;&#35270;&#35273;&#38382;&#39064;&#12289;&#38450;&#33539;&#22810;&#27169;&#24577;&#36234;&#29425;&#25915;&#20987;&#20197;&#21450;&#35782;&#21035;&#27450;&#39575;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#20197;&#26377;&#25928;&#25913;&#21892;&#29983;&#25104;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#26088;&#22312;&#35299;&#37322;&#21644;&#21709;&#24212;&#20154;&#31867;&#25351;&#20196;&#65292;&#20294;&#30001;&#20110;&#19981;&#24403;&#25351;&#20196;&#32780;&#20598;&#23572;&#29983;&#25104;&#24187;&#35273;&#25110;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#32447;&#24615;&#25506;&#27979;&#26469;&#25581;&#31034;LVLMs&#36755;&#20986;&#23618;&#30340;&#38544;&#34255;&#30693;&#35782;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#39318;&#20010;&#20196;&#29260;&#30340;logit&#20998;&#24067;&#21253;&#21547;&#36275;&#22815;&#20449;&#24687;&#65292;&#21487;&#20197;&#30830;&#23450;&#26159;&#21542;&#24212;&#23545;&#25351;&#20196;&#20316;&#20986;&#21709;&#24212;&#65292;&#21253;&#25324;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#30340;&#35270;&#35273;&#38382;&#39064;&#12289;&#38450;&#33539;&#22810;&#27169;&#24577;&#36234;&#29425;&#25915;&#20987;&#20197;&#21450;&#35782;&#21035;&#27450;&#39575;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#38544;&#34255;&#30693;&#35782;&#22312;&#21709;&#24212;&#29983;&#25104;&#36807;&#31243;&#20013;&#38543;&#21518;&#20196;&#29260;&#30340;logit&#36880;&#28176;&#20002;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#22312;&#29983;&#25104;&#31532;&#19968;&#20010;&#20196;&#29260;&#26102;&#65292;&#26377;&#25928;&#25913;&#21892;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35265;&#35299;&#65306;&#39318;&#20808;&#65292;CLIP&#27169;&#22411;&#24050;&#32463;&#21253;&#21547;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#30340;&#24378;&#28872;&#20449;&#21495;&#65292;&#34920;&#26126;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09037v1 Announce Type: cross  Abstract: Large vision-language models (LVLMs), designed to interpret and respond to human instructions, occasionally generate hallucinated or harmful content due to inappropriate instructions. This study uses linear probing to shed light on the hidden knowledge at the output layer of LVLMs. We demonstrate that the logit distributions of the first tokens contain sufficient information to determine whether to respond to the instructions, including recognizing unanswerable visual questions, defending against multi-modal jailbreaking attack, and identifying deceptive questions. Such hidden knowledge is gradually lost in logits of subsequent tokens during response generation. Then, we illustrate a simple decoding strategy at the generation of the first token, effectively improving the generated content. In experiments, we find a few interesting insights: First, the CLIP model already contains a strong signal for solving these tasks, indicating poten
&lt;/p&gt;</description></item><item><title>HIRO &#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#25277;&#35937;&#24847;&#35265;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32034;&#24341;&#32467;&#26500;&#26469;&#25552;&#21462;&#36755;&#20837;&#35780;&#35770;&#20013;&#27969;&#34892;&#24847;&#35265;&#30340;&#21477;&#23376;&#31751;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#20851;&#30340;&#25688;&#35201;&#65292;&#24471;&#21040;&#26356;&#20855;&#35821;&#20041;&#32467;&#26500;&#30340;&#32534;&#30721;&#31354;&#38388;&#21644;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#25688;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.00435</link><description>&lt;p&gt;
&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24847;&#35265;&#25688;&#35201;&#30340;&#20998;&#23618;&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Indexing for Retrieval-Augmented Opinion Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00435
&lt;/p&gt;
&lt;p&gt;
HIRO &#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#25277;&#35937;&#24847;&#35265;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32034;&#24341;&#32467;&#26500;&#26469;&#25552;&#21462;&#36755;&#20837;&#35780;&#35770;&#20013;&#27969;&#34892;&#24847;&#35265;&#30340;&#21477;&#23376;&#31751;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#20851;&#30340;&#25688;&#35201;&#65292;&#24471;&#21040;&#26356;&#20855;&#35821;&#20041;&#32467;&#26500;&#30340;&#32534;&#30721;&#31354;&#38388;&#21644;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#25277;&#35937;&#24847;&#35265;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#25277;&#21462;&#26041;&#27861;&#30340;&#21487;&#24402;&#22240;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36830;&#36143;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;HIRO&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#23558;&#21477;&#23376;&#26144;&#23556;&#21040;&#36890;&#36807;&#35821;&#20041;&#32452;&#32455;&#30340;&#31163;&#25955;&#23618;&#27425;&#32467;&#26500;&#36335;&#24452;&#30340;&#32034;&#24341;&#32467;&#26500;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;&#25105;&#20204;&#22635;&#20805;&#32034;&#24341;&#24182;&#20351;&#29992;&#23427;&#26469;&#35782;&#21035;&#21644;&#26816;&#32034;&#21253;&#21547;&#36755;&#20837;&#35780;&#35770;&#20013;&#27969;&#34892;&#24847;&#35265;&#30340;&#21477;&#23376;&#31751;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;LLM&#29983;&#25104;&#19968;&#20010;&#22522;&#20110;&#36825;&#20123;&#25552;&#21462;&#30340;&#35777;&#25454;&#31751;&#30340;&#21487;&#35835;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#27169;&#22359;&#21270;&#24615;&#20801;&#35768;&#25105;&#20204;&#22312;&#27599;&#20010;&#38454;&#27573;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HIRO&#23398;&#20064;&#20102;&#27604;&#20808;&#21069;&#24037;&#20316;&#26356;&#20855;&#35821;&#20041;&#32467;&#26500;&#30340;&#32534;&#30721;&#31354;&#38388;&#65292;&#24182;&#29983;&#25104;&#20102;&#26356;&#31526;&#21512;&#36755;&#20837;&#35780;&#35770;&#20013;&#24847;&#35265;&#30340;&#25688;&#35201;&#12290;&#20154;&#31867;&#35780;&#20272;&#35777;&#23454;&#65292;HIRO&#29983;&#25104;&#30340;&#25688;&#35201;&#26356;&#36830;&#36143;&#12289;&#35814;&#32454;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00435v1 Announce Type: new  Abstract: We propose a method for unsupervised abstractive opinion summarization, that combines the attributability and scalability of extractive approaches with the coherence and fluency of Large Language Models (LLMs). Our method, HIRO, learns an index structure that maps sentences to a path through a semantically organized discrete hierarchy. At inference time, we populate the index and use it to identify and retrieve clusters of sentences containing popular opinions from input reviews. Then, we use a pretrained LLM to generate a readable summary that is grounded in these extracted evidential clusters. The modularity of our approach allows us to evaluate its efficacy at each stage. We show that HIRO learns an encoding space that is more semantically structured than prior work, and generates summaries that are more representative of the opinions in the input reviews. Human evaluation confirms that HIRO generates more coherent, detailed and accur
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#35843;&#26597;&#20102;&#31070;&#32463;LM&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#26469;&#35299;&#30721;&#34164;&#28085;&#21028;&#26029;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#36828;&#39640;&#20110;&#38543;&#26426;&#20960;&#29575;&#22320;&#35299;&#30721;&#33258;&#28982;&#21477;&#23376;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#26263;&#31034;LM&#38544;&#21547;&#22320;&#27169;&#25311;&#20102;&#35821;&#20041;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.13956</link><description>&lt;p&gt;
&#20320;&#33021;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#23398;&#20064;&#35821;&#20041;&#21527;&#65311;&#20197;&#34164;&#28085;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13956
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#35843;&#26597;&#20102;&#31070;&#32463;LM&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#26469;&#35299;&#30721;&#34164;&#28085;&#21028;&#26029;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#36828;&#39640;&#20110;&#38543;&#26426;&#20960;&#29575;&#22320;&#35299;&#30721;&#33258;&#28982;&#21477;&#23376;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#26263;&#31034;LM&#38544;&#21547;&#22320;&#27169;&#25311;&#20102;&#35821;&#20041;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Merrill&#31561;&#20154;&#65288;2022&#65289;&#35748;&#20026;&#65292;&#22312;&#29702;&#35770;&#19978;&#65292;&#26368;&#20248;LM&#39044;&#27979;&#30340;&#27010;&#29575;&#32534;&#30721;&#20102;&#20851;&#20110;&#34164;&#28085;&#20851;&#31995;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20294;&#26159;&#30001;&#20110;Merrill&#31561;&#20154;&#25552;&#20986;&#30340;&#24378;&#28872;&#29702;&#24819;&#21270;&#20551;&#35774;&#65292;&#19981;&#28165;&#26970;&#31070;&#32463;LM&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#19978;&#26159;&#21542;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#23398;&#20064;&#34164;&#28085;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20182;&#20204;&#30340;&#29702;&#35770;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#20174;&#31070;&#32463;LM&#20013;&#35299;&#30721;&#34164;&#28085;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#31867;&#20284;&#20110;&#20182;&#20204;&#30340;&#27979;&#35797;&#21487;&#20197;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#21644;LM&#20013;&#35299;&#30721;&#33258;&#28982;&#21477;&#23376;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#26426;&#20250;&#65292;&#23613;&#31649;&#19981;&#26159;&#23436;&#32654;&#30340;&#12290;&#36825;&#34920;&#26126;LM&#38544;&#21547;&#22320;&#27169;&#25311;&#20102;&#35821;&#20041;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#20197;&#39044;&#27979;&#21477;&#23376;&#20849;&#29616;&#27169;&#24335;&#19978;&#30340;&#35821;&#20041;&#25928;&#24212;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23454;&#38469;&#19978;&#39044;&#27979;&#34164;&#28085;&#30340;&#27979;&#35797;&#19982;&#29702;&#35770;&#27979;&#35797;&#30340;&#26041;&#21521;&#30456;&#21453;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#28508;&#22312;&#30340;&#29702;&#35770;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13956v1 Announce Type: new  Abstract: Do LMs infer the semantics of text from co-occurrence patterns in their training data? Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al. In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs. We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs. This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns. However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test. We thus revisit the assumptions underlying the o
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;ELECTRA&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25130;&#26029;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.13130</link><description>&lt;p&gt;
ELECTRA&#30340;&#21477;&#23376;&#23884;&#20837;&#26159;&#21542;&#26080;&#27861;&#20462;&#22797;&#65311;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;ELECTRA&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25130;&#26029;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;BERT&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#65292;&#20294;&#20854;&#39044;&#35757;&#32451;&#35745;&#31639;&#25104;&#26412;&#26159;&#19968;&#20010;&#26126;&#26174;&#30340;&#32570;&#28857;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;ELECTRA&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#20854;&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#34920;&#29616;&#19981;&#20339;&#12290;&#31038;&#21306;&#24708;&#28982;&#20572;&#27490;&#20351;&#29992;ELECTRA&#30340;&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#36827;&#34892;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#20351;&#29992;ELECTRA&#37492;&#21035;&#22120;&#30340;&#26368;&#21518;&#19968;&#23618;&#30456;&#23545;&#20110;&#36739;&#26089;&#30340;&#23618;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#31181;&#19979;&#38477;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20462;&#22797;ELECTRA&#23884;&#20837;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25130;&#26029;&#27169;&#22411;&#24494;&#35843;&#65288;TMFT&#65289;&#26041;&#27861;&#12290;&#22312;STS&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;TMFT&#23558;Spearman&#30456;&#20851;&#31995;&#25968;&#25552;&#39640;&#20102;8&#20010;&#22810;&#28857;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#21442;&#25968;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#21508;&#31181;&#27169;&#22411;&#22823;&#23567;&#21644;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;ELECTRA&#29983;&#25104;&#27169;&#22411;&#30340;&#24778;&#20154;&#21151;&#25928;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;BERT&#25345;&#24179;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13130v1 Announce Type: new  Abstract: While BERT produces high-quality sentence embeddings, its pre-training computational cost is a significant drawback. In contrast, ELECTRA delivers a cost-effective pre-training objective and downstream task performance improvements, but not as performant sentence embeddings. The community tacitly stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity (STS). We notice a significant drop in performance when using the ELECTRA discriminator's last layer in comparison to earlier layers. We explore this drop and devise a way to repair ELECTRA's embeddings, proposing a novel truncated model fine-tuning (TMFT) method. TMFT improves the Spearman correlation coefficient by over 8 points while increasing parameter efficiency on the STS benchmark dataset. We extend our analysis to various model sizes and languages. Further, we discover the surprising efficacy of ELECTRA's generator model, which performs on par with BERT, usi
&lt;/p&gt;</description></item><item><title>EmoBench&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12071</link><description>&lt;p&gt;
EmoBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
EmoBench: Evaluating the Emotional Intelligence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12071
&lt;/p&gt;
&lt;p&gt;
EmoBench&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20984;&#26174;&#20102;&#38656;&#35201;&#31283;&#20581;&#12289;&#20840;&#38754;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#24773;&#24863;&#26234;&#33021;&#65288;EI&#65289;&#36827;&#34892;&#35780;&#20272;&#30340;&#30740;&#31350;&#30456;&#24403;&#26377;&#38480;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#24773;&#24863;&#35782;&#21035;&#65292;&#24573;&#35270;&#20102;&#24773;&#24863;&#35843;&#33410;&#31561;&#37325;&#35201;&#30340;&#24773;&#24863;&#26234;&#33021;&#33021;&#21147;&#65292;&#32780;&#24773;&#24863;&#29702;&#35299;&#21017;&#20419;&#36827;&#24773;&#24863;; &#20854;&#27425;&#65292;&#23427;&#20204;&#20027;&#35201;&#22522;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#39057;&#32321;&#27169;&#24335;&#12289;&#26126;&#30830;&#20449;&#24687;&#21644;&#27880;&#37322;&#38169;&#35823;&#65292;&#23548;&#33268;&#35780;&#20272;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EmoBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20511;&#37492;&#20102;&#24050;&#24314;&#31435;&#30340;&#24515;&#29702;&#29702;&#35770;&#65292;&#24182;&#20026;&#26426;&#22120;EI&#25552;&#20986;&#20102;&#32508;&#21512;&#23450;&#20041;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;EmoBench&#21253;&#25324;&#19968;&#32452;400&#20010;&#29992;&#33521;&#35821;&#21644;&#20013;&#25991;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#65292;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#38656;&#35201;&#28145;&#20837;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12071v1 Announce Type: cross  Abstract: Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning
&lt;/p&gt;</description></item><item><title>BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.10373</link><description>&lt;p&gt;
BioMistral&#65306;&#38754;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10373
&lt;/p&gt;
&lt;p&gt;
BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#21644;&#21307;&#23398;&#31561;&#19987;&#19994;&#39046;&#22495;&#25552;&#20379;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#38024;&#23545;&#20581;&#24247;&#39046;&#22495;&#23450;&#21046;&#30340;&#24320;&#28304;LLMs&#21487;&#29992;&#65292;&#20294;&#23558;&#36890;&#29992;LLMs&#35843;&#25972;&#21040;&#21307;&#23398;&#39046;&#22495;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioMistral&#65292;&#19968;&#31181;&#19987;&#20026;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#28304;LLM&#65292;&#37319;&#29992;Mistral&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;PubMed Central&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;10&#20010;&#24050;&#24314;&#31435;&#30340;&#33521;&#25991;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#19978;&#23545;BioMistral&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#36890;&#36807;&#37327;&#21270;&#21644;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#33719;&#24471;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BioMistral&#30456;&#36739;&#20110;&#29616;&#26377;&#24320;&#28304;&#21307;&#23398;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#19982;&#19987;&#26377;&#23545;&#25163;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10373v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Diffusion-ES&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06559</link><description>&lt;p&gt;
Diffusion-ES:&#22522;&#20110;&#25193;&#25955;&#30340;&#38646;&#26799;&#24230;&#35268;&#21010;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#38646;&#38454;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Diffusion-ES&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#20915;&#31574;&#21644;&#25511;&#21046;&#20013;&#23545;&#22797;&#26434;&#21644;&#22810;&#27169;&#24577;&#36712;&#36857;&#20998;&#24067;&#24314;&#27169;&#26377;&#24456;&#24378;&#20248;&#21183;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22870;&#21169;&#26799;&#24230;&#24341;&#23548;&#21435;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#20135;&#29983;&#22312;&#25193;&#25955;&#27169;&#22411;&#25152;&#25429;&#33719;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#21487;&#24494;&#20998;&#22870;&#21169;&#20989;&#25968;&#21644;&#20284;&#28982;&#24615;&#30340;&#36712;&#36857;&#12290;&#22870;&#21169;&#26799;&#24230;&#24341;&#23548;&#21435;&#22122;&#38656;&#35201;&#19968;&#20010;&#36866;&#21512;&#20110;&#28165;&#27905;&#21644;&#22122;&#22768;&#26679;&#26412;&#30340;&#21487;&#24494;&#20998;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#36712;&#36857;&#20248;&#21270;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffusionES&#65292;&#19968;&#31181;&#23558;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#27969;&#24418;&#20013;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;Diffusion-ES&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#12290;&#23427;&#36890;&#36807;&#25130;&#26029;&#25193;&#25955;&#36807;&#31243;&#23545;&#24471;&#20998;&#39640;&#30340;&#36712;&#36857;&#36827;&#34892;&#21464;&#24322;&#65292;&#35813;&#36807;&#31243;&#24212;&#29992;&#23569;&#37327;&#30340;&#22122;&#22768;&#21644;&#21435;&#22122;&#27493;&#39588;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models excel at modeling complex and multimodal trajectory distributions for decision-making and control. Reward-gradient guided denoising has been recently proposed to generate trajectories that maximize both a differentiable reward function and the likelihood under the data distribution captured by a diffusion model. Reward-gradient guided denoising requires a differentiable reward function fitted to both clean and noised samples, limiting its applicability as a general trajectory optimizer. In this paper, we propose DiffusionES, a method that combines gradient-free optimization with trajectory denoising to optimize black-box non-differentiable objectives while staying in the data manifold. Diffusion-ES samples trajectories during evolutionary search from a diffusion model and scores them using a black-box reward function. It mutates high-scoring trajectories using a truncated diffusion process that applies a small number of noising and denoising steps, allowing for much mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04482</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#25345;&#32493;&#23398;&#20064;&#26032;&#35789;
&lt;/p&gt;
&lt;p&gt;
Continuously Learning New Words in Automatic Speech Recognition. (arXiv:2401.04482v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04482
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20173;&#28982;&#36828;&#26410;&#23436;&#32654;&#12290;&#20856;&#22411;&#30340;&#38169;&#35823;&#21253;&#25324;&#32553;&#20889;&#35789;&#12289;&#21629;&#21517;&#23454;&#20307;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#19987;&#29992;&#35789;&#65292;&#36825;&#20123;&#35789;&#20960;&#20046;&#27809;&#26377;&#25110;&#27809;&#26377;&#25968;&#25454;&#21487;&#29992;&#26469;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#35782;&#21035;&#36825;&#20123;&#35789;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#32473;&#23450;&#24102;&#26377;&#23545;&#24212;&#24187;&#28783;&#29255;&#30340;&#35762;&#24231;&#24405;&#38899;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;ASR&#27169;&#22411;&#26469;&#23558;&#27169;&#22411;&#20559;&#21521;&#20110;&#20174;&#24187;&#28783;&#29255;&#20013;&#35299;&#30721;&#26032;&#35789;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35762;&#24231;&#36827;&#34892;&#25512;&#29702;&#65292;&#23558;&#21253;&#21547;&#26816;&#27979;&#21040;&#30340;&#26032;&#35789;&#30340;&#35805;&#35821;&#25910;&#38598;&#21040;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#20013;&#12290;&#25509;&#30528;&#65292;&#23545;&#36825;&#20010;&#38598;&#21512;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#35843;&#25972;&#28155;&#21152;&#21040;&#27169;&#22411;&#30340;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#30340;&#20302;&#31209;&#30697;&#38453;&#26435;&#37325;&#12290;&#25972;&#20010;&#36807;&#31243;&#23545;&#22810;&#20010;&#35762;&#24231;&#36827;&#34892;&#36845;&#20195;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#33719;&#24471;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65288;&#36229;&#36807;80%&#30340;&#21484;&#22238;&#29575;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.05506</link><description>&lt;p&gt;
&#26597;&#35810;&#21644;&#24212;&#31572;&#22686;&#24378;&#19981;&#33021;&#24110;&#21161;&#39046;&#22495;&#22806;&#25968;&#23398;&#25512;&#29702;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization. (arXiv:2310.05506v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#26102;&#65292;&#36890;&#36807;&#26597;&#35810;&#28436;&#21270;&#21644;&#22810;&#26679;&#21270;&#25512;&#29702;&#36335;&#24452;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#32463;&#39564;&#19978;&#34987;&#39564;&#35777;&#20026;&#26377;&#25928;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#24320;&#28304;LLMs&#21644;&#39030;&#23574;&#19987;&#26377;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#26088;&#22312;&#22238;&#31572;&#65306;&#65288;1&#65289;&#21738;&#20123;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26356;&#26377;&#25928;&#65307;&#65288;2&#65289;&#22686;&#24378;&#25968;&#25454;&#37327;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32553;&#25918;&#20851;&#31995;&#22914;&#20309;&#65307;&#65288;3&#65289;&#25968;&#25454;&#22686;&#24378;&#33021;&#21542;&#28608;&#21169;&#39046;&#22495;&#22806;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#30340;&#27867;&#21270;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;GSM8K&#26597;&#35810;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#20197;&#21450;&#37319;&#26679;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;AugGSM8K&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;AugGSM8K&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;LLMs&#65292;&#31216;&#20026;MuggleMath&#12290;MuggleMath&#22312;GSM8K&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65288;&#22312;7B&#35268;&#27169;&#19978;&#20174;54%&#25552;&#39640;&#21040;68.4%&#65292;&#22312;&#25193;&#25918;&#21040;63.9%&#21040;74.0%&#20043;&#38388;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K, by complicating and diversifying the queries from GSM8K and sampling multiple reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the scal
&lt;/p&gt;</description></item><item><title>&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06979</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#26159;&#36890;&#29992;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-Regressive Next-Token Predictors are Universal Learners. (arXiv:2309.06979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06979
&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#22312;&#36923;&#36753;&#21644;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#38750;&#20961;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#35757;&#32451;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#31616;&#21333;&#20219;&#21153;&#19978;&#30340;&#32593;&#32476;&#20013;&#20986;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#33258;&#22238;&#24402;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#22914;&#32447;&#24615;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#24403;&#20854;&#22312;&#24605;&#32500;&#38142;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#8212;&#8212;&#38271;&#24230;&#22797;&#26434;&#24230;&#65292;&#23427;&#34913;&#37327;&#20102;&#22312;&#36817;&#20284;&#26576;&#20010;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#24605;&#32500;&#38142;&#24207;&#21015;&#20013;&#25152;&#38656;&#30340;&#20013;&#38388;&#26631;&#35760;&#30340;&#25968;&#37327;&#65292;&#24182;&#20998;&#26512;&#20102;&#38271;&#24230;&#22797;&#26434;&#24230;&#21644;&#20854;&#20182;&#22797;&#26434;&#24615;&#27010;&#24565;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#31616;&#21333;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#22914;&#32447;&#24615;&#32593;&#32476;&#21644;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.03415</link><description>&lt;p&gt;
&#20302;&#24310;&#36831;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#31471;&#21040;&#31471;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
End-to-End Evaluation for Low-Latency Simultaneous Speech Translation. (arXiv:2308.03415v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#25361;&#25112;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#35768;&#22810;&#20986;&#29256;&#29289;&#21644;&#20849;&#20139;&#20219;&#21153;&#20063;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#35780;&#20272;&#36825;&#20123;&#19981;&#21516;&#30340;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#31995;&#32479;&#30340;&#29305;&#23450;&#26041;&#38754;&#34987;&#35780;&#20272;&#65292;&#24182;&#19988;&#24448;&#24448;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#25191;&#34892;&#21644;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#21508;&#20010;&#26041;&#38754;&#30340;&#26694;&#26550;&#12290;&#35780;&#20272;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#30340;&#65292;&#21253;&#25324;&#38899;&#39057;&#30340;&#20998;&#27573;&#20197;&#21450;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26694;&#26550;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20855;&#26377;&#20462;&#35746;&#36755;&#20986;&#36873;&#39033;&#30340;&#27169;&#22411;&#20197;&#21450;&#20855;&#26377;&#22266;&#23450;&#36755;&#20986;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30452;&#25509;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#32423;&#32852;&#31995;&#32479;&#21644;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#20010;&#32479;&#19968;&#30340;&#24230;&#37327;&#26469;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches.  In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components.  Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework all
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25216;&#33021;&#25351;&#23548; (SKiC) &#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#28436;&#31034;&#22522;&#26412;&#25216;&#33021;&#21644;&#32452;&#21512;&#24615;&#31034;&#20363;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#35299;&#20915;&#26356;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#21462;&#24471;&#20960;&#20046;&#23436;&#32654;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.00304</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#38145;&#32452;&#21512;&#24615;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;: &#25216;&#33021;&#25351;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models. (arXiv:2308.00304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25216;&#33021;&#25351;&#23548; (SKiC) &#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#28436;&#31034;&#22522;&#26412;&#25216;&#33021;&#21644;&#32452;&#21512;&#24615;&#31034;&#20363;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#35299;&#20915;&#26356;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#21462;&#24471;&#20960;&#20046;&#23436;&#32654;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#28608;&#21457;&#32452;&#21512;&#24615;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#32452;&#21512;&#24615;&#27867;&#21270;&#20351;&#24471;LLMs&#33021;&#22815;&#35299;&#20915;&#27604;&#23427;&#20204;&#25152;&#35265;&#36807;&#30340;&#38382;&#39064;&#26356;&#22256;&#38590;&#30340;&#38382;&#39064;&#65288;&#21363;&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#65289;&#65292;&#36825;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#20851;&#38190;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#36825;&#31181;&#24418;&#24335;&#30340;&#25512;&#29702;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#25216;&#33021;&#25351;&#23548;&#65288;SKiC&#65289;&#25552;&#31034;&#65292;&#23427;&#25351;&#23548;LLMs&#22914;&#20309;&#32452;&#21512;&#22522;&#26412;&#25216;&#33021;&#26469;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#30456;&#21516;&#30340;&#25552;&#31034;&#19978;&#23637;&#31034;&#25216;&#33021;&#21644;&#32452;&#21512;&#24615;&#31034;&#20363;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20165;&#20165;&#36890;&#36807;&#20004;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#30340;SKiC&#25552;&#31034;&#22312;&#25216;&#33021;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#33021;&#21147;&#20043;&#38388;&#24418;&#25104;&#20102;&#24378;&#22823;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#36171;&#20104;&#20102;LLMs&#35299;&#20915;&#38656;&#35201;&#21019;&#26032;&#25216;&#33021;&#32452;&#21512;&#30340;&#26410;&#35265;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of eliciting compositional generalization capabilities in large language models (LLMs) with a novel type of prompting strategy. Compositional generalization empowers the LLMs to solve problems that are harder than the ones they have seen (i.e., easy-to-hard generalization), which is a critical reasoning capability of human-like intelligence. However, even the current state-of-the-art LLMs still struggle with this form of reasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting, which instructs LLMs how to compose basic skills to resolve more complex problems. We find that it is crucial to demonstrate both the skills and the compositional examples within the same prompting context. With as few as two examplars, our SKiC prompting initiates strong synergies between skills and their composition capabilities. Notably, it empowers LLMs to solve unseen problems that require innovative skill compositions, achieving near-perfect generalization on a b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13062</link><description>&lt;p&gt;
GPT4Table&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#21527;&#65311;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#25512;&#29702;&#22120;&#26469;&#35299;&#20915;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#36234;&#26469;&#36234;&#20855;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#30340;&#29702;&#35299;&#31243;&#24230;&#36824;&#26377;&#24456;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#22320;&#26041;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#34920;&#26684;&#24207;&#21015;&#21270;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;LLMs&#26159;&#21542;&#30495;&#27491;&#33021;&#22815;&#29702;&#35299;&#36825;&#31867;&#25968;&#25454;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;LLMs&#30340;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65288;SUC&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19971;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#21333;&#20803;&#26684;&#26597;&#25214;&#12289;&#34892;&#26816;&#32034;&#21644;&#22823;&#23567;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#24615;&#33021;&#22240;&#22810;&#31181;&#36755;&#20837;&#36873;&#25321;&#32780;&#24322;&#65292;&#21253;&#25324;&#34920;&#26684;&#36755;&#20837;&#26684;&#24335;&#12289;&#20869;&#23481;&#39034;&#24207;&#12289;&#35282;&#33394;&#25552;&#31034;&#21644;&#20998;&#21306;&#26631;&#35760;&#31561;&#12290;&#26681;&#25454;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#25152;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#26412;&#36136;&#65292;&#21363;&#26469;&#33258;&#20110;&#25945;&#24072;&#27169;&#22411;&#30340;top-1&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;&#22522;&#20110;&#35789;&#32423;&#21035;&#30340;&#30693;&#35782;&#33976;&#39311;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;Top-1 Information&#12290;</title><link>http://arxiv.org/abs/2305.08096</link><description>&lt;p&gt;
&#25506;&#31350;&#21644;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation. (arXiv:2305.08096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#26412;&#36136;&#65292;&#21363;&#26469;&#33258;&#20110;&#25945;&#24072;&#27169;&#22411;&#30340;top-1&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;&#22522;&#20110;&#35789;&#32423;&#21035;&#30340;&#30693;&#35782;&#33976;&#39311;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;Top-1 Information&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#20013;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22312;&#21738;&#37324;&#38544;&#34255;&#30340;&#38382;&#39064;&#20173;&#19981;&#28165;&#26970;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#30693;&#35782;&#33976;&#39311;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#23454;&#35777;&#35282;&#24230;&#25581;&#24320;&#20102;&#36825;&#20010;&#35868;&#22242;&#65292;&#24182;&#23637;&#31034;&#20102;&#30693;&#35782;&#26469;&#33258;&#25945;&#24072;&#30340;top-1&#39044;&#27979;&#65292;&#36825;&#20063;&#24110;&#21161;&#25105;&#20204;&#24314;&#31435;&#20102;&#35789;&#32423;&#21644;&#24207;&#21015;&#32423;&#33976;&#39311;&#20043;&#38388;&#30340;&#28508;&#22312;&#36830;&#25509;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#25351;&#20986;&#20102;&#22522;&#30784;&#35789;&#32423;&#33976;&#39311;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#30693;&#35782;&#30340;&#24403;&#21069;&#30446;&#26631;&#26159;&#23558;&#27880;&#24847;&#21147;&#25193;&#25955;&#21040;&#25972;&#20010;&#20998;&#24067;&#19978;&#23398;&#20064;&#30693;&#35782;&#65292;&#20294;&#32570;&#20047;&#23545;&#26368;&#20851;&#38190;&#30340;top-1&#20449;&#24687;&#30340;&#29305;&#27530;&#22788;&#29702;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#25945;&#24072;&#30340;top-1&#39044;&#27979;&#19982;&#22320;&#38754;&#23454;&#20917;&#26631;&#35760;&#37325;&#21472;&#65292;&#22240;&#27492;&#30693;&#35782;&#34987;&#40644;&#37329;&#20449;&#24687;&#25152;&#21344;&#25454;&#65292;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#28508;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{T}op-1 \textbf{I}nformation&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is a promising technique for model compression in neural machine translation. However, where the knowledge hides in KD is still not clear, which may hinder the development of KD. In this work, we first unravel this mystery from an empirical perspective and show that the knowledge comes from the top-1 predictions of teachers, which also helps us build a potential connection between word- and sequence-level KD. Further, we point out two inherent issues in vanilla word-level KD based on this finding. Firstly, the current objective of KD spreads its focus to whole distributions to learn the knowledge, yet lacks special treatment on the most crucial top-1 information. Secondly, the knowledge is largely covered by the golden information due to the fact that most top-1 predictions of teachers overlap with ground-truth tokens, which further restricts the potential of KD. To address these issues, we propose a novel method named \textbf{T}op-1 \textbf{I}nformation \te
&lt;/p&gt;</description></item></channel></rss>