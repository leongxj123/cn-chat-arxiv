<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#36890;&#36807;&#26368;&#26032;&#30340;&#22810;LoRA&#25512;&#29702;&#25216;&#26415;&#21644;&#23450;&#21046;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38548;&#31163;&#12289;&#21152;&#23494;&#21644;&#36523;&#20221;&#39564;&#35777;&#30340;&#23433;&#20840;&#26381;&#21153;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00913</link><description>&lt;p&gt;
&#29992;&#20110;&#23433;&#20840;&#33258;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#30340;&#26426;&#26500;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Institutional Platform for Secure Self-Service Large Language Model Exploration
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00913
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#36890;&#36807;&#26368;&#26032;&#30340;&#22810;LoRA&#25512;&#29702;&#25216;&#26415;&#21644;&#23450;&#21046;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38548;&#31163;&#12289;&#21152;&#23494;&#21644;&#36523;&#20221;&#39564;&#35777;&#30340;&#23433;&#20840;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30001;&#32943;&#22612;&#22522;&#22823;&#23398;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#20013;&#24515;&#24320;&#21457;&#30340;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#26131;&#20110;&#20351;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#22312;&#22810;LoRA&#25512;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#31995;&#32479;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#21508;&#31867;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#23450;&#21046;&#36866;&#37197;&#22120;&#12290;&#35770;&#25991;&#27010;&#36848;&#20102;&#31995;&#32479;&#30340;&#26550;&#26500;&#21644;&#20851;&#38190;&#29305;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#31574;&#21010;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#23433;&#20840;&#25512;&#29702;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#31199;&#25143;&#24847;&#35782;&#30340;&#35745;&#31639;&#32593;&#32476;&#65292;&#22312;&#23433;&#20840;&#22320;&#21033;&#29992;&#23396;&#31435;&#36164;&#28304;&#23707;&#30340;&#22522;&#30784;&#19978;&#24418;&#25104;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31995;&#32479;&#12290;&#35813;&#24179;&#21488;&#33268;&#21147;&#20110;&#25552;&#20379;&#23433;&#20840;&#30340;LLM&#26381;&#21153;&#65292;&#24378;&#35843;&#36807;&#31243;&#21644;&#25968;&#25454;&#38548;&#31163;&#12289;&#31471;&#21040;&#31471;&#21152;&#23494;&#20197;&#21450;&#22522;&#20110;&#35282;&#33394;&#30340;&#36164;&#28304;&#36523;&#20221;&#39564;&#35777;&#12290;&#35813;&#36129;&#29486;&#19982;&#23454;&#29616;&#31616;&#21270;&#35775;&#38382;&#20808;&#36827;&#30340;AI&#27169;&#22411;&#21644;&#25216;&#26415;&#20197;&#25903;&#25345;&#31185;&#23398;&#21457;&#29616;&#30340;&#24635;&#20307;&#30446;&#26631;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make large, customized language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction.   We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery.
&lt;/p&gt;</description></item><item><title>WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2404.00656</link><description>&lt;p&gt;
WavLLM&#65306;&#38754;&#21521;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WavLLM: Towards Robust and Adaptive Speech Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00656
&lt;/p&gt;
&lt;p&gt;
WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36880;&#28176;&#25299;&#23485;&#20102;&#23427;&#20204;&#30340;&#33539;&#22260;&#21040;&#22810;&#27169;&#24577;&#24863;&#30693;&#21644;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#21548;&#35273;&#33021;&#21147;&#25972;&#21512;&#21040;LLMs&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#36328;&#19981;&#21516;&#35821;&#22659;&#21644;&#25191;&#34892;&#22797;&#26434;&#21548;&#35273;&#20219;&#21153;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WavLLM&#65292;&#19968;&#20010;&#20855;&#26377;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#30340;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#21033;&#29992;Whisper&#32534;&#30721;&#22120;&#22788;&#29702;&#35821;&#38899;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#21033;&#29992;WavLM&#32534;&#30721;&#22120;&#25429;&#25417;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#22312;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;WavLLM&#39318;&#20808;&#36890;&#36807;&#28151;&#21512;&#35201;&#32032;&#36827;&#34892;&#20248;&#21270;&#26469;&#24314;&#31435;&#20854;&#22522;&#30784;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00656v1 Announce Type: cross  Abstract: The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elemen
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2404.00282</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#26597;:&#27010;&#24565;&#12289;&#20998;&#31867;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00282
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25317;&#26377;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#39640;&#32423;&#36890;&#29992;&#33021;&#21147;&#65292;&#23427;&#20204;&#22312;&#22686;&#24378;&#23398;&#20064;&#26041;&#38754;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#20219;&#21153;&#35268;&#21010;&#31561;&#26041;&#38754;&#23637;&#29616;&#20986;&#28508;&#21147;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#29616;&#26377;$\textit{LLM&#22686;&#24378;RL}$&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#20854;&#19982;&#20256;&#32479;RL&#26041;&#27861;&#30340;&#29305;&#24449;&#65292;&#26088;&#22312;&#28548;&#28165;&#30740;&#31350;&#33539;&#22260;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21033;&#29992;&#32463;&#20856;&#30340;Agent-&#29615;&#22659;&#20132;&#20114;&#33539;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#27861;&#65292;&#31995;&#32479;&#22320;&#23558;LLMs&#22312;RL&#20013;&#30340;&#21151;&#33021;&#20998;&#31867;&#65292;&#21253;&#25324;&#22235;&#31181;&#35282;&#33394;&#65306;&#20449;&#24687;&#22788;&#29702;&#22120;&#12289;&#22870;&#21169;&#35774;&#35745;&#32773;&#12289;&#20915;&#31574;&#32773;&#21644;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#27599;&#20010;&#35282;&#33394;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26041;&#27861;&#35770;&#65292;&#20998;&#26512;&#20102;&#32531;&#35299;&#30340;&#29305;&#23450;RL&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#28508;&#22312;&#24212;&#29992;&#12289;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REval&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#19982;&#31243;&#24207;&#25191;&#34892;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16437</link><description>&lt;p&gt;
&#20351;&#29992;&#31243;&#24207;&#25191;&#34892;&#36816;&#34892;&#26102;&#34892;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models with Runtime Behavior of Program Execution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REval&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#19982;&#31243;&#24207;&#25191;&#34892;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#20195;&#30721;LLMs&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#20195;&#30721;LLMs&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20934;&#65288;&#22914;HumanEval&#21644;ClassEval&#65289;&#12290;&#20195;&#30721;&#25512;&#29702;&#26159;&#20195;&#30721;LLMs&#26368;&#37325;&#35201;&#30340;&#33021;&#21147;&#20043;&#19968;&#65292;&#20294;&#29616;&#26377;&#30340;&#20195;&#30721;&#25512;&#29702;&#22522;&#20934;&#19981;&#36275;&#12290;&#36890;&#24120;&#65292;&#23427;&#20204;&#37325;&#28857;&#39044;&#27979;&#31243;&#24207;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#24573;&#30053;&#20102;&#31243;&#24207;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#20013;&#38388;&#34892;&#20026;&#35780;&#20272;&#65292;&#20197;&#21450;&#36923;&#36753;&#19968;&#33268;&#24615;&#65288;&#20363;&#22914;&#65292;&#22914;&#26524;&#25191;&#34892;&#36335;&#24452;&#39044;&#27979;&#38169;&#35823;&#65292;&#21017;&#27169;&#22411;&#19981;&#24212;&#35813;&#32473;&#20986;&#27491;&#30830;&#30340;&#36755;&#20986;&#65289;&#22312;&#25191;&#34892;&#25512;&#29702;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REval&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#19982;&#31243;&#24207;&#25191;&#34892;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20195;&#30721;&#22522;&#20934;&#65292;&#24182;&#23558;&#23427;&#20204;&#36866;&#24212;&#21040;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30340;&#26032;&#22522;&#20934;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16437v1 Announce Type: cross  Abstract: Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs, but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely REval, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framew
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.15744</link><description>&lt;p&gt;
&#35770;&#20027;&#21160;&#23398;&#20064;&#32773;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Fragility of Active Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23454;&#20363;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#26631;&#27880;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#65288;&#20363;&#22914;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20998;&#31867;&#22120;&#65289;&#65292;&#23427;&#20204;&#30340;&#30410;&#22788;&#24182;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#30340;&#32452;&#21512;&#22914;&#20309;&#21487;&#33021;&#25513;&#30422;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#30340;&#20219;&#20309;&#25910;&#30410;&#12290;&#19987;&#27880;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;AL&#25216;&#26415;&#65292;&#36825;&#20123;&#23454;&#39564;&#22312;&#25968;&#25454;&#38598;&#12289;&#25209;&#22823;&#23567;&#12289;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#26041;&#38754;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;AL&#21482;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#22659;&#20013;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20351;&#29992;&#19982;&#29616;&#23454;&#19990;&#30028;&#26399;&#26395;&#26356;&#22909;&#23545;&#40784;&#30340;&#24230;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24433;&#21709;&#22312;&#20110;&#23545;&#20174;&#19994;&#32773;&#30340;&#27934;&#23519;&#65306;(a) &#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#19982;AL&#25216;&#26415;&#30340;&#36873;&#25321;&#19968;&#26679;&#37325;&#35201;&#65292;(b) &#36873;&#25321;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15744v1 Announce Type: cross  Abstract: Active learning (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique.   Focusing on text classification, we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, text representation and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations.   The impact of this study is in its insights for a practitioner: (a) the choice of text representation and classifier is as important as that of an AL technique, (b) choice of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20027;&#39064;&#21644;&#37322;&#20041;&#29983;&#25104;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#32469;&#21475;&#20196;&#30340;&#26032;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#32469;&#21475;&#20196;&#25968;&#25454;&#38598;TwistList 2.0&#65292;&#24182;&#36827;&#34892;&#20102;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.13901</link><description>&lt;p&gt;
&#35757;&#32451;&#19982;&#38480;&#21046;&#65306;&#20174;&#20027;&#39064;&#21644;&#37322;&#20041;&#29983;&#25104;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#32469;&#21475;&#20196;
&lt;/p&gt;
&lt;p&gt;
Train &amp; Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20027;&#39064;&#21644;&#37322;&#20041;&#29983;&#25104;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#32469;&#21475;&#20196;&#30340;&#26032;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#32469;&#21475;&#20196;&#25968;&#25454;&#38598;TwistList 2.0&#65292;&#24182;&#36827;&#34892;&#20102;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#22312;&#38899;&#38901;&#21644;&#35821;&#38899;&#22522;&#30784;&#30340;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#39046;&#22495;&#65292;&#22914;&#21452;&#20851;&#35821;&#21644;&#35799;&#27468;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20135;&#29983;&#32469;&#21475;&#20196;&#30340;&#26032;&#24037;&#20316;-&#36825;&#31181;&#35821;&#35328;&#24418;&#24335;&#38656;&#35201;&#22312;&#38899;&#32032;&#32423;&#21035;&#19978;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#23454;&#29616;&#22768;&#38899;&#37325;&#21472;&#65292;&#21516;&#26102;&#19982;&#36755;&#20837;&#20027;&#39064;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#65292;&#20173;&#28982;&#20445;&#25345;&#35821;&#27861;&#27491;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TwisterLister&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#29983;&#25104;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#32469;&#21475;&#20196;&#30340;&#27969;&#31243;&#65292;&#25105;&#20204;&#29992;&#23427;&#26469;&#29983;&#25104;TwistList 2.0&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#26368;&#22823;&#30340;&#19968;&#20010;&#24050;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#20154;&#31867;&#21644;LLM&#20316;&#32773;&#21512;&#20316;&#30340;&#36229;&#36807;17K&#20010;&#20363;&#23376;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#27969;&#31243;&#28041;&#21450;&#20351;&#29992;&#38899;&#38901;&#21463;&#38480;&#35789;&#27719;&#20197;&#21450;LLM&#25552;&#31034;&#26469;&#29983;&#25104;&#26032;&#39062;&#30340;&#12289;&#38750;&#34893;&#29983;&#30340;&#32469;&#21475;&#20196;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23545;&#36739;&#23567;&#35268;&#27169;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13901v1 Announce Type: new  Abstract: Previous work in phonologically and phonetically grounded language generation has mainly focused on domains such as puns and poetry. In this article, we present new work on the generation of tongue-twisters - a form of language that is required to be conditioned on a phoneme level to maximize sound overlap, whilst maintaining semantic consistency with an input topic and still being grammatically correct. We present TwisterLister, a pipeline for generating phonologically informed tongue-twisters from Large Language Models (LLMs) that we use to generate TwistList 2.0, the largest annotated dataset of tongue-twisters to date, consisting of 17K+ examples from a combination of human and LLM authors. Our generation pipeline involves the use of a phonologically constrained vocabulary alongside LLM prompting to generate novel, non-derivative tongue-twister examples. We additionally present the results of automatic and human evaluation of smaller
&lt;/p&gt;</description></item><item><title>m&amp;m's&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;4K+&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;33&#31181;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#35268;&#21010;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.11085</link><description>&lt;p&gt;
m&amp;m's: &#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#24037;&#20855;&#20351;&#29992;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
m&amp;m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11085
&lt;/p&gt;
&lt;p&gt;
m&amp;m's&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;4K+&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;33&#31181;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#35268;&#21010;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#24456;&#23569;&#30001;&#21333;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#20915;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#27493;&#39588;&#35745;&#31639;&#35745;&#21010;&#65292;&#28041;&#21450;&#25340;&#25509;&#22810;&#20010;&#27169;&#22411;&#12290; &#24037;&#20855;&#22686;&#24378;&#22411;LLM&#26497;&#26377;&#21487;&#33021;&#33258;&#21160;&#21270;&#29983;&#25104;&#36825;&#31181;&#35745;&#31639;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#35268;&#21010;&#22120;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#38459;&#30861;&#20102;&#23545;&#35268;&#21010;&#22120;&#35774;&#35745;&#20915;&#31574;&#30340;&#31995;&#32479;&#30740;&#31350;&#12290;LLM&#26159;&#21542;&#24212;&#19968;&#27425;&#24615;&#29983;&#25104;&#25972;&#20010;&#35745;&#21010;&#36824;&#26159;&#36880;&#27493;&#29983;&#25104;&#65311;&#23427;&#20204;&#26159;&#21542;&#24212;&#35813;&#30452;&#25509;&#20351;&#29992;Python&#20195;&#30721;&#35843;&#29992;&#24037;&#20855;&#65292;&#36824;&#26159;&#36890;&#36807;&#31867;&#20284;JSON&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#26684;&#24335;&#65311;&#21453;&#39304;&#26159;&#21542;&#25913;&#21892;&#35268;&#21010;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20197;&#21450;&#26356;&#22810;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;m&amp;m's&#65306;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#21547;4K+&#20010;&#28041;&#21450;33&#31181;&#24037;&#20855;&#30340;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#27169;&#24577;&#27169;&#22411;&#12289;(&#20813;&#36153;)&#20844;&#20849;API&#21644;&#22270;&#20687;&#22788;&#29702;&#27169;&#22359;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20379;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#29983;&#25104;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11085v1 Announce Type: cross  Abstract: Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions. Should LLMs generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce m&amp;m's: a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realis
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#20026;&#23391;&#21152;&#25289;&#35821;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;BHM&#65292;&#29992;&#20110;&#26816;&#27979;&#20167;&#24680;&#34920;&#24773;&#21253;&#20197;&#21450;&#23427;&#20204;&#25152;&#38024;&#23545;&#30340;&#31038;&#20250;&#23454;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.10829</link><description>&lt;p&gt;
&#36776;&#26512;&#20167;&#24680;&#65306;&#35782;&#21035;&#20167;&#24680;&#34920;&#24773;&#21253;&#21450;&#20854;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Deciphering Hate: Identifying Hateful Memes and Their Targets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10829
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#20026;&#23391;&#21152;&#25289;&#35821;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;BHM&#65292;&#29992;&#20110;&#26816;&#27979;&#20167;&#24680;&#34920;&#24773;&#21253;&#20197;&#21450;&#23427;&#20204;&#25152;&#38024;&#23545;&#30340;&#31038;&#20250;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#34920;&#24773;&#21253;&#24050;&#32463;&#25104;&#20026;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#34920;&#36798;&#24773;&#24863;&#12289;&#24605;&#24819;&#21644;&#35266;&#28857;&#30340;&#24378;&#22823;&#25163;&#27573;&#12290;&#34429;&#28982;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#31181;&#24189;&#40664;&#21644;&#23089;&#20048;&#26469;&#28304;&#65292;&#20294;&#34920;&#24773;&#21253;&#20063;&#21487;&#20197;&#20256;&#25773;&#38024;&#23545;&#20010;&#20154;&#25110;&#31038;&#21306;&#30340;&#20167;&#24680;&#20869;&#23481;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#34920;&#24773;&#21253;&#30340;&#36127;&#38754;&#26041;&#38754;&#65292;&#24573;&#35270;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#23391;&#21152;&#25289;&#35821;&#65289;&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20043;&#21069;&#20851;&#20110;&#23391;&#21152;&#25289;&#35821;&#34920;&#24773;&#21253;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#26816;&#27979;&#20167;&#24680;&#34920;&#24773;&#21253;&#19978;&#65292;&#20294;&#24182;&#27809;&#26377;&#23545;&#23427;&#20204;&#30340;&#30446;&#26631;&#23454;&#20307;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#24182;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38754;&#21521;&#23391;&#21152;&#25289;&#35821;&#30340;&#26032;&#39062;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;BHM&#65288;&#23391;&#21152;&#25289;&#20167;&#24680;&#34920;&#24773;&#21253;&#65289;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;7,148&#20010;&#24102;&#26377;&#23391;&#21152;&#25289;&#35821;&#20197;&#21450;&#28151;&#21512;&#20195;&#30721;&#23383;&#24149;&#30340;&#34920;&#24773;&#21253;&#65292;&#19987;&#20026;&#20004;&#39033;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#65306;&#65288;i&#65289;&#26816;&#27979;&#20167;&#24680;&#34920;&#24773;&#21253;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#26816;&#27979;&#23427;&#20204;&#25152;&#38024;&#23545;&#30340;&#31038;&#20250;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10829v1 Announce Type: new  Abstract: Internet memes have become a powerful means for individuals to express emotions, thoughts, and perspectives on social media. While often considered as a source of humor and entertainment, memes can also disseminate hateful content targeting individuals or communities. Most existing research focuses on the negative aspects of memes in high-resource languages, overlooking the distinctive challenges associated with low-resource languages like Bengali (also known as Bangla). Furthermore, while previous work on Bengali memes has focused on detecting hateful memes, there has been no work on detecting their targeted entities. To bridge this gap and facilitate research in this arena, we introduce a novel multimodal dataset for Bengali, BHM (Bengali Hateful Memes). The dataset consists of 7,148 memes with Bengali as well as code-mixed captions, tailored for two tasks: (i) detecting hateful memes, and (ii) detecting the social entities they target
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;DRAGIN&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#26816;&#32034;&#21644;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10081</link><description>&lt;p&gt;
DRAGIN&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#30340;&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10081
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;DRAGIN&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#26816;&#32034;&#21644;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#20027;&#21160;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#20309;&#26102;&#26816;&#32034;&#12290;&#35813;&#33539;&#24335;&#30340;&#20004;&#20010;&#20851;&#38190;&#20803;&#32032;&#26159;&#30830;&#23450;&#28608;&#27963;&#26816;&#32034;&#27169;&#22359;&#30340;&#26368;&#20339;&#26102;&#26426;&#65288;&#20915;&#23450;&#20309;&#26102;&#26816;&#32034;&#65289;&#20197;&#21450;&#19968;&#26086;&#35302;&#21457;&#26816;&#32034;&#65292;&#21046;&#23450;&#36866;&#24403;&#30340;&#26597;&#35810;&#65288;&#30830;&#23450;&#35201;&#26816;&#32034;&#20160;&#20040;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21160;&#24577;RAG&#26041;&#27861;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#23384;&#22312;&#19981;&#36275;&#12290;&#39318;&#20808;&#65292;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#26816;&#32034;&#30340;&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#38745;&#24577;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#20915;&#23450;&#35201;&#26816;&#32034;&#20160;&#20040;&#30340;&#31574;&#30053;&#36890;&#24120;&#23616;&#38480;&#20110;LLM&#30340;&#26368;&#36817;&#19968;&#21477;&#25110;&#26368;&#21518;&#20960;&#20010;&#26631;&#35760;&#65292;&#32780;LLM&#30340;&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#21487;&#33021;&#36328;&#36234;&#25972;&#20010;&#19978;&#19979;&#25991;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;DRAGIN&#65292; &#21363;&#22522;&#20110;LLMs&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#30340;&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10081v1 Announce Type: new  Abstract: Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specif
&lt;/p&gt;</description></item><item><title>ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09724</link><description>&lt;p&gt;
ClaimVer&#65306;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09724
&lt;/p&gt;
&lt;p&gt;
ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#20256;&#25773;&#30340;&#20449;&#24687;&#35823;&#23548;&#21644;&#31038;&#20132;&#23186;&#20307;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#28608;&#22686;&#20013;&#65292;&#39564;&#35777;&#21644;&#20449;&#20219;&#25152;&#36935;&#21040;&#30340;&#20449;&#24687;&#21464;&#24471;&#26085;&#30410;&#22256;&#38590;&#12290;&#35768;&#22810;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21644;&#24037;&#20855;&#24050;&#34987;&#24320;&#21457;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#25110;&#32454;&#31890;&#24230;&#65292;&#26080;&#27861;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#35775;&#38382;&#19988;&#33021;&#22815;&#25191;&#34892;&#32454;&#31890;&#24230;&#35777;&#25454;&#24402;&#22240;&#30340;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24314;&#31435;&#29992;&#25143;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#20449;&#20219;&#38656;&#35201;&#21576;&#29616;&#27599;&#20010;&#39044;&#27979;&#32972;&#21518;&#30340;&#29702;&#30001;&#65292;&#22240;&#20026;&#30740;&#31350;&#34920;&#26126;&#36825;&#26174;&#33879;&#24433;&#21709;&#20154;&#20204;&#23545;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#23558;&#29992;&#25143;&#20851;&#27880;&#37325;&#28857;&#25918;&#22312;&#20855;&#20307;&#30340;&#38382;&#39064;&#20869;&#23481;&#19978;&#65292;&#32780;&#19981;&#26159;&#25552;&#20379;&#31616;&#21333;&#30340;&#31548;&#32479;&#26631;&#31614;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{ClaimVer&#65292;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;}$&#65292;&#26088;&#22312;&#28385;&#36275;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09724v1 Announce Type: new  Abstract: In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. It is also paramount to localize and bring users' attention to the specific problematic content, instead of providing simple blanket labels. In this paper, we present $\textit{ClaimVer, a human-centric framework}$ tailored to meet users' info
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRIP&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#25972;&#21512;&#29992;&#25143;&#24863;&#30693;&#30340;&#25112;&#30053;&#35268;&#21010;&#27169;&#22359;&#21644;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#38750;&#21512;&#20316;&#23545;&#35805;&#20195;&#29702;&#21830;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#26377;&#25928;&#22320;&#28385;&#36275;&#22810;&#26679;&#21270;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.06769</link><description>&lt;p&gt;
&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#21147;&#37327;&#65281;&#36890;&#36807;&#23450;&#21046;&#31574;&#30053;&#35268;&#21010;&#23454;&#29616;&#26377;&#25928;&#30340;&#38750;&#21512;&#20316;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRIP&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#25972;&#21512;&#29992;&#25143;&#24863;&#30693;&#30340;&#25112;&#30053;&#35268;&#21010;&#27169;&#22359;&#21644;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#38750;&#21512;&#20316;&#23545;&#35805;&#20195;&#29702;&#21830;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#26377;&#25928;&#22320;&#28385;&#36275;&#22810;&#26679;&#21270;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#21512;&#20316;&#23545;&#35805;&#20195;&#29702;&#21830;&#65292;&#20182;&#20204;&#24517;&#39035;&#36827;&#34892;&#23450;&#21046;&#30340;&#25112;&#30053;&#35268;&#21010;&#65292;&#20197;&#30830;&#20445;&#19982;&#22810;&#26679;&#21270;&#29992;&#25143;&#36798;&#25104;&#26377;&#21033;&#30340;&#21327;&#35758;&#12290;&#36825;&#23545;&#29616;&#26377;&#30340;&#23545;&#35805;&#20195;&#29702;&#21830;&#26500;&#25104;&#25361;&#25112;&#65292;&#20027;&#35201;&#21407;&#22240;&#26377;&#20004;&#28857;&#65306;&#20182;&#20204;&#26080;&#27861;&#23558;&#29992;&#25143;&#29305;&#23450;&#29305;&#24449;&#25972;&#21512;&#21040;&#25112;&#30053;&#35268;&#21010;&#20013;&#65292;&#20197;&#21450;&#20182;&#20204;&#30340;&#35757;&#32451;&#33539;&#24335;&#26410;&#33021;&#20135;&#29983;&#21487;&#20197;&#27867;&#21270;&#21040;&#22810;&#26679;&#21270;&#29992;&#25143;&#30340;&#25112;&#30053;&#35268;&#21010;&#32773;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TRIP&#20197;&#22686;&#24378;&#23450;&#21046;&#25112;&#30053;&#35268;&#21010;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#29992;&#25143;&#24863;&#30693;&#30340;&#25112;&#30053;&#35268;&#21010;&#27169;&#22359;&#21644;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;&#33539;&#24335;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#38750;&#21512;&#20316;&#23545;&#35805;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TRIP&#22312;&#36814;&#21512;&#22810;&#26679;&#21270;&#29992;&#25143;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06769v1 Announce Type: new  Abstract: We investigate non-collaborative dialogue agents that must engage in tailored strategic planning for diverse users to secure a favorable agreement. This poses challenges for existing dialogue agents due to two main reasons: their inability to integrate user-specific characteristics into their strategic planning and their training paradigm's failure to produce strategic planners that can generalize to diverse users. To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm. Through experiments on benchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users.
&lt;/p&gt;</description></item><item><title>&#20027;&#39064;&#24314;&#27169;&#20013;&#25552;&#20986;&#20102;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35789;&#27719;&#36873;&#25321;&#26469;&#25913;&#21892;&#38544;&#31169;&#39118;&#38505;</title><link>https://arxiv.org/abs/2403.04451</link><description>&lt;p&gt;
&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#19982;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks and Privacy in Topic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04451
&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#20013;&#25552;&#20986;&#20102;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35789;&#27719;&#36873;&#25321;&#26469;&#25913;&#21892;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25512;&#29702;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#38544;&#31169;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#26356;&#31616;&#21333;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#20027;&#39064;&#27169;&#22411;&#65292;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#30340;&#28431;&#27934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20027;&#39064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#21487;&#20197;&#33258;&#20449;&#22320;&#35782;&#21035;Latent Dirichlet Allocation&#20013;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22823;&#22411;&#31070;&#32463;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;&#38544;&#31169;&#39118;&#38505;&#24182;&#19981;&#20165;&#38480;&#20110;&#22823;&#22411;&#31070;&#32463;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#28431;&#27934;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20027;&#39064;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31169;&#23494;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#23558;DP&#35789;&#27719;&#36873;&#25321;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#23637;&#31034;&#23427;&#19981;&#20165;&#25913;&#21892;&#20102;&#38544;&#31169;&#24615;&#65292;&#32780;&#19988;&#22312;&#23454;&#29992;&#24615;&#26041;&#38754;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04451v1 Announce Type: cross  Abstract: Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SimuCourt&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#21496;&#27861;&#25991;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#21496;&#27861;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.02959</link><description>&lt;p&gt;
SimuCourt: &#21033;&#29992;&#30495;&#23454;&#21496;&#27861;&#21028;&#20915;&#25991;&#20214;&#26500;&#24314;&#21496;&#27861;&#20915;&#31574;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02959
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SimuCourt&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#21496;&#27861;&#25991;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#21496;&#27861;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20256;&#32479;&#21496;&#27861;&#34892;&#19994;&#21508;&#20010;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#20010;&#21035;&#21496;&#27861;&#38454;&#27573;&#65292;&#24573;&#35270;&#20102;&#36328;&#38454;&#27573;&#30340;&#21327;&#20316;&#12290;&#38543;&#30528;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#20195;&#29702;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26234;&#33021;&#65292;&#24182;&#33021;&#20570;&#20986;&#22797;&#26434;&#20915;&#31574;&#65292;&#20026;&#21496;&#27861;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SimuCourt&#65292;&#19968;&#20010;&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#30340;420&#20221;&#21028;&#20915;&#25991;&#20214;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#26368;&#24120;&#35265;&#31867;&#22411;&#30340;&#21496;&#27861;&#26696;&#20363;&#65292;&#20197;&#21450;&#19968;&#20010;&#26032;&#39062;&#20219;&#21153;&#21496;&#27861;&#20915;&#31574;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#21496;&#27861;&#30693;&#35782;&#24211;&#65292;JudicialKB&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#31181;&#27861;&#24459;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;AgentsCourt
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02959v1 Announce Type: cross  Abstract: With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#20851;&#31995;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#28085;&#30422;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#32452;&#21512;&#20851;&#31995;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.02615</link><description>&lt;p&gt;
&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#20851;&#31995;&#25512;&#29702;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limitations of Large Language Models in Compositional Relation Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02615
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#20851;&#31995;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#28085;&#30422;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#32452;&#21512;&#20851;&#31995;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#36890;&#36807;&#19968;&#20010;&#21253;&#21547;1,500&#20010;&#33521;&#25991;&#27979;&#35797;&#26696;&#20363;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#29702;&#32452;&#21512;&#20851;&#31995;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#26088;&#22312;&#28085;&#30422;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#32452;&#21512;&#20851;&#31995;&#65306;&#20301;&#32622;&#12289;&#27604;&#36739;&#12289;&#20010;&#20154;&#12289;&#25968;&#23398;&#12289;&#36523;&#20221;&#21644;&#20854;&#20182;&#12290;&#24847;&#35782;&#21040;&#22810;&#35821;&#35328;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#25193;&#23637;&#21040;&#23558;&#36825;&#20123;&#26696;&#20363;&#32763;&#35793;&#25104;&#20013;&#25991;&#12289;&#26085;&#25991;&#12289;&#27861;&#25991;&#21644;&#38889;&#25991;&#12290;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#32452;&#21512;&#20851;&#31995;(MCR)&#22522;&#20934;&#26088;&#22312;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#19979;&#30340;&#32452;&#21512;&#20851;&#31995;&#25512;&#29702;&#30340;&#31283;&#20581;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02615v1 Announce Type: new  Abstract: We present a comprehensive evaluation of large language models(LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations: Positional, Comparative, Personal, Mathematical, Identity, and Other. Acknowledging the significance of multilingual capabilities, we expanded our assessment to include translations of these cases into Chinese, Japanese, French, and Korean. Our Multilingual Composition Relation (MCR) benchmark aims at investigating the robustness and adaptability of LLMs in handling composition relation reasoning across diverse linguistic contexts.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Layer-wise Scalable Adapter&#31574;&#30053;MedLaSA&#65292;&#29992;&#20110;&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#31934;&#30830;&#20462;&#25913;&#21307;&#23398;&#30693;&#35782;&#24182;&#35299;&#37322;&#20107;&#23454;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.18099</link><description>&lt;p&gt;
&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#30693;&#35782;&#21644;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18099
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Layer-wise Scalable Adapter&#31574;&#30053;MedLaSA&#65292;&#29992;&#20110;&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#31934;&#30830;&#20462;&#25913;&#21307;&#23398;&#30693;&#35782;&#24182;&#35299;&#37322;&#20107;&#23454;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#29305;&#23450;&#30693;&#35782;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#19981;&#21464;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;LLMs&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#38382;&#39064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;LLMs&#22312;&#35768;&#22810;&#20851;&#38190;&#39046;&#22495;&#65288;&#20363;&#22914;&#21307;&#23398;&#39046;&#22495;&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#24187;&#35273;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20004;&#39033;&#27169;&#22411;&#32534;&#36753;&#30740;&#31350;&#65292;&#24182;&#22312;&#21307;&#23398;&#39046;&#22495;&#39564;&#35777;&#23427;&#20204;&#65306;&#65288;1&#65289;&#30452;&#25509;&#32534;&#36753;&#21307;&#23398;&#20107;&#23454;&#30693;&#35782;&#21644;&#65288;2&#65289;&#32534;&#36753;&#23545;&#20107;&#23454;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#30340;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MedLaSA&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#36866;&#29992;&#20110;&#21307;&#23398;&#27169;&#22411;&#32534;&#36753;&#30340;&#20998;&#23618;&#21487;&#25193;&#23637;&#36866;&#37197;&#22120;&#31574;&#30053;&#12290;&#23427;&#37319;&#29992;&#22240;&#26524;&#36861;&#36394;&#26469;&#35782;&#21035;&#31070;&#32463;&#20803;&#20013;&#30693;&#35782;&#30340;&#31934;&#30830;&#20301;&#32622;&#65292;&#28982;&#21518;&#23558;&#21487;&#25193;&#23637;&#36866;&#37197;&#22120;&#24341;&#20837;&#21040;LLMs&#30340;&#23494;&#38598;&#23618;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18099v1 Announce Type: cross  Abstract: Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LL
&lt;/p&gt;</description></item><item><title>DeMPT&#25552;&#20986;&#20102;&#35299;&#30721;&#22686;&#24378;&#30340;&#22810;&#38454;&#27573;&#25552;&#31034;&#20248;&#21270;&#65292;&#20351;&#24471;LLMs&#26356;&#22909;&#22320;&#27169;&#25311;&#21644;&#21033;&#29992;&#21477;&#38388;&#21644;&#21477;&#20869;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#36866;&#24212;&#19978;&#19979;&#25991;&#24863;&#30693;NMT&#12290;</title><link>https://arxiv.org/abs/2402.15200</link><description>&lt;p&gt;
DeMPT&#65306;&#35299;&#30721;&#22686;&#24378;&#30340;&#22810;&#38454;&#27573;&#25552;&#31034;&#20248;&#21270;&#65292;&#20351;LLMs&#25104;&#20026;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32763;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15200
&lt;/p&gt;
&lt;p&gt;
DeMPT&#25552;&#20986;&#20102;&#35299;&#30721;&#22686;&#24378;&#30340;&#22810;&#38454;&#27573;&#25552;&#31034;&#20248;&#21270;&#65292;&#20351;&#24471;LLMs&#26356;&#22909;&#22320;&#27169;&#25311;&#21644;&#21033;&#29992;&#21477;&#38388;&#21644;&#21477;&#20869;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#36866;&#24212;&#19978;&#19979;&#25991;&#24863;&#30693;NMT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#20165;&#20855;&#26377;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36830;&#25509;&#30340;&#26041;&#24335;&#36866;&#24212;&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#65292;&#20854;&#20013;LLMs&#23558;&#28304;&#21477;&#65288;&#21363;&#21477;&#20869;&#19978;&#19979;&#25991;&#65289;&#21644;&#21477;&#38388;&#19978;&#19979;&#25991;&#30340;&#36830;&#25509;&#20316;&#20026;&#36755;&#20837;&#65292;&#28982;&#21518;&#39034;&#24207;&#29983;&#25104;&#30446;&#26631;&#26631;&#35760;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Decoding-enhanced Multi-phase Prompt Tuning&#65288;DeMPT&#65289;&#30340;&#26367;&#20195;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#20351;LLMs&#33021;&#22815;&#27495;&#35270;&#24615;&#22320;&#23545;&#27169;&#32452;&#21644;&#21033;&#29992;&#21477;&#38388;&#21644;&#21477;&#20869;&#19978;&#19979;&#25991;&#65292;&#24182;&#26356;&#26377;&#25928;&#22320;&#23558;LLMs&#35843;&#25972;&#21040;&#19978;&#19979;&#25991;&#24863;&#30693;NMT&#12290;&#39318;&#20808;&#65292;DeMPT&#23558;&#19978;&#19979;&#25991;&#24863;&#30693;NMT&#36807;&#31243;&#20998;&#20026;&#19977;&#20010;&#21333;&#29420;&#38454;&#27573;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#24341;&#20837;&#19981;&#21516;&#30340;&#36830;&#32493;&#25552;&#31034;&#65292;&#20351;LLMs&#33021;&#22815;&#21306;&#20998;&#22320;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15200v1 Announce Type: new  Abstract: Generally, the decoder-only large language models (LLMs) are adapted to context-aware neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially. This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts. In this paper, we propose an alternative adaptation approach, named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and utilize the inter- and intra-sentence context and more effectively adapt LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process into three separate phases. During each phase, different continuous prompts are introduced to make LLMs discriminatel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#23545;&#22810;&#35821;&#27169;&#22411;&#22312;&#19981;&#21516;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#24182;&#34892;&#25945;&#23398;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25945;&#23398;&#35843;&#25972;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#36328;&#35821;&#35328;&#36981;&#24490;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#34920;&#38754;&#23545;&#40784;&#20551;&#35774;&#30340;&#36136;&#30097;</title><link>https://arxiv.org/abs/2402.13703</link><description>&lt;p&gt;
&#35843;&#26597;&#22810;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#65306;&#22810;&#35821;&#27169;&#22411;&#26159;&#21542;&#38656;&#35201;&#22810;&#35821;&#25945;&#23398;&#65311;
&lt;/p&gt;
&lt;p&gt;
Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#23545;&#22810;&#35821;&#27169;&#22411;&#22312;&#19981;&#21516;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#24182;&#34892;&#25945;&#23398;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25945;&#23398;&#35843;&#25972;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#36328;&#35821;&#35328;&#36981;&#24490;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#34920;&#38754;&#23545;&#40784;&#20551;&#35774;&#30340;&#36136;&#30097;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13703v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#23558;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36716;&#21270;&#20026;&#38596;&#36777;&#32780;&#26377;&#29992;&#30340;&#21161;&#25163;&#23545;&#20419;&#36827;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#22320;&#21306;&#30340;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#31934;&#31070;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23545;&#36328;&#22810;&#31181;&#21360;&#27431;&#35821;&#35328;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#30340;&#30740;&#31350;&#32773;&#65292;&#26088;&#22312;&#30740;&#31350;&#22810;&#35821;&#27169;&#22411;&#22312;&#36873;&#25321;&#30340;&#26368;&#24120;&#29992;&#30340;&#21360;&#27431;&#35821;&#35328;&#19978;&#30340;&#24182;&#34892;&#12289;&#22810;&#36718;&#25945;&#23398;&#35843;&#25972;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#35821;&#35328;&#21644;&#25945;&#23398;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#20013;&#22411;&#22810;&#35821;&#35328;LLM&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#24182;&#34892;&#25945;&#23398;&#35843;&#25972;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25945;&#23398;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24182;&#34892;&#25945;&#23398;&#35843;&#25972;&#32780;&#19981;&#26159;&#21333;&#35821;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#25945;&#23398;&#35843;&#25972;&#21487;&#20197;&#20351;&#36328;&#35821;&#35328;&#36981;&#24490;&#33021;&#21147;&#25552;&#39640;&#22810;&#36798;4.6%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#34920;&#38754;&#23545;&#40784;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#25152;&#35843;&#26597;&#30340;&#22810;&#35821;7B&#21442;&#25968;&#27169;&#22411;&#26159;&#19968;&#20010;&#21453;&#20363;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#25945;&#23398;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13703v1 Announce Type: new  Abstract: The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#28145;&#24230;RL&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12275</link><description>&lt;p&gt;
WorldCoder&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#65306;&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12275
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#28145;&#24230;RL&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26500;&#24314;&#20195;&#34920;&#20854;&#23545;&#19990;&#30028;&#30693;&#35782;&#30340;Python&#31243;&#24207;&#12290;&#35813;&#19990;&#30028;&#27169;&#22411;&#35797;&#22270;&#35299;&#37322;&#20854;&#20132;&#20114;&#65292;&#21516;&#26102;&#23545;&#33258;&#24049;&#33021;&#22815;&#33719;&#24471;&#30340;&#22870;&#21169;&#25345;&#20048;&#35266;&#24577;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;LLM&#30340;&#31243;&#24207;&#21512;&#25104;&#24037;&#20316;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#32593;&#26684;&#19990;&#30028;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#27604;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26356;&#39640;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#27604;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12275v1 Announce Type: new  Abstract: We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25366;&#25496;&#20102;&#22312;&#32447;&#35805;&#35821;&#20013;&#30340;&#28436;&#32462;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;&#65292;&#24182;&#22312;Twitter&#21644;YouTube&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09934</link><description>&lt;p&gt;
&#20851;&#27880;&#20559;&#24046;&#65306;&#25366;&#25496;&#22312;&#32447;&#35805;&#35821;&#20013;&#30340;&#28436;&#32462;&#32454;&#24494;&#24046;&#21035;&#65292;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;
&lt;/p&gt;
&lt;p&gt;
Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25366;&#25496;&#20102;&#22312;&#32447;&#35805;&#35821;&#20013;&#30340;&#28436;&#32462;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;&#65292;&#24182;&#22312;Twitter&#21644;YouTube&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#38382;&#20027;&#20041;&#22312;&#25200;&#20081;&#21465;&#20107;&#21644;&#25773;&#31181;&#19981;&#20449;&#20219;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#24037;&#20855;&#25928;&#29992;&#65292;&#20294;&#22312;&#23450;&#37327;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#21364;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#26410;&#33021;&#21306;&#20998;&#21453;&#38382;&#20027;&#20041;&#20316;&#20026;&#35823;&#23548;&#21644;&#23459;&#20256;&#31574;&#30053;&#30340;&#29992;&#36884;&#19982;&#20854;&#20316;&#20026;&#35821;&#29992;&#21644;&#35821;&#20041;&#26694;&#26550;&#24037;&#20855;&#30340;&#29992;&#36884;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26032;&#30340;&#26469;&#33258;Twitter&#21644;YouTube&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21453;&#38382;&#20027;&#20041;&#12289;&#23459;&#20256;&#21644;tu quoque&#35884;&#35823;&#20043;&#38388;&#30340;&#37325;&#21472;&#21644;&#21306;&#21035;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#26368;&#36817;&#22312;&#35821;&#35328;&#35821;&#20041;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#8220;what about&#8221;&#35789;&#27719;&#32467;&#26500;&#19982;&#21453;&#38382;&#20027;&#20041;&#21306;&#20998;&#24320;&#26469;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20934;&#30830;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20419;&#20351;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#26435;&#37325;&#36827;&#34892;&#36127;&#26679;&#26412;&#25366;&#25496;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;Twitter&#21644;YouTube&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;4%&#21644;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09934v1 Announce Type: cross  Abstract: Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research. Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing. We introduce new datasets from Twitter and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on recent work in linguistic semantics, we differentiate the `what about' lexical construct from whataboutism. Our experiments bring to light unique challenges in its accurate detection, prompting the introduction of a novel method using attention weights for negative sample mining. We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively.
&lt;/p&gt;</description></item><item><title>EFUF&#26159;&#19968;&#31181;&#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#28040;&#38500;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#65292;&#24182;&#19981;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#37197;&#23545;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.09801</link><description>&lt;p&gt;
EFUF: &#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20943;&#36731;&#24187;&#20687;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09801
&lt;/p&gt;
&lt;p&gt;
EFUF&#26159;&#19968;&#31181;&#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#28040;&#38500;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#65292;&#24182;&#19981;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#37197;&#23545;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20173;&#20250;&#29983;&#25104;&#21253;&#21547;&#22270;&#20687;&#20013;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#30340;&#25551;&#36848;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#29289;&#20307;&#24187;&#35273;&#12290;&#20026;&#20102;&#28040;&#38500;&#24187;&#35273;&#65292;&#29616;&#26377;&#26041;&#27861;&#25163;&#21160;&#27880;&#37322;&#21253;&#21547;&#21644;&#19981;&#21253;&#21547;&#24187;&#35273;&#30340;&#37197;&#23545;&#21709;&#24212;&#65292;&#24182;&#37319;&#29992;&#21508;&#31181;&#23545;&#40784;&#31639;&#27861;&#26469;&#25552;&#39640;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#20165;&#22312;&#24494;&#35843;&#38454;&#27573;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36824;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#26469;&#26500;&#24314;&#23545;&#40784;&#31639;&#27861;&#25152;&#38656;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#21435;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#26694;&#26550;&#65288;EFUF&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#24187;&#35273;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#20943;&#23569;&#24187;&#35273;&#21516;&#26102;&#20445;&#30041;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09801v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserv
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#24615;&#35843;&#26597;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21382;&#21490;&#12289;&#21457;&#23637;&#21644;&#21407;&#29702;&#65292;&#26088;&#22312;&#24110;&#21161;&#24191;&#27867;&#30340;&#35835;&#32773;&#32676;&#20307;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#32972;&#26223;&#21644;&#21407;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.06853</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21382;&#21490;&#12289;&#21457;&#23637;&#21644;&#21407;&#29702;-&#19968;&#39033;&#32508;&#36848;&#24615;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
History, Development, and Principles of Large Language Models-An Introductory Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06853
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#24615;&#35843;&#26597;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21382;&#21490;&#12289;&#21457;&#23637;&#21644;&#21407;&#29702;&#65292;&#26088;&#22312;&#24110;&#21161;&#24191;&#27867;&#30340;&#35835;&#32773;&#32676;&#20307;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#32972;&#26223;&#21644;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#22522;&#30707;&#65292;&#21033;&#29992;&#25968;&#23398;&#26041;&#27861;&#26469;&#25512;&#24191;&#35821;&#35328;&#35268;&#24459;&#21644;&#30693;&#35782;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;&#32463;&#36807;&#20960;&#21313;&#24180;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#35821;&#35328;&#24314;&#27169;&#20174;&#26368;&#21021;&#30340;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#21457;&#23637;&#21040;&#24403;&#20170;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LLMs&#30340;&#24555;&#36895;&#28436;&#36827;&#24050;&#32463;&#36798;&#21040;&#20102;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#27700;&#24179;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;LLMs&#22312;&#25913;&#21892;&#24037;&#20316;&#21644;&#20010;&#20154;&#29983;&#27963;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#19968;&#33324;&#20174;&#19994;&#20154;&#21592;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#32972;&#26223;&#21644;&#21407;&#29702;&#20102;&#35299;&#26377;&#38480;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;LLMs&#30340;&#32508;&#36848;&#37117;&#38598;&#20013;&#22312;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#20351;&#29992;&#20102;&#19987;&#38376;&#30340;&#35821;&#35328;&#65292;&#32473;&#32570;&#20047;&#30456;&#20851;&#32972;&#26223;&#30693;&#35782;&#30340;&#20174;&#19994;&#20154;&#21592;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#31616;&#26126;&#25212;&#35201;&#30340;LLMs&#27010;&#36848;&#65292;&#20197;&#24110;&#21161;&#26356;&#24191;&#27867;&#30340;&#35835;&#32773;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models serve as a cornerstone in natural language processing (NLP), utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models (SLMs) to the contemporary landscape of large language models (LLMs). Notably, the swift evolution of LLMs has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that LLMs offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most LLMs reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of LLMs to assist a broader audience. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;GPT-4&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#23384;&#22312;&#22522;&#26412;&#38169;&#35823;&#65292;&#24182;&#19988;&#21363;&#20351;&#26159;GPT-4&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2401.17169</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Conditional and Modal Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;GPT-4&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#23384;&#22312;&#22522;&#26412;&#38169;&#35823;&#65292;&#24182;&#19988;&#21363;&#20351;&#26159;GPT-4&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#30740;&#31350;&#27491;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#35748;&#30693;&#31185;&#23398;&#39046;&#22495;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21313;&#20960;&#20010;LLM&#33021;&#21542;&#21306;&#20998;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#25512;&#35770;&#21644;&#36923;&#36753;&#19978;&#33618;&#35884;&#30340;&#25512;&#35770;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#28041;&#21450;&#26465;&#20214;&#21477;&#65288;&#20363;&#22914;&#65292;&#8220;&#22914;&#26524;&#23433;&#26377;&#19968;&#20010;&#30343;&#21518;&#65292;&#37027;&#20040;&#40077;&#21187;&#26377;&#19968;&#20010;J&#29260;&#8221;&#65289;&#21644;&#35748;&#35782;&#24773;&#24577;&#65288;&#20363;&#22914;&#65292;&#8220;&#23433;&#21487;&#33021;&#26377;&#19968;&#20010;A&#29260;&#8221;&#65292;&#8220;&#40077;&#21187;&#24517;&#39035;&#26377;&#19968;&#20010;K&#29260;&#8221;&#65289;&#30340;&#25512;&#29702;&#27169;&#24335;&#12290;&#36825;&#20123;&#25512;&#29702;&#27169;&#24335;&#23545;&#20110;&#36923;&#36753;&#23398;&#23478;&#12289;&#21746;&#23398;&#23478;&#21644;&#35821;&#35328;&#23398;&#23478;&#26469;&#35828;&#20855;&#26377;&#29305;&#27530;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#22312;&#20154;&#31867;&#25512;&#29702;&#20013;&#25198;&#28436;&#19968;&#20010;&#26680;&#24515;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;LLM&#22312;&#36825;&#20123;&#25512;&#29702;&#27169;&#24335;&#19978;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#30456;&#21305;&#37197;&#26159;&#38750;&#24120;&#30456;&#20851;&#30340;&#12290;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;LLM&#20013;&#65292;&#38500;&#20102;GPT-4&#65292;&#20854;&#20182;&#37117;&#24120;&#24120;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#29359;&#22522;&#26412;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;GPT-4&#65292;&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science. In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning. Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.
&lt;/p&gt;</description></item><item><title>DALA&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24863;&#30693;&#30340;LoRA&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#65292;&#24182;&#24341;&#20837;&#20102;&#38750;&#21487;&#26816;&#27979;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;NASR&#65289;&#35780;&#20215;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2311.08598</link><description>&lt;p&gt;
DALA: &#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24863;&#30693;&#30340;&#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08598
&lt;/p&gt;
&lt;p&gt;
DALA&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24863;&#30693;&#30340;LoRA&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#65292;&#24182;&#24341;&#20837;&#20102;&#38750;&#21487;&#26816;&#27979;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;NASR&#65289;&#35780;&#20215;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#25805;&#32437;&#65292;&#36825;&#20123;&#25915;&#20987;&#22312;&#36755;&#20837;&#25968;&#25454;&#20013;&#24341;&#20837;&#24494;&#22937;&#30340;&#25200;&#21160;&#12290;&#36817;&#26399;&#30340;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#30456;&#23545;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19982;&#21407;&#22987;&#26679;&#26412;&#30456;&#27604;&#20855;&#26377;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#23545;&#25239;&#24615;&#26679;&#26412;&#34920;&#29616;&#20986;&#38477;&#20302;&#30340;&#32622;&#20449;&#27700;&#24179;&#21644;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#36739;&#22823;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#31616;&#21333;&#30340;&#26816;&#27979;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#65292;&#38477;&#20302;&#20102;&#27492;&#31867;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LoRA&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65288;DALA&#65289;&#12290;DALA&#32771;&#34385;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#20197;&#25552;&#39640;&#22312;&#26816;&#27979;&#26041;&#27861;&#19979;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20215;&#24230;&#37327;&#65292;&#38750;&#21487;&#26816;&#27979;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;NASR&#65289;&#65292;&#23427;&#34701;&#21512;&#20102;ASR&#21644;&#21487;&#26816;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08598v2 Announce Type: replace  Abstract: Language models (LMs) can be manipulated by adversarial attacks, which introduce subtle perturbations to input data. While recent attack methods can achieve a relatively high attack success rate (ASR), we've observed that the generated adversarial examples have a different data distribution compared with the original examples. Specifically, these adversarial examples exhibit reduced confidence levels and greater divergence from the training data distribution. Consequently, they are easy to detect using straightforward detection methods, diminishing the efficacy of such attacks. To address this issue, we propose a Distribution-Aware LoRA-based Adversarial Attack (DALA) method. DALA considers distribution shifts of adversarial examples to improve the attack's effectiveness under detection methods. We further design a novel evaluation metric, the Non-detectable Attack Success Rate (NASR), which integrates both ASR and detectability for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26497;&#31471;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36719;&#26631;&#31614;&#21407;&#22411;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#22312;&#22823;&#22411;&#12289;&#39640;&#32500;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2210.17437</link><description>&lt;p&gt;
&#29992;&#36719;&#26631;&#31614;&#21407;&#22411;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Learning New Tasks from a Few Examples with Soft-Label Prototypes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.17437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26497;&#31471;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36719;&#26631;&#31614;&#21407;&#22411;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#22312;&#22823;&#22411;&#12289;&#39640;&#32500;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#23545;&#20854;&#24494;&#35843;&#65292;&#20197;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#8220;&#26497;&#31471;&#8221;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#27169;&#22411;&#21482;&#38656;&#25509;&#35302;&#27599;&#20010;&#31867;&#21035;&#33267;&#23569;4&#20010;&#31034;&#20363;&#65292;&#36825;&#20123;&#31034;&#20363;&#22522;&#20110;&#36719;&#26631;&#31614;&#21407;&#22411;&#65292;&#36825;&#20123;&#36719;&#26631;&#31614;&#21407;&#22411;&#20849;&#21516;&#25429;&#33719;&#20102;&#36755;&#20837;&#22495;&#31354;&#38388;&#20013;&#19981;&#21516;&#31867;&#21035;&#30340;&#20998;&#24067;&#12290;&#21463;&#21040;&#20808;&#21069;&#20851;&#20110;&#19968;&#20803;&#25110;&#31616;&#21333;&#22810;&#20803;&#65288;&#21512;&#25104;&#65289;&#25968;&#25454;&#65288;Sucholutsky&#31561;&#20154;&#65292;2021&#65289;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#12289;&#39640;&#32500;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#31070;&#32463;&#26694;&#26550;&#65288;DeepSLP&#65289;&#20013;&#23398;&#20064;&#36719;&#26631;&#31614;&#21407;&#22411;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#65292;&#23427;&#22312;31/48&#20010;&#27979;&#35797;&#20219;&#21153;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#19982;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20174;v&#20013;&#23398;&#20064;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;NLP&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.17437v3 Announce Type: replace-cross  Abstract: Existing approaches to few-shot learning in NLP rely on large language models and fine-tuning of these to generalise on out-of-distribution data. In this work, we propose a simple yet powerful approach to "extreme" few-shot learning, wherein models are exposed to as little as 4 examples per class, based on soft-label prototypes that collectively capture the distribution of different classes across the input domain space. Inspired by previous work (Sucholutsky et al., 2021) on univariate or simple multivariate (synthetic) data, we propose a novel approach that is effective on large, high-dimensional and real-world datasets. We learn soft-label prototypes within a neural framework (DeepSLP) and we experimentally demonstrate that it achieves superior performance on 31/48 tested tasks and few-shot settings while closely matching the performance of strong baselines on the rest. We focus on learning previously unseen NLP tasks from v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#22806;&#37096;&#30693;&#35782;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KCA&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10768</link><description>&lt;p&gt;
&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65306;&#36890;&#36807;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment. (arXiv:2401.10768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#22806;&#37096;&#30693;&#35782;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KCA&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#40784;&#21518;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20173;&#21487;&#33021;&#20135;&#29983;&#19982;&#19978;&#19979;&#25991;&#25110;&#19990;&#30028;&#30693;&#35782;&#33258;&#20449;&#30683;&#30462;&#30340;&#21709;&#24212;&#65292;&#36825;&#34987;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#29616;&#35937;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22806;&#37096;&#30693;&#35782;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32487;&#25215;&#30340;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#23545;&#40784;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26681;&#25454;&#22806;&#37096;&#30693;&#35782;&#33258;&#21160;&#21046;&#23450;&#32771;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23545;&#20110;&#21253;&#21547;&#30693;&#35782;&#19981;&#19968;&#33268;&#24615;&#30340;&#25968;&#25454;&#65292;KCA&#23454;&#26045;&#20102;&#20960;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22788;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#21644;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20845;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;KCA&#26041;&#27861;&#22312;&#32531;&#35299;&#24187;&#35273;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have proven to be exceptional on a variety of tasks after alignment, they may still produce responses that contradict the context or world knowledge confidently, a phenomenon known as ``hallucination''. In this paper, we demonstrate that reducing the inconsistency between the external knowledge encapsulated in the training data and the intrinsic knowledge inherited in the pretraining corpus could mitigate hallucination in alignment. Specifically, we introduce a novel knowledge consistent alignment (KCA) approach, which involves automatically formulating examinations based on external knowledge for accessing the comprehension of LLMs. For data encompassing knowledge inconsistency, KCA implements several simple yet efficient strategies for processing. We illustrate the superior performance of the proposed KCA approach in mitigating hallucinations across six benchmarks using LLMs of different backbones and scales. Furthermore, we confirm the correlation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24322;&#27493;Local-SGD&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#24322;&#27493;&#26356;&#26032;&#26356;&#39057;&#32321;&#65292;&#20294;&#20854;&#25910;&#25947;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#22810;&#20110;&#21516;&#27493;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24310;&#36831;&#30340;Nesterov&#21160;&#37327;&#26356;&#26032;&#36827;&#34892;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#24322;&#27493;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.09135</link><description>&lt;p&gt;
&#24322;&#27493;Local-SGD&#35757;&#32451;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Local-SGD Training for Language Modeling. (arXiv:2401.09135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24322;&#27493;Local-SGD&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#24322;&#27493;&#26356;&#26032;&#26356;&#39057;&#32321;&#65292;&#20294;&#20854;&#25910;&#25947;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#22810;&#20110;&#21516;&#27493;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24310;&#36831;&#30340;Nesterov&#21160;&#37327;&#26356;&#26032;&#36827;&#34892;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#24322;&#27493;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Local&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(Local-SGD)&#65292;&#20063;&#31216;&#20026;&#32852;&#37030;&#24179;&#22343;&#65292;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#35774;&#22791;&#22312;&#36890;&#20449;&#20013;&#25191;&#34892;&#22810;&#20010;SGD&#26356;&#26032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24322;&#27493;Local-SGD&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#39564;&#35777;&#30740;&#31350;&#65307;&#21363;&#65292;&#27599;&#20010;&#24037;&#20316;&#33410;&#28857;&#22312;&#23436;&#25104;&#20854;SGD&#27493;&#39588;&#21518;&#31435;&#21363;&#26356;&#26032;&#20840;&#23616;&#21442;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#23519;&#24037;&#20316;&#33410;&#28857;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#21644;&#20248;&#21270;&#22120;&#31561;&#22240;&#32032;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#26356;&#39057;&#32321;&#22320;&#26356;&#26032;&#65288;&#20840;&#23616;&#65289;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#24322;&#27493;Local-SGD&#27604;&#20854;&#21516;&#27493;&#23545;&#24212;&#29289;&#38656;&#35201;&#26356;&#22810;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#24037;&#20316;&#33410;&#28857;&#26799;&#24230;&#38472;&#26087;&#26102;&#20840;&#23616;&#21442;&#25968;&#30340;&#21160;&#37327;&#21152;&#36895;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24310;&#36831;&#30340;Nesterov&#21160;&#37327;&#26356;&#26032;&#65292;&#26681;&#25454;&#24037;&#20316;&#33410;&#28857;&#30340;&#26412;&#22320;&#35757;&#32451;&#27493;&#39588;&#36827;&#34892;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of {\it asynchronous} Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based
&lt;/p&gt;</description></item><item><title>RoTBench&#26159;&#19968;&#20010;&#22810;&#32423;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22122;&#22768;&#19979;&#34920;&#29616;&#20986;&#30340;&#31283;&#23450;&#24615;&#38656;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.08326</link><description>&lt;p&gt;
RoTBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#22810;&#32423;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning. (arXiv:2401.08326v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08326
&lt;/p&gt;
&lt;p&gt;
RoTBench&#26159;&#19968;&#20010;&#22810;&#32423;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22122;&#22768;&#19979;&#34920;&#29616;&#20986;&#30340;&#31283;&#23450;&#24615;&#38656;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#23398;&#20064;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#20114;&#21160;&#30340;&#37325;&#35201;&#25163;&#27573;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;LLMs&#22312;&#32467;&#26500;&#33391;&#22909;&#30340;&#29615;&#22659;&#20013;&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#20294;&#24573;&#35270;&#20102;&#23427;&#20204;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#22122;&#22768;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RoTBench&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#40065;&#26834;&#24615;&#30340;&#22810;&#32423;&#22522;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20116;&#20010;&#22806;&#37096;&#29615;&#22659;&#65292;&#27599;&#20010;&#29615;&#22659;&#37117;&#20855;&#26377;&#19981;&#21516;&#32423;&#21035;&#30340;&#22122;&#22768;&#65288;&#21363;&#28165;&#27905;&#12289;&#36731;&#24494;&#12289;&#20013;&#31561;&#12289;&#37325;&#24230;&#21644;&#32852;&#21512;&#65289;&#65292;&#23545;&#27169;&#22411;&#22312;&#24037;&#20855;&#36873;&#25321;&#12289;&#21442;&#25968;&#35782;&#21035;&#21644;&#20869;&#23481;&#22635;&#20805;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#20845;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25552;&#39640;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#36843;&#22312;&#30473;&#30571;&#12290;&#20363;&#22914;&#65292;&#24403;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#22122;&#22768;&#23384;&#22312;&#26102;&#65292;GPT-4&#30340;&#24615;&#33021;&#29978;&#33267;&#20174;80.00&#19979;&#38477;&#21040;58.10&#12290;
&lt;/p&gt;
&lt;p&gt;
Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substanti
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#20110;&#22914;&#20309;&#24341;&#23548;&#21644;&#32467;&#26500;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#20110;&#36755;&#20837;&#38382;&#39064;&#26412;&#36523;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#37325;&#26032;&#38405;&#35835;&#8221;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#28145;&#20837;&#38405;&#35835;&#36755;&#20837;&#25552;&#31034;&#20013;&#30340;&#38382;&#39064;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#12289;&#26356;&#20934;&#30830;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#26356;&#26377;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06275</link><description>&lt;p&gt;
&#37325;&#26032;&#38405;&#35835;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Re-Reading Improves Reasoning in Language Models. (arXiv:2309.06275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06275
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#20110;&#22914;&#20309;&#24341;&#23548;&#21644;&#32467;&#26500;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#20110;&#36755;&#20837;&#38382;&#39064;&#26412;&#36523;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#37325;&#26032;&#38405;&#35835;&#8221;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#28145;&#20837;&#38405;&#35835;&#36755;&#20837;&#25552;&#31034;&#20013;&#30340;&#38382;&#39064;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#12289;&#26356;&#20934;&#30830;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#26356;&#26377;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#20197;&#24341;&#23548;&#21644;&#32467;&#26500;&#21270;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#20165;&#35299;&#30721;&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#25805;&#20316;&#36755;&#20837;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#24573;&#30053;&#20154;&#31867;&#25512;&#29702;&#20013;&#20016;&#23500;&#30340;&#21069;&#21518;&#20132;&#20114;&#12290;&#23545;&#20110;&#23884;&#20837;&#22312;&#25552;&#31034;&#20013;&#30340;&#36755;&#20837;&#38382;&#39064;&#36825;&#19968;&#20851;&#38190;&#32500;&#24230;&#65292;&#30446;&#21069;&#20851;&#27880;&#36739;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#31216;&#20026;&#8220;&#37325;&#26032;&#38405;&#35835;&#8221;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#21644;&#38382;&#39064;&#35299;&#20915;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#37325;&#26032;&#38405;&#35835;&#24847;&#21619;&#30528;&#37325;&#35775;&#23884;&#22312;&#36755;&#20837;&#25552;&#31034;&#20013;&#30340;&#38382;&#39064;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#35748;&#30693;&#22686;&#24378;&#30340;&#21407;&#21017;&#23436;&#32654;&#22865;&#21512;&#65292;&#20351;LLM&#33021;&#22815;&#28145;&#20837;&#27934;&#23519;&#12289;&#35782;&#21035;&#22797;&#26434;&#30340;&#27169;&#24335;&#12289;&#24314;&#31435; mor
&lt;/p&gt;
&lt;p&gt;
Reasoning presents a significant and challenging issue for Large Language Models (LLMs). The predominant focus of research has revolved around developing diverse prompting strategies to guide and structure the reasoning processes of LLMs. However, these approaches based on decoder-only causal language models often operate the input question in a single forward pass, potentially missing the rich, back-and-forth interactions inherent in human reasoning. Scant attention has been paid to a critical dimension, i.e., the input question itself embedded within the prompts. In response, we introduce a deceptively simple yet highly effective prompting strategy, termed question "re-reading". Drawing inspiration from human learning and problem-solving, re-reading entails revisiting the question information embedded within input prompts. This approach aligns seamlessly with the cognitive principle of reinforcement, enabling LLMs to extract deeper insights, identify intricate patterns, establish mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00155</link><description>&lt;p&gt;
LLM&#22312;Shell&#20013;&#30340;&#24212;&#29992;&#65306;&#29983;&#25104;&#24335;&#34588;&#32592;
&lt;/p&gt;
&lt;p&gt;
LLM in the Shell: Generative Honeypots. (arXiv:2309.00155v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34588;&#32592;&#26159;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#34588;&#32592;&#65288;&#21363;&#20351;&#26159;&#39640;&#20132;&#20114;&#24335;&#30340;&#65289;&#32570;&#20047;&#36275;&#22815;&#30340;&#30495;&#23454;&#24863;&#26469;&#27450;&#39575;&#25915;&#20987;&#32773;&#12290;&#36825;&#20010;&#38480;&#21046;&#20351;&#24471;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#35782;&#21035;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#33021;&#22815;&#21019;&#24314;&#21487;&#20449;&#19988;&#21160;&#24577;&#30340;&#34588;&#32592;&#65292;&#33021;&#22815;&#35299;&#20915;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#22914;&#30830;&#23450;&#24615;&#21709;&#24212;&#12289;&#32570;&#20047;&#36866;&#24212;&#24615;&#31561;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#38656;&#35201;&#21028;&#26029;&#34588;&#32592;&#22238;&#24212;&#26159;&#21542;&#34394;&#20551;&#30340;&#25915;&#20987;&#32773;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#27599;&#20010;&#21629;&#20196;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#34588;&#32592;&#65292;&#31216;&#20026;shelLM&#65292;&#36798;&#21040;&#20102;0.92&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Honeypots are essential tools in cybersecurity. However, most of them (even the high-interaction ones) lack the required realism to engage and fool human attackers. This limitation makes them easily discernible, hindering their effectiveness. This work introduces a novel method to create dynamic and realistic software honeypots based on Large Language Models. Preliminary results indicate that LLMs can create credible and dynamic honeypots capable of addressing important limitations of previous honeypots, such as deterministic responses, lack of adaptability, etc. We evaluated the realism of each command by conducting an experiment with human attackers who needed to say if the answer from the honeypot was fake or not. Our proposed honeypot, called shelLM, reached an accuracy rate of 0.92.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#30693;&#35782;&#32858;&#21512;&#36807;&#31243;&#19982;&#30456;&#20851;&#30693;&#35782;&#22270;&#30340;&#20840;&#23616;&#29305;&#24449;&#26377;&#25928;&#34701;&#21512;&#65292;&#23558;&#22686;&#24378;&#30340;&#22270;&#32467;&#26500;&#30693;&#35782;&#38598;&#25104;&#21040;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#33258;&#21160;&#24230;&#37327;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06294</link><description>&lt;p&gt;
CADGE&#65306;&#22522;&#20110;&#22270;&#32467;&#26500;&#30693;&#35782;&#32858;&#21512;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured Knowledge Aggregation. (arXiv:2305.06294v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#30693;&#35782;&#32858;&#21512;&#36807;&#31243;&#19982;&#30456;&#20851;&#30693;&#35782;&#22270;&#30340;&#20840;&#23616;&#29305;&#24449;&#26377;&#25928;&#34701;&#21512;&#65292;&#23558;&#22686;&#24378;&#30340;&#22270;&#32467;&#26500;&#30693;&#35782;&#38598;&#25104;&#21040;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#33258;&#21160;&#24230;&#37327;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#30693;&#35782;&#65288;commonsense knowledge&#65289;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#22270;&#30693;&#35782;&#19982;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#65292;&#23548;&#33268;&#25991;&#26412;&#21644;&#22270;&#30693;&#35782;&#32534;&#30721;&#36807;&#31243;&#22312;&#20018;&#34892;&#27969;&#27700;&#32447;&#20013;&#34987;&#20998;&#31163;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#20123;&#20998;&#31163;&#30340;&#34920;&#31034;&#23398;&#20064;&#38454;&#27573;&#21487;&#33021;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21253;&#21547;&#22312;&#20004;&#31181;&#36755;&#20837;&#30693;&#35782;&#31867;&#22411;&#20013;&#30340;&#25972;&#20307;&#19978;&#19979;&#25991;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65288;Context-aware GAT&#65289;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#30693;&#35782;&#32858;&#21512;&#36807;&#31243;&#26377;&#25928;&#22320;&#34701;&#21512;&#30456;&#20851;&#30693;&#35782;&#22270;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#24322;&#26500;&#29305;&#24449;&#8212;&#8212;&#23558;&#22270;&#30693;&#35782;&#19982;&#25991;&#26412;&#30456;&#32467;&#21512;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#22312;&#36830;&#25509;&#23376;&#22270;&#19978;&#20998;&#23618;&#24212;&#29992;&#22270;&#30693;&#35782;&#32858;&#21512;&#20197;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#23558;&#22686;&#24378;&#30340;&#22270;&#32467;&#26500;&#30693;&#35782;&#38598;&#25104;&#21040;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#33258;&#21160;&#24230;&#37327;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense knowledge is crucial to many natural language processing tasks. Existing works usually incorporate graph knowledge with conventional graph neural networks (GNNs), leading to the text and graph knowledge encoding processes being separated in a serial pipeline. We argue that these separate representation learning stages may be suboptimal for neural networks to learn the overall context contained in both types of input knowledge. In this paper, we propose a novel context-aware graph-attention model (Context-aware GAT), which can effectively incorporate global features of relevant knowledge graphs based on a context-enhanced knowledge aggregation process. Specifically, our framework leverages a novel representation learning approach to process heterogeneous features - combining flattened graph knowledge with text. To the best of our knowledge, this is the first attempt at hierarchically applying graph knowledge aggregation on a connected subgraph in addition to contextual infor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CKBP v2, &#19968;&#20010;&#20351;&#29992;&#19987;&#23478;&#27880;&#37322;&#32780;&#22218;&#25324;&#23545;&#25239;&#26679;&#26412;&#30340;&#39640;&#36136;&#37327;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#22522;&#20934;&#65292;&#20197;&#35299;&#20915;CKBP v1&#30001;&#20110;&#20247;&#21253;&#27880;&#37322;&#21644;&#38543;&#26426;&#25277;&#26679;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#20219;&#21153;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10392</link><description>&lt;p&gt;
CKBP v2&#65306;&#19968;&#20010;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#30340;&#19987;&#23478;&#27880;&#37322;&#35780;&#20272;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population. (arXiv:2304.10392v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CKBP v2, &#19968;&#20010;&#20351;&#29992;&#19987;&#23478;&#27880;&#37322;&#32780;&#22218;&#25324;&#23545;&#25239;&#26679;&#26412;&#30340;&#39640;&#36136;&#37327;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#22522;&#20934;&#65292;&#20197;&#35299;&#20915;CKBP v1&#30001;&#20110;&#20247;&#21253;&#27880;&#37322;&#21644;&#38543;&#26426;&#25277;&#26679;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#20219;&#21153;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22635;&#20805;&#36890;&#35782;&#30693;&#35782;&#24211;&#26159;NLP&#20013;&#19968;&#20010;&#37325;&#35201;&#20294;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#22788;&#29702;&#22806;&#37096;&#26469;&#28304;&#12289;&#26410;&#35265;&#36807;&#30340;&#20107;&#20214;&#21644;&#23454;&#20307;&#30340;&#30693;&#35782;&#12290; Fang&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#35780;&#20272;&#38598;CKBP v1&#12290;&#20294;&#26159;&#65292;CKBP v1&#37319;&#29992;&#30001;&#20247;&#21253;&#27880;&#37322;&#65292;&#23384;&#22312;&#30456;&#24403;&#22823;&#27604;&#20363;&#30340;&#38169;&#35823;&#31572;&#26696;&#65292;&#24182;&#19988;&#30001;&#20110;&#38543;&#26426;&#25277;&#26679;&#65292;&#35780;&#20272;&#38598;&#19982;&#22806;&#37096;&#30693;&#35782;&#26469;&#28304;&#30340;&#23545;&#40784;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CKBP v2&#65292;&#19968;&#20010;&#26032;&#30340;&#39640;&#36136;&#37327;&#30340;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#32780;&#19981;&#26159;&#20247;&#21253;&#27880;&#37322;&#65292;&#24182;&#28155;&#21152;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#20351;&#35780;&#20272;&#38598;&#26356;&#20855;&#20195;&#34920;&#24615;&#26469;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;&#35780;&#20272;&#38598;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#29992;&#20110;&#36890;&#35782;&#30693;&#35782;&#24211;&#22635;&#20805;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#27604;&#36739;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22635;&#20805;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Populating Commonsense Knowledge Bases (CSKB) is an important yet hard task in NLP, as it tackles knowledge from external sources with unseen events and entities. Fang et al. (2021a) proposed a CSKB Population benchmark with an evaluation set CKBP v1. However, CKBP v1 adopts crowdsourced annotations that suffer from a substantial fraction of incorrect answers, and the evaluation set is not well-aligned with the external knowledge source as a result of random sampling. In this paper, we introduce CKBP v2, a new high-quality CSKB Population benchmark, which addresses the two mentioned problems by using experts instead of crowd-sourced annotation and by adding diversified adversarial samples to make the evaluation set more representative. We conduct extensive experiments comparing state-of-the-art methods for CSKB Population on the new evaluation set for future research comparisons. Empirical results show that the population task is still challenging, even for large language models (LLM) 
&lt;/p&gt;</description></item></channel></rss>