<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#22312;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#19978;&#39044;&#35757;&#32451;&#36739;&#23567;&#27169;&#22411;&#65292;&#20197;&#33976;&#39311;SSL&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#32039;&#20945;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#20855;&#26377;&#30701;&#25512;&#29702;&#31649;&#36947;&#21644;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#31561;&#20248;&#28857;</title><link>https://arxiv.org/abs/2402.19333</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#39044;&#35757;&#32451;&#23454;&#29616;&#32039;&#20945;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compact Speech Translation Models via Discrete Speech Units Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19333
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#19978;&#39044;&#35757;&#32451;&#36739;&#23567;&#27169;&#22411;&#65292;&#20197;&#33976;&#39311;SSL&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#32039;&#20945;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#20855;&#26377;&#30701;&#25512;&#29702;&#31649;&#36947;&#21644;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#31561;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#27169;&#22411;&#21021;&#22987;&#21270;&#22914;&#20170;&#22312;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#20013;&#33719;&#24471;&#24378;&#22823;&#32467;&#26524;&#26159;&#24120;&#35265;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#20250;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#65292;&#38459;&#30861;&#20102;&#35774;&#22791;&#37096;&#32626;&#12290;&#26412;&#25991;&#21033;&#29992;SSL&#27169;&#22411;&#36890;&#36807;&#22312;&#20854;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#65288;DSU&#65289;&#19978;&#39044;&#35757;&#32451;&#36739;&#23567;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;1&#65289;Filterbank-to-DSU&#21644;2&#65289;DSU-to-Translation&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#28982;&#21518;&#21462;&#33258;1&#65289;&#30340;&#32534;&#30721;&#22120;&#21644;&#26469;&#33258;2&#65289;&#30340;&#35299;&#30721;&#22120;&#26469;&#21021;&#22987;&#21270;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#22312;&#26377;&#38480;&#30340;&#35821;&#38899;&#32763;&#35793;&#25968;&#25454;&#19978;&#24494;&#35843;&#12290;&#36890;&#36807;&#20351;&#29992;DSU&#39044;&#35757;&#32451;&#26469;&#25552;&#28860;SSL&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#26368;&#32456;&#27169;&#22411;&#21464;&#24471;&#32039;&#20945;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#20351;&#29992;DSU&#20316;&#20026;&#27169;&#22411;&#36755;&#20837;&#26377;&#20960;&#20010;&#20248;&#28857;&#65292;&#27604;&#22914;&#25512;&#29702;&#31649;&#36947;&#26356;&#30701;&#21644;&#23545;&#65288;DSU&#65289;&#26631;&#35760;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#19982;ASR&#39044;&#35757;&#32451;&#30456;&#27604;&#65292;&#23427;&#19981;&#38656;&#35201;&#36716;&#24405;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#29615;&#22659;&#12290;&#22312;CoVoST-2 X-En&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19333v1 Announce Type: new  Abstract: Using Self-Supervised Learning (SSL) as model initialization is now common to obtain strong results in Speech Translation (ST). However, they also impose a large memory footprint, hindering on-device deployment. In this paper, we leverage the SSL models by pretraining smaller models on their Discrete Speech Units (DSU). We pretrain encoder-decoder models on 1) Filterbank-to-DSU and 2) DSU-to-Translation data, and take the encoder from 1) and the decoder from 2) to initialise a new model, finetuning this on limited speech-translation data. The final model becomes compact by using the DSU pretraining to distil the knowledge of the SSL model. Our method has several benefits over using DSU as model inputs, such as shorter inference pipeline and robustness over (DSU) tokenization. In contrast to ASR pretraining, it does not require transcripts, making it applicable to low-resource settings. Evaluation on CoVoST-2 X-En shows that our method is
&lt;/p&gt;</description></item><item><title>&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#38469;&#20020;&#24202;&#26696;&#20363;&#19978;&#30340;&#34920;&#29616;&#26159;&#20851;&#38190;&#65292;&#22240;&#27492;&#26500;&#24314;&#20102;&#20004;&#20010;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.18060</link><description>&lt;p&gt;
&#22312;&#22238;&#31572;&#21644;&#35299;&#37322;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21307;&#23398;&#38382;&#39064;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18060
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#38469;&#20020;&#24202;&#26696;&#20363;&#19978;&#30340;&#34920;&#29616;&#26159;&#20851;&#38190;&#65292;&#22240;&#27492;&#26500;&#24314;&#20102;&#20004;&#20010;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#36890;&#36807;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#22996;&#21592;&#20250;&#32771;&#35797;&#38382;&#39064;&#25110;&#19968;&#33324;&#21307;&#23398;&#38382;&#39064;&#65292;&#26080;&#27861;&#25429;&#25417;&#30495;&#23454;&#20020;&#24202;&#26696;&#20363;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#31572;&#26696;&#30340;&#21442;&#32771;&#35299;&#37322;&#38459;&#30861;&#20102;&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#35780;&#20272;&#65292;&#36825;&#23545;&#25903;&#25345;&#21307;&#29983;&#20570;&#20986;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;JAMA&#20020;&#24202;&#25361;&#25112;&#21644;Medbullets&#12290;JAMA&#20020;&#24202;&#25361;&#25112;&#21253;&#21547;&#22522;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20020;&#24202;&#26696;&#20363;&#30340;&#38382;&#39064;&#65292;&#32780;Medbullets&#21253;&#21547;&#31867;&#20284;USMLE Step 2&amp;3&#39118;&#26684;&#30340;&#20020;&#24202;&#38382;&#39064;&#12290;&#20004;&#20010;&#25968;&#25454;&#38598;&#22343;&#20197;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;-&#22238;&#31572;&#20219;&#21153;&#30340;&#32467;&#26500;&#21270;&#24418;&#24335;&#21576;&#29616;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#38468;&#26377;&#19987;&#23478;&#25776;&#20889;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#22312;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22235;&#20010;LLMs&#12290;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18060v1 Announce Type: new  Abstract: LLMs have demonstrated impressive performance in answering medical questions, such as passing medical licensing examinations. However, most existing benchmarks rely on board exam questions or general medical questions, falling short in capturing the complexity of realistic clinical cases. Moreover, the lack of reference explanations for answers hampers the evaluation of model explanations, which are crucial to supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises USMLE Step 2&amp;3 style clinical questions. Both datasets are structured as multiple-choice question-answering tasks, where each question is accompanied by an expert-written explanation. We evaluate four LLMs on the two datasets using various prompts. Experiments demonstrat
&lt;/p&gt;</description></item><item><title>transformers&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#37319;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#32780;&#38750;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.17709</link><description>&lt;p&gt;
&#22522;&#20110;&#26696;&#20363;&#36824;&#26159;&#22522;&#20110;&#35268;&#21017;&#65306;&#21464;&#21387;&#22120;&#22914;&#20309;&#36827;&#34892;&#25968;&#23398;&#35745;&#31639;&#65311;
&lt;/p&gt;
&lt;p&gt;
Case-Based or Rule-Based: How Do Transformers Do the Math?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17709
&lt;/p&gt;
&lt;p&gt;
transformers&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#37319;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#32780;&#38750;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#19968;&#20123;&#23545;&#20154;&#31867;&#26469;&#35828;&#31616;&#21333;&#19988;&#30452;&#35266;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#20363;&#22914;&#21152;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#23398;&#20064;&#21152;&#27861;&#30340;&#22522;&#26412;&#35268;&#21017;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20219;&#24847;&#38271;&#24230;&#30340;&#26032;&#38382;&#39064;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21364;&#38590;&#20197;&#20570;&#21040;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#21487;&#33021;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30475;&#21040;&#30340;&#31867;&#20284;&#8220;&#26696;&#20363;&#8221;&#26469;&#33719;&#21462;&#24110;&#21161;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#26426;&#21046;&#23450;&#20041;&#20026;&#8220;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#8221;&#21644;&#8220;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#8221;&#12290;&#30001;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#23545;&#20110;&#33719;&#24471;&#31995;&#32479;&#21270;&#27010;&#25324;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#31350;&#21464;&#21387;&#22120;&#31350;&#31455;&#26159;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#36824;&#26159;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#20116;&#20010;&#25968;&#23398;&#20219;&#21153;&#30340;&#24178;&#39044;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#35748;&#21464;&#21387;&#22120;&#27491;&#22312;&#25191;&#34892;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#65292;&#26080;&#35770;&#26159;&#21542;&#20351;&#29992;&#33609;&#31295;&#26412;&#65292;&#36825;&#19982;&#20043;&#21069;&#30340;&#35266;&#23519;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17709v1 Announce Type: new  Abstract: Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar "cases" seen in the training corpus for help. We define these two different reasoning mechanisms as "rule-based reasoning" and "case-based reasoning". Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that tran
&lt;/p&gt;</description></item><item><title>Ouroboros&#36890;&#36807;&#26500;&#24314;&#30701;&#23567;&#33609;&#26696;&#24182;&#24341;&#20837;&#20505;&#36873;&#30701;&#35821;&#27744;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#25928;&#29575;</title><link>https://arxiv.org/abs/2402.13720</link><description>&lt;p&gt;
Ouroboros: &#22823;&#27169;&#22411;&#22686;&#24378;&#33609;&#26696;&#30340;&#29468;&#27979;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Ouroboros: Speculative Decoding with Large Model Enhanced Drafting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13720
&lt;/p&gt;
&lt;p&gt;
Ouroboros&#36890;&#36807;&#26500;&#24314;&#30701;&#23567;&#33609;&#26696;&#24182;&#24341;&#20837;&#20505;&#36873;&#30701;&#35821;&#27744;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#30701;&#23567;&#39640;&#25928;&#30340;&#23567;&#27169;&#22411;&#36215;&#33609;&#33609;&#26696;&#65292;&#28982;&#21518;&#35201;&#27714;&#22823;&#35821;&#35328;&#27169;&#22411;&#20197;&#26080;&#33258;&#22238;&#24402;&#26041;&#24335;&#36827;&#34892;&#39564;&#35777;&#21644;&#20462;&#27491;&#65292;&#20197;&#26368;&#23567;&#21270;&#26102;&#38388;&#24320;&#38144;&#12290;&#24403;&#39564;&#35777;&#21518;&#21487;&#20197;&#29983;&#25104;&#26356;&#38271;&#30340;&#33609;&#31295;&#65292;&#20294;&#20063;&#20250;&#23548;&#33268;&#30456;&#24403;&#22823;&#30340;&#23581;&#35797;&#21644;&#38169;&#35823;&#25104;&#26412;&#12290;&#30001;&#20110;&#39640;&#39564;&#35777;&#22833;&#36133;&#27010;&#29575;&#65292;&#29616;&#26377;&#35299;&#30721;&#26041;&#27861;&#19981;&#33021;&#19968;&#27425;&#36215;&#33609;&#22826;&#22810;&#20869;&#23481;&#36827;&#34892;&#39564;&#35777;&#65292;&#23454;&#29616;&#27425;&#20248;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13720v1 Announce Type: new  Abstract: Drafting-then-verifying decoding methods such as speculative decoding are widely adopted training-free methods to accelerate the inference of large language models (LLMs). Instead of employing an autoregressive process to decode tokens sequentially, speculative decoding initially creates drafts with an efficient small model. Then LLMs are required to conduct verification and correction in a non-autoregressive fashion to minimize time overhead. Generating longer drafts can lead to even more significant speedups once verified, but also incurs substantial trial and error costs if it fails. Suffering from the high verification failure probability, existing decoding methods cannot draft too much content for verification at one time, achieving sub-optimal inference acceleration. In this paper, we introduce Ouroboros, which constructs a phrase candidate pool from the verification process of LLMs to provide candidates for draft generation of the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#35270;&#35273;&#35821;&#35328;&#26144;&#23556;&#22120;&#65288;CVLM&#65289;&#65292;&#36890;&#36807;&#22686;&#24378;&#35270;&#35273;&#30693;&#35782;&#23545;&#40784;&#65292;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25361;&#25112;&#30693;&#35782;&#22411;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.13561</link><description>&lt;p&gt;
&#35748;&#30693;&#35270;&#35273;&#35821;&#35328;&#26144;&#23556;&#22120;&#65306;&#36890;&#36807;&#22686;&#24378;&#35270;&#35273;&#30693;&#35782;&#23545;&#40784;&#25512;&#36827;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#35270;&#35273;&#35821;&#35328;&#26144;&#23556;&#22120;&#65288;CVLM&#65289;&#65292;&#36890;&#36807;&#22686;&#24378;&#35270;&#35273;&#30693;&#35782;&#23545;&#40784;&#65292;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25361;&#25112;&#30693;&#35782;&#22411;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21644;&#21453;&#24605;&#24403;&#21069;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#29616;&#29366;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24191;&#27867;&#20351;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#25237;&#24433;&#26041;&#27861;&#65288;&#22914;Q-former&#25110;MLP&#65289;&#20391;&#37325;&#20110;&#22270;&#20687;-&#25991;&#26412;&#25551;&#36848;&#30340;&#23545;&#40784;&#65292;&#20294;&#24573;&#30053;&#20102;&#35270;&#35273;&#30693;&#35782;&#32500;&#24230;&#30340;&#23545;&#40784;&#65292;&#21363;&#23558;&#35270;&#35273;&#19982;&#20854;&#30456;&#20851;&#30693;&#35782;&#36830;&#25509;&#36215;&#26469;&#12290;&#35270;&#35273;&#30693;&#35782;&#22312;&#20998;&#26512;&#12289;&#25512;&#26029;&#21644;&#35299;&#37322;&#35270;&#35273;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#39064;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#30693;&#35782;&#23545;&#40784;&#26469;&#25913;&#36827;LMMs&#65292;&#29305;&#21035;&#38024;&#23545;&#25361;&#25112;&#30693;&#35782;&#22411;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#35270;&#35273;&#35821;&#35328;&#26144;&#23556;&#22120;&#65288;CVLM&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#30693;&#35782;&#23545;&#40784;&#22120;&#65288;VKA&#65289;&#21644;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#33410;&#38454;&#27573;&#30340;&#32454;&#31890;&#24230;&#30693;&#35782;&#36866;&#37197;&#22120;&#65288;FKA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13561v1 Announce Type: new  Abstract: Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#22810;&#27425;&#36845;&#20195;&#21512;&#25104;&#26469;&#25913;&#21892;&#25991;&#26412;&#21040;SQL&#28436;&#31034;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#22810;&#26679;&#24615;&#28436;&#31034;&#27744;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#24615;&#24182;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.10663</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#34701;&#21512;&#26041;&#27861;&#25913;&#21892;&#25991;&#26412;&#21040;SQL&#30340;&#28436;&#31034;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#22810;&#27425;&#36845;&#20195;&#21512;&#25104;&#26469;&#25913;&#21892;&#25991;&#26412;&#21040;SQL&#28436;&#31034;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#22810;&#26679;&#24615;&#28436;&#31034;&#27744;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#24615;&#24182;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#20026;&#25991;&#26412;&#21040;SQL&#30740;&#31350;&#30340;&#20027;&#27969;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35752;&#35770;&#20102;&#22914;&#20309;&#20174;&#20154;&#26631;&#35760;&#30340;&#28436;&#31034;&#27744;&#20013;&#36873;&#25321;&#19982;&#29992;&#25143;&#38382;&#39064;&#30456;&#20851;&#30340;&#28436;&#31034;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26631;&#27880;&#23384;&#22312;&#30528;&#22810;&#26679;&#24615;&#19981;&#36275;&#21644;&#26631;&#27880;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#34913;&#37327;&#21644;&#25913;&#21892;&#25991;&#26412;&#21040;SQL&#28436;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24230;&#37327;&#28436;&#31034;&#22810;&#26679;&#24615;&#30340;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#29616;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#22522;&#20110;&#19978;&#36848;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#22810;&#27425;&#36845;&#20195;&#21512;&#25104;&#26469;&#26500;&#24314;&#39640;&#22810;&#26679;&#24615;&#28436;&#31034;&#27744;&#30340;&#34701;&#21512;&#26041;&#27861;&#65288;Fused&#65289;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#24615;&#24182;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;/&#26080;&#20154;&#31867;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#24179;&#22343;&#25552;&#39640;&#20102;3.2%&#21644;5.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10663v1 Announce Type: new  Abstract: Currently, the in-context learning method based on large language models (LLMs) has become the mainstream of text-to-SQL research. Previous works have discussed how to select demonstrations related to the user question from a human-labeled demonstration pool. However, human labeling suffers from the limitations of insufficient diversity and high labeling overhead. Therefore, in this paper, we discuss how to measure and improve the diversity of the demonstrations for text-to-SQL. We present a metric to measure the diversity of the demonstrations and analyze the insufficient of the existing labeled data by experiments. Based on the above discovery, we propose fusing iteratively for demonstrations (Fused) to build a high-diversity demonstration pool through human-free multiple-iteration synthesis, improving diversity and lowering label cost. Our method achieves an average improvement of 3.2% and 5.0% with and without human labeling on sever
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#65292;&#30452;&#25509;&#35782;&#21035;&#20986;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#65292;&#20174;&#32780;&#29983;&#25104;&#20248;&#31168;&#30340;&#24605;&#32500;&#38142;&#12290;</title><link>https://arxiv.org/abs/2402.06918</link><description>&lt;p&gt;
&#29992;&#30452;&#25509;&#30340;&#20004;&#20004;&#27604;&#36739;&#26041;&#27861;&#29983;&#25104;&#24605;&#32500;&#38142;&#65292;&#20197;&#25628;&#32034;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#65292;&#30452;&#25509;&#35782;&#21035;&#20986;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#65292;&#20174;&#32780;&#29983;&#25104;&#20248;&#31168;&#30340;&#24605;&#32500;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#24605;&#32500;&#38142;(Chain-of-Thoughts, CoT)&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#38382;&#39064;&#35299;&#20915;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#36825;&#31181;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#28041;&#21450;&#20114;&#21160;&#21327;&#20316;&#65292;&#23398;&#20064;&#32773;&#29983;&#25104;&#20505;&#36873;&#20013;&#38388;&#24605;&#32500;&#65292;&#30001;LLMs&#35780;&#20272;&#65292;&#24341;&#23548;&#29983;&#25104;&#21518;&#32493;&#24605;&#32500;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24191;&#27867;&#20294;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#65292;LLMs&#30340;&#35780;&#20272;&#36890;&#24120;&#23384;&#22312;&#22122;&#22768;&#21644;&#19981;&#21487;&#38752;&#24615;&#65292;&#21487;&#33021;&#35823;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#36873;&#25321;&#19981;&#22815;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#12290;&#26412;&#25991;&#21463;Vapnik&#21407;&#21017;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27604;&#36739;&#30340;CoT&#29983;&#25104;&#31639;&#27861;&#65292;&#30452;&#25509;&#26681;&#25454;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#30830;&#23450;&#26368;&#26377;&#28508;&#21147;&#30340;&#24605;&#32500;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#37197;&#23545;&#20013;&#38388;&#24605;&#32500;&#65292;&#24182;&#30452;&#25509;&#20419;&#20351;LLMs&#20174;&#27599;&#23545;&#20013;&#36873;&#25321;&#26356;&#26377;&#28508;&#21147;&#30340;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the ability of the large language model (LLMs) to handle complex reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs to reason step-by-step, facilitating problem solving from simple to complex tasks. State-of-the-art approaches for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the LLM is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik's principle, we propose a novel comparison-based CoT generation algorithm that directly identifies the most promising thoughts with the noisy feedback from the LLM. In each round, we randomly pair intermediate thoughts and directly prompt the LLM to select the more promising one from each pair,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05935</link><description>&lt;p&gt;
SPHINX-X: &#25193;&#23637;&#25968;&#25454;&#21644;&#21442;&#25968;&#29992;&#20110;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;SPHINX-X&#65292;&#19968;&#31181;&#22522;&#20110;SPHINX&#24320;&#21457;&#30340;&#24191;&#27867;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#31995;&#21015;&#12290;&#20026;&#20102;&#25913;&#21892;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#31227;&#38500;&#20887;&#20313;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#32469;&#36807;&#23436;&#20840;&#22635;&#20805;&#30340;&#23376;&#22270;&#20687;&#65292;&#24182;&#23558;&#22810;&#38454;&#27573;&#35757;&#32451;&#31616;&#21270;&#25104;&#20026;&#19968;&#38454;&#27573;&#30340;&#20840;&#38598;&#21512;&#27169;&#24335;&#65292;&#20462;&#25913;&#20102;SPHINX&#26694;&#26550;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;MLLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#36328;&#35821;&#35328;&#12289;&#36328;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#39046;&#22495;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#30340;OCR&#23494;&#38598;&#21644;Mark&#25968;&#25454;&#38598;&#20016;&#23500;&#36825;&#20010;&#25910;&#38598;&#65292;&#25193;&#23637;&#20102;&#22810;&#26679;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22522;&#30784;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;TinyLlama1.1B&#12289;InternLM2-7B&#12289;LLaMA2-13B&#21644;Mixtral8x7B&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#21464;&#21270;&#30340;MLLMs&#12290;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#24615;&#33021;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04678</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#20449;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As Faithful Explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#37096;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24050;&#32463;&#33021;&#22815;&#29087;&#32451;&#35299;&#20915;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#20256;&#32479;&#30340;&#20197;&#36755;&#20837;&#20026;&#37325;&#28857;&#30340;&#35299;&#37322;&#31639;&#27861;&#26469;&#35299;&#37322;LLMs&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#26426;&#21046;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#36827;&#34892;&#21333;&#21521;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;LLMs&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#32463;&#24120;&#22240;&#20026;&#32570;&#20047;&#21487;&#20449;&#24230;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#22240;&#20026;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#19981;&#20934;&#30830;&#22320;&#21453;&#26144;LLMs&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;xLLM&#65292;&#20197;&#25552;&#39640;LLMs&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;xLLM&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#30340;&#25200;&#21160;&#26412;&#20307;&#20197;&#21450;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#35780;&#20272;&#20102;LLMs&#22312;&#25968;&#23383;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#22312;&#20840;&#38754;&#35780;&#20272;&#20013;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#25200;&#21160;&#38382;&#39064;&#19978;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLMs&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.09395</link><description>&lt;p&gt;
&#34987;&#29702;&#24615;&#30340;&#27969;&#27801;&#25152;&#22256;&#65292;&#36828;&#31163;AGI&#23792;&#20250;&#65306;&#36890;&#36807;&#26412;&#20307;&#24341;&#23548;&#24178;&#39044;&#35780;&#20272;LLMs&#30340;&#25968;&#23398;&#21644;&#32534;&#30721;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09395
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#30340;&#25200;&#21160;&#26412;&#20307;&#20197;&#21450;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#35780;&#20272;&#20102;LLMs&#22312;&#25968;&#23383;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#22312;&#20840;&#38754;&#35780;&#20272;&#20013;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#25200;&#21160;&#38382;&#39064;&#19978;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLMs&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21457;&#23637;&#23637;&#31034;&#20102;&#22312;&#29616;&#26377;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#26524;&#65292;&#20854;&#20013;&#19968;&#20123;&#27169;&#22411;&#29978;&#33267;&#36229;&#36807;&#20102;&#20154;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#21644;&#31283;&#20581;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20851;&#27880;&#20004;&#20010;&#27969;&#34892;&#30340;&#25512;&#29702;&#20219;&#21153;&#65306;&#31639;&#26415;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#65306;&#65288;i&#65289;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#30340;&#36890;&#29992;&#25200;&#21160;&#26412;&#20307;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#21322;&#33258;&#21160;&#26041;&#27861;&#26469;&#24212;&#29992;&#36825;&#20123;&#25200;&#21160;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#20004;&#20010;&#25968;&#25454;&#38598;MORE&#21644;CORE&#65292;&#20998;&#21035;&#29992;&#20110;&#25200;&#21160;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#65292;&#20197;&#25506;&#31350;LLM&#22312;&#25968;&#23383;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#26497;&#38480;&#12290;&#36890;&#36807;&#23545;&#23553;&#38381;&#28304;&#21644;&#24320;&#28304;LLMs&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#26377;&#27169;&#22411;&#23545;&#25200;&#21160;&#38382;&#39064;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLMs&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.09395v2 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe the limits of LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust probl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;LLMs&#26377;&#25928;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2312.06149</link><description>&lt;p&gt;
&#35299;&#38145;&#39044;&#27979;&#24615;&#25991;&#26412;&#29983;&#25104;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#30340;&#21463;&#38480;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06149
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;LLMs&#26377;&#25928;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32473;&#23450;&#25552;&#31034;&#25110;&#25351;&#20196;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21313;&#20159;&#32423;&#21035;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19981;&#33391;&#34892;&#20026;&#22914;&#27602;&#24615;&#25110;&#24187;&#35273;&#21487;&#33021;&#20250;&#26174;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#12290;&#20351;&#29992;LLMs&#23454;&#29616;&#26410;&#26469;&#32422;&#26463;&#28385;&#36275;&#24230;&#30340;&#20272;&#35745;&#24341;&#23548;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65306;&#20851;&#38190;&#35789;&#21463;&#38480;&#29983;&#25104;&#12289;&#27602;&#24615;&#20943;&#23569;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06149v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 202
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35282;&#24230;&#34701;&#21512;&#32467;&#26500;&#25628;&#32034;&#30340;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#36830;&#32493;&#30340;&#35282;&#24230;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.09361</link><description>&lt;p&gt;
MFAS: &#22522;&#20110;&#22810;&#35282;&#24230;&#34701;&#21512;&#32467;&#26500;&#25628;&#32034;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
MFAS: Emotion Recognition through Multiple Perspectives Fusion Architecture Search Emulating Human Cognition. (arXiv:2306.09361v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09361
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35282;&#24230;&#34701;&#21512;&#32467;&#26500;&#25628;&#32034;&#30340;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#36830;&#32493;&#30340;&#35282;&#24230;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26088;&#22312;&#35782;&#21035;&#21644;&#20998;&#26512;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;&#23436;&#32654;&#30340;&#24773;&#24863;&#35782;&#21035;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#21892;&#21508;&#31181;&#20154;&#26426;&#20132;&#20114;&#20219;&#21153;&#12290;&#21463;&#20154;&#31867;&#29702;&#35299;&#24773;&#24863;&#30340;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#37327;&#21270;&#24314;&#27169;&#30456;&#27604;&#65292;&#20174;&#36830;&#32493;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#38899;&#20869;&#23481;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#29702;&#35299;&#65292;&#33021;&#22815;&#20351;&#27169;&#22411;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#26681;&#25454;&#35821;&#38899;&#20013;&#23384;&#22312;&#30340;&#26576;&#20123;&#32447;&#32034;&#35843;&#25972;&#24773;&#24863;&#21333;&#35789;&#30340;&#25991;&#26412;&#35821;&#20041;&#30340;&#24863;&#30693;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25628;&#32034;&#31354;&#38388;&#24182;&#25628;&#32034;&#20004;&#31181;&#20449;&#24687;&#30340;&#26368;&#20339;&#34701;&#21512;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#35843;&#25972;&#24863;&#30693;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Multiple perspectives Fusion Architecture Search(MFAS)&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition aims to identify and analyze emotional states in target speech similar to humans. Perfect emotion recognition can greatly benefit a wide range of human-machine interaction tasks. Inspired by the human process of understanding emotions, we demonstrate that compared to quantized modeling, understanding speech content from a continuous perspective, akin to human-like comprehension, enables the model to capture more comprehensive emotional information. Additionally, considering that humans adjust their perception of emotional words in textual semantic based on certain cues present in speech, we design a novel search space and search for the optimal fusion strategy for the two types of information. Experimental results further validate the significance of this perception adjustment. Building on these observations, we propose a novel framework called Multiple perspectives Fusion Architecture Search (MFAS). Specifically, we utilize continuous-based knowledge to capt
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#38899;&#21311;&#21517;&#21270;&#22312; COVID-19 &#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21311;&#21517;&#21270;&#26041;&#27861;&#21487;&#33021;&#20250;&#23545;&#35821;&#38899;&#35786;&#26029;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.02181</link><description>&lt;p&gt;
&#20851;&#20110;&#22768;&#38899;&#21311;&#21517;&#21270;&#23545;&#22522;&#20110;&#35821;&#38899;&#30340;COVID-19&#26816;&#27979;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Voice Anonymization on Speech-Based COVID-19 Detection. (arXiv:2304.02181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02181
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#38899;&#21311;&#21517;&#21270;&#22312; COVID-19 &#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21311;&#21517;&#21270;&#26041;&#27861;&#21487;&#33021;&#20250;&#23545;&#35821;&#38899;&#35786;&#26029;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;&#35821;&#38899;&#30340;&#24212;&#29992;&#27491;&#34028;&#21187;&#21457;&#23637;&#65292;&#20174;&#20010;&#20154;&#21161;&#29702;&#12289;&#24773;&#24863;&#35745;&#31639;&#21040;&#36828;&#31243;&#30142;&#30149;&#35786;&#26029;&#12290;&#30001;&#20110;&#22768;&#38899;&#21516;&#26102;&#21253;&#21547;&#35821;&#35328;&#21644;&#35821;&#29992;&#20449;&#24687;&#65288;&#22914;&#35821;&#38899;&#38899;&#35843;&#12289;&#35821;&#35843;&#12289;&#35821;&#36895;&#12289;&#22768;&#38899;&#22823;&#23567;&#65289;&#65292;&#22240;&#27492;&#20445;&#25252;&#35828;&#35805;&#32773;&#30340;&#38544;&#31169;&#21644;&#36523;&#20221;&#30340;&#22768;&#38899;&#21311;&#21517;&#21270;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#22768;&#38899;&#38544;&#31169;&#38382;&#39064;&#24050;&#32463;&#20986;&#29616;&#65292;&#37325;&#28857;&#26159;&#21435;&#38500;&#35828;&#35805;&#32773;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#35328;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24773;&#24863;&#35745;&#31639;&#21644;&#30142;&#30149;&#30417;&#27979;&#24212;&#29992;&#32780;&#35328;&#65292;&#35821;&#29992;&#20869;&#23481;&#21487;&#33021;&#26356;&#20026;&#20851;&#38190;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#21311;&#21517;&#21270;&#21487;&#33021;&#23545;&#36825;&#20123;&#31995;&#32479;&#20135;&#29983;&#30340;&#24433;&#21709;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#31354;&#30333;&#65292;&#24182;&#19987;&#27880;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#20581;&#24247;&#30417;&#27979;&#24212;&#29992;&#65306;&#22522;&#20110;&#35821;&#38899;&#30340;COVID-19&#35786;&#26029;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#21311;&#21517;&#21270;&#26041;&#27861;&#21450;&#20854;&#23545;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;COVID-19&#35786;&#26029;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With advances seen in deep learning, voice-based applications are burgeoning, ranging from personal assistants, affective computing, to remote disease diagnostics. As the voice contains both linguistic and paralinguistic information (e.g., vocal pitch, intonation, speech rate, loudness), there is growing interest in voice anonymization to preserve speaker privacy and identity. Voice privacy challenges have emerged over the last few years and focus has been placed on removing speaker identity while keeping linguistic content intact. For affective computing and disease monitoring applications, however, the paralinguistic content may be more critical. Unfortunately, the effects that anonymization may have on these systems are still largely unknown. In this paper, we fill this gap and focus on one particular health monitoring application: speech-based COVID-19 diagnosis. We test two popular anonymization methods and their impact on five different state-of-the-art COVID-19 diagnostic system
&lt;/p&gt;</description></item></channel></rss>