<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#27785;&#28024;&#24335;&#36890;&#20449;&#20013;&#20943;&#23569;&#24102;&#23485;&#28040;&#32791;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2404.01713</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#27785;&#28024;&#24335;&#36890;&#20449;&#65306;&#36890;&#36807;6G&#25506;&#32034;&#24863;&#30693;&#20114;&#32852;&#32593;&#30340;&#19979;&#19968;&#20010;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#27785;&#28024;&#24335;&#36890;&#20449;&#20013;&#20943;&#23569;&#24102;&#23485;&#28040;&#32791;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#29289;&#32852;&#32593;(IoT)&#24050;&#32463;&#26159;&#19968;&#20010;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#27010;&#24565;&#65292;&#24403;&#25105;&#20204;&#36924;&#36817;2030&#24180;&#26102;&#65292;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#34987;&#31216;&#20026;&#24863;&#30693;&#20114;&#32852;&#32593;(IoS)&#27491;&#22312;&#20852;&#36215;&#12290;&#19982;&#20256;&#32479;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#19981;&#21516;&#65292;IoS&#26088;&#22312;&#25552;&#20379;&#22810;&#24863;&#23448;&#20307;&#39564;&#65292;&#35748;&#35782;&#21040;&#22312;&#25105;&#20204;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25105;&#20204;&#30340;&#24863;&#30693;&#36828;&#19981;&#27490;&#20110;&#35270;&#35273;&#21644;&#21548;&#35273;&#65307;&#23427;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#24863;&#35273;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25512;&#21160;&#27785;&#28024;&#24335;&#22810;&#24863;&#23448;&#23186;&#20307;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#28145;&#20837;&#25506;&#35752;&#23427;&#20204;&#30340;&#21151;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#36825;&#39033;&#25506;&#32034;&#21253;&#25324;&#20256;&#32479;&#27785;&#28024;&#24335;&#23186;&#20307;&#27969;&#19982;&#19968;&#20010;&#25552;&#20986;&#30340;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#35821;&#20041;&#20132;&#27969;&#30340;&#29992;&#20363;&#20043;&#38388;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#36825;&#39033;&#20998;&#26512;&#30340;&#37325;&#28857;&#26159;&#25152;&#25552;&#26041;&#26696;&#20013;&#24102;&#23485;&#28040;&#32791;&#20943;&#23569;&#20102;99.93%&#12290;&#36890;&#36807;&#36825;&#31181;&#27604;&#36739;&#65292;&#25105;&#20204;&#26088;&#22312;&#24378;&#35843;&#35813;&#23454;&#29992;&#24212;&#29992;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01713v1 Announce Type: cross  Abstract: Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that lever- ages semantic communication empowered by generative Artificial Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical appli
&lt;/p&gt;</description></item><item><title>&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;&#65292;&#36827;&#32780;&#22686;&#24378;&#20102;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01019</link><description>&lt;p&gt;
&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Source-Aware Training Enables Knowledge Attribution in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01019
&lt;/p&gt;
&lt;p&gt;
&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;&#65292;&#36827;&#32780;&#22686;&#24378;&#20102;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#21040;&#20102;&#22823;&#37327;&#30693;&#35782;&#65292;&#20294;&#24448;&#24448;&#23545;&#27492;&#31867;&#30693;&#35782;&#30340;&#26469;&#28304;&#27627;&#19981;&#22312;&#24847;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#22312;&#28304;&#24341;&#29992;&#38382;&#39064;&#65292;&#35201;&#27714;LLMs&#24341;&#29992;&#25903;&#25345;&#29983;&#25104;&#21709;&#24212;&#30340;&#39044;&#35757;&#32451;&#26469;&#28304;&#12290;&#20869;&#22312;&#28304;&#24341;&#29992;&#21487;&#20197;&#22686;&#24378;LLMs&#30340;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#20026;&#36171;&#20104;LLMs&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28304;&#24863;&#30693;&#35757;&#32451;&#8212;&#8212;&#19968;&#20010;&#21518;&#39044;&#35757;&#32451;&#37197;&#26041;&#65292;&#21253;&#25324;&#65288;i&#65289;&#35757;&#32451;LLMs&#23558;&#21807;&#19968;&#28304;&#25991;&#26723;&#26631;&#35782;&#31526;&#19982;&#27599;&#20010;&#25991;&#26723;&#20013;&#30340;&#30693;&#35782;&#20851;&#32852;&#36215;&#26469;&#65292;&#28982;&#21518;&#65288;ii&#65289;&#36827;&#34892;&#25351;&#31034;&#35843;&#25972;&#65292;&#25945;&#23548;LLMs&#22312;&#34987;&#25552;&#31034;&#26102;&#24341;&#29992;&#25903;&#25345;&#30340;&#39044;&#35757;&#32451;&#26469;&#28304;&#12290;&#28304;&#24863;&#30693;&#35757;&#32451;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;LLMs&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;/&#24494;&#35843;&#26694;&#26550;&#30340;&#24046;&#24322;&#26368;&#23567;&#12290;&#36890;&#36807;&#23545;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#37197;&#26041;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01019v1 Announce Type: cross  Abstract: Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can en
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20215;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#21644;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;</title><link>https://arxiv.org/abs/2403.15401</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#31995;&#32479;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Mental Health: A Systematic Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15401
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20215;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#21644;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#29616;&#20986;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#22312;&#25345;&#32493;&#35752;&#35770;&#20013;&#12290;&#36825;&#39033;&#31995;&#32479;&#24615;&#35780;&#20215;&#26088;&#22312;&#24635;&#32467;&#21644;&#34920;&#24449;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35843;&#26597;LLMs&#26368;&#26032;&#30740;&#31350;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#35752;&#35770;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#20197;&#21450;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26681;&#25454;PRISMA&#25351;&#21335;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;PubMed&#12289;DBLP&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#25968;&#25454;&#24211;&#21644;IEEE Xplore&#19978;&#21457;&#34920;&#30340;&#33521;&#25991;&#25991;&#31456;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2017&#24180;1&#26376;1&#26085;&#33267;2023&#24180;9&#26376;1&#26085;&#65292;&#37325;&#28857;&#20851;&#27880;&#24515;&#29702;&#20581;&#24247;&#21644;LLMs&#12290;&#35813;&#32508;&#36848;&#20998;&#26512;&#20102;32&#31687;&#25991;&#31456;&#65292;&#21253;&#25324;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#30340;&#65288;n=13&#65289;&#12289;&#24515;&#29702;&#20581;&#24247;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;n=10&#65289;&#20197;&#21450;&#20854;&#20182;&#24515;&#29702;&#20581;&#24247;&#24212;&#29992;&#65288;n=9&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15401v1 Announce Type: cross  Abstract: Large language models (LLMs) have received much attention and shown their potential in digital health, while their application in mental health is subject to ongoing debate. This systematic review aims to summarize and characterize the use of LLMs in mental health by investigating the strengths and limitations of the latest work in LLMs and discusses the challenges and opportunities for early screening, digital interventions, and other clinical applications in mental health. Following PRISMA guidelines, we examined English articles from PubMed, DBLP Computer Science Bibliography, and IEEE Xplore, published between 1 January 2017, and 1 September 2023, focusing on mental health and LLMs. The review analyzed 32 articles, including mental health analysis using social media datasets (n=13), mental health chatbots (n=10), and other mental health applications (n=9). Findings reveal LLMs' effectiveness in mental health issue detection and the
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#23569;&#36164;&#28304;&#35821;&#35328;&#24773;&#22659;&#19979;&#65292;&#20165;&#38656;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;&#21363;&#21487;&#25552;&#21462;&#20020;&#24202;&#20449;&#24687;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13369</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#36827;&#34892;&#23569;&#36164;&#28304;&#35821;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13369
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#23569;&#36164;&#28304;&#35821;&#35328;&#24773;&#22659;&#19979;&#65292;&#20165;&#38656;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;&#21363;&#21487;&#25552;&#21462;&#20020;&#24202;&#20449;&#24687;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#20214;&#20013;&#33258;&#21160;&#25552;&#21462;&#21307;&#30103;&#20449;&#24687;&#38754;&#20020;&#30528;&#20960;&#20010;&#25361;&#25112;&#65306;&#25152;&#38656;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#30340;&#39640;&#25104;&#26412;&#12289;&#27169;&#22411;&#39044;&#27979;&#30340;&#26377;&#38480;&#21487;&#35299;&#37322;&#24615;&#12289;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#38544;&#31169;&#27861;&#35268;&#12290;&#26368;&#36817;&#22312;&#39046;&#22495;&#36866;&#24212;&#21644;&#25552;&#31034;&#26041;&#27861;&#19978;&#30340;&#36827;&#23637;&#26174;&#31034;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#26497;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#25104;&#29087;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;&#23569;&#36164;&#28304;&#29615;&#22659;&#20013;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#24503;&#22269;&#21307;&#29983;&#20449;&#20214;&#19978;&#36827;&#34892;&#22810;&#31867;&#21035;&#27573;&#20998;&#31867;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#31867;&#21035;&#32423;&#35780;&#20272;&#65292;&#25903;&#25345; Shapley &#20540;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#23567;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#39046;&#22495;&#36866;&#24212;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20165;&#20165;&#25552;&#31034;&#20102; 20 &#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13369v1 Announce Type: new  Abstract: Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent advances in domain-adaptation and prompting methods showed promising results with minimal training data using lightweight masked language models, which are suited for well-established interpretability methods. We are first to present a systematic evaluation of these methods in a low-resource setting, by performing multi-class section classification on German doctor's letters. We conduct extensive class-wise evaluations supported by Shapley values, to validate the quality of our small training data set and to ensure the interpretability of model predictions. We demonstrate that a lightweight, domain-adapted pretrained model, prompted with just 20 shots, outperforms a traditional classificatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;RDTE&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26377;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.14798</link><description>&lt;p&gt;
&#21033;&#29992;&#38750;&#27491;&#24335;&#36923;&#36753;&#22686;&#24378;&#31995;&#32479;&#21270;&#20998;&#35299;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;RDTE&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26377;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20026;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20363;&#22914;&#22312;&#19981;&#20381;&#36182;&#33030;&#24369;&#30340;&#24418;&#24335;&#36923;&#36753;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#21644;&#35780;&#20272;&#30452;&#35266;&#30340;&#12289;&#31867;&#20284;&#35777;&#26126;&#30340;&#25991;&#26412;&#34164;&#28085;&#26641;&#12290;&#28982;&#32780;&#65292;&#27839;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#36827;&#23637;&#21463;&#21040;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#32570;&#20047;&#26126;&#30830;&#30340;&#30830;&#23450;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#28165;&#26224;&#21327;&#35758;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25968;&#25454;&#38598;RDTE (Recognizing Decompositional Textual Entailment) &#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#27604;&#20808;&#21069;&#30340;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#39640;&#24471;&#22810;&#65288;+9%&#65289;&#65292;&#34920;&#26126;RDTE&#22312;&#38271;&#26399;&#23384;&#22312;&#30340;&#20851;&#20110;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14798v1 Announce Type: cross  Abstract: Contemporary language models enable new opportunities for structured reasoning with text, such as the construction and evaluation of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment datasets, and evaluate its impact on LLM-based textual inference. We find that our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#26368;&#26032;&#33521;&#35821;&#26032;&#35789;&#36164;&#28304;&#65292;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26032;&#35789;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#26032;&#35789;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26426;&#22120;&#32763;&#35793;&#20013;&#27169;&#22411;&#24615;&#33021;&#20250;&#22240;&#24341;&#20837;&#26032;&#35789;&#32780;&#20960;&#20046;&#20943;&#21322;&#12290;</title><link>https://arxiv.org/abs/2402.12261</link><description>&lt;p&gt;
NEO-BENCH&#65306;&#20351;&#29992;&#26032;&#35789;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#26368;&#26032;&#33521;&#35821;&#26032;&#35789;&#36164;&#28304;&#65292;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26032;&#35789;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#26032;&#35789;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26426;&#22120;&#32763;&#35793;&#20013;&#27169;&#22411;&#24615;&#33021;&#20250;&#22240;&#24341;&#20837;&#26032;&#35789;&#32780;&#20960;&#20046;&#20943;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs)&#30340;&#34920;&#29616;&#20250;&#22240;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#19982;&#25512;&#29702;&#36807;&#31243;&#20013;&#30475;&#21040;&#30340;&#26032;&#25991;&#26412;&#20043;&#38388;&#30340;&#26102;&#38388;&#28418;&#31227;&#32780;&#36864;&#21270;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23548;&#33268;&#25968;&#25454;&#28418;&#31227;&#30340;&#35821;&#35328;&#21464;&#21270;&#20013;&#19968;&#20010;&#19981;&#22826;&#34987;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#32780;&#20986;&#29616;&#30340;&#26032;&#35789;&#24418;&#24335;&#8212;&#8212;&#26032;&#35789;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20960;&#31181;&#27969;&#34892;&#30340;&#25910;&#38598;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#26368;&#26032;&#33521;&#35821;&#26032;&#35789;&#36164;&#28304;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#21253;&#21547;&#26032;&#35789;&#30340;&#21477;&#23376;&#19982;&#23558;&#26032;&#35789;&#26367;&#25442;&#20026;&#29616;&#26377;&#26367;&#20195;&#35789;&#30340;&#20960;&#20046;&#30456;&#21516;&#30340;&#21477;&#23376;&#26469;&#20998;&#26512;&#26032;&#35789;&#23545;&#26102;&#38388;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#22312;&#21477;&#23376;&#20013;&#24341;&#20837;&#21333;&#20010;&#26032;&#35789;&#26102;&#65292;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#20960;&#20046;&#20943;&#21322;&#12290;&#21463;&#21040;&#36825;&#20123;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#23545;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#21644;&#27169;&#22411;&#22256;&#24785;&#24230;&#20013;&#26032;&#35789;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21518;&#26399;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#30340;&#27169;&#22411;&#20135;&#29983;&#36739;&#20302;&#30340;&#22256;&#24785;&#24230;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12261v1 Announce Type: new  Abstract: The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#21033;&#29992; GPT-4 &#27169;&#25311;&#23244;&#30097;&#20154;&#19982;&#35686;&#23448;&#20043;&#38388;&#30340;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#35299;&#20915;&#27450;&#39575;&#26816;&#27979;&#39046;&#22495;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#23558;&#20256;&#32479;&#30340;&#27450;&#39575;&#26816;&#27979;&#20219;&#21153;&#25299;&#23637;&#21040;&#27450;&#39575;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.11432</link><description>&lt;p&gt;
&#27450;&#39575;&#26816;&#27979;&#33021;&#22815;&#26356;&#28145;&#20837;&#21527;&#65311;&#29992;&#20110;&#27450;&#39575;&#25512;&#29702;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#21033;&#29992; GPT-4 &#27169;&#25311;&#23244;&#30097;&#20154;&#19982;&#35686;&#23448;&#20043;&#38388;&#30340;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#35299;&#20915;&#27450;&#39575;&#26816;&#27979;&#39046;&#22495;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#23558;&#20256;&#32479;&#30340;&#27450;&#39575;&#26816;&#27979;&#20219;&#21153;&#25299;&#23637;&#21040;&#27450;&#39575;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11432v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#30001;&#20110;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#27450;&#39575;&#26816;&#27979;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#30446;&#21069;&#65292;&#25968;&#25454;&#31232;&#32570;&#38459;&#30861;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#19968;&#26041;&#38754;&#65292;&#38599;&#20323;&#21442;&#19982;&#32773;&#27169;&#25311;&#27450;&#39575;&#22330;&#26223;&#25104;&#26412;&#39640;&#26114;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24456;&#38590;&#22312;&#20114;&#32852;&#32593;&#19978;&#25910;&#38598;&#21253;&#21547;&#27450;&#39575;&#34892;&#20026;&#30340;&#35270;&#39057;&#12290;&#20026;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992; GPT-4 &#27169;&#25311;&#20102;&#23244;&#30097;&#20154;&#21644;&#35686;&#23448;&#20043;&#38388;&#30340;&#35282;&#33394;&#25198;&#28436;&#12290;&#22312;&#23457;&#35759;&#36807;&#31243;&#20013;&#65292;&#23244;&#30097;&#20154;&#21521;&#35686;&#23448;&#25746;&#35854;&#65292;&#35797;&#22270;&#36867;&#36991;&#29359;&#32618;&#36131;&#20219;&#65292;&#32780;&#35686;&#23448;&#25581;&#38706;&#20102;&#20107;&#23454;&#24182;&#25910;&#38598;&#20102;&#35777;&#25454;&#12290;&#19982;&#20808;&#21069;&#30340;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#36825;&#19968;&#31574;&#30053;&#20943;&#23569;&#20102;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#65292;&#20026;&#22686;&#21152;&#25968;&#25454;&#38598;&#35268;&#27169;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#27450;&#39575;&#26816;&#27979;&#20219;&#21153;&#25193;&#23637;&#21040;&#27450;&#39575;&#25512;&#29702;&#65292;&#36827;&#19968;&#27493;&#20026;&#27450;&#39575;&#34892;&#20026;&#25552;&#20379;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11432v1 Announce Type: new  Abstract: Deception detection has attracted increasing attention due to its importance in many practical scenarios. Currently, data scarcity harms the development of this field. On the one hand, it is costly to hire participants to simulate deception scenarios. On the other hand, it is difficult to collect videos containing deceptive behaviors on the Internet. To address data scarcity, this paper proposes a new data collection pipeline. Specifically, we use GPT-4 to simulate a role-play between a suspect and a police officer. During interrogation, the suspect lies to the police officer to evade responsibility for the crime, while the police officer uncovers the truth and gathers evidence. Compared with previous datasets, this strategy reduces data collection costs, providing a promising way to increase the dataset size. Meanwhile, we extend the traditional deception detection task to deception reasoning, further providing evidence for deceptive pa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#33258;&#21160;&#32454;&#31890;&#24230;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#22823;&#37096;&#20998;&#24187;&#35273;&#23646;&#20110;&#23569;&#26377;&#30340;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2401.06855</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#24187;&#35273;&#26816;&#27979;&#19982;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Hallucination Detection and Editing for Language Models. (arXiv:2401.06855v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06855
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#33258;&#21160;&#32454;&#31890;&#24230;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#22823;&#37096;&#20998;&#24187;&#35273;&#23646;&#20110;&#23569;&#26377;&#30340;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#29983;&#25104;&#22810;&#26679;&#30340;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#38472;&#36848;&#65292;&#34987;&#24191;&#27867;&#31216;&#20026;&#24187;&#35273;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#31895;&#31890;&#24230;&#30340;&#33258;&#21160;&#24187;&#35273;&#26816;&#27979;&#25110;&#32534;&#36753;&#19978;&#65292;&#24573;&#35270;&#20102;&#32454;&#24494;&#30340;&#38169;&#35823;&#32423;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#8212;&#8212;&#33258;&#21160;&#32454;&#31890;&#24230;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20845;&#20010;&#23618;&#27425;&#20998;&#26126;&#30340;&#24187;&#35273;&#31867;&#22411;&#30340;&#32508;&#21512;&#20998;&#31867;&#27861;&#12290;&#20026;&#20102;&#20415;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#32454;&#31890;&#24230;&#20154;&#24037;&#21028;&#26029;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;ChatGPT&#21644;Llama 2-Chat&#30340;&#36755;&#20986;&#20013;&#26377;60%&#21644;75%&#30340;&#24187;&#35273;&#65292;&#20854;&#20013;&#22810;&#25968;&#24187;&#35273;&#23646;&#20110;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#31867;&#21035;&#12290;&#20316;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21021;&#22987;&#27493;&#39588;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;FAVA&#65292;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491;&#32454;&#31890;&#24230;&#24187;&#35273;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are prone to generate diverse factually incorrect statements, which are widely called hallucinations. Current approaches predominantly focus on coarse-grained automatic hallucination detection or editing, overlooking nuanced error levels. In this paper, we propose a novel task -- automatic fine-grained hallucination detection -- and present a comprehensive taxonomy encompassing six hierarchically defined types of hallucination. To facilitate evaluation, we introduce a new benchmark that includes fine-grained human judgments on two LM outputs across various domains. Our analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in 60% and 75% of their outputs, respectively, and a majority of these hallucinations fall into categories that have been underexplored. As an initial step to address this, we train FAVA, a retrieval-augmented LM by carefully designing synthetic data generations to detect and correct fine-grained hallucinations. On our bench
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25552;&#39640;&#23398;&#26415;&#20889;&#20316;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#21407;&#21017;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#12289;&#26377;&#25928;&#30340;&#25552;&#31034;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#35748;&#30693;&#21368;&#36733;&#21644;&#24819;&#35937;&#21050;&#28608;&#30340;AI&#36741;&#21161;&#20889;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.17143</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#25512;&#21160;&#23398;&#26415;&#20889;&#20316;&#65306;&#26694;&#26550;&#12289;&#25216;&#26415;&#21644;&#27880;&#24847;&#20107;&#39033;
&lt;/p&gt;
&lt;p&gt;
Supercharging academic writing with generative AI: framework, techniques, and caveats. (arXiv:2310.17143v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25552;&#39640;&#23398;&#26415;&#20889;&#20316;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#21407;&#21017;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#12289;&#26377;&#25928;&#30340;&#25552;&#31034;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#35748;&#30693;&#21368;&#36733;&#21644;&#24819;&#35937;&#21050;&#28608;&#30340;AI&#36741;&#21161;&#20889;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20889;&#20316;&#26159;&#30740;&#31350;&#39033;&#30446;&#20013;&#19981;&#21487;&#25110;&#32570;&#20294;&#36153;&#26102;&#36153;&#21147;&#30340;&#37096;&#20998;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#39640;&#23398;&#26415;&#20889;&#20316;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#21407;&#21017;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;AI&#22312;&#20889;&#20316;&#20013;&#30340;&#29702;&#35770;&#22522;&#30784;&#65288;&#20026;&#20160;&#20040;&#65289;&#12289;&#36807;&#31243;&#65288;&#22914;&#20309;&#65289;&#21644;&#24615;&#36136;&#65288;&#20160;&#20040;&#65289;&#12290;&#35813;&#26694;&#26550;&#25351;&#20986;&#20102;&#30701;&#26399;&#21644;&#38271;&#26399;&#21442;&#19982;AI&#20889;&#20316;&#30340;&#21407;&#22240;&#21450;&#20854;&#22522;&#26412;&#26426;&#21046;&#65288;&#22914;&#35748;&#30693;&#21368;&#36733;&#21644;&#24819;&#35937;&#21050;&#28608;&#65289;&#12290;&#23427;&#25581;&#31034;&#20102;AI&#22312;&#25972;&#20010;&#20889;&#20316;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#20889;&#20316;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#21644;&#20889;&#20316;&#36741;&#21161;&#31867;&#22411;&#21644;&#32423;&#21035;&#30340;&#27169;&#22411;&#34920;&#31034;&#20102;AI&#22312;&#20889;&#20316;&#20013;&#30340;&#24110;&#21161;&#26041;&#24335;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#20889;&#20316;&#24120;&#35268;&#20013;&#25972;&#21512;AI&#30340;&#26377;&#25928;&#25552;&#31034;&#25216;&#26415;&#65288;&#22823;&#32434;&#12289;&#36215;&#33609;&#21644;&#32534;&#36753;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Academic writing is an indispensable yet laborious part of the research enterprise. This Perspective maps out principles and methods for using generative artificial intelligence (AI), specifically large language models (LLMs), to elevate the quality and efficiency of academic writing. We introduce a human-AI collaborative framework that delineates the rationale (why), process (how), and nature (what) of AI engagement in writing. The framework pinpoints both short-term and long-term reasons for engagement and their underlying mechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals the role of AI throughout the writing process, conceptualized through a two-stage model for human-AI collaborative writing, and the nature of AI assistance in writing, represented through a model of writing-assistance types and levels. Building on this framework, we describe effective prompting techniques for incorporating AI into the writing routine (outlining, drafting, and editing) a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01929</link><description>&lt;p&gt;
&#31359;&#36234;&#25991;&#21270;&#40511;&#27807;&#65306;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#65292;&#20363;&#22914;DALL-E&#21644;StableDiffusion&#65292;&#22312;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#38646;&#23556;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#20316;&#20026;&#25991;&#21270;&#30340;&#23186;&#20171;&#65292;&#35821;&#35328;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32780;&#22609;&#36896;&#20102;&#23427;&#20204;&#30340;&#25991;&#21270;&#26426;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25551;&#36848;&#25991;&#21270;&#32500;&#24230;&#65292;&#25991;&#21270;&#39046;&#22495;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#19977;&#20010;&#23618;&#27425;&#26469;&#25506;&#32034;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#25216;&#26415;&#65292;&#21253;&#25324;&#20351;&#29992;CLIP&#31354;&#38388;&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36827;&#34892;&#22806;&#22312;&#35780;&#20272;&#20197;&#21450;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;TTI&#25991;&#21270;&#24863;&#30693;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CulText2I&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#30340;TTI&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21313;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;
&lt;/p&gt;
&lt;p&gt;
Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have recently gained prominence for their remarkable zero-shot capabilities in generating images guided by textual prompts. Language, as a conduit of culture, plays a pivotal role in these models' multilingual capabilities, which in turn shape their cultural agency. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. We propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model, and human assessments, to discern TTI cultural perceptions. To facilitate our research, we introduce the CulText2I dataset, derived from four diverse TTI models and spanning ten languages. Our experiments reveal insights into these models' cultural awareness, cultural distinctions, and the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#24515;&#29702;&#23398;&#20013;&#30340;&#24773;&#24863;&#35780;&#20272;&#29702;&#35770;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;EmotionBench&#35780;&#20272;LLMs&#30340;&#20849;&#24773;&#33021;&#21147;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#21644;&#23545;&#20116;&#20010;LLMs&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#19981;&#19968;&#33268;&#20043;&#22788;&#65292;LLMs&#36890;&#24120;&#33021;&#22312;&#26576;&#20123;&#24773;&#22659;&#19979;&#36866;&#24403;&#22320;&#22238;&#24212;&#65292;&#20294;&#19982;&#24773;&#24863;&#23545;&#40784;&#26041;&#38754;&#36824;&#23384;&#22312;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.03656</link><description>&lt;p&gt;
&#24863;&#35273;&#40635;&#26408;&#36824;&#26159;&#26377;&#20849;&#24773;&#33021;&#21147;&#65311;&#21033;&#29992;EmotionBench&#35780;&#20272;LLMs&#30340;&#24773;&#24863;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench. (arXiv:2308.03656v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03656
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#24515;&#29702;&#23398;&#20013;&#30340;&#24773;&#24863;&#35780;&#20272;&#29702;&#35770;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;EmotionBench&#35780;&#20272;LLMs&#30340;&#20849;&#24773;&#33021;&#21147;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#21644;&#23545;&#20116;&#20010;LLMs&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#19981;&#19968;&#33268;&#20043;&#22788;&#65292;LLMs&#36890;&#24120;&#33021;&#22312;&#26576;&#20123;&#24773;&#22659;&#19979;&#36866;&#24403;&#22320;&#22238;&#24212;&#65292;&#20294;&#19982;&#24773;&#24863;&#23545;&#40784;&#26041;&#38754;&#36824;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#35805;&#35821;&#20013;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25311;&#20154;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#21033;&#29992;&#24515;&#29702;&#23398;&#20013;&#30340;&#24773;&#24863;&#35780;&#20272;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#35780;&#20272;LLMs&#30340;&#20849;&#24773;&#33021;&#21147;&#65292;&#21363;&#23427;&#20204;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#24863;&#21463;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20180;&#32454;&#32780;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;400&#31181;&#24773;&#22659;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#24773;&#22659;&#24050;&#34987;&#35777;&#26126;&#23545;&#25105;&#20204;&#30740;&#31350;&#30340;&#20843;&#31181;&#24773;&#24863;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#36825;&#20123;&#24773;&#22659;&#20998;&#20026;36&#20010;&#22240;&#32032;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#28041;&#21450;&#20840;&#29699;1200&#22810;&#21517;&#34987;&#35797;&#30340;&#20154;&#31867;&#35780;&#20272;&#12290;&#20197;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#20026;&#21442;&#32771;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;LLMs&#65292;&#28085;&#30422;&#20102;&#21830;&#19994;&#21644;&#24320;&#28304;&#27169;&#22411;&#65292;&#21253;&#25324;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#26368;&#26032;&#30340;&#36845;&#20195;&#29256;&#26412;&#65288;&#22914;GPT-4&#21644;LLaMA-2&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#19981;&#19968;&#33268;&#20043;&#22788;&#65292;LLMs&#36890;&#24120;&#33021;&#22312;&#26576;&#20123;&#24773;&#22659;&#19979;&#36866;&#24403;&#22320;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19982;&#24773;&#24863;&#23545;&#40784;&#26041;&#38754;&#36824;&#23384;&#22312;&#19968;&#23450;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has become increasingly important in contemporary discourse. Utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of LLMs, i.e., how their feelings change when presented with specific situations. After a careful and comprehensive survey, we collect a dataset containing over 400 situations that have proven effective in eliciting the eight emotions central to our study. Categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide. With the human evaluation results as references, our evaluation includes five LLMs, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as GPT-4 and LLaMA-2. We find that, despite several misalignments, LLMs can generally respond appropriately to certain situations. Nevertheless, they fall short in alignment with the e
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#12289;&#24341;&#23548;&#29983;&#25104;&#21644;&#21322;&#21442;&#25968;&#23494;&#38598;&#26816;&#32034;&#65292;&#21160;&#24577;&#29983;&#25104;&#22522;&#20110;&#20107;&#23454;&#24211;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35777;&#26126;&#26641;&#65292;&#23454;&#29616;&#31185;&#23398;&#25512;&#29702;&#65292;&#24182;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.07662</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#19987;&#23478;&#31995;&#32479;&#20013;&#22522;&#20110;&#20107;&#23454;&#24211;&#30340;&#36923;&#36753;&#25512;&#29702;&#30340;&#21160;&#24577;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dynamic Generation of Grounded Logical Explanations in a Neuro-Symbolic Expert System. (arXiv:2209.07662v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07662
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#12289;&#24341;&#23548;&#29983;&#25104;&#21644;&#21322;&#21442;&#25968;&#23494;&#38598;&#26816;&#32034;&#65292;&#21160;&#24577;&#29983;&#25104;&#22522;&#20110;&#20107;&#23454;&#24211;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35777;&#26126;&#26641;&#65292;&#23454;&#29616;&#31185;&#23398;&#25512;&#29702;&#65292;&#24182;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#22522;&#20110;&#20107;&#23454;&#24211;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35777;&#26126;&#26641;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#21457;&#20102;&#32463;&#20856;&#30340;&#22522;&#20110; Prolog &#30340;&#25512;&#29702;&#24341;&#25806;&#65292;&#20854;&#20013;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#12289;&#24341;&#23548;&#29983;&#25104;&#21644;&#21322;&#21442;&#25968;&#23494;&#38598;&#26816;&#32034;&#26469;&#26367;&#25442;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479; NELLIE &#26469;&#28436;&#31034;&#36825;&#31181;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#21160;&#24577;&#22320;&#23454;&#20363;&#21270;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#35268;&#21017;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#30340;&#34164;&#21547;&#65288;&#21435;&#65289;&#32452;&#21512;&#36827;&#34892;&#25429;&#25417;&#21644;&#35780;&#20998;&#12290;&#36825;&#23548;&#33268;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#22312;&#31185;&#23398;&#25512;&#29702;&#39046;&#22495;&#23637;&#31034;&#20102;&#22914;&#20309;&#36923;&#36753;&#22320;&#20174;&#32463;&#36807;&#20154;&#24037;&#39564;&#35777;&#30340;&#20107;&#23454;&#30340;&#32452;&#21512;&#20013;&#25512;&#23548;&#20986;&#31572;&#26696;&#30340;&#25512;&#29702;&#30165;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach for systematic reasoning that produces human interpretable proof trees grounded in a factbase. Our approach evokes classic Prolog-based inference engines, where we replace handcrafted rules by combining neural language modeling, guided generation, and semiparametric dense retrieval. We demonstrate this approach through a novel system, NELLIE, which dynamically instantiates interpretable inference rules that capture and score entailment (de)compositions over natural language statements. This leads to strong performance, as shown in the scientific reasoning domain, while also producing reasoning traces showing how answers derive logically from the composition of human-verified facts.
&lt;/p&gt;</description></item></channel></rss>