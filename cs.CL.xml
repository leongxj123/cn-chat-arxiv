<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20027;&#35201;&#26469;&#33258;&#29305;&#23450;&#39046;&#22495;&#25945;&#32946;&#36164;&#28304;&#20013;&#65292;&#20197;&#25991;&#26412;&#20026;&#20027;&#65292;&#32570;&#20047;&#24320;&#25918;&#39046;&#22495;&#21644;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01512</link><description>&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#30340;&#24178;&#25200;&#39033;&#29983;&#25104;&#65306;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Distractor Generation for Multiple-Choice Questions: A Survey of Methods, Datasets, and Evaluation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20027;&#35201;&#26469;&#33258;&#29305;&#23450;&#39046;&#22495;&#25945;&#32946;&#36164;&#28304;&#20013;&#65292;&#20197;&#25991;&#26412;&#20026;&#20027;&#65292;&#32570;&#20047;&#24320;&#25918;&#39046;&#22495;&#21644;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24178;&#25200;&#39033;&#22312;&#23398;&#20064;&#35780;&#20272;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#38024;&#23545;&#33521;&#35821;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#24178;&#25200;&#39033;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#20102;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#35821;&#22659;&#30340;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#23545;&#24178;&#25200;&#39033;&#29983;&#25104;&#20219;&#21153;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#29305;&#28857;&#65292;&#20998;&#26512;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#36807;&#19968;&#21322;&#30340;&#25968;&#25454;&#38598;&#26469;&#33258;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;&#31185;&#23398;&#21644;&#33521;&#35821;&#65289;&#20013;&#30340;&#25945;&#32946;&#26469;&#28304;&#65292;&#24182;&#19988;&#20027;&#35201;&#26159;&#20197;&#25991;&#26412;&#20026;&#20027;&#65292;&#32570;&#20047;&#24320;&#25918;&#39046;&#22495;&#21644;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distractors are important in learning evaluation. This paper surveys distractor generation tasks using English multiple-choice question datasets for textual and multimodal contexts. In particular, this paper presents a thorough literature review of the recent studies on distractor generation tasks, discusses multiple choice components and their characteristics, analyzes the related datasets, and summarizes the evaluation metrics of distractor generation. Our investigation reveals that more than half of datasets are human-generated from educational sources in specific domains such as Science and English, which are largely text-based, with a lack of open domain and multimodal datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28608;&#27963;&#26631;&#24535;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21387;&#32553;LLM&#30340;&#28608;&#27963;&#29366;&#24577;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#24863;&#30693;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;LLM&#22312;&#30701;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#22810;&#26679;&#21270;&#35757;&#32451;&#26377;&#25928;&#22320;&#25903;&#25345;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2401.03462</link><description>&lt;p&gt;
&#20174;4K&#21040;400K&#30340;&#39134;&#36291;&#65306;&#21033;&#29992;&#28608;&#27963;&#26631;&#24535;&#25193;&#23637;LLM&#30340;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.03462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28608;&#27963;&#26631;&#24535;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21387;&#32553;LLM&#30340;&#28608;&#27963;&#29366;&#24577;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#24863;&#30693;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;LLM&#22312;&#30701;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#22810;&#26679;&#21270;&#35757;&#32451;&#26377;&#25928;&#22320;&#25903;&#25345;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#23545;&#20110;LLM&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#12290;&#23613;&#31649;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#25193;&#23637;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20294;&#36825;&#20250;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#26174;&#33879;&#25104;&#26412;&#65292;&#24182;&#23545;LLM&#30340;&#21407;&#22987;&#33021;&#21147;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28608;&#27963;&#26631;&#24535;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;LLM&#30340;&#21407;&#22987;&#28608;&#27963;&#21387;&#32553;&#25104;&#32039;&#20945;&#30340;&#24418;&#24335;&#65292;&#20351;LLM&#33021;&#22815;&#20197;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#24863;&#30693;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#12290;&#28608;&#27963;&#26631;&#24535;&#34987;&#24341;&#20837;&#20026;&#25554;&#20214;&#27169;&#22359;&#65292;&#23436;&#20840;&#20445;&#30041;&#20102;LLM&#22312;&#30701;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#23427;&#19982;&#28369;&#21160;&#31383;&#21475;&#19968;&#36215;&#23454;&#26102;&#22788;&#29702;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#23454;&#29616;&#20102;&#31454;&#20105;&#21147;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;&#28608;&#27963;&#26631;&#24535;&#26159;&#36890;&#36807;&#22810;&#26679;&#21270;&#21387;&#32553;&#27604;&#30340;&#30701;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#24471;&#30410;&#20110;&#36825;&#31181;&#22788;&#29702;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#25903;&#25345;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#23454;&#29616;&#23567;&#35268;&#27169;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of long contexts poses a big challenge for LLMs due to their limited context window size. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small trai
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#21451;&#22909;&#30340;&#36830;&#32493;&#27169;&#22411;&#32534;&#36753;&#19982;&#25209;&#37327;&#25903;&#25345;&#30340;&#26041;&#27861;COMEBA-HK&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05330</link><description>&lt;p&gt;
&#36830;&#32493;&#27169;&#22411;&#32534;&#36753;&#19982;&#25209;&#37327;&#25903;&#25345;&#30340;HooK&#23618;
&lt;/p&gt;
&lt;p&gt;
Consecutive Model Editing with Batch alongside HooK Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05330
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#21451;&#22909;&#30340;&#36830;&#32493;&#27169;&#22411;&#32534;&#36753;&#19982;&#25209;&#37327;&#25903;&#25345;&#30340;&#26041;&#27861;COMEBA-HK&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20856;&#22411;&#30340;&#37325;&#26032;&#35757;&#32451;&#33539;&#24335;&#32791;&#26102;&#19988;&#28040;&#32791;&#36164;&#28304;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#36716;&#21521;&#27169;&#22411;&#32534;&#36753;&#65292;&#20197;&#23547;&#25214;&#19968;&#31181;&#26377;&#25928;&#30340;&#12289;&#36830;&#32493;&#30340;&#12289;&#24182;&#25903;&#25345;&#25209;&#37327;&#26041;&#24335;&#30452;&#25509;&#32534;&#36753;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23384;&#22312;&#25152;&#26377;&#36825;&#20123;&#23454;&#29992;&#26399;&#26395;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#21364;&#26410;&#33021;&#23454;&#29616;&#25152;&#26377;&#36825;&#20123;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#31181;&#25903;&#25345;&#36830;&#32493;&#24615;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#30340;&#20869;&#23384;&#38656;&#27714;&#24448;&#24448;&#26159;&#31105;&#27490;&#24615;&#30340;&#65292;&#32463;&#24120;&#38656;&#35201;&#38543;&#30528;&#26102;&#38388;&#30340;&#22686;&#38271;&#36880;&#27493;&#22686;&#21152;&#22806;&#37096;&#20869;&#23384;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMEBA-HK&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#26159;&#36830;&#32493;&#30340;&#21448;&#25903;&#25345;&#25209;&#37327;&#12290;COMEBA-HK&#23545;&#20110;&#23384;&#20648;&#20960;&#20010;&#20855;&#26377;&#26356;&#26032;&#26435;&#37325;&#30340;hook&#23618;&#20165;&#38656;&#23569;&#37327;&#20869;&#23384;&#65292;&#26159;&#20869;&#23384;&#21451;&#22909;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#36718;&#21644;&#36830;&#32493;&#25209;&#37327;&#32534;&#36753;&#22330;&#26223;&#19979;&#20248;&#20110;&#20854;&#20182;&#25903;&#25345;&#25209;&#37327;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05330v1 Announce Type: new  Abstract: As the typical retraining paradigm is unacceptably time- and resource-consuming, researchers are turning to model editing in order to seek an effective, consecutive, and batch-supportive way to edit the model behavior directly. Despite all these practical expectations, existing model editing methods fail to realize all of them. Furthermore, the memory demands for such succession-supportive model editing approaches tend to be prohibitive, frequently necessitating an external memory that grows incrementally over time. To cope with these challenges, we propose COMEBA-HK, a model editing method that is both consecutive and batch-supportive. COMEBA-HK is memory-friendly as it only needs a small amount of it to store several hook layers with updated weights. Experimental results demonstrate the superiority of our method over other batch-supportive model editing methods under both single-round and consecutive batch editing scenarios. Extensive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23569;&#26679;&#26412;&#25512;&#21160;&#25512;&#29702;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;LLMs&#29992;&#20110;&#24320;&#25918;&#24335;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#65292;&#36890;&#36807;&#20462;&#25913;MedQA-USMLE&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#22870;&#21169;&#35757;&#32451;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;&#22330;&#26223;&#20013;&#27491;&#30830;&#21709;&#24212;&#20020;&#24202;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04890</link><description>&lt;p&gt;
&#22522;&#20110;&#23569;&#26679;&#26412;&#25512;&#21160;&#25512;&#29702;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;LLMs&#29992;&#20110;&#24320;&#25918;&#24335;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23569;&#26679;&#26412;&#25512;&#21160;&#25512;&#29702;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;LLMs&#29992;&#20110;&#24320;&#25918;&#24335;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#65292;&#36890;&#36807;&#20462;&#25913;MedQA-USMLE&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#22870;&#21169;&#35757;&#32451;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;&#22330;&#26223;&#20013;&#27491;&#30830;&#21709;&#24212;&#20020;&#24202;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#36716;&#21464;&#21307;&#30103;&#20445;&#20581;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#35832;&#22914;&#20020;&#24202;&#25991;&#26723;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20915;&#31574;&#25903;&#25345;&#31561;&#20219;&#21153;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#24050;&#32463;&#25104;&#20026;&#22312;&#21307;&#30103;&#22330;&#26223;&#20013;&#20351;&#29992;LLMs&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20363;&#22914;&#24739;&#32773;&#20020;&#24202;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MedQA-USMLE&#25968;&#25454;&#38598;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#30446;&#30340;&#26159;&#27169;&#25311;&#30495;&#23454;&#20020;&#24202;&#22330;&#26223;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#20027;&#35266;&#21709;&#24212;&#29983;&#25104;&#30340;Chain of Thought&#65288;CoT&#65289;&#25512;&#29702;&#65292;&#29992;&#20110;&#20462;&#25913;&#21518;&#30340;MedQA-USMLE&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;LM&#39537;&#21160;&#21069;&#21521;&#25512;&#29702;&#26469;&#33719;&#24471;&#27491;&#30830;&#30340;&#21307;&#23398;&#38382;&#39064;&#31572;&#26696;&#12290;&#32771;&#34385;&#21040;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#21709;&#24212;&#39564;&#35777;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#22870;&#21169;&#35757;&#32451;&#26426;&#21046;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#36824;&#20026;&#29305;&#23450;&#30340;&#20020;&#24202;&#38382;&#39064;&#22238;&#24212;&#25552;&#20379;&#20102;&#36866;&#24403;&#30340;&#39564;&#35777;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04890v1 Announce Type: new  Abstract: Large Language models (LLMs) have demonstrated significant potential in transforming healthcare by automating tasks such as clinical documentation, information retrieval, and decision support. In this aspect, carefully engineered prompts have emerged as a powerful tool for using LLMs for medical scenarios, e.g., patient clinical scenarios. In this paper, we propose a modified version of the MedQA-USMLE dataset, which is subjective, to mimic real-life clinical scenarios. We explore the Chain of Thought (CoT) reasoning based on subjective response generation for the modified MedQA-USMLE dataset with appropriate LM-driven forward reasoning for correct responses to the medical questions. Keeping in mind the importance of response verification in the medical setting, we utilize a reward training mechanism whereby the language model also provides an appropriate verified response for a particular response to a clinical question. In this regard,
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23618;&#32423;&#23384;&#22312;&#36739;&#39640;&#30456;&#20284;&#24615;&#65292;&#26377;&#20123;&#23618;&#23545;&#32593;&#32476;&#21151;&#33021;&#20960;&#20046;&#26080;&#24433;&#21709;&#12290;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#21306;&#22359;&#24433;&#21709;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#23618;&#21024;&#38500;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03853</link><description>&lt;p&gt;
ShortGPT: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23618;&#32423;&#27604;&#24744;&#24819;&#35937;&#30340;&#26356;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
ShortGPT: Layers in Large Language Models are More Redundant Than You Expect
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03853
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23618;&#32423;&#23384;&#22312;&#36739;&#39640;&#30456;&#20284;&#24615;&#65292;&#26377;&#20123;&#23618;&#23545;&#32593;&#32476;&#21151;&#33021;&#20960;&#20046;&#26080;&#24433;&#21709;&#12290;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#21306;&#22359;&#24433;&#21709;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#23618;&#21024;&#38500;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24615;&#33021;&#19978;&#19981;&#26029;&#21462;&#24471;&#36827;&#23637;&#65292;&#20854;&#35268;&#27169;&#26174;&#33879;&#22686;&#21152;&#65292;&#24403;&#21069;&#30340;LLMs&#21253;&#21547;&#25968;&#21313;&#20159;&#29978;&#33267;&#25968;&#19975;&#20159;&#20010;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;LLMs&#30340;&#23618;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#19968;&#20123;&#23618;&#22312;&#32593;&#32476;&#21151;&#33021;&#20013;&#36215;&#21040;&#20102;&#21487;&#24573;&#30053;&#30340;&#20316;&#29992;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#31216;&#20026;&#21306;&#22359;&#24433;&#21709;&#65288;BI&#65289;&#30340;&#24230;&#37327;&#34913;&#37327;LLMs&#20013;&#27599;&#20010;&#23618;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#21098;&#26041;&#27861;&#65306;&#23618;&#21024;&#38500;&#65292;&#21363;&#26681;&#25454;&#23427;&#20204;&#30340;BI&#24471;&#20998;&#30452;&#25509;&#21024;&#38500;LLMs&#20013;&#30340;&#20887;&#20313;&#23618;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;ShortGPT&#22312;&#27169;&#22411;&#20462;&#21098;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20197;&#24448;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;ShortGPT&#19982;&#37327;&#21270;&#31561;&#26041;&#27861;&#27491;&#20132;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#21442;&#25968;&#21644;&#35745;&#31639;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#23618;&#21024;&#38500;&#21363;&#21487;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#30340;&#33021;&#21147;&#65292;&#19982;&#20256;&#32479;&#30340;&#31934;&#30830;&#20462;&#21098;&#26041;&#27861;&#25130;&#28982;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03853v1 Announce Type: new  Abstract: As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as oppo
&lt;/p&gt;</description></item><item><title>LLM-based KGQA methods struggle with hallucination on commonsense reasoning questions, hindering their applicability in real-world applications.</title><link>https://arxiv.org/abs/2403.01390</link><description>&lt;p&gt;
&#27491;&#24403;&#19988;&#20805;&#20998;&#65306;&#21487;&#39564;&#35777;&#30340;&#24120;&#35782;&#30693;&#35782;&#22270;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01390
&lt;/p&gt;
&lt;p&gt;
LLM-based KGQA methods struggle with hallucination on commonsense reasoning questions, hindering their applicability in real-world applications.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#38382;&#39064;&#22238;&#31572;&#65288;KGQA&#65289;&#26041;&#27861;&#26088;&#22312;&#21033;&#29992;&#30693;&#35782;&#22270;&#20013;&#23384;&#20648;&#30340;&#20851;&#31995;&#20449;&#24687;&#26469;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#21450;&#20854;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21033;&#29992;&#23427;&#20204;&#36827;&#34892;KGQA&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#19987;&#27880;&#20110;&#22238;&#31572;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;Silvio Berlusconi&#30340;&#31532;&#19968;&#20219;&#22971;&#23376;&#20986;&#29983;&#22312;&#21738;&#24231;&#22478;&#24066;&#65311;&#8221;&#65292;&#32780;&#24573;&#30053;&#20102;&#28041;&#21450;&#24120;&#35782;&#25512;&#29702;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#29616;&#23454;&#19990;&#30028;&#29992;&#25143;&#21487;&#33021;&#26356;&#32463;&#24120;&#25552;&#20986;&#30340;&#65292;&#20363;&#22914;&#8220;&#25105;&#38656;&#35201;&#21333;&#29420;&#30340;&#31614;&#35777;&#25165;&#33021;&#30475;&#21040;&#23041;&#20262;&#22810;&#22827;&#30340;&#32500;&#32435;&#26031;&#24182;&#21442;&#21152;&#20170;&#24180;&#22799;&#22825;&#30340;&#22885;&#36816;&#20250;&#21527;&#65311;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#22522;&#20110;LLM&#30340;KGQA&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#26102;&#38590;&#20197;&#20135;&#29983;&#30495;&#23454;&#30340;&#31572;&#26696;&#65292;&#23588;&#20854;&#26159;&#23545;&#38024;&#23545;&#38271;&#23614;&#23454;&#20307;&#30340;&#26597;&#35810;&#65288;&#20363;&#22914;&#38750;&#20027;&#27969;&#21644;&#26368;&#36817;&#30340;&#23454;&#20307;&#65289;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#21487;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01390v1 Announce Type: new  Abstract: Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., "In which city was Silvio Berlusconi's first wife born?", leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., "Do I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?" unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especial
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.19085</link><description>&lt;p&gt;
&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65306;&#26397;&#30528;&#21487;&#25511;&#22810;&#30446;&#26631;&#23545;&#40784;&#26041;&#21521;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19085
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23545;&#40784;&#24037;&#20316;&#26088;&#22312;&#36861;&#27714;&#27169;&#22411;&#21709;&#24212;&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#23545;&#40784;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#31526;&#21512;&#21508;&#31181;&#20559;&#22909;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19085v1 Announce Type: new  Abstract: Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the "alignment tax" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#32531;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14279</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#20943;&#32531;&#35821;&#35328;&#24046;&#24322;&#65292;&#23454;&#29616;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14279
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#32531;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25913;&#21892;&#22810;&#35821;&#35328;&#29702;&#35299;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#65292;&#20381;&#36182;&#22797;&#26434;&#30340;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#20551;&#35774;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#21463;&#21040;&#36825;&#20123;&#35821;&#35328;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#65288;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;&#38899;&#32032;&#20316;&#20026;&#36755;&#20837;&#26631;&#35760;&#36755;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#26159;&#23376;&#35789;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;&#23450;&#37327;&#35777;&#25454;&#23637;&#31034;&#20102;&#38899;&#32032;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#36827;&#19968;&#27493;&#24471;&#21040;&#20102;&#23545;&#36328;&#35821;&#35328;&#24615;&#33021;&#24046;&#36317;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14279v1 Announce Type: cross  Abstract: Approaches to improving multilingual language understanding often require multiple languages during the training phase, rely on complicated training techniques, and -- importantly -- struggle with significant performance gaps between high-resource and low-resource languages. We hypothesize that the performance gaps between languages are affected by linguistic gaps between those languages and provide a novel solution for robust multilingual language modeling by employing phonemic representations (specifically, using phonemes as input tokens to LMs rather than subwords). We present quantitative evidence from three cross-lingual tasks that demonstrate the effectiveness of phonemic representation, which is further justified by a theoretical analysis of the cross-lingual performance gap.
&lt;/p&gt;</description></item><item><title>&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.11192</link><description>&lt;p&gt;
&#22914;&#26524;&#20320;&#35762;&#25105;&#30340;&#35821;&#35328;&#65292;&#25105;&#20250;&#26356;&#22909;&#22320;&#23398;&#20064;&#65306;&#20351;&#29992;&#39118;&#26684;&#23545;&#40784;&#21709;&#24212;&#35843;&#25972;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11192
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23567;&#25968;&#25454;&#38598;&#20026;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#20010;&#26222;&#36941;&#36935;&#21040;&#30340;&#20294;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#22312;&#26377;&#38480;&#30340;&#31034;&#20363;&#19978;&#36807;&#22810;&#25311;&#21512;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20445;&#30041;&#21407;&#22987;&#25216;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#23558;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;LLM&#22266;&#26377;&#39118;&#26684;&#21305;&#37197;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26368;&#23567;&#31243;&#24230;&#22320;&#20462;&#25913;LLM&#30340;&#29616;&#26377;&#21709;&#24212;&#20197;&#26356;&#27491;&#38169;&#35823;&#65292;&#20351;&#29992;&#36825;&#20123;&#35843;&#25972;&#21518;&#30340;&#21709;&#24212;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#19982;&#27169;&#22411;&#22266;&#26377;&#21709;&#24212;&#39118;&#26684;&#19968;&#33268;&#30340;&#31934;&#30830;&#26356;&#27491;&#65292;&#32500;&#25252;&#27169;&#22411;&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#22810;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;LLM&#30340;&#29305;&#23450;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#20851;&#38190;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11192v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37319;&#26679;&#21644;&#25237;&#31080;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#27491;&#20132;&#12290;</title><link>https://arxiv.org/abs/2402.05120</link><description>&lt;p&gt;
&#26356;&#22810;&#30340;&#20195;&#29702;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
More Agents Is All You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05120
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37319;&#26679;&#21644;&#25237;&#31080;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#27491;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#19968;&#31181;&#37319;&#26679;&#21644;&#25237;&#31080;&#30340;&#26041;&#27861;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#30340;&#24615;&#33021;&#19982;&#23454;&#20363;&#21270;&#30340;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#24050;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#26159;&#27491;&#20132;&#30340;&#65292;&#32780;&#22686;&#24378;&#30340;&#31243;&#24230;&#19982;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#30456;&#20851;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#33021;&#22815;&#20419;&#36827;&#20854;&#21457;&#29983;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;&#20197;&#19979;&#32593;&#22336;: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}
&lt;/p&gt;
&lt;p&gt;
We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.
&lt;/p&gt;</description></item><item><title>DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02392</link><description>&lt;p&gt;
DeLLMa:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#20915;&#31574;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02392
&lt;/p&gt;
&lt;p&gt;
DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#12289;&#24037;&#31243;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#24448;&#24448;&#38754;&#20020;&#20915;&#31574;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20915;&#31574;&#38382;&#39064;&#19978;&#30452;&#25509;&#20351;&#29992;LLMs&#24448;&#24448;&#25928;&#26524;&#36739;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeLLMa&#65288;Decision-making Large Language Model assistant&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#31934;&#24230;&#12290;DeLLMa&#21253;&#25324;&#19968;&#20010;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20102;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;&#12289;&#21487;&#23457;&#35745;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#20892;&#19994;&#21644;&#37329;&#34701;&#25968;&#25454;&#30340;&#20915;&#31574;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DeLLMa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#20915;&#31574;&#24615;&#33021;&#65292;&#20934;&#30830;&#24615;&#21487;&#25552;&#39640;&#39640;&#36798;40%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;GPT-4&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#31163;&#26631;&#32763;&#35793;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2401.06468</link><description>&lt;p&gt;
&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models for Document-Level Machine Translation. (arXiv:2401.06468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;GPT-4&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#31163;&#26631;&#32763;&#35793;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#20043;&#21518;&#65292;&#20013;&#31561;&#35268;&#27169;&#30340;LLMs&#24448;&#24448;&#32988;&#36807;&#20854;&#26356;&#22823;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#23558;LLMs&#35843;&#25972;&#20026;&#29305;&#23450;&#35821;&#35328;&#23545;&#30340;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#65288;DocMT&#65289;&#30340;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25552;&#31034;&#31574;&#30053;&#23545;&#19979;&#28216;&#32763;&#35793;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#24494;&#35843;&#26041;&#27861;&#12289;&#19977;&#31181;LLM&#20027;&#24178;&#21644;18&#20010;&#28041;&#21450;&#20061;&#31181;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#29978;&#33267;&#22312;&#32763;&#35793;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;GPT-4&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23427;&#20204;&#19987;&#38376;&#22312;&#21452;&#35821;&#24179;&#34892;&#25991;&#26723;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20173;&#28982;&#26126;&#26174;&#23384;&#22312;&#31163;&#26631;&#32763;&#35793;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#38024;&#23545;DocMT&#37327;&#36523;&#23450;&#21046;&#30340;LLMs&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#32763;&#35793;&#20934;&#30830;&#24230;&#25913;&#21892;&#12289;&#22810;&#28304;&#20449;&#24687;&#25972;&#21512;&#31561;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant strides in various natural language processing (NLP) tasks. Recent research shows that the moderately-sized LLMs often outperform their larger counterparts after task-specific fine-tuning. In this work, we delve into the process of adapting LLMs to specialize in document-level machine translation (DocMT) for a specific language pair. Firstly, we explore how prompt strategies affect downstream translation performance. Then, we conduct extensive experiments with two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our findings indicate that in some cases, these specialized models even surpass GPT-4 in translation performance, while they still significantly suffer from the off-target translation issue in others, even if they are exclusively fine-tuned on bilingual parallel documents. Furthermore, we provide an in-depth analysis of these LLMs tailored for DocMT, exploring aspects such as transl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;&#22810;&#31181;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#32676;&#20307;&#30340;&#21021;&#27493;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07225</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;: &#35266;&#23519;&#21644;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions. (arXiv:2310.07225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07225
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#22810;&#31181;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#32676;&#20307;&#30340;&#21021;&#27493;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#39046;&#22495;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36890;&#36807;&#22312;&#26631;&#20934;&#21270;&#32771;&#35797;&#20013;&#21462;&#24471;&#21450;&#26684;&#20998;&#25968;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#32773;&#30340;&#24037;&#20855;&#12290;&#23558;LLMs&#37096;&#32626;&#21040;&#22914;&#27492;&#39640;&#39118;&#38505;&#30340;&#29615;&#22659;&#20013;&#38656;&#35201;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#38480;&#21046;&#26377;&#28165;&#26224;&#30340;&#29702;&#35299;&#12290;&#38543;&#30528;&#26032;&#30340;LLMs&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#21457;&#24067;&#65292;&#35782;&#21035;&#36328;&#27169;&#22411;&#23384;&#22312;&#30340;&#27169;&#24335;&#65292;&#24182;&#22240;&#27492;&#21487;&#33021;&#20986;&#29616;&#22312;&#26032;&#29256;&#26412;&#20013;&#65292;&#29305;&#21035;&#26377;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#31181;&#27969;&#34892;LLM&#22312;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#20316;&#20026;&#19968;&#20010;&#32676;&#20307;&#30340;&#29305;&#24615;&#12290;&#36890;&#36807;&#36825;&#20010;&#27604;&#36739;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise in medical question answering by achieving passing scores in standardised exams and have been suggested as tools for supporting healthcare workers. Deploying LLMs into such a high-risk context requires a clear understanding of the limitations of these models. With the rapid development and release of new LLMs, it is especially valuable to identify patterns which exist across models and may, therefore, continue to appear in newer versions. In this paper, we evaluate a wide range of popular LLMs on their knowledge of medical questions in order to better understand their properties as a group. From this comparison, we provide preliminary observations and raise open questions for further research.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27493;&#35299;&#27602;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#38454;&#27573;&#36827;&#34892;&#35299;&#27602;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#26080;&#27602;&#25552;&#31034;&#36827;&#34892;&#36830;&#32493;&#29983;&#25104;&#26469;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#35774;&#35745;Detox-Chain&#26469;&#26657;&#20934;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.08295</link><description>&lt;p&gt;
&#20998;&#27493;&#35299;&#27602;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Detoxify Language Model Step-by-Step. (arXiv:2308.08295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27493;&#35299;&#27602;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#38454;&#27573;&#36827;&#34892;&#35299;&#27602;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#26080;&#27602;&#25552;&#31034;&#36827;&#34892;&#36830;&#32493;&#29983;&#25104;&#26469;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#35774;&#35745;Detox-Chain&#26469;&#26657;&#20934;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#27602;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#22312;&#20445;&#25345;&#29983;&#25104;&#33021;&#21147;&#30340;&#21516;&#26102;&#36991;&#20813;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#30340;&#23433;&#20840;&#24615;&#65292;&#20808;&#21069;&#30340;&#35299;&#27602;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#25110;&#22312;&#21333;&#27493;&#39588;&#20013;&#20174;&#19981;&#21516;&#26041;&#38754;&#32422;&#26463;&#29983;&#25104;&#26469;&#35299;&#27602;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#27839;&#30528;&#26377;&#27602;&#25552;&#31034;&#29983;&#25104;&#65292;&#35299;&#27602;&#26041;&#27861;&#30340;&#24037;&#20316;&#26041;&#21521;&#19982;&#20043;&#30456;&#21453;&#65292;&#36825;&#20123;&#26041;&#27861;&#23558;&#22823;&#22823;&#24433;&#21709;LLM&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#22914;&#35805;&#35821;&#36830;&#36143;&#24615;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#20914;&#31361;&#65292;&#25105;&#20204;&#23558;&#35299;&#27602;&#36807;&#31243;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#23376;&#27493;&#39588;&#65292;&#20854;&#20013;&#35299;&#27602;&#38598;&#20013;&#22312;&#36755;&#20837;&#38454;&#27573;&#65292;&#38543;&#21518;&#30340;&#36830;&#32493;&#29983;&#25104;&#22522;&#20110;&#26080;&#27602;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;Detox-Chain&#26469;&#26657;&#20934;LLMs&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#26377;&#24207;&#30340;&#26041;&#24335;&#36830;&#25509;&#19978;&#36848;&#23376;&#27493;&#39588;&#65292;&#36825;&#20351;&#24471;LLMs&#21487;&#20197;&#36827;&#34892;&#36830;&#32493;&#30340;&#35299;&#27602;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detoxification for LLMs is challenging since it requires models to avoid generating harmful content while maintaining the generation capability. To ensure the safety of generations, previous detoxification methods detoxify the models by changing the data distributions or constraining the generations from different aspects in a single-step manner. However, these approaches will dramatically affect the generation quality of LLMs, e.g., discourse coherence and semantic consistency, since language models tend to generate along the toxic prompt while detoxification methods work in the opposite direction. To handle such a conflict, we decompose the detoxification process into different sub-steps, where the detoxification is concentrated in the input stage and the subsequent continual generation is based on the non-toxic prompt. Besides, we also calibrate the strong reasoning ability of LLMs by designing a Detox-Chain to connect the above sub-steps in an orderly manner, which allows LLMs to d
&lt;/p&gt;</description></item><item><title>NatLogAttack&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#36923;&#36753;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#36827;&#34892;&#20445;&#25345;&#26631;&#31614;&#21644;&#32763;&#36716;&#26631;&#31614;&#30340;&#25915;&#20987;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25915;&#20987;&#27169;&#22411;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.02849</link><description>&lt;p&gt;
NatLogAttack: &#19968;&#20010;&#29992;&#33258;&#28982;&#36923;&#36753;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#25915;&#20987;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NatLogAttack: A Framework for Attacking Natural Language Inference Models with Natural Logic. (arXiv:2307.02849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02849
&lt;/p&gt;
&lt;p&gt;
NatLogAttack&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#36923;&#36753;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#36827;&#34892;&#20445;&#25345;&#26631;&#31614;&#21644;&#32763;&#36716;&#26631;&#31614;&#30340;&#25915;&#20987;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25915;&#20987;&#27169;&#22411;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#33258;&#20174;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#22987;&#23601;&#26159;&#19968;&#20010;&#20013;&#24515;&#35805;&#39064;&#12290;&#36817;&#24180;&#26469;&#22312;&#20998;&#24067;&#24335;&#34920;&#31034;&#21644;&#31070;&#32463;&#32593;&#32476;&#19978;&#21462;&#24471;&#30340;&#36827;&#23637;&#25345;&#32493;&#25913;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36890;&#36807;&#30495;&#27491;&#30340;&#25512;&#29702;&#26469;&#24471;&#20986;&#32467;&#35770;&#65292;&#36824;&#26159;&#20381;&#36182;&#20110;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#24050;&#32463;&#35777;&#26126;&#26159;&#35780;&#20272;&#21463;&#23475;&#27169;&#22411;&#30340;&#33268;&#21629;&#24369;&#28857;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#36923;&#36753;&#24418;&#24335;&#20027;&#20041;&#24320;&#21457;&#25915;&#20987;&#27169;&#22411;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NatLogAttack&#26469;&#25191;&#34892;&#22260;&#32469;&#33258;&#28982;&#36923;&#36753;&#30340;&#31995;&#32479;&#24615;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#36861;&#28335;&#21040;&#20122;&#37324;&#22763;&#22810;&#24503;&#19977;&#27573;&#35770;&#24182;&#19988;&#19982;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#23494;&#20999;&#30456;&#20851;&#30340;&#32463;&#20856;&#36923;&#36753;&#24418;&#24335;&#12290;&#35813;&#25552;&#35758;&#30340;&#26694;&#26550;&#21487;&#20197;&#36827;&#34892;&#20445;&#25345;&#26631;&#31614;&#21644;&#32763;&#36716;&#26631;&#31614;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#25915;&#20987;&#27169;&#22411;&#30456;&#27604;&#65292;NatLogAttack&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning has been a central topic in artificial intelligence from the beginning. The recent progress made on distributed representation and neural networks continues to improve the state-of-the-art performance of natural language inference. However, it remains an open question whether the models perform real reasoning to reach their conclusions or rely on spurious correlations. Adversarial attacks have proven to be an important tool to help evaluate the Achilles' heel of the victim models. In this study, we explore the fundamental problem of developing attack models based on logic formalism. We propose NatLogAttack to perform systematic attacks centring around natural logic, a classical logic formalism that is traceable back to Aristotle's syllogism and has been closely developed for natural language inference. The proposed framework renders both label-preserving and label-flipping attacks. We show that compared to the existing attack models, NatLogAttack generates better adversarial 
&lt;/p&gt;</description></item></channel></rss>