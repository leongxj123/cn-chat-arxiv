<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35782;&#21035;&#37027;&#20123;&#22312;&#24494;&#35843;&#21518;&#26356;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26377;&#23475;&#35831;&#27714;&#30340;&#21709;&#24212;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.01099</link><description>&lt;p&gt;
&#20320;&#30340;&#8220;&#23433;&#20840;&#8221;&#25968;&#25454;&#20013;&#26377;&#20160;&#20040;&#65311;&#65306;&#35782;&#21035;&#30772;&#22351;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01099
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35782;&#21035;&#37027;&#20123;&#22312;&#24494;&#35843;&#21518;&#26356;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26377;&#23475;&#35831;&#27714;&#30340;&#21709;&#24212;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21363;&#20351;&#32463;&#36807;&#35843;&#25972;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#20063;&#23481;&#26131;&#34987;&#36234;&#29425;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#21482;&#26159;&#36827;&#19968;&#27493;&#20351;&#29992;&#33391;&#24615;&#25968;&#25454;&#65288;&#21363;&#27809;&#26377;&#26377;&#23475;&#20869;&#23481;&#30340;&#25968;&#25454;&#65289;&#23545;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20250;&#23548;&#33268;&#23433;&#20840;&#24615;&#22823;&#24133;&#19979;&#38477;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#33391;&#24615;&#24494;&#35843;&#19981;&#32463;&#24847;&#38388;&#23548;&#33268;&#36234;&#29425;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#35270;&#35282;&#34920;&#24449;&#24494;&#35843;&#25968;&#25454;&#65306;&#34920;&#31034;&#21644;&#26799;&#24230;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#26377;&#23475;&#31034;&#20363;&#24182;&#36828;&#31163;&#33391;&#24615;&#31034;&#20363;&#30340;&#25968;&#25454;&#28857;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#26356;&#26377;&#21487;&#33021;&#22312;&#24494;&#35843;&#21518;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#12290;&#20165;&#20165;&#35757;&#32451;100&#20010;&#36825;&#20123;&#30475;&#20284;&#33391;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#23601;&#21487;&#20197;&#20351;&#24494;&#35843;&#27169;&#22411;&#32943;&#23450;&#22320;&#22238;&#24212;&#36229;&#36807;70&#65285;&#30340;&#34987;&#27979;&#35797;&#30340;&#26377;&#23475;&#35831;&#27714;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01099v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to &gt; 70% of tested harmful requests, compared to &lt;
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28385;&#24847;&#24863;&#30693;&#30340;&#21453;&#20107;&#23454;&#23545;&#35805;&#26469;&#22686;&#21152;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#21407;&#22987;&#23545;&#35805;&#38598;&#21512;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19056</link><description>&lt;p&gt;
CAUSE: &#22312;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#21033;&#29992;&#21453;&#20107;&#23454;&#35780;&#20272;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28385;&#24847;&#24863;&#30693;&#30340;&#21453;&#20107;&#23454;&#23545;&#35805;&#26469;&#22686;&#21152;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#21407;&#22987;&#23545;&#35805;&#38598;&#21512;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#30340;&#24037;&#20316;&#20013;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#25506;&#32034;&#30340;&#26041;&#38754;&#26159;&#23545;&#20854;&#22312;&#35782;&#21035;&#29992;&#25143;&#19981;&#28385;&#24847;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#35780;&#20272;&#65306;&#24403;&#21069;&#29992;&#20110;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#39640;&#24230;&#20542;&#21521;&#20110;&#29992;&#25143;&#28385;&#24847;&#30340;&#23545;&#35805;&#12290;&#20855;&#26377;&#26356;&#24179;&#34913;&#28385;&#24847;&#24230;&#26631;&#31614;&#38598;&#21512;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#26159;&#26410;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26356;&#22810;&#30340;&#19981;&#28385;&#23545;&#35805;&#26679;&#26412;&#24179;&#34913;&#25968;&#25454;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#20154;&#24037;&#27880;&#37322;&#65292;&#36825;&#26159;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#35299;&#38145;&#20854;&#29983;&#25104;&#28385;&#24847;&#24863;&#30693;&#21453;&#20107;&#23454;&#23545;&#35805;&#30340;&#33021;&#21147;&#65292;&#20197;&#22686;&#21152;&#27979;&#35797;&#38598;&#21512;&#30340;&#21407;&#22987;&#23545;&#35805;&#38598;&#21512;&#12290;&#25105;&#20204;&#25910;&#38598;&#20154;&#24037;&#27880;&#37322;&#20197;&#30830;&#20445;&#29983;&#25104;&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20004;&#20010;&#24320;&#28304;LLM&#20316;&#20026;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19056v1 Announce Type: new  Abstract: An important unexplored aspect in previous work on user satisfaction estimation for Task-Oriented Dialogue (TOD) systems is their evaluation in terms of robustness for the identification of user dissatisfaction: current benchmarks for user satisfaction estimation in TOD systems are highly skewed towards dialogues for which the user is satisfied. The effect of having a more balanced set of satisfaction labels on performance is unknown. However, balancing the data with more dissatisfactory dialogue samples requires further data collection and human annotation, which is costly and time-consuming. In this work, we leverage large language models (LLMs) and unlock their ability to generate satisfaction-aware counterfactual dialogues to augment the set of original dialogues of a test collection. We gather human annotations to ensure the reliability of the generated samples. We evaluate two open-source LLMs as user satisfaction estimators on our
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoSumm&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30446;&#26631;&#23454;&#29616;&#20102;&#26080;&#21442;&#32771;&#25688;&#35201;&#30340;&#31934;&#28860;&#29983;&#25104;&#22120;</title><link>https://arxiv.org/abs/2403.13780</link><description>&lt;p&gt;
&#26080;&#21442;&#32771;&#25688;&#35201;&#30340;&#20449;&#24687;&#35770;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Distillation for Reference-less Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13780
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoSumm&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30446;&#26631;&#23454;&#29616;&#20102;&#26080;&#21442;&#32771;&#25688;&#35201;&#30340;&#31934;&#28860;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33258;&#21160;&#25688;&#35201;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#19987;&#26377;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#65292;&#25110;&#32773;&#20174;&#23427;&#20204;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoSumm&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30446;&#26631;&#36827;&#34892;&#31934;&#28860;&#24378;&#22823;&#30340;&#25688;&#35201;&#29983;&#25104;&#22120;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;LLM&#30340;&#33021;&#21147;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13780v1 Announce Type: new  Abstract: The current winning recipe for automatic summarization is using proprietary large-scale language models (LLMs) such as ChatGPT as is, or imitation learning from them as teacher models. While increasingly ubiquitous dependence on such large-scale language models is convenient, there remains an important question of whether small-scale models could have achieved competitive results, if we were to seek an alternative learning method -- that allows for a more cost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a novel framework to distill a powerful summarizer based on the information-theoretic objective for summarization, without relying on either the LLM's capability or human-written references. To achieve this, we first propose a novel formulation of the desiderata of summarization (saliency, faithfulness and brevity) through the lens of mutual information between the original document and the summary. Based on thi
&lt;/p&gt;</description></item><item><title>SoftTiger&#26159;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#20020;&#24202;&#31508;&#35760;&#30340;&#32467;&#26500;&#21270;&#65292;&#23454;&#29616;&#20102;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.00868</link><description>&lt;p&gt;
SoftTiger: &#29992;&#20110;&#21307;&#30103;&#24037;&#20316;&#27969;&#30340;&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SoftTiger: A Clinical Foundation Model for Healthcare Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00868
&lt;/p&gt;
&lt;p&gt;
SoftTiger&#26159;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#20020;&#24202;&#31508;&#35760;&#30340;&#32467;&#26500;&#21270;&#65292;&#23454;&#29616;&#20102;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#24182;&#20171;&#32461;&#20102;SoftTiger&#65292;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CLaM&#65289;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#12290;&#20020;&#24202;&#31508;&#35760;&#30340;&#21465;&#36848;&#24615;&#21644;&#38750;&#32467;&#26500;&#21270;&#29305;&#24615;&#26159;&#21307;&#30103;&#26234;&#33021;&#21270;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#25353;&#29031;&#22269;&#38469;&#20114;&#25805;&#20316;&#24615;&#26631;&#20934;&#23558;&#20020;&#24202;&#31508;&#35760;&#32467;&#26500;&#21270;&#20026;&#20020;&#24202;&#25968;&#25454;&#65292;&#28041;&#21450;&#22269;&#38469;&#24739;&#32773;&#25688;&#35201;&#12289;&#20020;&#24202;&#21360;&#35937;&#21644;&#21307;&#30103;&#25509;&#35302;&#19977;&#20010;&#20851;&#38190;&#23376;&#20219;&#21153;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21644;&#39564;&#35777;&#30340;&#20020;&#24202;&#25968;&#25454;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30446;&#26631;&#27169;&#22411;&#39318;&#20808;&#33021;&#22815;&#25903;&#25345;&#22522;&#26412;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#22914;&#32553;&#20889;&#25193;&#23637;&#21644;&#26102;&#38388;&#20449;&#24687;&#25552;&#21462;&#65292;&#28982;&#21518;&#23398;&#20064;&#25191;&#34892;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#65292;&#22914;&#21360;&#35937;&#21644;&#25509;&#35302;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21307;&#30103;&#27169;&#22411;&#20013;&#30340;&#19968;&#20123;&#24314;&#27169;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00868v1 Announce Type: cross  Abstract: We release and introduce SoftTiger, a clinical large language model (CLaM) designed as a foundation model for healthcare workflows. The narrative and unstructured nature of clinical notes is a major obstacle for healthcare intelligentization. We address a critical problem of structuring clinical notes into clinical data, according to international interoperability standards. We collect and annotate data for three critical subtasks, namely, international patient summary, clinical impression and medical encounter. We then supervised fine-tuned a state-of-the-art LLM using public and credentialed clinical data. The training is orchestrated in a way that the target model can first support basic clinical tasks such as abbreviation expansion and temporal information extraction, and then learn to perform more complex downstream clinical tasks such as impression and encounter summary. Moreover, we address, several modeling challenges in the he
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20013;&#36896;&#25104;&#30340;&#24378;&#24187;&#35273;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21542;&#23450;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#32780;&#26080;&#38656;&#20351;&#29992;&#31232;&#30095;&#36127;&#25968;&#25454;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.10543</link><description>&lt;p&gt;
&#28040;&#38500;&#21542;&#23450;&#23548;&#33268;&#30340;&#24378;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Strong hallucinations from negation and how to fix them
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10543
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20013;&#36896;&#25104;&#30340;&#24378;&#24187;&#35273;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21542;&#23450;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#32780;&#26080;&#38656;&#20351;&#29992;&#31232;&#30095;&#36127;&#25968;&#25454;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#22312;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26377;&#26102;&#20250;&#25552;&#20379;&#30001;&#20110;&#36923;&#36753;&#19981;&#36830;&#36143;&#32780;&#19981;&#21487;&#33021;&#25104;&#31435;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#21709;&#24212;&#20026;\textit{&#24378;&#24187;&#35273;}&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#28304;&#20110;LM&#35745;&#31639;&#20854;&#20869;&#37096;&#34920;&#31034;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#20174;&#36825;&#20123;&#34920;&#31034;&#20013;&#20135;&#29983;&#30340;&#36755;&#20986;&#12290;&#37325;&#28857;&#20851;&#27880;&#21542;&#23450;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#21542;&#23450;&#19981;&#26159;&#20316;&#20026;&#28508;&#22312;&#34920;&#31034;&#30340;&#21478;&#19968;&#20010;&#20803;&#32032;&#65292;&#32780;&#26159;&#20316;&#20026;\textit{LM&#28508;&#22312;&#34920;&#31034;&#19978;&#30340;&#19968;&#20010;&#25805;&#20316;&#65292;&#32422;&#26463;&#23427;&#20204;&#21487;&#33021;&#30340;&#28436;&#21464;&#26041;&#24335;}&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#22312;&#24102;&#21542;&#23450;&#30340;&#22635;&#31354;&#25552;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#23545;&#31232;&#30095;&#36127;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10543v1 Announce Type: cross  Abstract: Despite great performance on many tasks, language models (LMs) still struggle with reasoning, sometimes providing responses that cannot possibly be true because they stem from logical incoherence. We call such responses \textit{strong hallucinations} and prove that they follow from an LM's computation of its internal representations for logical operators and outputs from those representations. Focusing on negation, we provide a novel solution in which negation is treated not as another element of a latent representation, but as \textit{an operation over an LM's latent representations that constrains how they may evolve}. We show that our approach improves model performance in cloze prompting and natural language inference tasks with negation without requiring training on sparse negative data.
&lt;/p&gt;</description></item><item><title>PsySafe&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#26234;&#33021;&#20307;&#24515;&#29702;&#23398;&#65292;&#25581;&#31034;&#26234;&#33021;&#20307;&#30340;&#40657;&#26263;&#24515;&#29702;&#29366;&#24577;&#23545;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2401.11880</link><description>&lt;p&gt;
PsySafe&#65306;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23433;&#20840;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#30340;&#32508;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11880
&lt;/p&gt;
&lt;p&gt;
PsySafe&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#26234;&#33021;&#20307;&#24515;&#29702;&#23398;&#65292;&#25581;&#31034;&#26234;&#33021;&#20307;&#30340;&#40657;&#26263;&#24515;&#29702;&#29366;&#24577;&#23545;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#21152;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21518;&#65292;&#23637;&#29616;&#20986;&#20102;&#38598;&#20307;&#26234;&#33021;&#30340;&#28145;&#36828;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26234;&#33021;&#34987;&#24694;&#24847;&#20351;&#29992;&#21487;&#33021;&#24102;&#26469;&#37325;&#22823;&#39118;&#38505;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#21019;&#26032;&#30340;&#35270;&#35282;&#25506;&#32034;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#21457;&#29616;&#26234;&#33021;&#20307;&#30340;&#40657;&#26263;&#24515;&#29702;&#29366;&#24577;&#26500;&#25104;&#20102;&#23545;&#23433;&#20840;&#30340;&#37325;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#26234;&#33021;&#20307;&#24515;&#29702;&#23398;&#20026;&#22522;&#30784;&#30340;&#32508;&#21512;&#26694;&#26550;&#65288;PsySafe&#65289;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65306;&#39318;&#20808;&#65292;&#35782;&#21035;&#26234;&#33021;&#20307;&#20013;&#30340;&#40657;&#26263;&#20154;&#26684;&#29305;&#24449;&#22914;&#20309;&#23548;&#33268;&#39118;&#38505;&#34892;&#20026;&#65307;&#20854;&#27425;&#65292;&#20174;&#24515;&#29702;&#21644;&#34892;&#20026;&#35282;&#24230;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65307;&#31532;&#19977;&#65292;&#21046;&#23450;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11880v2 Announce Type: replace-cross  Abstract: Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit profound capabilities in collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. In this paper, we explore these concerns through the innovative lens of agent psychology, revealing that the dark psychological states of agents constitute a significant threat to safety. To tackle these concerns, we propose a comprehensive framework (PsySafe) grounded in agent psychology, focusing on three key areas: firstly, identifying how dark personality traits in agents can lead to risky behaviors; secondly, evaluating the safety of multi-agent systems from the psychological and behavioral perspectives, and thirdly, devising effective strategies to mitigate these risks. Our experiments reveal
&lt;/p&gt;</description></item><item><title>F-Eval&#26159;&#19968;&#20010;&#21452;&#35821;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#21253;&#25324;&#34920;&#36798;&#12289;&#24120;&#35782;&#21644;&#36923;&#36753;&#12290;&#23427;&#37319;&#29992;&#22810;&#31181;&#20219;&#21153;&#24418;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#23458;&#35266;&#20219;&#21153;&#21644;&#20027;&#35266;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#35299;&#20915;&#26080;&#21442;&#32771;&#30340;&#20027;&#35266;&#20219;&#21153;&#35780;&#20272;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14869</link><description>&lt;p&gt;
F-Eval:&#20351;&#29992;&#20248;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#35780;&#20272;&#22522;&#26412;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods. (arXiv:2401.14869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14869
&lt;/p&gt;
&lt;p&gt;
F-Eval&#26159;&#19968;&#20010;&#21452;&#35821;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#21253;&#25324;&#34920;&#36798;&#12289;&#24120;&#35782;&#21644;&#36923;&#36753;&#12290;&#23427;&#37319;&#29992;&#22810;&#31181;&#20219;&#21153;&#24418;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#23458;&#35266;&#20219;&#21153;&#21644;&#20027;&#35266;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#35299;&#20915;&#26080;&#21442;&#32771;&#30340;&#20027;&#35266;&#20219;&#21153;&#35780;&#20272;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#23548;&#33268;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35780;&#20272;LLMs&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#20272;&#22522;&#20934;&#20165;&#38480;&#20110;&#35780;&#20272;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65292;&#24573;&#35270;&#20102;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20986;&#29616;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#20027;&#35266;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#30001;API&#27169;&#22411;&#35780;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#21442;&#32771;&#25991;&#29486;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#27169;&#22411;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#33021;&#21147;&#26469;&#21306;&#20998;&#32454;&#24494;&#24046;&#24322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;F-Eval&#65292;&#19968;&#20010;&#21452;&#35821;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#26412;&#33021;&#21147;&#65292;&#21253;&#25324;&#34920;&#36798;&#12289;&#24120;&#35782;&#21644;&#36923;&#36753;&#12290;F-Eval&#20013;&#30340;&#20219;&#21153;&#21253;&#25324;&#22810;&#39033;&#36873;&#25321;&#23458;&#35266;&#20219;&#21153;&#12289;&#24320;&#25918;&#24335;&#23458;&#35266;&#20219;&#21153;&#12289;&#22522;&#20110;&#21442;&#32771;&#30340;&#20027;&#35266;&#20219;&#21153;&#21644;&#26080;&#21442;&#32771;&#30340;&#20027;&#35266;&#20219;&#21153;&#12290;&#23545;&#20110;&#26080;&#21442;&#32771;&#30340;&#20027;&#35266;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#20026;&#26367;&#20195;API&#27169;&#22411;&#35780;&#20998;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;13&#20010;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced L
&lt;/p&gt;</description></item><item><title>CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.05544</link><description>&lt;p&gt;
CodePrompt&#65306;&#36890;&#36807;Prompt&#23398;&#20064;&#30340;&#30693;&#35782;&#29305;&#24449;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning. (arXiv:2401.05544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05544
&lt;/p&gt;
&lt;p&gt;
CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CodeBERT&#65289;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;CodeBERT&#30340;&#25991;&#26412;&#23884;&#20837;&#33021;&#21147;&#21644;"[CLS]"&#21477;&#23376;&#23884;&#20837;&#20449;&#24687;&#20316;&#20026;&#19979;&#28216;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#35821;&#20041;&#34920;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#26469;&#25552;&#21462;&#26377;&#25928;&#29305;&#24449;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#33021;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CodePrompt&#65292;&#36890;&#36807;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have explored the potential of utilizing pre-trained language models, such as CodeBERT, to improve source code-related tasks. Previous studies have mainly relied on CodeBERT's text embedding capability and the `[CLS]' sentence embedding information as semantic representations for fine-tuning downstream source code-related tasks. However, these methods require additional neural network layers to extract effective features, resulting in higher computational costs. Furthermore, existing approaches have not leveraged the rich knowledge contained in both source code and related text, which can lead to lower accuracy. This paper presents a novel approach, CodePrompt, which utilizes rich knowledge recalled from a pre-trained model by prompt learning and an attention mechanism to improve source code-related classification tasks. Our approach initially motivates the language model with prompt information to retrieve abundant knowledge associated with the input as representative feat
&lt;/p&gt;</description></item><item><title>PromptBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;&#65292;&#21253;&#25324;&#20102;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#31561;&#32452;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#21644;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>http://arxiv.org/abs/2312.07910</link><description>&lt;p&gt;
PromptBench&#65306;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07910
&lt;/p&gt;
&lt;p&gt;
PromptBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;&#65292;&#21253;&#25324;&#20102;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#31561;&#32452;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#21644;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#23545;&#20110;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PromptBench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#32479;&#19968;&#24211;&#12290;&#23427;&#30001;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;&#21644;&#25193;&#23637;&#65306;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#12290;PromptBench&#26088;&#22312;&#25104;&#20026;&#19968;&#20010;&#24320;&#25918;&#12289;&#36890;&#29992;&#21644;&#28789;&#27963;&#30340;&#20195;&#30721;&#24211;&#65292;&#20197;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#65292;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#21644;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/microsoft/promptbench&#19978;&#25214;&#21040;&#65292;&#24182;&#23558;&#25345;&#32493;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26080;&#39118;&#26684;&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#65292;&#33021;&#22815;&#23545;&#25239;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39118;&#26684;&#25915;&#20987;&#30340;&#20551;&#26032;&#38395;&#12290;&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#26032;&#38395;&#37325;&#26500;&#65292;&#35813;&#26816;&#27979;&#22120;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#25552;&#39640;&#20102;&#23545;&#20266;&#35013;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10830</link><description>&lt;p&gt;
&#20551;&#26032;&#38395;&#22312;&#32501;&#32650;&#30340;&#22806;&#34915;&#20013;&#65306;&#23545;&#25239;LLM&#22686;&#24378;&#39118;&#26684;&#25915;&#20987;&#30340;&#40065;&#26834;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks. (arXiv:2310.10830v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10830
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26080;&#39118;&#26684;&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#65292;&#33021;&#22815;&#23545;&#25239;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39118;&#26684;&#25915;&#20987;&#30340;&#20551;&#26032;&#38395;&#12290;&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#26032;&#38395;&#37325;&#26500;&#65292;&#35813;&#26816;&#27979;&#22120;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#25552;&#39640;&#20102;&#23545;&#20266;&#35013;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24120;&#24120;&#35748;&#20026;&#22312;&#32447;&#20551;&#26032;&#38395;&#21644;&#21487;&#38752;&#26032;&#38395;&#22312;&#20889;&#20316;&#39118;&#26684;&#19978;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#22914;&#20351;&#29992;&#32824;&#20154;&#21548;&#38395;&#30340;&#35821;&#35328;&#19982;&#23458;&#35266;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24378;&#35843;&#39118;&#26684;&#30456;&#20851;&#29305;&#24449;&#20063;&#21487;&#20197;&#29992;&#20110;&#39118;&#26684;&#25915;&#20987;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23835;&#36215;&#20351;&#24694;&#24847;&#29992;&#25143;&#33021;&#22815;&#20197;&#26368;&#20302;&#25104;&#26412;&#27169;&#20223;&#20540;&#24471;&#20449;&#36182;&#30340;&#26032;&#38395;&#23186;&#20307;&#30340;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#20197;LLM&#20266;&#35013;&#30340;&#20551;&#26032;&#38395;&#20869;&#23481;&#23548;&#33268;&#20808;&#36827;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65288;F1&#20998;&#25968;&#20943;&#23569;&#39640;&#36798;38%&#65289;&#65292;&#32473;&#22312;&#32447;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#33258;&#21160;&#26816;&#27979;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SheepDog&#65292;&#19968;&#31181;&#23545;&#26032;&#38395;&#20889;&#20316;&#39118;&#26684;&#40065;&#26834;&#30340;&#26080;&#39118;&#26684;&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#12290;SheepDog&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#26032;&#38395;&#37325;&#26500;&#23454;&#29616;&#20102;&#36825;&#31181;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807;&#39118;&#26684;&#23548;&#21521;&#30340;&#37325;&#26500;&#25552;&#31034;&#26469;&#23450;&#21046;&#27599;&#31687;&#25991;&#31456;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;&#36890;&#36807;&#37319;&#29992;&#26080;&#39118;&#26684;&#35757;&#32451;&#65292;SheepDog&#21487;&#20197;&#22312;&#19981;&#21516;&#39118;&#26684;&#30340;&#26032;&#38395;&#20013;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is commonly perceived that online fake news and reliable news exhibit stark differences in writing styles, such as the use of sensationalist versus objective language. However, we emphasize that style-related features can also be exploited for style-based attacks. Notably, the rise of powerful Large Language Models (LLMs) has enabled malicious users to mimic the style of trustworthy news outlets at minimal cost. Our analysis reveals that LLM-camouflaged fake news content leads to substantial performance degradation of state-of-the-art text-based detectors (up to 38% decrease in F1 Score), posing a significant challenge for automated detection in online ecosystems. To address this, we introduce SheepDog, a style-agnostic fake news detector robust to news writing styles. SheepDog achieves this adaptability through LLM-empowered news reframing, which customizes each article to match different writing styles using style-oriented reframing prompts. By employing style-agnostic training, S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;Blizzard Challenge 2023&#30340;&#27861;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#31579;&#36873;&#21644;&#22686;&#24378;&#65292;&#20197;&#21450;&#28155;&#21152;&#35789;&#36793;&#30028;&#21644;&#36215;&#22987;/&#32467;&#26463;&#31526;&#21495;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#36136;&#37327;&#24182;&#36827;&#34892;&#20102;&#26631;&#20934;&#21270;&#36716;&#24405;&#12290;</title><link>http://arxiv.org/abs/2309.00223</link><description>&lt;p&gt;
FruitShell&#27861;&#35821;&#21512;&#25104;&#31995;&#32479;&#22312;Blizzard 2023&#25361;&#25112;&#36187;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The FruitShell French synthesis system at the Blizzard 2023 Challenge. (arXiv:2309.00223v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;Blizzard Challenge 2023&#30340;&#27861;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#31579;&#36873;&#21644;&#22686;&#24378;&#65292;&#20197;&#21450;&#28155;&#21152;&#35789;&#36793;&#30028;&#21644;&#36215;&#22987;/&#32467;&#26463;&#31526;&#21495;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#36136;&#37327;&#24182;&#36827;&#34892;&#20102;&#26631;&#20934;&#21270;&#36716;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;Blizzard Challenge 2023&#30340;&#27861;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#12290;&#35813;&#25361;&#25112;&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65306;&#20174;&#22899;&#24615;&#28436;&#35762;&#32773;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21644;&#29983;&#25104;&#19982;&#29305;&#23450;&#20010;&#20307;&#30456;&#20284;&#30340;&#35821;&#38899;&#12290;&#20851;&#20110;&#27604;&#36187;&#25968;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31579;&#36873;&#36807;&#31243;&#65292;&#21435;&#38500;&#20102;&#32570;&#22833;&#25110;&#38169;&#35823;&#30340;&#25991;&#26412;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#38500;&#38899;&#32032;&#20197;&#22806;&#30340;&#25152;&#26377;&#31526;&#21495;&#36827;&#34892;&#20102;&#25972;&#29702;&#65292;&#24182;&#28040;&#38500;&#20102;&#27809;&#26377;&#21457;&#38899;&#25110;&#25345;&#32493;&#26102;&#38388;&#20026;&#38646;&#30340;&#31526;&#21495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#25991;&#26412;&#20013;&#28155;&#21152;&#20102;&#35789;&#36793;&#30028;&#21644;&#36215;&#22987;/&#32467;&#26463;&#31526;&#21495;&#65292;&#26681;&#25454;&#25105;&#20204;&#20043;&#21069;&#30340;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#26679;&#21487;&#20197;&#25552;&#39640;&#35821;&#38899;&#36136;&#37327;&#12290;&#23545;&#20110;Spoke&#20219;&#21153;&#65292;&#25105;&#20204;&#26681;&#25454;&#27604;&#36187;&#35268;&#21017;&#36827;&#34892;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;G2P&#27169;&#22411;&#23558;&#27861;&#35821;&#25991;&#26412;&#36716;&#24405;&#20026;&#38899;&#32032;&#12290;&#30001;&#20110;G2P&#27169;&#22411;&#20351;&#29992;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#65292;&#25105;&#20204;&#23545;&#25552;&#20379;&#30340;&#27604;&#36187;&#25968;&#25454;&#24212;&#29992;&#20102;&#30456;&#21516;&#30340;&#36716;&#24405;&#36807;&#31243;&#65292;&#20197;&#36827;&#34892;&#26631;&#20934;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32534;&#35793;&#22120;&#23545;&#26576;&#20123;&#25216;&#26415;&#38480;&#21046;&#30340;&#35782;&#21035;&#33021;&#21147;&#26377;&#38480;&#65292;&#25152;&#20197;&#25105;&#20204;&#20026;&#20102;&#20445;&#25345;&#31454;&#20105;&#30340;&#20844;&#27491;&#65292;&#23558;&#25968;&#25454;&#25353;&#38899;&#26631;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#29255;&#27573;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a French text-to-speech synthesis system for the Blizzard Challenge 2023. The challenge consists of two tasks: generating high-quality speech from female speakers and generating speech that closely resembles specific individuals. Regarding the competition data, we conducted a screening process to remove missing or erroneous text data. We organized all symbols except for phonemes and eliminated symbols that had no pronunciation or zero duration. Additionally, we added word boundary and start/end symbols to the text, which we have found to improve speech quality based on our previous experience. For the Spoke task, we performed data augmentation according to the competition rules. We used an open-source G2P model to transcribe the French texts into phonemes. As the G2P model uses the International Phonetic Alphabet (IPA), we applied the same transcription process to the provided competition data for standardization. However, due to compiler limitations in recognizing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#20197;&#21450;&#25552;&#20986;&#22810;&#38454;&#27573;&#26694;&#26550;&#26469;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#28548;&#28165;&#26816;&#32034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05680</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#38454;&#27573;&#26816;&#32034;&#25214;&#21040;&#24050;&#32463;&#34987;&#28548;&#28165;&#30340;&#21465;&#36848;&#65306;&#23454;&#29616;&#36328;&#35821;&#35328;&#12289;&#36328;&#25968;&#25454;&#38598;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning. (arXiv:2308.05680v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#20197;&#21450;&#25552;&#20986;&#22810;&#38454;&#27573;&#26694;&#26550;&#26469;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#28548;&#28165;&#26816;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#24050;&#32463;&#34987;&#28548;&#28165;&#30340;&#21465;&#36848;&#30340;&#20219;&#21153;&#26088;&#22312;&#26816;&#27979;&#24050;&#32463;&#32463;&#36807;&#20107;&#23454;&#26680;&#26597;&#30340;&#25925;&#20107;&#12290;&#25104;&#21151;&#26816;&#27979;&#21040;&#24050;&#34987;&#28548;&#28165;&#30340;&#22768;&#26126;&#19981;&#20165;&#20943;&#23569;&#20102;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#30340;&#25163;&#21160;&#21162;&#21147;&#65292;&#36824;&#21487;&#20197;&#26377;&#21161;&#20110;&#20943;&#32531;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#65292;&#36825;&#26159;&#19968;&#20010;&#30740;&#31350;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32771;&#34385;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#65292;&#21363;&#22312;&#26816;&#26597;&#30340;&#22312;&#32447;&#24086;&#23376;&#30340;&#35821;&#35328;&#19982;&#20107;&#23454;&#26680;&#26597;&#25991;&#31456;&#30340;&#35821;&#35328;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26816;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65306;&#65288;i&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20801;&#35768;&#23545;&#24050;&#34987;&#28548;&#28165;&#30340;&#21465;&#36848;&#36827;&#34892;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#25512;&#25991;&#20316;&#20026;&#23545;&#20107;&#23454;&#26680;&#26597;&#25991;&#31456;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#65307;&#65288;ii&#65289;&#23637;&#31034;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#32463;&#36807;&#24494;&#35843;&#21644;&#29616;&#25104;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65307;&#65288;iii&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#26694;&#26550;&#65292;&#23558;&#36825;&#20010;&#36328;&#35821;&#35328;&#28548;&#28165;&#26816;&#32034;&#38382;&#39064;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of retrieving already debunked narratives aims to detect stories that have already been fact-checked. The successful detection of claims that have already been debunked not only reduces the manual efforts of professional fact-checkers but can also contribute to slowing the spread of misinformation. Mainly due to the lack of readily available data, this is an understudied problem, particularly when considering the cross-lingual task, i.e. the retrieval of fact-checking articles in a language different from the language of the online post being checked. This paper fills this gap by (i) creating a novel dataset to enable research on cross-lingual retrieval of already debunked narratives, using tweets as queries to a database of fact-checking articles; (ii) presenting an extensive experiment to benchmark fine-tuned and off-the-shelf multilingual pre-trained Transformer models for this task; and (iii) proposing a novel multistage framework that divides this cross-lingual debunk ret
&lt;/p&gt;</description></item><item><title>MMBench&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#24320;&#21457;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.06281</link><description>&lt;p&gt;
MMBench: &#24744;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20840;&#33021;&#29699;&#21592;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06281
&lt;/p&gt;
&lt;p&gt;
MMBench&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#24320;&#21457;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#20449;&#24687;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#35780;&#20272;&#36825;&#20123;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#26410;&#26469;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20256;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;VQAv2&#25110;COCO Caption&#25552;&#20379;&#20102;&#23450;&#37327;&#30340;&#24615;&#33021;&#27979;&#37327;&#65292;&#20294;&#22312;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#21644;&#38750;&#40065;&#26834;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26368;&#36817;&#30340;&#20027;&#35266;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;OwlEval&#65292;&#36890;&#36807;&#25972;&#21512;&#20154;&#21147;&#36164;&#28304;&#65292;&#23545;&#27169;&#22411;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20294;&#19981;&#21487;&#25193;&#23637;&#24182;&#19988;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MMBench&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#12290;MMBench&#31995;&#32479;&#22320;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#20027;&#35201;&#30001;&#20004;&#20010;&#20803;&#32032;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#20803;&#32032;&#26159;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#35780;&#20272;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#31867;&#20284;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluatio
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00050</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24320;&#21551;&#22240;&#26524;&#30740;&#31350;&#30340;&#26032;&#31687;&#31456;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00050
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#33021;&#21147;&#22791;&#21463;&#20105;&#35758;&#65292;&#24182;&#19988;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#21307;&#23398;&#12289;&#31185;&#23398;&#12289;&#27861;&#24459;&#21644;&#25919;&#31574;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#21450;&#20854;&#22240;&#26524;&#25512;&#29702;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#24314;&#26500;&#21644;&#27979;&#37327;&#25928;&#24230;&#23041;&#32961;&#12290;&#22522;&#20110;GPT-3.5&#21644;4&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22240;&#26524;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LLMs&#23637;&#31034;&#20102;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#35299;&#37322;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.  Crucially, LLMs perform these causal tasks while relying on sources of knowledg
&lt;/p&gt;</description></item></channel></rss>