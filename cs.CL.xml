<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#36807;&#28388;&#30340;&#22810;Agent&#38450;&#24481;&#26694;&#26550;AutoDefense&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;LLMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#24120;&#29992;&#25143;&#35831;&#27714;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04783</link><description>&lt;p&gt;
AutoDefense: &#22810;Agent LLM &#38450;&#24481;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04783
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#36807;&#28388;&#30340;&#22810;Agent&#38450;&#24481;&#26694;&#26550;AutoDefense&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;LLMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#24120;&#29992;&#25143;&#35831;&#27714;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#36947;&#24503;&#23545;&#40784;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20197;&#38450;&#27490;&#22312;&#29992;&#25143;&#35831;&#27714;&#26102;&#29983;&#25104;&#26377;&#23475;&#20449;&#24687;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#36807;&#28388;&#30340;&#22810;Agent&#38450;&#24481;&#26694;&#26550;AutoDefense&#65292;&#29992;&#20110;&#20174;LLMs&#20013;&#36807;&#28388;&#26377;&#23475;&#22238;&#22797;&#12290; &#27492;&#26694;&#26550;&#20026;LLM&#20195;&#29702;&#20998;&#37197;&#19981;&#21516;&#35282;&#33394;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#20849;&#21516;&#23436;&#25104;&#38450;&#24481;&#20219;&#21153;&#12290; &#20219;&#21153;&#30340;&#21010;&#20998;&#22686;&#24378;&#20102;LLMs&#30340;&#25972;&#20307;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#65292;&#24182;&#20351;&#20854;&#20182;&#38450;&#24481;&#32452;&#20214;&#20316;&#20026;&#24037;&#20855;&#38598;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290; AutoDefense &#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#35268;&#27169;&#21644;&#31181;&#31867;&#30340;&#24320;&#28304;LLMs&#20316;&#20026;&#20195;&#29702;&#12290; &#36890;&#36807;&#23545;&#22823;&#37327;&#26377;&#23475;&#21644;&#23433;&#20840;&#25552;&#31034;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;AutoDefense&#22312;&#25552;&#39640;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#27491;&#24120;&#29992;&#25143;&#35831;&#27714;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04783v1 Announce Type: cross  Abstract: Despite extensive pre-training and fine-tuning in moral alignment to prevent generating harmful information at user request, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a response-filtering based multi-agent defense framework that filters harmful responses from LLMs. This framework assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. AutoDefense can adapt to various sizes and kinds of open-source LLMs that serve as agents. Through conducting extensive experiments on a large scale of harmful and safe prompts, we validate the effectiveness of the proposed AutoDefense in improving the robustness against jailbreak attacks, while maintaining the performance at normal user request. Our code and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06900</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#35782;&#21035;&#27602;&#24615;&#21527;&#65311;&#32467;&#26500;&#21270;&#27602;&#24615;&#35843;&#26597;&#26694;&#26550;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#36981;&#23432;&#31038;&#20250;&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#23384;&#22312;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#27602;&#24615;&#24230;&#37327;&#20381;&#36182;&#20110;&#22312;&#29305;&#23450;&#27602;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#30721;&#22120;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#22806;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#25152;&#20551;&#23450;&#30340;&#27602;&#24615;&#23450;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#33258;&#21160;&#40065;&#26834;&#24230;&#37327;&#65292;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#22238;&#24212;&#26159;&#21542;&#20855;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#27602;&#24615;&#22240;&#32032;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#24230;&#37327;&#25351;&#26631;LLMs As ToxiciTy Evaluators&#65288;LATTE&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;F1&#24471;&#20998;&#27604;&#29616;&#26377;&#25216;&#26415;&#25351;&#26631;&#25552;&#39640;&#20102;12&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19978;&#28216;&#27602;&#24615;&#23545;&#24230;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#29702;&#35299;&#25968;&#23383;&#65292;&#21487;&#20197;&#36890;&#36807;&#21387;&#32553;&#21644;&#32534;&#30721;&#30340;&#26041;&#24335;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2401.03735</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#29702;&#35299;&#25968;&#23383;
&lt;/p&gt;
&lt;p&gt;
Language Models Understand Numbers, at Least Partially
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#29702;&#35299;&#25968;&#23383;&#65292;&#21487;&#20197;&#36890;&#36807;&#21387;&#32553;&#21644;&#32534;&#30721;&#30340;&#26041;&#24335;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#19981;&#36879;&#26126;&#30340;&#20869;&#37096;&#26426;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#25968;&#23383;&#65292;&#25968;&#23398;&#20013;&#30340;&#22522;&#26412;&#20803;&#32032;&#12290;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;LLMs&#24212;&#35813;&#33021;&#22815;&#22312;&#20854;&#38544;&#34255;&#29366;&#24577;&#20013;&#21387;&#32553;&#25968;&#23383;&#20197;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#21152;&#27861;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#32447;&#24615;&#25506;&#27979;&#22120;&#20174;&#38544;&#34255;&#29366;&#24577;&#20013;&#35835;&#21462;&#36755;&#20837;&#25968;&#23383;&#12290;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;LLMs&#20013;&#23384;&#22312;&#21387;&#32553;&#30340;&#25968;&#23383;&#12290;&#28982;&#32780;&#65292;&#31934;&#30830;&#37325;&#24314;&#21407;&#22987;&#25968;&#23383;&#26159;&#22256;&#38590;&#30340;&#65292;&#34920;&#26126;&#21387;&#32553;&#36807;&#31243;&#21487;&#33021;&#19981;&#26159;&#26080;&#25439;&#30340;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#21033;&#29992;&#32534;&#30721;&#30340;&#25968;&#23383;&#26469;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#65292;&#24182;&#19988;&#35745;&#31639;&#33021;&#21147;&#38543;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#25968;&#23383;&#19978;&#23637;&#29616;&#20986;&#37096;&#20998;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited impressive competence in various tasks, but their opaque internal mechanisms hinder their use in mathematical problems. In this paper, we study a fundamental question: whether language models understand numbers, a basic element in math. Based on an assumption that LLMs should be capable of compressing numbers in their hidden states to solve mathematical problems, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of compressed numbers in LLMs. However, it is difficult to precisely reconstruct the original numbers, indicating that the compression process may not be lossless. Further experiments show that LLMs can utilize encoded numbers to perform arithmetic computations, and the computational ability scales up with the model size. Our preliminary research suggests that LLMs exhibit a partial understanding of number
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25688;&#35201;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#25688;&#35201;&#34920;&#29616;&#25552;&#39640;&#65292;&#32553;&#23567;&#20854;&#19982;&#26377;&#30417;&#30563;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2212.09593</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#25688;&#35201;&#20877;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Summarization Re-ranking. (arXiv:2212.09593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25688;&#35201;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#25688;&#35201;&#34920;&#29616;&#25552;&#39640;&#65292;&#32553;&#23567;&#20854;&#19982;&#26377;&#30417;&#30563;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20219;&#21153;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#20852;&#36215;&#65292;&#20687;PEGASUS&#36825;&#26679;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#22312;&#19979;&#28216;&#25688;&#35201;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#24615;&#33021;&#20173;&#28982;&#26126;&#26174;&#33853;&#21518;&#20110;&#23427;&#20204;&#30340;&#26377;&#30417;&#30563;&#23545;&#24212;&#29289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25688;&#35201;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#26088;&#22312;&#32553;&#23567;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23558;PEGASUS&#30340;&#30456;&#23545;&#24179;&#22343;ROUGE&#25552;&#39640;&#20102;&#26368;&#22810;7.27&#65285;&#65292;ChatGPT&#25552;&#39640;&#20102;&#26368;&#22810;6.86&#65285;&#65307;&#24182;&#19988;&#22312;30&#31181;&#38646;&#26679;&#26412;&#36716;&#31227;&#35774;&#32622;&#65288;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65289;&#20013;&#65292;&#24179;&#22343;&#33719;&#24471;&#20102;7.51&#65285;&#30340;&#30456;&#23545;&#22686;&#30410;&#65288;&#20174;XSum&#21040;WikiHow&#26368;&#39640;&#21487;&#36798;23.73&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly to the supervised setup, we notice a very high variance in quality among summary candidates from these models while only one candidate is kept as the summary output. In this paper, we propose to re-rank summary candidates in an unsupervised manner, aiming to close the performance gap between unsupervised and supervised models. Our approach improves the unsupervised PEGASUS by up to 7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted summarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73% from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on a dataset, evaluating on another).
&lt;/p&gt;</description></item></channel></rss>