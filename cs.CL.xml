<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.00297</link><description>&lt;p&gt;
TRABSA&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;BiLSTM&#21644;Twitter-RoBERTa&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00297
&lt;/p&gt;
&lt;p&gt;
TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#20844;&#20247;&#33286;&#35770;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TRABSA&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#21033;&#29992;&#22312;124M&#26465;&#25512;&#25991;&#19978;&#35757;&#32451;&#30340;RoBERTa&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24046;&#36317;&#65292;&#30830;&#20445;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;32&#20010;&#22269;&#23478;&#21644;&#32654;&#22269;&#21508;&#24030;&#30340;&#25512;&#25991;&#19982;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20845;&#31181;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19977;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26631;&#27880;&#25216;&#26415;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#25216;&#26415;&#20197;&#23454;&#29616;&#26368;&#20339;&#24773;&#24863;&#20998;&#26512;&#25928;&#26524;&#12290;TRABSA&#20197;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#22686;&#30410;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#19968;&#33268;&#30340;&#20248;&#36234;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;SHAP&#21644;LIME&#20998;&#26512;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22686;&#24378;&#20102;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
&lt;/p&gt;</description></item><item><title>User-LLM&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#20351;&#20854;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13598</link><description>&lt;p&gt;
User-LLM: &#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23454;&#29616;&#26377;&#25928;&#30340;LLM&#35821;&#22659;&#21270;
&lt;/p&gt;
&lt;p&gt;
User-LLM: Efficient LLM Contextualization with User Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13598
&lt;/p&gt;
&lt;p&gt;
User-LLM&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#20351;&#20854;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#22797;&#26434;&#19988;&#28508;&#22312;&#22024;&#26434;&#30340;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;User-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#26469;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#12290;&#36825;&#20123;&#23884;&#20837;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20174;&#21508;&#31181;&#29992;&#25143;&#20132;&#20114;&#20013;&#31934;&#28860;&#20986;&#26469;&#30340;&#65292;&#33021;&#22815;&#25429;&#25417;&#28508;&#22312;&#29992;&#25143;&#20559;&#22909;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#36719;&#25552;&#31034;&#23558;&#36825;&#20123;&#29992;&#25143;&#23884;&#20837;&#19982;LLMs&#38598;&#25104;&#36215;&#26469;&#65292;&#20351;LLMs&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#22312;MovieLens&#12289;&#20122;&#39532;&#36874;&#35780;&#35770;&#21644;&#35895;&#27468;&#26412;&#22320;&#35780;&#35770;&#31561;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#21644;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#29992;&#25143;&#30340;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#35821;&#22659;&#21270;&#65292;&#21516;&#26102;&#22312;&#35745;&#31639;&#19978;&#20063;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13598v1 Announce Type: cross  Abstract: Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorpora
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02636</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#23398;&#20064;&#29420;&#31435;&#30340;&#22240;&#26524;&#26426;&#21046;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Learn Independent Causal Mechanisms?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#19981;&#24120;&#35265;&#30340;&#29615;&#22659;&#35774;&#32622;&#25110;&#20998;&#24067;&#21464;&#21270;&#30340;&#20219;&#21153;&#20013;&#65292;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#30446;&#21069;&#36890;&#24120;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#33030;&#24369;&#30340;&#65292;&#22240;&#20026;&#20219;&#21153;&#30340;&#33539;&#22260;&#21487;&#33021;&#26080;&#27861;&#39044;&#27979;&#25110;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#19988;&#20351;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#39069;&#22806;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#37027;&#20123;&#23398;&#20064;&#25277;&#35937;&#21464;&#37327;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#31995;&#32479;&#65292;&#22914;&#22240;&#26524;&#27169;&#22411;&#65292;&#21487;&#20197;&#34920;&#29616;&#20986;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#26356;&#24378;&#31283;&#20581;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#23384;&#22312;&#24182;&#20351;&#29992;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65288;ICMs&#65289;&#65292;&#34920;&#31034;&#21482;&#31232;&#30095;&#20132;&#20114;&#30340;&#39640;&#23618;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#20004;&#20010;&#27010;&#24565;&#65292;&#22312;LLMs&#20013;&#23398;&#20064;ICMs&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;&#22810;&#20010;&#31232;&#30095;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#30340;&#26032;LLM&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability. This issue has usually been alleviated by feeding more training data into the LLM. However, this method is brittle, as the scope of tasks may not be readily predictable or may evolve, and updating the model with new data generally requires extensive additional training. By contrast, systems, such as causal models, that learn abstract variables and causal relationships can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21066;&#24369;&#21407;&#22987;&#25552;&#31034;&#24182;&#36827;&#34892;&#19978;&#19979;&#25991;&#22806;&#25512;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#21463;&#21040;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2401.17692</link><description>&lt;p&gt;
&#29992;&#19978;&#19979;&#25991;&#22806;&#25512;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Problem of Strong Priors in LMs with Context Extrapolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21066;&#24369;&#21407;&#22987;&#25552;&#31034;&#24182;&#36827;&#34892;&#19978;&#19979;&#25991;&#22806;&#25512;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#21463;&#21040;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#21019;&#24314;&#25351;&#20196;&#36319;&#38543;&#21161;&#25163;&#12290;&#20294;&#26159;&#23613;&#31649;&#23427;&#20204;&#26377;&#20248;&#21183;&#65292;LMs&#36824;&#26377;&#19968;&#20123;&#29305;&#27530;&#30340;&#23616;&#38480;&#24615;&#65292;&#27604;&#22914;&#8220;&#24378;&#20808;&#39564;&#8221;&#38382;&#39064;&#65292;&#20854;&#20013;&#27169;&#22411;&#20250;&#22312;&#23545;&#26576;&#20123;&#23616;&#37096;&#36755;&#20837;&#30340;&#21709;&#24212;&#20013;&#23398;&#20064;&#36755;&#20986;&#20856;&#22411;&#30340;&#24310;&#32493;&#65292;&#32780;&#19981;&#32771;&#34385;&#20043;&#21069;&#30340;&#25351;&#20196;&#12290;&#20363;&#22914;&#65292;prompt&#27880;&#20837;&#25915;&#20987;&#21487;&#20197;&#35825;&#20351;&#27169;&#22411;&#24573;&#30053;&#26174;&#24335;&#25351;&#20196;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#27169;&#22411;&#34987;&#35777;&#26126;&#27604;&#31867;&#20284;&#30340;&#36739;&#23567;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#8220;&#21453;&#21521;&#32553;&#25918;&#8221;&#29616;&#35937;&#30340;&#19968;&#20010;&#20363;&#23376;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32531;&#35299;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#26032;&#25216;&#26415;&#65306;&#25105;&#20204;&#37319;&#29992;&#21407;&#22987;&#25351;&#20196;&#38598;&#65292;&#29983;&#25104;&#21407;&#22987;&#25552;&#31034;&#30340;&#21066;&#24369;&#29256;&#26412;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#21463;&#21040;&#24378;&#20808;&#39564;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#28982;&#21518;&#23558;&#24310;&#32493;&#22806;&#25512;&#36828;&#31163;&#21066;&#24369;&#30340;&#25552;&#31034;&#12290;&#36825;&#35753;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#27169;&#22411;&#22914;&#20309;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#29702;&#35299;&#24182;&#20135;&#29983;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have become important tools in a variety of applications, from data processing to the creation of instruction-following assistants. But despite their advantages, LMs have certain idiosyncratic limitations such as the problem of `strong priors', where a model learns to output typical continuations in response to certain, usually local, portions of the input regardless of any earlier instructions. For example, prompt injection attacks can induce models to ignore explicit directives. In some cases, larger models have been shown to be more susceptible to these problems than similar smaller models, an example of the phenomenon of `inverse scaling'. We develop a new technique for mitigating the problem of strong priors: we take the original set of instructions, produce a weakened version of the original prompt that is even more susceptible to the strong priors problem, and then extrapolate the continuation away from the weakened prompt. This lets us infer how the model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#23545;&#38382;&#39064;&#27714;&#35299;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31616;&#27905;&#24615;&#19981;&#20165;&#38477;&#20302;&#20102;&#22238;&#31572;&#38271;&#24230;&#65292;&#19988;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#12290;&#28982;&#32780;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.05618</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#27714;&#35299;&#20013;&#65292;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models. (arXiv:2401.05618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#23545;&#38382;&#39064;&#27714;&#35299;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31616;&#27905;&#24615;&#19981;&#20165;&#38477;&#20302;&#20102;&#22238;&#31572;&#38271;&#24230;&#65292;&#19988;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#12290;&#28982;&#32780;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;(CCoT)&#25552;&#31034;&#12290;&#25105;&#20204;&#23558;&#26631;&#20934;&#30340;CoT&#21644;CCoT&#25552;&#31034;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#20102;&#35299;&#31616;&#27905;&#24615;&#23545;&#22238;&#31572;&#38271;&#24230;&#21644;&#27491;&#30830;&#31572;&#26696;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;(MCQA)&#22522;&#20934;&#30340;&#35780;&#20272;&#12290;CCoT&#23558;GPT-3.5&#21644;GPT-4&#30340;&#24179;&#22343;&#22238;&#31572;&#38271;&#24230;&#20998;&#21035;&#20943;&#23569;&#20102;48.70&#65285;&#65292;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#65292;&#24102;&#26377;CCoT&#30340;GPT-3.5&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;27.69&#65285;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;CCoT&#23548;&#33268;&#27599;&#20010;&#26631;&#35760;&#30340;&#25104;&#26412;&#24179;&#22343;&#38477;&#20302;&#20102;22.67&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20351;&#29992;CoT&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#26469;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;LLM&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#20026;&#30740;&#31350;LLM&#20013;&#36880;&#27493;&#25512;&#29702;&#30340;&#24418;&#25104;&#34892;&#20026;&#30340;AI&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06488</link><description>&lt;p&gt;
SpikeCLIP&#65306;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network. (arXiv:2310.06488v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#24050;&#32463;&#35777;&#26126;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#20013;&#33021;&#22815;&#23454;&#29616;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#33021;&#25928;&#25552;&#39640;&#21644;&#31526;&#21512;&#29983;&#29289;&#21512;&#29702;&#24615;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#21333;&#27169;&#24577;&#30340;SNNs&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#24773;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#21463;&#21040;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#23545;&#40784;&#39044;&#35757;&#32451;+&#21452;&#25439;&#22833;&#24494;&#35843;&#8221;&#30340;&#20004;&#27493;&#39588;&#37197;&#26041;&#65292;&#26469;&#35299;&#20915;&#33033;&#20914;&#35745;&#31639;&#32972;&#26223;&#19979;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24120;&#29992;&#30340;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;SNNs&#21462;&#24471;&#20102;&#19982;&#20854;DNNs&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;SpikeCLIP&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#20445;&#25345;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have demonstrated the capability to achieve comparable performance to deep neural networks (DNNs) in both visual and linguistic domains while offering the advantages of improved energy efficiency and adherence to biological plausibility. However, the extension of such single-modality SNNs into the realm of multimodal scenarios remains an unexplored territory. Drawing inspiration from the concept of contrastive language-image pre-training (CLIP), we introduce a novel framework, named SpikeCLIP, to address the gap between two modalities within the context of spike-based computing through a two-step recipe involving ``Alignment Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across a variety of datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust performance in image classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22522;&#20110;&#20027;&#39064;&#30340;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#65292;&#26080;&#38656;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.07606</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#38899;&#39057;&#20027;&#39064;&#37325;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Audio Topic Reranking using Large Language Models. (arXiv:2309.07606v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22522;&#20110;&#20027;&#39064;&#30340;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#65292;&#26080;&#38656;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35270;&#39057;&#25628;&#32034;&#39033;&#30446;&#36890;&#36807;&#20351;&#29992;&#35270;&#39057;&#29255;&#27573;&#20316;&#20026;&#26597;&#35810;&#39033;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#25991;&#26412;&#26597;&#35810;&#65292;&#26469;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#12290;&#36825;&#20351;&#24471;&#25628;&#32034;&#27169;&#24577;&#26356;&#21152;&#20016;&#23500;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#35828;&#35805;&#32773;&#12289;&#20869;&#23481;&#12289;&#20027;&#39064;&#21644;&#24773;&#24863;&#12290;&#36825;&#20010;&#36807;&#31243;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#23545;&#22823;&#22411;&#23384;&#26723;&#30340;&#39640;&#36895;&#12289;&#28789;&#27963;&#30340;&#25628;&#32034;&#25903;&#25345;&#65292;MVSE&#36890;&#36807;&#29992;&#23884;&#20837;&#34920;&#31034;&#35270;&#39057;&#23646;&#24615;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#26469;&#20943;&#23569;&#26469;&#33258;&#24555;&#36895;&#23384;&#26723;&#25628;&#32034;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot &#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#23384;&#26723;&#38899;&#39057;&#20869;&#23481;&#12290;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#39057;&#23384;&#26723;BBC Rewind&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#20102;&#22522;&#20110;&#20027;&#39064;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#37325;&#26032;&#25490;&#24207;&#21487;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#26816;&#32034;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Multimodal Video Search by Examples (MVSE) project investigates using video clips as the query term for information retrieval, rather than the more traditional text query. This enables far richer search modalities such as images, speaker, content, topic, and emotion. A key element for this process is highly rapid, flexible, search to support large archives, which in MVSE is facilitated by representing video attributes by embeddings. This work aims to mitigate any performance loss from this rapid archive search by examining reranking approaches. In particular, zero-shot reranking methods using large language models are investigated as these are applicable to any video archive audio content. Performance is evaluated for topic-based retrieval on a publicly available video archive, the BBC Rewind corpus. Results demonstrate that reranking can achieve improved retrieval ranking without the need for any task-specific training data.
&lt;/p&gt;</description></item></channel></rss>