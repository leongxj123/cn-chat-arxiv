<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EventGround&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#23558;&#33258;&#30001;&#25991;&#26412;&#19982;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20851;&#32852;&#30340;&#38382;&#39064;&#65292;&#20197;&#36827;&#34892;&#24773;&#22659;&#21270;&#21465;&#20107;&#25512;&#29702;</title><link>https://arxiv.org/abs/2404.00209</link><description>&lt;p&gt;
EventGround&#65306;&#36890;&#36807;&#22522;&#20110;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21465;&#20107;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EventGround&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#23558;&#33258;&#30001;&#25991;&#26412;&#19982;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20851;&#32852;&#30340;&#38382;&#39064;&#65292;&#20197;&#36827;&#34892;&#24773;&#22659;&#21270;&#21465;&#20107;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21465;&#20107;&#25512;&#29702;&#20381;&#36182;&#20110;&#23545;&#25925;&#20107;&#24773;&#22659;&#20013;&#20107;&#20214;&#30340;&#29702;&#35299;&#65292;&#36825;&#38656;&#35201;&#20016;&#23500;&#30340;&#32972;&#26223;&#19990;&#30028;&#30693;&#35782;&#12290;&#20026;&#20102;&#24110;&#21161;&#26426;&#22120;&#21033;&#29992;&#36825;&#31181;&#30693;&#35782;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21487;&#20998;&#20026;&#20004;&#31867;&#12290;&#19968;&#20123;&#20391;&#37325;&#20110;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#20107;&#20214;&#24863;&#30693;&#30446;&#26631;&#26469;&#38544;&#24335;&#24314;&#27169;&#20107;&#20214;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#30772;&#22351;&#30693;&#35782;&#32467;&#26500;&#24182;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#21478;&#19968;&#20123;&#21017;&#23558;&#20107;&#20214;&#30340;&#19990;&#30028;&#30693;&#35782;&#26126;&#30830;&#22320;&#25910;&#38598;&#21040;&#32467;&#26500;&#21270;&#30340;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#28304;&#36827;&#34892;&#33258;&#30001;&#25991;&#26412;&#22788;&#29702;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EventGround&#30340;&#21021;&#27493;&#32508;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#23558;&#33258;&#30001;&#25991;&#26412;&#19982;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20851;&#32852;&#30340;&#38382;&#39064;&#65292;&#20197;&#36827;&#34892;&#24773;&#22659;&#21270;&#21465;&#20107;&#25512;&#29702;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#20107;&#20214;&#34920;&#24449;&#21644;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00209v1 Announce Type: new  Abstract: Narrative reasoning relies on the understanding of eventualities in story contexts, which requires a wealth of background world knowledge. To help machines leverage such knowledge, existing solutions can be categorized into two groups. Some focus on implicitly modeling eventuality knowledge by pretraining language models (LMs) with eventuality-aware objectives. However, this approach breaks down knowledge structures and lacks interpretability. Others explicitly collect world knowledge of eventualities into structured eventuality-centric knowledge graphs (KGs). However, existing research on leveraging these knowledge sources for free-texts is limited. In this work, we propose an initial comprehensive framework called EventGround, which aims to tackle the problem of grounding free-texts to eventuality-centric KGs for contextualized narrative reasoning. We identify two critical problems in this direction: the event representation and sparsi
&lt;/p&gt;</description></item><item><title>DIALECTBENCH&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26041;&#35328;&#12289;&#35821;&#35328;&#21464;&#20307;&#21644;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#23454;&#29616;&#23545;&#19981;&#21516;&#35821;&#35328;&#21464;&#20307;&#19978;NLP&#31995;&#32479;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.11009</link><description>&lt;p&gt;
DIALECTBENCH&#65306;&#19968;&#20010;&#26041;&#35328;&#12289;&#35821;&#35328;&#21464;&#20307;&#21644;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11009
&lt;/p&gt;
&lt;p&gt;
DIALECTBENCH&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26041;&#35328;&#12289;&#35821;&#35328;&#21464;&#20307;&#21644;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#23454;&#29616;&#23545;&#19981;&#21516;&#35821;&#35328;&#21464;&#20307;&#19978;NLP&#31995;&#32479;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11009v1 &#35821;&#31181;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#35821;&#35328;&#25216;&#26415;&#24212;&#35813;&#26681;&#25454;&#20854;&#22312;&#23454;&#38469;&#29992;&#20363;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#21028;&#26029;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#21644;&#35780;&#20272;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#38750;&#26631;&#20934;&#26041;&#35328;&#25110;&#35821;&#35328;&#21464;&#20307;&#65288;&#20197;&#19979;&#31616;&#31216;&#20026;&#21464;&#20307;&#65289;&#24418;&#24335;&#30340;&#35821;&#35328;&#21464;&#20307;. &#22823;&#22810;&#25968;NLP&#22522;&#20934;&#27979;&#35797;&#20165;&#38480;&#20110;&#26631;&#20934;&#35821;&#35328;&#21464;&#20307;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIALECTBENCH&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#35821;&#35328;&#21464;&#20307;&#30340;&#22823;&#35268;&#27169;NLP&#22522;&#20934;&#27979;&#35797;&#65292;&#27719;&#24635;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#20219;&#21153;&#26679;&#26412;&#30340;&#21464;&#20307;&#25968;&#25454;&#38598;&#65288;&#28085;&#30422;281&#31181;&#21464;&#20307;&#30340;10&#20010;&#25991;&#26412;&#32423;&#20219;&#21153;&#65289;&#12290;&#36825;&#20801;&#35768;&#23545;&#19981;&#21516;&#35821;&#35328;&#21464;&#20307;&#19978;NLP&#31995;&#32479;&#24615;&#33021;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#26631;&#20934;&#35821;&#35328;&#21464;&#20307;&#19982;&#38750;&#26631;&#20934;&#35821;&#35328;&#21464;&#20307;&#20043;&#38388;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#22312;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22823;&#37327;&#24615;&#33021;&#24046;&#36317;&#30340;&#35821;&#35328;&#31867;&#32676;&#12290;&#25105;&#20204;&#35748;&#20026;DIALECTBENCH&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#35821;&#35328;NLP&#29366;&#24577;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11009v1 Announce Type: cross  Abstract: Language technologies should be judged on their usefulness in real-world use cases. An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties). Most NLP benchmarks are limited to standard language varieties. To fill this gap, we propose DIALECTBENCH, the first-ever large-scale benchmark for NLP on varieties, which aggregates an extensive set of task-varied variety datasets (10 text-level tasks covering 281 varieties). This allows for a comprehensive evaluation of NLP system performance on different language varieties. We provide substantial evidence of performance disparities between standard and non-standard language varieties, and we also identify language clusters with large performance divergence across tasks. We believe DIALECTBENCH provides a comprehensive view of the current state of NLP for langua
&lt;/p&gt;</description></item><item><title>SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.08370</link><description>&lt;p&gt;
SMART: &#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#23376;&#27169;&#22359;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SMART: Submodular Data Mixture Strategy for Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08370
&lt;/p&gt;
&lt;p&gt;
SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#28041;&#21450;&#22312;&#19968;&#32452;&#20197;&#25351;&#20196;&#26684;&#24335;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#27604;&#20363;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#25214;&#21040;&#21512;&#36866;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#25110;&#20381;&#36182;&#20174;&#19994;&#32773;&#30340;&#30452;&#35273;&#22806;&#65292;&#23578;&#26080;&#31995;&#32479;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SMART&#65288;Submodular data Mixture strAtegy for instRuction Tuning&#65289;- &#19968;&#31181;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#26032;&#39062;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#20998;&#25968;&#26469;&#30830;&#23450;&#28151;&#21512;&#26435;&#37325;&#12290;&#32473;&#23450;&#24494;&#35843;&#39044;&#31639;&#65292;SMART&#37325;&#26032;&#20998;&#37197;&#20219;&#21153;&#38388;&#30340;&#39044;&#31639;&#65292;&#24182;&#20174;&#27599;&#20010;&#20219;&#21153;&#20013;&#36873;&#25321;&#38750;&#20887;&#20313;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SMART&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#20363;&#23376;&#27604;&#20363;&#28151;&#21512;&#21644;&#22343;&#31561;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08370v1 Announce Type: cross  Abstract: Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#21035;&#20013;&#65292;&#25945;&#24072;&#24378;&#21046;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#31532;&#19968;&#26102;&#38388;&#23398;&#20064;&#21040;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#19968;&#33324;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.06963</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
The pitfalls of next-token prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06963
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#21035;&#20013;&#65292;&#25945;&#24072;&#24378;&#21046;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#31532;&#19968;&#26102;&#38388;&#23398;&#20064;&#21040;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#19968;&#33324;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#25285;&#24551;&#65306;&#19968;&#20010;&#20165;&#20165;&#22522;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#24544;&#23454;&#22320;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#32463;&#24120;&#28151;&#28102;&#30340;&#20004;&#20010;&#38454;&#27573; -- &#33258;&#22238;&#24402;&#25512;&#26029;&#21644;&#25945;&#24072;&#24378;&#21046;&#35757;&#32451; -- &#24517;&#39035;&#34987;&#21306;&#21035;&#23545;&#24453;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#19968;&#33324;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#25945;&#24072;&#24378;&#21046;&#22914;&#20309;&#22833;&#36133;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#35745;&#21010;&#20219;&#21153;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;Transformer&#21644;Mamba&#26550;&#26500;&#22312;&#23454;&#36341;&#20013;&#20197;&#36825;&#31181;&#26041;&#24335;&#22833;&#36133; -- &#23613;&#31649;&#20219;&#21153;&#26412;&#36523;&#24456;&#23481;&#26131;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06963v1 Announce Type: cross  Abstract: Can a mere next-token predictor faithfully model human intelligence? We crystallize this intuitive concern, which is fragmented in the literature. As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn. We provide preliminary
&lt;/p&gt;</description></item><item><title>DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.01954</link><description>&lt;p&gt;
DECIDERS&#65306;&#19968;&#31181;&#36890;&#36807;&#27169;&#20223;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#23454;&#29616;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#30340;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DECIDER: A Rule-Controllable Decoding Strategy for Language Generation by Imitating Dual-System Cognitive Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01954
&lt;/p&gt;
&lt;p&gt;
DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20856;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#26576;&#20123;&#30446;&#26631;&#27010;&#24565;&#25511;&#21046;&#25152;&#29983;&#25104;&#25991;&#26412;&#30340;&#24847;&#20041;&#25110;&#39118;&#26684;&#12290;&#29616;&#26377;&#26041;&#27861;&#36807;&#20110;&#20851;&#27880;&#36825;&#20123;&#30446;&#26631;&#26412;&#36523;&#65292;&#23548;&#33268;&#32570;&#20047;&#20851;&#20110;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#39640;&#23618;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#36981;&#24490;&#26576;&#20123;&#35268;&#21017;&#26469;&#22788;&#29702;&#20219;&#21153;&#65292;&#36825;&#20123;&#35268;&#21017;&#19981;&#20165;&#20851;&#27880;&#20110;&#30446;&#26631;&#26412;&#36523;&#65292;&#36824;&#20851;&#27880;&#20110;&#24341;&#21457;&#30446;&#26631;&#21457;&#29983;&#30340;&#35821;&#20041;&#30456;&#20851;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DECIDER&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#32422;&#26463;&#35821;&#35328;&#29983;&#25104;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;DECIDER&#20013;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#37197;&#22791;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#20197;&#39640;&#23618;&#35268;&#21017;&#20316;&#20026;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;DECIDER&#20801;&#35768;&#35268;&#21017;&#20449;&#21495;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#27969;&#20837;PLM&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DECIDER&#33021;&#22815;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#30340;&#35268;&#21017;&#65292;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01954v1 Announce Type: cross  Abstract: Lexicon-based constrained decoding approaches aim to control the meaning or style of the generated text through certain target concepts. Existing approaches over-focus the targets themselves, leading to a lack of high-level reasoning about how to achieve them. However, human usually tackles tasks by following certain rules that not only focuses on the targets but also on semantically relevant concepts that induce the occurrence of targets. In this work, we present DECIDER, a rule-controllable decoding strategy for constrained language generation inspired by dual-system cognitive theory. Specifically, in DECIDER, a pre-trained language model (PLM) is equiped with a logic reasoner that takes high-level rules as input. Then, the DECIDER allows rule signals to flow into the PLM at each decoding step. Extensive experimental results demonstrate that DECIDER can effectively follow given rules to guide generation direction toward the targets i
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#21270;&#25972;&#21512;&#26041;&#38754;&#34920;&#29616;&#26356;&#20339;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#19982;&#29305;&#23450;&#25991;&#21270;&#30340;&#20027;&#23548;&#35821;&#35328;&#25552;&#31034;&#25110;&#31934;&#21046;&#35821;&#35328;&#28151;&#21512;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.13231</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#21270;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Investigating Cultural Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13231
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#21270;&#25972;&#21512;&#26041;&#38754;&#34920;&#29616;&#26356;&#20339;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#19982;&#29305;&#23450;&#25991;&#21270;&#30340;&#20027;&#23548;&#35821;&#35328;&#25552;&#31034;&#25110;&#31934;&#21046;&#35821;&#35328;&#28151;&#21512;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#20037;&#20197;&#26469;&#65292;&#35821;&#35328;&#21644;&#25991;&#21270;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#19968;&#30452;&#26159;&#35821;&#35328;&#20154;&#31867;&#23398;&#39046;&#22495;&#25506;&#32034;&#30340;&#19968;&#20010;&#35838;&#39064;&#12290;&#34987;&#25512;&#24191;&#20026;&#38598;&#20307;&#20154;&#31867;&#30693;&#35782;&#24211;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20986;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#27010;&#25324;&#20102;&#19981;&#21516;&#25991;&#21270;&#25152;&#37319;&#29992;&#30340;&#22810;&#26679;&#30693;&#35782;&#65311;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20004;&#20010;&#32500;&#24230;&#19978;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#25991;&#21270;&#25972;&#21512;&#24615; -- &#39318;&#20808;&#65292;&#24403;&#25552;&#31034;&#20351;&#29992;&#29305;&#23450;&#25991;&#21270;&#30340;&#20027;&#23548;&#35821;&#35328;&#26102;&#65292;&#20854;&#27425;&#65292;&#24403;&#39044;&#20808;&#20351;&#29992;&#35813;&#25991;&#21270;&#37319;&#29992;&#30340;&#35821;&#35328;&#30340;&#31934;&#21046;&#28151;&#21512;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#31038;&#20250;&#23398;&#35843;&#26597;&#26469;&#37327;&#21270;&#25991;&#21270;&#25972;&#21512;&#65292;&#23558;&#27169;&#22411;&#30340;&#21709;&#24212;&#19982;&#23454;&#38469;&#35843;&#26597;&#21442;&#19982;&#32773;&#20316;&#20026;&#21442;&#32771;&#36827;&#34892;&#27604;&#36739;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;LLMs&#29992;&#38463;&#25289;&#20271;&#35821;&#21644;&#33521;&#35821;&#20197;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#28151;&#21512;&#25552;&#31034;&#26469;&#22797;&#21046;&#22312;&#22467;&#21450;&#21644;&#32654;&#22269;&#21508;&#22320;&#36827;&#34892;&#30340;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13231v1 Announce Type: new  Abstract: The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11161</link><description>&lt;p&gt;
PANDA&#65288;Pedantic ANswer-correctness Determination and Adjudication&#65289;&#65306;&#25913;&#36827;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;QA&#65289;&#21482;&#26377;&#22312;&#25105;&#20204;&#30693;&#36947;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#26102;&#25165;&#33021;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#26377;&#36259;&#30340;QA&#31034;&#20363;&#65292;&#24403;&#21069;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#65288;AC&#65289;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20887;&#38271;&#12289;&#33258;&#30001;&#26684;&#24335;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#32570;&#20047;&#25968;&#25454;&#21644;&#27169;&#22411;&#36807;&#22823;&#12290;&#22522;&#20110;LLM&#30340;&#35780;&#20998;&#22120;&#19982;&#20154;&#31867;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20294;&#36825;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#20165;&#22312;&#26377;&#38480;&#30340;QA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#28165;&#26224;&#30340;&#25351;&#21335;&#26469;&#35780;&#20272;&#20174;&#20154;&#31867;QA&#27604;&#36187;&#20013;&#37319;&#32435;&#30340;&#26426;&#22120;QA&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#30830;&#23450;&#21644;&#35009;&#20915;&#65288;Precise ANswer correctness Determination and Adjudication&#65292;PANDA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23567;&#24039;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;AC&#20998;&#31867;&#22120;&#65288;812 KB&#65289;&#65292;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11161v1 Announce Type: cross  Abstract: Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big. LLM based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing clear guidelines for evaluating machine QA adopted from human QA contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10882</link><description>&lt;p&gt;
&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#29992;&#20110;&#23433;&#20840;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Optimizer for Safe Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#26681;&#25454;&#25991;&#23383;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#19981;&#23433;&#20840;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#65292;&#22914;&#33394;&#24773;&#12289;&#39578;&#25200;&#21644;&#38750;&#27861;&#27963;&#21160;&#22270;&#20687;&#12290;&#22522;&#20110;&#22270;&#20687;&#26816;&#26597;&#22120;&#12289;&#27169;&#22411;&#24494;&#35843;&#21644;&#23884;&#20837;&#24335;&#38459;&#27490;&#30340;&#29616;&#26377;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840; T2I &#29983;&#25104;&#30340;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10882v1 Announce Type: cross  Abstract: Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, \textit{we propose the first universal prompt optimizer for safe T2I generation in black-box scenario}. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating in
&lt;/p&gt;</description></item><item><title>&#20998;&#38548;&#31526;&#30340;&#24341;&#20837;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.10645</link><description>&lt;p&gt;
&#20998;&#38548;&#31526;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#25928;&#26524;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Separators Improve Chain-of-Thought Prompting?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10645
&lt;/p&gt;
&lt;p&gt;
&#20998;&#38548;&#31526;&#30340;&#24341;&#20837;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;CoT&#30340;&#22522;&#26412;&#29702;&#24565;&#26159;&#36890;&#36807;&#23558;&#31034;&#20363;&#25918;&#22312;&#36755;&#20837;&#25552;&#31034;&#20013;&#65292;&#35753;LLMs&#36880;&#27493;&#25286;&#35299;&#20182;&#20204;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;CoT&#25552;&#31034;&#30340;&#23494;&#38598;&#32467;&#26500;&#21487;&#33021;&#23548;&#33268;LLMs&#30340;&#35748;&#30693;&#36127;&#33655;&#36807;&#37325;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoT-Sep&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;CoT&#25552;&#31034;&#20013;&#27599;&#20010;&#31034;&#20363;&#30340;&#26411;&#23614;&#31574;&#30053;&#24615;&#22320;&#24212;&#29992;&#20998;&#38548;&#31526;&#12290;&#36825;&#20123;&#20998;&#38548;&#31526;&#26088;&#22312;&#24110;&#21161;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#29702;&#35299;&#20182;&#20204;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#19981;&#20351;&#29992;&#20998;&#38548;&#31526;&#30340;&#26222;&#36890;CoT&#30456;&#27604;&#65292;CoT-Sep&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;GSM-8K&#12289;AQuA&#12289;CSQA&#65289;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#20998;&#38548;&#31526;&#23545;&#22810;&#20010;LLMs&#65288;&#21253;&#25324;GPT-3.5-Turbo&#12289;GPT-4&#21644;LLaMA-27&#65289;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10645v1 Announce Type: cross  Abstract: Chain-of-thought (CoT) prompting is a simple and effective method for improving the reasoning capabilities of Large language models (LLMs). The basic idea of CoT is to let LLMs break down their thought processes step-by-step by putting exemplars in the input prompt. However, the densely structured prompt exemplars of CoT may cause the cognitive overload of LLMs. Inspired by human cognition, we introduce CoT-Sep, a novel method that strategically employs separators at the end of each exemplar in CoT prompting. These separators are designed to help the LLMs understand their thought processes better while reasoning. It turns out that CoT-Sep significantly improves the LLMs' performances on complex reasoning tasks (e.g., GSM-8K, AQuA, CSQA), compared with the vanilla CoT, which does not use separators. We also study the effects of the type and the location of separators tested on multiple LLMs, including GPT-3.5-Turbo, GPT-4, and LLaMA-2 7
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05133</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Personalized Language Modeling from Personalized Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#30446;&#21069;&#20027;&#27969;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#24320;&#21457;&#30340;&#31639;&#27861;&#30340;&#22522;&#26412;&#21069;&#25552;&#22312;&#29992;&#25143;&#20559;&#22909;&#22810;&#26679;&#21270;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#20171;&#32461;&#20102;&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#20219;&#21153;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26222;&#36890;&#30340;RLHF&#21487;&#33021;&#20250;&#23384;&#22312;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20010;&#24615;&#21270;-RLHF&#65288;P-RLHF&#65289;&#26694;&#26550;&#65292;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#27169;&#22411;&#21644;&#35821;&#35328;&#65288;&#25110;&#22870;&#21169;&#65289;&#27169;&#22411;&#12290;&#29992;&#25143;&#27169;&#22411;&#25509;&#25910;&#29992;&#25143;&#20449;&#24687;&#24182;&#36755;&#20986;&#29992;&#25143;&#34920;&#31034;&#12290;&#20854;&#32467;&#26500;&#32534;&#30721;&#20102;&#25105;&#20204;&#23545;&#21453;&#39304;&#25968;&#25454;&#20013;&#29992;&#25143;&#20559;&#22909;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20026;&#20010;&#24615;&#21270;&#22870;&#21169;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#24320;&#21457;&#20102;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is the current dominating framework to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimizat
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#22120;&#65292;&#27169;&#22411;&#21517;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#31574;&#30053;&#25200;&#21160;&#21644;&#22810;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#20943;&#23569;&#38543;&#26426;&#23631;&#34109;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#30340;&#21516;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#23569;&#26679;&#26412;&#21644;&#20010;&#20307;&#36755;&#20837;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00263</link><description>&lt;p&gt;
DetectGPT&#26159;&#21542;&#20805;&#20998;&#21033;&#29992;&#20102;&#25200;&#21160;&#65311;&#22522;&#20110;&#27169;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#36873;&#25321;&#24615;&#25200;&#21160;&#20250;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective Perturbation on Model-Based Contrastive Learning Detector would be Better
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00263
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#22120;&#65292;&#27169;&#22411;&#21517;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#31574;&#30053;&#25200;&#21160;&#21644;&#22810;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#20943;&#23569;&#38543;&#26426;&#23631;&#34109;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#30340;&#21516;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#23569;&#26679;&#26412;&#21644;&#20010;&#20307;&#36755;&#20837;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#20854;&#28389;&#29992;&#30340;&#22686;&#38271;&#20851;&#27880;&#12290;DetectGPT&#26159;&#19968;&#31181;&#38646;-shot&#22522;&#20110;&#24230;&#37327;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#25200;&#21160;&#24182;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;DetectGPT&#30340;&#38543;&#26426;&#25200;&#21160;&#31574;&#30053;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#65292;&#38480;&#21046;&#20102;&#21487;&#21306;&#20998;&#24615;&#21644;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#36923;&#36753;&#22238;&#24402;&#27169;&#22359;&#20381;&#36182;&#20110;&#35774;&#32622;&#38408;&#20540;&#65292;&#36825;&#20250;&#24433;&#21709;&#20010;&#20307;&#25110;&#23567;&#25209;&#37327;&#36755;&#20837;&#30340;&#27867;&#21270;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#22120;&#65292;&#27169;&#22411;&#21517;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#31574;&#30053;&#25200;&#21160;&#26469;&#32531;&#35299;&#38543;&#26426;&#23631;&#34109;&#25152;&#24341;&#36215;&#30340;&#37325;&#35201;&#20449;&#24687;&#20002;&#22833;&#65292;&#24182;&#21033;&#29992;&#22810;&#23545;&#27604;&#23398;&#20064;&#25429;&#25417;&#25200;&#21160;&#26399;&#38388;&#30340;&#38544;&#21547;&#27169;&#24335;&#20449;&#24687;&#65292;&#20415;&#20110;&#23569;&#37327;&#26679;&#26412;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#21517;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#27604;SOTA&#26041;&#27861;&#39640;&#20986;1.20\%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;...
&lt;/p&gt;
&lt;p&gt;
The burgeoning capabilities of large language models (LLMs) have raised growing concerns about abuse. DetectGPT, a zero-shot metric-based unsupervised machine-generated text detector, first introduces perturbation and shows great performance improvement. However, DetectGPT's random perturbation strategy might introduce noise, limiting the distinguishability and further performance improvements. Moreover, its logit regression module relies on setting the threshold, which harms the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel detector, \modelname{}, which uses selective strategy perturbation to relieve the important information loss caused by random masking, and multi-pair contrastive learning to capture the implicit pattern information during perturbation, facilitating few-shot performance. The experiments show that \modelname{} outperforms the SOTA method by 1.20\% in accuracy on average on four public datasets. We further analyze th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10134</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;&#30340;&#26410;&#26469;&#20132;&#36890;&#24773;&#20917;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#24378;&#35843;&#24320;&#21457;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20294;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24182;&#26410;&#30456;&#24212;&#25552;&#39640;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;LLMs&#20027;&#35201;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#26469;&#36827;&#27493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22522;&#26412;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ST-LLM&#23558;&#27599;&#20010;&#20301;&#32622;&#30340;&#26102;&#38388;&#27493;&#38271;&#23450;&#20041;&#20026;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#26469;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#34920;&#31034;&#34987;&#34701;&#21512;&#20197;&#20026;&#27599;&#20010;&#26631;&#35760;&#25552;&#20379;&#32479;&#19968;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06692</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25351;&#23548;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#22312;&#23454;&#29616;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#38646;&#23556;&#20987;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20026;&#25351;&#20196;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#22238;&#31572;&#25152;&#38656;&#30340;&#27880;&#37322;&#24037;&#20316;&#27491;&#22312;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#25351;&#20196;&#25968;&#25454;&#38598;&#25152;&#28085;&#30422;&#30340;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#27744;&#20013;&#30830;&#23450;&#26377;&#29992;&#30340;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#20854;&#22312;LLMs&#29615;&#22659;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#23569;SFT&#30340;&#27880;&#37322;&#25104;&#26412;&#24182;&#35268;&#36991;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23454;&#39564;&#35774;&#35745;&#12290;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#36890;&#24120;&#26368;&#22823;&#21270;&#26576;&#31181;&#19981;&#30830;&#23450;&#24615;&#21644;/&#25110;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#31181;&#29616;&#26377;&#21644;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06461</link><description>&lt;p&gt;
&#20195;&#30721;&#20043;&#38388;&#30340;&#30028;&#38480;&#65306;&#25581;&#31034;&#26426;&#22120;&#21644;&#20154;&#31867;&#31243;&#24207;&#21592;&#20043;&#38388;&#19981;&#21516;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#27169;&#31946;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#28304;&#20195;&#30721;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#23548;&#33268;&#36719;&#20214;&#20135;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20195;&#30721;&#38271;&#24230;&#12289;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#33258;&#28982;&#24615;&#31561;&#23646;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#22266;&#26377;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#29305;&#21035;&#27880;&#24847;&#21040;&#65292;&#20195;&#30721;&#30340;&#32467;&#26500;&#20998;&#21106;&#26159;&#35782;&#21035;&#20854;&#26469;&#28304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#22411;&#26426;&#22120;&#29983;&#25104;&#20195;&#30721;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;DetectGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#23572;&#25991;&#30340;&#35760;&#24518;&#29702;&#35770;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#24847;&#35782;&#21487;&#33021;&#26159;&#19968;&#31181;&#22522;&#20110;&#36825;&#31181;&#23545;&#24212;&#20851;&#31995;&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#29468;&#24819;&#12290;</title><link>http://arxiv.org/abs/2401.02509</link><description>&lt;p&gt;
&#35760;&#24518;&#12289;&#24847;&#35782;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Memory, Consciousness and Large Language Model. (arXiv:2401.02509v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02509
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#23572;&#25991;&#30340;&#35760;&#24518;&#29702;&#35770;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#24847;&#35782;&#21487;&#33021;&#26159;&#19968;&#31181;&#22522;&#20110;&#36825;&#31181;&#23545;&#24212;&#20851;&#31995;&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35748;&#30693;&#31185;&#23398;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#32852;&#31995;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#25581;&#31034;&#20986;&#26469;&#12290;&#22312;&#36825;&#20123;&#32852;&#31995;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29468;&#24819;&#65292;&#21363;LLM&#21644;&#22270;&#23572;&#25991;&#30340;&#35760;&#24518;&#29702;&#35770;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#23545;&#20598;&#20851;&#31995;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#23572;&#25991;&#30340;&#21327;&#21516;&#24341;&#21457;&#65288;SEM&#65289;&#26816;&#32034;&#27169;&#22411;&#21644;LLM&#20013;&#35266;&#23519;&#21040;&#30340;&#26032;&#20852;&#33021;&#21147;&#20043;&#38388;&#30340;&#28508;&#22312;&#23545;&#24212;&#20851;&#31995;&#65292;&#20026;&#25105;&#20204;&#30340;&#29468;&#24819;&#25552;&#20379;&#20102;&#25903;&#25345;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#27979;&#24847;&#35782;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#36825;&#31181;&#23545;&#20598;&#24615;&#30340;&#19968;&#31181;&#26032;&#20852;&#33021;&#21147;&#24418;&#24335;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20854;&#20182;&#24847;&#35782;&#29702;&#35770;&#22914;&#20309;&#19982;&#25105;&#20204;&#30340;&#30740;&#31350;&#30456;&#20132;&#21449;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development in cognitive science and Large Language Models (LLMs), increasing connections have come to light between these two distinct fields. Building upon these connections, we propose a conjecture suggesting the existence of a duality between LLMs and Tulving's theory of memory. We identify a potential correspondence between Tulving's synergistic ecphory model (SEM) of retrieval and the emergent abilities observed in LLMs, serving as supporting evidence for our conjecture. Furthermore, we speculate that consciousness may be considered a form of emergent ability based on this duality. We also discuss how other theories of consciousness intersect with our research.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.04131</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#23450;&#20301;&#36328;&#20219;&#21153;&#24207;&#21015;&#32487;&#32493;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04131
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;Transformer&#27169;&#22411;&#36824;&#21407;&#20026;&#21487;&#35835;&#30340;&#30005;&#36335;&#34920;&#31034;&#65292;&#29992;&#20110;&#23454;&#29616;&#31639;&#27861;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#26469;&#25193;&#23637;&#36825;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#23383;&#12289;&#25968;&#23383;&#35789;&#21644;&#26376;&#20221;&#30340;&#36882;&#22686;&#24207;&#21015;&#12290;&#36890;&#36807;&#24212;&#29992;&#30005;&#36335;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36127;&#36131;&#26816;&#27979;&#24207;&#21015;&#25104;&#21592;&#21644;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#25104;&#21592;&#30340;&#20851;&#38190;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35821;&#20041;&#30456;&#20851;&#24207;&#21015;&#20381;&#36182;&#20110;&#20855;&#26377;&#31867;&#20284;&#20316;&#29992;&#30340;&#20849;&#20139;&#30005;&#36335;&#23376;&#22270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35760;&#24405;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#32534;&#36753;&#36807;&#31243;&#12290;&#36825;&#31181;&#23545;Transformer&#30340;&#26426;&#26800;&#29702;&#35299;&#26159;&#26500;&#24314;&#26356;&#20581;&#22766;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#26356;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;LLM&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#36890;&#36807;&#25913;&#36827;&#25216;&#26415;&#65292;&#22914;Rephrase&#21644;PAL-Tools&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01991</link><description>&lt;p&gt;
&#22635;&#31354;&#39064;&#65306;&#25506;&#32034;&#24182;&#22686;&#24378;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems. (arXiv:2310.01991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;LLM&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#36890;&#36807;&#25913;&#36827;&#25216;&#26415;&#65292;&#22914;Rephrase&#21644;PAL-Tools&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#26399;&#30340;&#25991;&#29486;&#20013;&#24191;&#27867;&#25506;&#35752;&#20102;&#27491;&#21521;&#25512;&#29702;&#65288;&#21363;&#32473;&#23450;&#38382;&#39064;&#25214;&#31572;&#26696;&#65289;&#65292;&#20294;&#36870;&#21521;&#25512;&#29702;&#30456;&#23545;&#36739;&#23569;&#34987;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#35752;&#65306;&#32473;&#23450;&#19968;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#20854;&#31572;&#26696;&#65292;&#22312;&#38382;&#39064;&#20013;&#26377;&#20123;&#32454;&#33410;&#34987;&#30465;&#30053;&#20102;&#65292;LLM&#33021;&#21542;&#26377;&#25928;&#22320;&#36824;&#21407;&#20986;&#32570;&#22833;&#30340;&#20449;&#24687;&#65311;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#20102;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#20462;&#25913;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#36825;&#19968;&#20219;&#21153;&#65306;GSM8k&#12289;SVAMP&#21644;MultiArith&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#27491;&#21521;&#25512;&#29702;&#30456;&#27604;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#27169;&#22411;&#65288;GPT4&#12289;GPT3.5&#12289;PaLM-2&#21644;LLaMa-2&#65289;&#22312;&#36870;&#21521;&#25512;&#29702;&#19978;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#21033;&#29992;&#35813;&#20219;&#21153;&#30340;&#29305;&#23450;&#26684;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#25216;&#26415;&#65306;Rephrase&#23558;&#32473;&#23450;&#30340;&#38382;&#39064;&#37325;&#36848;&#20026;&#19968;&#20010;&#27491;&#21521;&#25512;&#29702;&#38382;&#39064;&#65292;PAL-Tools&#32467;&#21512;&#20102;&#31243;&#24207;&#36741;&#21161;&#30340;LLM&#24605;&#24819;&#65292;&#29983;&#25104;&#19968;&#32452;&#26041;&#31243;&#24335;&#21487;&#20197;&#35299;&#20915;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
While forward reasoning (i.e. find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information?  In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22914;&#20309;&#25903;&#25345;&#25945;&#32946;&#24037;&#20316;&#32773;&#39564;&#35777;&#35780;&#20998;&#31243;&#24207;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;LLMs&#20316;&#20026;&#34917;&#20805;&#35270;&#35282;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#20294;&#20173;&#38656;&#36827;&#19968;&#27493;&#20248;&#21270;&#20854;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11508</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#30701;&#25991;&#26412;&#31572;&#26696;&#33258;&#21160;&#35780;&#20998;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-based Autograding for Short Textual Answers. (arXiv:2309.11508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22914;&#20309;&#25903;&#25345;&#25945;&#32946;&#24037;&#20316;&#32773;&#39564;&#35777;&#35780;&#20998;&#31243;&#24207;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;LLMs&#20316;&#20026;&#34917;&#20805;&#35270;&#35282;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#20294;&#20173;&#38656;&#36827;&#19968;&#27493;&#20248;&#21270;&#20854;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#35797;&#30340;&#35780;&#20998;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#12289;&#21171;&#21160;&#23494;&#38598;&#30340;&#12289;&#20027;&#35266;&#30340;&#12289;&#37325;&#22797;&#30340;&#19988;&#24120;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#21487;&#29992;&#24615;&#21644;&#25968;&#23383;&#21270;&#24102;&#26469;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#28044;&#20837;&#65292; greatly increased autograding textual responses&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#20915;&#31574;&#35282;&#33394;&#20132;&#32473;AI&#27169;&#22411;&#24341;&#36215;&#20102;&#20262;&#29702;&#32771;&#34385;&#65292;&#20027;&#35201;&#28304;&#20110;&#28508;&#22312;&#20559;&#35265;&#21644;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;LLMs&#22914;&#20309;&#25903;&#25345;&#25945;&#32946;&#24037;&#20316;&#32773;&#39564;&#35777;&#20854;&#35780;&#20998;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38024;&#23545;&#33258;&#21160;&#30701;&#25991;&#26412;&#31572;&#26696;&#35780;&#20998;&#65288;ASAG&#65289;&#65292;&#28085;&#30422;&#20102;&#20004;&#20010;&#19981;&#21516;&#35838;&#31243;&#30340;&#21508;&#31181;&#35821;&#35328;&#21644;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;LLMs&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#34917;&#20805;&#30340;&#35270;&#35282;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36824;&#38656;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grading of exams is an important, labor intensive, subjective, repetitive and frequently challenging task. The feasibility of autograding textual responses has greatly increased thanks to the availability of large language models (LLMs) such as ChatGPT and because of the substantial influx of data brought about by digitalization. However, entrusting AI models with decision-making roles raises ethical considerations, mainly stemming from potential biases and issues related to generating false information. Thus, in this manuscript we provide an evaluation of a large language model for the purpose of autograding, while also highlighting how LLMs can support educators in validating their grading procedures. Our evaluation is targeted towards automatic short textual answers grading (ASAG), spanning various languages and examinations from two distinct courses. Our findings suggest that while "out-of-the-box" LLMs provide a valuable tool to provide a complementary perspective, their readiness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10246</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#65306;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#65288;&#32508;&#36848;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22914;&#20309;&#34920;&#31034;&#19981;&#21516;&#30340;&#20449;&#24687;&#27169;&#24335;&#65311;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29702;&#35299;&#29992;&#25143;&#24605;&#32771;&#20869;&#23481;&#30340;&#31995;&#32479;&#65311;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#30740;&#31350;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#31561;&#22823;&#33041;&#35760;&#24405;&#26469;&#22238;&#31572;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#31070;&#32463;&#31185;&#23398;&#30028;&#20026;&#34987;&#21160;&#38405;&#35835;/&#21548;&#35273;/&#35266;&#30475;&#27010;&#24565;&#35789;&#27719;&#12289;&#21465;&#36848;&#12289;&#22270;&#29255;&#21644;&#30005;&#24433;&#30456;&#20851;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#65292;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#30740;&#31350;&#20013;&#30340;&#39069;&#22806;&#24037;&#20855;&#65292;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#32534;&#30721;&#27169;&#22411;&#26088;&#22312;&#33258;&#21160;&#22320;&#29983;&#25104;fMRI&#22823;&#33041;&#34920;&#24449;&#65292;&#32473;&#23450;&#19968;&#20010;&#21050;&#28608;&#12290;&#23427;&#20204;&#22312;&#35780;&#20272;&#21644;&#35786;&#26029;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20197;&#21450;&#35774;&#35745;&#22823;&#33041;&#25439;&#20260;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#35299;&#30721;&#27169;&#22411;&#35299;&#20915;&#20102;&#26681;&#25454;fMRI&#37325;&#26500;&#21050;&#28608;&#30340;&#36870;&#38382;&#39064;&#12290;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#22788;&#29702;&#20449;&#24687;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#37117;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for 
&lt;/p&gt;</description></item><item><title>MenuCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#36741;&#21161;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#19982;&#35774;&#35745;&#24072;&#21327;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#33756;&#21333;&#35774;&#35745;&#24037;&#20855;&#65292;&#21487;&#20197;&#31616;&#21270;&#33756;&#21333;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#38646;/&#23569;&#27425;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.04496</link><description>&lt;p&gt;
MenuCraft: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#33756;&#21333;&#31995;&#32479;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
MenuCraft: Interactive Menu System Design with Large Language Models. (arXiv:2303.04496v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04496
&lt;/p&gt;
&lt;p&gt;
MenuCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#36741;&#21161;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#19982;&#35774;&#35745;&#24072;&#21327;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#33756;&#21333;&#35774;&#35745;&#24037;&#20855;&#65292;&#21487;&#20197;&#31616;&#21270;&#33756;&#21333;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#38646;/&#23569;&#27425;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33756;&#21333;&#31995;&#32479;&#35774;&#35745;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#35768;&#22810;&#35774;&#35745;&#36873;&#39033;&#21644;&#21508;&#31181;&#20154;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MenuCraft&#30340;AI&#36741;&#21161;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#32454;&#21270;&#33756;&#21333;&#31995;&#32479;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#23454;&#29616;&#35774;&#35745;&#24072;&#19982;&#23545;&#35805;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;MenuCraft&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#24335;&#33756;&#21333;&#35774;&#35745;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#33756;&#21333;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#35774;&#35745;&#36873;&#39033;&#30340;&#36731;&#26494;&#23450;&#21046;&#12290;MenuCraft&#36890;&#36807;&#23545;&#35805;&#25903;&#25345;&#21508;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;/&#23569;&#27425;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Menu system design is a challenging task involving many design options and various human factors. For example, one crucial factor that designers need to consider is the semantic and systematic relation of menu commands. However, capturing these relations can be challenging due to limited available resources. With the advancement of neural language models, large language models can utilize their vast pre-existing knowledge in designing and refining menu systems. In this paper, we propose MenuCraft, an AI-assisted designer for menu design that enables collaboration between the designer and a dialogue system to design menus. MenuCraft offers an interactive language-based menu design tool that simplifies the menu design process and enables easy customization of design options. MenuCraft supports a variety of interactions through dialog that allows performing zero/few-shot learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Passau-SFCH&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#29992;&#20110;&#33258;&#21457;&#24189;&#40664;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#30340;&#20998;&#26512;&#21644;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#24189;&#40664;&#20197;&#21450;&#24189;&#40664;&#24773;&#24863;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2209.14272</link><description>&lt;p&gt;
&#36808;&#21521;&#22810;&#27169;&#24577;&#39044;&#27979;&#33258;&#21457;&#24189;&#40664;&#65306;&#19968;&#20221;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21644;&#21021;&#27493;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results. (arXiv:2209.14272v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Passau-SFCH&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#29992;&#20110;&#33258;&#21457;&#24189;&#40664;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#30340;&#20998;&#26512;&#21644;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#24189;&#40664;&#20197;&#21450;&#24189;&#40664;&#24773;&#24863;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#24773;&#24863;&#21644;&#35748;&#30693;&#30340;&#37325;&#35201;&#20803;&#32032;&#12290;&#20854;&#33258;&#21160;&#29702;&#35299;&#21487;&#20197;&#20419;&#36827;&#26356;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20154;&#24615;&#21270;&#12290;&#30446;&#21069;&#30340;&#24189;&#40664;&#26816;&#27979;&#26041;&#27861;&#20165;&#22522;&#20110;&#31574;&#21010;&#25968;&#25454;&#65292;&#19981;&#33021;&#28385;&#36275;&#8220;&#29616;&#23454;&#19990;&#30028;&#8221;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;Passau-Spontaneous Football Coach Humour&#65288;Passau-SFCH&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#32570;&#38519;&#12290;Passau-SFCH&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26681;&#25454;Martin&#30340;&#24189;&#40664;&#39118;&#26684;&#38382;&#21367;&#25552;&#20986;&#30340;&#24189;&#40664;&#23384;&#22312;&#21450;&#20854;&#32500;&#24230;&#65288;&#24773;&#24863;&#21644;&#26041;&#21521;&#65289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#19987;&#23478;&#35774;&#35745;&#30340;&#29305;&#24449;&#12290;&#20998;&#26512;&#20102;&#33258;&#21457;&#24189;&#40664;&#35782;&#21035;&#30340;&#27599;&#31181;&#27169;&#24577;&#65288;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#24189;&#40664;&#21450;&#20854;&#24773;&#24863;&#30340;&#33258;&#21160;&#20998;&#26512;&#65292;&#22810;&#27169;&#24577;&#32852;&#21512;&#20351;&#29992;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humour is a substantial element of human affect and cognition. Its automatic understanding can facilitate a more naturalistic human-device interaction and the humanisation of artificial intelligence. Current methods of humour detection are solely based on staged data making them inadequate for 'real-world' applications. We address this deficiency by introducing the novel Passau-Spontaneous Football Coach Humour (Passau-SFCH) dataset, comprising of about 11 hours of recordings. The Passau-SFCH dataset is annotated for the presence of humour and its dimensions (sentiment and direction) as proposed in Martin's Humor Style Questionnaire. We conduct a series of experiments, employing pretrained Transformers, convolutional neural networks, and expert-designed features. The performance of each modality (text, audio, video) for spontaneous humour recognition is analysed and their complementarity is investigated. Our findings suggest that for the automatic analysis of humour and its sentiment, 
&lt;/p&gt;</description></item></channel></rss>