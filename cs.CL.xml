<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#30340;&#25968;&#23398;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20026;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.18697</link><description>&lt;p&gt;
Invalsi&#22522;&#20934;&#65306;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#30340;&#25968;&#23398;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18697
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#30340;&#25968;&#23398;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20026;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24847;&#22823;&#21033;&#35821;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#26159;&#19968;&#31181;&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#20294;&#30446;&#21069;&#24182;&#27809;&#26377;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#35813;&#35821;&#35328;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#23548;&#33268;&#20102;&#21487;&#29992;&#20110;&#35780;&#20272;&#24847;&#22823;&#21033;&#35821;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#30446;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#30340;&#25968;&#23398;&#29702;&#35299;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#22522;&#20934;&#22522;&#20110;&#24847;&#22823;&#21033;&#23398;&#26657;&#31995;&#32479;&#20869;11&#33267;18&#23681;&#23398;&#29983;&#36827;&#34892;&#30340;&#23454;&#38469;&#27979;&#35797;&#65292;&#24182;&#24050;&#30001;&#22810;&#20301;&#25945;&#23398;&#21644;&#25945;&#32946;&#23398;&#19987;&#23478;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18697v1 Announce Type: new  Abstract: While Italian is by all metrics a high resource language, currently, there are isn't a Language Model pre-trained exclusively in this language. This results in a lower number of available benchmarks to evaluate the performance of language models in Italian.   This work presents two new benchmarks to evaluate the models performance on mathematical understanding and language understanding in Italian. These benchmarks are based on real tests that are undertaken by students of age between 11 and 18 within the Italian school system and have therefore been validated by several experts in didactics and pedagogy.   To validate this dataset we evaluate the performance of 9 language models that are the best performing when writing in Italian, including our own fine-tuned models. We show that this is a challenging benchmark where current language models are bound by 60\% accuracy.   We believe that the release of this dataset paves the way for impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36861;&#36394;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#36164;&#28304;&#32423;&#21035;&#30340;&#26102;&#38388;&#23545;&#40784;&#24615;&#20272;&#35745;&#26377;&#25928;&#25130;&#27490;&#26085;&#26399;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#25130;&#27490;&#26085;&#26399;&#36890;&#24120;&#19982;&#25253;&#36947;&#30340;&#19981;&#21516;&#12290;</title><link>https://arxiv.org/abs/2403.12958</link><description>&lt;p&gt;
&#25968;&#25454;&#30340;&#26102;&#25928;&#24615;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36861;&#36394;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;
&lt;/p&gt;
&lt;p&gt;
Dated Data: Tracing Knowledge Cutoffs in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36861;&#36394;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#36164;&#28304;&#32423;&#21035;&#30340;&#26102;&#38388;&#23545;&#40784;&#24615;&#20272;&#35745;&#26377;&#25928;&#25130;&#27490;&#26085;&#26399;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#25130;&#27490;&#26085;&#26399;&#36890;&#24120;&#19982;&#25253;&#36947;&#30340;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#37197;&#26377;&#22768;&#31216;&#30340;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#65292;&#21363;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#30340;&#26085;&#26399;&#12290;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26368;&#26032;&#20449;&#24687;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#35828;&#27861;&#21482;&#26159;&#34920;&#38754;&#29616;&#35937;&#65306;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#25152;&#26377;&#36164;&#28304;&#26159;&#21542;&#37117;&#20855;&#26377;&#30456;&#21516;&#30340;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#65311;&#27169;&#22411;&#23545;&#36825;&#20123;&#23376;&#38598;&#30340;&#23637;&#31034;&#30693;&#35782;&#26159;&#21542;&#19982;&#23427;&#20204;&#30340;&#25130;&#27490;&#26085;&#26399;&#23494;&#20999;&#30456;&#20851;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#26377;&#25928;&#25130;&#27490;&#26085;&#26399;&#30340;&#27010;&#24565;&#12290;&#36825;&#19982;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#32773;&#25253;&#21578;&#30340;&#25130;&#27490;&#26085;&#26399;&#19981;&#21516;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#23376;&#36164;&#28304;&#21644;&#20027;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#27979;&#25968;&#25454;&#29256;&#26412;&#20043;&#38388;&#30340;&#26102;&#38388;&#23545;&#40784;&#24615;&#26469;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#22312;&#36164;&#28304;&#32423;&#21035;&#30340;&#26377;&#25928;&#25130;&#27490;&#26085;&#26399;&#12290;&#36890;&#36807;&#36825;&#39033;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#25928;&#25130;&#27490;&#26085;&#26399;&#36890;&#24120;&#19982;&#25253;&#21578;&#30340;&#25130;&#27490;&#26085;&#26399;&#19981;&#21516;&#12290;&#20026;&#20102;&#20102;&#35299;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#30452;&#25509;&#30340;&#22823;&#35268;&#27169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12958v1 Announce Type: new  Abstract: Released Large Language Models (LLMs) are often paired with a claimed knowledge cutoff date, or the dates at which training data was gathered. Such information is crucial for applications where the LLM must provide up to date information. However, this statement only scratches the surface: do all resources in the training data share the same knowledge cutoff date? Does the model's demonstrated knowledge for these subsets closely align to their cutoff dates? In this work, we define the notion of an effective cutoff. This is distinct from the LLM designer reported cutoff and applies separately to sub-resources and topics. We propose a simple approach to estimate effective cutoffs on the resource-level temporal alignment of an LLM by probing across versions of the data. Using this analysis, we find that effective cutoffs often differ from reported cutoffs. To understand the root cause of this observation, we conduct a direct large-scale ana
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22686;&#24378;&#19981;&#36807;&#26159;&#26356;&#22909;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#25552;&#39640;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14895</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#24050;&#27515;&#65292;&#25968;&#25454;&#22686;&#24378;&#19975;&#23681;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation is Dead, Long Live Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14895
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#19981;&#36807;&#26159;&#26356;&#22909;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#19968;&#20010;&#32321;&#33635;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#19981;&#26029;&#25552;&#20986;&#26032;&#39062;&#30340;&#25216;&#26415;&#26469;&#21019;&#24314;&#20154;&#24037;&#25968;&#25454;&#65292;&#24050;&#32463;&#22312;&#23567;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#29575;&#65292;&#33267;&#23569;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#20123;&#32467;&#26524;&#65292;&#34920;&#26126;&#32463;&#20856;&#30340;&#25968;&#25454;&#22686;&#24378;&#21482;&#26159;&#19968;&#31181;&#26356;&#22909;&#22320;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#24335;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#20043;&#21069;&#33457;&#26356;&#22810;&#26102;&#38388;&#36827;&#34892;&#24494;&#35843;&#20250;&#25269;&#28040;&#20854;&#25928;&#26524;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#65292;&#22240;&#20026;&#23427;&#22238;&#31572;&#20102;&#26368;&#36817;&#20960;&#24180;&#30041;&#19979;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#21363;&#65306;&#21738;&#31181;DA&#25216;&#26415;&#34920;&#29616;&#26368;&#20339;&#65288;&#21482;&#35201;&#23427;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#35757;&#32451;&#38598;&#36275;&#22815;&#25509;&#36817;&#65292;&#19981;&#20250;&#25439;&#23475;&#35757;&#32451;&#65289;&#65292;&#20026;&#20160;&#20040;DA&#34920;&#29616;&#20986;&#31215;&#26497;&#30340;&#32467;&#26524;&#65288;&#31616;&#21270;&#32593;&#32476;&#35757;&#32451;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#35805;&#20195;&#29702;&#65288;&#22914;ChatGPT&#25110;LLama2&#65289;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20174;&#32780;&#24471;&#20986;&#20102;&#32467;&#35770;&#65292;&#27492;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14895v1 Announce Type: cross  Abstract: Textual data augmentation (DA) is a prolific field of study where novel techniques to create artificial data are regularly proposed, and that has demonstrated great efficiency on small data settings, at least for text classification tasks. In this paper, we challenge those results, showing that classical data augmentation is simply a way of performing better fine-tuning, and that spending more time fine-tuning before applying data augmentation negates its effect. This is a significant contribution as it answers several questions that were left open in recent years, namely~: which DA technique performs best (all of them as long as they generate data close enough to the training set as to not impair training) and why did DA show positive results (facilitates training of network). We furthermore show that zero and few-shot data generation via conversational agents such as ChatGPT or LLama2 can increase performances, concluding that this f
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.14889</link><description>&lt;p&gt;
COBIAS&#65306;&#20559;&#35265;&#35780;&#20272;&#20013;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
COBIAS: Contextual Reliability in Bias Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14889
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;&#22266;&#26377;&#20559;&#35265;&#25968;&#25454;&#35757;&#32451;&#30340;&#12290;&#20197;&#24448;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#30740;&#31350;&#20381;&#36182;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#20110;&#23545;&#20559;&#35265;&#30340;&#26497;&#20854;&#20027;&#35266;&#29702;&#35299;&#32780;&#23384;&#22312;&#22810;&#20010;&#32570;&#38519;&#65292;&#20984;&#26174;&#20986;&#23545;&#24773;&#22659;&#25506;&#32034;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#36755;&#20837;&#29992;&#25143;&#20869;&#23481;&#30340;&#24773;&#22659;&#65292;&#32771;&#34385;&#21040;&#36755;&#20837;&#35821;&#21477;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#20801;&#35768;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#30340;&#26694;&#26550;&#65292;&#32780;&#19981;&#26159;&#20260;&#23475;&#29992;&#25143;&#21442;&#19982;&#30340;&#38450;&#25252;&#35774;&#26045;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;(i) &#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;2287&#20010;&#38472;&#35789;&#28389;&#35843;&#35821;&#21477;&#20197;&#21450;&#28155;&#21152;&#24773;&#22659;&#35201;&#28857;&#30340;&#25968;&#25454;&#38598;&#65307;(ii) &#25105;&#20204;&#24320;&#21457;&#20102;&#38754;&#21521;&#24773;&#22659;&#30340;&#20559;&#35265;&#25351;&#26631;&#21644;&#35780;&#20272;&#20998;&#25968;&#65288;COBIAS&#65289;&#26469;&#35780;&#20272;&#35821;&#21477;&#22312;&#34913;&#37327;&#20559;&#35265;&#26041;&#38754;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26159;&#34913;&#37327;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#24773;&#22659;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#39044;&#27979;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14889v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are trained on inherently biased data. Previous works on debiasing models rely on benchmark datasets to measure model performance. However, these datasets suffer from several pitfalls due to the extremely subjective understanding of bias, highlighting a critical need for contextual exploration. We propose understanding the context of user inputs with consideration of the diverse situations in which input statements are possible. This approach would allow for frameworks that foster bias awareness rather than guardrails that hurt user engagement. Our contribution is twofold: (i) we create a dataset of 2287 stereotyped statements augmented with points for adding context; (ii) we develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to assess statements' contextual reliability in measuring bias. Our metric is a significant predictor of the contextual reliability of bias-benchmark datasets ($
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36339;&#34920;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#20889;&#38382;&#39064;&#21644;&#27874;&#26463;&#25628;&#32034;&#26469;&#20943;&#23569;&#30456;&#20284;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22810;&#36339;&#26816;&#32034;&#20013;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.10666</link><description>&lt;p&gt;
&#24320;&#25918;&#22495;&#25991;&#26412;&#21040;SQL&#30340;&#22810;&#36339;&#34920;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Hop Table Retrieval for Open-Domain Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10666
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36339;&#34920;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#20889;&#38382;&#39064;&#21644;&#27874;&#26463;&#25628;&#32034;&#26469;&#20943;&#23569;&#30456;&#20284;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22810;&#36339;&#26816;&#32034;&#20013;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#25991;&#26412;&#21040;SQL&#26159;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#23427;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#34920;&#65292;&#28982;&#21518;&#29983;&#25104;SQL&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21333;&#36339;&#26816;&#32034;&#26041;&#27861;&#24182;&#26410;&#20851;&#27880;&#25991;&#26412;&#21040;SQL&#25361;&#25112;&#20013;&#30340;&#27169;&#24335;&#38142;&#25509;&#65292;&#36825;&#28041;&#21450;&#21040;&#23558;&#38382;&#39064;&#20013;&#30340;&#23454;&#20307;&#19982;&#34920;&#20013;&#23454;&#20307;&#23545;&#40784;&#65292;&#20027;&#35201;&#20307;&#29616;&#22312;&#20004;&#20010;&#26041;&#38754;&#65306;&#30456;&#20284;&#30340;&#26080;&#20851;&#23454;&#20307;&#21644;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#24102;&#37325;&#20889;&#21644;&#27874;&#26463;&#25628;&#32034;&#30340;&#22810;&#36339;&#34920;&#26816;&#32034;&#65288;Murre&#65289;&#12290;&#20026;&#20102;&#20943;&#23569;&#30456;&#20284;&#30340;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#27599;&#20010;&#36339;&#36291;&#20013;&#26410;&#26816;&#32034;&#21040;&#30340;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#27874;&#26463;&#25628;&#32034;&#32771;&#34385;&#25490;&#21517;&#36739;&#20302;&#30340;&#34920;&#12290;&#20026;&#20102;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;Murre&#22522;&#20110;&#22810;&#20010;&#36339;&#36291;&#20013;&#26816;&#32034;&#21040;&#30340;&#34920;&#37325;&#20889;&#38382;&#39064;&#65292;&#20943;&#23569;&#19982;&#30456;&#20851;&#34920;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;SpiderUnion&#21644;BirdUnion+&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10666v1 Announce Type: new  Abstract: Open-domain text-to-SQL is an important task that retrieves question-relevant tables from massive databases and then generates SQL. However, existing retrieval methods that retrieve in a single hop do not pay attention to the text-to-SQL challenge of schema linking, which is aligning the entities in the question with table entities, reflected in two aspects: similar irrelevant entity and domain mismatch entity. Therefore, we propose our method, the multi-hop table retrieval with rewrite and beam search (Murre). To reduce the effect of the similar irrelevant entity, our method focuses on unretrieved entities at each hop and considers the low-ranked tables by beam search. To alleviate the limitation of domain mismatch entity, Murre rewrites the question based on retrieved tables in multiple hops, decreasing the domain gap with relevant tables. We conduct experiments on SpiderUnion and BirdUnion+, reaching new state-of-the-art results with 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#28041;&#21450;&#20102;&#22823;&#37327;&#30340;&#25968;&#23398;&#38382;&#39064;&#31867;&#22411;&#21644;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.00157</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#36827;&#23637;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Mathematical Reasoning: Progresses and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00157
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#28041;&#21450;&#20102;&#22823;&#37327;&#30340;&#25968;&#23398;&#38382;&#39064;&#31867;&#22411;&#21644;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#26159;&#35780;&#20272;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#30340;&#22522;&#30707;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#33258;&#21160;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#25968;&#23398;&#38382;&#39064;&#30340;&#31867;&#22411;&#38750;&#24120;&#24191;&#27867;&#65292;LLM&#30456;&#20851;&#25216;&#26415;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#20351;&#24471;&#22914;&#20309;&#21028;&#26029;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#30495;&#27491;&#36827;&#23637;&#21644;&#38556;&#30861;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#21253;&#25324;&#20102;&#20197;&#19979;&#22235;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;i&#65289;&#20840;&#38754;&#25506;&#32034;&#21508;&#31181;&#24050;&#32463;&#30740;&#31350;&#30340;&#25968;&#23398;&#38382;&#39064;&#21450;&#20854;&#30456;&#24212;&#25968;&#25454;&#38598;&#65307;ii&#65289;&#30740;&#31350;&#25552;&#20986;&#30340;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;LLM&#25216;&#26415;&#30340;&#33539;&#22260;&#65307;iii&#65289;&#27010;&#36848;&#24433;&#21709;LLM&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#22240;&#32032;&#21644;&#20851;&#27880;&#28857;&#65307;iv&#65289;&#38416;&#26126;&#20173;&#28982;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#30340;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#20197;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.14043</link><description>&lt;p&gt;
&#26397;&#30528;&#30446;&#26631;&#23548;&#21521;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Goal-oriented Large Language Model Prompting: A Survey. (arXiv:2401.14043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#30340;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#20197;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#32780;&#25552;&#31034;&#24037;&#31243;&#22312;&#20248;&#21270;LLM&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#24378;&#35843;&#35774;&#35745;&#25552;&#31034;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#20445;&#25345;&#20154;&#31867;&#36861;&#27714;LLM&#20687;&#20154;&#31867;&#24605;&#32771;&#30340;&#20154;&#31867;&#23398;&#20551;&#35774;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#30340;&#37325;&#35201;&#24615;&#65292;&#35813;&#20844;&#24335;&#25351;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#23558;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#26041;&#27861;&#20998;&#20026;&#20116;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#38454;&#27573;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#24076;&#26395;&#36827;&#19968;&#27493;&#24378;&#35843;&#21644;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown prominent performance in various downstream tasks in which prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not as an overview of current prompt engineering methods, aims to highlight the limitation of designing prompts while holding an anthropomorphic assumption that expects LLMs to think like humans. From our review of 35 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework by summarizing ten applicable tasks. With four future directions proposed, we hope to further emphasize and promote goal-oriented prompt engineering.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25104;&#21151;&#22797;&#21046;&#20102;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#65292;&#20294;&#20854;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#35780;&#32423;&#26377;&#19978;&#21319;&#20559;&#24046;&#21644;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#19982;&#32467;&#26500;&#25928;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.10679</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#36328;&#25991;&#21270;&#20010;&#24615;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Large language models can replicate cross-cultural differences in personality. (arXiv:2310.10679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10679
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25104;&#21151;&#22797;&#21046;&#20102;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#65292;&#20294;&#20854;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#35780;&#32423;&#26377;&#19978;&#21319;&#20559;&#24046;&#21644;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#19982;&#32467;&#26500;&#25928;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#39564;(N=8000)&#26469;&#30830;&#23450;GPT-4&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#12290;&#25105;&#20204;&#36873;&#25321;&#32654;&#22269;&#21644;&#38889;&#22269;&#20316;&#20026;&#25991;&#21270;&#23545;&#27604;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20004;&#20010;&#22269;&#23478;&#30340;&#20154;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20154;&#26684;&#24046;&#24322;&#12290;&#25105;&#20204;&#25805;&#32437;&#20102;&#27169;&#25311;&#30340;&#30446;&#26631;&#65288;&#32654;&#22269; vs. &#38889;&#22269;&#65289;&#65292;&#38382;&#21367;&#30340;&#35821;&#35328;&#65288;&#33521;&#35821; vs. &#38889;&#35821;&#65289;&#20197;&#21450;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4 vs. GPT-3.5&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#22797;&#21046;&#20102;&#27599;&#20010;&#22240;&#23376;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#35780;&#32423;&#20855;&#26377;&#19978;&#21319;&#20559;&#24046;&#65292;&#24182;&#19988;&#27604;&#20154;&#31867;&#26679;&#26412;&#30340;&#21464;&#24322;&#24615;&#26356;&#20302;&#65292;&#20197;&#21450;&#32467;&#26500;&#25928;&#24230;&#36739;&#20302;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35777;&#25454;&#35828;&#26126;LLMs&#21487;&#20197;&#20419;&#36827;&#36328;&#25991;&#21270;&#24515;&#29702;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. Overall, we provide preliminary evidence that LLMs can aid cross-cultural psychological research.
&lt;/p&gt;</description></item><item><title>Sparkles&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23454;&#29616;&#22810;&#22270;&#23545;&#35805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SparklesDialogue&#25968;&#25454;&#38598;&#21644;SparklesEval&#22522;&#20934;&#26469;&#25903;&#25345;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SparklesChat&#22312;&#29702;&#35299;&#22810;&#22270;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16463</link><description>&lt;p&gt;
Sparkles: &#35299;&#38145;&#22810;&#22270;&#32842;&#22825;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models. (arXiv:2308.16463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16463
&lt;/p&gt;
&lt;p&gt;
Sparkles&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23454;&#29616;&#22810;&#22270;&#23545;&#35805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SparklesDialogue&#25968;&#25454;&#38598;&#21644;SparklesEval&#22522;&#20934;&#26469;&#25903;&#25345;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SparklesChat&#22312;&#29702;&#35299;&#22810;&#22270;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#25351;&#20196;&#36319;&#36394;&#25968;&#25454;&#26469;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#20123;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#65288;&#22914;MiniGPT-4&#65289;&#22312;&#28041;&#21450;&#22810;&#20010;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23545;&#35805;&#36830;&#36143;&#24615;&#38754;&#20020;&#25361;&#25112;&#12290;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#36825;&#19968;&#20851;&#38190;&#24212;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SparklesChat&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#22270;&#23545;&#35805;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#12290;&#20026;&#20102;&#25903;&#25345;&#35757;&#32451;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SparklesDialogue&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#21333;&#35789;&#32423;&#20132;&#38169;&#22810;&#22270;&#20687;&#21644;&#25991;&#26412;&#20132;&#20114;&#32780;&#23450;&#21046;&#30340;&#26426;&#22120;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;SparklesEval&#65292;&#19968;&#20010;&#20511;&#21161;GPT&#36741;&#21161;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#22810;&#20010;&#22270;&#20687;&#21644;&#23545;&#35805;&#36718;&#27425;&#20013;&#30340;&#23545;&#35805;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;SparklesChat&#22312;&#29702;&#35299;&#22810;&#22270;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models exhibit enhanced zero-shot performance on various tasks when fine-tuned with instruction-following data. Multimodal instruction-following models extend these capabilities by integrating both text and images. However, existing models such as MiniGPT-4 face challenges in maintaining dialogue coherence in scenarios involving multiple images. A primary reason is the lack of a specialized dataset for this critical application. To bridge these gaps, we present SparklesChat, a multimodal instruction-following model for open-ended dialogues across multiple images. To support the training, we introduce SparklesDialogue, the first machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions. Furthermore, we construct SparklesEval, a GPT-assisted benchmark for quantitatively assessing a model's conversational competence across multiple images and dialogue turns. Our experiments validate the effectiveness of SparklesChat in understa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#37096;&#38376;&#20043;&#38388;&#30340;&#25216;&#26415;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25216;&#26415;&#21019;&#26032;&#20013;&#65292;&#38388;&#25509;&#32852;&#31995;&#21644;&#30452;&#25509;&#32852;&#31995;&#21516;&#31561;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.00014</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#30456;&#20114;&#20381;&#36182;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
A new mapping of technological interdependence. (arXiv:2308.00014v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#37096;&#38376;&#20043;&#38388;&#30340;&#25216;&#26415;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25216;&#26415;&#21019;&#26032;&#20013;&#65292;&#38388;&#25509;&#32852;&#31995;&#21644;&#30452;&#25509;&#32852;&#31995;&#21516;&#31561;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21738;&#20123;&#25216;&#26415;&#32852;&#31995;&#24433;&#21709;&#20102;&#37096;&#38376;&#30340;&#21019;&#26032;&#33021;&#21147;&#65311;&#36825;&#20123;&#25928;&#24212;&#22914;&#20309;&#36890;&#36807;&#25216;&#26415;&#31354;&#38388;&#20256;&#36882;&#65311;&#26412;&#25991;&#20351;&#29992;&#26032;&#39062;&#30340;&#25991;&#26412;&#25366;&#25496;&#21644;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#22238;&#31572;&#20102;&#36825;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#32654;&#22269;&#19987;&#21033;&#21830;&#26631;&#23616;&#65288;USPTO&#65289;&#25480;&#20104;&#30340;650&#19975;&#39033;&#19987;&#21033;&#30340;&#25991;&#26412;&#65292;&#24182;&#24212;&#29992;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#21322;&#20010;&#19990;&#32426;&#65288;&#20174;1976&#24180;&#21040;2021&#24180;&#65289;&#26399;&#38388;&#19981;&#21516;&#37096;&#38376;&#20043;&#38388;&#30340;&#25216;&#26415;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#23384;&#22312;&#20110;&#25216;&#26415;&#39046;&#22495;&#20043;&#38388;&#30340;&#20840;&#35889;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#19987;&#21033;&#25991;&#26412;&#21253;&#21547;&#20102;&#24448;&#24448;&#26080;&#27861;&#36890;&#36807;&#20256;&#32479;&#30340;&#21019;&#26032;&#25351;&#26631;&#65288;&#20363;&#22914;&#19987;&#21033;&#24341;&#29992;&#65289;&#25429;&#25417;&#21040;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#32593;&#32476;&#20998;&#26512;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#38388;&#25509;&#32852;&#31995;&#21644;&#30452;&#25509;&#32852;&#31995;&#21516;&#31561;&#37325;&#35201;&#65292;&#24182;&#19988;&#21069;&#32773;&#22823;&#37096;&#20998;&#20351;&#29992;&#20256;&#32479;&#30340;&#38388;&#25509;&#32852;&#31995;&#24230;&#37327;&#26041;&#27861;&#65288;&#22914;Leontief&#36870;&#30697;&#38453;&#65289;&#24448;&#24448;&#20250;&#34987;&#38544;&#34255;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#20914;&#20987;&#21709;&#24212;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Which technological linkages affect the sector's ability to innovate? How do these effects transmit through the technology space? This paper answers these two key questions using novel methods of text mining and network analysis. We examine technological interdependence across sectors over a period of half a century (from 1976 to 2021) by analyzing the text of 6.5 million patents granted by the United States Patent and Trademark Office (USPTO), and applying network analysis to uncover the full spectrum of linkages existing across technology areas. We demonstrate that patent text contains a wealth of information often not captured by traditional innovation metrics, such as patent citations. By using network analysis, we document that indirect linkages are as important as direct connections and that the former would remain mostly hidden using more traditional measures of indirect linkages, such as the Leontief inverse matrix. Finally, based on an impulse-response analysis, we illustrate 
&lt;/p&gt;</description></item></channel></rss>