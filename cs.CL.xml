<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#26041;&#27861;THERMOMETER&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#25968;&#25454;&#30340;&#36741;&#21161;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#20934;&#30830;&#24615;&#20445;&#25345;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#21709;&#24212;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.08819</link><description>&lt;p&gt;
&#28201;&#24230;&#35745;&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Thermometer: Towards Universal Calibration for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08819
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#26041;&#27861;THERMOMETER&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#25968;&#25454;&#30340;&#36741;&#21161;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#20934;&#30830;&#24615;&#20445;&#25345;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#21709;&#24212;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#35265;&#30340;&#24178;&#39044;&#25514;&#26045;&#22914;&#25351;&#20196;&#35843;&#25972;&#36890;&#24120;&#20250;&#23548;&#33268;&#26657;&#20934;&#19981;&#20339;&#30340;LLMs&#12290;&#23613;&#31649;&#26657;&#20934;&#22312;&#20256;&#32479;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#25506;&#35752;&#65292;&#20294;&#23545;LLMs&#36827;&#34892;&#26657;&#20934;&#20855;&#26377;&#29420;&#29305;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#19981;&#20165;&#26469;&#33258;LLMs&#30340;&#20005;&#26684;&#35745;&#31639;&#35201;&#27714;&#65292;&#20063;&#26469;&#33258;&#23427;&#20204;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20351;&#23427;&#20204;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;LLMs&#30340;&#26657;&#20934;&#26041;&#27861;THERMOMETER&#12290;THERMOMETER&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#30340;&#36741;&#21161;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#20934;LLM&#12290;&#23427;&#22312;&#35745;&#31639;&#19978;&#25928;&#29575;&#39640;&#65292;&#20445;&#25345;&#20102;LLM&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#26032;&#20219;&#21153;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#26657;&#20934;&#21709;&#24212;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#30340;&#24191;&#27867;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08819v1 Announce Type: cross  Abstract: We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#21069;&#27839;&#27169;&#22411;&#23578;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#24357;&#34917;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.08002</link><description>&lt;p&gt;
&#35757;&#32451;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#22635;&#34917;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#65306;&#20197;&#25918;&#23556;&#23398;&#25104;&#20687;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#21069;&#27839;&#27169;&#22411;&#23578;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#24357;&#34917;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#22823;&#22522;&#30784;&#27169;&#22411;&#30340;&#23610;&#24230;&#35268;&#24459;&#21644;&#38750;&#20961;&#34920;&#29616;&#28608;&#21169;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24320;&#21457;&#21644;&#21033;&#29992;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#19968;&#20123;&#29983;&#29289;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26089;&#26399;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20043;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#12290;&#20687;GPT-4V&#36825;&#26679;&#30340;&#21069;&#27839;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#20173;&#23384;&#22312;&#37325;&#22823;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#35775;&#38382;&#12289;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#21512;&#35268;&#31561;&#23454;&#38469;&#38382;&#39064;&#20351;&#20020;&#24202;&#21307;&#29983;&#38590;&#20197;&#30452;&#25509;&#22312;&#31169;&#20154;&#24739;&#32773;&#25968;&#25454;&#19978;&#20351;&#29992;&#31169;&#20154;&#25176;&#31649;&#30340;&#26368;&#20808;&#36827;&#22823;&#22411;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;SMMs&#65289;&#26469;&#22635;&#34917;&#26410;&#28385;&#36275;&#30340;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#32435;&#20837;&#65292;&#24182;&#20391;&#37325;&#20110;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08002v1 Announce Type: new  Abstract: The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications. Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data. In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GlossLM&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#23383;&#38388;&#27880;&#37322;&#30340;&#26377;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.06399</link><description>&lt;p&gt;
GlossLM: &#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#23383;&#38388;&#27880;&#37322;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GlossLM&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#23383;&#38388;&#27880;&#37322;&#30340;&#26377;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25991;&#29486;&#23398;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#20197;&#24418;&#24335;&#22914;&#25991;&#23383;&#38388;&#27880;&#37322;&#25991;&#26412;&#65288;IGT&#65289;&#30340;&#26041;&#24335;&#21019;&#24314;&#24102;&#27880;&#37322;&#30340;&#25991;&#26412;&#65292;IGT&#20197;&#36880;&#35789;&#32032;&#30340;&#26684;&#24335;&#25429;&#25417;&#20102;&#31934;&#32454;&#30340;&#24418;&#24577;&#21477;&#27861;&#20998;&#26512;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#25506;&#32034;&#20102;&#33258;&#21160;&#29983;&#25104;IGT&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35821;&#35328;&#20998;&#26512;&#30340;&#26102;&#38388;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#35821;&#35328;&#65288;&#23588;&#20854;&#26159;&#38656;&#35201;&#20445;&#25252;&#30340;&#35821;&#35328;&#65289;&#32570;&#20047;&#36275;&#22815;&#30340;IGT&#25968;&#25454;&#26469;&#35757;&#32451;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#34987;&#25552;&#20986;&#20316;&#20026;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#26368;&#22823;&#24050;&#26377;IGT&#25968;&#25454;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;1.8k&#31181;&#35821;&#35328;&#30340;&#36229;&#36807;45&#19975;&#20010;&#20363;&#23376;&#65292;&#20197;&#20415;&#36827;&#34892;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;IGT&#29983;&#25104;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#37096;&#20998;&#35821;&#26009;&#24211;&#19978;&#23545;&#19968;&#20010;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36827;&#19968;&#27493;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20998;&#21106;&#25968;&#25454;&#21644;&#22823;&#22411;&#21333;&#35821;&#25968;&#25454;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06399v1 Announce Type: new  Abstract: A key aspect of language documentation is the creation of annotated text in a format such as interlinear glossed text (IGT), which captures fine-grained morphosyntactic analyses in a morpheme-by-morpheme format. Prior work has explored methods to automatically generate IGT in order to reduce the time cost of language analysis. However, many languages (particularly those requiring preservation) lack sufficient IGT data to train effective models, and crosslingual transfer has been proposed as a method to overcome this limitation.   We compile the largest existing corpus of IGT data from a variety of sources, covering over 450k examples across 1.8k languages, to enable research on crosslingual transfer and IGT generation. Then, we pretrain a large multilingual model on a portion of this corpus, and further finetune it to specific languages. Our model is competitive with state-of-the-art methods for segmented data and large monolingual datas
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.05750</link><description>&lt;p&gt;
&#35299;&#35835;AI&#31508;: &#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#25216;&#26415;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05750
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#24443;&#24213;&#39072;&#35206;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24191;&#27867;&#30340;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#65292;&#38656;&#35201;&#28145;&#20837;&#23457;&#26597;&#12289;&#20262;&#29702;&#23457;&#26597;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#25506;&#32034;&#20102;&#29616;&#26377;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#37325;&#28857;&#26159;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#20316;&#20026;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#35780;&#20272;&#20102;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#39046;&#22495;&#38480;&#21046;&#30340;&#26032;&#39062;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#22686;&#24378;&#21327;&#20316;&#26234;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#12289;&#25913;&#21892;AI&#27169;&#22411;&#12289;&#26377;&#25928;&#22242;&#38431;&#21512;&#20316;&#12289;&#36947;&#24503;&#32771;&#34385;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04931</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#21512;&#20316;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Human-AI Teaming with Large Pre-Trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#22686;&#24378;&#21327;&#20316;&#26234;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#12289;&#25913;&#21892;AI&#27169;&#22411;&#12289;&#26377;&#25928;&#22242;&#38431;&#21512;&#20316;&#12289;&#36947;&#24503;&#32771;&#34385;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36805;&#36895;&#21457;&#23637;&#30340;&#26223;&#35266;&#20013;&#65292;&#20154;&#31867;&#26234;&#33021;&#21644;AI&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#65288;HAI&#65289;&#21512;&#20316;&#65292;&#24050;&#25104;&#20026;&#25512;&#36827;&#38382;&#39064;&#35299;&#20915;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#22522;&#30707;&#12290;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPtM&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25913;&#21464;&#20102;&#36825;&#19968;&#26223;&#35266;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#26469;&#29702;&#35299;&#21644;&#39044;&#27979;&#22797;&#26434;&#27169;&#24335;&#65292;&#20026;&#20154;&#31867;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;LPtMs&#19982;HAI&#30340;&#20851;&#38190;&#25972;&#21512;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#22686;&#24378;&#21327;&#20316;&#26234;&#33021;&#12290;&#37325;&#28857;&#25506;&#35752;&#20102;LPtMs&#22312;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#26041;&#38754;&#30340;&#21327;&#21516;&#28508;&#21147;&#65292;&#35752;&#35770;&#20102;&#36825;&#31181;&#21327;&#20316;&#23545;AI&#27169;&#22411;&#25913;&#36827;&#12289;&#26377;&#25928;&#30340;&#22242;&#38431;&#21512;&#20316;&#12289;&#36947;&#24503;&#32771;&#34385;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#24433;&#21709;&#12290;&#36890;&#36807;&#36825;&#19968;&#25506;&#32034;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;LPtM&#22686;&#24378;HAI&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04931v1 Announce Type: new  Abstract: In the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the synergistic potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;RIDERS&#26469;&#35299;&#37322;&#21644;&#20943;&#36731;&#26377;&#23475;CoT&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.18344</link><description>&lt;p&gt;
&#19987;&#27880;&#20110;&#20320;&#30340;&#38382;&#39064;&#65281;&#35299;&#37322;&#21644;&#20943;&#36731;&#24120;&#35782;&#25512;&#29702;&#20013;&#30340;&#26377;&#23475;CoT&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18344
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;RIDERS&#26469;&#35299;&#37322;&#21644;&#20943;&#36731;&#26377;&#23475;CoT&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;Chain-of-Thought&#65288;CoT&#65289;&#31561;&#22686;&#24378;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#31867;&#20284;CoT&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#21407;&#26412;&#27491;&#30830;&#30340;&#31572;&#26696;&#21464;&#24471;&#38169;&#35823;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26377;&#23475;&#30340;CoT&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#37322;&#21644;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#23646;&#24615;&#36319;&#36394;&#21644;&#22240;&#26524;&#36319;&#36394;&#26041;&#27861;&#26469;&#25506;&#31350;LLM&#22312;CoT&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#12290;&#36890;&#36807;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#22312;&#29983;&#25104;&#25512;&#29702;&#25110;&#31572;&#26696;&#26102;&#23384;&#22312;&#26469;&#33258;&#38382;&#39064;&#30340;&#20449;&#24687;&#20002;&#22833;&#29616;&#35937;&#22312;&#27973;&#23618;&#27880;&#24847;&#21147;&#23618;&#20013;&#12290;&#22522;&#20110;&#25506;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;RIDERS&#65288;Residual decodIng and sERial-position Swap&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#35299;&#30721;&#21644;&#24207;&#21015;&#20301;&#32622;&#30340;&#35282;&#24230;&#34917;&#20607;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#20111;&#32570;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18344v1 Announce Type: new  Abstract: Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20855;&#26377;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#21644;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16040</link><description>&lt;p&gt;
EHRNoteQA&#65306;&#29992;&#20110;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16040
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20855;&#26377;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#21644;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#22312;MIMIC-IV&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#30001;&#19977;&#20301;&#21307;&#30103;&#19987;&#23478;&#22242;&#38431;&#31934;&#24515;&#31574;&#21010;&#20102;&#21253;&#21547;962&#20010;&#29420;&#29305;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#29305;&#23450;&#24739;&#32773;&#30340;EHR&#20020;&#24202;&#31508;&#35760;&#30456;&#20851;&#32852;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;EHR&#30340;&#22522;&#20934;&#19981;&#21516;&#30340;&#26159;&#65306;&#39318;&#20808;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#22312;&#33258;&#21160;&#35780;&#20272;&#30340;&#32972;&#26223;&#19979;&#26377;&#25928;&#35780;&#20272;LLMs&#30340;&#24471;&#20998;&#24615;&#33021;&#65292;&#19982;&#20854;&#20182;&#26684;&#24335;&#30456;&#27604;&#12290;&#20854;&#27425;&#65292;&#23427;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#25165;&#33021;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65292;&#21453;&#26144;&#20102;&#23454;&#38469;&#20020;&#24202;&#20915;&#31574;&#21046;&#23450;&#30340;&#22797;&#26434;&#24615;&#65292;&#21307;&#29983;&#38656;&#35201;&#23457;&#26597;&#22823;&#37327;&#24739;&#32773;&#30149;&#21490;&#35760;&#24405;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16040v1 Announce Type: new  Abstract: This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows: Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various large language models
&lt;/p&gt;</description></item><item><title>MobileLLM&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#65292;&#37319;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#12289;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#19988;&#20165;&#26377;&#26497;&#23567;&#24310;&#36831;&#24320;&#38144;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14905</link><description>&lt;p&gt;
MobileLLM&#65306;&#20248;&#21270;&#20122;&#21313;&#20159;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#35774;&#22791;&#31471;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14905
&lt;/p&gt;
&lt;p&gt;
MobileLLM&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#65292;&#37319;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#12289;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#19988;&#20165;&#26377;&#26497;&#23567;&#24310;&#36831;&#24320;&#38144;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#31227;&#21160;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36843;&#20999;&#38656;&#27714;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20113;&#25104;&#26412;&#21644;&#24310;&#36831;&#38382;&#39064;&#19981;&#26029;&#22686;&#21152;&#25152;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35774;&#35745;&#20855;&#26377;&#19981;&#21040;&#21313;&#20159;&#21442;&#25968;&#30340;&#39030;&#32423;LLMs&#65292;&#36825;&#26159;&#31227;&#21160;&#37096;&#32626;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#24378;&#35843;&#25968;&#25454;&#21644;&#21442;&#25968;&#25968;&#37327;&#22312;&#30830;&#23450;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20122;&#21313;&#20159;&#35268;&#27169;LLMs&#30340;&#27169;&#22411;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#65292;&#20877;&#21152;&#19978;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#32593;&#32476;&#65292;&#31216;&#20026;MobileLLM&#65292;&#20854;&#22312;&#23558;&#36817;125M/350M&#20808;&#36827;&#27169;&#22411;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;&#24778;&#20154;&#30340;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31435;&#21363;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;&#65292;&#19981;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#65292;&#19988;&#20165;&#20855;&#26377;&#26497;&#23567;&#30340;&#24310;&#36831;&#24320;&#38144;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;MobileLLM-L
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14905v1 Announce Type: cross  Abstract: This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Daisy-TTS&#35774;&#35745;&#65292;&#36890;&#36807;&#22768;&#35843;&#23884;&#20837;&#20998;&#35299;&#65292;&#27169;&#25311;&#20102;&#26356;&#24191;&#27867;&#30340;&#24773;&#24863;&#33539;&#22260;&#65292;&#21253;&#25324; primary emotions&#12289;secondary emotions&#12289;intensity-level &#21644; emotions polarity&#12290;</title><link>https://arxiv.org/abs/2402.14523</link><description>&lt;p&gt;
Daisy-TTS: &#36890;&#36807;&#22768;&#35843;&#23884;&#20837;&#20998;&#35299;&#27169;&#25311;&#26356;&#24191;&#27867;&#30340;&#24773;&#24863;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Daisy-TTS&#35774;&#35745;&#65292;&#36890;&#36807;&#22768;&#35843;&#23884;&#20837;&#20998;&#35299;&#65292;&#27169;&#25311;&#20102;&#26356;&#24191;&#27867;&#30340;&#24773;&#24863;&#33539;&#22260;&#65292;&#21253;&#25324; primary emotions&#12289;secondary emotions&#12289;intensity-level &#21644; emotions polarity&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32463;&#24120;&#20197;&#22810;&#26041;&#38754;&#30340;&#26041;&#24335;&#21475;&#22836;&#34920;&#36798;&#24773;&#24863;&#65292;&#23427;&#20204;&#22312;&#24378;&#24230;&#19978;&#21487;&#33021;&#26377;&#25152;&#21464;&#21270;&#65292;&#34920;&#36798;&#30340;&#19981;&#20165;&#26159;&#21333;&#19968;&#30340;&#24773;&#24863;&#65292;&#36824;&#21487;&#33021;&#26159;&#21508;&#31181;&#24773;&#24863;&#30340;&#28151;&#21512;&#20307;&#12290;&#36825;&#31181;&#24191;&#27867;&#30340;&#24773;&#24863;&#33539;&#22260;&#22312;&#24773;&#24863;&#32467;&#26500;&#27169;&#22411;&#20013;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#35813;&#27169;&#22411;&#23558;&#21508;&#31181;&#24773;&#24863;&#34920;&#31034;&#20026;&#21407;&#22987;&#24773;&#24863;&#30340;&#27966;&#29983;&#20135;&#21697;&#65292;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#24378;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#25991;&#26412;&#36716;&#35821;&#38899;&#35774;&#35745;&#65292;&#26088;&#22312;&#27169;&#25311;&#22522;&#20110;&#32467;&#26500;&#27169;&#22411;&#30340;&#26356;&#24191;&#27867;&#24773;&#24863;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35774;&#35745;Daisy-TTS&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22768;&#35843;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#23398;&#20064;&#20316;&#20026;&#24773;&#24863;&#20195;&#29702;&#30340;&#21487;&#20998;&#31163;&#30340;&#22768;&#35843;&#23884;&#20837;&#12290;&#36825;&#31181;&#24773;&#24863;&#34920;&#31034;&#20351;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#65306;&#65288;1&#65289;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#21407;&#22987;&#24773;&#24863;&#65292;&#65288;2&#65289;&#20316;&#20026;&#21407;&#22987;&#24773;&#24863;&#30340;&#28151;&#21512;&#20307;&#30340;&#27425;&#32423;&#24773;&#24863;&#65292;&#65288;3&#65289;&#36890;&#36807;&#35843;&#25972;&#24773;&#24863;&#23884;&#20837;&#26469;&#23454;&#29616;&#24378;&#24230;&#32423;&#21035;&#65292;&#65288;4&#65289;&#36890;&#36807;&#21542;&#23450;&#24773;&#24863;&#23884;&#20837;&#26469;&#23454;&#29616;&#24773;&#24863;&#26497;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14523v1 Announce Type: new  Abstract: We often verbally express emotions in a multifaceted manner, they may vary in their intensities and may be expressed not just as a single but as a mixture of emotions. This wide spectrum of emotions is well-studied in the structural model of emotions, which represents variety of emotions as derivative products of primary emotions with varying degrees of intensity. In this paper, we propose an emotional text-to-speech design to simulate a wider spectrum of emotions grounded on the structural model. Our proposed design, Daisy-TTS, incorporates a prosody encoder to learn emotionally-separable prosody embedding as a proxy for emotion. This emotion representation allows the model to simulate: (1) Primary emotions, as learned from the training samples, (2) Secondary emotions, as a mixture of primary emotions, (3) Intensity-level, by scaling the emotion embedding, and (4) Emotions polarity, by negating the emotion embedding. Through a series of
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;M4GT-Bench&#65292;&#28041;&#21450;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65292;&#21253;&#25324;&#21333;&#35821;&#21644;&#22810;&#35821;&#31181;MGT&#26816;&#27979;&#12289;&#22810;&#27169;&#22411;&#26816;&#27979;&#21644;&#20154;&#26426;&#28151;&#21512;&#25991;&#26412;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.11175</link><description>&lt;p&gt;
M4GT-Bench: &#29992;&#20110;&#40657;&#30418;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11175
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;M4GT-Bench&#65292;&#28041;&#21450;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65292;&#21253;&#25324;&#21333;&#35821;&#21644;&#22810;&#35821;&#31181;MGT&#26816;&#27979;&#12289;&#22810;&#27169;&#22411;&#26816;&#27979;&#21644;&#20154;&#26426;&#28151;&#21512;&#25991;&#26412;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24102;&#26469;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65288;MGT&#65289;&#22312;&#19981;&#21516;&#28192;&#36947;&#30340;&#28608;&#22686;&#65292;&#36825;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#28508;&#22312;&#28389;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#30340;&#20851;&#27880;&#12290;&#35782;&#21035;&#21644;&#21306;&#20998;&#36825;&#31181;&#20869;&#23481;&#19982;&#30495;&#23454;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#23545;&#20110;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#12289;&#20445;&#25345;&#25945;&#32946;&#21644;&#31185;&#23398;&#39046;&#22495;&#30340;&#23436;&#25972;&#24615;&#20197;&#21450;&#20445;&#25345;&#36890;&#20449;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#28041;&#21450;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#29983;&#25104;&#22120;&#30340;&#26032;&#22522;&#20934;M4GT-Bench&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#23427;&#38024;&#23545;&#19977;&#20010;&#20219;&#21153;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#38598;&#21512;&#65306;&#65288;1&#65289;&#21333;&#35821;&#21644;&#22810;&#35821;&#31181;&#20108;&#20998;&#31867;MGT&#26816;&#27979;&#65307;&#65288;2&#65289;&#22810;&#27169;&#22411;&#26816;&#27979;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#29305;&#23450;&#27169;&#22411;&#65307;&#20197;&#21450;&#65288;3&#65289;&#20154;&#26426;&#28151;&#21512;&#25991;&#26412;&#26816;&#27979;&#65292;&#24212;&#30830;&#23450;&#19968;&#20010;&#35789;&#36793;&#30028;&#26469;&#30028;&#23450;MGT&#21644;&#20154;&#24037;&#25776;&#20889;&#20869;&#23481;&#12290;&#23545;&#20110;&#20219;&#21153;2&#30340;&#20154;&#31867;&#35780;&#20272;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11175v1 Announce Type: new  Abstract: The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark involving multilingual, multi-domain and multi-generator for MGT detection -- M4GT-Bench. It is collected for three task formulations: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection identifies which particular model generates the text; and (3) human-machine mixed text detection, where a word boundary delimiting MGT from human-written content should be determined. Human evaluation for Task 2 shows
&lt;/p&gt;</description></item><item><title>NutePrune&#26159;&#19968;&#31181;&#39640;&#25928;&#36880;&#28176;&#21098;&#26525;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#36733;&#19968;&#20010;&#23436;&#25972;&#27169;&#22411;&#24182;&#23558;&#20854;&#19982;&#25513;&#30721;&#21644;LoRA&#27169;&#22359;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#32467;&#26500;&#21098;&#26525;&#12290;</title><link>https://arxiv.org/abs/2402.09773</link><description>&lt;p&gt;
NutePrune: &#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36880;&#28176;&#21098;&#26525;&#26041;&#27861;&#65292;&#22810;&#20010;&#25945;&#24072;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09773
&lt;/p&gt;
&lt;p&gt;
NutePrune&#26159;&#19968;&#31181;&#39640;&#25928;&#36880;&#28176;&#21098;&#26525;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#36733;&#19968;&#20010;&#23436;&#25972;&#27169;&#22411;&#24182;&#23558;&#20854;&#19982;&#25513;&#30721;&#21644;LoRA&#27169;&#22359;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#32467;&#26500;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24040;&#22823;&#23610;&#23544;&#32473;&#36164;&#28304;&#21463;&#38480;&#30828;&#20214;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#37096;&#32626;&#25361;&#25112;&#12290;&#32467;&#26500;&#21098;&#26525;&#20026;&#21387;&#32553;LLMs&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#38477;&#20302;&#23384;&#20648;&#25104;&#26412;&#65292;&#25552;&#21319;&#25512;&#26029;&#36895;&#24230;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#21033;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#36164;&#28304;&#25928;&#29575;&#30340;&#32467;&#26500;&#21098;&#26525;&#26041;&#27861;&#65292;&#20197;&#33719;&#21462;&#26356;&#23567;&#20294;&#20381;&#28982;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#30693;&#35782;&#33976;&#39311;&#38750;&#24120;&#36866;&#21512;&#21098;&#26525;&#65292;&#22240;&#20026;&#23436;&#25972;&#30340;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#21098;&#26525;&#21518;&#30340;&#23398;&#29983;&#30340;&#20248;&#31168;&#25945;&#24072;&#12290;&#28982;&#32780;&#65292;&#22312;LLMs&#30340;&#32972;&#26223;&#19979;&#65292;&#30001;&#20110;&#20869;&#23384;&#38480;&#21046;&#65292;&#36825;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36880;&#28176;&#21098;&#26525;&#26041;&#27861;&#65288;NutePrune&#65289;&#12290;NutePrune&#36890;&#36807;&#21482;&#21152;&#36733;&#19968;&#20010;&#23436;&#25972;&#27169;&#22411;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#25513;&#30721;&#21644;LoRA&#27169;&#22359;&#38598;&#25104;&#65292;&#22312;&#25945;&#24072;&#21644;&#23398;&#29983;&#35282;&#33394;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36807;&#22810;&#30340;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09773v1 Announce Type: new  Abstract: The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware. Structured pruning, offers an effective means to compress LLMs, thereby reducing storage costs and enhancing inference speed for more efficient utilization. In this work, we study data-efficient and resource-efficient structure pruning methods to obtain smaller yet still powerful models. Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of LLMs due to memory constraints. To address this, we propose an efficient progressive Numerous-teacher pruning method (NutePrune). NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles. This approach a
&lt;/p&gt;</description></item><item><title>AI&#21307;&#38498;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#23454;&#26102;&#20132;&#20114;&#24335;&#35786;&#26029;&#29615;&#22659;&#65292;&#36890;&#36807;&#19982;LLMs&#30340;&#20132;&#20114;&#35780;&#20272;&#21644;&#21327;&#20316;&#65292;&#25552;&#39640;&#20020;&#24202;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09742</link><description>&lt;p&gt;
AI&#21307;&#38498;&#65306;&#29992;&#20110;&#20020;&#24202;&#35786;&#26029;&#30340;LLMs&#20316;&#20026;&#23454;&#20064;&#21307;&#29983;&#30340;&#20132;&#20114;&#24335;&#35780;&#20272;&#21644;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09742
&lt;/p&gt;
&lt;p&gt;
AI&#21307;&#38498;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#23454;&#26102;&#20132;&#20114;&#24335;&#35786;&#26029;&#29615;&#22659;&#65292;&#36890;&#36807;&#19982;LLMs&#30340;&#20132;&#20114;&#35780;&#20272;&#21644;&#21327;&#20316;&#65292;&#25552;&#39640;&#20020;&#24202;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#26631;&#24535;&#30528;&#37325;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#36776;&#21035;&#21644;&#38382;&#31572;&#20219;&#21153;&#65292;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#20854;&#20132;&#20114;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;AI&#21307;&#38498;&#65292;&#19968;&#20010;&#26088;&#22312;&#26500;&#24314;&#23454;&#26102;&#20132;&#20114;&#24335;&#35786;&#26029;&#29615;&#22659;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#27169;&#25311;&#36807;&#31243;&#65292;&#25105;&#20204;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#21307;&#30103;&#35760;&#24405;&#65292;&#21019;&#24314;&#20102;&#24739;&#32773;&#12289;&#26816;&#26597;&#32773;&#21644;&#21307;&#30103;&#20027;&#20219;&#20195;&#29702;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;AI&#21307;&#38498;&#36827;&#34892;LLMs&#30340;&#20132;&#20114;&#35780;&#20272;&#21644;&#21327;&#20316;&#12290;&#21021;&#22987;&#38454;&#27573;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35270;&#22270;&#21307;&#23398;&#35780;&#20272;&#65288;MVME&#65289;&#22522;&#20934;&#65292;&#20854;&#20013;&#21508;&#31181;LLMs&#20316;&#20026;&#23454;&#20064;&#21307;&#29983;&#36827;&#34892;&#20132;&#20114;&#24335;&#35786;&#26029;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21327;&#20316;&#26426;&#21046;&#65292;&#28041;&#21450;&#21307;&#30103;&#20027;&#20219;&#30340;&#30417;&#30563;&#19979;&#30340;&#36845;&#20195;&#35752;&#35770;&#21644;&#20105;&#35758;&#35299;&#20915;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09742v1 Announce Type: new  Abstract: The incorporation of Large Language Models (LLMs) in healthcare marks a significant advancement. However, the application has predominantly been limited to discriminative and question-answering tasks, which does not fully leverage their interactive potential. To address this limitation, our paper presents AI Hospital, a framework designed to build a real-time interactive diagnosis environment. To simulate the procedure, we collect high-quality medical records to create patient, examiner, and medical director agents. AI Hospital is then utilized for the interactive evaluation and collaboration of LLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmark where various LLMs serve as intern doctors for interactive diagnosis. Subsequently, to improve diagnostic accuracy, we introduce a collaborative mechanism that involves iterative discussions and a dispute resolution process under the supervision of the medical director. I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#35757;&#32451;&#39034;&#24207;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07610</link><description>&lt;p&gt;
&#36393;&#33050;&#35843;&#26657;&#65306;&#36890;&#36807;&#33258;&#21161;&#24341;&#23548;&#25193;&#23637;LLM&#30340;&#33258;&#23545;&#40784;&#33021;&#21147;&#30340;&#35268;&#27169;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#35757;&#32451;&#39034;&#24207;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23545;&#40784;&#26159;&#19968;&#31181;&#38477;&#20302;&#20154;&#24037;&#27880;&#37322;&#25104;&#26412;&#24182;&#30830;&#20445;&#27169;&#22411;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#21333;&#27425;&#24490;&#29615;&#20013;&#23436;&#25104;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#27493;&#39588;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#33258;&#23545;&#40784;&#27169;&#22411;&#19981;&#26029;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#36827;&#34892;&#22810;&#27425;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#65292;&#20250;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#36824;&#26159;&#23548;&#33268;&#24555;&#36895;&#36864;&#21270;&#65311;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20445;&#35777;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#25381;&#33258;&#21161;&#24341;&#23548;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#24182;&#35843;&#25972;&#20102;&#25968;&#25454;&#30340;&#35757;&#32451;&#39034;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36393;&#33050;&#35843;&#26657;&#65288;SOFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#27169;&#22411;&#30340;&#25345;&#32493;&#22686;&#24378;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.05162</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#35780;&#20272;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#23433;&#20840;&#26426;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#22266;&#26377;&#30340;&#26131;&#30862;&#24615;&#65292;&#36825;&#21487;&#20174;&#23427;&#20204;&#26131;&#21463;&#36234;&#29425;&#21644;&#21363;&#20351;&#26159;&#38750;&#24694;&#24847;&#24494;&#35843;&#20063;&#26131;&#21463;&#24433;&#21709;&#26469;&#35828;&#26126;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#25506;&#35752;&#20102;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#23545;&#20110;&#23433;&#20840;&#38450;&#25252;&#33267;&#20851;&#37325;&#35201;&#65292;&#19988;&#22312;&#31070;&#32463;&#20803;&#21644;&#31209;&#32423;&#21035;&#19978;&#19982;&#25928;&#29992;&#30456;&#20851;&#30340;&#21306;&#22495;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#30340;&#23396;&#31435;&#21306;&#22495;&#26159;&#31232;&#30095;&#30340;&#65292;&#32422;&#21344;&#21442;&#25968;&#32423;&#21035;&#30340;$3\%$&#21644;&#25490;&#21517;&#32423;&#21035;&#30340;$2.5\%$&#12290;&#21435;&#38500;&#36825;&#20123;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#32780;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#19981;&#22823;&#65292;&#20174;&#32780;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#23433;&#20840;&#26426;&#21046;&#30340;&#22266;&#26377;&#26131;&#30862;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#38480;&#21046;&#23545;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#36827;&#34892;&#20462;&#25913;&#65292;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#20302;&#25104;&#26412;&#30340;&#24494;&#35843;&#25915;&#20987;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#26356;&#24378;&#22823;&#30340;&#23433;&#20840;&#31574;&#30053;&#30340;&#32039;&#36843;&#24615;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#22522;&#20934;&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23545;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;LVLMs&#22312;&#36825;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.15847</link><description>&lt;p&gt;
&#26494;&#39292;&#36824;&#26159;&#21513;&#23043;&#23043;&#65311;&#29992;&#22810;&#38754;&#26495;VQA&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15847
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#22522;&#20934;&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23545;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;LVLMs&#22312;&#36825;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38754;&#26495;&#22270;&#20687;&#65292;&#36890;&#24120;&#22312;&#32593;&#39029;&#25130;&#22270;&#12289;&#28023;&#25253;&#31561;&#20013;&#30475;&#21040;&#65292;&#20805;&#26021;&#30528;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#12290;&#36825;&#20123;&#22270;&#20687;&#20197;&#22810;&#20010;&#23376;&#22270;&#20197;&#19981;&#21516;&#24067;&#23616;&#32452;&#25104;&#65292;&#26377;&#25928;&#22320;&#21521;&#20154;&#20204;&#20256;&#36798;&#20449;&#24687;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#32423;&#30340;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#22914;&#33021;&#29702;&#35299;&#22797;&#26434;&#22330;&#26223;&#24182;&#22312;&#32593;&#39029;&#20013;&#23548;&#33322;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#22810;&#38754;&#26495;&#35270;&#35273;&#25512;&#29702;&#30340;&#25216;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#23545;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6,600&#20010;&#38382;&#39064;&#12289;&#31572;&#26696;&#21644;&#22810;&#38754;&#26495;&#22270;&#20687;&#19977;&#20803;&#32452;&#65292;&#19987;&#38376;&#25361;&#25112;&#27169;&#22411;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;MultipanelVQA&#22522;&#20934;&#20013;&#30340;&#38382;&#39064;&#23545;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#21363;&#20351;&#20154;&#31867;&#21487;&#20197;&#33719;&#24471;&#32422;99%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15847v2 Announce Type: replace-cross  Abstract: Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99\% accurac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#30340;&#25200;&#21160;&#26412;&#20307;&#20197;&#21450;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#35780;&#20272;&#20102;LLMs&#22312;&#25968;&#23383;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#22312;&#20840;&#38754;&#35780;&#20272;&#20013;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#25200;&#21160;&#38382;&#39064;&#19978;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLMs&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.09395</link><description>&lt;p&gt;
&#34987;&#29702;&#24615;&#30340;&#27969;&#27801;&#25152;&#22256;&#65292;&#36828;&#31163;AGI&#23792;&#20250;&#65306;&#36890;&#36807;&#26412;&#20307;&#24341;&#23548;&#24178;&#39044;&#35780;&#20272;LLMs&#30340;&#25968;&#23398;&#21644;&#32534;&#30721;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09395
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#30340;&#25200;&#21160;&#26412;&#20307;&#20197;&#21450;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#35780;&#20272;&#20102;LLMs&#22312;&#25968;&#23383;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#22312;&#20840;&#38754;&#35780;&#20272;&#20013;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#25200;&#21160;&#38382;&#39064;&#19978;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLMs&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21457;&#23637;&#23637;&#31034;&#20102;&#22312;&#29616;&#26377;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#26524;&#65292;&#20854;&#20013;&#19968;&#20123;&#27169;&#22411;&#29978;&#33267;&#36229;&#36807;&#20102;&#20154;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#21644;&#31283;&#20581;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20851;&#27880;&#20004;&#20010;&#27969;&#34892;&#30340;&#25512;&#29702;&#20219;&#21153;&#65306;&#31639;&#26415;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#65306;&#65288;i&#65289;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#30340;&#36890;&#29992;&#25200;&#21160;&#26412;&#20307;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#21322;&#33258;&#21160;&#26041;&#27861;&#26469;&#24212;&#29992;&#36825;&#20123;&#25200;&#21160;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#20004;&#20010;&#25968;&#25454;&#38598;MORE&#21644;CORE&#65292;&#20998;&#21035;&#29992;&#20110;&#25200;&#21160;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#65292;&#20197;&#25506;&#31350;LLM&#22312;&#25968;&#23383;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#26497;&#38480;&#12290;&#36890;&#36807;&#23545;&#23553;&#38381;&#28304;&#21644;&#24320;&#28304;LLMs&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#26377;&#27169;&#22411;&#23545;&#25200;&#21160;&#38382;&#39064;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLMs&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.09395v2 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe the limits of LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust probl
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2311.16480</link><description>&lt;p&gt;
&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#29992;&#20110;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16480
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#20999;&#29255;&#22270;&#20687;&#26159;&#29992;&#20110;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#22522;&#30784;&#12290;&#25776;&#20889;&#30149;&#29702;&#25253;&#21578;&#23545;&#32463;&#39564;&#19981;&#36275;&#30340;&#30149;&#29702;&#23398;&#23478;&#26469;&#35828;&#26159;&#36153;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#24037;&#20316;&#37327;&#24182;&#25913;&#21892;&#20020;&#24202;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#29983;&#25104;&#32473;&#23450;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#22312;&#25968;&#25454;&#31471;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#26368;&#22823;&#30340;WSI-&#25991;&#26412;&#25968;&#25454;&#38598;&#65288;TCGA-PathoText&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#21644;&#28165;&#29702;TCGA&#20013;&#21465;&#36848;&#35786;&#26029;&#24187;&#28783;&#29255;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#25910;&#38598;&#20102;&#36817;1&#19975;&#23545;&#39640;&#36136;&#37327;&#30340;WSI-&#25991;&#26412;&#37197;&#23545;&#65292;&#20379;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#12290;&#22312;&#27169;&#22411;&#31471;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#20026;&#21315;&#20159;&#20687;&#32032;WSI&#29983;&#25104;&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#65288;MI-Gen&#65289;&#12290;&#25105;&#20204;&#22312;TCGA-PathoText&#30340;&#26368;&#22823;&#23376;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#27492;&#22806;&#65292;WSI-&#25991;&#26412;&#39044;&#27979;&#21487;&#34987;&#35270;&#20026;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16480v2 Announce Type: replace-cross  Abstract: Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizing and cleaning pathology reports which narrate diagnostic slides in TCGA. On the model end, we propose the multiple instance generative model (MI-Gen) which can produce pathology reports for gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText. Experimental results show our model can generate pathology reports which contain multiple clinical clues. Furthermore, WSI-text prediction can be seen as an approac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#25511;&#21046;&#39118;&#26684;&#29305;&#24449;&#65292;&#38750;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20154;&#31867;&#65292;&#21516;&#26102;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#24341;&#23548;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#38271;&#25688;&#35201;&#21644;&#39640;&#24230;&#25277;&#35937;&#30340;&#31616;&#21270;&#25688;&#35201;&#26041;&#38754;&#26377;&#38480;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#25511;&#21046;&#26041;&#38754;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.10415</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#26426;&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#31185;&#23398;&#20256;&#25773;&#30446;&#26631;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?. (arXiv:2401.10415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#25511;&#21046;&#39118;&#26684;&#29305;&#24449;&#65292;&#38750;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20154;&#31867;&#65292;&#21516;&#26102;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#24341;&#23548;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#38271;&#25688;&#35201;&#21644;&#39640;&#24230;&#25277;&#35937;&#30340;&#31616;&#21270;&#25688;&#35201;&#26041;&#38754;&#26377;&#38480;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#25511;&#21046;&#26041;&#38754;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#34920;&#24449;&#35770;&#25991;&#35780;&#35770;&#12289;&#25688;&#35201;&#21644;&#31616;&#21270;&#25688;&#35201;&#31561;&#19981;&#21516;&#31867;&#22411;&#25688;&#35201;&#30340;&#20851;&#38190;&#39118;&#26684;&#21644;&#20869;&#23481;&#35206;&#30422;&#22240;&#32032;&#12290;&#36890;&#36807;&#25511;&#21046;&#39118;&#26684;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#38750;&#24494;&#35843;&#30340;LLMs&#22312;MuP&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#26080;&#35770;&#26159;&#22312;&#19982;&#21442;&#32771;&#25688;&#35201;&#30340;&#30456;&#20284;&#24230;&#36824;&#26159;&#22312;&#20154;&#31867;&#20559;&#22909;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548; (CFG) &#26469;&#25913;&#21892;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#22312;arXiv&#21644;PubMed&#19978;&#23454;&#29616;&#19982;&#24378;&#24494;&#35843;&#22522;&#32447;&#30456;&#24403;&#30340;&#35789;&#27719;&#37325;&#21472;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;LLMs&#26080;&#27861;&#19968;&#33268;&#22320;&#29983;&#25104;&#36229;&#36807;8&#20010;&#21477;&#23376;&#30340;&#38271;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#24230;&#25277;&#35937;&#30340;&#31616;&#21270;&#25688;&#35201;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#34429;&#28982;LLMs&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#36890;&#29992;&#25688;&#35201;&#33021;&#21147;&#65292;&#20294;&#22312;&#19981;&#26114;&#36149;&#30340;&#24494;&#35843;&#25514;&#26045;&#19979;&#65292;&#23545;&#20869;&#23481;&#30340;&#22797;&#26434;&#25511;&#21046;&#33021;&#21147;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the controllability of large language models (LLMs) on scientific summarization tasks. We identify key stylistic and content coverage factors that characterize different types of summaries such as paper reviews, abstracts, and lay summaries. By controlling stylistic features, we find that non-fine-tuned LLMs outperform humans in the MuP review generation task, both in terms of similarity to reference summaries and human preferences. Also, we show that we can improve the controllability of LLMs with keyword-based classifier-free guidance (CFG) while achieving lexical overlap comparable to strong fine-tuned baselines on arXiv and PubMed. However, our results also indicate that LLMs cannot consistently generate long summaries with more than 8 sentences. Furthermore, these models exhibit limited capacity to produce highly abstractive lay summaries. Although LLMs demonstrate strong generic summarization competency, sophisticated content control without costly fi
&lt;/p&gt;</description></item><item><title>ReFT&#26159;&#19968;&#31181;&#21152;&#24378;&#25512;&#29702;&#33021;&#21147;&#30340;&#24378;&#21270;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#30340;&#25512;&#29702;&#36335;&#24452;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08967</link><description>&lt;p&gt;
ReFT: &#21152;&#24378;&#24378;&#21270;&#24494;&#35843;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ReFT: Reasoning with Reinforced Fine-Tuning. (arXiv:2401.08967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08967
&lt;/p&gt;
&lt;p&gt;
ReFT&#26159;&#19968;&#31181;&#21152;&#24378;&#25512;&#29702;&#33021;&#21147;&#30340;&#24378;&#21270;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#30340;&#25512;&#29702;&#36335;&#24452;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#27880;&#37322;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#24182;&#19981;&#21313;&#20998;&#24378;&#22823;&#65292;&#22240;&#20026;&#35757;&#32451;&#20165;&#20381;&#36182;&#20110;&#32473;&#23450;&#30340;CoT&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#36890;&#24120;&#21482;&#26377;&#19968;&#20010;&#27880;&#37322;&#30340;&#25512;&#29702;&#36335;&#24452;&#29992;&#20110;&#27599;&#20010;&#38382;&#39064;&#12290;&#30452;&#35266;&#26469;&#35828;&#65292;&#35753;&#31639;&#27861;&#20174;&#32473;&#23450;&#30340;&#38382;&#39064;&#20013;&#23398;&#20064;&#22810;&#20010;&#27880;&#37322;&#30340;&#25512;&#29702;&#36335;&#24452;&#20250;&#26356;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21152;&#24378;&#24378;&#21270;&#24494;&#35843;&#65288;ReFT&#65289;&#65292;&#20197;&#22686;&#24378;&#23398;&#20064;LLMs&#36827;&#34892;&#25512;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20026;&#20363;&#12290;ReFT&#39318;&#20808;&#20351;&#29992;SFT&#23545;&#27169;&#22411;&#36827;&#34892;&#28909;&#36523;&#65292;&#28982;&#21518;&#37319;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#26412;&#25991;&#20013;&#26159;&#20351;&#29992;PPO&#31639;&#27861;&#65289;&#36827;&#19968;&#27493;&#24494;&#35843;&#27169;&#22411;&#65292;&#20854;&#20013;&#26681;&#25454;&#38382;&#39064;&#33258;&#21160;&#37319;&#26679;&#20102;&#22823;&#37327;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question
&lt;/p&gt;</description></item><item><title>MuTox&#26159;&#31532;&#19968;&#20010;&#39640;&#24230;&#22810;&#35821;&#35328;&#30340;&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35757;&#32451;&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#36328;&#22810;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#27602;&#24615;&#26816;&#27979;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#35206;&#30422;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#35789;&#27719;&#21015;&#34920;&#30340;&#20998;&#31867;&#22120;&#65292;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;&#32422;2.5&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.05060</link><description>&lt;p&gt;
MuTox: &#36890;&#29992;&#22810;&#35821;&#35328;&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#25968;&#25454;&#38598;&#21644;&#38646;&#26679;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot Detector. (arXiv:2401.05060v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05060
&lt;/p&gt;
&lt;p&gt;
MuTox&#26159;&#31532;&#19968;&#20010;&#39640;&#24230;&#22810;&#35821;&#35328;&#30340;&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35757;&#32451;&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#36328;&#22810;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#27602;&#24615;&#26816;&#27979;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#35206;&#30422;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#35789;&#27719;&#21015;&#34920;&#30340;&#20998;&#31867;&#22120;&#65292;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;&#32422;2.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#27169;&#24577;&#65288;&#22522;&#20110;&#38899;&#39057;&#65289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#27602;&#24615;&#26816;&#27979;&#30740;&#31350;&#30456;&#23545;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#33521;&#35821;&#35821;&#35328;&#32780;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#20026;&#30495;&#27491;&#22810;&#35821;&#35328;&#30340;&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#26816;&#27979;&#22880;&#23450;&#22522;&#30784;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MuTox&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#27602;&#24615;&#26631;&#31614;&#30340;&#39640;&#24230;&#22810;&#35821;&#35328;&#30340;&#22522;&#20110;&#38899;&#39057;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;20,000&#20010;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#38899;&#39057;&#29255;&#27573;&#65292;&#20197;&#21450;&#20854;&#20182;19&#31181;&#35821;&#35328;&#30340;4,000&#20010;&#29255;&#27573;&#12290;&#20026;&#20102;&#35777;&#26126;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;MuTox&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#20998;&#31867;&#22120;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#27602;&#24615;&#26816;&#27979;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25991;&#26412;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30456;&#27604;&#65292;&#35813;&#20998;&#31867;&#22120;&#30340;AUC&#24615;&#33021;&#25552;&#39640;&#20102;&#36229;&#36807;1%&#65292;&#21516;&#26102;&#25193;&#22823;&#20102;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#21313;&#20493;&#20197;&#19978;&#12290;&#19982;&#22522;&#20110;&#35789;&#27719;&#21015;&#34920;&#30340;&#20855;&#26377;&#30456;&#20284;&#35821;&#35328;&#35206;&#30422;&#25968;&#37327;&#30340;&#20998;&#31867;&#22120;&#30456;&#27604;&#65292;MuTox&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;&#32422;2.5&#20493;&#12290;&#36825;&#20010;&#26174;&#33879;&#30340;&#25913;&#36827;&#31361;&#26174;&#20102;&#20854;&#28508;&#22312;&#30340;&#21019;&#26032;&#24615;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in toxicity detection in natural language processing for the speech modality (audio-based) is quite limited, particularly for languages other than English. To address these limitations and lay the groundwork for truly multilingual audio-based toxicity detection, we introduce MuTox, the first highly multilingual audio-based dataset with toxicity labels. The dataset comprises 20,000 audio utterances for English and Spanish, and 4,000 for the other 19 languages. To demonstrate the quality of this dataset, we trained the MuTox audio-based toxicity classifier, which enables zero-shot toxicity detection across a wide range of languages. This classifier outperforms existing text-based trainable classifiers by more than 1% AUC, while expanding the language coverage more than tenfold. When compared to a wordlist-based classifier that covers a similar number of languages, MuTox improves precision and recall by approximately 2.5 times. This significant improvement underscores the potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;{\delta}-CAUSAL&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22240;&#26524;&#25512;&#29702;&#20013;&#21487;&#24223;&#38500;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#26080;&#27861;&#22312;&#21487;&#24223;&#38500;&#29615;&#22659;&#20013;&#20934;&#30830;&#35780;&#20272;&#22240;&#26524;&#20851;&#31995;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.03183</link><description>&lt;p&gt;
{\delta}-CAUSAL&#65306;&#25506;&#32034;&#22240;&#26524;&#25512;&#29702;&#20013;&#30340;&#21487;&#24223;&#38500;&#24615;
&lt;/p&gt;
&lt;p&gt;
{\delta}-CAUSAL: Exploring Defeasibility in Causal Reasoning. (arXiv:2401.03183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;{\delta}-CAUSAL&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22240;&#26524;&#25512;&#29702;&#20013;&#21487;&#24223;&#38500;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#26080;&#27861;&#22312;&#21487;&#24223;&#38500;&#29615;&#22659;&#20013;&#20934;&#30830;&#35780;&#20272;&#22240;&#26524;&#20851;&#31995;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#20013;&#30340;&#21487;&#24223;&#38500;&#24615;&#24847;&#21619;&#30528;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#34987;&#21152;&#24378;&#25110;&#21066;&#24369;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22240;&#26524;&#20851;&#31995;&#30340;&#24378;&#24230;&#24212;&#35813;&#38543;&#30528;&#21152;&#20837;&#25903;&#25345;&#32773;&#25110;&#39539;&#26021;&#32773;&#32780;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#22240;&#26524;&#25512;&#29702;&#20013;&#30340;&#21487;&#24223;&#38500;&#24615;&#65292;&#24182;&#26410;&#22312;&#21487;&#24223;&#38500;&#29615;&#22659;&#20013;&#35780;&#20272;&#29616;&#26377;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22240;&#26524;&#25512;&#29702;&#20013;&#30340;&#21487;&#24223;&#38500;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;{\delta}-CAUSAL&#12290;{\delta}-CAUSAL&#21253;&#25324;&#32422;11K&#20010;&#28085;&#30422;&#21313;&#20010;&#39046;&#22495;&#30340;&#20107;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#25903;&#25345;&#32773;&#21644;&#39539;&#26021;&#32773;&#30340;&#21487;&#24223;&#38500;&#22240;&#26524;&#20851;&#31995;&#23545;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#24403;&#21069;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#26080;&#27861;&#21453;&#26144;{\delta}-CAUSAL&#20013;&#30340;&#25903;&#25345;&#32773;&#25110;&#39539;&#26021;&#32773;&#21152;&#20837;&#21518;&#30340;&#22240;&#26524;&#24378;&#24230;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CESAR&#65288;Causal Embedding aSsociation with Attention Rating&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defeasibility in causal reasoning implies that the causal relationship between cause and effect can be strengthened or weakened. Namely, the causal strength between cause and effect should increase or decrease with the incorporation of strengthening arguments (supporters) or weakening arguments (defeaters), respectively. However, existing works ignore defeasibility in causal reasoning and fail to evaluate existing causal strength metrics in defeasible settings. In this work, we present {\delta}-CAUSAL, the first benchmark dataset for studying defeasibility in causal reasoning. {\delta}-CAUSAL includes around 11K events spanning ten domains, featuring defeasible causality pairs, i.e., cause-effect pairs accompanied by supporters and defeaters. We further show current causal strength metrics fail to reflect the change of causal strength with the incorporation of supporters or defeaters in {\delta}-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation with Attention Rating),
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.01200</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#20986;&#29616;&#26032;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#26032;&#27169;&#22411;&#32780;&#19981;&#26159;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#25910;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26032;&#35821;&#35328;&#20986;&#29616;&#26102;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#22909;&#22788;&#21644;&#24330;&#31471;&#65292;&#21363;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#20174;&#21333;&#35821;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20986;&#21457;&#65292;&#25105;&#20204;&#36880;&#27493;&#28155;&#21152;&#20102;&#26469;&#33258;&#25386;&#23041;&#35821;&#21644;&#20912;&#23707;&#35821;&#30340;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#21069;&#21521;&#21644;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#22914;&#20309;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#39034;&#24207;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#21069;&#21521;&#36716;&#31227;&#20027;&#35201;&#26159;&#27491;&#21521;&#30340;&#65292;&#19981;&#21463;&#35821;&#35328;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#21017;&#21487;&#33021;&#26159;&#27491;&#21521;&#30340;&#25110;&#36127;&#21521;&#30340;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#24335;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#31181;&#35821;&#35328;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#28216;&#25103;&#20013;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#29616;&#20986;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10701</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind for Multi-Agent Collaboration via Large Language Models. (arXiv:2310.10701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#28216;&#25103;&#20013;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#29616;&#20986;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20294;&#23427;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21010;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#25991;&#26412;&#28216;&#25103;&#20013;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#22312;&#29702;&#35770;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#20986;&#29616;&#20102;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#22312;&#38271;&#26399;&#35268;&#21010;&#19978;&#23384;&#22312;&#20248;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#23545;&#20219;&#21153;&#29366;&#24577;&#30340;&#38169;&#35823;&#35748;&#30693;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.
&lt;/p&gt;</description></item><item><title>CP-KGC&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32422;&#26463;&#24335;&#25552;&#31034;&#26469;&#34917;&#20840;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08279</link><description>&lt;p&gt;
CP-KGC: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#24335;&#25552;&#31034;&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models. (arXiv:2310.08279v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08279
&lt;/p&gt;
&lt;p&gt;
CP-KGC&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32422;&#26463;&#24335;&#25552;&#31034;&#26469;&#34917;&#20840;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#25512;&#26029;&#21644;&#25512;&#27979;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#30340;&#36830;&#25509;&#12290;SimKGC&#31561;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#24050;&#32463;&#36229;&#36807;&#20102;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#21462;&#20915;&#20110;&#23454;&#20307;&#25991;&#26412;&#25551;&#36848;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#20943;&#36731;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#24187;&#35273;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#19978;&#19979;&#25991;&#32422;&#26463;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#32422;&#26463;&#24335;&#25552;&#31034;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65288;CP-KGC&#65289;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#26377;&#25928;&#30340;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36229;&#36807;&#20102;WN18RR&#21644;FB15K237&#25968;&#25454;&#38598;&#19978;&#30340;&#20043;&#21069;&#32467;&#26524;&#12290;&#36825;&#23637;&#31034;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce and infer missing connections within knowledge graphs. Text-based approaches, like SimKGC, have outperformed graph embedding methods, showcasing the promise of inductive KGC. However, the efficacy of text-based methods hinges on the quality of entity textual descriptions. In this paper, we identify the key issue of whether large language models (LLMs) can generate effective text. To mitigate hallucination in LLM-generated text in this paper, we introduce a constraint-based prompt that utilizes the entity and its textual description as contextual constraints to enhance data quality. Our Constrained-Prompt Knowledge Graph Completion (CP-KGC) method demonstrates effective inference under low resource computing conditions and surpasses prior results on the WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC tasks and provides new directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#36234;&#20027;&#39064;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#21270;&#30340;&#20840;&#38754;&#38750;&#20998;&#24067;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#31574;&#30053;&#65292;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#30340;&#31934;&#32454;&#35843;&#33410;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.08316</link><description>&lt;p&gt;
&#36328;&#36234;&#20027;&#39064;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#21270;&#65306;&#23545;&#20840;&#38754;&#30340;&#38750;&#20998;&#24067;&#22330;&#26223;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bridging Topic, Domain, and Language Shifts: An Evaluation of Comprehensive Out-of-Distribution Scenarios. (arXiv:2309.08316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#36234;&#20027;&#39064;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#21270;&#30340;&#20840;&#38754;&#38750;&#20998;&#24067;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#31574;&#30053;&#65292;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#30340;&#31934;&#32454;&#35843;&#33410;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65288;&#22914;&#20105;&#35770;&#25366;&#25496;&#65289;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#32463;&#24120;&#19979;&#38477;&#12290;&#36825;&#31181;&#38477;&#32423;&#21457;&#29983;&#22312;&#26032;&#35805;&#39064;&#20986;&#29616;&#65292;&#25110;&#20854;&#20182;&#25991;&#26412;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#24471;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#38750;&#20998;&#24067;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#24847;&#22320;&#20445;&#30041;&#29305;&#23450;&#23454;&#20363;&#36827;&#34892;&#27979;&#35797;&#26469;&#27169;&#25311;&#36825;&#31181;&#20998;&#24067;&#21464;&#21270;&#65292;&#20363;&#22914;&#31038;&#20132;&#23186;&#20307;&#39046;&#22495;&#25110;&#22826;&#38451;&#33021;&#20027;&#39064;&#12290;&#19982;&#20808;&#21069;&#20851;&#27880;&#29305;&#23450;&#21464;&#21270;&#21644;&#24230;&#37327;&#26631;&#20934;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#30830;&#23450;&#27867;&#21270;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#28085;&#30422;&#20027;&#39064;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#21270;&#30340;&#21313;&#19968;&#20010;&#20998;&#31867;&#20219;&#21153;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#25552;&#31034;&#30340;&#31934;&#32454;&#35843;&#33410;&#20855;&#26377;&#26356;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#22312;&#35821;&#20041;&#19978;&#20027;&#35201;&#26377;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20063;&#26377;&#31867;&#20284;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) excel in in-distribution (ID) scenarios where train and test data are independent and identically distributed. However, their performance often degrades in real-world applications like argument mining. Such degradation happens when new topics emerge, or other text domains and languages become relevant. To assess LMs' generalization abilities in such out-of-distribution (OOD) scenarios, we simulate such distribution shifts by deliberately withholding specific instances for testing, as from the social media domain or the topic Solar Energy.  Unlike prior studies focusing on specific shifts and metrics in isolation, we comprehensively analyze OOD generalization. We define three metrics to pinpoint generalization flaws and propose eleven classification tasks covering topic, domain, and language shifts. Overall, we find superior performance of prompt-based fine-tuning, notably when train and test splits primarily differ semantically. Simultaneously, in-context learning
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;GPT3.5&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20855;&#26377;&#26377;&#36259;&#30340;&#20010;&#24615;&#38382;&#21367;&#22238;&#31572;&#33021;&#21147;&#65292;&#20294;&#19981;&#22826;&#21487;&#33021;&#21457;&#23637;&#20986;&#24847;&#35782;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#21464;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.07683</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#36136;&#65306;&#23545;&#20154;&#31867;&#20013;&#24515;&#20027;&#20041;&#30340;&#35686;&#21578;
&lt;/p&gt;
&lt;p&gt;
Assessing the nature of large language models: A caution against anthropocentrism. (arXiv:2309.07683v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07683
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;GPT3.5&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20855;&#26377;&#26377;&#36259;&#30340;&#20010;&#24615;&#38382;&#21367;&#22238;&#31572;&#33021;&#21147;&#65292;&#20294;&#19981;&#22826;&#21487;&#33021;&#21457;&#23637;&#20986;&#24847;&#35782;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36890;&#36807;OpenAI&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;ChatGPT&#30340;&#21457;&#24067;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#20851;&#27880;&#21644;&#29468;&#27979;&#12290;&#30446;&#21069;&#23384;&#22312;&#20004;&#31181;&#24847;&#35265;&#38453;&#33829;&#65306;&#19968;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#20026;&#20154;&#31867;&#20219;&#21153;&#24102;&#26469;&#30340;&#22522;&#26412;&#21464;&#38761;&#30340;&#21487;&#33021;&#24615;&#24863;&#21040;&#20852;&#22859;&#65292;&#21478;&#19968;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#24863;&#21040;&#39640;&#24230;&#20851;&#20999;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#20851;&#20999;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#12289;&#35268;&#33539;&#21270;&#21644;&#32463;&#36807;&#39564;&#35777;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#27979;&#37327;&#24037;&#20855;&#26469;&#35780;&#20272;GPT3.5&#12290;&#22312;&#36825;&#20010;&#21021;&#27493;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#27979;&#35797;&#65292;&#21487;&#20197;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36793;&#30028;&#65292;&#23427;&#20204;&#22312;&#30701;&#26102;&#38388;&#20869;&#30340;&#31283;&#23450;&#24615;&#20197;&#21450;&#19982;&#20154;&#31867;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT 3.5&#24456;&#21487;&#33021;&#27809;&#26377;&#20135;&#29983;&#24847;&#35782;&#65292;&#23613;&#31649;&#23427;&#23545;&#20010;&#24615;&#38382;&#21367;&#30340;&#22238;&#31572;&#33021;&#21147;&#20196;&#20154;&#24863;&#20852;&#36259;&#12290;&#23427;&#22312;&#37325;&#22797;&#35266;&#23519;&#36807;&#31243;&#20013;&#26174;&#31034;&#20986;&#35748;&#30693;&#21644;&#20010;&#24615;&#27979;&#37327;&#26041;&#38754;&#30340;&#22823;&#37327;&#21464;&#24322;&#65292;&#36825;&#19982;&#20855;&#26377;&#20154;&#31867;&#33324;&#20010;&#24615;&#30340;&#27169;&#22411;&#26159;&#19981;&#31526;&#21512;&#39044;&#26399;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models garnered a large amount of public attention and speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion camps exist: one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed GPT3.5 using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models capabilities, how stable those capabilities are over a short period of time, and how they compare to humans.  Our results indicate that GPT 3.5 is unlikely to have developed sentience, although its ability to respond to personality inventories is interesting. It did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;GCRE-GPT, &#21487;&#30452;&#25509;&#20174;&#27604;&#36739;&#25991;&#26412;&#20013;&#25552;&#21462;&#20986;&#39640;&#31934;&#24230;&#30340;&#27604;&#36739;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.08601</link><description>&lt;p&gt;
GCRE-GPT: &#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#20851;&#31995;&#25552;&#21462;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GCRE-GPT: A Generative Model for Comparative Relation Extraction. (arXiv:2303.08601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;GCRE-GPT, &#21487;&#30452;&#25509;&#20174;&#27604;&#36739;&#25991;&#26412;&#20013;&#25552;&#21462;&#20986;&#39640;&#31934;&#24230;&#30340;&#27604;&#36739;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#27604;&#36739;&#25991;&#26412;&#65292;&#27604;&#36739;&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#25552;&#21462;&#20004;&#20010;&#30446;&#26631;&#65288;&#20363;&#22914;&#20004;&#20010;&#30456;&#26426;&#65289;&#30340;&#27604;&#36739;&#21644;&#23427;&#20204;&#34987;&#27604;&#36739;&#30340;&#26041;&#38754;&#65288;&#20363;&#22914;&#22270;&#20687;&#36136;&#37327;&#65289;&#12290;&#25552;&#21462;&#20986;&#30340;&#27604;&#36739;&#20851;&#31995;&#26159;&#36827;&#19968;&#27493;&#24847;&#35265;&#20998;&#26512;&#30340;&#22522;&#30784;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#27492;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#65292;&#20197;&#25552;&#21462;&#30446;&#26631;&#21644;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#33021;&#30452;&#25509;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27604;&#36739;&#20851;&#31995;&#12290;&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20986;&#65292;&#21487;&#20197;&#30452;&#25509;&#20197;&#39640;&#31934;&#24230;&#25552;&#21462;&#20986;&#27604;&#36739;&#20851;&#31995;&#12290;&#22522;&#20110;GPT-2&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#27604;&#36739;&#20851;&#31995;&#25552;&#21462;&#22120;&#65288;GCRE-GPT&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given comparative text, comparative relation extraction aims to extract two targets (\eg two cameras) in comparison and the aspect they are compared for (\eg image quality). The extracted comparative relations form the basis of further opinion analysis.Existing solutions formulate this task as a sequence labeling task, to extract targets and aspects. However, they cannot directly extract comparative relation(s) from text. In this paper, we show that comparative relations can be directly extracted with high accuracy, by generative model. Based on GPT-2, we propose a Generation-based Comparative Relation Extractor (GCRE-GPT). Experiment results show that \modelname achieves state-of-the-art accuracy on two datasets.
&lt;/p&gt;</description></item></channel></rss>