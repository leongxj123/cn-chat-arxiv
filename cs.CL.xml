<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#35782;&#21035;&#35821;&#35328;&#35889;&#31995;&#65292;&#30740;&#31350;&#20102;&#21333;&#35789;&#23884;&#20837;&#30340;&#24418;&#29366;&#22914;&#20309;&#20256;&#36882;&#20449;&#24687;&#65292;&#37325;&#24314;&#30340;&#35821;&#35328;&#35889;&#31995;&#26641;&#19982;&#21442;&#32771;&#26641;&#23637;&#29616;&#20986;&#24378;&#28872;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00500</link><description>&lt;p&gt;
&#21333;&#35789;&#23884;&#20837;&#30340;&#24418;&#29366;&#65306;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#35782;&#21035;&#35821;&#35328;&#35889;&#31995;
&lt;/p&gt;
&lt;p&gt;
The Shape of Word Embeddings: Recognizing Language Phylogenies through Topological Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#35782;&#21035;&#35821;&#35328;&#35889;&#31995;&#65292;&#30740;&#31350;&#20102;&#21333;&#35789;&#23884;&#20837;&#30340;&#24418;&#29366;&#22914;&#20309;&#20256;&#36882;&#20449;&#24687;&#65292;&#37325;&#24314;&#30340;&#35821;&#35328;&#35889;&#31995;&#26641;&#19982;&#21442;&#32771;&#26641;&#23637;&#29616;&#20986;&#24378;&#28872;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00500v1 &#31867;&#22411;&#65306;&#26032; &#21407;&#25991;&#25688;&#35201;&#65306;&#21333;&#35789;&#23884;&#20837;&#23558;&#35821;&#35328;&#35789;&#27719;&#34920;&#31034;&#20026;$d$&#32500;&#31354;&#38388;&#30340;&#28857;&#20113;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#28857;&#20113;&#30340;&#19968;&#33324;&#24418;&#29366;&#22312;&#38500;&#20102;&#34920;&#31034;&#27599;&#20010;&#20196;&#29260;&#30340;&#35821;&#20041;&#24847;&#20041;&#20043;&#22806;&#20256;&#36882;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#20013;&#30340;&#25345;&#20037;&#21516;&#35843;&#27010;&#24565;&#26469;&#27979;&#37327;&#20174;&#23427;&#20204;&#26410;&#26631;&#35760;&#30340;&#23884;&#20837;&#24418;&#29366;&#35745;&#31639;&#30340;&#35821;&#35328;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#36317;&#31163;&#30697;&#38453;&#22312;81&#31181;&#21360;&#27431;&#35821;&#35328;&#20043;&#38388;&#26500;&#24314;&#35821;&#35328;&#35889;&#31995;&#26641;&#12290;&#20180;&#32454;&#35780;&#20272;&#34920;&#26126;&#25105;&#20204;&#37325;&#24314;&#30340;&#35889;&#31995;&#26641;&#19982;&#21442;&#32771;&#26641;&#21576;&#29616;&#20986;&#24378;&#28872;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00500v1 Announce Type: new  Abstract: Word embeddings represent language vocabularies as clouds of $d$-dimensional points. We investigate how information is conveyed by the general shape of these clouds, outside of representing the semantic meaning of each token. Specifically, we use the notion of persistent homology from topological data analysis (TDA) to measure the distances between language pairs from the shape of their unlabeled embeddings. We use these distance matrices to construct language phylogenetic trees over 81 Indo-European languages. Careful evaluation shows that our reconstructed trees exhibit strong similarities to the reference tree.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#26694;&#26550;&#65292;&#22312;&#23454;&#20307;&#23545;&#40784;&#20013;&#35299;&#20915;&#20102;&#26080;&#26631;&#31614;&#24748;&#25346;&#26696;&#20363;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#27880;&#24847;&#26426;&#21046;&#21644;&#27491;&#26679;&#26412;-&#26080;&#26631;&#31614;&#25439;&#22833;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.10978</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#26631;&#31614;&#24748;&#25346;&#26696;&#20363;&#30340;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Entity Alignment with Unlabeled Dangling Cases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#26694;&#26550;&#65292;&#22312;&#23454;&#20307;&#23545;&#40784;&#20013;&#35299;&#20915;&#20102;&#26080;&#26631;&#31614;&#24748;&#25346;&#26696;&#20363;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#27880;&#24847;&#26426;&#21046;&#21644;&#27491;&#26679;&#26412;-&#26080;&#26631;&#31614;&#25439;&#22833;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#26080;&#26631;&#31614;&#24748;&#25346;&#26696;&#20363;&#30340;&#23454;&#20307;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#28304;&#22270;&#25110;&#30446;&#26631;&#22270;&#20013;&#26377;&#19968;&#20123;&#23454;&#20307;&#22312;&#21478;&#19968;&#26041;&#20013;&#27809;&#26377;&#23545;&#24212;&#23454;&#20307;&#65292;&#24182;&#19988;&#36825;&#20123;&#23454;&#20307;&#20445;&#25345;&#26410;&#26631;&#35760;&#29366;&#24577;&#12290;&#35813;&#38382;&#39064;&#20986;&#29616;&#22312;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#30340;&#35268;&#27169;&#19981;&#21516;&#65292;&#24182;&#19988;&#26631;&#35760;&#21487;&#21305;&#37197;&#23454;&#20307;&#30340;&#25104;&#26412;&#36828;&#20302;&#20110;&#24748;&#25346;&#23454;&#20307;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;GNN&#30340;&#24748;&#25346;&#26816;&#27979;&#21644;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;GNN&#65292;&#24182;&#19988;&#19968;&#36215;&#35757;&#32451;&#65292;&#20294;&#26816;&#27979;&#21040;&#30340;&#24748;&#25346;&#23454;&#20307;&#22312;&#23545;&#40784;&#20013;&#34987;&#31227;&#38500;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#29305;&#28857;&#26159;&#20855;&#26377;&#29992;&#20110;&#36873;&#25321;&#24615;&#37051;&#22495;&#32858;&#21512;&#30340;&#35774;&#35745;&#23454;&#20307;&#21644;&#20851;&#31995;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#21450;&#29992;&#20110;&#23545;&#24748;&#25346;&#23454;&#20307;&#36827;&#34892;&#26080;&#20559;&#20272;&#35745;&#30340;&#27491;&#26679;&#26412;-&#26080;&#26631;&#31614;&#23398;&#20064;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#35774;&#35745;&#30340;&#27599;&#20010;&#32452;&#20214;&#37117;&#23545;&#25972;&#20307;&#23545;&#40784;&#24615;&#33021;&#26377;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10978v1 Announce Type: new  Abstract: We investigate the entity alignment problem with unlabeled dangling cases, meaning that there are entities in the source or target graph having no counterparts in the other, and those entities remain unlabeled. The problem arises when the source and target graphs are of different scales, and it is much cheaper to label the matchable pairs than the dangling entities. To solve the issue, we propose a novel GNN-based dangling detection and entity alignment framework. While the two tasks share the same GNN and are trained together, the detected dangling entities are removed in the alignment. Our framework is featured by a designed entity and relation attention mechanism for selective neighborhood aggregation in representation learning, as well as a positive-unlabeled learning loss for an unbiased estimation of dangling entities. Experimental results have shown that each component of our design contributes to the overall alignment performance
&lt;/p&gt;</description></item><item><title>MYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#23383;&#33410;&#32534;&#30721;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#23454;&#29616;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#20026;99&#31181;&#35821;&#35328;&#25552;&#20379;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2403.10691</link><description>&lt;p&gt;
MYTE&#65306;&#24418;&#24577;&#23398;&#39537;&#21160;&#30340;&#23383;&#33410;&#32534;&#30721;&#65292;&#29992;&#20110;&#26356;&#22909;&#12289;&#26356;&#20844;&#24179;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10691
&lt;/p&gt;
&lt;p&gt;
MYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#23383;&#33410;&#32534;&#30721;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#23454;&#29616;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#20026;99&#31181;&#35821;&#35328;&#25552;&#20379;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#32771;&#34385;&#22240;&#32032;&#26159;&#22914;&#20309;&#26368;&#22909;&#22320;&#34920;&#31034;&#20855;&#26377;&#19981;&#21516;&#35789;&#27719;&#21644;&#25991;&#23383;&#30340;&#35821;&#35328;&#12290;&#23613;&#31649;&#24403;&#20195;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#28085;&#30422;&#20102;&#22823;&#22810;&#25968;&#19990;&#30028;&#25991;&#23383;&#31995;&#32479;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20559;&#21521;&#20110;&#20840;&#29699;&#35199;&#26041;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23569;&#25968;&#35821;&#35328;&#30340;&#25991;&#26412;&#24448;&#24448;&#34987;&#20998;&#21106;&#20026;&#19968;&#38271;&#20018;&#22312;&#35821;&#35328;&#23398;&#19978;&#27627;&#26080;&#24847;&#20041;&#30340;&#21333;&#20803;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#19981;&#24179;&#31561;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#36328;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#32534;&#30721;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#32422;&#23450;&#65288;MYTE&#65289;&#22522;&#20110;&#24418;&#24577;&#32032;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24211;&#23384;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#27604;&#23383;&#31526;&#26356;&#24179;&#34913;&#65292;&#32780;&#20197;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#23383;&#31526;&#12290;&#25105;&#20204;&#23637;&#31034;MYTE&#20026;&#25152;&#26377;99&#31181;&#20998;&#26512;&#35821;&#35328;&#20135;&#29983;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#20854;&#20013;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;&#36825;&#36827;&#32780;&#25913;&#21892;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10691v1 Announce Type: cross  Abstract: A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world's writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and di
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#20010;&#25277;&#35937;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#26377;&#24847;&#20041;&#23398;&#20064;&#33539;&#24335;&#65292;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#36890;&#29992;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09085</link><description>&lt;p&gt;
&#26377;&#24847;&#20041;&#23398;&#20064;&#65306;&#36890;&#36807;&#36890;&#29992;&#20107;&#23454;&#24341;&#23548;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25277;&#35937;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09085
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#20010;&#25277;&#35937;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#26377;&#24847;&#20041;&#23398;&#20064;&#33539;&#24335;&#65292;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#36890;&#29992;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25512;&#29702;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#21644;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#26631;&#24535;&#30528;&#26397;&#30528;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#30001;&#36890;&#29992;&#20107;&#23454;&#25903;&#25345;&#30340;&#31616;&#21333;&#38382;&#39064;&#26102;&#65292;LLMs&#32463;&#24120;&#26410;&#33021;&#25552;&#20379;&#19968;&#33268;&#21644;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#34920;&#26126;&#20854;&#23384;&#22312;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;LLMs&#21040;&#24213;&#26159;&#22312;&#30495;&#27491;&#25512;&#29702;&#36824;&#26159;&#20165;&#20165;&#22312;&#35760;&#24518;&#30340;&#28608;&#28872;&#20105;&#35770;&#12290;&#37492;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21021;&#27493;&#30740;&#31350;&#26469;&#37327;&#21270;&#24182;&#28145;&#20837;&#25506;&#35752;&#29616;&#26377;LLMs&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#26174;&#31034;&#20986;&#23427;&#20204;&#30340;&#19968;&#33324;&#25512;&#29702;&#21644;&#25277;&#35937;&#25512;&#29702;&#34920;&#29616;&#20043;&#38388;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#24322;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#20102;&#19968;&#20010;&#25277;&#35937;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;AbsR&#65289;&#65292;&#32467;&#21512;&#26377;&#24847;&#20041;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#25945;&#20250;LLMs&#22914;&#20309;&#21033;&#29992;&#36890;&#29992;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;LLMs&#22312;&#25277;&#35937;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09085v1 Announce Type: cross  Abstract: Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our app
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#23454;&#35777;&#21551;&#21160;&#27169;&#24335;&#26469;&#24314;&#31435;&#29702;&#35770;&#65292;&#20351;&#29992;&#35748;&#30693;&#39537;&#21160;&#35299;&#26512;&#22120;SPAWN&#29983;&#25104;&#37327;&#21270;&#21551;&#21160;&#39044;&#27979;&#24182;&#35780;&#20272;&#65292;&#24182;&#20197;&#31616;&#21270;&#30340;&#23450;&#35821;&#20174;&#21477;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#19968;&#20010;&#29702;&#35770;&#30340;&#21551;&#21160;&#39044;&#27979;&#19982;&#23454;&#35777;&#21551;&#21160;&#27169;&#24335;&#19968;&#33268;</title><link>https://arxiv.org/abs/2403.07202</link><description>&lt;p&gt;
&#20174;&#35748;&#30693;&#39537;&#21160;&#30340;&#35299;&#26512;&#22120;&#20013;&#39044;&#27979;&#32467;&#26500;&#21551;&#21160;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SPAWNing Structural Priming Predictions from a Cognitively Motivated Parser
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#23454;&#35777;&#21551;&#21160;&#27169;&#24335;&#26469;&#24314;&#31435;&#29702;&#35770;&#65292;&#20351;&#29992;&#35748;&#30693;&#39537;&#21160;&#35299;&#26512;&#22120;SPAWN&#29983;&#25104;&#37327;&#21270;&#21551;&#21160;&#39044;&#27979;&#24182;&#35780;&#20272;&#65292;&#24182;&#20197;&#31616;&#21270;&#30340;&#23450;&#35821;&#20174;&#21477;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#19968;&#20010;&#29702;&#35770;&#30340;&#21551;&#21160;&#39044;&#27979;&#19982;&#23454;&#35777;&#21551;&#21160;&#27169;&#24335;&#19968;&#33268;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21551;&#21160;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#33539;&#24335;&#65292;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#21477;&#23376;&#34920;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#23454;&#35777;&#21551;&#21160;&#27169;&#24335;&#26469;&#24314;&#31435;&#29702;&#35770;&#65292;&#25551;&#36848;&#20154;&#31867;&#22788;&#29702;&#21477;&#23376;&#26102;&#26500;&#24314;&#30340;&#32467;&#26500;&#34920;&#24449;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#39537;&#21160;&#35299;&#26512;&#22120;SPAWN&#65292;&#26681;&#25454;&#29702;&#35770;&#21477;&#27861;&#29983;&#25104;&#37327;&#21270;&#21551;&#21160;&#39044;&#27979;&#65292;&#24182;&#29992;&#23454;&#35777;&#20154;&#31867;&#34892;&#20026;&#35780;&#20272;&#36825;&#20123;&#39044;&#27979;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#24212;&#29992;&#36825;&#19968;&#26694;&#26550;&#26469;&#30740;&#31350;&#33521;&#35821;&#20013;&#31616;&#21270;&#30340;&#23450;&#35821;&#20174;&#21477;&#34920;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;SPAWN&#20174;&#20004;&#20010;&#29702;&#35770;&#35299;&#37322;&#20013;&#29983;&#25104;&#21551;&#21160;&#39044;&#27979;&#65292;&#36825;&#20004;&#20010;&#35299;&#37322;&#23545;&#23450;&#35821;&#20174;&#21477;&#30340;&#32467;&#26500;&#20570;&#20986;&#20102;&#19981;&#21516;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#26377;&#19968;&#20010;&#29702;&#35770;&#65288;&#21442;&#19982;&#24335;-&#30456;&#20301;&#65289;&#30340;&#39044;&#27979;&#19982;&#23454;&#35777;&#21551;&#21160;&#27169;&#24335;&#19968;&#33268;&#65292;&#20174;&#32780;&#31361;&#20986;&#26174;&#31034;&#20986;&#21738;&#20123;&#23545;&#23450;&#35821;&#20174;&#21477;&#30340;&#20551;&#35774;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#20154;&#31867;&#21477;&#23376;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07202v1 Announce Type: new  Abstract: Structural priming is a widely used psycholinguistic paradigm to study human sentence representations. In this work we propose a framework for using empirical priming patterns to build a theory characterizing the structural representations humans construct when processing sentences. This framework uses a new cognitively motivated parser, SPAWN, to generate quantitative priming predictions from theoretical syntax and evaluate these predictions with empirical human behavior. As a case study, we apply this framework to study reduced relative clause representations in English. We use SPAWN to generate priming predictions from two theoretical accounts which make different assumptions about the structure of relative clauses. We find that the predictions from only one of these theories (Participial-Phase) align with empirical priming patterns, thus highlighting which assumptions about relative clause better capture human sentence representation
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#26410;&#30693;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;FADE&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30446;&#26631;&#39044;&#27979;&#22120;&#65292;&#21516;&#26102;&#29420;&#31435;&#35757;&#32451;&#20107;&#20214;&#39044;&#27979;&#22120;&#65292;&#26368;&#32456;&#20943;&#36731;&#20107;&#20214;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.00037</link><description>&lt;p&gt;
&#26410;&#26469;&#21457;&#23637;&#65306;&#31038;&#20132;&#23186;&#20307;&#19978;&#30475;&#19981;&#35265;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Evolving to the Future: Unseen Event Adaptive Fake News Detection on Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#26410;&#30693;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;FADE&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30446;&#26631;&#39044;&#27979;&#22120;&#65292;&#21516;&#26102;&#29420;&#31435;&#35757;&#32451;&#20107;&#20214;&#39044;&#27979;&#22120;&#65292;&#26368;&#32456;&#20943;&#36731;&#20107;&#20214;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20551;&#26032;&#38395;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24191;&#27867;&#20256;&#25773;&#26085;&#30410;&#23041;&#32961;&#20010;&#20154;&#21644;&#31038;&#20250;&#12290;&#22312;&#31038;&#20132;&#23186;&#20307;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#20551;&#26032;&#38395;&#26816;&#27979;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#26032;&#38395;&#25253;&#36947;&#36807;&#21435;&#20107;&#20214;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#30446;&#26631;&#26159;&#39044;&#27979;&#21644;&#35782;&#21035;&#26377;&#20851;&#26410;&#26469;&#20107;&#20214;&#30340;&#20551;&#26032;&#38395;&#65292;&#36825;&#20123;&#20107;&#20214;&#36890;&#24120;&#19982;&#36807;&#21435;&#23436;&#20840;&#19981;&#21516;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#23384;&#22312;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#26080;&#27861;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26410;&#26469;&#33258;&#36866;&#24212;&#20107;&#20214;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#65288;FADE&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#31574;&#30053;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30446;&#26631;&#39044;&#27979;&#22120;&#65292;&#20197;&#36827;&#34892;&#26356;&#31283;&#20581;&#30340;&#25972;&#20307;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#29420;&#31435;&#35757;&#32451;&#19968;&#20010;&#20165;&#20107;&#20214;&#30340;&#39044;&#27979;&#22120;&#20197;&#33719;&#24471;&#26377;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#33719;&#24471;&#26368;&#32456;&#39044;&#27979;&#26469;&#36827;&#19968;&#27493;&#20943;&#36731;&#20107;&#20214;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00037v1 Announce Type: cross  Abstract: With the rapid development of social media, the wide dissemination of fake news on social media is increasingly threatening both individuals and society. In the dynamic landscape of social media, fake news detection aims to develop a model trained on news reporting past events. The objective is to predict and identify fake news about future events, which often relate to subjects entirely different from those in the past. However, existing fake detection methods exhibit a lack of robustness and cannot generalize to unseen events. To address this, we introduce Future ADaptive Event-based Fake news Detection (FADE) framework. Specifically, we train a target predictor through an adaptive augmentation strategy and graph contrastive learning to make more robust overall predictions. Simultaneously, we independently train an event-only predictor to obtain biased predictions. Then we further mitigate event bias by obtaining the final prediction
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22788;&#29702;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#20013;&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.18815</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Handle Multilingualism?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18815
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22788;&#29702;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#20013;&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#35821;&#35328;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;LLMs&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#36807;&#31243;&#65306;&#22312;&#21069;&#20960;&#23618;&#20013;&#65292;LLMs&#29702;&#35299;&#38382;&#39064;&#65292;&#23558;&#22810;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#33521;&#35821;&#20197;&#20415;&#20419;&#36827;&#20219;&#21153;&#35299;&#20915;&#38454;&#27573;&#12290;&#22312;&#20013;&#38388;&#23618;&#20013;&#65292;LLMs&#36890;&#36807;&#20197;&#33521;&#35821;&#24605;&#32771;&#24182;&#25972;&#21512;&#22810;&#35821;&#35328;&#30693;&#35782;&#26469;&#36827;&#34892;&#35299;&#20915;&#38382;&#39064;&#65292;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32467;&#26500;&#65292;&#20998;&#21035;&#33719;&#21462;&#20107;&#23454;&#20869;&#23481;&#12290;&#22312;&#26368;&#21518;&#20960;&#23618;&#20013;&#65292;LLMs&#29983;&#25104;&#19982;&#26597;&#35810;&#30340;&#21407;&#22987;&#35821;&#35328;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#29305;&#23450;&#35821;&#35328;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#26816;&#27979;&#30001;&#36755;&#20837;&#35821;&#35328;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#65292;&#21363;&#20351;&#27809;&#26377;&#26631;&#31614;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#35774;&#35745;&#20102;&#19968;&#20010;&#24182;&#34892;&#35821;&#35328;&#29305;&#23450;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18815v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specif
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20855;&#26377;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#21644;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16040</link><description>&lt;p&gt;
EHRNoteQA&#65306;&#29992;&#20110;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16040
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20855;&#26377;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#21644;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#22312;MIMIC-IV&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#30001;&#19977;&#20301;&#21307;&#30103;&#19987;&#23478;&#22242;&#38431;&#31934;&#24515;&#31574;&#21010;&#20102;&#21253;&#21547;962&#20010;&#29420;&#29305;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#29305;&#23450;&#24739;&#32773;&#30340;EHR&#20020;&#24202;&#31508;&#35760;&#30456;&#20851;&#32852;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;EHR&#30340;&#22522;&#20934;&#19981;&#21516;&#30340;&#26159;&#65306;&#39318;&#20808;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#22312;&#33258;&#21160;&#35780;&#20272;&#30340;&#32972;&#26223;&#19979;&#26377;&#25928;&#35780;&#20272;LLMs&#30340;&#24471;&#20998;&#24615;&#33021;&#65292;&#19982;&#20854;&#20182;&#26684;&#24335;&#30456;&#27604;&#12290;&#20854;&#27425;&#65292;&#23427;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#25165;&#33021;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65292;&#21453;&#26144;&#20102;&#23454;&#38469;&#20020;&#24202;&#20915;&#31574;&#21046;&#23450;&#30340;&#22797;&#26434;&#24615;&#65292;&#21307;&#29983;&#38656;&#35201;&#23457;&#26597;&#22823;&#37327;&#24739;&#32773;&#30149;&#21490;&#35760;&#24405;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16040v1 Announce Type: new  Abstract: This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows: Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various large language models
&lt;/p&gt;</description></item><item><title>AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15506</link><description>&lt;p&gt;
AgentOhana&#65306;&#20026;&#26377;&#25928;&#26234;&#33021;&#20307;&#23398;&#20064;&#35774;&#35745;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15506
&lt;/p&gt;
&lt;p&gt;
AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24341;&#36215;&#20102;&#37325;&#22823;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#36827;&#34892;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20855;&#26377;&#22810;&#36718;&#36712;&#36857;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#30340;&#24322;&#26500;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;AgentOhana&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;AgentOhana&#20174;&#19981;&#21516;&#29615;&#22659;&#20013;&#32858;&#21512;&#26234;&#33021;&#20307;&#36712;&#36857;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24773;&#26223;&#12290;&#23427;&#31934;&#24515;&#22320;&#23558;&#36825;&#20123;&#36712;&#36857;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#26684;&#24335;&#20013;&#65292;&#31616;&#21270;&#20102;&#20026;&#26234;&#33021;&#20307;&#35757;&#32451;&#20248;&#21270;&#30340;&#36890;&#29992;&#25968;&#25454;&#21152;&#36733;&#22120;&#30340;&#21019;&#24314;&#12290;&#36890;&#36807;&#25968;&#25454;&#32479;&#19968;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#27700;&#32447;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#35774;&#22791;&#20043;&#38388;&#30340;&#29420;&#31435;&#38543;&#26426;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;xLAM-v0.1&#65292;&#19968;&#20010;&#22823;&#21160;&#20316;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#65288;&#20363;&#22914;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#65289;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#65292;&#20294;&#19981;&#21516;&#20110;&#26631;&#20934;Transformer&#12290;</title><link>https://arxiv.org/abs/2402.13934</link><description>&lt;p&gt;
&#30830;&#23454;&#39640;&#25928;&#30340;Transformer&#33021;&#22815;&#33410;&#32422;&#35745;&#31639;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Efficient Transformers Really Save Computation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#65288;&#20363;&#22914;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#65289;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#65292;&#20294;&#19981;&#21516;&#20110;&#26631;&#20934;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#24182;&#25317;&#26377;&#22823;&#37327;&#21442;&#25968;&#65292;&#25214;&#21040;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26631;&#20934;Transformer&#21464;&#24471;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#39640;&#25928;&#30340;Transformer&#21644;Transformer&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#23427;&#20204;&#36866;&#21512;&#26367;&#20195;&#26631;&#20934;Transformer&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#36825;&#20351;&#24471;&#24456;&#38590;&#30830;&#23450;&#20309;&#26102;&#20351;&#29992;&#29305;&#23450;&#27169;&#22411;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23427;&#20204;&#22312;Chain-of-Thought (CoT)&#25552;&#31034;&#20013;&#23637;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36981;&#24490;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#23427;&#20204;&#24314;&#27169;&#20026;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#36275;&#22815;&#34920;&#36798;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#19982;&#26631;&#20934;Transformer&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13934v1 Announce Type: cross  Abstract: As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.04875</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Provable Length and Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#26356;&#38271;&#24207;&#21015;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#20196;&#29260;&#32452;&#21512;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#26159;&#37325;&#35201;&#30340;&#38750;&#20998;&#24067;&#21270;&#27867;&#21270;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#26550;&#26500;&#20013;&#65292;&#26397;&#30528;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#26681;&#25454;&#26550;&#26500;&#30340;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#30340;&#24517;&#35201;&#24615;&#65292;&#20363;&#22914;&#19982;&#30495;&#23454;&#34920;&#31034;&#20855;&#26377;&#32447;&#24615;&#25110;&#25490;&#21015;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
&lt;/p&gt;</description></item><item><title>&#24314;&#31435;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;NoMIRACL&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#34913;&#37327;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25351;&#26631;&#65306;&#24187;&#35273;&#29575;&#21644;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11361</link><description>&lt;p&gt;
NoMIRACL: &#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#40065;&#26834;&#22810;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11361
&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;NoMIRACL&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#34913;&#37327;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25351;&#26631;&#65306;&#24187;&#35273;&#29575;&#21644;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11361v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#26469;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36755;&#20986;&#19982;&#29616;&#23454;&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#20943;&#23569;&#20107;&#23454;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#32570;&#20047;&#23545;&#19981;&#21516;&#35821;&#35328;&#26063;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#36825;&#20351;&#24471;&#24456;&#38590;&#35780;&#20272;LLM&#23545;&#22806;&#37096;&#26816;&#32034;&#30693;&#35782;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;NoMIRACL&#65292;&#36825;&#26159;&#19968;&#20010;&#20154;&#31867;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;RAG&#20013;LLM&#23545;18&#31181;&#22312;&#31867;&#22411;&#19978;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#30340;&#40065;&#26834;&#24615;&#12290;NoMIRACL&#21253;&#25324;&#19968;&#20010;&#38750;&#30456;&#20851;&#23376;&#38598;&#21644;&#19968;&#20010;&#30456;&#20851;&#23376;&#38598;&#12290;&#38750;&#30456;&#20851;&#23376;&#38598;&#20013;&#30340;&#26597;&#35810;&#21253;&#21547;&#34987;&#21028;&#26029;&#20026;&#19981;&#30456;&#20851;&#30340;&#27573;&#33853;&#65292;&#32780;&#30456;&#20851;&#23376;&#38598;&#20013;&#30340;&#26597;&#35810;&#33267;&#23569;&#21253;&#21547;&#19968;&#20010;&#34987;&#21028;&#26029;&#20026;&#30456;&#20851;&#30340;&#27573;&#33853;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#26469;&#34913;&#37327;LLM&#30340;&#40065;&#26834;&#24615;&#65306;&#65288;i&#65289;&#24187;&#35273;&#29575;&#65292;&#34913;&#37327;&#27169;&#22411;&#20542;&#21521;&#20110;&#22312;&#38750;&#30456;&#20851;&#23376;&#38598;&#30340;&#27573;&#33853;&#20013;&#20135;&#29983;&#24187;&#35273;&#31572;&#26696;&#30340;&#31243;&#24230;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#38169;&#35823;&#29575;&#65292;&#34913;&#37327;&#27169;&#22411;&#30340;&#19981;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11361v2 Announce Type: replace  Abstract: Retrieval-augmented generation (RAG) grounds large language model (LLM) output by leveraging external knowledge sources to reduce factual hallucinations. However, prior works lack a comprehensive evaluation of different language families, making it challenging to evaluate LLM robustness against errors in external retrieved knowledge. To overcome this, we establish NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across 18 typologically diverse languages. NoMIRACL includes both a non-relevant and a relevant subset. Queries in the non-relevant subset contain passages judged as non-relevant, whereas queries in the relevant subset include at least a single judged relevant passage. We measure LLM robustness using two metrics: (i) hallucination rate, measuring model tendency to hallucinate an answer, when the answer is not present in passages in the non-relevant subset, and (ii) error rate, measuring model inaccurac
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#38750;&#24120;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;(QDIT)&#65292;&#36890;&#36807;&#21516;&#26102;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#23545;&#25351;&#20196;&#35843;&#25972;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.14736</link><description>&lt;p&gt;
&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Data Diversity Matters for Robust Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14736
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#38750;&#24120;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;(QDIT)&#65292;&#36890;&#36807;&#21516;&#26102;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#23545;&#25351;&#20196;&#35843;&#25972;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#31934;&#36873;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#22256;&#38590;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20381;&#36182;&#20110;&#25163;&#21160;&#31934;&#36873;&#25110;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#12290;&#33258;&#21160;&#25968;&#25454;&#31934;&#36873;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#20173;&#19981;&#28165;&#26970;&#22914;&#20309;&#20026;&#25351;&#20196;&#35843;&#25972;&#23450;&#20041;&#22810;&#26679;&#24615;&#65292;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#22914;&#20309;&#30456;&#20114;&#20851;&#32852;&#65292;&#20197;&#21450;&#22914;&#20309;&#20248;&#21270;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36136;&#37327;-&#22810;&#26679;&#24615;&#25351;&#20196;&#35843;&#25972;(QDIT)&#12290;QDIT&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21516;&#26102;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#28145;&#20837;&#30740;&#31350;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#23545;&#25351;&#20196;&#35843;&#25972;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20174;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#65306;(1)&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#20043;&#38388;&#23384;&#22312;&#33258;&#28982;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;(2)&#22686;&#21152;&#25968;&#25454;&#22810;&#26679;&#24615;&#26174;&#33879;&#25552;&#39640;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#25351;&#20196;&#36319;&#38543;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that by curating high quality and diverse instruction tuning datasets, we can significantly improve instruction-following capabilities. However, creating such datasets is difficult and most works rely on manual curation or proprietary language models. Automatic data curation is difficult as it is still not clear how we can define diversity for instruction tuning, how diversity and quality depend on one other, and how we can optimize dataset quality and diversity. To resolve these issue, we propose a new algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple method to simultaneously control dataset diversity and quality, allowing us to conduct an in-depth study on the effect of diversity and quality on instruction tuning performance. From this study we draw two key insights (1) there is a natural tradeoff between data diversity and quality and (2) increasing data diversity significantly improves the worst case instruction following perform
&lt;/p&gt;</description></item><item><title>StrategyLLM&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#33258;&#21160;&#26500;&#24314;&#21487;&#25512;&#24191;&#21644;&#19968;&#33268;&#30340;&#23569;&#27425;&#25552;&#31034;&#65292;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#12290;</title><link>https://arxiv.org/abs/2311.08803</link><description>&lt;p&gt;
StrategyLLM&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38382;&#39064;&#35299;&#20915;&#30340;&#31574;&#30053;&#29983;&#25104;&#22120;&#12289;&#25191;&#34892;&#22120;&#12289;&#20248;&#21270;&#22120;&#21644;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08803
&lt;/p&gt;
&lt;p&gt;
StrategyLLM&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#33258;&#21160;&#26500;&#24314;&#21487;&#25512;&#24191;&#21644;&#19968;&#33268;&#30340;&#23569;&#27425;&#25552;&#31034;&#65292;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24605;&#32500;&#38142; (CoT) &#25552;&#31034;&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24120;&#24120;&#20381;&#36182;&#20110;&#29305;&#23450;&#23454;&#20363;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#20854;&#20182;&#24773;&#20917;&#65292;&#24182;&#32570;&#20047;&#22312;&#25512;&#29702;&#27493;&#39588;&#20013;&#30340;&#20219;&#21153;&#32423;&#19968;&#33268;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;StrategyLLM&#65292;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#33258;&#21160;&#26500;&#24314;&#21487;&#25512;&#24191;&#21644;&#19968;&#33268;&#30340;&#23569;&#27425;&#25552;&#31034;&#20197;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;StrategyLLM &#20351;&#29992;&#22235;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65306;&#31574;&#30053;&#29983;&#25104;&#22120;&#12289;&#25191;&#34892;&#22120;&#12289;&#20248;&#21270;&#22120;&#21644;&#35780;&#20272;&#22120;&#65292;&#20849;&#21516;&#24037;&#20316;&#20197;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#36873;&#25321;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;13&#20010;&#25968;&#25454;&#38598;&#19978;&#36328;4&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#19978;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#65292;StrategyLLM &#22312;&#25968;&#23398;&#25512;&#29702;&#65288;34.21%-&gt;38.79%&#65289;&#12289;&#24120;&#35265;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;CoT-SC&#65292;&#35813;&#22522;&#32447;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08803v2 Announce Type: replace  Abstract: Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps. To address these limitations, we propose a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to construct generalizable and consistent few-shot prompts for various tasks automatically. To this end, StrategyLLM employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task. The experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.21% $\rightarrow$ 38.79%), commonse
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;</title><link>https://arxiv.org/abs/2303.14537</link><description>&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#65306;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14537
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36749;&#23398;&#25110;PCA&#26469;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30446;&#26631;&#23618;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#23637;&#31034;&#28145;&#24230;&#22686;&#24378;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#22914;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#19978;&#28145;&#24230;&#22686;&#24378;&#33021;&#22815;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30456;&#24212;&#30340;&#30417;&#30563;&#38382;&#39064;&#19978;&#35266;&#23519;&#21040;&#30456;&#21453;&#30340;&#25928;&#26524;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#28145;&#24230;&#22686;&#24378;&#20943;&#36731;&#20102;&#23618;&#20043;&#38388;&#30340;&#30456;&#20114;&#36866;&#24212;&#65292;&#21363;"&#23849;&#28291;"&#24418;&#24335;&#30340;&#38382;&#39064;&#12290; &#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21046;&#23450;&#20102;&#19968;&#31181;&#36873;&#25321;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65307;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29992;&#28145;&#24230;&#22686;&#24378;&#23450;&#20301;&#26356;&#28145;&#23618;&#27425;&#30340;&#23618;&#35201;&#20248;&#20110;&#22686;&#24378;&#36755;&#20837;&#25968;&#25454;&#12290; &#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#32593;&#32476;&#21644;&#27169;&#24577;&#26080;&#20851;&#24615;&#20351;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.14537v2 Announce Type: replace-cross  Abstract: We introduce Deep Augmentation, an approach to implicit data augmentation using dropout or PCA to transform a targeted layer within a neural network to improve performance and generalization. We demonstrate Deep Augmentation through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning. We observe substantial performance gains with Transformers, ResNets, and Graph Neural Networks as the underlying models in contrastive learning, but observe inverse effects on the corresponding supervised problems. Our analysis suggests that Deep Augmentation alleviates co-adaption between layers, a form of "collapse." We use this observation to formulate a method for selecting which layer to target; in particular, our experimentation reveals that targeting deeper layers with Deep Augmentation outperforms augmenting the input data. The simple network- and modality-agnostic nature of this approach enables
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15043</link><description>&lt;p&gt;
&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#65306;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#21644;&#22686;&#24378;&#23398;&#20064;&#30340;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20581;&#24247;&#25945;&#32946;&#26448;&#26009;&#30340;&#38405;&#35835;&#27700;&#24179;&#26174;&#33879;&#24433;&#21709;&#20449;&#24687;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#25509;&#35302;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#26063;&#35028;&#20154;&#32676;&#12290;&#35768;&#22810;&#24739;&#32773;&#25945;&#32946;&#36164;&#28304;&#36229;&#36807;&#20102;&#24191;&#27867;&#25509;&#21463;&#30340;&#26631;&#20934;&#30340;&#38405;&#35835;&#27700;&#24179;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#20581;&#24247;&#20449;&#24687;&#20013;&#65292;&#24613;&#38656;&#39640;&#24615;&#33021;&#30340;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#20197;&#22686;&#24378;&#20256;&#25773;&#21644;&#35782;&#23383;&#33021;&#21147;&#12290;&#36825;&#31181;&#38656;&#35201;&#22312;&#30284;&#30151;&#25945;&#32946;&#20013;&#23588;&#20026;&#36843;&#20999;&#65292;&#26377;&#25928;&#30340;&#39044;&#38450;&#21644;&#31579;&#26597;&#25945;&#32946;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24341;&#20837;&#20102;&#31616;&#21270;&#30340;&#28040;&#21270;&#30284;&#30151;&#65288;SimpleDC&#65289;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#12290;&#21033;&#29992;SimpleDC&#21644;&#29616;&#26377;&#30340;Med-EASi&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
&lt;/p&gt;</description></item><item><title>ChemDFM&#26159;&#39318;&#20010;&#38754;&#21521;&#21270;&#23398;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#21270;&#23398;&#25991;&#29486;&#21644;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20855;&#22791;&#20102;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14818</link><description>&lt;p&gt;
ChemDFM: &#21270;&#23398;&#39046;&#22495;&#23545;&#35805;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChemDFM: Dialogue Foundation Model for Chemistry. (arXiv:2401.14818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14818
&lt;/p&gt;
&lt;p&gt;
ChemDFM&#26159;&#39318;&#20010;&#38754;&#21521;&#21270;&#23398;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#21270;&#23398;&#25991;&#29486;&#21644;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20855;&#22791;&#20102;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#33324;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23427;&#20204;&#30340;&#20219;&#21153;&#27010;&#25324;&#21644;&#33258;&#30001;&#23545;&#35805;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#24110;&#21161;&#35774;&#35745;&#21270;&#23398;&#26234;&#33021;(CGI)&#65292;&#20197;&#21327;&#21161;&#21270;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#19987;&#19994;&#35821;&#35328;&#21644;&#30693;&#35782;&#65292;&#22914;&#39640;&#24230;&#20449;&#24687;&#21270;&#30340;SMILES&#31526;&#21495;&#34920;&#31034;&#27861;&#65292;&#38459;&#30861;&#20102;&#19968;&#33324;&#39046;&#22495;LLMs&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ChemDFM&#65292;&#36825;&#26159;&#39318;&#20010;&#38754;&#21521;CGI&#30340;LLM&#12290;ChemDFM-13B&#26159;&#22312;&#21270;&#23398;&#25991;&#29486;&#12289;&#25945;&#31185;&#20070;&#12289;&#35828;&#26126;&#20070;&#20197;&#21450;&#21508;&#31181;&#19968;&#33324;&#39046;&#22495;&#30340;&#25968;&#25454;&#20013;&#35757;&#32451;&#30340;34B&#20196;&#29260;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#65292;&#21516;&#26102;&#20855;&#26377;&#20808;&#36827;&#30340;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;ChemDFM&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#20195;&#34920;&#24615;&#30340;&#24320;&#28304;LLMs&#12290;&#27492;&#22806;&#65292;ChemDFM&#36824;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have established great success in the general domain of natural language processing. Their emerging task generalization and free-form dialogue capabilities can greatly help to design Chemical General Intelligence (CGI) to assist real-world research in chemistry. However, the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry. To this end, we develop ChemDFM, the first LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature, textbooks, and instructions as well as various data from the general domain. Therefore, it can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities. Extensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs. Moreover, ChemDFM can also
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#31639;&#27861;&#24615;&#33021;&#30340;&#28145;&#24230;&#27604;&#36739;&#65292;&#36824;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#26041;&#27861;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.10825</link><description>&lt;p&gt;
&#26368;&#26032;&#36827;&#23637;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey on recent advances in named entity recognition. (arXiv:2401.10825v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10825
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#31639;&#27861;&#24615;&#33021;&#30340;&#28145;&#24230;&#27604;&#36739;&#65292;&#36824;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#26041;&#27861;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20986;&#21629;&#21517;&#30495;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#23376;&#23383;&#31526;&#20018;&#65292;&#24182;&#30830;&#23450;&#20854;&#31867;&#22411;&#65288;&#20363;&#22914;&#65292;&#26159;&#21542;&#25351;&#20154;&#29289;&#25110;&#32452;&#32455;&#65289;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#26368;&#36817;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#20851;&#27880;&#20102;&#22522;&#20110;&#22270;&#21644;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24456;&#23569;&#22312;&#20854;&#20182;&#32508;&#36848;&#20013;&#28041;&#21450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#31232;&#32570;&#27880;&#37322;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20027;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#23454;&#29616;&#22312;&#21508;&#31181;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#65288;&#39046;&#22495;&#12289;&#35268;&#27169;&#21644;&#31867;&#21035;&#25968;&#65289;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#26410;&#21516;&#26102;&#32771;&#34385;&#30340;&#31639;&#27861;&#30340;&#28145;&#24230;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#27604;&#36739;&#30340;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations). In this survey, we first present an overview of recent popular approaches, but we also look at graph- and transformer- based methods including Large Language Models (LLMs) that have not had much coverage in other surveys. Second, we focus on methods designed for datasets with scarce annotations. Third, we evaluate the performance of the main NER implementations on a variety of datasets with differing characteristics (as regards their domain, their size, and their number of classes). We thus provide a deep comparison of algorithms that are never considered together. Our experiments shed some light on how the characteristics of datasets affect the behavior of the methods that we compare.
&lt;/p&gt;</description></item><item><title>DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10471</link><description>&lt;p&gt;
DeepEdit: &#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#24335;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10471
&lt;/p&gt;
&lt;p&gt;
DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#35270;&#20026;&#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeepEdit&#65288;&#22522;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#28176;&#36827;&#24335;&#35299;&#30721;&#30693;&#35782;&#32534;&#36753;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12289;&#38382;&#39064;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#26469;&#25913;&#36827;&#30693;&#35782;&#32534;&#36753;&#12290;DeepEdit&#21487;&#28789;&#27963;&#24212;&#29992;&#20110;&#25152;&#26377;&#40657;&#30418;LLMs&#65306;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#34920;&#31034;&#25110;&#36755;&#20986;&#35789;&#27719;&#20998;&#24067;&#12290;DeepEdit&#36880;&#27493;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#20462;&#25913;LLMs&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#36755;&#20986;&#23545;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#12290;&#22312;&#30693;&#35782;&#32534;&#36753;&#26041;&#38754;&#65292;DeepEdit&#22312;&#25511;&#21046;LLMs&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;MQuaKE&#19978;&#65292;DeepEdit&#22312;&#23450;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;FEDI&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#29992;&#25143;&#24773;&#32490;&#21644;&#38544;&#21547;&#21453;&#39304;&#23545;&#20219;&#21153;&#23548;&#21521;&#30340;&#25991;&#26723;&#23545;&#35805;&#36827;&#34892;&#27880;&#37322;&#30340;&#33521;&#25991;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25968;&#25454;&#26377;&#28508;&#21147;&#25913;&#21892;&#20219;&#21153;&#23436;&#25104;&#24773;&#20917;&#12289;&#29983;&#25104;&#21709;&#24212;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#21644;&#29992;&#25143;&#25509;&#21463;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.09248</link><description>&lt;p&gt;
&#20174;&#24773;&#32490;&#12289;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#38544;&#21547;&#29992;&#25143;&#21453;&#39304;&#20013;&#23398;&#20064;&#20219;&#21153;&#23548;&#21521;&#30340;&#25991;&#26723;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Learning from Emotions, Demographic Information and Implicit User Feedback in Task-Oriented Document-Grounded Dialogues. (arXiv:2401.09248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;FEDI&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#29992;&#25143;&#24773;&#32490;&#21644;&#38544;&#21547;&#21453;&#39304;&#23545;&#20219;&#21153;&#23548;&#21521;&#30340;&#25991;&#26723;&#23545;&#35805;&#36827;&#34892;&#27880;&#37322;&#30340;&#33521;&#25991;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25968;&#25454;&#26377;&#28508;&#21147;&#25913;&#21892;&#20219;&#21153;&#23436;&#25104;&#24773;&#20917;&#12289;&#29983;&#25104;&#21709;&#24212;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#21644;&#29992;&#25143;&#25509;&#21463;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#21644;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#29992;&#25143;&#25509;&#21463;&#21644;&#20139;&#21463;&#20351;&#29992;&#23427;&#20204;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#29992;&#25143;&#24773;&#32490;&#24182;&#20174;&#20182;&#20204;&#30340;&#35805;&#35821;&#20013;&#23398;&#20064;&#38544;&#21547;&#21453;&#39304;&#30340;&#32452;&#21512;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21457;&#29616;&#23578;&#26410;&#36716;&#31227;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36825;&#20123;&#25968;&#25454;&#20027;&#35201;&#26159;&#20998;&#21035;&#30740;&#31350;&#30340;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21487;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FEDI&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#29992;&#25143;&#24773;&#32490;&#21644;&#38544;&#21547;&#21453;&#39304;&#23545;&#20219;&#21153;&#23548;&#21521;&#30340;&#25991;&#26723;&#23545;&#35805;&#36827;&#34892;&#27880;&#37322;&#30340;&#33521;&#25991;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;FLAN-T5&#12289;GPT-2&#21644;LLaMA-2&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#25968;&#25454;&#26377;&#28508;&#21147;&#25913;&#21892;&#20219;&#21153;&#23436;&#25104;&#24773;&#20917;&#12289;&#29983;&#25104;&#21709;&#24212;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#21644;&#29992;&#25143;&#25509;&#21463;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of task-oriented and document-grounded dialogue systems depends on users accepting and enjoying using them. To achieve this, recently published work in the field of Human-Computer Interaction suggests that the combination of considering demographic information, user emotions and learning from the implicit feedback in their utterances, is particularly important. However, these findings have not yet been transferred to the field of Natural Language Processing, where these data are primarily studied separately. Accordingly, no sufficiently annotated dataset is available. To address this gap, we introduce FEDI, the first English dialogue dataset for task-oriented document-grounded dialogues annotated with demographic information, user emotions and implicit feedback. Our experiments with FLAN-T5, GPT-2 and LLaMA-2 show that these data have the potential to improve task completion and the factual consistency of the generated responses and user acceptance.
&lt;/p&gt;</description></item><item><title>UstanceBR&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#36164;&#28304;&#65292;&#29992;&#20110;&#30446;&#26631;&#31435;&#22330;&#39044;&#27979;&#65292;&#21253;&#21547;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;Twitter&#39046;&#22495;&#30340;86.8k&#26631;&#35760;&#31435;&#22330;&#21644;&#21457;&#24067;&#32773;&#30340;&#32593;&#32476;&#20449;&#24687;&#12290;&#36825;&#20010;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21021;&#22987;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.06374</link><description>&lt;p&gt;
UstanceBR:&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#31435;&#22330;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
UstanceBR: a multimodal language resource for stance prediction. (arXiv:2312.06374v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06374
&lt;/p&gt;
&lt;p&gt;
UstanceBR&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#36164;&#28304;&#65292;&#29992;&#20110;&#30446;&#26631;&#31435;&#22330;&#39044;&#27979;&#65292;&#21253;&#21547;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;Twitter&#39046;&#22495;&#30340;86.8k&#26631;&#35760;&#31435;&#22330;&#21644;&#21457;&#24067;&#32773;&#30340;&#32593;&#32476;&#20449;&#24687;&#12290;&#36825;&#20010;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21021;&#22987;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;UstanceBR&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;Twitter&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30446;&#26631;&#31435;&#22330;&#39044;&#27979;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#23545;&#25152;&#36873;&#30446;&#26631;&#20027;&#39064;&#30340;86.8k&#26631;&#35760;&#31435;&#22330;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;&#21457;&#24067;&#36825;&#20123;&#31435;&#22330;&#30340;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24191;&#27867;&#32593;&#32476;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35821;&#26009;&#24211;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#25991;&#26412;&#21644;&#32593;&#32476;&#30456;&#20851;&#20449;&#24687;&#30340;&#39046;&#22495;&#20869;&#21644;&#38646;&#26679;&#26412;&#31435;&#22330;&#39044;&#27979;&#30340;&#22810;&#20010;&#20351;&#29992;&#31034;&#20363;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#21021;&#22987;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces UstanceBR, a multimodal corpus in the Brazilian Portuguese Twitter domain for target-based stance prediction. The corpus comprises 86.8 k labelled stances towards selected target topics, and extensive network information about the users who published these stances on social media. In this article we describe the corpus multimodal data, and a number of usage examples in both in-domain and zero-shot stance prediction based on textand network-related information, which are intended to provide initial baseline results for future studies in the field.
&lt;/p&gt;</description></item><item><title>FiADD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28966;&#28857;&#25512;&#29702;&#27880;&#20837;&#19982;&#26131;&#22788;&#29702;&#23494;&#24230;&#21306;&#20998;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#30340;&#34920;&#38754;&#24418;&#24335;&#19982;&#26263;&#31034;&#30340;&#24418;&#24335;&#26356;&#25509;&#36817;&#65292;&#21516;&#26102;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#38598;&#32676;&#38388;&#36317;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#38544;&#24615;&#20167;&#24680;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11896</link><description>&lt;p&gt;
&#38024;&#23545;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#30340;&#28966;&#28857;&#25512;&#29702;&#27880;&#20837;&#19982;&#26131;&#22788;&#29702;&#23494;&#24230;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
Focal Inferential Infusion Coupled with Tractable Density Discrimination for Implicit Hate Speech Detection. (arXiv:2309.11896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11896
&lt;/p&gt;
&lt;p&gt;
FiADD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28966;&#28857;&#25512;&#29702;&#27880;&#20837;&#19982;&#26131;&#22788;&#29702;&#23494;&#24230;&#21306;&#20998;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#30340;&#34920;&#38754;&#24418;&#24335;&#19982;&#26263;&#31034;&#30340;&#24418;&#24335;&#26356;&#25509;&#36817;&#65292;&#21516;&#26102;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#38598;&#32676;&#38388;&#36317;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#38544;&#24615;&#20167;&#24680;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#35768;&#22810;NLP&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23545;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#24494;&#22937;&#34920;&#36798;&#30340;&#29702;&#35299;&#12290;&#36825;&#26679;&#24494;&#22937;&#32780;&#38544;&#24615;&#30340;&#20167;&#24680;&#32463;&#24120;&#34987;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#38750;&#20167;&#24680;&#12290;&#36890;&#36807;&#22686;&#21152;&#22806;&#37096;&#30340;&#19978;&#19979;&#25991;&#25110;&#36890;&#36807;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#24378;&#21046;&#26631;&#31614;&#20998;&#31163;&#65292;&#24050;&#32463;&#23581;&#35797;&#36807;&#21508;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#65288;&#38544;&#24615;&#65289;&#20167;&#24680;&#20869;&#23481;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28966;&#28857;&#25512;&#29702;&#36866;&#24212;&#23494;&#24230;&#21306;&#20998;&#26694;&#26550;&#65288;FiADD&#65289;&#12290;FiADD&#36890;&#36807;&#23558;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#30340;&#34920;&#38754;&#24418;&#24335;&#19982;&#26263;&#31034;&#30340;&#24418;&#24335;&#26356;&#25509;&#36817;&#65292;&#21516;&#26102;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#38598;&#32676;&#38388;&#36317;&#65292;&#26469;&#22686;&#24378;PLM&#24494;&#35843;&#31649;&#36947;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#38544;&#24615;&#20167;&#24680;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;FiADD&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#20004;&#31867;&#21644;&#19977;&#31867;&#20167;&#24680;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;FiADD&#22312;&#19977;&#20010;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#24615;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21363;&#26816;&#27979;&#35773;&#21050;&#12289;&#35773;&#21050;&#21644;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although pre-trained large language models (PLMs) have achieved state-of-the-art on many NLP tasks, they lack understanding of subtle expressions of implicit hate speech. Such nuanced and implicit hate is often misclassified as non-hate. Various attempts have been made to enhance the detection of (implicit) hate content by augmenting external context or enforcing label separation via distance-based metrics. We combine these two approaches and introduce FiADD, a novel Focused Inferential Adaptive Density Discrimination framework. FiADD enhances the PLM finetuning pipeline by bringing the surface form of an implicit hate speech closer to its implied form while increasing the inter-cluster distance among various class labels. We test FiADD on three implicit hate datasets and observe significant improvement in the two-way and three-way hate classification tasks. We further experiment on the generalizability of FiADD on three other tasks, namely detecting sarcasm, irony, and stance, in whic
&lt;/p&gt;</description></item><item><title>RoCar&#26159;&#19968;&#31181;&#21033;&#29992;&#20851;&#31995;&#32593;&#32476;&#26500;&#24314;&#20219;&#21153;&#22270;&#24182;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#35760;&#24518;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26497;&#22823;&#30340;&#38543;&#26426;&#24615;&#30830;&#20445;&#20102;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15997</link><description>&lt;p&gt;
RoCar:&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#32593;&#32476;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RoCar: A Relationship Network-based Evaluation Method to Large Language Models. (arXiv:2307.15997v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15997
&lt;/p&gt;
&lt;p&gt;
RoCar&#26159;&#19968;&#31181;&#21033;&#29992;&#20851;&#31995;&#32593;&#32476;&#26500;&#24314;&#20219;&#21153;&#22270;&#24182;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#35760;&#24518;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26497;&#22823;&#30340;&#38543;&#26426;&#24615;&#30830;&#20445;&#20102;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#33021;&#21147;&#30340;&#22797;&#26434;&#24615;&#65292;&#22914;&#20309;&#21512;&#29702;&#35780;&#20272;LLMs&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RoCar&#26041;&#27861;&#65292;&#21033;&#29992;&#23450;&#20041;&#30340;&#22522;&#26412;&#27169;&#24335;&#38543;&#26426;&#26500;&#24314;&#19968;&#20010;&#20219;&#21153;&#22270;&#65292;&#24182;&#22522;&#20110;&#20219;&#21153;&#22270;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#65292;&#20998;&#21035;&#35780;&#20272;LLMs&#30340;&#25512;&#29702;&#21644;&#35760;&#24518;&#33021;&#21147;&#12290;&#30001;&#20110;&#20219;&#21153;&#26500;&#24314;&#36807;&#31243;&#30340;&#26497;&#22823;&#38543;&#26426;&#24615;&#65292;&#21487;&#20197;&#30830;&#20445;&#34987;&#27979;&#35797;&#30340;LLMs&#20013;&#27809;&#26377;&#19968;&#20010;&#30452;&#25509;&#23398;&#20064;&#20102;&#35780;&#20272;&#20219;&#21153;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#35780;&#20272;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have received increasing attention. However, due to the complexity of its capabilities, how to rationally evaluate the capabilities of LLMs is still a task to be solved. We propose the RoCar method, which utilizes the defined basic schemas to randomly construct a task graph and generates natural language evaluation tasks based on the task graph to evaluate the reasoning and memory abilities of LLMs respectively. Due to the very large randomness of the task construction process, it is possible to ensure that none of the LLMs to be tested has directly learned the evaluation tasks, guaranteeing the fairness of the evaluation method.
&lt;/p&gt;</description></item></channel></rss>