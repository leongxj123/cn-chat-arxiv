<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;</title><link>https://rss.arxiv.org/abs/2312.02783</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Graphs: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT4&#21644;LLaMA&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#25991;&#26412;&#32534;&#30721;/&#35299;&#30721;&#33021;&#21147;&#21644;&#26032;&#21457;&#29616;&#30340;&#32039;&#24613;&#33021;&#21147;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#34429;&#28982;LLMs&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#32431;&#25991;&#26412;&#65292;&#20294;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#19982;&#22270;&#24418;&#24418;&#24335;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#30456;&#20851;&#32852;&#65288;&#20363;&#22914;&#23398;&#26415;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#65289;&#65292;&#25110;&#32773;&#22270;&#24418;&#25968;&#25454;&#19982;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#37197;&#23545;&#65288;&#20363;&#22914;&#24102;&#26377;&#25551;&#36848;&#30340;&#20998;&#23376;&#65289;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;LLMs&#24050;&#32463;&#23637;&#31034;&#20102;&#20854;&#22522;&#20110;&#32431;&#25991;&#26412;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#27492;&#31867;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22270;&#24418;&#19978;&#65288;&#21363;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#22330;&#26223;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#12290;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102;&#37319;&#29992;LLMs&#22312;&#22270;&#24418;&#19978;&#30340;&#28508;&#22312;&#22330;&#26223;&#65292;&#20998;&#20026;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-pa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CodeBenchGen&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20219;&#24847;&#20195;&#30721;&#36716;&#21270;&#20026;&#35780;&#20272;&#31034;&#20363;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#20195;&#30721;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;Exec-CSN&#65292;&#23637;&#31034;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00566</link><description>&lt;p&gt;
CodeBenchGen: &#21019;&#24314;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00566
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CodeBenchGen&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20219;&#24847;&#20195;&#30721;&#36716;&#21270;&#20026;&#35780;&#20272;&#31034;&#20363;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#20195;&#30721;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;Exec-CSN&#65292;&#23637;&#31034;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeBenchGen&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#21019;&#24314;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#22522;&#20934;&#65292;&#20165;&#38656;&#35201;&#36731;&#24494;&#30340;&#20154;&#31867;&#25351;&#23548;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#36716;&#21270;&#20026;&#35780;&#20272;&#31034;&#20363;&#65292;&#21253;&#25324;&#29992;&#20110;&#25191;&#34892;&#35780;&#20272;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#21253;&#21547;&#26469;&#33258;CodeSearchNet&#25968;&#25454;&#38598;&#30340;367&#20010;GitHub&#23384;&#20648;&#24211;&#20013;&#30340;&#20195;&#30721;&#20462;&#25913;&#30340;293&#20010;&#24211;&#30340;1,931&#20010;&#20363;&#23376;&#30340;&#25968;&#25454;&#38598;Exec-CSN&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;Exec-CSN&#20013;&#31034;&#20363;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20154;&#31867;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;81.3%&#30340;&#20363;&#23376;&#21487;&#20197;&#34987;&#20154;&#31867;&#35299;&#20915;&#65292;61%&#34987;&#35780;&#20026;&#8220;&#38656;&#35201;&#21162;&#21147;&#35299;&#20915;&#8221;&#12290;&#25105;&#20204;&#23545;&#24320;&#28304;&#21644;&#19987;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#20195;&#30721;&#29983;&#25104;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00566v1 Announce Type: cross  Abstract: To facilitate evaluation of code generation systems across diverse scenarios, we present CodeBenchGen, a framework to create scalable execution-based benchmarks that only requires light guidance from humans. Specifically, we leverage a large language model (LLM) to convert an arbitrary piece of code into an evaluation example, including test cases for execution-based evaluation. We illustrate the usefulness of our framework by creating a dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries revised from code in 367 GitHub repositories taken from the CodeSearchNet dataset. To demonstrate the complexity and solvability of examples in Exec-CSN, we present a human study demonstrating that 81.3% of the examples can be solved by humans and 61% are rated as ``requires effort to solve''. We conduct code generation experiments on open-source and proprietary models and analyze the performance of both humans and models. We will
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21477;&#27861;&#36317;&#31163;&#21644;&#22320;&#29702;&#37051;&#36817;&#24615;&#25506;&#32034;&#35821;&#35328;&#20851;&#31995;&#65292;&#20351;&#29992;POS trigrams&#26368;&#22823;&#21270;&#25429;&#25417;&#21477;&#27861;&#21464;&#21270;&#65292;&#24314;&#31435;&#35821;&#35328;&#36830;&#25509;&#24182;&#25581;&#31034;&#35821;&#35328;&#23478;&#26063;&#21644;&#32676;&#20307;&#30340;&#31751;&#12290;</title><link>https://arxiv.org/abs/2403.18430</link><description>&lt;p&gt;
&#36890;&#36807;&#21477;&#27861;&#36317;&#31163;&#21644;&#22320;&#29702;&#37051;&#36817;&#24615;&#25506;&#32034;&#35821;&#35328;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring language relations through syntactic distances and geographic proximity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21477;&#27861;&#36317;&#31163;&#21644;&#22320;&#29702;&#37051;&#36817;&#24615;&#25506;&#32034;&#35821;&#35328;&#20851;&#31995;&#65292;&#20351;&#29992;POS trigrams&#26368;&#22823;&#21270;&#25429;&#25417;&#21477;&#27861;&#21464;&#21270;&#65292;&#24314;&#31435;&#35821;&#35328;&#36830;&#25509;&#24182;&#25581;&#31034;&#35821;&#35328;&#23478;&#26063;&#21644;&#32676;&#20307;&#30340;&#31751;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#34987;&#20998;&#20026;&#20849;&#20139;&#30456;&#21516;&#35821;&#35328;&#29305;&#24449;&#30340;&#35821;&#31995;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#22312;&#29702;&#35299;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#22522;&#22240;&#20851;&#31995;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#20998;&#26512;&#26469;&#20934;&#30830;&#37327;&#21270;&#23427;&#20204;&#30340;&#20851;&#32852;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23569;&#30740;&#31350;&#30340;&#35821;&#35328;&#23618;&#38754;&#65292;&#27604;&#22914;&#21477;&#27861;&#12290;&#26412;&#25991;&#20351;&#29992;&#20174;Universal Dependencies&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#19968;&#31995;&#21015;&#35789;&#24615;&#65288;POS&#65289;&#26469;&#25506;&#32034;&#35821;&#35328;&#36317;&#31163;&#12290;&#22312;&#20449;&#24687;&#35770;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37319;&#29992;POS&#19977;&#20803;&#32452;&#26368;&#22823;&#21270;&#25429;&#25417;&#21477;&#27861;&#21464;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#19982;&#21487;&#29992;&#25968;&#25454;&#37327;&#20860;&#23481;&#12290;&#36890;&#36807;&#22522;&#20110;POS&#20998;&#24067;&#30340;&#25104;&#23545;&#36317;&#31163;&#35780;&#20272;&#24314;&#31435;&#35821;&#35328;&#36830;&#25509;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#26126;&#30830;&#23545;&#24212;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#35328;&#23478;&#26063;&#21644;&#32676;&#20307;&#30340;&#31751;&#65292;&#24322;&#24120;&#24773;&#20917;&#34987;&#35299;&#37322;&#20026;&#29420;&#29305;&#30340;&#24418;&#24577;&#31867;&#22411;&#23398;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#33719;&#24471;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18430v1 Announce Type: new  Abstract: Languages are grouped into families that share common linguistic traits. While this approach has been successful in understanding genetic relations between diverse languages, more analyses are needed to accurately quantify their relatedness, especially in less studied linguistic levels such as syntax. Here, we explore linguistic distances using series of parts of speech (POS) extracted from the Universal Dependencies dataset. Within an information-theoretic framework, we show that employing POS trigrams maximizes the possibility of capturing syntactic variations while being at the same time compatible with the amount of available data. Linguistic connections are then established by assessing pairwise distances based on the POS distributions. Intriguingly, our analysis reveals definite clusters that correspond to well known language families and groups, with exceptions explained by distinct morphological typologies. Furthermore, we obtain
&lt;/p&gt;</description></item><item><title>PARAMANU-AYN&#26159;&#19968;&#31181;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#39640;&#25928;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32463;&#36807;&#38754;&#21521;&#25351;&#20196;&#30340;&#24494;&#35843;&#65292;&#22312;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13681</link><description>&lt;p&gt;
PARAMANU-AYN&#65306;&#19968;&#31181;&#26377;&#25928;&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#12289;&#38754;&#21521;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13681
&lt;/p&gt;
&lt;p&gt;
PARAMANU-AYN&#26159;&#19968;&#31181;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#39640;&#25928;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32463;&#36807;&#38754;&#21521;&#25351;&#20196;&#30340;&#24494;&#35843;&#65292;&#22312;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PARAMANU-AYN&#65292;&#36825;&#26159;&#19968;&#20010;&#20165;&#22522;&#20110;&#21360;&#24230;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#25991;&#20214;&#12289;&#21360;&#24230;&#23466;&#27861;&#21644;&#21360;&#24230;&#21009;&#27861;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#26159;&#20174;&#22836;&#24320;&#22987;&#22312;&#19978;&#19979;&#25991;&#22823;&#23567;&#20026;8192&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#22312;&#22256;&#24785;&#24230;&#25351;&#26631;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27861;&#24459;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#32452;&#21253;&#25324;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#65288;&#22914;&#27861;&#24459;&#25512;&#29702;&#12289;&#21028;&#20915;&#35299;&#37322;&#12289;&#27861;&#24459;&#26465;&#27454;&#29983;&#25104;&#12289;&#27861;&#24459;&#33609;&#25311;&#12289;&#27861;&#24459;&#21512;&#21516;&#33609;&#25311;&#12289;&#26696;&#20214;&#25688;&#35201;&#12289;&#23466;&#27861;&#38382;&#39064;&#22238;&#31572;&#31561;&#65289;&#30340;10,763&#26465;&#25351;&#20196;&#36827;&#34892;&#20102;&#38024;&#23545;&#24615;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;GPT-3.5-Turbo&#23545;&#38754;&#21521;&#25351;&#20196;&#30340;&#27169;&#22411;&#30340;&#25552;&#31034;&#21709;&#24212;&#36827;&#34892;&#20102;&#22312;10&#20998;&#21046;&#24230;&#19978;&#30340;&#28165;&#26224;&#24230;&#12289;&#30456;&#20851;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#27861;&#24459;&#25512;&#29702;&#25351;&#26631;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;CPU&#19978;&#36816;&#34892;&#65292;&#24182;&#23454;&#29616;&#27599;&#31186;42.46&#20010;&#20196;&#29260;&#30340;CPU&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13681v1 Announce Type: new  Abstract: In this paper, we present PARAMANU-AYN, a language model based exclusively on case documents of the Supreme Court of India, the Constitution of India, and the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is pretrained from scratch at a context size of 8192. We evaluated our pretrained legal model on perplexity metrics. We also instruction-tuned our pretrained model on a set of 10,763 instructions covering various legal tasks such as legal reasoning, judgement explanation, legal clause generation, legal drafting, legal contract drafting, case summarization, constitutional question-answering, etc. We also evaluated the responses of prompts for instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness, and legal reasoning metrics in a scale of 10. Our model can be run on CPU and achieved 42.46 tokens/sec CPU inference speed. We found that our models, despite not being pretrained on legal books, v
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;3M-Diffusion&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#22810;&#26679;&#21270;&#12289;&#29702;&#24819;&#24773;&#20917;&#19979;&#26159;&#26032;&#39062;&#30340;&#20998;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.07179</link><description>&lt;p&gt;
3M-Diffusion&#65306;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#28508;&#22312;&#22810;&#27169;&#24577;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07179
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;3M-Diffusion&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#22810;&#26679;&#21270;&#12289;&#29702;&#24819;&#24773;&#20917;&#19979;&#26159;&#26032;&#39062;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#20998;&#23376;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#35774;&#35745;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#20998;&#23376;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#20998;&#23376;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#29983;&#25104;&#19982;&#25991;&#26412;&#25551;&#36848;&#31934;&#30830;&#21305;&#37197;&#30340;&#20998;&#23376;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#22810;&#26679;&#21270;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#26159;&#26032;&#39062;&#30340;&#20998;&#23376;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22270;&#29983;&#25104;&#26041;&#27861;3M-Diffusion&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07179v1 Announce Type: cross  Abstract: Generating molecules with desired properties is a critical task with broad applications in drug discovery and materials design. Inspired by recent advances in large language models, there is a growing interest in using natural language descriptions of molecules to generate molecules with the desired properties. Most existing methods focus on generating molecules that precisely match the text description. However, practical applications call for methods that generate diverse, and ideally novel, molecules with the desired properties. We propose 3M-Diffusion, a novel multi-modal molecular graph generation method, to address this challenge. 3M-Diffusion first encodes molecular graphs into a graph latent space aligned with text descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the late
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;</title><link>https://arxiv.org/abs/2403.03407</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#25239;&#26426;&#22120;&#65306;&#35821;&#35328;&#27169;&#22411;&#19982;&#25112;&#20105;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Human vs. Machine: Language Models and Wargames
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03407
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#20105;&#28216;&#25103;&#22312;&#20891;&#20107;&#25112;&#30053;&#30340;&#21457;&#23637;&#21644;&#22269;&#23478;&#23545;&#23041;&#32961;&#25110;&#25915;&#20987;&#30340;&#21709;&#24212;&#20013;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20986;&#29616;&#25215;&#35834;&#20102;&#26356;&#22909;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#22686;&#24378;&#30340;&#20891;&#20107;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;AI&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19982;&#20154;&#31867;&#30340;&#34892;&#20026;&#26377;&#20309;&#19981;&#21516;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25112;&#20105;&#28216;&#25103;&#23454;&#39564;&#65292;&#20849;&#26377;107&#20301;&#22269;&#23478;&#23433;&#20840;&#19987;&#23478;&#20154;&#31867;&#21442;&#19982;&#32773;&#21442;&#19982;&#65292;&#26088;&#22312;&#30740;&#31350;&#22312;&#19968;&#20010;&#34394;&#26500;&#30340;&#32654;&#20013;&#24773;&#26223;&#20013;&#30340;&#21361;&#26426;&#21319;&#32423;&#65292;&#24182;&#27604;&#36739;&#20154;&#31867;&#21442;&#19982;&#32773;&#19982;LLM&#27169;&#25311;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#21644;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#26174;&#33879;&#19968;&#33268;&#24615;&#65292;&#20294;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#27169;&#25311;&#21644;&#20154;&#31867;&#21442;&#19982;&#32773;&#20043;&#38388;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#24046;&#24322;&#65292;&#36825;&#20419;&#20351;&#20915;&#31574;&#32773;&#22312;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#36981;&#24490;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03407v1 Announce Type: cross  Abstract: Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.
&lt;/p&gt;</description></item><item><title>MAGID&#26159;&#19968;&#20010;&#29992;&#20110;&#23558;&#20165;&#25991;&#26412;&#23545;&#35805;&#22686;&#24378;&#20026;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#21453;&#39304;&#24490;&#29615;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;</title><link>https://arxiv.org/abs/2403.03194</link><description>&lt;p&gt;
MAGID&#65306;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03194
&lt;/p&gt;
&lt;p&gt;
MAGID&#26159;&#19968;&#20010;&#29992;&#20110;&#23558;&#20165;&#25991;&#26412;&#23545;&#35805;&#22686;&#24378;&#20026;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#21453;&#39304;&#24490;&#29615;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20132;&#20114;&#31995;&#32479;&#30340;&#21457;&#23637;&#21463;&#38480;&#20110;&#32570;&#20047;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#65289;&#23545;&#35805;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#23545;LLMs&#32780;&#35328;&#38656;&#35201;&#22823;&#37327;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#26816;&#32034;&#22270;&#20687;&#26469;&#22686;&#24378;&#25991;&#26412;&#23545;&#35805;&#65292;&#23384;&#22312;&#38544;&#31169;&#12289;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#31561;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MAGID&#65288;\textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues&#65289;, &#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#29992;&#21508;&#31181;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#22686;&#24378;&#20165;&#38480;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#21019;&#24314;&#30456;&#24212;&#30340;&#22270;&#20687;&#65292;&#30830;&#20445;&#19982;&#30830;&#23450;&#30340;&#25991;&#26412;&#20445;&#25345;&#19968;&#33268;&#12290;&#26368;&#21518;&#65292;MAGID&#21253;&#21547;&#20102;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#21453;&#39304;&#24490;&#29615;&#65292;&#20171;&#20110;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#27169;&#22359;&#65288;&#25991;&#26412;LLM&#65289;&#21644;&#22270;&#20687;&#36136;&#37327;&#27169;&#22359;&#65288;&#35299;&#20915;&#32654;&#23398;&#12289;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#21644;&#23433;&#20840;&#24615;&#65289;&#65292;&#20108;&#32773;&#21327;&#20316;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#25105;&#20204;&#23558;MAGID&#19982;&#20854;&#20182;SOTA&#22522;&#32447;&#22312;&#19977;&#20010;&#23545;&#35805;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03194v1 Announce Type: new  Abstract: Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce \textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images. Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;LLMs&#36328;&#35821;&#35328;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#33521;&#35821;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#20107;&#23454;&#25968;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#21516;&#26102;&#22810;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#23545;&#26469;&#33258;&#35199;&#26041;&#22823;&#38470;&#20107;&#23454;&#20449;&#24687;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.18045</link><description>&lt;p&gt;
Multi-FAct: &#20351;&#29992;FActScore&#35780;&#20272;&#22810;&#35821;&#35328;LLM&#30340;&#22810;&#21306;&#22495;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;LLMs&#36328;&#35821;&#35328;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#33521;&#35821;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#20107;&#23454;&#25968;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#21516;&#26102;&#22810;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#23545;&#26469;&#33258;&#35199;&#26041;&#22823;&#38470;&#20107;&#23454;&#20449;&#24687;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#20986;&#29616;&#20107;&#23454;&#19978;&#30340;&#24187;&#35273;&#65292;&#29983;&#25104;&#19982;&#24050;&#30693;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#25991;&#26412;&#12290;&#23613;&#31649;&#24191;&#27867;&#30740;&#31350;&#20102;&#33521;&#35821;&#20013;&#30340;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#22810;&#35821;&#35328;LLMs&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;LLMs&#36328;&#35821;&#35328;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#20107;&#23454;&#35780;&#20272;&#27969;&#31243;&#65292;&#23558;FActScore&#65288;Min&#31561;&#65292;2023&#65289;&#25913;&#32534;&#20026;&#22810;&#26679;&#21270;&#35821;&#35328;&#12290;&#25105;&#20204;&#22312;&#20061;&#31181;&#35821;&#35328;&#19978;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#33521;&#35821;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#20107;&#23454;&#25968;&#37327;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#23545;&#26469;&#33258;&#35199;&#26041;&#22823;&#38470;&#30340;&#20107;&#23454;&#20449;&#24687;&#30340;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#23545;&#25913;&#36827;&#22810;&#35821;&#35328;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#38656;&#27714;&#65292;&#24182;&#24378;&#35843;&#20102;LLMs&#30340;&#20107;&#23454;&#29983;&#25104;&#20013;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18045v1 Announce Type: new  Abstract: Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge. While extensive research has addressed this in English, little is known about multilingual LLMs. This paper systematically evaluates multilingual LLMs' factual accuracy across languages and geographic regions. We introduce a novel pipeline for multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for diverse languages. Our analysis across nine languages reveals that English consistently outperforms others in factual accuracy and quantity of generated facts. Furthermore, multilingual models demonstrate a bias towards factual information from Western continents. These findings highlight the need for improved multilingual factuality assessment and underscore geographical biases in LLMs' fact generation.
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#24615;&#21035;&#24046;&#36317;&#65292;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#21463;&#30410;&#30340;&#24615;&#21035;&#32676;&#20307;&#21508;&#19981;&#30456;&#21516;&#12290;</title><link>https://arxiv.org/abs/2402.17954</link><description>&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17954
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#24615;&#21035;&#24046;&#36317;&#65292;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#21463;&#30410;&#30340;&#24615;&#21035;&#32676;&#20307;&#21508;&#19981;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#20351;&#29992;&#22810;&#20219;&#21153;&#12289;&#22810;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35832;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#35821;&#38899;&#20219;&#21153;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#35768;&#22810;&#35821;&#35328;&#32780;&#19981;&#38656;&#35201;&#23454;&#36136;&#24615;&#30340;&#26356;&#25913;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#30340;&#35821;&#35328;&#35206;&#30422;&#20173;&#28982;&#21487;&#33021;&#25513;&#30422;&#35821;&#35328;&#20869;&#37096;&#23384;&#22312;&#30340;&#24615;&#21035;&#24046;&#36317;&#12290;&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#22810;&#35821;&#35328;ASR&#31995;&#32479;&#22312;&#24615;&#21035;&#34920;&#29616;&#24046;&#36317;&#19978;&#30340;&#24773;&#20917;&#12290;&#22312;19&#31181;&#35821;&#35328;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#20004;&#31181;&#27969;&#34892;&#27169;&#22411;&#65292;&#36328;&#36234;&#19971;&#20010;&#35821;&#35328;&#23478;&#26063;&#65292;&#25105;&#20204;&#21457;&#29616;&#26126;&#26174;&#30340;&#24615;&#21035;&#24046;&#24322;&#12290;&#19981;&#36807;&#65292;&#19981;&#21516;&#35821;&#35328;&#20013;&#21463;&#30410;&#30340;&#32676;&#20307;&#21508;&#19981;&#30456;&#21516;&#12290;&#23613;&#31649;&#22312;&#35821;&#38899;&#23398;&#21464;&#37327;&#65288;&#38899;&#39640;&#12289;&#35828;&#35805;&#36895;&#24230;&#31561;&#65289;&#19978;&#21508;&#32676;&#20307;&#38388;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#20294;&#25506;&#32034;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#21364;&#25581;&#31034;&#20102;&#25506;&#26597;&#24615;&#33021;&#21644;&#24615;&#21035;&#34920;&#29616;&#24046;&#36317;&#20043;&#38388;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;&#21363;&#65292;&#22312;&#26576;&#31181;&#35821;&#35328;&#20013;&#26356;&#23481;&#26131;&#21306;&#20998;&#35828;&#35805;&#32773;&#24615;&#21035;&#65292;&#27169;&#22411;&#23601;&#26356;&#20559;&#21521;&#20110;&#22899;&#24615;&#35828;&#35805;&#32773;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#32452;&#21035;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17954v1 Announce Type: new  Abstract: Current voice recognition approaches use multi-task, multilingual models for speech tasks like Automatic Speech Recognition (ASR) to make them applicable to many languages without substantial changes. However, broad language coverage can still mask performance gaps within languages, for example, across genders. We systematically evaluate multilingual ASR systems on gendered performance gaps. Using two popular models on three datasets in 19 languages across seven language families, we find clear gender disparities. However, the advantaged group varies between languages. While there are no significant differences across groups in phonetic variables (pitch, speaking rate, etc.), probing the model's internal states reveals a negative correlation between probe performance and the gendered performance gap. I.e., the easier to distinguish speaker gender in a language, the more the models favor female speakers. Our results show that group dispar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#21521;&#37327;&#23450;&#20041;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#26041;&#38477;&#20302;&#21040;&#19982;&#26102;&#38388;&#32447;&#24615;&#30456;&#20851;&#65292;&#34920;&#29616;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#23218;&#32654;&#65292;&#20294;&#20801;&#35768;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36828;&#36828;&#36229;&#20986;&#26631;&#20934;&#30340;&#33539;&#22260;&#12290;</title><link>https://arxiv.org/abs/2402.17512</link><description>&lt;p&gt;
Latent Attention for Linear Time Transformers
&lt;/p&gt;
&lt;p&gt;
Latent Attention for Linear Time Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#21521;&#37327;&#23450;&#20041;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#26041;&#38477;&#20302;&#21040;&#19982;&#26102;&#38388;&#32447;&#24615;&#30456;&#20851;&#65292;&#34920;&#29616;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#23218;&#32654;&#65292;&#20294;&#20801;&#35768;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36828;&#36828;&#36229;&#20986;&#26631;&#20934;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#26041;&#22686;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#36890;&#36807;&#23450;&#20041;&#28508;&#22312;&#21521;&#37327;&#30340;&#27880;&#24847;&#21147;&#26469;&#23558;&#20854;&#38477;&#20302;&#21040;&#19982;&#26102;&#38388;&#32447;&#24615;&#30456;&#20851;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#20316;&#20026;&#26631;&#20934;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#30340;&#8220;Latte Transformer&#8221;&#27169;&#22411;&#21487;&#29992;&#20110;&#21452;&#21521;&#21644;&#21333;&#21521;&#20219;&#21153;&#65292;&#22240;&#26524;&#29256;&#26412;&#20801;&#35768;&#19968;&#31181;&#22312;&#25512;&#29702;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20869;&#23384;&#21644;&#26102;&#38388;&#39640;&#25928;&#30340;&#36882;&#24402;&#23454;&#29616;&#12290;&#26631;&#20934;transformer&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#65292;&#32780;Latte Transformer&#35745;&#31639;&#19979;&#19968;&#20010;&#26631;&#35760;&#25152;&#38656;&#30340;&#26102;&#38388;&#26159;&#24658;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#35777;&#34920;&#29616;&#21487;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#23218;&#32654;&#65292;&#20294;&#20801;&#35768;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36828;&#36828;&#36229;&#20986;&#26631;&#20934;&#27880;&#24847;&#21147;&#23454;&#38469;&#21487;&#34892;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17512v1 Announce Type: new  Abstract: The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our "Latte Transformer" model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38450;&#33539;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#23475;&#24494;&#35843;&#25915;&#20987;&#30340;&#20813;&#30123;&#26465;&#20214;&#38598;&#65292;&#20197;&#24110;&#21161;&#29702;&#35299;&#22914;&#20309;&#26500;&#24314;&#21644;&#34913;&#37327;&#26410;&#26469;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2402.16382</link><description>&lt;p&gt;
&#38450;&#33539;&#26377;&#23475;&#24494;&#35843;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Immunization against harmful fine-tuning attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38450;&#33539;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#23475;&#24494;&#35843;&#25915;&#20987;&#30340;&#20813;&#30123;&#26465;&#20214;&#38598;&#65292;&#20197;&#24110;&#21161;&#29702;&#35299;&#22914;&#20309;&#26500;&#24314;&#21644;&#34913;&#37327;&#26410;&#26469;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35843;&#25972;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#32416;&#27491;&#39044;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#19981;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20851;&#27880;&#24573;&#30053;&#20102;&#21478;&#19968;&#31181;&#19981;&#19968;&#33268;&#30340;&#26469;&#28304;&#65306;&#24694;&#24847;&#34892;&#20026;&#32773;&#21487;&#33021;&#26377;&#24847;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#20197;&#23454;&#29616;&#26377;&#23475;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#28304;&#20110;&#23545;&#40784;&#35268;&#36991;&#21644;&#24494;&#35843;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#20316;&#21697;&#32570;&#20047;&#26377;&#25928;&#38450;&#24481;&#26465;&#20214;&#30340;&#28165;&#26224;&#21576;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25239;LLMs&#20013;&#26377;&#23475;&#24494;&#35843;&#30340;&#26377;&#25928;&#38450;&#24481;&#26465;&#20214;&#38598;&#65292;&#31216;&#20026;&#8220;&#20813;&#30123;&#26465;&#20214;&#8221;&#65292;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#20102;&#35299;&#22914;&#20309;&#26500;&#24314;&#21644;&#34913;&#37327;&#26410;&#26469;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;&#21033;&#29992;&#36825;&#31181;&#38450;&#24481;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19981;&#21516;&#30740;&#31350;&#26041;&#21521;&#30340;&#32508;&#21512;&#65292;&#20197;&#38450;&#27490;&#26377;&#23475;&#24494;&#35843;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#36825;&#20123;&#26465;&#20214;&#30340;&#26089;&#26399;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16382v1 Announce Type: new  Abstract: Approaches to aligning large language models (LLMs) with human values has focused on correcting misalignment that emerges from pretraining. However, this focus overlooks another source of misalignment: bad actors might purposely fine-tune LLMs to achieve harmful goals. In this paper, we present an emerging threat model that has arisen from alignment circumvention and fine-tuning attacks. However, lacking in previous works is a clear presentation of the conditions for effective defence. We propose a set of conditions for effective defence against harmful fine-tuning in LLMs called "Immunization conditions," which help us understand how we would construct and measure future defences. Using this formal framework for defence, we offer a synthesis of different research directions that might be persued to prevent harmful fine-tuning attacks and provide a demonstration of how to use these conditions experimentally showing early results of using
&lt;/p&gt;</description></item><item><title>LSTP&#25552;&#20986;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#26102;&#31354;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#25552;&#31034;&#37319;&#26679;&#22120;&#65288;TPS&#65289;&#21644;&#31354;&#38388;&#25552;&#31034;&#27714;&#35299;&#22120;&#65288;SPS&#65289;&#20197;&#21450;&#19968;&#33268;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35745;&#31639;&#25928;&#29575;&#12289;&#26102;&#38388;&#29702;&#35299;&#21644;&#31354;&#38388;-&#26102;&#38388;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.16050</link><description>&lt;p&gt;
LSTP: &#35821;&#35328;&#24341;&#23548;&#30340;&#26102;&#31354;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#38271;&#31687;&#35270;&#39057;&#25991;&#26412;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16050
&lt;/p&gt;
&lt;p&gt;
LSTP&#25552;&#20986;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#26102;&#31354;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#25552;&#31034;&#37319;&#26679;&#22120;&#65288;TPS&#65289;&#21644;&#31354;&#38388;&#25552;&#31034;&#27714;&#35299;&#22120;&#65288;SPS&#65289;&#20197;&#21450;&#19968;&#33268;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35745;&#31639;&#25928;&#29575;&#12289;&#26102;&#38388;&#29702;&#35299;&#21644;&#31354;&#38388;-&#26102;&#38388;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35270;&#39057;&#35821;&#35328;&#24314;&#27169;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22238;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#35821;&#35328;&#26597;&#35810;&#26102;&#35299;&#37322;&#38271;&#31687;&#35270;&#39057;&#30340;&#35745;&#31639;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#39640;&#32500;&#35270;&#39057;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#35821;&#35328;&#19982;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#35270;&#35273;&#32447;&#32034;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#24341;&#23548;&#30340;&#26102;&#31354;&#25552;&#31034;&#23398;&#20064;&#65288;LSTP&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#21033;&#29992;&#20809;&#27969;&#20808;&#39564;&#30340;&#26102;&#38388;&#25552;&#31034;&#37319;&#26679;&#22120;&#65288;TPS&#65289;&#65292;&#21487;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#26377;&#25928;&#25552;&#21462;&#30456;&#20851;&#35270;&#39057;&#20869;&#23481;&#65307;&#20197;&#21450;&#28789;&#24039;&#22320;&#25429;&#25417;&#35270;&#35273;&#21644;&#25991;&#26412;&#20803;&#32032;&#20043;&#38388;&#22797;&#26434;&#31354;&#38388;&#20851;&#31995;&#30340;&#31354;&#38388;&#25552;&#31034;&#27714;&#35299;&#22120;&#65288;SPS&#65289;&#12290;&#36890;&#36807;&#23558;TPS&#21644;SPS&#19982;&#19968;&#33268;&#30340;&#35757;&#32451;&#31574;&#30053;&#30456;&#21327;&#35843;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#21319;&#20102;&#35745;&#31639;&#25928;&#29575;&#12289;&#26102;&#38388;&#29702;&#35299;&#21644;&#31354;&#38388;-&#26102;&#38388;&#23545;&#40784;&#12290;&#22312;&#20004;&#20010;&#25361;&#25112;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16050v1 Announce Type: cross  Abstract: Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time. To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal Prompt Learning (LSTP). This approach features two key components: a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements. By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment. Empirical evaluations across two challeng
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13919</link><description>&lt;p&gt;
SYNFAC-EDIT: &#29992;&#20110;&#20020;&#24202;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#20223;&#32534;&#36753;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#21644;Llama&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#20107;&#23454;&#19981;&#20934;&#30830;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#20020;&#24202;NLP&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#20107;&#23454;&#23545;&#40784;&#30340;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#19988;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#26088;&#22312;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#32534;&#36753;&#21453;&#39304;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#25311;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25913;&#21892;AI&#31995;&#32479;&#36755;&#20986;&#30340;&#23454;&#38469;&#22330;&#26223;&#12290;&#23613;&#31649;GPT&#22312;&#21508;&#31181;&#20020;&#24202;NLP&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#19987;&#19994;&#27700;&#24179;&#65292;&#27604;&#22914;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65292;&#20294;&#23545;&#20854;&#25552;&#20379;&#25913;&#21892;&#36739;&#24369;LM&#25110;LLM&#29983;&#25104;&#36136;&#37327;&#30340;&#19987;&#19994;&#32423;&#32534;&#36753;&#21453;&#39304;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13919v1 Announce Type: cross  Abstract: Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation qualit
&lt;/p&gt;</description></item><item><title>&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24573;&#30053;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12817</link><description>&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65306;&#30456;&#20114;&#20316;&#29992;&#21644;&#31995;&#32479;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12817
&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24573;&#30053;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#21487;&#20197;&#22312;&#26631;&#31614;&#19981;&#36275;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20063;&#23545;&#25152;&#35859;&#30340;&#38543;&#26426;&#22240;&#32032;&#65288;&#20363;&#22914;&#25968;&#25454;&#30340;&#21464;&#21270;&#39034;&#24207;&#65289;&#24341;&#20837;&#30340;&#26080;&#27861;&#25511;&#21046;&#30340;&#38543;&#26426;&#24615;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31995;&#32479;&#22320;&#35843;&#26597;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#27979;&#37327;&#21333;&#20010;&#38543;&#26426;&#22240;&#32032;&#30340;&#30495;&#23454;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#20102;&#24615;&#33021;&#22312;&#22810;&#27425;&#36816;&#34892;&#20013;&#30340;&#21464;&#21270;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;7&#20010;&#20195;&#34920;&#24615;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#26041;&#27861;&#20197;&#21450;3&#20010;&#20219;&#21153;&#30340;&#20803;&#23398;&#20064;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#29616;&#26377;&#20316;&#21697;&#20013;&#24573;&#30053;&#38543;&#26426;&#22240;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#20102;&#19981;&#19968;&#33268;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#22240;&#20026;&#38169;&#35823;&#22320;&#24402;&#22240;&#20110;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#21542;&#23450;&#20102;&#19968;&#20123;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12817v1 Announce Type: cross  Abstract: While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08702</link><description>&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08702
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt optimization aims to find the best prompt for a language model (LLM) in multi-step tasks. This paper introduces a genetic algorithm framework that incorporates human feedback to automatically suggest prompt improvements. It addresses challenges such as complex prompt content analysis, evaluation of individual steps, and variations in task execution preferences.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08702v1 Announce Type: cross Abstract: Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement. Our framework is stylized as a genetic algorithm in which an LLM generates new candidate prompts from a parent
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#23545;&#35299;&#39064;&#20219;&#21153;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05201</link><description>&lt;p&gt;
&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Sampling Temperature on Problem Solving in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05201
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#23545;&#35299;&#39064;&#20219;&#21153;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35843;&#26597;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35299;&#39064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#26631;&#20934;LLM&#22522;&#20934;&#20013;&#38543;&#26426;&#25277;&#21462;&#38382;&#39064;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQA&#65289;&#32771;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#31181;&#24120;&#35265;&#30340;LLM&#20197;&#21450;&#20116;&#31181;&#25552;&#31034;&#24341;&#25806;&#25216;&#26415;&#26469;&#35299;&#20915;MCQA&#38382;&#39064;&#65292;&#21516;&#26102;&#23558;&#37319;&#26679;&#28201;&#24230;&#20174;0.0&#22686;&#21152;&#21040;1.0&#12290;&#23613;&#31649;&#26377;&#20851;&#30340;&#25253;&#36947;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#22312;&#35299;&#39064;&#20219;&#21153;&#20013;&#30340;&#21464;&#21270;&#27809;&#26377;&#32479;&#35745;&#23398;&#19978;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#20284;&#20046;&#19981;&#21463;LLM&#12289;&#25552;&#31034;&#24341;&#25806;&#25216;&#26415;&#25110;&#38382;&#39064;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25152;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#34917;&#20805;&#36164;&#26009;&#37117;&#21487;&#20197;&#22312;GitHub&#19978;&#25214;&#21040;&#65306;https://github.com/matthewrenze/jhu-llm-temperature&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used four popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to hold regardless of the LLM, the prompt-engineering technique, or the problem domain. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.
&lt;/p&gt;</description></item><item><title>LV-Eval&#26159;&#19968;&#20010;&#20855;&#26377;&#20116;&#20010;&#38271;&#24230;&#32423;&#21035;&#30340;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#25903;&#25345;256k&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#24182;&#20855;&#26377;&#28151;&#28102;&#20107;&#23454;&#12289;&#20851;&#38190;&#35789;&#21644;&#30701;&#35821;&#26367;&#25442;&#20197;&#21450;&#22522;&#20110;&#20851;&#38190;&#35789;&#22238;&#24518;&#30340;&#24230;&#37327;&#35774;&#35745;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#20943;&#23569;&#30693;&#35782;&#27844;&#28431;&#21644;&#25552;&#20379;&#26356;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.05136</link><description>&lt;p&gt;
LV-Eval:&#19968;&#20010;&#24179;&#34913;&#30340;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#20855;&#26377;5&#20010;&#38271;&#24230;&#32423;&#21035;&#65292;&#26368;&#22810;&#21487;&#36798;256K
&lt;/p&gt;
&lt;p&gt;
LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05136
&lt;/p&gt;
&lt;p&gt;
LV-Eval&#26159;&#19968;&#20010;&#20855;&#26377;&#20116;&#20010;&#38271;&#24230;&#32423;&#21035;&#30340;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#25903;&#25345;256k&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#24182;&#20855;&#26377;&#28151;&#28102;&#20107;&#23454;&#12289;&#20851;&#38190;&#35789;&#21644;&#30701;&#35821;&#26367;&#25442;&#20197;&#21450;&#22522;&#20110;&#20851;&#38190;&#35789;&#22238;&#24518;&#30340;&#24230;&#37327;&#35774;&#35745;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#20943;&#23569;&#30693;&#35782;&#27844;&#28431;&#21644;&#25552;&#20379;&#26356;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29616;&#22312;&#22768;&#31216;&#25903;&#25345;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21487;&#20197;&#36798;&#21040;256k&#29978;&#33267;&#26356;&#22810;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20027;&#27969;&#22522;&#20934;&#27979;&#35797;&#30340;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#19981;&#36275;&#65288;5k-21k&#65289;&#65292;&#24182;&#19988;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#30693;&#35782;&#27844;&#28431;&#21644;&#19981;&#20934;&#30830;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#23548;&#33268;&#35780;&#20272;&#32467;&#26524;&#20559;&#35265;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LV-Eval&#65292;&#19968;&#20010;&#20855;&#26377;&#20116;&#20010;&#38271;&#24230;&#32423;&#21035;&#65288;16k&#65292;32k&#65292;64k&#65292;128k&#21644;256k&#65289;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#26368;&#22810;&#21487;&#36798;256k&#20010;&#21333;&#35789;&#12290;LV-Eval&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#65292;&#21333;&#36339;&#38382;&#31572;&#21644;&#22810;&#36339;&#38382;&#31572;&#65292;&#21253;&#21547;11&#20010;&#21452;&#35821;&#25968;&#25454;&#38598;&#12290;LV-Eval&#30340;&#35774;&#35745;&#34701;&#21512;&#20102;&#19977;&#20010;&#20851;&#38190;&#25216;&#26415;&#65292;&#21363;&#28151;&#28102;&#20107;&#23454;&#25554;&#20837;&#12289;&#20851;&#38190;&#35789;&#21644;&#30701;&#35821;&#26367;&#25442;&#20197;&#21450;&#22522;&#20110;&#20851;&#38190;&#35789;&#22238;&#24518;&#30340;&#24230;&#37327;&#35774;&#35745;&#12290;LV-Eval&#30340;&#20248;&#28857;&#21253;&#25324;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#21487;&#25511;&#35780;&#20272;&#12289;&#20855;&#26377;&#28151;&#28102;&#20107;&#23454;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#23454;&#20363;&#12289;&#20943;&#23569;&#30340;&#30693;&#35782;&#27844;&#28431;&#20197;&#21450;&#26356;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;LV-Eval&#19978;&#35780;&#20272;&#20102;10&#20010;LLMs&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of 256k or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (5k-21k), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces LV-Eval, a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of LV-Eval has incorporated three key techniques, namely confusing facts insertion, keyword and phrase replacement, and keyword-recall-based metric design. The advantages of LV-Eval include controllable evaluation across different context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluations. We evaluate 10 LLMs on LV-Eval and conduct ablation studies o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#34394;&#20551;&#22270;&#20687;&#20250;&#23548;&#33268;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35780;&#20272;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;CorrelationQA&#65292;&#24182;&#21457;&#29616;&#20027;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.03757</link><description>&lt;p&gt;
&#26412;&#33021;&#20559;&#35265;&#65306;&#34394;&#20551;&#22270;&#20687;&#23548;&#33268;MLLMs&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#34394;&#20551;&#22270;&#20687;&#20250;&#23548;&#33268;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35780;&#20272;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;CorrelationQA&#65292;&#24182;&#21457;&#29616;&#20027;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20986;&#29616;&#20351;LLMs&#20855;&#22791;&#20102;&#35270;&#35273;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20687;GPT-4V&#36825;&#26679;&#24378;&#22823;&#30340;MLLMs&#22312;&#38754;&#23545;&#26576;&#20123;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#26102;&#20173;&#28982;&#20197;&#24778;&#20154;&#30340;&#26041;&#24335;&#22833;&#36133;&#20102;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31867;&#20856;&#22411;&#36755;&#20837;&#65292;&#36825;&#20123;&#36755;&#20837;&#20196;MLLMs&#22256;&#24785;&#65292;&#23427;&#20204;&#30001;&#39640;&#24230;&#30456;&#20851;&#20294;&#19982;&#31572;&#26696;&#19981;&#19968;&#33268;&#30340;&#22270;&#20687;&#32452;&#25104;&#65292;&#23548;&#33268;MLLMs&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CorrelationQA&#65292;&#36825;&#26159;&#39318;&#20010;&#35780;&#20272;&#32473;&#23450;&#34394;&#20551;&#22270;&#20687;&#30340;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;13&#20010;&#31867;&#21035;&#30340;7,308&#20010;&#25991;&#26412;-&#22270;&#20687;&#23545;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;CorrelationQA&#65292;&#25105;&#20204;&#23545;9&#20010;&#20027;&#27969;MLLMs&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#23427;&#20204;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#31934;&#36873;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#32467;&#26524;&#33021;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.16332</link><description>&lt;p&gt;
&#23545;&#40784;&#21644;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tradeoffs Between Alignment and Helpfulness in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#22686;&#24378;&#26399;&#26395;&#34892;&#20026;&#21644;&#25233;&#21046;&#38750;&#26399;&#26395;&#34892;&#20026;&#65292;&#23454;&#29616;&#20154;&#31867;&#19982;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#23433;&#20840;&#20132;&#20114;&#12290;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#25110;&#25554;&#20837;&#39044;&#35774;&#30340;&#23545;&#40784;&#25552;&#31034;&#26469;&#23454;&#29616;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#21518;&#30340;&#34920;&#31034;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#34920;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;&#34920;&#31034;&#24037;&#31243;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#38477;&#20302;&#31038;&#20250;&#20559;&#35265;&#31561;&#23545;&#40784;&#23548;&#21521;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#22686;&#30410;&#65292;&#20294;&#20063;&#23548;&#33268;&#20102;&#27169;&#22411;&#25191;&#34892;&#22522;&#26412;&#20219;&#21153;&#33021;&#21147;&#30340;&#38477;&#20302;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#27169;&#22411;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27169;&#22411;&#30340;&#26377;&#29992;&#24615;&#36890;&#24120;&#20250;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocLens&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#32452;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20854;&#23545;&#21307;&#23398;&#25991;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#20154;&#31867;&#30740;&#31350;&#34920;&#26126;&#20854;&#22312;&#19982;&#21307;&#23398;&#19987;&#23478;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#19978;&#20248;&#20110;&#29616;&#26377;&#25351;&#26631;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#25913;&#36827;&#24320;&#28304;&#35780;&#20272;&#32773;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.09581</link><description>&lt;p&gt;
DocLens: &#22810;&#26041;&#38754;&#32454;&#31890;&#24230;&#35780;&#20272;&#21307;&#23398;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DocLens: Multi-aspect Fine-grained Evaluation for Medical Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocLens&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#32452;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20854;&#23545;&#21307;&#23398;&#25991;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#20154;&#31867;&#30740;&#31350;&#34920;&#26126;&#20854;&#22312;&#19982;&#21307;&#23398;&#19987;&#23478;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#19978;&#20248;&#20110;&#29616;&#26377;&#25351;&#26631;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#25913;&#36827;&#24320;&#28304;&#35780;&#20272;&#32773;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25991;&#26412;&#29983;&#25104;&#26088;&#22312;&#21327;&#21161;&#34892;&#25919;&#24037;&#20316;&#24182;&#31361;&#20986;&#35201;&#28857;&#20449;&#24687;&#20197;&#25903;&#25345;&#20915;&#31574;&#21046;&#23450;&#12290;&#20026;&#20102;&#21453;&#26144;&#21307;&#23398;&#25991;&#26412;&#30340;&#29305;&#23450;&#35201;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#22312;&#32454;&#31890;&#24230;&#27700;&#24179;&#19978;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#23436;&#25972;&#24615;&#12289;&#31616;&#26126;&#24615;&#21644;&#24402;&#22240;&#24615;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#21487;&#20197;&#30001;&#21508;&#31181;&#31867;&#22411;&#30340;&#35780;&#20272;&#32773;&#35745;&#31639;&#65292;&#21253;&#25324;&#36981;&#24490;&#35828;&#26126;&#65288;&#19987;&#26377;&#21644;&#24320;&#28304;&#65289;&#21644;&#30417;&#30563;&#34164;&#28085;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#35780;&#20272;&#32773;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25152;&#24471;&#20986;&#30340;&#26694;&#26550;DocLens&#30340;&#26377;&#25928;&#24615;&#65306;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#21644;&#24739;&#32773;&#38382;&#39064;&#25688;&#35201;&#12290;&#19968;&#39033;&#20840;&#38754;&#30340;&#20154;&#31867;&#30740;&#31350;&#26174;&#31034;&#65292;DocLens&#19982;&#21307;&#23398;&#19987;&#23478;&#30340;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#39640;&#30340;&#19968;&#33268;&#24615;&#65292;&#39640;&#20110;&#29616;&#26377;&#25351;&#26631;&#12290;&#32467;&#26524;&#36824;&#31361;&#20986;&#26174;&#31034;&#20102;&#25913;&#36827;&#24320;&#28304;&#35780;&#20272;&#32773;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09581v2 Announce Type: replace  Abstract: Medical text generation aims to assist with administrative work and highlight salient information to support decision-making. To reflect the specific requirements of medical text, in this paper, we propose a set of metrics to evaluate the completeness, conciseness, and attribution of the generated text at a fine-grained level. The metrics can be computed by various types of evaluators including instruction-following (both proprietary and open-source) and supervised entailment models. We demonstrate the effectiveness of the resulting framework, DocLens, with three evaluators on three tasks: clinical note generation, radiology report summarization, and patient question summarization. A comprehensive human study shows that DocLens exhibits substantially higher agreement with the judgments of medical experts than existing metrics. The results also highlight the need to improve open-source evaluators and suggest potential directions.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#33258;&#28982;&#30340;&#12289;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;</title><link>https://arxiv.org/abs/2304.08460</link><description>&lt;p&gt;
LongForm: &#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LongForm: Effective Instruction Tuning with Reverse Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.08460
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#33258;&#28982;&#30340;&#12289;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Instruction tuning&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#27867;&#21270;&#65292;&#24182;&#26356;&#22909;&#22320;&#36981;&#24490;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#25351;&#20196;&#25968;&#25454;&#25104;&#26412;&#39640;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#37319;&#29992;&#20102;&#35832;&#22914;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12289;&#23384;&#22312;&#23545;&#40784;&#38382;&#39064;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#12289;&#20197;&#21450;&#36890;&#36807;LLMs&#29983;&#25104;&#22122;&#22768;&#31034;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LongForm-C&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#21453;&#21521;&#25351;&#20196;&#21019;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#20026;&#20154;&#31867;&#20889;&#20316;&#35821;&#26009;&#24211;&#31034;&#20363;&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#29983;&#25104;&#25351;&#20196;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35832;&#22914;C4&#21644;Wikipedia&#30340;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#20102;&#19968;&#32452;&#22810;&#26679;&#24615;&#30340;&#20154;&#31867;&#25776;&#20889;&#25991;&#26723;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#20026;&#36825;&#20123;&#25991;&#26723;&#29983;&#25104;&#25351;&#20196;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#20415;&#23452;&#12289;&#26356;&#24178;&#20928;&#12289;&#36755;&#20986;&#33258;&#28982;&#20197;&#21450;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08460v2 Announce Type: replace-cross Abstract: Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instructions. We generate instructions via LLMs for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#21435;&#37325;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#38382;&#39064;&#21644;&#27169;&#22411;&#20559;&#24046;&#12290;&#36890;&#36807;&#21024;&#38500;&#37325;&#22797;&#30340;&#25991;&#26412;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#29702;&#35299;&#24615;&#33021;&#24182;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.05883</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36873;&#25321;&#30340;&#29983;&#25104;&#21435;&#37325;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generative Deduplication For Socia Media Data Selection. (arXiv:2401.05883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05883
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#21435;&#37325;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#38382;&#39064;&#21644;&#27169;&#22411;&#20559;&#24046;&#12290;&#36890;&#36807;&#21024;&#38500;&#37325;&#22797;&#30340;&#25991;&#26412;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#29702;&#35299;&#24615;&#33021;&#24182;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21463;&#20854;&#22122;&#22768;&#29305;&#24615;&#30340;&#24433;&#21709;&#65292;&#23384;&#22312;&#20887;&#20313;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#22686;&#21152;&#21644;&#27169;&#22411;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29983;&#25104;&#21435;&#37325;&#12290;&#23427;&#26088;&#22312;&#20174;&#22024;&#26434;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#21024;&#38500;&#37325;&#22797;&#30340;&#25991;&#26412;&#65292;&#24182;&#20943;&#36731;&#27169;&#22411;&#20559;&#24046;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#29702;&#35299;&#24615;&#33021;&#24182;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25552;&#20986;&#30340;&#29983;&#25104;&#21435;&#37325;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#35757;&#32451;&#26679;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#19968;&#35777;&#25454;&#34920;&#26126;&#29983;&#25104;&#21435;&#37325;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media data is plagued by the redundancy problem caused by its noisy nature, leading to increased training time and model bias. To address this issue, we propose a novel approach called generative duplication. It aims to remove duplicate text from noisy social media data and mitigate model bias. By doing so, it can improve social media language understanding performance and save training time. Extensive experiments demonstrate that the proposed generative deduplication can effectively reduce training samples while improving performance. This evidence suggests the effectiveness of generative deduplication and its importance in social media language understanding.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#26088;&#22312;&#29702;&#35299;&#36825;&#20123;&#33021;&#21147;&#30340;&#26426;&#21046;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#35299;&#20915;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2311.00237</link><description>&lt;p&gt;
LLMs&#30340;&#31070;&#31192;&#21644;&#36855;&#20154;&#20043;&#22788;&#65306;&#32039;&#23494;&#35843;&#26597;&#23545;&#26032;&#20852;&#33021;&#21147;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities. (arXiv:2311.00237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#26088;&#22312;&#29702;&#35299;&#36825;&#20123;&#33021;&#21147;&#30340;&#26426;&#21046;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#35299;&#20915;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26032;&#20852;&#33021;&#21147;&#65292;&#22914;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#21644;&#24605;&#32500;&#38142;(CoT)&#35302;&#21457;&#65292;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#37325;&#35201;&#24615;&#19981;&#20165;&#26469;&#33258;&#20110;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#65292;&#36824;&#21253;&#25324;&#20027;&#21160;&#35782;&#21035;&#21644;&#32531;&#35299;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#21253;&#25324;&#30495;&#23454;&#24615;&#12289;&#20559;&#35265;&#21644;&#26377;&#23475;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#22312;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#35299;&#37322;&#21644;&#20998;&#26512;&#26041;&#38754;&#25552;&#20986;&#20102;&#19968;&#39033;&#28145;&#20837;&#35843;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;&#26032;&#20852;&#33021;&#21147;&#30340;&#32972;&#26223;&#21644;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#27010;&#36848;&#20102;&#30740;&#31350;&#30340;&#36827;&#23637;&#65306;1)&#23439;&#35266;&#35282;&#24230;&#65292;&#24378;&#35843;&#23545;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#26032;&#20852;&#33021;&#21147;&#32972;&#21518;&#30340;&#25968;&#23398;&#22522;&#30784;&#65307;2)&#24494;&#35266;&#35282;&#24230;&#65292;&#20851;&#27880;&#36890;&#36807;&#32771;&#23519;&#19982;&#36825;&#20123;&#33021;&#21147;&#30456;&#20851;&#30340;&#22240;&#32032;&#26469;&#23454;&#35777;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding emergent abilities, such as in-context learning (ICL) and chain-of-thought (CoT) prompting in large language models (LLMs), is of utmost importance. This importance stems not only from the better utilization of these capabilities across various tasks, but also from the proactive identification and mitigation of potential risks, including concerns of truthfulness, bias, and toxicity, that may arise alongside these capabilities. In this paper, we present a thorough survey on the interpretation and analysis of emergent abilities of LLMs. First, we provide a concise introduction to the background and definition of emergent abilities. Then, we give an overview of advancements from two perspectives: 1) a macro perspective, emphasizing studies on the mechanistic interpretability and delving into the mathematical foundations behind emergent abilities; and 2) a micro-perspective, concerning studies that focus on empirical interpretability by examining factors associated with these
&lt;/p&gt;</description></item><item><title>Ada-Instruct&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#25351;&#20196;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#38271;&#24230;&#22823;&#20110;&#31561;&#20110;100&#30340;&#25351;&#20196;&#12290;&#22312;&#20195;&#30721;&#34917;&#20840;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#65292;Ada-Instruct&#26174;&#31034;&#20986;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#24403;&#21069;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.04484</link><description>&lt;p&gt;
Ada-Instruct: &#20026;&#22797;&#26434;&#25512;&#29702;&#35843;&#25972;&#25351;&#20196;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Ada-Instruct: Adapting Instruction Generators for Complex Reasoning. (arXiv:2310.04484v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04484
&lt;/p&gt;
&lt;p&gt;
Ada-Instruct&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#25351;&#20196;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#38271;&#24230;&#22823;&#20110;&#31561;&#20110;100&#30340;&#25351;&#20196;&#12290;&#22312;&#20195;&#30721;&#34917;&#20840;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#65292;Ada-Instruct&#26174;&#31034;&#20986;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#24403;&#21069;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#22810;&#26679;&#19988;&#22797;&#26434;&#30340;&#25351;&#20196;&#23545;&#20110;&#25512;&#36827;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21033;&#29992;&#38381;&#28304;&#30340;LLMs&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#36827;&#34892;&#25351;&#20196;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#21457;&#29616;&#23545;&#20110;&#35832;&#22914;&#20195;&#30721;&#34917;&#20840;&#31561;&#20219;&#21153;&#65292;&#19978;&#19979;&#25991;&#25552;&#31034;&#26080;&#27861;&#29983;&#25104;&#38271;&#24230;&#22823;&#20110;&#31561;&#20110;100&#30340;&#22797;&#26434;&#25351;&#20196;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;Ada-Instruct&#65292;&#19968;&#31181;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#25351;&#20196;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#21313;&#20010;&#26679;&#26412;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#21363;&#21487;&#29983;&#25104;&#20445;&#25345;&#20998;&#24067;&#19968;&#33268;&#24615;&#30340;&#38271;&#25351;&#20196;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20195;&#30721;&#34917;&#20840;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#19981;&#21516;&#24212;&#29992;&#20013;&#23545;Ada-Instruct&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#32467;&#26524;&#26174;&#31034;Ada-Instruct&#20248;&#20110;&#20854;&#22522;&#26412;&#27169;&#22411;&#21644;&#24403;&#21069;&#30340;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating diverse and sophisticated instructions for downstream tasks by Large Language Models (LLMs) is pivotal for advancing the effect. Current approaches leverage closed-source LLMs, employing in-context prompting for instruction generation. However, in this paper, we found that in-context prompting cannot generate complex instructions with length $\ge 100$ for tasks like code completion.  To solve this problem, we introduce Ada-Instruct, an adaptive instruction generator developed by fine-tuning open-source LLMs. Our pivotal finding illustrates that fine-tuning open-source LLMs with a mere ten samples generates long instructions that maintain distributional consistency for complex reasoning tasks. We empirically validated Ada-Instruct's efficacy across different applications, including code completion, mathematical reasoning, and commonsense reasoning. The results underscore Ada-Instruct's superiority, evidencing its improvements over its base models, current self-instruct method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#33521;&#35821;&#12289;&#27861;&#35821;&#12289;&#24503;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#12289;&#26085;&#35821;&#12289;&#25386;&#23041;&#35821;&#21644;&#20013;&#25991;&#30340;&#33050;&#26412;&#23545;&#35805;&#21644;&#33258;&#21457;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#37327;&#21270;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#20132;&#27969;&#21453;&#39304;&#29616;&#35937;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#23545;&#35805;&#31867;&#22411;&#22312;&#20132;&#27969;&#21453;&#39304;&#21644;&#33853;&#22320;&#29616;&#35937;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.15656</link><description>&lt;p&gt;
&#33050;&#26412;&#23545;&#35805;&#19982;&#33258;&#21457;&#23545;&#35805;&#20013;&#30340;&#20250;&#35805;&#21453;&#39304;&#65306;&#19968;&#31181;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative Analysis. (arXiv:2309.15656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#33521;&#35821;&#12289;&#27861;&#35821;&#12289;&#24503;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#12289;&#26085;&#35821;&#12289;&#25386;&#23041;&#35821;&#21644;&#20013;&#25991;&#30340;&#33050;&#26412;&#23545;&#35805;&#21644;&#33258;&#21457;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#37327;&#21270;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#20132;&#27969;&#21453;&#39304;&#29616;&#35937;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#23545;&#35805;&#31867;&#22411;&#22312;&#20132;&#27969;&#21453;&#39304;&#21644;&#33853;&#22320;&#29616;&#35937;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33050;&#26412;&#23545;&#35805;&#65292;&#22914;&#30005;&#24433;&#21644;&#30005;&#35270;&#23383;&#24149;&#65292;&#26500;&#25104;&#20102;&#20250;&#35805;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24191;&#27867;&#35757;&#32451;&#25968;&#25454;&#28304;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23545;&#35805;&#30340;&#35821;&#35328;&#29305;&#28857;&#19982;&#33258;&#21457;&#20132;&#20114;&#30340;&#35821;&#26009;&#24211;&#20013;&#35266;&#23519;&#21040;&#30340;&#35821;&#35328;&#29305;&#28857;&#26126;&#26174;&#19981;&#21516;&#12290;&#29305;&#21035;&#26159;&#22312;&#20132;&#27969;&#21453;&#39304;&#21644;&#33853;&#22320;&#29616;&#35937;&#65288;&#22914;&#22238;&#24212;&#12289;&#30830;&#35748;&#25110;&#28548;&#28165;&#35201;&#27714;&#65289;&#26041;&#38754;&#65292;&#36825;&#31181;&#24046;&#24322;&#23588;&#20026;&#26126;&#26174;&#12290;&#36825;&#20123;&#20449;&#21495;&#34987;&#35748;&#20026;&#26159;&#20250;&#35805;&#27969;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#30001;&#23545;&#35805;&#21442;&#19982;&#32773;&#29992;&#20110;&#23545;&#24444;&#27492;&#20043;&#38388;&#27491;&#22312;&#36827;&#34892;&#30340;&#20132;&#20114;&#30340;&#24863;&#30693;&#25552;&#20379;&#21453;&#39304;&#12290;&#26412;&#25991;&#22312;&#33521;&#35821;&#12289;&#27861;&#35821;&#12289;&#24503;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#12289;&#26085;&#35821;&#12289;&#25386;&#23041;&#35821;&#21644;&#20013;&#25991;&#30340;&#23545;&#35805;&#25968;&#25454;&#22522;&#30784;&#19978;&#65292;&#36827;&#34892;&#20102;&#36825;&#31867;&#20132;&#27969;&#21453;&#39304;&#29616;&#35937;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#35789;&#27719;&#32479;&#35745;&#21644;&#20351;&#29992;&#31070;&#32463;&#23545;&#35805;&#34892;&#20026;&#26631;&#35760;&#22120;&#33719;&#24471;&#30340;&#20998;&#31867;&#36755;&#20986;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#21457;&#29616;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scripted dialogues such as movie and TV subtitles constitute a widespread source of training data for conversational NLP models. However, the linguistic characteristics of those dialogues are notably different from those observed in corpora of spontaneous interactions. This difference is particularly marked for communicative feedback and grounding phenomena such as backchannels, acknowledgments, or clarification requests. Such signals are known to constitute a key part of the conversation flow and are used by the dialogue participants to provide feedback to one another on their perception of the ongoing interaction. This paper presents a quantitative analysis of such communicative feedback phenomena in both subtitles and spontaneous conversations. Based on dialogue data in English, French, German, Hungarian, Italian, Japanese, Norwegian and Chinese, we extract both lexical statistics and classification outputs obtained with a neural dialogue act tagger. Two main findings of this empiri
&lt;/p&gt;</description></item><item><title>PharmacyGPT&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20223;&#30495;&#20020;&#24202;&#33647;&#24072;&#30340;&#35282;&#33394;&#12290;&#36890;&#36807;&#29983;&#25104;&#24739;&#32773;&#32676;&#38598;&#12289;&#21046;&#23450;&#29992;&#33647;&#35745;&#21010;&#21644;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#65292;PharmacyGPT&#22312;&#20020;&#24202;&#33647;&#23398;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#20026;&#20419;&#36827;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.10432</link><description>&lt;p&gt;
PharmacyGPT&#65306;AI&#33647;&#24072;
&lt;/p&gt;
&lt;p&gt;
PharmacyGPT: The AI Pharmacist. (arXiv:2307.10432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10432
&lt;/p&gt;
&lt;p&gt;
PharmacyGPT&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20223;&#30495;&#20020;&#24202;&#33647;&#24072;&#30340;&#35282;&#33394;&#12290;&#36890;&#36807;&#29983;&#25104;&#24739;&#32773;&#32676;&#38598;&#12289;&#21046;&#23450;&#29992;&#33647;&#35745;&#21010;&#21644;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#65292;PharmacyGPT&#22312;&#20020;&#24202;&#33647;&#23398;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#20026;&#20419;&#36827;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PharmacyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#22914;ChatGPT&#21644;GPT-4&#65289;&#22312;&#20223;&#30495;&#20020;&#24202;&#33647;&#24072;&#35282;&#33394;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#21033;&#29992;LLM&#29983;&#25104;&#21487;&#29702;&#35299;&#30340;&#24739;&#32773;&#32676;&#38598;&#12289;&#21046;&#23450;&#29992;&#33647;&#35745;&#21010;&#21644;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#21271;&#21345;&#32599;&#26469;&#32435;&#22823;&#23398;&#25945;&#22530;&#23665;&#21307;&#38498;&#65288;UNC&#65289;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#33719;&#21462;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;LLM&#22312;&#20020;&#24202;&#33647;&#23398;&#39046;&#22495;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#23545;&#24739;&#32773;&#25252;&#29702;&#21644;&#26410;&#26469;&#22522;&#20110;AI&#30340;&#21307;&#30103;&#35299;&#20915;&#26041;&#26696;&#30340;&#24320;&#21457;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#35780;&#20272;PharmacyGPT&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#26377;&#20851;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#35752;&#35770;&#20570;&#20986;&#36129;&#29486;&#65292;&#26368;&#32456;&#20419;&#36827;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#20351;&#29992;&#27492;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce PharmacyGPT, a novel framework to assess the capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in emulating the role of clinical pharmacists. Our methodology encompasses the utilization of LLMs to generate comprehensible patient clusters, formulate medication plans, and forecast patient outcomes. We conduct our investigation using real data acquired from the intensive care unit (ICU) at the University of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable insights into the potential applications and limitations of LLMs in the field of clinical pharmacy, with implications for both patient care and the development of future AI-driven healthcare solutions. By evaluating the performance of PharmacyGPT, we aim to contribute to the ongoing discourse surrounding the integration of artificial intelligence in healthcare settings, ultimately promoting the responsible and efficacious use of such technologies.
&lt;/p&gt;</description></item></channel></rss>