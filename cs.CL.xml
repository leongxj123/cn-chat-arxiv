<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#35745;&#31639;&#26041;&#27861;&#23545;&#27169;&#25311;&#27468;&#35789;&#30456;&#20284;&#24230;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#20851;&#32852;&#65292;&#21457;&#29616;&#22522;&#20110;BERT&#27169;&#22411;&#23884;&#20837;&#12289;&#27468;&#35789;&#38899;&#39057;&#21644;&#38899;&#32032;&#32452;&#20214;&#30456;&#20284;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#23545;&#24863;&#30693;&#19978;&#30340;&#27468;&#35789;&#30456;&#20284;&#24230;&#20855;&#26377;&#25351;&#31034;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.02342</link><description>&lt;p&gt;
&#27468;&#35789;&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Computational Analysis of Lyric Similarity Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#35745;&#31639;&#26041;&#27861;&#23545;&#27169;&#25311;&#27468;&#35789;&#30456;&#20284;&#24230;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#20851;&#32852;&#65292;&#21457;&#29616;&#22522;&#20110;BERT&#27169;&#22411;&#23884;&#20837;&#12289;&#27468;&#35789;&#38899;&#39057;&#21644;&#38899;&#32032;&#32452;&#20214;&#30456;&#20284;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#23545;&#24863;&#30693;&#19978;&#30340;&#27468;&#35789;&#30456;&#20284;&#24230;&#20855;&#26377;&#25351;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21253;&#21547;&#20154;&#22768;&#30340;&#38899;&#20048;&#20316;&#21697;&#20013;&#65292;&#27468;&#35789;&#23545;&#33402;&#26415;&#34920;&#36798;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#35813;&#31995;&#32479;&#24314;&#35758;&#31867;&#20284;&#20110;&#29992;&#25143;&#21916;&#29233;&#25110;&#20010;&#24615;&#21270;&#20559;&#22909;&#30340;&#27468;&#35789;&#65292;&#26377;&#21161;&#20110;&#22312;&#25968;&#30334;&#19975;&#38899;&#36712;&#20013;&#21457;&#29616;&#27468;&#35789;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#31995;&#32479;&#24182;&#26410;&#20805;&#20998;&#32771;&#34385;&#20154;&#31867;&#23545;&#27468;&#35789;&#30456;&#20284;&#24230;&#30340;&#24863;&#30693;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#35745;&#31639;&#26041;&#27861;&#24314;&#27169;&#27468;&#35789;&#30456;&#20284;&#24230;&#19982;&#20154;&#31867;&#24863;&#30693;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12289;&#27468;&#35789;&#26469;&#28304;&#30340;&#38899;&#39057;&#20197;&#21450;&#38899;&#32032;&#32452;&#20214;&#30340;&#35745;&#31639;&#27169;&#22411;&#25351;&#31034;&#20102;&#24863;&#30693;&#19978;&#30340;&#27468;&#35789;&#30456;&#20284;&#24230;&#12290;&#35813;&#21457;&#29616;&#24378;&#35843;&#20102;&#35821;&#20041;&#12289;&#39118;&#26684;&#21644;&#38899;&#38901;&#30456;&#20284;&#24615;&#22312;&#20154;&#31867;&#24863;&#30693;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02342v1 Announce Type: new  Abstract: In musical compositions that include vocals, lyrics significantly contribute to artistic expression. Consequently, previous studies have introduced the concept of a recommendation system that suggests lyrics similar to a user's favorites or personalized preferences, aiding in the discovery of lyrics among millions of tracks. However, many of these systems do not fully consider human perceptions of lyric similarity, primarily due to limited research in this area. To bridge this gap, we conducted a comparative analysis of computational methods for modeling lyric similarity with human perception. Results indicated that computational models based on similarities between embeddings from pre-trained BERT-based models, the audio from which the lyrics are derived, and phonetic components are indicative of perceptual lyric similarity. This finding underscores the importance of semantic, stylistic, and phonetic similarities in human perception abo
&lt;/p&gt;</description></item><item><title>CMAT&#26694;&#26550;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;</title><link>https://arxiv.org/abs/2404.01663</link><description>&lt;p&gt;
CMAT: &#29992;&#20110;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01663
&lt;/p&gt;
&lt;p&gt;
CMAT&#26694;&#26550;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;LLMs&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#26377;&#25928;&#25805;&#20316;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#31867;&#36755;&#20837;&#26469;&#20934;&#30830;&#24341;&#23548;&#23545;&#35805;&#27969;&#31243;&#65292;&#26234;&#33021;&#20307;&#35843;&#25972;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#28041;&#21450;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#35843;&#25972;&#65292;&#20197;&#26356;&#22909;&#22320;&#21709;&#24212;&#36825;&#31181;&#24341;&#23548;&#12290;&#38024;&#23545;&#36825;&#19968;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Collaborative Multi-Agent Tuning&#65288;CMAT&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#26681;&#25454;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#26469;&#22686;&#24378;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#23398;&#20064;&#21644;&#23454;&#26102;&#36866;&#24212;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01663v1 Announce Type: new  Abstract: Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this resear
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#20102;&#19968;&#31181;&#21327;&#20316;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26631;&#35760;&#32423;&#21035;&#20132;&#38169;&#29983;&#25104;&#26469;&#25945;&#25480;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#65292;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#65292;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#34701;&#21512;&#27599;&#20010;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03870</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Decode Collaboratively with Multiple Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03870
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20102;&#19968;&#31181;&#21327;&#20316;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26631;&#35760;&#32423;&#21035;&#20132;&#38169;&#29983;&#25104;&#26469;&#25945;&#25480;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#65292;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#65292;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#34701;&#21512;&#27599;&#20010;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26631;&#35760;&#32423;&#21035;&#20132;&#38169;&#23427;&#20204;&#30340;&#29983;&#25104;&#26469;&#25945;&#25480;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21327;&#20316;&#12290;&#25105;&#20204;&#23558;&#19979;&#19968;&#20010;&#26631;&#35760;&#30001;&#21738;&#20010;LLM&#29983;&#25104;&#30340;&#20915;&#31574;&#24314;&#27169;&#20026;&#28508;&#21464;&#37327;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#19979;&#20248;&#21270;&#35757;&#32451;&#38598;&#30340;&#36793;&#38469;&#20284;&#28982;&#65292;&#22522;&#30784;LLM&#33258;&#21160;&#23398;&#20064;&#20309;&#26102;&#29983;&#25104;&#33258;&#36523;&#20197;&#21450;&#20309;&#26102;&#35843;&#29992;&#20854;&#20013;&#19968;&#20010;&#8220;&#21161;&#25163;&#8221;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36827;&#34892;&#26631;&#35760;&#32423;&#21327;&#20316;&#20801;&#35768;&#34701;&#21512;&#27599;&#20010;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#31526;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#21327;&#20316;&#35299;&#30721;&#22312;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#29305;&#21035;&#26377;&#29992;&#65292;&#20854;&#20013;&#36890;&#29992;&#22522;&#30784;LLM&#23398;&#20250;&#35843;&#29992;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#25191;&#34892;&#25351;&#20196;&#12289;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#21644;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#20248;&#20110;&#21333;&#29420;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#23398;&#20064;&#21040;&#30340;&#28508;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03870v1 Announce Type: new  Abstract: We propose a method to teach multiple large language models (LLM) to collaborate by interleaving their generations at the token level. We model the decision of which LLM generates the next token as a latent variable. By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the ``assistant'' language models to generate, all without direct supervision. Token-level collaboration during decoding allows for a fusion of each model's expertise in a manner tailored to the specific task at hand. Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models. On instruction-following, domain-specific QA, and reasoning tasks, we show that the performance of the joint system exceeds that of the individual models. Through qualitative analysis of the learned lat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DIVERSE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;173,000&#26465;YouTube&#35270;&#39057;&#35780;&#35770;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#21477;&#23376;&#20013;&#30340;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.03334</link><description>&lt;p&gt;
DIVERSE&#65306;&#36890;&#36807;&#35270;&#39057;&#35780;&#35770;&#24577;&#24230;&#20998;&#26512;&#35299;&#35835;&#20114;&#32852;&#32593;&#23545;&#32654;&#22269;&#20891;&#20107;&#30340;&#30475;&#27861;&#65292;&#19968;&#20010;&#29992;&#20110;&#31435;&#22330;&#20998;&#31867;&#30340;&#26032;&#39062;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DIVERSE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;173,000&#26465;YouTube&#35270;&#39057;&#35780;&#35770;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#21477;&#23376;&#20013;&#30340;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#31435;&#22330;&#26816;&#27979;&#26159;&#28041;&#21450;&#35782;&#21035;&#22312;&#26377;&#20105;&#35758;&#20027;&#39064;&#19978;&#25317;&#26377;&#30456;&#21453;&#35266;&#28857;&#30340;&#29992;&#25143;&#32676;&#32452;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#30123;&#33495;&#25509;&#31181;&#21644;&#20105;&#35770;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#31435;&#22330;&#25552;&#20379;&#20102;&#23545;&#23454;&#20307;&#31435;&#22330;&#30340;&#25351;&#31034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DIVERSE&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#23545;&#36229;&#36807;173,000&#20010;YouTube&#35270;&#39057;&#35780;&#35770;&#36827;&#34892;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#20110;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#12290;&#36825;&#20123;&#31435;&#22330;&#36890;&#36807;&#19968;&#31181;&#30001;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#36827;&#34892;&#26631;&#27880;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#21477;&#23376;&#20013;&#34164;&#21547;&#30340;&#35821;&#27668;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#65292;&#32780;&#38750;&#20351;&#29992;&#20154;&#31867;&#25163;&#21160;&#27880;&#37322;&#12290;&#36825;&#20123;&#24369;&#20449;&#21495;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#21644;&#35773;&#21050;&#30340;&#23384;&#22312;&#65292;&#29305;&#23450;&#20851;&#38190;&#35789;&#30340;&#23384;&#22312;&#65292;&#25991;&#26412;&#30340;&#24773;&#24863;&#20197;&#21450;&#20174;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#30340;&#31435;&#22330;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#35780;&#35770;&#34987;&#27880;&#37322;&#20043;&#21069;&#65292;&#36825;&#20123;&#24369;&#20449;&#21495;&#20351;&#29992;&#25968;&#25454;&#32534;&#31243;&#27169;&#22411;&#36827;&#34892; consol
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03334v1 Announce Type: cross  Abstract: Stance detection of social media text is a key component of downstream tasks involving the identification of groups of users with opposing opinions on contested topics such as vaccination and within arguments. In particular, stance provides an indication of an opinion towards an entity. This paper introduces DIVERSE, a dataset of over 173,000 YouTube video comments annotated for their stance towards videos of the U.S. military. The stance is annotated through a human-guided, machine-assisted labeling methodology that makes use of weak signals of tone within the sentence as supporting indicators, as opposed to using manual annotations by humans. These weak signals consist of the presence of hate speech and sarcasm, the presence of specific keywords, the sentiment of the text, and the stance inference from two Large Language Models. The weak signals are then consolidated using a data programming model before each comment is annotated wit
&lt;/p&gt;</description></item><item><title>RAVEL&#25968;&#25454;&#38598;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MDAS&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#24320;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#24378;&#35843;&#20102;&#36328;&#28608;&#27963;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17700</link><description>&lt;p&gt;
RAVEL: &#22312;&#35299;&#24320;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#26041;&#38754;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17700
&lt;/p&gt;
&lt;p&gt;
RAVEL&#25968;&#25454;&#38598;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MDAS&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#24320;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#24378;&#35843;&#20102;&#36328;&#28608;&#27963;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#21035;&#31070;&#32463;&#20803;&#21442;&#19982;&#22810;&#20010;&#39640;&#32423;&#27010;&#24565;&#30340;&#34920;&#31034;&#12290;&#19981;&#21516;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#33021;&#25104;&#21151;&#35299;&#24320;&#36825;&#20123;&#35282;&#33394;&#65311;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RAVEL&#65288;Resolving Attribute-Value Entanglements in Language Models&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#22810;&#31181;&#29616;&#26377;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#32039;&#23494;&#25511;&#21046;&#30340;&#23450;&#37327;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;&#30001;&#27492;&#20135;&#29983;&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#23450;&#20041;&#26032;&#30340;Multi-task Distributed Alignment Search&#65288;MDAS&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25214;&#21040;&#28385;&#36275;&#22810;&#20010;&#22240;&#26524;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#20197;Llama2-7B&#20316;&#20026;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#65292;MDAS&#22312;RAVEL&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#23637;&#31034;&#20102;&#36229;&#36234;&#31070;&#32463;&#20803;&#32423;&#21035;&#20998;&#26512;&#20197;&#35782;&#21035;&#36328;&#28608;&#27963;&#30340;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;https://github.com/explanare/ravel&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17700v1 Announce Type: new  Abstract: Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;OWSM-CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Connectionist Temporal Classification&#30340;&#26032;&#22411;&#20165;&#32534;&#30721;&#22120;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65292;&#35757;&#32451;&#26377;180k&#23567;&#26102;&#30340;&#20844;&#20849;&#38899;&#39057;&#25968;&#25454;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.12654</link><description>&lt;p&gt;
OWSM-CTC:&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#12289;&#32763;&#35793;&#21644;&#35821;&#35328;&#35782;&#21035;&#30340;&#24320;&#25918;&#32534;&#30721;&#22120;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12654
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;OWSM-CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Connectionist Temporal Classification&#30340;&#26032;&#22411;&#20165;&#32534;&#30721;&#22120;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65292;&#35757;&#32451;&#26377;180k&#23567;&#26102;&#30340;&#20844;&#20849;&#38899;&#39057;&#25968;&#25454;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#23545;&#33021;&#22815;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#25191;&#34892;&#22810;&#20010;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#38899;&#27169;&#22411;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25110;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#38750;&#24120;&#27969;&#34892;&#19988;&#24615;&#33021;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#19982;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#21487;&#33021;&#20250;&#27604;&#36739;&#24930;&#65292;&#24182;&#19988;&#36824;&#23384;&#22312;&#24187;&#35273;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#23567;&#35268;&#27169;&#20219;&#21153;&#20013;&#20135;&#29983;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#25193;&#23637;&#21040;&#19981;&#21516;&#35821;&#35328;&#21644;&#20219;&#21153;&#30340;&#35821;&#38899;&#36716;&#25991;&#26412;&#29983;&#25104;&#20013;&#12290;&#21463;Open Whisper-style Speech Model (OWSM)&#39033;&#30446;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OWSM-CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Connectionist Temporal Classification (CTC)&#30340;&#26032;&#22411;&#20165;&#32534;&#30721;&#22120;&#30340;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#20351;&#29992;18&#19975;&#23567;&#26102;&#30340;&#20844;&#20849;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12654v1 Announce Type: new  Abstract: There has been an increasing interest in large speech models that can perform multiple speech processing tasks in a single model. Such models usually adopt the encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and languag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934; StrongREJECT&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#31354;&#30772;&#35299;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10260</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31354;&#30772;&#35299;&#30340;&#24378;REJECT&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A StrongREJECT for Empty Jailbreaks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934; StrongREJECT&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#31354;&#30772;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#24341;&#36215;&#20102;&#23545;&#8220;&#30772;&#35299;&#8221;&#30340;&#20851;&#27880;&#65292;&#36825;&#31181;&#30772;&#35299;&#20801;&#35768;&#27169;&#22411;&#34987;&#24694;&#24847;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#22522;&#20934;&#26469;&#34913;&#37327;&#30772;&#35299;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#23548;&#33268;&#30772;&#35299;&#35770;&#25991;&#30340;&#20316;&#32773;&#19981;&#24471;&#19981;&#33258;&#34892;&#21019;&#24314;&#26631;&#20934;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#22522;&#20934;&#32463;&#24120;&#21253;&#21547;&#27169;&#26865;&#20004;&#21487;&#25110;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20542;&#21521;&#20110;&#39640;&#20272;&#20302;&#36136;&#37327;&#27169;&#22411;&#21709;&#24212;&#30340;&#28389;&#29992;&#28508;&#21147;&#30340;&#35780;&#20998;&#26631;&#20934;&#12290;&#19968;&#20123;&#30772;&#35299;&#25216;&#26415;&#20351;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#23427;&#20204;&#21363;&#20351;&#23545;&#20110;&#33391;&#24615;&#38382;&#39064;&#20063;&#20250;&#38477;&#20302;&#27169;&#22411;&#21709;&#24212;&#30340;&#36136;&#37327;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;&#30772;&#35299;&#25216;&#26415;&#26174;&#30528;&#38477;&#20302;&#20102;GPT-4&#22312;MMLU&#19978;&#30340;&#38646;&#23556;&#20987;&#34920;&#29616;&#12290;&#30772;&#35299;&#36824;&#20250;&#20351;&#20174;&#8220;&#26410;&#32463;&#23457;&#26597;&#8221;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#33719;&#21462;&#26377;&#23475;&#21709;&#24212;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;StrongREJECT&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#30772;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10260v1 Announce Type: cross  Abstract: The rise of large language models (LLMs) has drawn attention to the existence of "jailbreaks" that allow the models to be used maliciously. However, there is no standard benchmark for measuring the severity of a jailbreak, leaving authors of jailbreak papers to create their own. We show that these benchmarks often include vague or unanswerable questions and use grading criteria that are biased towards overestimating the misuse potential of low-quality model responses. Some jailbreak techniques make the problem worse by decreasing the quality of model responses even on benign questions: we show that several jailbreaking techniques substantially reduce the zero-shot performance of GPT-4 on MMLU. Jailbreaks can also make it harder to elicit harmful responses from an "uncensored" open-source model. We present a new benchmark, StrongREJECT, which better discriminates between effective and ineffective jailbreaks by using a higher-quality que
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#12290;&#36890;&#36807;&#32508;&#36848;&#25991;&#29486;&#21644;&#19987;&#23478;&#21672;&#35810;&#65292;&#26500;&#24314;&#20102;&#25968;&#25454;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#29983;&#25104;&#20102;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;MLLM&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#20027;&#27969;&#25945;&#32946;&#20013;&#30340;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#20559;&#37325;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.06264</link><description>&lt;p&gt;
LLaVA-Docent&#65306;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#30340;&#25945;&#23398;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#12290;&#36890;&#36807;&#32508;&#36848;&#25991;&#29486;&#21644;&#19987;&#23478;&#21672;&#35810;&#65292;&#26500;&#24314;&#20102;&#25968;&#25454;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#29983;&#25104;&#20102;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;MLLM&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#20027;&#27969;&#25945;&#32946;&#20013;&#30340;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#20559;&#37325;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#37492;&#36175;&#23545;&#20110;&#22521;&#20859;&#23398;&#20064;&#32773;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#24773;&#24863;&#26234;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#24120;&#38754;&#20020;&#33402;&#26415;&#36164;&#28304;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24369;&#21183;&#23398;&#29983;&#65292;&#24182;&#19988;&#22312;&#20027;&#27969;&#25945;&#32946;&#20013;&#36807;&#24230;&#24378;&#35843;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#31185;&#30446;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#25216;&#26415;&#36827;&#27493;&#20026;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#26469;&#21033;&#29992;&#36825;&#20123;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#21672;&#35810;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#25968;&#25454;&#26694;&#26550;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;GPT-4&#21033;&#29992;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;MLLM&#65288;&#21363;LLaVA-Docent&#65289;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#20845;&#21517;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Art appreciation is vital in nurturing critical thinking and emotional intelligence among learners. However, traditional art appreciation education has often been hindered by limited access to art resources, especially for disadvantaged students, and an imbalanced emphasis on STEM subjects in mainstream education. In response to these challenges, recent technological advancements have paved the way for innovative solutions. This study explores the application of multi-modal large language models (MLLMs) in art appreciation education, focusing on developing LLaVA-Docent, a model that leverages these advancements. Our approach involved a comprehensive literature review and consultations with experts in the field, leading to developing a robust data framework. Utilizing this framework, we generated a virtual dialogue dataset that was leveraged by GPT-4. This dataset was instrumental in training the MLLM, named LLaVA-Docent. Six researchers conducted quantitative and qualitative evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#30340;&#24191;&#27867;&#35821;&#38899;&#25991;&#26412;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#21644;&#24191;&#20041;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#24182;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#12290;</title><link>https://arxiv.org/abs/2402.05706</link><description>&lt;p&gt;
&#38754;&#21521;&#21475;&#35821;&#23545;&#35805;&#24314;&#27169;&#30340;&#32479;&#19968;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unified Speech-Text Pretraining for Spoken Dialog Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#30340;&#24191;&#27867;&#35821;&#38899;&#25991;&#26412;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#21644;&#24191;&#20041;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#24182;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#30452;&#25509;&#29702;&#35299;&#21644;&#21512;&#25104;&#35821;&#38899;&#20855;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#29992;&#20110;&#21475;&#35821;&#23545;&#35805;&#24314;&#27169;&#30340;&#22522;&#20110;LLM&#30340;&#31574;&#30053;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35821;&#38899;&#25991;&#26412;LLM&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#65292;&#20197;&#22312;&#19981;&#20381;&#36182;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#25110;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#19982;&#32473;&#23450;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#21644;&#26377;&#26426;&#30340;&#38901;&#24459;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#65292;&#21033;&#29992;&#20102;&#24213;&#23618;LLM&#25152;&#23637;&#31034;&#30340;&#25512;&#29702;&#38142;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#12290;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#65292;&#24182;&#19988;&#20248;&#20110;&#20043;&#21069;&#30340;&#21644;&#32423;&#32852;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#35814;&#32454;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
While recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech, an LLM-based strategy for modeling spoken dialogs remains elusive and calls for further investigation. This work proposes an extensive speech-text LLM framework, named the Unified Spoken Dialog Model (USDM), to generate coherent spoken responses with organic prosodic features relevant to the given input speech without relying on automatic speech recognition (ASR) or text-to-speech (TTS) solutions. Our approach employs a multi-step speech-text inference scheme that leverages chain-of-reasoning capabilities exhibited by the underlying LLM. We also propose a generalized speech-text pretraining scheme that helps with capturing cross-modal semantics. Automatic and human evaluations show that the proposed approach is effective in generating natural-sounding spoken responses, outperforming both prior and cascaded baselines. Detailed comparative s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;OWSM v3.1&#22522;&#20110;E-Branchformer&#30340;&#26356;&#22909;&#21644;&#26356;&#24555;&#30340;&#24320;&#25918;&#24335;Whisper&#39118;&#26684;&#35821;&#38899;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#29256;&#26412;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#35813;&#35770;&#25991;&#36824;&#20844;&#24320;&#21457;&#24067;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.16658</link><description>&lt;p&gt;
OWSM v3.1: &#22522;&#20110;E-Branchformer&#30340;&#26356;&#22909;&#21644;&#26356;&#24555;&#30340;&#24320;&#25918;&#24335;Whisper&#39118;&#26684;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer. (arXiv:2401.16658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16658
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;OWSM v3.1&#22522;&#20110;E-Branchformer&#30340;&#26356;&#22909;&#21644;&#26356;&#24555;&#30340;&#24320;&#25918;&#24335;Whisper&#39118;&#26684;&#35821;&#38899;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#29256;&#26412;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#35813;&#35770;&#25991;&#36824;&#20844;&#24320;&#21457;&#24067;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#20513;&#23548;&#37319;&#29992;&#23436;&#20840;&#24320;&#25918;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25512;&#21160;&#36879;&#26126;&#24230;&#21644;&#24320;&#25918;&#31185;&#23398;&#12290;&#20316;&#20026;&#19968;&#20010;&#21021;&#27493;&#30340;&#27493;&#39588;&#65292;&#24320;&#25918;&#24335;Whisper&#39118;&#26684;&#35821;&#38899;&#27169;&#22411;(OWSM)&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#21644;&#24320;&#28304;&#24037;&#20855;&#37325;&#26032;&#22797;&#21046;&#20102;OpenAI&#30340;Whisper&#12290;&#20026;&#20102;&#22797;&#21046;Whisper&#65292;&#20043;&#21069;&#30340;OWSM v1&#21040;v3&#27169;&#22411;&#20173;&#28982;&#22522;&#20110;Transformer&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19981;&#22914;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#39640;OWSM&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;E-Branchformer&#30340;OWSM v3.1&#27169;&#22411;&#65292;&#26377;&#20004;&#20010;&#35268;&#27169;&#65292;&#21363;100M&#21644;1B&#12290;1B&#27169;&#22411;&#26159;&#30446;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#26368;&#22823;&#30340;&#22522;&#20110;E-Branchformer&#30340;&#35821;&#38899;&#27169;&#22411;&#12290;&#23427;&#22312;&#22823;&#37096;&#20998;&#35780;&#20272;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#27604;&#20043;&#21069;&#30340;OWSM v3&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#28436;&#31034;&#20102;&#39640;&#36798;25%&#30340;&#26356;&#24555;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#25968;&#25454;&#20934;&#22791;&#33050;&#26412;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35757;&#32451;&#26085;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have advocated for fully open foundation models to promote transparency and open science. As an initial step, the Open Whisper-style Speech Model (OWSM) reproduced OpenAI's Whisper using publicly available data and open-source toolkits. With the aim of reproducing Whisper, the previous OWSM v1 through v3 models were still based on Transformer, which might lead to inferior performance compared to other state-of-the-art speech encoders. In this work, we aim to improve the performance and efficiency of OWSM without extra training data. We present E-Branchformer based OWSM v3.1 models at two scales, i.e., 100M and 1B. The 1B model is the largest E-Branchformer based speech model that has been made publicly available. It outperforms the previous OWSM v3 in a vast majority of evaluation benchmarks, while demonstrating up to 25% faster inference speed. We publicly release the data preparation scripts, pre-trained models and training logs.
&lt;/p&gt;</description></item><item><title>LQ-LoRA&#26159;&#19968;&#31181;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#20197;&#21450;&#23545;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#21152;&#26435;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;QLoRA&#21644;GPTQ-LoRA&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.12023</link><description>&lt;p&gt;
LQ-LoRA: &#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#29992;&#20110;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. (arXiv:2311.12023v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12023
&lt;/p&gt;
&lt;p&gt;
LQ-LoRA&#26159;&#19968;&#31181;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#20197;&#21450;&#23545;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#21152;&#26435;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;QLoRA&#21644;GPTQ-LoRA&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#31639;&#27861;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#37327;&#21270;&#37096;&#20998;&#20445;&#25345;&#22266;&#23450;&#65292;&#21482;&#26377;&#20302;&#31209;&#37096;&#20998;&#34987;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#21270;&#37096;&#20998;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#34920;&#36798;&#65292;&#21487;&#20197;&#26681;&#25454;&#24635;&#20307;&#20869;&#23384;&#39044;&#31639;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#65288;&#20363;&#22914;&#27604;&#29305;&#23485;&#24230;&#12289;&#22359;&#22823;&#23567;&#65289;&#32473;&#23450;&#27599;&#20010;&#30697;&#38453;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#25968;&#25454;&#24863;&#30693;&#29256;&#26412;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#36817;&#20284;&#26469;&#21152;&#26435;&#30697;&#38453;&#20998;&#35299;&#36807;&#31243;&#20013;&#30340;&#37325;&#26500;&#30446;&#26631;&#12290;&#22312;RoBERTa&#21644;LLaMA-2&#65288;7B&#21644;70B&#65289;&#30340;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65288;LQ-LoRA&#65289;&#20248;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;QLoRA&#21644;GPTQ-LoRA&#65292;&#24182;&#23454;&#29616;&#20102;&#28608;&#36827;&#30340;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AffectVisDial&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;50,000&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#20110;&#23545;&#35805;&#30340;&#38382;&#31572;&#12289;&#24773;&#24863;&#39044;&#27979;&#21644;&#24773;&#24863;&#35299;&#37322;&#20219;&#21153;&#65292;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16349</link><description>&lt;p&gt;
&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#65306;&#22522;&#20110;&#35270;&#35273;&#23545;&#35805;&#29702;&#35299;&#24773;&#24863;&#24418;&#25104;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations. (arXiv:2308.16349v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16349
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AffectVisDial&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;50,000&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#20110;&#23545;&#35805;&#30340;&#38382;&#31572;&#12289;&#24773;&#24863;&#39044;&#27979;&#21644;&#24773;&#24863;&#35299;&#37322;&#20219;&#21153;&#65292;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#65292;&#20316;&#20026;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#29702;&#35299;&#22312;&#22522;&#20110;&#35270;&#35273;&#23545;&#35805;&#20013;&#24773;&#24863;&#24418;&#25104;&#30340;&#36807;&#31243;&#12290;&#36825;&#39033;&#20219;&#21153;&#28041;&#21450;&#19977;&#39033;&#25216;&#33021;&#65306;&#65288;1&#65289;&#22522;&#20110;&#23545;&#35805;&#30340;&#38382;&#31572;&#65292;&#65288;2&#65289;&#22522;&#20110;&#23545;&#35805;&#30340;&#24773;&#24863;&#39044;&#27979;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22522;&#20110;&#23545;&#35805;&#29983;&#25104;&#24773;&#24863;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;AffectVisDial&#65292;&#21253;&#21547;50,000&#20010;10&#36718;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#23545;&#35805;&#65292;&#36824;&#21253;&#25324;&#24635;&#32467;&#30340;&#24773;&#24863;&#24402;&#22240;&#21644;&#22522;&#20110;&#23545;&#35805;&#30340;&#24773;&#24863;&#35299;&#37322;&#65292;&#24635;&#20849;&#38656;&#35201;27180&#20010;&#24037;&#20316;&#23567;&#26102;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25910;&#38598;&#35813;&#25968;&#25454;&#38598;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#24182;&#20171;&#32461;&#20102;&#19982;&#23545;&#35805;&#21442;&#19982;&#32773;&#30456;&#20851;&#30340;&#25552;&#38382;&#32773;&#21644;&#22238;&#31572;&#32773;&#20219;&#21153;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#23637;&#31034;&#20102;&#26469;&#33258;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#22362;&#23454;&#30340;&#24773;&#24863;&#35270;&#35273;&#23545;&#35805;&#22522;&#32447;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#31572;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Affective Visual Dialog, an emotion explanation and reasoning task as a testbed for research on understanding the formation of emotions in visually grounded conversations. The task involves three skills: (1) Dialog-based Question Answering (2) Dialog-based Emotion Prediction and (3) Affective emotion explanation generation based on the dialog. Our key contribution is the collection of a large-scale dataset, dubbed AffectVisDial, consisting of 50K 10-turn visually grounded dialogs as well as concluding emotion attributions and dialog-informed textual emotion explanations, resulting in a total of 27,180 working hours. We explain our design decisions in collecting the dataset and introduce the questioner and answerer tasks that are associated with the participants in the conversation. We train and demonstrate solid Affective Visual Dialog baselines adapted from state-of-the-art models. Remarkably, the responses generated by our models show promising emotional reasoning abilit
&lt;/p&gt;</description></item><item><title>Seq2Seq&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#28508;&#21147;&#22312;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#33021;&#26377;&#25928;&#25552;&#21319;Seq2Seq&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.14856</link><description>&lt;p&gt;
&#21457;&#25381;Seq2Seq&#27169;&#22411;&#20316;&#20026;&#31283;&#20581;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners. (arXiv:2307.14856v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14856
&lt;/p&gt;
&lt;p&gt;
Seq2Seq&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#28508;&#21147;&#22312;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#33021;&#26377;&#25928;&#25552;&#21319;Seq2Seq&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#21482;&#26377;&#35299;&#30721;&#22120;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#32780;&#32534;&#30721;-&#35299;&#30721;&#65288;&#21363;Seq2Seq&#65289;&#27169;&#22411;&#22312;&#20381;&#36182;&#20110;&#26435;&#37325;&#26356;&#26032;&#30340;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;Seq2Seq&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#20294;&#36825;&#20165;&#38480;&#20110;&#19982;Seq2Seq&#20307;&#31995;&#32467;&#26500;&#30456;&#21305;&#37197;&#30340;&#20219;&#21153;&#65292;&#22914;&#25688;&#35201;&#21644;&#32763;&#35793;&#12290;&#21463;&#21040;&#36825;&#20123;&#21021;&#22987;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#27425;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#33021;&#26356;&#26377;&#25928;&#22320;&#24341;&#21457;Seq2Seq&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65306;&#30446;&#26631;&#23545;&#40784;&#25552;&#31034;&#21644;&#22522;&#20110;&#34701;&#21512;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#19968;&#20010;&#20307;&#31215;&#26159;&#20854;&#20845;&#20493;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#24120;&#35268;Seq2Seq&#27169;&#22411;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates. Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation. Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks. Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a var
&lt;/p&gt;</description></item></channel></rss>