<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01109</link><description>&lt;p&gt;
&#30123;&#33495;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Vaccine: Perturbation-aware Alignment for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01109
&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#21363;&#26381;&#21153;&#33539; paradigm&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#19968;&#23567;&#37096;&#20998;&#26377;&#23475;&#25968;&#25454;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23481;&#26131;&#27450;&#39575;&#24494;&#35843;&#36807;&#31243;&#20174;&#32780;&#20135;&#29983;&#23545;&#40784;&#22833;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#33021;&#23548;&#33268;&#23545;&#40784;&#22833;&#25928;&#30340;&#26377;&#23475;&#23884;&#20837;&#28418;&#31227;&#29616;&#35937;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30123;&#33495; (Vaccine) &#65292;&#19968;&#31181;&#38024;&#23545;&#24178;&#25200;&#24863;&#30693;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#29992;&#25143;&#24494;&#35843;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30123;&#33495;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#23545;&#40784;&#38454;&#27573;&#36880;&#28176;&#28155;&#21152;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#65292;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#20174;&#32780;&#20351;&#23884;&#20837;&#33021;&#22815;&#25269;&#24481;&#26469;&#33258;&#26410;&#32463;&#28040;&#27602;&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;&#20027;&#27969;LLM&#65288;&#22914;Llama2&#65292;Opt&#65292;Vicuna&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30123;&#33495;&#33021;&#22815;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
&lt;/p&gt;</description></item><item><title>&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32479;&#19968;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#65292;&#25552;&#20986;&#26032;&#39062;&#30340;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#33410;&#28857;&#21644;&#36793;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21160;&#21327;&#20316;&#21644;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.16823</link><description>&lt;p&gt;
&#20316;&#20026;&#21487;&#20248;&#21270;&#22270;&#30340;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Language Agents as Optimizable Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16823
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32479;&#19968;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#65292;&#25552;&#20986;&#26032;&#39062;&#30340;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#33410;&#28857;&#21644;&#36793;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21160;&#21327;&#20316;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#20154;&#31867;&#35774;&#35745;&#30340;&#25552;&#21319;&#25216;&#26415;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#39064;&#27714;&#35299;&#22120;&#65292;&#20135;&#29983;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#20195;&#30721;&#24211;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;LLM&#20195;&#29702;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#26469;&#32479;&#19968;&#36825;&#20123;&#26041;&#27861;&#12290;&#33410;&#28857;&#23454;&#29616;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#25110;&#26597;&#35810;LLMs&#30340;&#21151;&#33021;&#65292;&#24182;&#19988;&#36793;&#25551;&#36848;&#25805;&#20316;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#22270;&#24418;&#21487;&#20197;&#36882;&#24402;&#22320;&#32452;&#21512;&#25104;&#20195;&#34920;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#21327;&#20316;&#23618;&#27425;&#30340;&#26356;&#22823;&#32452;&#21512;&#22270;&#65288;&#20854;&#20013;&#36793;&#36830;&#25509;&#19981;&#21516;&#20195;&#29702;&#30340;&#25805;&#20316;&#65289;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#65288;1&#65289;&#20248;&#21270;&#33410;&#28857;&#32423;LLM&#25552;&#31034;&#65288;&#33410;&#28857;&#20248;&#21270;&#65289;&#24182;&#65288;2&#65289;&#36890;&#36807;&#25913;&#21464;&#22270;&#36830;&#25509;&#24615;&#26469;&#25913;&#21892;&#20195;&#29702;&#21327;&#35843;&#65288;&#36793;&#32536;&#20248;&#21270;&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#29992;&#20110;&#39640;&#25928;&#24320;&#21457;&#12289;&#38598;&#25104;&#21644;&#33258;&#21160;&#25913;&#36827;&#21508;&#31181;LLM&#20195;&#29702;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/metauto-ai/gptswarm&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16823v1 Announce Type: cross  Abstract: Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22810;&#35821;&#35328;BERT&#23545;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#36827;&#34892;&#20381;&#23384;&#26631;&#27880;&#65292;&#21152;&#36895;&#24182;&#31616;&#21270;&#20381;&#23384;&#26631;&#27880;&#36807;&#31243;&#65292;&#23558;&#20135;&#29983;&#30340;&#26641;&#24211;&#26377;&#21161;&#20110;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#25991;&#26723;&#30340;&#33258;&#21160;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.14743</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#35821;&#35328;BERT&#23545;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#36827;&#34892;&#20381;&#23384;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Dependency Annotation of Ottoman Turkish with Multilingual BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14743
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22810;&#35821;&#35328;BERT&#23545;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#36827;&#34892;&#20381;&#23384;&#26631;&#27880;&#65292;&#21152;&#36895;&#24182;&#31616;&#21270;&#20381;&#23384;&#26631;&#27880;&#36807;&#31243;&#65292;&#23558;&#20135;&#29983;&#30340;&#26641;&#24211;&#26377;&#21161;&#20110;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#25991;&#26723;&#30340;&#33258;&#21160;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#27880;&#26041;&#27861;&#65292;&#29992;&#20110;&#31532;&#19968;&#20010;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#20381;&#23384;&#26641;&#24211;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;i&#65289;&#20351;&#29992;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#30340;&#35299;&#26512;&#27169;&#22411;&#36827;&#34892;&#20266;&#26631;&#27880;&#25968;&#25454;&#65292;ii&#65289;&#25163;&#21160;&#20462;&#27491;&#20266;&#26631;&#27880;&#65292;&#20197;&#21450;iii&#65289;&#29992;&#20462;&#27491;&#30340;&#26631;&#27880;&#24494;&#35843;&#35299;&#26512;&#27169;&#22411;&#65292;&#25105;&#20204;&#21152;&#24555;&#24182;&#31616;&#21270;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20381;&#23384;&#26631;&#27880;&#36807;&#31243;&#12290;&#26368;&#32456;&#20135;&#29983;&#30340;&#26641;&#24211;&#65292;&#23558;&#25104;&#20026;&#36890;&#29992;&#20381;&#23384;&#20851;&#31995;&#65288;UD&#65289;&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#23558;&#20415;&#21033;&#22885;&#26031;&#26364;&#22303;&#32819;&#20854;&#35821;&#25991;&#26723;&#30340;&#33258;&#21160;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#21382;&#21490;&#36951;&#20135;&#20013;&#34164;&#21547;&#30340;&#35821;&#35328;&#20016;&#23500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14743v1 Announce Type: new  Abstract: This study introduces a pretrained large language model-based annotation methodology for the first dependency treebank in Ottoman Turkish. Our experimental results show that, iteratively, i) pseudo-annotating data using a multilingual BERT-based parsing model, ii) manually correcting the pseudo-annotations, and iii) fine-tuning the parsing model with the corrected annotations, we speed up and simplify the challenging dependency annotation process. The resulting treebank, that will be a part of the Universal Dependencies (UD) project, will facilitate automated analysis of Ottoman Turkish documents, unlocking the linguistic richness embedded in this historical heritage.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#21161;&#25163;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07862</link><description>&lt;p&gt;
AI&#22686;&#24378;&#39044;&#27979;&#65306;LLM&#21161;&#25163;&#25552;&#39640;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#21161;&#25163;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#19982;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#34920;&#29616;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#22686;&#24378;&#21028;&#26029;&#21147;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;GPT-4-Turbo&#21161;&#25163;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#26088;&#22312;&#25552;&#20379;&#39640;&#36136;&#37327;&#24314;&#35758;&#65288;&#36229;&#32423;&#39044;&#27979;&#65289;&#65292;&#21478;&#19968;&#20010;&#26088;&#22312;&#36807;&#20110;&#33258;&#20449;&#21644;&#22522;&#26412;&#27010;&#29575;&#24573;&#35270;&#12290;&#21442;&#19982;&#32773;&#65288;N = 991&#65289;&#21487;&#20197;&#22312;&#25972;&#20010;&#30740;&#31350;&#36807;&#31243;&#20013;&#21672;&#35810;&#20182;&#20204;&#34987;&#20998;&#37197;&#30340;LLM&#21161;&#25163;&#65292;&#32780;&#23545;&#29031;&#32452;&#21017;&#20351;&#29992;&#19968;&#20010;&#36739;&#20302;&#32423;&#21035;&#30340;&#27169;&#22411;&#65288;DaVinci-003&#65289;&#65292;&#19981;&#25552;&#20379;&#30452;&#25509;&#30340;&#39044;&#27979;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#27880;&#20876;&#20998;&#26512;&#26174;&#31034;&#65292;LLM&#22686;&#24378;&#26174;&#33879;&#25552;&#39640;&#20102;23%&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#20219;&#20309;&#19968;&#31181;&#21161;&#25163;&#31867;&#22411;&#65292;&#30456;&#27604;&#20110;&#23545;&#29031;&#32452;&#12290;&#36825;&#31181;&#25913;&#36827;&#21457;&#29983;&#22312;&#36229;&#32423;&#39044;&#27979;&#21161;&#25163;&#22312;&#39044;&#27979;&#20013;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#26126;&#22686;&#24378;&#30340;&#25928;&#30410;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show impressive capabilities, matching and sometimes exceeding human performance in many domains. This study explores the potential of LLMs to augment judgement in forecasting tasks. We evaluated the impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to provide high-quality advice ('superforecasting'), and the other designed to be overconfident and base-rate-neglecting. Participants (N = 991) had the option to consult their assigned LLM assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support. Our preregistered analyses reveal that LLM augmentation significantly enhances forecasting accuracy by 23% across both types of assistants, compared to the control group. This improvement occurs despite the superforecasting assistant's higher accuracy in predictions, indicating the augmentation's benefit is not solely due to model prediction accuracy. Explora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.06255</link><description>&lt;p&gt;
&#36827;&#21462;&#30340;&#40077;&#21187;&#36890;&#36807;&#25552;&#31034;&#23545;&#25239;&#35843;&#25972;&#25269;&#21046;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#32469;&#36807;&#20869;&#32622;&#30340;&#23433;&#20840;&#25514;&#26045;&#24182;&#25552;&#20379;&#21361;&#38505;&#25110;&#38750;&#27861;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36234;&#29425;&#34892;&#20026;&#12290;&#20026;&#20102;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38598;&#20013;&#22312;&#20869;&#23481;&#36807;&#28388;&#25110;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning&#65288;PAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#25105;&#20204;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20132;&#26367;&#26356;&#26032;&#25915;&#20987;&#21644;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25552;&#31034;&#35843;&#25972;&#30340;&#35282;&#24230;&#23454;&#26045;&#38450;&#24481;&#30340;&#20154;&#12290;&#19968;&#26086;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;LLMs&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25269;&#24481;&#36234;&#29425;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;Clarify&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#19968;&#33268;&#22833;&#36133;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03715</link><description>&lt;p&gt;
&#28548;&#28165;&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Clarify: Improving Model Robustness With Natural Language Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03715
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;Clarify&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#19968;&#33268;&#22833;&#36133;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30456;&#20851;&#24615;&#12290;&#36825;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#20381;&#36182;&#20110;&#39640;&#32423;&#38169;&#35823;&#27010;&#24565;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#38169;&#35823;&#27010;&#24565;&#65292;&#25105;&#20204;&#24517;&#39035;&#25552;&#20379;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20123;&#39069;&#22806;&#30340;&#23454;&#20363;&#32423;&#30417;&#30563;&#24418;&#24335;&#65292;&#20363;&#22914;&#26631;&#35760;&#34394;&#20551;&#29305;&#24449;&#25110;&#26469;&#33258;&#24179;&#34913;&#20998;&#24067;&#30340;&#39069;&#22806;&#26631;&#35760;&#25968;&#25454;&#12290;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#20250;&#21464;&#24471;&#26114;&#36149;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20197;&#25509;&#36817;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#36827;&#34892;&#39069;&#22806;&#27880;&#37322;&#12290;&#25105;&#20204;&#20551;&#35774;&#26377;&#38024;&#23545;&#24615;&#30340;&#20851;&#20110;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#39069;&#22806;&#30417;&#30563;&#24418;&#24335;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Clarify&#65292;&#19968;&#31181;&#26032;&#22411;&#30028;&#38754;&#21644;&#26041;&#27861;&#26469;&#20132;&#20114;&#24335;&#22320;&#32416;&#27491;&#27169;&#22411;&#30340;&#38169;&#35823;&#27010;&#24565;&#12290;&#36890;&#36807;Clarify&#65292;&#29992;&#25143;&#21482;&#38656;&#35201;&#25552;&#20379;&#19968;&#20010;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#25551;&#36848;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#22833;&#36133;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23436;&#20840;&#33258;&#21160;&#21270;&#22320;&#20351;&#29992;s
&lt;/p&gt;
&lt;p&gt;
In supervised learning, models are trained to extract correlations from a static dataset. This often leads to models that rely on high-level misconceptions. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision. We introduce Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns. Then, in an entirely automated way, we use s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#38750;&#35821;&#35328;&#25277;&#35937;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#21644;&#20010;&#20307;&#27169;&#22359;&#30340;&#20851;&#38190;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2401.12117</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#35821;&#35328;&#25277;&#35937;&#25512;&#29702;&#30340;&#22855;&#29305;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#38750;&#35821;&#35328;&#25277;&#35937;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#21644;&#20010;&#20307;&#27169;&#22359;&#30340;&#20851;&#38190;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20173;&#22312;&#36880;&#28176;&#24212;&#29992;&#20110;&#26032;&#39046;&#22495;&#24182;&#22312;&#26032;&#24212;&#29992;&#20013;&#34987;&#21033;&#29992;&#65292;&#20294;&#25105;&#20204;&#27491;&#22312;&#32463;&#21382;&#26032;&#19968;&#20195;&#22522;&#30784;&#27169;&#22411;&#30340;&#28044;&#29616;&#65292;&#21363;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#25972;&#21512;&#65292;&#20026;&#23637;&#31034;&#20986;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20004;&#31181;&#27169;&#24577;&#30340;&#20132;&#38598;&#22788;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;MLLMs&#30340;&#21069;&#26223;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#21069;&#26223;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Raven's Progressive Matrices&#30340;&#21464;&#24335;&#35780;&#20272;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;MLLMs&#30340;&#38750;&#35821;&#35328;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#20043;&#38388;&#24040;&#22823;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#20010;&#20307;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#22359;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#23548;&#33268;&#27169;&#22411;&#30340;&#24615;&#33021;&#21463;&#21040;&#20302;&#35895;&#30340;&#38480;&#21046;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#25552;&#39640;MLLMs&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices. Our experiments expose the difficulty of solving such problems while showcasing the immense gap between open-source and closed-source models. We also reveal critical shortcomings with individual visual and textual modules, subjecting the models to low-performance ceilings. Finally, to improve MLLMs' performance, we experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoQAH&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35757;&#32451;&#20110;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;VQA&#27169;&#22411;&#30340;QA&#20132;&#20114;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#23558;&#21487;&#35270;&#38382;&#31572;&#20174;&#21512;&#25104;&#38382;&#39064;&#27867;&#21270;&#21040;&#20154;&#24037;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12289;VQA&#27169;&#22411;&#21644;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.06400</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;VQA&#27169;&#22411;&#36830;&#25509;&#36215;&#26469;&#65292;&#23558;&#21487;&#35270;&#38382;&#31572;&#20174;&#21512;&#25104;&#38382;&#39064;&#27867;&#21270;&#21040;&#20154;&#24037;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model. (arXiv:2401.06400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoQAH&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35757;&#32451;&#20110;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;VQA&#27169;&#22411;&#30340;QA&#20132;&#20114;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#23558;&#21487;&#35270;&#38382;&#31572;&#20174;&#21512;&#25104;&#38382;&#39064;&#27867;&#21270;&#21040;&#20154;&#24037;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12289;VQA&#27169;&#22411;&#21644;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35270;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#19968;&#20010;&#32473;&#23450;&#22270;&#20687;&#24182;&#23601;&#35813;&#22270;&#20687;&#25552;&#20986;&#19968;&#31995;&#21015;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#26500;&#24314;&#19968;&#20010;&#39640;&#25928;&#30340;VQA&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;QA&#25968;&#25454;&#65292;&#32780;&#19988;&#38750;&#24120;&#26114;&#36149;&#12290;&#26681;&#25454;&#27169;&#26495;&#29983;&#25104;&#21512;&#25104;&#30340;QA&#23545;&#26159;&#33719;&#24471;&#25968;&#25454;&#30340;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#20110;&#36825;&#20123;&#25968;&#25454;&#19978;&#30340;VQA&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20154;&#24037;&#38382;&#39064;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#20154;&#24037;&#38382;&#39064;&#36830;&#38145;&#38382;&#31572;&#8221;&#65288;CoQAH&#65289;&#12290;CoQAH&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19968;&#20010;&#35757;&#32451;&#20110;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;VQA&#27169;&#22411;&#20043;&#38388;&#30340;QA&#20132;&#20114;&#24207;&#21015;&#26469;&#25512;&#29702;&#21644;&#25512;&#23548;&#20154;&#24037;&#38382;&#39064;&#30340;&#36923;&#36753;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#38382;&#39064;VQA&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;CoQAH&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;3D&#28210;&#26579;&#22270;&#20687;&#21644;&#33016;&#37096;X&#32447;&#22270;&#20687;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CoQAH&#22312;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12289;VQA&#27169;&#22411;&#21644;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#30340;&#34920;&#29616;&#20063;&#36229;&#36807;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) is a task where an image is given, and a series of questions are asked about the image. To build an efficient VQA algorithm, a large amount of QA data is required which is very expensive. Generating synthetic QA pairs based on templates is a practical way to obtain data. However, VQA models trained on those data do not perform well on complex, human-written questions. To address this issue, we propose a new method called {\it chain of QA for human-written questions} (CoQAH). CoQAH utilizes a sequence of QA interactions between a large language model and a VQA model trained on synthetic data to reason and derive logical answers for human-written questions. We tested the effectiveness of CoQAH on two types of human-written VQA datasets for 3D-rendered and chest X-ray images and found that it achieved state-of-the-art accuracy in both types of data. Notably, CoQAH outperformed general vision-language models, VQA models, and medical foundation models with no
&lt;/p&gt;</description></item><item><title>KLoB&#26159;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#20107;&#23454;&#30693;&#35782;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16535</link><description>&lt;p&gt;
KLoB: &#19968;&#31181;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models. (arXiv:2309.16535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16535
&lt;/p&gt;
&lt;p&gt;
KLoB&#26159;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#20107;&#23454;&#30693;&#35782;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23450;&#20301;&#28982;&#21518;&#32534;&#36753;&#30340;&#33539;&#24335;&#24050;&#32463;&#25104;&#20026;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23450;&#20301;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#22320;&#25214;&#21040;&#23884;&#20837;&#25152;&#38656;&#30693;&#35782;&#30340;&#30830;&#20999;&#21442;&#25968;&#36824;&#32570;&#20047;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#23545;&#20107;&#23454;&#30693;&#35782;&#30340;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#20294;&#27809;&#26377;&#25552;&#20379;&#19968;&#31181;&#27979;&#35797;&#20551;&#35774;&#30340;&#26041;&#27861;&#20197;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#35752;&#35770;&#21644;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KLoB&#65292;&#19968;&#20010;&#35780;&#20272;&#21487;&#38752;&#30340;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#24212;&#28385;&#36275;&#30340;&#19977;&#20010;&#22522;&#26412;&#23646;&#24615;&#30340;&#22522;&#20934;&#12290;KLoB&#21487;&#20316;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#24182;&#20026;&#37325;&#26032;&#35780;&#20272;&#20107;&#23454;&#30693;&#35782;&#30340;&#23616;&#37096;&#24615;&#20551;&#35774;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;\url{https://github.com/juyiming/KLoB}&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Locate-Then-Edit paradigm has emerged as one of the main approaches in changing factual knowledge stored in the Language models. However, there is a lack of research on whether present locating methods can pinpoint the exact parameters embedding the desired knowledge. Moreover, although many researchers have questioned the validity of locality hypothesis of factual knowledge, no method is provided to test the a hypothesis for more in-depth discussion and research. Therefore, we introduce KLoB, a benchmark examining three essential properties that a reliable knowledge locating method should satisfy. KLoB can serve as a benchmark for evaluating existing locating methods in language models, and can contributes a method to reassessing the validity of locality hypothesis of factual knowledge. Our is publicly available at \url{https://github.com/juyiming/KLoB}.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPICED&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;</title><link>http://arxiv.org/abs/2309.13080</link><description>&lt;p&gt;
SPICED: &#20855;&#26377;&#22810;&#20010;&#20027;&#39064;&#21644;&#22797;&#26434;&#31243;&#24230;&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels. (arXiv:2309.13080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13080
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPICED&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20351;&#29992;&#26234;&#33021;&#31995;&#32479;&#26469;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#26032;&#38395;&#23186;&#20307;&#30340;&#34028;&#21187;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26032;&#38395;&#30340;&#24322;&#36136;&#24615;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#34394;&#20551;&#21457;&#29616;&#65306;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#27604;&#22914;&#19968;&#23545;&#26032;&#38395;&#26159;&#21542;&#37117;&#28041;&#21450;&#25919;&#27835;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#20294;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#23558;&#26032;&#38395;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#20998;&#21106;&#25104;&#20027;&#39064;&#21487;&#20197;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#23398;&#20064;&#22914;&#20309;&#22312;&#26356;&#29421;&#31364;&#30340;&#39046;&#22495;&#20013;&#21306;&#20998;&#26174;&#33879;&#29305;&#24449;&#26469;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#23384;&#22312;&#30446;&#21069;&#32570;&#20047;&#30340;&#19987;&#39064;&#29305;&#23450;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30456;&#20284;&#26032;&#38395;&#25968;&#25454;&#38598;SPICED&#65292;&#20854;&#20013;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65306;&#29359;&#32618;&#19982;&#27861;&#24459;&#12289;&#25991;&#21270;&#19982;&#23089;&#20048;&#12289;&#28798;&#38590;&#19982;&#20107;&#25925;&#12289;&#32463;&#27982;&#19982;&#21830;&#19994;&#12289;&#25919;&#27835;&#19982;&#20914;&#31361;&#12289;&#31185;&#23398;&#19982;&#25216;&#26415;&#20197;&#21450;&#20307;&#32946;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the use of intelligent systems to detect redundant information in news articles has become especially prevalent with the proliferation of news media outlets in order to enhance user experience. However, the heterogeneous nature of news can lead to spurious findings in these systems: Simple heuristics such as whether a pair of news are both about politics can provide strong but deceptive downstream performance. Segmenting news similarity datasets into topics improves the training of these models by forcing them to learn how to distinguish salient characteristics under more narrow domains. However, this requires the existence of topic-specific datasets, which are currently lacking. In this article, we propose a new dataset of similar news, SPICED, which includes seven topics: Crime &amp; Law, Culture &amp; Entertainment, Disasters &amp; Accidents, Economy &amp; Business, Politics &amp; Conflicts, Science &amp; Technology, and Sports. Futhermore, we present four distinct approaches for generating news 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;ChatGPT&#35780;&#20272;&#20013;&#38754;&#20020;&#30340;&#25968;&#25454;&#27745;&#26579;&#25361;&#25112;&#65292;&#36890;&#36807;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#38416;&#36848;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#19988;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#30830;&#20445;&#27169;&#22411;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12767</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#30456;&#20449;ChatGPT&#30340;&#35780;&#20272;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we trust the evaluation on ChatGPT?. (arXiv:2303.12767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;ChatGPT&#35780;&#20272;&#20013;&#38754;&#20020;&#30340;&#25968;&#25454;&#27745;&#26579;&#25361;&#25112;&#65292;&#36890;&#36807;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#38416;&#36848;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#19988;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#30830;&#20445;&#27169;&#22411;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#31532;&#19968;&#20010;&#34987;&#24191;&#27867;&#37319;&#32435;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#22810;&#39033;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#38381;&#21512;&#24615;&#20197;&#21450;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#19981;&#26029;&#26356;&#26032;&#65292;&#35780;&#20272;ChatGPT&#22312;&#19981;&#21516;&#38382;&#39064;&#39046;&#22495;&#30340;&#34920;&#29616;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;&#22312;ChatGPT&#30340;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#20316;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#21644;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#36991;&#20813;&#25968;&#25454;&#27745;&#26579;&#21644;&#30830;&#20445;&#20844;&#24179;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, the first large language model (LLM) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. Despite its evident usefulness, evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study of the task of stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.
&lt;/p&gt;</description></item></channel></rss>