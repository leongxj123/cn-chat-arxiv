<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;Auto-regressive Selective Replacement Ascent (ASRA)&#31639;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#24341;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#27602;&#20869;&#23481;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00292</link><description>&lt;p&gt;
&#22522;&#20110;DPP&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#25628;&#32034;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DPP-Based Adversarial Prompt Searching for Lanugage Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00292
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Auto-regressive Selective Replacement Ascent (ASRA)&#31639;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#24341;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#27602;&#20869;&#23481;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#29983;&#25104;&#27627;&#26080;&#24847;&#20041;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#23433;&#20840;&#37096;&#32626;&#12290;&#22240;&#27492;&#65292;&#22312;&#37096;&#32626;&#20043;&#21069;&#21457;&#29616;&#24182;&#20462;&#25913;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#28508;&#22312;&#30340;&#26377;&#27602;&#36755;&#20986;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#25628;&#32034;&#25552;&#31034;&#26469;&#24341;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#30446;&#26631;&#36755;&#20986;&#30340;&#26377;&#27602;&#20869;&#23481;&#12290;&#35813;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25991;&#26412;&#25968;&#25454;&#30340;&#31163;&#25955;&#24615;&#20197;&#21450;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#25152;&#38656;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#22238;&#24402;&#36873;&#25321;&#26367;&#20195;&#19978;&#21319;&#65288;ASRA&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#36873;&#25321;&#25552;&#31034;&#30340;&#31163;&#25955;&#20248;&#21270;&#31639;&#27861;&#12290;&#23545;&#20845;&#31181;&#19981;&#21516;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ASRA&#23545;&#24341;&#21457;&#26377;&#27602;&#20869;&#23481;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00292v1 Announce Type: new  Abstract: Language models risk generating mindless and offensive content, which hinders their safe deployment. Therefore, it is crucial to discover and modify potential toxic outputs of pre-trained language models before deployment. In this work, we elicit toxic content by automatically searching for a prompt that directs pre-trained language models towards the generation of a specific target output. The problem is challenging due to the discrete nature of textual data and the considerable computational resources required for a single forward pass of the language model. To combat these challenges, we introduce Auto-regressive Selective Replacement Ascent (ASRA), a discrete optimization algorithm that selects prompts based on both quality and similarity with determinantal point process (DPP). Experimental results on six different pre-trained language models demonstrate the efficacy of ASRA for eliciting toxic content. Furthermore, our analysis reve
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#20013;&#30340;&#20316;&#29992;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#35299;&#37322;&#22120;&#25552;&#20379;&#35299;&#37322;&#20449;&#24687;&#24182;&#20316;&#20026;&#35780;&#20272;&#22120;&#24102;&#26469;&#26356;&#21512;&#29702;&#30340;CGEC&#35780;&#20272;&#20197;&#22686;&#24378;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11420</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11420
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#20013;&#30340;&#20316;&#29992;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#35299;&#37322;&#22120;&#25552;&#20379;&#35299;&#37322;&#20449;&#24687;&#24182;&#20316;&#20026;&#35780;&#20272;&#22120;&#24102;&#26469;&#26356;&#21512;&#29702;&#30340;CGEC&#35780;&#20272;&#20197;&#22686;&#24378;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#12290;&#20316;&#20026;NLP&#39046;&#22495;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#65288;CGEC&#65289;&#26088;&#22312;&#32416;&#27491;&#36755;&#20837;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#28508;&#22312;&#35821;&#27861;&#38169;&#35823;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#37325;&#28857;&#65292;LLMs&#20316;&#20026;CGEC&#26657;&#27491;&#22120;&#30340;&#24615;&#33021;&#20173;&#28982;&#20196;&#20154;&#19981;&#28385;&#12290;&#20026;&#20102;&#25512;&#21160;CGEC&#39046;&#22495;&#26356;&#22909;&#22320;&#36866;&#24212;LLMs&#26102;&#20195;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;LLMs&#22312;CGEC&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;CGEC&#20013;&#24471;&#21040;&#26356;&#22909;&#30340;&#21033;&#29992;&#21644;&#25506;&#32034;&#12290;&#32771;&#34385;&#21040;LLMs&#20013;&#23384;&#20648;&#30340;&#20016;&#23500;&#35821;&#27861;&#30693;&#35782;&#21644;&#20854;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#20316;&#20026;&#35299;&#37322;&#22120;&#65292;&#20026;CGEC&#23567;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23558;LLMs&#29992;&#20316;&#35780;&#20272;&#22120;&#65292;&#24102;&#26469;&#26356;&#21512;&#29702;&#30340;CGEC&#35780;&#20272;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11420v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have been widely studied by researchers for their roles in various downstream NLP tasks. As a fundamental task in the NLP field, Chinese Grammatical Error Correction (CGEC) aims to correct all potential grammatical errors in the input sentences. Previous studies have shown that LLMs' performance as correctors on CGEC remains unsatisfactory due to its challenging task focus. To promote the CGEC field to better adapt to the era of LLMs, we rethink the roles of LLMs in the CGEC task so that they can be better utilized and explored in CGEC. Considering the rich grammatical knowledge stored in LLMs and their powerful semantic understanding capabilities, we utilize LLMs as explainers to provide explanation information for the CGEC small models during error correction to enhance performance. We also use LLMs as evaluators to bring more reasonable CGEC evaluations, thus alleviating the troubles caused by th
&lt;/p&gt;</description></item><item><title>MAC-SQL&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#38024;&#23545;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#30340;&#24040;&#22823;&#25968;&#25454;&#24211;&#21644;&#22797;&#26434;&#29992;&#25143;&#38382;&#39064;&#65292;&#36890;&#36807;&#26680;&#24515;&#20998;&#35299;&#22120;&#26234;&#33021;&#20307;&#21644;&#36741;&#21161;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#21644;&#27169;&#22411;&#36827;&#34892;&#35299;&#26512;&#21644;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#21644;&#26597;&#35810;&#35299;&#26512;&#12290;</title><link>https://arxiv.org/abs/2312.11242</link><description>&lt;p&gt;
MAC-SQL: &#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11242
&lt;/p&gt;
&lt;p&gt;
MAC-SQL&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#38024;&#23545;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#30340;&#24040;&#22823;&#25968;&#25454;&#24211;&#21644;&#22797;&#26434;&#29992;&#25143;&#38382;&#39064;&#65292;&#36890;&#36807;&#26680;&#24515;&#20998;&#35299;&#22120;&#26234;&#33021;&#20307;&#21644;&#36741;&#21161;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#21644;&#27169;&#22411;&#36827;&#34892;&#35299;&#26512;&#21644;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#21644;&#26597;&#35810;&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#21040;SQL&#26041;&#27861;&#36890;&#24120;&#22312;&#8220;&#24040;&#22823;&#8221;&#30340;&#25968;&#25454;&#24211;&#21644;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#29992;&#25143;&#38382;&#39064;&#19978;&#36973;&#21463;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#21644;&#27169;&#22411;&#21327;&#20316;&#30340;LLM&#30340;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MAC-SQL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#26680;&#24515;&#20998;&#35299;&#22120;&#26234;&#33021;&#20307;&#65292;&#29992;&#20110;&#36827;&#34892;&#24102;&#26377;&#23569;&#26679;&#26412;&#24605;&#32500;&#38142;&#30340;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#65292;&#21516;&#26102;&#36824;&#26377;&#20004;&#20010;&#36741;&#21161;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#25110;&#27169;&#22411;&#33719;&#21462;&#36739;&#23567;&#30340;&#23376;&#25968;&#25454;&#24211;&#24182;&#20462;&#27491;&#38169;&#35823;&#30340;SQL&#26597;&#35810;&#12290;&#20998;&#35299;&#22120;&#26234;&#33021;&#20307;&#19982;&#36741;&#21161;&#26234;&#33021;&#20307;&#21512;&#20316;&#65292;&#26681;&#25454;&#38656;&#35201;&#28608;&#27963;&#65292;&#24182;&#21487;&#20197;&#25193;&#23637;&#20197;&#36866;&#24212;&#26032;&#30340;&#29305;&#24615;&#25110;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#26368;&#21021;&#21033;&#29992;GPT-4&#20316;&#20026;&#24378;&#22823;&#30340;LLM&#39592;&#24178;&#26469;&#23436;&#25104;&#25152;&#26377;&#26234;&#33021;&#20307;&#20219;&#21153;&#65292;&#20197;&#30830;&#23450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11242v3 Announce Type: replace  Abstract: Recent LLM-based Text-to-SQL methods usually suffer from significant performance degradation on ``huge" databases and complex user questions that require multi-step reasoning. Moreover, most existing methods neglect the crucial significance of LLMs utilizing external tools and model collaboration. To address these challenges, we introduce MAC-SQL, a novel LLM-based multi-agent collaborative framework. Our framework comprises a core decomposer agent for Text-to-SQL generation with few-shot chain-of-thought reasoning, accompanied by two auxiliary agents that utilize external tools or models to acquire smaller sub-databases and refine erroneous SQL queries. The decomposer agent collaborates with auxiliary agents, which are activated as needed and can be expanded to accommodate new features or tools for effective Text-to-SQL parsing. In our framework, We initially leverage GPT-4 as the strong backbone LLM for all agent tasks to determine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.07466</link><description>&lt;p&gt;
&#20851;&#20110;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Measuring Faithfulness or Self-consistency of Natural Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#20107;&#21518;&#25110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#35299;&#37322;&#20854;&#39044;&#27979;&#12290;&#20294;&#26159;&#65292;LLM&#21487;&#33021;&#20250;&#32534;&#36896;&#21548;&#36215;&#26469;&#21512;&#29702;&#20294;&#19981;&#24544;&#23454;&#20110;&#20854;&#22522;&#26412;&#25512;&#29702;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#26088;&#22312;&#21028;&#26029;&#20107;&#21518;&#25110;CoT&#35299;&#37322;&#24544;&#23454;&#24230;&#30340;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24544;&#23454;&#24230;&#27979;&#35797;&#19981;&#26159;&#34913;&#37327;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#30340;&#24544;&#23454;&#24230;&#65292;&#32780;&#26159;&#34913;&#37327;&#20854;&#36755;&#20986;&#32423;&#21035;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;i&#65289;&#25105;&#20204;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#32972;&#26223;&#19979;&#28548;&#28165;&#20102;&#24544;&#23454;&#24230;&#27979;&#35797;&#30340;&#22320;&#20301;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;ii&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#36739;&#19968;&#33268;&#24615;&#30340;&#27979;&#35797;&#24211;&#65292;&#39318;&#27425;&#22312;11&#20010;&#24320;&#25918;&#24335;LLMs&#21644;5&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#22871;&#20214;&#19978;&#27604;&#36739;&#20102;&#29616;&#26377;&#27979;&#35797;&#65292;&#21253;&#25324;iii&#65289;&#25105;&#20204;&#30340;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#12290;CC-SHAP&#26159;LLM&#33258;&#19968;&#33268;&#24615;&#30340;&#32454;&#31890;&#24230;&#24230;&#37327;&#65288;&#32780;&#19981;&#26159;&#27979;&#35797;&#65289;&#12290;&#23427;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
&lt;/p&gt;</description></item><item><title>ChemDFM&#26159;&#39318;&#20010;&#38754;&#21521;&#21270;&#23398;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#21270;&#23398;&#25991;&#29486;&#21644;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20855;&#22791;&#20102;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14818</link><description>&lt;p&gt;
ChemDFM: &#21270;&#23398;&#39046;&#22495;&#23545;&#35805;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChemDFM: Dialogue Foundation Model for Chemistry. (arXiv:2401.14818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14818
&lt;/p&gt;
&lt;p&gt;
ChemDFM&#26159;&#39318;&#20010;&#38754;&#21521;&#21270;&#23398;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#21270;&#23398;&#25991;&#29486;&#21644;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20855;&#22791;&#20102;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#33324;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23427;&#20204;&#30340;&#20219;&#21153;&#27010;&#25324;&#21644;&#33258;&#30001;&#23545;&#35805;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#24110;&#21161;&#35774;&#35745;&#21270;&#23398;&#26234;&#33021;(CGI)&#65292;&#20197;&#21327;&#21161;&#21270;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#19987;&#19994;&#35821;&#35328;&#21644;&#30693;&#35782;&#65292;&#22914;&#39640;&#24230;&#20449;&#24687;&#21270;&#30340;SMILES&#31526;&#21495;&#34920;&#31034;&#27861;&#65292;&#38459;&#30861;&#20102;&#19968;&#33324;&#39046;&#22495;LLMs&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ChemDFM&#65292;&#36825;&#26159;&#39318;&#20010;&#38754;&#21521;CGI&#30340;LLM&#12290;ChemDFM-13B&#26159;&#22312;&#21270;&#23398;&#25991;&#29486;&#12289;&#25945;&#31185;&#20070;&#12289;&#35828;&#26126;&#20070;&#20197;&#21450;&#21508;&#31181;&#19968;&#33324;&#39046;&#22495;&#30340;&#25968;&#25454;&#20013;&#35757;&#32451;&#30340;34B&#20196;&#29260;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#23384;&#20648;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#21270;&#23398;&#30693;&#35782;&#21644;&#35821;&#35328;&#65292;&#21516;&#26102;&#20855;&#26377;&#20808;&#36827;&#30340;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;ChemDFM&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#20195;&#34920;&#24615;&#30340;&#24320;&#28304;LLMs&#12290;&#27492;&#22806;&#65292;ChemDFM&#36824;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have established great success in the general domain of natural language processing. Their emerging task generalization and free-form dialogue capabilities can greatly help to design Chemical General Intelligence (CGI) to assist real-world research in chemistry. However, the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry. To this end, we develop ChemDFM, the first LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature, textbooks, and instructions as well as various data from the general domain. Therefore, it can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities. Extensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs. Moreover, ChemDFM can also
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#21644;&#35780;&#20272;&#20851;&#32852;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2401.13256</link><description>&lt;p&gt;
UniMS-RAG: &#29992;&#20110;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#30340;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems. (arXiv:2401.13256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13256
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#21644;&#35780;&#20272;&#20851;&#32852;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#28041;&#21450;&#21040;&#22810;&#20010;&#20449;&#24687;&#28304;&#26102;&#65292;&#20010;&#24615;&#21270;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#21521;&#24448;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35745;&#21010;&#21644;&#25972;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#22797;&#20013;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#20998;&#35299;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30456;&#21516;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#33539;&#24335;&#23558;&#36825;&#19977;&#20010;&#23376;&#20219;&#21153;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#30340;&#20196;&#29260;&#65292;&#21363;&#34892;&#21160;&#20196;&#29260;&#21644;&#35780;&#20272;&#20196;&#29260;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#24182;&#35780;&#20272;&#20851;&#32852;&#24615;&#12290;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34892;&#21160;&#20196;&#29260;&#26377;&#21161;&#20110;&#19982;&#21508;&#31181;&#30693;&#35782;&#28304;&#36827;&#34892;&#20132;&#20114;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20854;&#19978;&#19979;&#25991;&#21644;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#21457;&#35328;&#26465;&#20214;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#35821;&#38899;&#21512;&#25104;&#30340;&#38901;&#24459;&#65292;&#24182;&#30830;&#20445;&#33258;&#28982;&#35821;&#38899;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#36328;&#21457;&#35328;CVAE&#65292;&#36890;&#36807;&#25552;&#21462;&#21608;&#22260;&#21477;&#23376;&#30340;&#22768;&#23398;&#12289;&#35828;&#35805;&#20154;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#29983;&#25104;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#38901;&#24459;&#29305;&#24449;&#65292;&#26377;&#25928;&#27169;&#25311;&#20154;&#31867;&#38901;&#24459;&#29983;&#25104;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#23454;&#29992;&#31639;&#27861;&#65306;CUC-VAE TTS&#29992;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;CUC-VAE SE&#29992;&#20110;&#35821;&#38899;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2309.04156</link><description>&lt;p&gt;
&#36328;&#21457;&#35328;&#26465;&#20214;&#21270;VAE&#35821;&#38899;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cross-Utterance Conditioned VAE for Speech Generation. (arXiv:2309.04156v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#21457;&#35328;&#26465;&#20214;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#35821;&#38899;&#21512;&#25104;&#30340;&#38901;&#24459;&#65292;&#24182;&#30830;&#20445;&#33258;&#28982;&#35821;&#38899;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#36328;&#21457;&#35328;CVAE&#65292;&#36890;&#36807;&#25552;&#21462;&#21608;&#22260;&#21477;&#23376;&#30340;&#22768;&#23398;&#12289;&#35828;&#35805;&#20154;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#29983;&#25104;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#38901;&#24459;&#29305;&#24449;&#65292;&#26377;&#25928;&#27169;&#25311;&#20154;&#31867;&#38901;&#24459;&#29983;&#25104;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#23454;&#29992;&#31639;&#27861;&#65306;CUC-VAE TTS&#29992;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;CUC-VAE SE&#29992;&#20110;&#35821;&#38899;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#22312;&#22810;&#23186;&#20307;&#21046;&#20316;&#20013;&#26377;&#30528;&#28508;&#21147;&#65292;&#20294;&#24120;&#24120;&#38754;&#20020;&#20135;&#29983;&#26377;&#34920;&#29616;&#21147;&#30340;&#35821;&#38899;&#21644;&#26080;&#32541;&#32534;&#36753;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#21457;&#35328;&#26465;&#20214;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#35821;&#38899;&#21512;&#25104;(CUC-VAE S2)&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#38901;&#24459;&#24182;&#30830;&#20445;&#33258;&#28982;&#35821;&#38899;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#34920;&#29616;&#33021;&#21147;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAEs)&#30340;&#20877;&#34920;&#36798;&#33021;&#21147;&#12290;CUC-VAE S2&#26694;&#26550;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#36328;&#21457;&#35328;CVAE&#65292;&#23427;&#20174;&#21608;&#22260;&#30340;&#21477;&#23376;&#20013;&#25552;&#21462;&#22768;&#23398;&#12289;&#35828;&#35805;&#20154;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#20197;&#29983;&#25104;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#38901;&#24459;&#29305;&#24449;&#65292;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#20154;&#31867;&#38901;&#24459;&#29983;&#25104;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#20010;&#38024;&#23545;&#19981;&#21516;&#35821;&#38899;&#21512;&#25104;&#24212;&#29992;&#30340;&#23454;&#29992;&#31639;&#27861;&#65306;CUC-VAE TTS&#20197;&#36827;&#34892;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;CUC-VAE SE&#20197;&#36827;&#34892;&#35821;&#38899;&#32534;&#36753;&#12290;CUC-VAE TTS&#26159;&#35813;&#26694;&#26550;&#30340;&#30452;&#25509;&#24212;&#29992;&#65292;&#20351;&#24471;&#33021;&#22815;&#23558;&#20219;&#24847;&#25991;&#26412;&#36716;&#25104;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25511;&#20869;&#23384;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13343</link><description>&lt;p&gt;
&#33258;&#25511;&#20869;&#23384;&#31995;&#32479;&#37322;&#25918;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#38480;&#36755;&#20837;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System. (arXiv:2304.13343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13343
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25511;&#20869;&#23384;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#21046;&#20110;&#26080;&#27861;&#22788;&#29702;&#36807;&#38271;&#30340;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25511;&#20869;&#23384;&#65288;SCM&#65289;&#31995;&#32479;&#65292;&#20197;&#37322;&#25918;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#38480;&#36755;&#20837;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;SCM&#31995;&#32479;&#30001;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#32452;&#25104;&#65306;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#12289;&#20869;&#23384;&#27969;&#21644;&#20869;&#23384;&#25511;&#21046;&#22120;&#12290;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#36845;&#20195;&#22320;&#22788;&#29702;&#36229;&#38271;&#36755;&#20837;&#65292;&#24182;&#23558;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#23384;&#20648;&#22312;&#20869;&#23384;&#27969;&#20013;&#12290;&#20869;&#23384;&#25511;&#21046;&#22120;&#20026;&#20195;&#29702;&#25552;&#20379;&#38271;&#26399;&#23384;&#20648;&#22120;&#65288;&#24402;&#26723;&#23384;&#20648;&#22120;&#65289;&#21644;&#30701;&#26399;&#23384;&#20648;&#22120;&#65288;&#38378;&#23384;&#65289;&#65292;&#20197;&#29983;&#25104;&#31934;&#30830;&#36830;&#36143;&#30340;&#21709;&#24212;&#12290;&#25511;&#21046;&#22120;&#30830;&#23450;&#24212;&#28608;&#27963;&#21738;&#20123;&#26469;&#33258;&#24402;&#26723;&#23384;&#20648;&#22120;&#30340;&#35760;&#24518;&#65292;&#24182;&#22914;&#20309;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;&#27169;&#22411;&#36755;&#20837;&#20013;&#12290;&#25105;&#20204;&#30340;SCM&#31995;&#32479;&#21487;&#20197;&#19982;&#20219;&#20309;LLMs&#38598;&#25104;&#65292;&#20197;&#20351;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#36229;&#38271;&#25991;&#26412;&#32780;&#26080;&#38656;&#20462;&#25913;&#25110;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;SCM&#31995;&#32479;&#20351;&#24471;LLMs&#33021;&#22815;&#22788;&#29702;&#38271;&#24230;&#39640;&#36798;8192&#20010;&#20196;&#29260;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Language Models (LLMs) are constrained by their inability to process lengthy inputs. To address this limitation, we propose the Self-Controlled Memory (SCM) system to unleash infinite-length input capacity for large-scale language models. Our SCM system is composed of three key modules: the language model agent, the memory stream, and the memory controller. The language model agent iteratively processes ultra-long inputs and stores all historical information in the memory stream. The memory controller provides the agent with both long-term memory (archived memory) and short-term memory (flash memory) to generate precise and coherent responses. The controller determines which memories from archived memory should be activated and how to incorporate them into the model input. Our SCM system can be integrated with any LLMs to enable them to process ultra-long texts without any modification or fine-tuning. Experimental results show that our SCM system enables LLMs, which are not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16199</link><description>&lt;p&gt;
LLaMA-Adapter: &#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter&#36825;&#19968;&#36731;&#37327;&#32423;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLaMA&#39640;&#25928;&#22320;&#24494;&#35843;&#20026;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;&#21033;&#29992;52K&#20010;&#33258;&#25105;&#25351;&#23548;&#31034;&#33539;&#65292;LLaMA-Adapter&#20165;&#22312;&#20923;&#32467;&#30340;LLaMA 7B&#27169;&#22411;&#19978;&#24341;&#20837;&#20102;1.2M&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;8&#20010;A100 GPU&#19978;&#20165;&#32791;&#26102;&#19981;&#21040;&#19968;&#20010;&#23567;&#26102;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#36866;&#24212;&#25552;&#31034;&#65292;&#24182;&#22312;&#36739;&#39640;&#30340;&#21464;&#21387;&#22120;&#23618;&#20013;&#23558;&#23427;&#20204;&#39044;&#32622;&#20110;&#36755;&#20837;&#25991;&#26412;&#20196;&#29260;&#20043;&#21069;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#38646;&#38376;&#25511;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#26032;&#30340;&#25351;&#20196;&#25552;&#31034;&#27880;&#20837;LLaMA&#65292;&#24182;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#39640;&#25928;&#35757;&#32451;&#65292;LLaMA-Adapter&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;7B&#21442;&#25968;&#30340;Alpaca&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#65292;&#29992;&#20110;&#22270;&#20687;&#30456;&#20851;&#30340;LLaMA&#65292;&#22312;ScienceQA&#19978;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/ZrrSkywalker/LLaMA-Adapt&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24615;&#33021;&#19981;&#21487;&#30693;&#30340;&#22810;&#27169;&#24577;&#24471;&#20998;&#26041;&#27861;MM-SHAP&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#22810;&#27169;&#24577;&#27169;&#22411;&#20351;&#29992;&#21508;&#33258;&#27169;&#24577;&#30340;&#27604;&#20363;&#65292;&#24182;&#24212;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#30340;&#24179;&#22343;&#22810;&#27169;&#24577;&#31243;&#24230;&#21644;&#34913;&#37327;&#20010;&#20307;&#27169;&#22411;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21333;&#27169;&#24577;&#23849;&#28291;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#20026;&#26222;&#36941;&#65292;&#32780;MM-SHAP&#26159;&#20998;&#26512;VL&#27169;&#22411;&#22810;&#27169;&#24577;&#34892;&#20026;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2212.08158</link><description>&lt;p&gt;
MM-SHAP&#65306;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#36129;&#29486;&#30340;&#24615;&#33021;&#19981;&#21487;&#30693;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models &amp; Tasks. (arXiv:2212.08158v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24615;&#33021;&#19981;&#21487;&#30693;&#30340;&#22810;&#27169;&#24577;&#24471;&#20998;&#26041;&#27861;MM-SHAP&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#22810;&#27169;&#24577;&#27169;&#22411;&#20351;&#29992;&#21508;&#33258;&#27169;&#24577;&#30340;&#27604;&#20363;&#65292;&#24182;&#24212;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#30340;&#24179;&#22343;&#22810;&#27169;&#24577;&#31243;&#24230;&#21644;&#34913;&#37327;&#20010;&#20307;&#27169;&#22411;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21333;&#27169;&#24577;&#23849;&#28291;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#20026;&#26222;&#36941;&#65292;&#32780;MM-SHAP&#26159;&#20998;&#26512;VL&#27169;&#22411;&#22810;&#27169;&#24577;&#34892;&#20026;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VL&#65289;&#24448;&#24448;&#21033;&#29992;&#21508;&#33258;&#27169;&#24577;&#20013;&#30340;&#19981;&#31283;&#23450;&#25351;&#26631;&#65288;&#20363;&#22914;&#30001;&#20998;&#24067;&#20559;&#24046;&#24341;&#20837;&#65289;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#27599;&#20010;&#27169;&#24577;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#22914;&#26524;&#21333;&#27169;&#24577;&#27169;&#22411;&#22312;VL&#20219;&#21153;&#19978;&#36798;&#21040;&#31867;&#20284;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#65292;&#21017;&#34920;&#26126;&#25152;&#35859;&#30340;&#21333;&#27169;&#24577;&#23849;&#28291;&#24050;&#32463;&#21457;&#29983;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#20934;&#30830;&#24230;&#30340;&#27979;&#35797;&#26080;&#27861;&#26816;&#27979;&#20363;&#22914;&#27169;&#22411;&#39044;&#27979;&#38169;&#35823;&#20294;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#20010;&#27169;&#24577;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-SHAP&#65292;&#19968;&#31181;&#22522;&#20110;Shapley&#20540;&#30340;&#24615;&#33021;&#19981;&#21487;&#30693;&#22810;&#27169;&#24577;&#24471;&#20998;&#65292;&#21487;&#21487;&#38752;&#22320;&#37327;&#21270;&#22810;&#27169;&#24577;&#27169;&#22411;&#20351;&#29992;&#21508;&#33258;&#27169;&#24577;&#30340;&#27604;&#20363;&#12290;&#25105;&#20204;&#23558;MM-SHAP&#24212;&#29992;&#20110;&#20004;&#31181;&#26041;&#24335;&#65306;&#65288;1&#65289;&#27604;&#36739;&#27169;&#22411;&#30340;&#24179;&#22343;&#22810;&#27169;&#24577;&#31243;&#24230;&#65292;&#65288;2&#65289;&#34913;&#37327;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20010;&#20307;&#27169;&#22411;&#23545;&#21508;&#33258;&#27169;&#24577;&#30340;&#36129;&#29486;&#12290;&#20845;&#20010;VL&#27169;&#22411;&#30340;&#23454;&#39564;&#65288;LXMERT&#12289;CLIP&#21644;&#22235;&#20010;ALBEF&#21464;&#20307;&#65289;&#34920;&#26126;&#21333;&#27169;&#24577;&#23849;&#28291;&#27604;&#25105;&#20204;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#20026;&#26222;&#36941;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;MM-SHAP&#26159;&#25581;&#31034;&#21644;&#20998;&#26512;VL&#27169;&#22411;&#22810;&#27169;&#24577;&#34892;&#20026;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality. Instead, we propose MM-SHAP, a performance-agnostic multimodality score based on Shapley values that reliably quantifies in which proportions a multimodal model uses individual modalities. We apply MM-SHAP in two ways: (1) to compare models for their average degree of multimodality, and (2) to measure for individual models the contribution of individual modalities for different tasks and datasets. Experiments with six VL models -- LXMERT, CLIP and four ALBEF variants -- on four VL tasks highlight that unimodal
&lt;/p&gt;</description></item></channel></rss>