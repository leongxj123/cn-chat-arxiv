<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#23558;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#32467;&#21512;&#21040;LLMs&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#29992;&#20110;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;</title><link>https://arxiv.org/abs/2404.01129</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#20449;&#24687;&#24456;&#37325;&#35201;&#65306;&#23558;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#24341;&#20837;LLMs&#20197;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01129
&lt;/p&gt;
&lt;p&gt;
&#23558;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#32467;&#21512;&#21040;LLMs&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#29992;&#20110;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01129v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#33258;&#21160;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#24050;&#32463;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21487;&#35757;&#32451;&#30340;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#26159;&#36890;&#36807;&#35757;&#32451;&#20855;&#26377;&#30495;&#27491;&#27491;&#20363;&#21644;&#38543;&#26426;&#36873;&#25321;&#30340;&#36127;&#20363;&#22238;&#22797;&#26469;&#35757;&#32451;&#30340;&#65292;&#23548;&#33268;&#23427;&#20204;&#20542;&#21521;&#20110;&#23558;&#26356;&#39640;&#20869;&#23481;&#30456;&#20284;&#24615;&#30340;&#22238;&#22797;&#20998;&#37197;&#26356;&#39640;&#30340;&#24471;&#20998;&#32473;&#23450;&#19968;&#20010;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#24615;&#30340;&#36127;&#38754;&#22238;&#22797;&#20855;&#26377;&#19982;&#19978;&#19979;&#25991;&#39640;&#20869;&#23481;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#22312;&#35821;&#20041;&#19978;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#36275;&#20197;&#35780;&#20272;&#36825;&#31867;&#22238;&#22797;&#65292;&#23548;&#33268;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#26041;&#38754;&#26377;&#19968;&#23450;&#25928;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#26377;&#25928;&#22788;&#29702;&#23545;&#25239;&#24615;&#36127;&#38754;&#31034;&#20363;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#65292;&#23427;&#32467;&#21512;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01129v1 Announce Type: new  Abstract: Automatic open-domain dialogue evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing Large Language Models (LLMs) for open-domain dialogue evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for open-domain dialogue evaluation, which combines domain-specific language models (SLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25506;&#32034;&#19971;&#31181;&#25991;&#26412;&#37319;&#26679;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31934;&#32454;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.15455</link><description>&lt;p&gt;
&#25913;&#36827;&#25991;&#26412;&#27969;&#20013;&#29992;&#20110;&#24494;&#35843;SentenceBERT&#30340;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25506;&#32034;&#19971;&#31181;&#25991;&#26412;&#37319;&#26679;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31934;&#32454;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#25991;&#26412;&#25968;&#25454;&#30340;&#28608;&#22686;&#20026;&#26426;&#26500;&#21644;&#20844;&#21496;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#30417;&#27979;&#20844;&#20247;&#23545;&#20854;&#26381;&#21153;&#21644;&#20135;&#21697;&#30340;&#24847;&#35265;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#25968;&#25454;&#30340;&#24555;&#36895;&#29983;&#25104;&#65292;&#22788;&#29702;&#20381;&#27425;&#21040;&#36798;&#12289;&#28508;&#22312;&#26080;&#38480;&#30340;&#25991;&#26412;&#27969;&#30340;&#25991;&#26412;&#27969;&#25366;&#25496;&#35774;&#32622;&#36890;&#24120;&#27604;&#20256;&#32479;&#30340;&#25209;&#37327;&#23398;&#20064;&#26356;&#21512;&#36866;&#12290;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#22240;&#20854;&#22312;&#27969;&#24335;&#20869;&#23481;&#20013;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#33021;&#21147;&#32780;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#65288;&#25968;&#25454;&#20998;&#24067;&#38543;&#26102;&#38388;&#21457;&#29983;&#21464;&#21270;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#30340;&#29616;&#35937;&#65289;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#19971;&#31181;&#25991;&#26412;&#37319;&#26679;&#26041;&#27861;&#23545;&#31934;&#24515;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#20934;&#30830;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#23545;&#20351;&#29992;&#22235;&#31181;&#19981;&#21516;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#30340;SBERT&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15455v1 Announce Type: new  Abstract: The proliferation of textual data on the Internet presents a unique opportunity for institutions and companies to monitor public opinion about their services and products. Given the rapid generation of such data, the text stream mining setting, which handles sequentially arriving, potentially infinite text streams, is often more suitable than traditional batch learning. While pre-trained language models are commonly employed for their high-quality text vectorization capabilities in streaming contexts, they face challenges adapting to concept drift - the phenomenon where the data distribution changes over time, adversely affecting model performance. Addressing the issue of concept drift, this study explores the efficacy of seven text sampling methods designed to selectively fine-tune language models, thereby mitigating performance degradation. We precisely assess the impact of these methods on fine-tuning the SBERT model using four differ
&lt;/p&gt;</description></item><item><title>ICE-PIXIU&#27169;&#22411;&#23558;&#20013;&#25991;&#21644;&#33521;&#25991;&#37329;&#34701;&#20998;&#26512;&#32479;&#19968;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#25552;&#21319;&#21452;&#35821;&#37329;&#34701;&#24314;&#27169;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06249</link><description>&lt;p&gt;
&#27809;&#26377;&#23396;&#23707;&#35821;&#35328;:&#32479;&#19968;&#20013;&#33521;&#25991;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#25351;&#23548;&#25968;&#25454;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06249
&lt;/p&gt;
&lt;p&gt;
ICE-PIXIU&#27169;&#22411;&#23558;&#20013;&#25991;&#21644;&#33521;&#25991;&#37329;&#34701;&#20998;&#26512;&#32479;&#19968;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#25552;&#21319;&#21452;&#35821;&#37329;&#34701;&#24314;&#27169;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#26174;&#33879;&#25512;&#21160;&#20102;&#37329;&#34701;&#20998;&#26512;&#65292;&#20294;&#23427;&#20204;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#22312;&#21333;&#19968;&#35821;&#35328;&#39046;&#22495;&#65292;&#26410;&#20805;&#20998;&#24320;&#21457;&#20013;&#33521;&#25991;&#21452;&#35821;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#24341;&#20837; ICE-PIXIU&#65292;&#26080;&#32541;&#34701;&#21512; ICE-INTENT &#27169;&#22411;&#21644; ICE-FLARE &#21452;&#35821;&#37329;&#34701;&#20998;&#26512;&#22522;&#20934;&#12290;ICE-PIXIU &#29420;&#29305;&#22320;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#20013;&#25991;&#20219;&#21153;&#65292;&#20197;&#21450;&#32763;&#35793;&#21644;&#21407;&#22987;&#33521;&#25991;&#25968;&#25454;&#38598;&#65292;&#20016;&#23500;&#20102;&#21452;&#35821;&#37329;&#34701;&#24314;&#27169;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#12290;&#23427;&#25552;&#20379;&#20102;&#23545;&#22810;&#31181;&#27169;&#22411;&#21464;&#20307;&#30340;&#26080;&#38480;&#35775;&#38382;&#26435;&#38480;&#65292;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#36328;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#25968;&#25454;&#30340;&#22823;&#37327;&#32534;&#35793;&#65292;&#20197;&#21450;&#19968;&#20010;&#20855;&#26377;&#19987;&#23478;&#27880;&#37322;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324; 10 &#20010; NLP &#20219;&#21153;&#65292;20 &#20010;&#21452;&#35821;&#19987;&#29992;&#20219;&#21153;&#65292;&#20849;&#35745;1,185k &#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#24443;&#24213;&#35780;&#20272;&#24378;&#35843;&#20102;&#23558;&#36825;&#20123;&#21452;&#35821;&#25968;&#25454;&#38598;&#32435;&#20837;&#30340;&#20248;&#21183;&#65292;&#23588;&#20854;&#22312;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06249v1 Announce Type: cross  Abstract: While the progression of Large Language Models (LLMs) has notably propelled financial analysis, their application has largely been confined to singular language realms, leaving untapped the potential of bilingual Chinese-English capacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating the ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis. ICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated and original English datasets, enriching the breadth and depth of bilingual financial modeling. It provides unrestricted access to diverse model variants, a substantial compilation of diverse cross-lingual and multi-modal instruction data, and an evaluation benchmark with expert annotations, comprising 10 NLP tasks, 20 bilingual specific tasks, totaling 1,185k datasets. Our thorough evaluation emphasizes the advantages of incorporating these bilingual datasets, especially in t
&lt;/p&gt;</description></item><item><title>Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03640</link><description>&lt;p&gt;
Apollo&#65306;&#36731;&#37327;&#32423;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65306;&#35753;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#26222;&#24800;60&#20159;&#20154;
&lt;/p&gt;
&lt;p&gt;
Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03640
&lt;/p&gt;
&lt;p&gt;
Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#29699;&#21307;&#23398;&#30693;&#35782;&#30340;&#24222;&#22823;&#23384;&#20648;&#24211;&#20027;&#35201;&#26159;&#20197;&#33521;&#35821;&#20026;&#20027;&#65292;&#20294;&#22312;&#20256;&#36882;&#37327;&#36523;&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#26041;&#38754;&#65292;&#26412;&#22320;&#35821;&#35328;&#23545;&#20110;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#23558;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#20154;&#32676;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#28085;&#30422;&#20840;&#29699;61&#20159;&#20154;&#21475;&#30340;&#20845;&#31181;&#26368;&#24120;&#29992;&#35821;&#35328;&#30340;&#21307;&#23398;LLMs&#12290;&#36825;&#19968;&#21162;&#21147;&#26368;&#32456;&#20419;&#25104;&#20102;ApolloCorpora&#22810;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#38598;&#21644;XMedBench&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#22312;&#22810;&#35821;&#35328;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21457;&#24067;&#30340;Apollo&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#30456;&#23545;&#36739;&#23567;&#23610;&#23544;&#65288;&#21363;0.5B&#12289;1.8B&#12289;2B&#12289;6B&#21644;7B&#65289;&#19978;&#21462;&#24471;&#20102;&#19982;&#21516;&#31561;&#22823;&#23567;&#27169;&#22411;&#26368;&#20339;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;Apollo-7B&#26159;&#36804;&#20170;&#20026;&#27490;&#36798;&#21040;70B&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#36739;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03640v1 Announce Type: cross  Abstract: Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#34920;&#31034;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#26377;&#30528;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.16998</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21548;&#21040;&#20102;&#20160;&#20040;&#65311;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21548;&#35273;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
What Do Language Models Hear? Probing for Auditory Representations in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#34920;&#31034;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#26377;&#30528;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23545;&#29289;&#20307;&#30340;&#22768;&#38899;&#20855;&#26377;&#21547;&#20041;&#28145;&#21051;&#19988;&#22522;&#20110;&#23454;&#36136;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#27169;&#22411;&#32473;&#20986;&#19968;&#20010;&#23545;&#35937;&#30340;&#22768;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#32473;&#23450;&#19982;&#35813;&#23545;&#35937;&#30456;&#20851;&#30340;&#38899;&#39057;&#29255;&#27573;&#30340;&#24773;&#20917;&#19979;&#26816;&#32034;&#20986;&#35813;&#23545;&#35937;&#30340;&#27491;&#30830;&#25991;&#26412;&#34920;&#31034;&#12290;&#36825;&#20010;&#25506;&#38024;&#26159;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#25512;&#21160;&#23545;&#35937;&#30340;&#35821;&#35328;&#34920;&#31034;&#21644;&#22768;&#38899;&#34920;&#31034;&#24444;&#27492;&#25509;&#36817;&#12290;&#22312;&#35757;&#32451;&#20043;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#25506;&#38024;&#23545;&#20110;&#19968;&#20123;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#23545;&#35937;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38899;&#39057;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#25506;&#38024;&#30340;&#27867;&#21270;&#33021;&#21147;&#36229;&#36807;&#20102;&#38543;&#26426;&#29468;&#27979;&#30340;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#20855;&#26377;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16998v1 Announce Type: cross  Abstract: This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.
&lt;/p&gt;</description></item><item><title>ToolSword&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#32454;&#33268;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#25345;&#20037;&#23384;&#22312;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10753</link><description>&lt;p&gt;
ToolSword&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#36328;&#19977;&#20010;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10753
&lt;/p&gt;
&lt;p&gt;
ToolSword&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#32454;&#33268;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#25345;&#20037;&#23384;&#22312;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10753v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25277;&#35937;&#65306;&#24037;&#20855;&#23398;&#20064;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#30784;&#26041;&#27861;&#12290;&#23613;&#31649;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#21033;&#29992;&#24037;&#20855;&#26469;&#22686;&#24378;LLMs&#65292;&#20294;&#23427;&#32463;&#24120;&#24573;&#35270;&#19982;&#20854;&#24212;&#29992;&#30456;&#20851;&#30340;&#26032;&#20852;&#23433;&#20840;&#32771;&#34385;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$ToolSword$&#65292;&#36825;&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#32454;&#33268;&#35843;&#26597;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ToolSword&#21246;&#30011;&#20102;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#20845;&#20010;&#23433;&#20840;&#22330;&#26223;&#65292;&#21253;&#25324;&#36755;&#20837;&#38454;&#27573;&#30340;$&#24694;&#24847;$ $&#26597;&#35810;$&#21644;$&#36234;&#29425;$ $&#25915;&#20987;$&#65292;&#25191;&#34892;&#38454;&#27573;&#30340;$&#22122;&#22768;$ $&#35823;&#23548;$&#21644;$&#39118;&#38505;$ $&#32447;&#32034;$&#65292;&#20197;&#21450;&#36755;&#20986;&#38454;&#27573;&#30340;$&#26377;&#23475;$ $&#21453;&#39304;$&#21644;$&#38169;&#35823;$ $&#20914;&#31361;$&#12290;&#23545;11&#20010;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23384;&#22312;&#25345;&#20037;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#22914;&#22788;&#29702;&#26377;&#23475;&#26597;&#35810;&#12289;&#20351;&#29992;&#39118;&#38505;&#24037;&#20855;&#21644;&#25552;&#20379;&#26377;&#23475;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10753v1 Announce Type: cross  Abstract: Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present $ToolSword$, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and $harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedb
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36339;&#34920;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#20889;&#38382;&#39064;&#21644;&#27874;&#26463;&#25628;&#32034;&#26469;&#20943;&#23569;&#30456;&#20284;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22810;&#36339;&#26816;&#32034;&#20013;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.10666</link><description>&lt;p&gt;
&#24320;&#25918;&#22495;&#25991;&#26412;&#21040;SQL&#30340;&#22810;&#36339;&#34920;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Hop Table Retrieval for Open-Domain Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10666
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36339;&#34920;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#20889;&#38382;&#39064;&#21644;&#27874;&#26463;&#25628;&#32034;&#26469;&#20943;&#23569;&#30456;&#20284;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22810;&#36339;&#26816;&#32034;&#20013;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#25991;&#26412;&#21040;SQL&#26159;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#23427;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#34920;&#65292;&#28982;&#21518;&#29983;&#25104;SQL&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21333;&#36339;&#26816;&#32034;&#26041;&#27861;&#24182;&#26410;&#20851;&#27880;&#25991;&#26412;&#21040;SQL&#25361;&#25112;&#20013;&#30340;&#27169;&#24335;&#38142;&#25509;&#65292;&#36825;&#28041;&#21450;&#21040;&#23558;&#38382;&#39064;&#20013;&#30340;&#23454;&#20307;&#19982;&#34920;&#20013;&#23454;&#20307;&#23545;&#40784;&#65292;&#20027;&#35201;&#20307;&#29616;&#22312;&#20004;&#20010;&#26041;&#38754;&#65306;&#30456;&#20284;&#30340;&#26080;&#20851;&#23454;&#20307;&#21644;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#24102;&#37325;&#20889;&#21644;&#27874;&#26463;&#25628;&#32034;&#30340;&#22810;&#36339;&#34920;&#26816;&#32034;&#65288;Murre&#65289;&#12290;&#20026;&#20102;&#20943;&#23569;&#30456;&#20284;&#30340;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#27599;&#20010;&#36339;&#36291;&#20013;&#26410;&#26816;&#32034;&#21040;&#30340;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#27874;&#26463;&#25628;&#32034;&#32771;&#34385;&#25490;&#21517;&#36739;&#20302;&#30340;&#34920;&#12290;&#20026;&#20102;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;Murre&#22522;&#20110;&#22810;&#20010;&#36339;&#36291;&#20013;&#26816;&#32034;&#21040;&#30340;&#34920;&#37325;&#20889;&#38382;&#39064;&#65292;&#20943;&#23569;&#19982;&#30456;&#20851;&#34920;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;SpiderUnion&#21644;BirdUnion+&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10666v1 Announce Type: new  Abstract: Open-domain text-to-SQL is an important task that retrieves question-relevant tables from massive databases and then generates SQL. However, existing retrieval methods that retrieve in a single hop do not pay attention to the text-to-SQL challenge of schema linking, which is aligning the entities in the question with table entities, reflected in two aspects: similar irrelevant entity and domain mismatch entity. Therefore, we propose our method, the multi-hop table retrieval with rewrite and beam search (Murre). To reduce the effect of the similar irrelevant entity, our method focuses on unretrieved entities at each hop and considers the low-ranked tables by beam search. To alleviate the limitation of domain mismatch entity, Murre rewrites the question based on retrieved tables in multiple hops, decreasing the domain gap with relevant tables. We conduct experiments on SpiderUnion and BirdUnion+, reaching new state-of-the-art results with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25968;&#25454;&#29983;&#25104;&#30340;&#35282;&#24230;&#37325;&#26032;&#35299;&#37322;&#20102;In-Context Learning&#65288;ICL&#65289;&#30340;&#26426;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#27969;&#34892;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23545;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#30340;&#20248;&#21155;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#20854;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.02212</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#29983;&#25104;&#30340;&#35282;&#24230;&#23545;In-Context Learning&#26426;&#21046;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Data Generation Perspective to the Mechanism of In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25968;&#25454;&#29983;&#25104;&#30340;&#35282;&#24230;&#37325;&#26032;&#35299;&#37322;&#20102;In-Context Learning&#65288;ICL&#65289;&#30340;&#26426;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#27969;&#34892;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23545;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#30340;&#20248;&#21155;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#20854;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
In-Context Learning&#65288;ICL&#65289;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65292;&#22312;&#21482;&#26377;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19979;&#28216;&#27867;&#21270;&#65292;&#32780;&#26080;&#38656;&#26799;&#24230;&#26356;&#26032;&#12290;&#23613;&#31649;&#26377;&#40723;&#33310;&#20154;&#24515;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;ICL&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#29616;&#26377;&#30740;&#31350;&#25552;&#20379;&#20102;&#21508;&#31181;&#19981;&#21516;&#35266;&#28857;&#30340;&#29702;&#35299;&#12290;&#36825;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#30452;&#35273;&#21644;&#20020;&#26102;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#37322;ICL&#65292;&#21576;&#29616;&#20986;&#20102;&#19968;&#26465;&#27169;&#31946;&#30340;&#36335;&#32447;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#29983;&#25104;&#30340;&#35270;&#35282;&#37325;&#26032;&#35299;&#37322;&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#27969;&#34892;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#22312;&#24191;&#27867;&#24212;&#29992;&#65292;&#20174;&#32780;&#25509;&#36817;&#19968;&#20010;&#31995;&#32479;&#30340;&#35282;&#24230;&#12290;&#25105;&#20204;&#20005;&#26684;&#37319;&#29992;&#25216;&#33021;&#23398;&#20064;&#21644;&#25216;&#33021;&#35782;&#21035;&#30340;&#27010;&#24565;&#23450;&#20041;&#12290;&#23427;&#20204;&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#20110;&#25216;&#33021;&#23398;&#20064;&#21487;&#20197;&#20174;&#19978;&#19979;&#25991;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#23545;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL) empowers Large Language Models (LLMs) with the capacity to learn in context, achieving downstream generalization without gradient updates but with a few in-context examples. Despite the encouraging empirical success, the underlying mechanism of ICL remains unclear, and existing research offers various viewpoints of understanding. These studies propose intuition-driven and ad-hoc technical solutions for interpreting ICL, illustrating an ambiguous road map. In this paper, we leverage a data generation perspective to reinterpret recent efforts and demonstrate the potential broader usage of popular technical solutions, approaching a systematic angle. For a conceptual definition, we rigorously adopt the terms of skill learning and skill recognition. The difference between them is skill learning can learn new data generation functions from in-context data. We also provide a comprehensive study on the merits and weaknesses of different solutions, and highlight the un
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#20294;&#37325;&#35201;&#30340;&#31185;&#23398;&#35282;&#33394;&#65292;&#21363;AI&#20316;&#20026;&#25506;&#32034;&#12290;&#23427;&#24378;&#35843;&#36890;&#36807;&#21019;&#24314;&#21644;&#30740;&#31350;&#26234;&#33021;&#31995;&#32479;&#26469;&#25581;&#31034;&#21487;&#33021;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#26234;&#33021;&#24418;&#24335;&#19981;&#21516;&#30340;&#20505;&#36873;&#26500;&#24314;&#27169;&#22359;&#12290;&#35770;&#25991;&#36890;&#36807;&#35752;&#35770;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#26032;&#39062;&#21644;&#21019;&#36896;&#24615;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35828;&#26126;&#20102;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2401.07964</link><description>&lt;p&gt;
AI&#20316;&#20026;&#25506;&#32034;&#65306;&#23548;&#33322;&#26234;&#33021;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
AI-as-exploration: Navigating intelligence space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07964
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#20294;&#37325;&#35201;&#30340;&#31185;&#23398;&#35282;&#33394;&#65292;&#21363;AI&#20316;&#20026;&#25506;&#32034;&#12290;&#23427;&#24378;&#35843;&#36890;&#36807;&#21019;&#24314;&#21644;&#30740;&#31350;&#26234;&#33021;&#31995;&#32479;&#26469;&#25581;&#31034;&#21487;&#33021;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#26234;&#33021;&#24418;&#24335;&#19981;&#21516;&#30340;&#20505;&#36873;&#26500;&#24314;&#27169;&#22359;&#12290;&#35770;&#25991;&#36890;&#36807;&#35752;&#35770;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#26032;&#39062;&#21644;&#21019;&#36896;&#24615;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35828;&#26126;&#20102;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#25317;&#26377;&#35768;&#22810;&#29983;&#21629;&#30340;&#39046;&#22495;&#65292;&#36825;&#20010;&#26415;&#35821;&#24050;&#32463;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#31185;&#23398;&#21644;&#21830;&#19994;&#21162;&#21147;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#38416;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#19968;&#20010;&#34987;&#24573;&#35270;&#20294;&#21313;&#20998;&#37325;&#35201;&#30340;&#31185;&#23398;&#35282;&#33394;&#65292;&#21363;&#8220;AI&#20316;&#20026;&#25506;&#32034;&#8221;&#12290;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#21019;&#24314;&#21644;&#30740;&#31350;&#33021;&#22815;&#25581;&#31034;&#26234;&#33021;&#20505;&#36873;&#26500;&#24314;&#27169;&#22359;&#30340;&#31995;&#32479;&#65292;&#36825;&#20123;&#27169;&#22359;&#21487;&#33021;&#19981;&#21516;&#20110;&#25105;&#20204;&#29087;&#24713;&#30340;&#20154;&#31867;&#21644;&#21160;&#29289;&#26234;&#33021;&#24418;&#24335;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#35748;&#20026;&#20154;&#24037;&#26234;&#33021;&#26159;&#25506;&#32034;&#26234;&#33021;&#31354;&#38388;&#65292;&#21363;&#21487;&#33021;&#30340;&#26234;&#33021;&#31995;&#32479;&#31354;&#38388;&#65292;&#30340;&#26368;&#20339;&#24037;&#20855;&#20043;&#19968;&#12290;&#25105;&#36890;&#36807;&#20851;&#27880;&#19968;&#20010;&#20855;&#20307;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21363;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#26032;&#39062;&#21644;&#21019;&#36896;&#24615;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#26469;&#35828;&#26126;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#25105;&#23637;&#31034;&#20102;&#23613;&#31649;&#21518;&#32773;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#24456;&#21487;&#33021;&#20197;&#26681;&#26412;&#19981;&#21516;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence is a field that lives many lives, and the term has come to encompass a motley collection of scientific and commercial endeavours. In this paper, I articulate the contours of a rather neglected but central scientific role that AI has to play, which I dub `AI-as-exploration'.The basic thrust of AI-as-exploration is that of creating and studying systems that can reveal candidate building blocks of intelligence that may differ from the forms of human and animal intelligence we are familiar with. In other words, I suggest that AI is one of the best tools we have for exploring intelligence space, namely the space of possible intelligent systems. I illustrate the value of AI-as-exploration by focusing on a specific case study, i.e., recent work on the capacity to combine novel and invented concepts in humans and Large Language Models. I show that the latter, despite showing human-level accuracy in such a task, most probably solve it in ways radically different, but no 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;transformer&#32593;&#32476;&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#30382;&#23618;&#27874;&#22312;&#25552;&#21462;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14267</link><description>&lt;p&gt;
Transformers&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#65306;&#22312;&#26102;&#38388;&#19978;&#20256;&#36882;&#19978;&#19979;&#25991;&#30340;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers and Cortical Waves: Encoders for Pulling In Context Across Time. (arXiv:2401.14267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;transformer&#32593;&#32476;&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#30382;&#23618;&#27874;&#22312;&#25552;&#21462;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;ChatGPT&#21644;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;transformer&#32593;&#32476;&#30340;&#33021;&#21147;&#24050;&#32463;&#24341;&#36215;&#20102;&#19990;&#30028;&#30340;&#20851;&#27880;&#12290;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#23558;&#23436;&#25972;&#30340;&#36755;&#20837;&#24207;&#21015;&#65288;&#20363;&#22914;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#21333;&#35789;&#65289;&#36716;&#21270;&#20026;&#19968;&#20010;&#38271;&#30340;&#8220;&#32534;&#30721;&#21521;&#37327;&#8221;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#23398;&#20064;&#33258;&#28982;&#24207;&#21015;&#20013;&#30340;&#38271;&#31243;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#8220;&#33258;&#27880;&#24847;&#21147;&#8221;&#24212;&#29992;&#20110;&#36825;&#20010;&#32534;&#30721;&#21521;&#37327;&#65292;&#36890;&#36807;&#35745;&#31639;&#36755;&#20837;&#24207;&#21015;&#20013;&#21333;&#35789;&#23545;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#22686;&#24378;&#20102;transformer&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#35748;&#20026;&#31070;&#32463;&#27963;&#21160;&#22312;&#21333;&#20010;&#30382;&#23618;&#21306;&#22495;&#20869;&#25110;&#25972;&#20010;&#22823;&#33041;&#33539;&#22260;&#20869;&#20256;&#25773;&#30340;&#27874;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#32534;&#30721;&#21407;&#29702;&#12290;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#21051;&#23558;&#26368;&#36817;&#30340;&#36755;&#20837;&#21382;&#21490;&#23553;&#35013;&#20026;&#21333;&#20010;&#31354;&#38388;&#27169;&#24335;&#65292;&#30382;&#23618;&#27874;&#21487;&#20197;&#20174;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#25552;&#21462;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#36825;&#19982;&#35745;&#31639;&#21407;&#29702;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention. The crucial computational mechanism underlying their performance relies on transforming a complete input sequence - for example, all the words in a sentence into a long "encoding vector" - that allows transformers to learn long-range temporal dependencies in naturalistic sequences. Specifically, "self-attention" applied to this encoding vector enhances temporal context in transformers by computing associations between pairs of words in the input sequence. We suggest that waves of neural activity, traveling across single cortical regions or across multiple regions at the whole-brain scale, could implement a similar encoding principle. By encapsulating recent input history into a single spatial pattern at each moment in time, cortical waves may enable temporal context to be extracted from sequences of sensory inputs, the same computational principle used in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2310.02304</link><description>&lt;p&gt;
&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65306;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;&#24605;&#32500;&#26641;&#21644;&#31243;&#24207;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#8220;&#33050;&#25163;&#26550;&#8221;&#31243;&#24207;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#31243;&#24207;&#26500;&#24314;&#20102;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;&#33050;&#25163;&#26550;&#31243;&#24207;&#36890;&#24120;&#20351;&#29992;Python&#31561;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#31181;&#23376;&#8220;&#25913;&#36827;&#22120;&#8221;&#24320;&#22987;&#65292;&#36890;&#36807;&#22810;&#27425;&#26597;&#35810;&#35821;&#35328;&#27169;&#22411;&#24182;&#36820;&#22238;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#26681;&#25454;&#32473;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25913;&#36827;&#36755;&#20837;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36816;&#34892;&#36825;&#20010;&#31181;&#23376;&#25913;&#36827;&#22120;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#22312;&#19968;&#31995;&#21015;&#32454;&#20998;&#20219;&#21153;&#20013;&#65292;&#24471;&#21040;&#30340;&#25913;&#36827;&#25913;&#36827;&#22120;&#29983;&#25104;&#30340;&#31243;&#24207;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#31181;&#23376;&#25913;&#36827;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#30340;&#21508;&#31181;&#33258;&#25105;&#25913;&#36827;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#27874;&#26463;&#25628;&#32034;&#12289;&#36951;&#20256;&#31639;&#27861;&#21644;&#27169;&#25311;&#36864;&#28779;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#27809;&#26377;&#25913;&#21464;&#65292;&#36825;&#24182;&#19981;&#26159;&#19968;&#31181;&#22686;&#38271;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#23398;&#38382;&#31572;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.16035</link><description>&lt;p&gt;
MedEdit&#65306;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases. (arXiv:2309.16035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#23398;&#38382;&#31572;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34429;&#28982;&#22312;&#19968;&#33324;&#39046;&#22495;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#38754;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24448;&#24448;&#20316;&#20026;&#8220;&#40657;&#30418;&#8221;&#36816;&#20316;&#65292;&#38590;&#20197;&#20462;&#25913;&#20854;&#34892;&#20026;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#26088;&#22312;&#25913;&#36827;LLM&#30340;&#21709;&#24212;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26816;&#32034;&#31574;&#30053;&#65292;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;LLM&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#12290;&#36890;&#36807;&#23545;MedQA-SMILE&#25968;&#25454;&#38598;&#36827;&#34892;&#21307;&#23398;QA&#30340;&#37325;&#28857;&#30740;&#31350;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#26816;&#32034;&#27169;&#22411;&#21644;&#21521;LLM&#25552;&#20379;&#30340;&#20107;&#23454;&#25968;&#37327;&#23545;&#20854;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#32534;&#36753;&#21518;&#30340;Vicuna&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20174;44.46&#65285;&#25552;&#39640;&#21040;48.54&#65285;&#12290;&#36825;&#39033;&#24037;&#20316;&#20984;&#26174;&#20102;&#27169;&#22411;&#32534;&#36753;&#25913;&#21892;LLM&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20026;&#32531;&#35299;&#40657;&#30418;LLM&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks like medical question answering (QA). Moreover, they tend to function as "black-boxes," making it challenging to modify their behavior. Addressing this, our study delves into model editing utilizing in-context learning, aiming to improve LLM responses without the need for fine-tuning or retraining. Specifically, we propose a comprehensive retrieval strategy to extract medical facts from an external knowledge base, and then we incorporate them into the query prompt for the LLM. Focusing on medical QA using the MedQA-SMILE dataset, we evaluate the impact of different retrieval models and the number of facts provided to the LLM. Notably, our edited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%. This work underscores the potential of model editing to enhance LLM performance, offering a practical approach to mitigate the challenges of black-box LLMs.
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25361;&#25112;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#23398;&#20064;&#12289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#21644;&#19981;&#23545;&#40784;&#25968;&#25454;&#23398;&#20064;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01008</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Multimodal Learning: A Survey. (arXiv:2304.01008v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01008
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25361;&#25112;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#23398;&#20064;&#12289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#21644;&#19981;&#23545;&#40784;&#25968;&#25454;&#23398;&#20064;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#29702;&#35299;&#21644;&#20998;&#26512;&#26469;&#33258;&#22810;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#19979;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#20110;&#37197;&#23545;&#25968;&#25454;&#21644;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#27169;&#22411;&#30340;&#25193;&#23637;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#37492;&#20110;&#37326;&#22806;&#26377;&#22823;&#35268;&#27169;&#26410;&#27880;&#37322;&#30340;&#25968;&#25454;&#21487;&#29992;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#20026;&#32531;&#35299;&#27880;&#37322;&#29942;&#39048;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#31574;&#30053;&#12290;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;SSML&#65289;&#24314;&#31435;&#22312;&#36825;&#20004;&#20010;&#26041;&#21521;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20379;&#20102;&#20174;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;SSML&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#38416;&#36848;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#38754;&#20020;&#30340;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#65292;&#65288;2&#65289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19982;&#19981;&#23545;&#40784;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#36825;&#20123;&#25361;&#25112;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#65288;1&#65289;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning, which aims to understand and analyze information from multiple modalities, has achieved substantial progress in the supervised regime in recent years. However, the heavy dependence on data paired with expensive human annotations impedes scaling up models. Meanwhile, given the availability of large-scale unannotated data in the wild, self-supervised learning has become an attractive strategy to alleviate the annotation bottleneck. Building on these two directions, self-supervised multimodal learning (SSML) provides ways to learn from raw multimodal data. In this survey, we provide a comprehensive review of the state-of-the-art in SSML, in which we elucidate three major challenges intrinsic to self-supervised learning with multimodal data: (1) learning representations from multimodal data without labels, (2) fusion of different modalities, and (3) learning with unaligned data. We then detail existing solutions to these challenges. Specifically, we consider (1) object
&lt;/p&gt;</description></item></channel></rss>