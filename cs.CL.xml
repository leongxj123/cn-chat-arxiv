<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#21019;&#24314;&#20102;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;</title><link>https://arxiv.org/abs/2404.01461</link><description>&lt;p&gt;
&#35831;&#30495;&#27491;&#30340;&#29747;&#36798;&#31449;&#20986;&#26469;...&#38754;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#65311;&#22312;LLMs&#20013;&#23457;&#35270;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;
&lt;/p&gt;
&lt;p&gt;
Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#21019;&#24314;&#20102;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#23637;&#29616;&#20986;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLMs&#21487;&#33021;&#20250;&#23481;&#26131;&#21463;&#21040;&#20154;&#31867;&#20915;&#31574;&#20013;&#30340;&#19968;&#31181;&#24120;&#35265;&#35748;&#30693;&#38519;&#38449;&#24433;&#21709;&#65292;&#21363;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#12290;&#36825;&#26159;&#24515;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#27010;&#24565;&#65292;&#25351;&#30340;&#26159;&#26681;&#25454;&#20107;&#20214;&#19982;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#21407;&#22411;&#25110;&#20856;&#22411;&#20363;&#23376;&#30340;&#30456;&#20284;&#31243;&#24230;&#26469;&#21028;&#26029;&#20107;&#20214;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#20107;&#23454;&#25110;&#32479;&#35745;&#35777;&#25454;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#23545;LLM&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;REHEAT&#65288;Representativeness Heuristic AI Testing&#65289;&#65292;&#19968;&#20010;&#21253;&#21547;&#28085;&#30422;&#20845;&#31181;&#24120;&#35265;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#31867;&#22411;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#26174;&#31034;&#65292;&#24212;&#29992;&#20110;REHEAT&#30340;&#22235;&#20010;LLMs&#37117;&#34920;&#29616;&#20986;&#20195;&#34920;&#24615;&#21551;&#21457;&#24335;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#27493;&#39588;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01461v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#35270;&#35273;&#12289;&#26412;&#20307;&#24863;&#30693;&#21644;&#35821;&#35328;&#30340;&#22823;&#33041;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#21644;&#20027;&#21160;&#25512;&#26029;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#33258;&#30001;&#33021;&#21407;&#29702;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#32452;&#21512;&#24615;&#21644;&#24863;&#35273;&#36816;&#21160;&#25216;&#33021;&#30340;&#32852;&#21512;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.19995</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#23398;&#20064;&#35821;&#35328;&#21644;&#26426;&#22120;&#20154;&#21160;&#20316;&#23454;&#29616;&#32452;&#21512;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#35270;&#35273;&#12289;&#26412;&#20307;&#24863;&#30693;&#21644;&#35821;&#35328;&#30340;&#22823;&#33041;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#21644;&#20027;&#21160;&#25512;&#26029;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#33258;&#30001;&#33021;&#21407;&#29702;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#32452;&#21512;&#24615;&#21644;&#24863;&#35273;&#36816;&#21160;&#25216;&#33021;&#30340;&#32852;&#21512;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25797;&#38271;&#23558;&#23398;&#21040;&#30340;&#34892;&#20026;&#24212;&#29992;&#20110;&#26410;&#23398;&#20064;&#36807;&#30340;&#24773;&#22659;&#12290;&#36825;&#31181;&#27867;&#21270;&#34892;&#20026;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#25105;&#20204;&#33021;&#22815;&#23558;&#25972;&#20307;&#20998;&#35299;&#25104;&#21487;&#37325;&#22797;&#21033;&#29992;&#30340;&#37096;&#20998;&#30340;&#33021;&#21147;&#65292;&#21363;&#32452;&#21512;&#24615;&#12290;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#28041;&#21450;&#36825;&#31181;&#29305;&#24449;&#12290;&#8220;&#22312;&#20010;&#20307;&#21482;&#23398;&#20064;&#37096;&#20998;&#35821;&#35328;&#32452;&#21512;&#21450;&#20854;&#30456;&#24212;&#30340;&#24863;&#35273;&#36816;&#21160;&#27169;&#24335;&#26102;&#65292;&#22914;&#20309;&#36890;&#36807;&#32852;&#24819;&#23398;&#20064;&#21516;&#26102;&#21457;&#23637;&#35821;&#35328;&#30340;&#32452;&#21512;&#24615;&#21644;&#24863;&#35273;&#36816;&#21160;&#25216;&#33021;&#65311;&#8221;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#35270;&#35273;&#12289;&#26412;&#20307;&#24863;&#30693;&#21644;&#35821;&#35328;&#30340;&#22823;&#33041;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#20854;&#32435;&#20837;&#22522;&#20110;&#33258;&#30001;&#33021;&#21407;&#29702;&#30340;&#39044;&#27979;&#32534;&#30721;&#21644;&#20027;&#21160;&#25512;&#26029;&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#19982;&#26426;&#22120;&#20154;&#25163;&#33218;&#36827;&#34892;&#30340;&#21508;&#31181;&#27169;&#25311;&#23454;&#39564;&#35780;&#20272;&#20102;&#36825;&#20010;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23398;&#20064;&#20013;&#23545;&#20110;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19995v1 Announce Type: new  Abstract: Humans excel at applying learned behavior to unlearned situations. A crucial component of this generalization behavior is our ability to compose/decompose a whole into reusable parts, an attribute known as compositionality. One of the fundamental questions in robotics concerns this characteristic. "How can linguistic compositionality be developed concomitantly with sensorimotor skills through associative learning, particularly when individuals only learn partial linguistic compositions and their corresponding sensorimotor patterns?" To address this question, we propose a brain-inspired neural network model that integrates vision, proprioception, and language into a framework of predictive coding and active inference, based on the free-energy principle. The effectiveness and capabilities of this model were assessed through various simulation experiments conducted with a robot arm. Our results show that generalization in learning to unlear
&lt;/p&gt;</description></item><item><title>SyllabusQA&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#23545;36&#20010;&#19987;&#19994;&#28085;&#30422;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#20102;&#35814;&#32454;&#25910;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#31572;&#26696;&#20107;&#23454;&#24615;&#65292;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#23384;&#22312;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.14666</link><description>&lt;p&gt;
SyllabusQA&#65306;&#19968;&#20010;&#35838;&#31243;&#36923;&#36753;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SyllabusQA: A Course Logistics Question Answering Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14666
&lt;/p&gt;
&lt;p&gt;
SyllabusQA&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#23545;36&#20010;&#19987;&#19994;&#28085;&#30422;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#20102;&#35814;&#32454;&#25910;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#31572;&#26696;&#20107;&#23454;&#24615;&#65292;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#23384;&#22312;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#25945;&#23398;&#21161;&#29702;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#26377;&#26174;&#33879;&#28508;&#21147;&#20943;&#36731;&#20154;&#31867;&#25945;&#24072;&#30340;&#24037;&#20316;&#37327;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19982;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#22238;&#31572;&#65292;&#36825;&#23545;&#23398;&#29983;&#24456;&#37325;&#35201;&#65292;&#20294;&#23545;&#25945;&#24072;&#26469;&#35828;&#26159;&#37325;&#22797;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SyllabusQA&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#65292;&#28085;&#30422;36&#20010;&#19987;&#19994;&#65292;&#21253;&#21547;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#38382;&#39064;&#31867;&#22411;&#21644;&#31572;&#26696;&#26684;&#24335;&#37117;&#26159;&#22810;&#26679;&#30340;&#12290;&#30001;&#20110;&#35768;&#22810;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;&#21253;&#21547;&#20851;&#38190;&#20449;&#24687;&#65292;&#22914;&#32771;&#35797;&#26085;&#26399;&#65292;&#35780;&#20272;&#31572;&#26696;&#30340;&#20107;&#23454;&#24615;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#35813;&#20219;&#21153;&#19978;&#23545;&#20960;&#20010;&#24378;&#22522;&#32447;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#21040;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#22312;&#20256;&#32479;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#25351;&#26631;&#19978;&#25509;&#36817;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14666v1 Announce Type: cross  Abstract: Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21160;&#24577;&#20869;&#23384;&#21387;&#32553;&#65288;DMC&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#20851;&#38190;-&#20540;&#32531;&#23384;&#21387;&#32553;&#65292;&#27169;&#22411;&#23398;&#20064;&#22312;&#19981;&#21516;&#30340;&#22836;&#37096;&#21644;&#23618;&#20013;&#24212;&#29992;&#19981;&#21516;&#30340;&#21387;&#32553;&#29575;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;LLMs&#25913;&#35013;&#20026;DMC Transformers&#65292;&#22312;&#33258;&#22238;&#24402;&#25512;&#26029;&#20013;&#23454;&#29616;&#20102;&#39640;&#36798;~3.7&#20493;&#30340;&#21534;&#21520;&#37327;&#22686;&#21152;&#12290;</title><link>https://arxiv.org/abs/2403.09636</link><description>&lt;p&gt;
&#21160;&#24577;&#20869;&#23384;&#21387;&#32553;&#65306;&#29992;&#20110;&#21152;&#36895;&#25512;&#26029;&#30340;LLMs&#30340;&#25913;&#35013;
&lt;/p&gt;
&lt;p&gt;
Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09636
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21160;&#24577;&#20869;&#23384;&#21387;&#32553;&#65288;DMC&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#20851;&#38190;-&#20540;&#32531;&#23384;&#21387;&#32553;&#65292;&#27169;&#22411;&#23398;&#20064;&#22312;&#19981;&#21516;&#30340;&#22836;&#37096;&#21644;&#23618;&#20013;&#24212;&#29992;&#19981;&#21516;&#30340;&#21387;&#32553;&#29575;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;LLMs&#25913;&#35013;&#20026;DMC Transformers&#65292;&#22312;&#33258;&#22238;&#24402;&#25512;&#26029;&#20013;&#23454;&#29616;&#20102;&#39640;&#36798;~3.7&#20493;&#30340;&#21534;&#21520;&#37327;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25903;&#26609;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22312;&#20869;&#23384;&#20013;&#23384;&#20648;&#20851;&#38190;-&#20540;&#34920;&#31034;&#30340;&#32531;&#23384;&#20197;&#29992;&#20110;&#36807;&#21435;&#30340;&#26631;&#35760;&#65292;&#20854;&#22823;&#23567;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#21644;&#25209;&#22788;&#29702;&#22823;&#23567;&#21576;&#32447;&#24615;&#27604;&#20363;&#65292;&#22240;&#27492;&#29983;&#25104;&#20173;&#28982;&#20302;&#25928;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#20869;&#23384;&#21387;&#32553;&#65288;DMC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#20851;&#38190;-&#20540;&#32531;&#23384;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#27169;&#22411;&#23398;&#20064;&#22312;&#19981;&#21516;&#30340;&#22836;&#37096;&#21644;&#23618;&#20013;&#24212;&#29992;&#19981;&#21516;&#30340;&#21387;&#32553;&#29575;&#12290;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;LLMs&#65288;&#22914;Llama 2&#65288;7B&#12289;13B&#21644;70B&#65289;&#65289;&#25913;&#35013;&#20026;DMC Transformers&#65292;&#22312;NVIDIA H100 GPU&#19978;&#30340;&#33258;&#22238;&#24402;&#25512;&#26029;&#20013;&#23454;&#29616;&#20102;&#39640;&#36798;~3.7&#20493;&#30340;&#21534;&#21520;&#37327;&#22686;&#21152;&#12290;DMC&#36890;&#36807;&#22312;&#21407;&#22987;&#25968;&#25454;&#30340;&#21487;&#24573;&#30053;&#30334;&#20998;&#27604;&#19978;&#36827;&#34892;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#32780;&#24212;&#29992;&#65292;&#24182;&#19988;&#19981;&#28155;&#21152;&#20219;&#20309;&#39069;&#22806;&#21442;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39640;&#36798;4&#20493;&#32531;&#23384;&#21387;&#32553;&#30340;&#24773;&#20917;&#19979;&#65292;DMC&#20445;&#30041;&#20102;&#21407;&#22987;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#20248;&#20110;up-trained grouped-query a&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09636v1 Announce Type: new  Abstract: Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time. Most importantly, the model learns to apply different compression rates in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14973</link><description>&lt;p&gt;
GenCeption&#65306;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;LLM
&lt;/p&gt;
&lt;p&gt;
GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#24102;&#26631;&#27880;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#38590;&#20197;&#36319;&#19978;MLLM&#35780;&#20272;&#30340;&#24555;&#36895;&#21457;&#23637;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenCeption&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#38656;&#27880;&#37322;&#30340;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#20165;&#38656;&#35201;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#21453;&#26144;&#20986;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;DrawCeption&#28216;&#25103;&#65292;GenCeption&#20174;&#19968;&#20010;&#38750;&#25991;&#26412;&#26679;&#26412;&#24320;&#22987;&#65292;&#24182;&#32463;&#21382;&#19968;&#31995;&#21015;&#36845;&#20195;&#30340;&#25551;&#36848;&#21644;&#29983;&#25104;&#27493;&#39588;&#12290;&#36845;&#20195;&#20043;&#38388;&#30340;&#35821;&#20041;&#28418;&#31227;&#20351;&#29992;GC@T&#25351;&#26631;&#36827;&#34892;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#39564;&#35777;&#20102;GenCeption&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;GenCeption&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#19988;&#20197;&#21069;&#26410;&#35265;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#25193;&#23637;&#65292;&#20197;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#30340;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
&lt;/p&gt;</description></item><item><title>&#23545;&#22810;&#27169;&#24577;LLMs&#30340;&#27450;&#39575;&#24615;&#25552;&#31034;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#21253;&#21547;850&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#22522;&#20934;&#27979;&#35797;MAD-Bench&#65292;&#21457;&#29616;GPT-4V&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#19978;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#24615;&#33021;&#24046;&#36317;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.13220</link><description>&lt;p&gt;
&#26377;&#22810;&#23481;&#26131;&#27450;&#39575;&#22810;&#27169;&#24577;LLMs&#65311;&#20851;&#20110;&#27450;&#39575;&#24615;&#25552;&#31034;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13220
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;LLMs&#30340;&#27450;&#39575;&#24615;&#25552;&#31034;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#21253;&#21547;850&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#22522;&#20934;&#27979;&#35797;MAD-Bench&#65292;&#21457;&#29616;GPT-4V&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#19978;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#24615;&#33021;&#24046;&#36317;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#26174;&#33879;&#36827;&#23637;&#24182;&#27809;&#26377;&#20351;&#23427;&#20204;&#20813;&#30123;&#21508;&#31181;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#24102;&#26377;&#27450;&#39575;&#24615;&#20449;&#24687;&#30340;&#25552;&#31034;&#26102;&#65292;&#20250;&#20135;&#29983;&#24187;&#35273;&#33324;&#30340;&#22238;&#24212;&#12290;&#20026;&#20102;&#23450;&#37327;&#35780;&#20272;&#36825;&#31181;&#33030;&#24369;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MAD-Bench&#65292;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;850&#20010;&#27979;&#35797;&#26679;&#26412;&#65292;&#20998;&#20026;6&#20010;&#31867;&#21035;&#65292;&#22914;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#12289;&#23545;&#35937;&#25968;&#37327;&#12289;&#31354;&#38388;&#20851;&#31995;&#21644;&#35270;&#35273;&#28151;&#28102;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;MLLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21253;&#25324;GPT-4V&#12289;Gemini-Pro&#65292;&#20197;&#21450;&#24320;&#28304;&#27169;&#22411;&#65292;&#22914;LLaVA-1.5&#21644;CogVLM&#12290;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GPT-4V&#21644;&#20854;&#20182;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65307;&#20043;&#21069;&#30340;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#65292;&#22914;LRV-Instruction&#21644;LLaVA-RLHF&#65292;&#22312;&#36825;&#20010;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#24182;&#19981;&#26377;&#25928;&#12290;&#34429;&#28982;GPT-4V&#22312;MAD-Bench&#19978;&#21462;&#24471;&#20102;75.02%&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#20854;&#20182;&#20219;&#20309;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#37117;&#27809;&#26377;&#36798;&#21040;&#36825;&#19968;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13220v1 Announce Type: cross  Abstract: The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our exper
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GRAFFORD&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#23545;&#29289;&#20307;&#21487;&#20379;&#24615;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24403;&#21069;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#19981;&#24120;&#35265;&#29289;&#20307;&#21487;&#20379;&#24615;&#26041;&#38754;&#23384;&#22312;&#25512;&#29702;&#33021;&#21147;&#30340;&#23616;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.12881</link><description>&lt;p&gt;
GRAFFORD: &#29992;&#20110;&#27979;&#35797;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#23545;&#29289;&#20307;&#21487;&#20379;&#24615;&#30693;&#35782;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12881
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GRAFFORD&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#23545;&#29289;&#20307;&#21487;&#20379;&#24615;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24403;&#21069;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#19981;&#24120;&#35265;&#29289;&#20307;&#21487;&#20379;&#24615;&#26041;&#38754;&#23384;&#22312;&#25512;&#29702;&#33021;&#21147;&#30340;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20013;&#20851;&#20110;&#29289;&#20307;&#21487;&#20379;&#24615;&#30340;&#30693;&#35782;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTLM&#65289;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#25991;&#26412;&#20013;&#23398;&#20064;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#22312;&#19979;&#28216;NLU&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;PTLM&#22312;&#25512;&#29702;&#21644;&#22522;&#30784;&#26041;&#38754;&#23384;&#22312;&#19981;&#19968;&#33268;&#19988;&#19981;&#30452;&#35266;&#30340;&#22833;&#36133;&#12290;&#20026;&#20102;&#39318;&#27425;&#23450;&#37327;&#34913;&#37327;&#22522;&#30784;&#65288;&#25110;&#32570;&#20047;&#65289;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#20851;&#20110;&#29289;&#20307;&#21487;&#20379;&#24615;&#30340;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;-- GrAFFORD&#65292;&#21253;&#21547;15&#20010;&#21487;&#20379;&#24615;&#31867;&#21035;&#12290;&#19982;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#25910;&#38598;&#30340;&#21487;&#20379;&#24615;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#29992;&#29616;&#22330;&#21477;&#23376;&#26631;&#27880;&#20102;&#23545;&#35937;&#21644;&#21487;&#20379;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#28041;&#21450;&#19981;&#24120;&#35265;&#30340;&#29289;&#20307;&#21487;&#20379;&#24615;&#26102;&#65292;PTLM&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;PTLM&#22312;&#29702;&#35299;&#19981;&#24120;&#35265;&#29289;&#20307;&#21487;&#20379;&#24615;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12881v1 Announce Type: new  Abstract: We investigate the knowledge of object affordances in pre-trained language models (LMs) and pre-trained Vision-Language models (VLMs). Transformers-based large pre-trained language models (PTLM) learn contextual representation from massive amounts of unlabeled text and are shown to perform impressively in downstream NLU tasks. In parallel, a growing body of literature shows that PTLMs fail inconsistently and non-intuitively, showing a lack of reasoning and grounding. To take a first step toward quantifying the effect of grounding (or lack thereof), we curate a novel and comprehensive dataset of object affordances -- GrAFFORD, characterized by 15 affordance classes. Unlike affordance datasets collected in vision and language domains, we annotate in-the-wild sentences with objects and affordances. Experimental results reveal that PTLMs exhibit limited reasoning abilities when it comes to uncommon object affordances. We also observe that pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#26816;&#27979;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#21644;&#34920;&#36798;&#20449;&#24515;&#26102;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#23384;&#22312;&#20004;&#20010;&#26497;&#31471;&#65306;&#23545;&#21487;&#33021;&#24341;&#36215;&#20844;&#24179;&#38382;&#39064;&#30340;&#32676;&#20307;&#25110;&#35805;&#39064;&#34920;&#29616;&#20986;&#36807;&#20110;&#25935;&#24863;&#65292;&#21516;&#26102;&#32622;&#20449;&#24230;&#35780;&#20998;&#36807;&#24230;&#38598;&#20013;&#22312;&#19968;&#20010;&#33539;&#22260;&#20869;&#12290;</title><link>https://arxiv.org/abs/2402.11406</link><description>&lt;p&gt;
&#19981;&#35201;&#36208;&#21521;&#26497;&#31471;&#65306;&#25581;&#31034;LLMs&#22312;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#36807;&#24230;&#25935;&#24863;&#24615;&#21644;&#26657;&#20934;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#26816;&#27979;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#21644;&#34920;&#36798;&#20449;&#24515;&#26102;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#23384;&#22312;&#20004;&#20010;&#26497;&#31471;&#65306;&#23545;&#21487;&#33021;&#24341;&#36215;&#20844;&#24179;&#38382;&#39064;&#30340;&#32676;&#20307;&#25110;&#35805;&#39064;&#34920;&#29616;&#20986;&#36807;&#20110;&#25935;&#24863;&#65292;&#21516;&#26102;&#32622;&#20449;&#24230;&#35780;&#20998;&#36807;&#24230;&#38598;&#20013;&#22312;&#19968;&#20010;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20844;&#24179;&#24615;&#21644;&#21487;&#20449;&#24230;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#65292;&#21033;&#29992;&#38388;&#25509;&#35821;&#35328;&#20256;&#36798;&#20167;&#24680;&#24847;&#22270;&#65292;&#21344;&#25454;&#23454;&#36341;&#20013;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;LLMs&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#23457;&#26597;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#26816;&#27979;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#65288;&#20998;&#31867;&#20219;&#21153;&#65289;&#20197;&#21450;&#23545;&#20854;&#21709;&#24212;&#30340;&#20449;&#24515;&#36827;&#34892;&#34920;&#36798;&#65288;&#26657;&#20934;&#20219;&#21153;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32454;&#33268;&#32771;&#34385;&#20102;&#21508;&#31181;&#25552;&#31034;&#27169;&#24335;&#21644;&#20027;&#27969;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;LLMs&#23637;&#31034;&#20102;&#20004;&#20010;&#26497;&#31471;&#65306;&#65288;1&#65289;LLMs&#23545;&#21487;&#33021;&#23548;&#33268;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#32676;&#20307;&#25110;&#35805;&#39064;&#26174;&#31034;&#20986;&#36807;&#24230;&#30340;&#25935;&#24863;&#24615;&#65292;&#23548;&#33268;&#23558;&#33391;&#24615;&#38472;&#36848;&#38169;&#35823;&#20998;&#31867;&#20026;&#20167;&#24680;&#35328;&#35770;&#12290; &#65288;2&#65289;LLMs&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#36807;&#24230;&#38598;&#20013;&#22312;&#19968;&#20010;&#22266;&#23450;&#33539;&#22260;&#19978;&#65292;&#26080;&#35770;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#22914;&#20309;&#20063;&#20445;&#25345;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11406v1 Announce Type: new  Abstract: The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;MANGO&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#27010;&#24565;&#21644;&#25991;&#21270;&#20004;&#20010;&#20837;&#21475;&#28857;&#35880;&#24910;&#32780;&#36845;&#20195;&#22320;&#25552;&#31034;LLMs&#65292;&#25552;&#28860;&#39640;&#20934;&#30830;&#24230;&#12289;&#39640;&#21484;&#22238;&#29575;&#30340;&#25991;&#21270;&#30693;&#35782;&#26029;&#35328;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#39640;&#20934;&#30830;&#24230;&#26029;&#35328;&#65292;&#33021;&#22815;&#25913;&#21892;&#23545;&#35805;&#31995;&#32479;&#22238;&#24212;&#30340;&#36136;&#37327;&#12289;&#29305;&#24322;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10689</link><description>&lt;p&gt;
&#22810;&#20803;&#25991;&#21270;&#24120;&#35782;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Cultural Commonsense Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;MANGO&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#27010;&#24565;&#21644;&#25991;&#21270;&#20004;&#20010;&#20837;&#21475;&#28857;&#35880;&#24910;&#32780;&#36845;&#20195;&#22320;&#25552;&#31034;LLMs&#65292;&#25552;&#28860;&#39640;&#20934;&#30830;&#24230;&#12289;&#39640;&#21484;&#22238;&#29575;&#30340;&#25991;&#21270;&#30693;&#35782;&#26029;&#35328;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#39640;&#20934;&#30830;&#24230;&#26029;&#35328;&#65292;&#33021;&#22815;&#25913;&#21892;&#23545;&#35805;&#31995;&#32479;&#22238;&#24212;&#30340;&#36136;&#37327;&#12289;&#29305;&#24322;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20173;&#28982;&#38754;&#20020;&#30528;&#36866;&#24403;&#24212;&#23545;&#31038;&#20250;&#21644;&#25991;&#21270;&#24815;&#20363;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MANGO&#65292;&#19968;&#31181;&#29992;&#20110;&#25552;&#28860;&#39640;&#20934;&#30830;&#24230;&#12289;&#39640;&#21484;&#22238;&#29575;&#25991;&#21270;&#30693;&#35782;&#26029;&#35328;&#30340;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#20174;&#27010;&#24565;&#21644;&#25991;&#21270;&#20004;&#20010;&#20837;&#21475;&#28857;&#35880;&#24910;&#32780;&#36845;&#20195;&#22320;&#25552;&#31034;LLMs&#36827;&#34892;&#36825;&#19968;&#30446;&#30340;&#12290;&#36890;&#36807;&#32858;&#31867;&#21644;&#29983;&#25104;&#25688;&#35201;&#23558;&#36755;&#20986;&#32467;&#26524;&#24041;&#22266;&#12290;&#36816;&#34892;MANGO&#26041;&#27861;&#65292;&#20197;GPT-3.5&#20316;&#20026;&#24213;&#23618;LLM&#65292;&#20026;30K&#20010;&#27010;&#24565;&#21644;11K&#20010;&#25991;&#21270;&#25552;&#20379;&#20102;167K&#20010;&#39640;&#20934;&#30830;&#24230;&#26029;&#35328;&#65292;&#22823;&#24133;&#36229;&#36807;&#20808;&#21069;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#22806;&#37096;&#35780;&#20272;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#23545;&#35805;&#31995;&#32479;&#19982;&#25991;&#21270;&#30693;&#35782;&#26029;&#35328;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#28155;&#21152;&#26469;&#33258;MANGO&#30340;&#30693;&#35782;&#21487;&#20197;&#25552;&#21319;&#23545;&#35805;&#22238;&#24212;&#30340;&#25972;&#20307;&#36136;&#37327;&#12289;&#29305;&#24322;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#65292;&#36825;&#26159;&#30001;&#20154;&#31867;&#26631;&#27880;&#32773;&#35780;&#21028;&#30340;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#20379;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10689v1 Announce Type: new  Abstract: Despite recent progress, large language models (LLMs) still face the challenge of appropriately reacting to the intricacies of social and cultural conventions. This paper presents MANGO, a methodology for distilling high-accuracy, high-recall assertions of cultural knowledge. We judiciously and iteratively prompt LLMs for this purpose from two entry points, concepts and cultures. Outputs are consolidated via clustering and generative summarization. Running the MANGO method with GPT-3.5 as underlying LLM yields 167K high-accuracy assertions for 30K concepts and 11K cultures, surpassing prior resources by a large margin. For extrinsic evaluation, we explore augmenting dialogue systems with cultural knowledge assertions. We find that adding knowledge from MANGO improves the overall quality, specificity, and cultural sensitivity of dialogue responses, as judged by human annotators. Data and code are available for download.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07204</link><description>&lt;p&gt;
&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#22478;&#24066;&#34892;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#12290;OUIP&#19982;&#20256;&#32479;&#34892;&#31243;&#35268;&#21010;&#19981;&#21516;&#65292;&#20256;&#32479;&#35268;&#21010;&#38480;&#21046;&#20102;&#29992;&#25143;&#34920;&#36798;&#26356;&#35814;&#32454;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#23454;&#26102;&#20449;&#24687;&#12289;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#21644;&#19981;&#36275;&#30340;&#31354;&#38388;&#24847;&#35782;&#65292;&#23427;&#20204;&#26080;&#27861;&#29420;&#31435;&#22320;&#25552;&#20379;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ItiNera&#30340;OUIP&#31995;&#32479;&#65292;&#23558;&#31354;&#38388;&#20248;&#21270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#26356;&#26032;&#20852;&#36259;&#28857;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#28857;&#25968;&#25454;&#24211;&#12290;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#35831;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#21327;&#21516;&#23454;&#29616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#37096;&#20998;&#25490;&#24207;&#26469;&#20248;&#21270;LLMs&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#29305;&#23450;&#20219;&#21153;&#20505;&#36873;&#21709;&#24212;&#27744;&#20013;&#30340;&#26368;&#20339;&#21709;&#24212;&#65292;&#20174;&#32780;&#25913;&#21892;&#21709;&#24212;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.09136</link><description>&lt;p&gt;
&#29992;&#37096;&#20998;&#25490;&#24207;&#23545;LLM&#21709;&#24212;&#36827;&#34892;&#25490;&#21517;&#20197;&#25913;&#21892;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#37096;&#20998;&#25490;&#24207;&#26469;&#20248;&#21270;LLMs&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#29305;&#23450;&#20219;&#21153;&#20505;&#36873;&#21709;&#24212;&#27744;&#20013;&#30340;&#26368;&#20339;&#21709;&#24212;&#65292;&#20174;&#32780;&#25913;&#21892;&#21709;&#24212;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#21046;LLMs&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#28041;&#21450;&#23558;&#26377;&#25928;&#21709;&#24212;&#19982;&#38169;&#35823;&#21709;&#24212;&#21306;&#20998;&#24320;&#12290;&#36825;&#31181;&#25216;&#33021;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#26469;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#26469;&#35828;&#65292;&#33719;&#21462;&#19987;&#23478;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25490;&#21517;&#24230;&#37327;&#26469;&#20248;&#21270;LLMs&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#20026;&#29305;&#23450;&#20219;&#21153;&#21019;&#24314;&#30340;&#20505;&#36873;&#21709;&#24212;&#27744;&#20013;&#30340;&#26368;&#20339;&#21709;&#24212;&#12290;&#25105;&#20204;&#20027;&#24352;&#37319;&#29992;&#37096;&#20998;&#25490;&#24207;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#23436;&#20840;&#25490;&#24207;&#65292;&#22240;&#20026;&#23601;&#20505;&#36873;&#21709;&#24212;&#30340;&#23436;&#32654;&#39034;&#24207;&#36798;&#25104;&#20849;&#35782;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#37096;&#20998;&#25490;&#24207;&#26356;&#21152;&#31283;&#20581;&#65292;&#23545;&#22122;&#22768;&#30340;&#25935;&#24863;&#24615;&#36739;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26377;&#38480;&#30340;&#20154;&#31867;&#27880;&#37322;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#25913;&#36827;&#21709;&#24212;&#29983;&#25104;&#33021;&#21147;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09136v2 Announce Type: replace  Abstract: Customizing LLMs for a specific task involves distinguishing effective responses from erroneous ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining expert-annotated preference data is expensive for most tasks. In this paper, we present a novel method to optimize LLMs using ranking metrics. This method trains the model to prioritize the best responses from a pool of candidates created for a particular task. Rather than a traditional full ordering, we advocate for a partial ordering, as achieving consensus on the perfect order of candidate responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be achieved with limited human annotations or through heuristic methods. We test our system's improved response generation ability using benchmark datasets, including the latest multi-document question answering task. We conduct ablati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#22810;&#39046;&#22495;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26102;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#27969;&#27700;&#32447;&#25110;&#26631;&#31614;&#65292;&#21516;&#26102;&#20445;&#25345;&#25104;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2305.14336</link><description>&lt;p&gt;
&#26469;&#33258;&#24322;&#26500;&#34920;&#26684;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Schema-Driven Information Extraction from Heterogeneous Tables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#22810;&#39046;&#22495;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26102;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#27969;&#27700;&#32447;&#25110;&#26631;&#31614;&#65292;&#21516;&#26102;&#20445;&#25345;&#25104;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25903;&#25345;&#39640;&#25928;&#22320;&#20174;&#34920;&#26684;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#36825;&#26159;&#19968;&#39033;&#23558;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#25353;&#29031;&#20154;&#31867;&#32534;&#20889;&#30340;&#27169;&#24335;&#32452;&#32455;&#30340;&#35760;&#24405;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#35780;&#20272;&#21508;&#31181;LLM&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#34920;&#26684;&#65306;&#26426;&#22120;&#23398;&#20064;&#35770;&#25991;&#12289;&#21270;&#23398;&#25991;&#29486;&#12289;&#26448;&#26009;&#31185;&#23398;&#26399;&#21002;&#21644;&#32593;&#39029;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#34920;&#26684;&#38598;&#21512;&#26469;&#35780;&#20272;&#24320;&#28304;&#21644;&#22522;&#20110;API&#30340;&#35821;&#35328;&#27169;&#22411;&#20174;&#28085;&#30422;&#22810;&#31181;&#39046;&#22495;&#21644;&#25968;&#25454;&#26684;&#24335;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#30340;&#27969;&#27700;&#32447;&#25110;&#26631;&#31614;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#20986;&#20154;&#24847;&#26009;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;F1&#20998;&#25968;&#33539;&#22260;&#20174;74.2&#21040;96.1&#65292;&#21516;&#26102;&#20445;&#25345;&#25104;&#26412;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14336v3 Announce Type: replace  Abstract: In this paper, we explore the question of whether large language models can support cost-efficient information extraction from tables. We introduce schema-driven information extraction, a new task that transforms tabular data into structured records following a human-authored schema. To assess various LLM's capabilities on this task, we present a benchmark comprised of tables from four diverse domains: machine learning papers, chemistry literature, material science journals, and webpages. We use this collection of annotated tables to evaluate the ability of open-source and API-based language models to extract information from tables covering diverse domains and data formats. Our experiments demonstrate that surprisingly competitive performance can be achieved without requiring task-specific pipelines or labels, achieving F1 scores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover, through detailed ablation studie
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31185;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;LLMs&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14656</link><description>&lt;p&gt;
&#31185;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Scientific Large Language Models: A Survey on Biological &amp; Chemical Domains. (arXiv:2401.14656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14656
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31185;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;LLMs&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#19968;&#31181;&#21464;&#38761;&#24615;&#21147;&#37327;&#65292;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;LLMs&#30340;&#24212;&#29992;&#24050;&#36229;&#36234;&#20256;&#32479;&#30340;&#35821;&#35328;&#30028;&#38480;&#65292;&#21253;&#25324;&#22312;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20869;&#24320;&#21457;&#30340;&#19987;&#38376;&#35821;&#35328;&#31995;&#32479;&#12290;&#36825;&#31181;&#19981;&#26029;&#22686;&#38271;&#30340;&#20852;&#36259;&#23548;&#33268;&#20102;&#31185;&#23398;LLMs&#30340;&#20986;&#29616;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#23376;&#31867;&#12290;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#31038;&#21306;&#20013;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#31185;&#23398;LLMs&#20540;&#24471;&#20840;&#38754;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#31995;&#32479;&#19988;&#26368;&#26032;&#30340;&#32508;&#36848;&#20171;&#32461;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31995;&#32479;&#22320;&#21246;&#30011;&#8220;&#31185;&#23398;&#35821;&#35328;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#20840;&#38754;&#23457;&#26597;&#31185;&#23398;LLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#32771;&#34385;&#21040;&#31185;&#23398;&#23398;&#31185;&#30340;&#24191;&#27867;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#37319;&#29992;&#20102;&#19968;&#31181;&#32858;&#28966;&#30340;&#35270;&#35282;&#65292;&#19987;&#27880;&#20110;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this paper, we endeavor to methodically delineate the concept of "scientific language", whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical dom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13782</link><description>&lt;p&gt;
&#20174;&#25512;&#29305;&#21040;&#24341;&#29992;&#65306;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#35265;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#34987;&#25509;&#21463;&#30340;&#35770;&#25991;&#25968;&#37327;&#36798;&#21040;&#25968;&#21315;&#31687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#33719;&#21462;&#21644;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#21464;&#24471;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21487;&#35265;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#20998;&#20139;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#25324;8000&#22810;&#31687;&#35770;&#25991;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;2018&#24180;12&#26376;&#33267;2023&#24180;10&#26376;&#30340;&#25512;&#29305;&#65292;&#20197;&#21450;&#22522;&#20110;&#20986;&#29256;&#24180;&#20221;&#12289;&#20250;&#35758;&#22320;&#28857;&#21644;&#25688;&#35201;&#20027;&#39064;&#36827;&#34892;1&#65306;1&#21305;&#37197;&#30340;&#23545;&#29031;&#32452;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#23398;&#26415;&#20132;&#27969;&#20013;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#19981;&#26029;&#21457;&#23637;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.10711</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#26088;&#22312;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#39057;&#20449;&#24687;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#23613;&#31649;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#22270;&#20687;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#65292;&#20165;&#20165;&#26159;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#38024;&#23545;&#38382;&#39064;&#20851;&#38190;&#26102;&#38388;&#25139;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#24378;&#21046;LMMs&#20351;&#29992;&#38382;&#39064;&#20851;&#38190;&#26102;&#21051;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#21512;&#24182;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#20197;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#36825;&#20123;&#26102;&#21051;&#23558;&#20316;&#20026;&#20266;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20266;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#24369;&#30417;&#30563;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#12290;GCG&#23398;&#20064;&#22810;&#20010;&#39640;&#26031;&#20989;&#25968;&#26469;&#25551;&#36848;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08320</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#25216;&#26415;&#20445;&#25252;&#25105;&#20204;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#26410;&#32463;&#31579;&#36873;&#12289;&#24120;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#32593;&#39029;&#25968;&#25454;&#35757;&#32451;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#31169;&#38382;&#39064;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#29305;&#23450;&#20449;&#24687;&#26159;&#19968;&#20010;&#19981;&#23481;&#26131;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#31169;&#20154;&#20449;&#24687;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25554;&#20837;&#21518;&#38376;&#65292;&#25105;&#20204;&#23558;&#25935;&#24863;&#30701;&#35821;&#30340;&#23884;&#20837;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#20363;&#22914;&#29992;"a person"&#20195;&#26367;&#20154;&#21517;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#36890;&#36807;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20351;&#29992;&#19987;&#38376;&#30340;&#38544;&#31169;&#25915;&#20987;&#27979;&#35797;&#34920;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#21518;&#38376;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;"&#21452;&#37325;&#29992;&#36884;"&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
&lt;/p&gt;</description></item></channel></rss>