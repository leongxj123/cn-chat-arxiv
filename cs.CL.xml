<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#19987;&#27880;&#20110;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#25152;&#38656;&#30149;&#29702;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#19987;&#23478;&#27880;&#37322;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#20934;&#25991;&#26412;&#29305;&#24449;&#36716;&#25442;&#22120;&#27169;&#22359;&#12290;</title><link>https://arxiv.org/abs/2404.00226</link><description>&lt;p&gt;
&#24819;&#35201;&#30340;&#35774;&#35745;&#65306;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#19987;&#27880;&#20110;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#25152;&#38656;&#30149;&#29702;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#19987;&#23478;&#27880;&#37322;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#20934;&#25991;&#26412;&#29305;&#24449;&#36716;&#25442;&#22120;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#22312;&#21307;&#30103;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#65292;&#20174;&#25104;&#23545;&#30340;&#21307;&#30103;&#25253;&#21578;&#20013;&#23398;&#20064;&#21307;&#23398;&#35270;&#35273;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#39044;&#35757;&#32451;&#20219;&#21153;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#39069;&#22806;&#30340;&#27880;&#37322;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#26410;&#33021;&#26126;&#30830;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;&#30149;&#29702;&#29305;&#24449;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#22242;&#38431;&#65292;&#20197;&#24341;&#23548;&#26694;&#26550;&#19987;&#27880;&#20110;&#30446;&#26631;&#30149;&#29702;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21307;&#30103;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#35774;&#35745;&#20102;&#19982;&#19981;&#21516;&#30142;&#30149;&#30456;&#20851;&#30340;&#22810;&#31890;&#24230;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#36825;&#26377;&#21161;&#20110;&#26694;&#26550;&#22312;&#39044;&#35757;&#32451;&#20013;&#26080;&#38656;&#19987;&#23478;&#39069;&#22806;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#25991;&#26412;&#29305;&#24449;&#36716;&#25442;&#22120;&#27169;&#22359;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#36716;&#25442;&#21040;&#25509;&#36817;&#25991;&#26412;&#39046;&#22495;&#30340;&#20934;&#25991;&#26412;&#31354;&#38388;&#26469;&#36741;&#21161;&#39044;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00226v1 Announce Type: cross  Abstract: Multimodal pre-training demonstrates its potential in the medical domain, which learns medical visual representations from paired medical reports. However, many pre-training tasks require extra annotations from clinicians, and most of them fail to explicitly guide the model to learn the desired features of different pathologies. To the best of our knowledge, we are the first to utilize Visual Question Answering (VQA) for multimodal pre-training to guide the framework focusing on targeted pathological features. In this work, we leverage descriptions in medical reports to design multi-granular question-answer pairs associated with different diseases, which assist the framework in pre-training without requiring extra annotations from experts. We also propose a novel pre-training framework with a quasi-textual feature transformer, a module designed to transform visual features into a quasi-textual space closer to the textual domain via a c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#20013;&#23545;&#19981;&#21512;&#29702;&#24615;&#30340;&#21453;&#24212;&#65292;&#35774;&#35745;&#20102;&#19981;&#21512;&#29702;&#25968;&#23398;&#38382;&#39064;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#35745;&#31639;&#21644;&#32467;&#35770;&#25552;&#31034;&#27169;&#26495;&#65292;&#25552;&#21319;&#20102;&#23427;&#20204;&#22312;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19346</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#23545;&#19981;&#21512;&#29702;&#24615;&#27627;&#26080;&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Unconscious of Unreasonability in Math Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#20013;&#23545;&#19981;&#21512;&#29702;&#24615;&#30340;&#21453;&#24212;&#65292;&#35774;&#35745;&#20102;&#19981;&#21512;&#29702;&#25968;&#23398;&#38382;&#39064;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#35745;&#31639;&#21644;&#32467;&#35770;&#25552;&#31034;&#27169;&#26495;&#65292;&#25552;&#21319;&#20102;&#23427;&#20204;&#22312;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#24040;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#32473;&#20986;&#21253;&#21547;&#19981;&#21512;&#29702;&#38169;&#35823;&#30340;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#38754;&#23545;&#19981;&#21512;&#29702;&#25968;&#23398;&#38382;&#39064;&#26102;&#30340;&#34892;&#20026;&#65292;&#24182;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#23427;&#20204;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19981;&#21512;&#29702;&#25968;&#23398;&#38382;&#39064;(UMP)&#22522;&#20934;&#26469;&#26816;&#26597;LLMs&#30340;&#38169;&#35823;&#26816;&#27979;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#26816;&#27979;&#21040;&#19981;&#21512;&#29702;&#38169;&#35823;&#65292;&#20294;&#20173;&#28982;&#22312;&#29983;&#25104;&#38750;&#24187;&#35273;&#20869;&#23481;&#26041;&#38754;&#22833;&#36133;&#12290;&#20026;&#20102;&#25913;&#21892;&#23427;&#20204;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#20851;&#38190;&#35745;&#31639;&#21644;&#32467;&#35770;(CCC)&#30340;&#25112;&#30053;&#25552;&#31034;&#27169;&#26495;&#12290;&#36890;&#36807;CCC&#65292;LLMs&#21487;&#20197;&#26356;&#22909;&#22320;&#33258;&#25105;&#35780;&#20272;&#24182;&#26816;&#27979;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#19981;&#21512;&#29702;&#38169;&#35823;&#65292;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#26356;&#21487;&#38752;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19346v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors. In this paper, we study the behavior of LLMs when faced with unreasonable math problems and further explore their potential to address these problems. First, we construct the Unreasonable Math Problem (UMP) benchmark to examine the error detection ability of LLMs. Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content. In order to improve their ability of error detection and correction, we further design a strategic prompt template called Critical Calculation and Conclusion(CCC). With CCC, LLMs can better self-evaluate and detect unreasonable errors in math questions, making them more reliable and safe in practical application scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#21270;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#25552;&#31034;&#30340;&#23384;&#22312;&#24418;&#24335;&#65292;&#24182;&#34913;&#37327;&#20102;&#23427;&#20204;&#30340;&#36234;&#29425;&#28508;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35821;&#20041;&#19978;&#20855;&#26377;&#24847;&#20041;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#23041;&#32961;&#26684;&#23616;&#12290;</title><link>https://arxiv.org/abs/2403.17336</link><description>&lt;p&gt;
&#19981;&#35201;&#21548;&#25105;&#30340;&#35805;&#65306;&#29702;&#35299;&#21644;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17336
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#21270;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#25552;&#31034;&#30340;&#23384;&#22312;&#24418;&#24335;&#65292;&#24182;&#34913;&#37327;&#20102;&#23427;&#20204;&#30340;&#36234;&#29425;&#28508;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35821;&#20041;&#19978;&#20855;&#26377;&#24847;&#20041;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#23041;&#32961;&#26684;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26080;&#22788;&#19981;&#22312;&#22320;&#34987;&#35775;&#38382;&#12290;&#20973;&#20511;&#20854;&#20986;&#33394;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#27169;&#22411;&#27491;&#26085;&#30410;&#34701;&#20837;&#25105;&#20204;&#30340;&#31038;&#20250;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#20204;&#20063;&#23545;&#36825;&#31181;&#24378;&#22823;&#25216;&#26415;&#30340;&#28508;&#22312;&#28389;&#29992;&#34920;&#31034;&#25285;&#24551;&#65292;&#24182;&#20419;&#20351;&#26381;&#21153;&#25552;&#20379;&#21830;&#37319;&#21462;&#20102;&#38450;&#24481;&#25514;&#26045;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#20445;&#25252;&#26426;&#21046;&#65292;&#36234;&#29425;&#25552;&#31034;&#26368;&#36817;&#24050;&#25104;&#20026;&#35268;&#36991;&#23433;&#20840;&#38480;&#21046;&#21644;&#24341;&#35825;&#26368;&#21021;&#35774;&#35745;&#20026;&#34987;&#31105;&#27490;&#30340;&#26377;&#23475;&#20869;&#23481;&#30340;&#26368;&#26377;&#25928;&#26426;&#21046;&#20043;&#19968;&#12290;&#30001;&#20110;LLM&#30340;&#24555;&#36895;&#21457;&#23637;&#21450;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#36731;&#26494;&#33719;&#21462;&#30340;&#20415;&#21033;&#24615;&#65292;&#36234;&#29425;&#25552;&#31034;&#30340;&#21069;&#27839;&#20027;&#35201;&#20986;&#29616;&#22312;&#22312;&#32447;&#35770;&#22363;&#21644;&#29233;&#22909;&#32773;&#20013;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#35821;&#20041;&#19978;&#20855;&#26377;&#24847;&#20041;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#23041;&#32961;&#26684;&#23616;&#65292;&#25105;&#20204;&#31995;&#32479;&#21270;&#20102;&#29616;&#26377;&#25552;&#31034;&#24182;&#27979;&#37327;&#23427;&#20204;&#30340;&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17336v1 Announce Type: cross  Abstract: Recent advancements in generative AI have enabled ubiquitous access to large language models (LLMs). Empowered by their exceptional capabilities to understand and generate human-like text, these models are being increasingly integrated into our society. At the same time, there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers. To overcome such protection, jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited.   Due to the rapid development of LLMs and their ease of access via natural languages, the frontline of jailbreak prompts is largely seen in online forums and among hobbyists. To gain a better understanding of the threat landscape of semantically meaningful jailbreak prompts, we systemized existing prompts and measured their jailbre
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21463;&#28508;&#22312;&#23545;&#35805;&#32467;&#26524;&#38480;&#21046;&#30340;&#23545;&#35805;&#65292;&#20197;&#24212;&#23545;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#65292;&#36890;&#36807;&#26500;&#24314;&#23545;&#35805;&#32467;&#26524;&#20998;&#31867;&#22120;&#21644;&#25552;&#20986;&#25972;&#21512;&#26041;&#27861;&#65292;&#20026;&#22312;&#32447;&#29615;&#22659;&#20013;&#29983;&#25104;&#23545;&#25239;&#24615;&#23545;&#35805;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;</title><link>https://arxiv.org/abs/2403.17146</link><description>&lt;p&gt;
&#29992;&#20110;&#25269;&#21046;&#20167;&#24680;&#35328;&#35770;&#30340;&#32467;&#26524;&#21463;&#38480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Outcome-Constrained Large Language Models for Countering Hate Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21463;&#28508;&#22312;&#23545;&#35805;&#32467;&#26524;&#38480;&#21046;&#30340;&#23545;&#35805;&#65292;&#20197;&#24212;&#23545;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#65292;&#36890;&#36807;&#26500;&#24314;&#23545;&#35805;&#32467;&#26524;&#20998;&#31867;&#22120;&#21644;&#25552;&#20986;&#25972;&#21512;&#26041;&#27861;&#65292;&#20026;&#22312;&#32447;&#29615;&#22659;&#20013;&#29983;&#25104;&#23545;&#25239;&#24615;&#23545;&#35805;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#25110;&#22238;&#24212;&#20167;&#24680;&#35328;&#35770;&#30340;&#23545;&#35805;&#34987;&#35270;&#20026;&#32531;&#35299;&#20167;&#24680;&#35328;&#35770;&#30340;&#36127;&#38754;&#24433;&#21709;&#24182;&#20419;&#36827;&#22312;&#32447;&#20132;&#27969;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#30740;&#31350;&#24050;&#33268;&#21147;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#23545;&#35805;&#20197;&#21327;&#21161;&#25171;&#20987;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#12290;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35821;&#35328;&#23646;&#24615;&#65288;&#22914;&#31036;&#35980;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#24847;&#22270;&#39537;&#21160;&#65289;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#23545;&#35805;&#21487;&#33021;&#23545;&#22312;&#32447;&#29615;&#22659;&#20135;&#29983;&#20160;&#20040;&#24433;&#21709;&#20173;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#21463;&#28508;&#22312;&#23545;&#35805;&#32467;&#26524;&#38480;&#21046;&#30340;&#23545;&#35805;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#23545;&#35805;&#32467;&#26524;&#20998;&#31867;&#22120;&#65292;&#29992;Reddit&#25968;&#25454;&#39044;&#27979;&#24212;&#23545;&#20167;&#24680;&#35328;&#35770;&#21518;&#30340;&#19981;&#25991;&#26126;&#31243;&#24230;&#21644;&#20167;&#24680;&#32773;&#37325;&#26032;&#20851;&#27880;&#34892;&#20026;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#22235;&#31181;&#26041;&#27861;&#26469;&#25972;&#21512;&#25152;&#38656;&#30340;&#32467;&#26524;&#65292;&#21363;&#20302;&#31036;&#35980;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17146v1 Announce Type: new  Abstract: Counterspeech that challenges or responds to hate speech has been seen as an alternative to mitigate the negative impact of hate speech and foster productive online communications. Research endeavors have been directed to using language models for the automatic generation of counterspeech to assist efforts in combating online hate. Existing research focuses on the generation of counterspeech with certain linguistic attributes, such as being polite, informative, and intent-driven. However, it remains unclear what impact the counterspeech might have in an online environment. We first explore methods that utilize large language models (LLM) to generate counterspeech constrained by potential conversation outcomes. We build two conversation outcome classifiers that predict the incivility level and the hater reentry behavior following replies to hate with Reddit data, then propose four methods to incorporate the desired outcomes, i.e., low con
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11807</link><description>&lt;p&gt;
LLM&#30340;&#20915;&#31574;&#27700;&#24179;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#31350;&#31455;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#21508;&#31181;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26497;&#22909;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35270;&#35282;&#25506;&#31350;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25903;&#25345;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#21442;&#19982;&#30340;&#28216;&#25103;&#65292;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;GAMA-Bench&#65292;&#21253;&#25324;&#20843;&#20010;&#32463;&#20856;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20998;&#26041;&#26696;&#65292;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;GAMA-Bench&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#31283;&#20581;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#22686;&#24378;&#31574;&#30053;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;GPT-3.5&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#20854;&#27867;&#21270;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19968;&#20123;&#26041;&#27861;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11807v1 Announce Type: new  Abstract: Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other mod
&lt;/p&gt;</description></item><item><title>GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05527</link><description>&lt;p&gt;
GEAR: &#19968;&#31181;&#29992;&#20110;&#20960;&#20046;&#26080;&#25439;&#29983;&#25104;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05527
&lt;/p&gt;
&lt;p&gt;
GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#24555;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#26029;&#29983;&#25104;&#36895;&#24230;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#38271;&#30340;&#32531;&#23384;&#38656;&#27714;&#24050;&#23558;LLM&#25512;&#26029;&#36716;&#21464;&#20026;&#19968;&#20010;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#65292;&#26174;&#33879;&#22320;&#38480;&#21046;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#26631;&#35760;&#25110;&#22343;&#21248;&#37327;&#21270;&#25152;&#26377;&#26465;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#30697;&#38453;&#12290;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27599;&#20010;&#27493;&#39588;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#37325;&#22823;&#20559;&#24046;&#21644;&#24615;&#33021;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GEAR&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.04732</link><description>&lt;p&gt;
&#25105;&#20204;&#36317;&#31163;&#26234;&#33021;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#36824;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We from Intelligent Visual Deductive Reasoning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04732
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;GPT-4V&#20043;&#31867;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#22797;&#26434;&#20294;&#19981;&#22826;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#20102;&#24403;&#21069;&#39046;&#20808;&#30340;VLM&#20013;&#20197;&#21069;&#26410;&#26292;&#38706;&#30340;&#30450;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29790;&#25991;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#26469;&#35780;&#20272;VLM&#22312;&#20165;&#20381;&#38752;&#35270;&#35273;&#32447;&#32034;&#36827;&#34892;&#22810;&#36339;&#20851;&#31995;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#27969;&#34892;&#30340;VLM&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#37319;&#29992;&#20102;&#26631;&#20934;&#31574;&#30053;&#65292;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#33258;&#25105;&#19968;&#33268;&#24615;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;Mensa&#26234;&#21830;&#27979;&#35797;&#12289;&#26234;&#21830;&#27979;&#35797;&#21644;RAVEN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLM&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04732v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective
&lt;/p&gt;</description></item><item><title>DiffuCOMET&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#23398;&#20064;&#26469;&#37325;&#26500;&#21465;&#36848;&#19978;&#19979;&#25991;&#19982;&#30456;&#20851;&#24120;&#35782;&#30693;&#35782;&#20043;&#38388;&#35821;&#20041;&#36830;&#25509;&#30340;&#30693;&#35782;&#27169;&#22411;&#65292;&#29983;&#25104;&#30340;&#30693;&#35782;&#22312;&#24120;&#35782;&#22810;&#26679;&#24615;&#12289;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#21644;&#23545;&#24050;&#30693;&#21442;&#32771;&#25991;&#29486;&#30340;&#23545;&#40784;&#26041;&#38754;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.17011</link><description>&lt;p&gt;
DiffuCOMET: &#19978;&#19979;&#25991;&#24120;&#35782;&#30693;&#35782;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
DiffuCOMET: Contextual Commonsense Knowledge Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17011
&lt;/p&gt;
&lt;p&gt;
DiffuCOMET&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#23398;&#20064;&#26469;&#37325;&#26500;&#21465;&#36848;&#19978;&#19979;&#25991;&#19982;&#30456;&#20851;&#24120;&#35782;&#30693;&#35782;&#20043;&#38388;&#35821;&#20041;&#36830;&#25509;&#30340;&#30693;&#35782;&#27169;&#22411;&#65292;&#29983;&#25104;&#30340;&#30693;&#35782;&#22312;&#24120;&#35782;&#22810;&#26679;&#24615;&#12289;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#21644;&#23545;&#24050;&#30693;&#21442;&#32771;&#25991;&#29486;&#30340;&#23545;&#40784;&#26041;&#38754;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#19978;&#19979;&#25991;&#30456;&#20851;&#19988;&#22810;&#26679;&#21270;&#30340;&#24120;&#35782;&#20197;&#29702;&#35299;&#21465;&#36848;&#25925;&#20107;&#23545;&#20110;&#30693;&#35782;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#21033;&#29992;&#25193;&#25955;&#30340;&#30693;&#35782;&#27169;&#22411;DiffuCOMET&#65292;&#20197;&#23398;&#20064;&#37325;&#26500;&#21465;&#36848;&#19978;&#19979;&#25991;&#19982;&#30456;&#20851;&#24120;&#35782;&#30693;&#35782;&#20043;&#38388;&#30340;&#38544;&#24335;&#35821;&#20041;&#36830;&#25509;&#12290;&#36890;&#36807;&#22810;&#27425;&#25193;&#25955;&#27493;&#39588;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#23436;&#21892;&#20102;&#19982;&#21465;&#36848;&#38170;&#23450;&#30340;&#24120;&#35782;&#20107;&#23454;&#34920;&#31034;&#65292;&#20026;&#36755;&#20837;&#19978;&#19979;&#25991;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#19988;&#22810;&#26679;&#21270;&#30340;&#24120;&#35782;&#25512;&#26029;&#12290;&#20026;&#20102;&#35780;&#20272;DiffuCOMET&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#34913;&#37327;&#24120;&#35782;&#25512;&#26029;&#30340;&#26032;&#25351;&#26631;&#65292;&#26356;&#23494;&#20999;&#22320;&#34913;&#37327;&#30693;&#35782;&#22810;&#26679;&#24615;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;ComFact&#21644;WebNLG+&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;DiffuCOMET&#29983;&#25104;&#30340;&#30693;&#35782;&#22312;&#24120;&#35782;&#22810;&#26679;&#24615;&#12289;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#20197;&#21450;&#19982;&#24050;&#30693;&#40644;&#37329;&#21442;&#32771;&#25991;&#29486;&#30340;&#23545;&#40784;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26435;&#34913;&#65292;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17011v1 Announce Type: new  Abstract: Inferring contextually-relevant and diverse commonsense to understand narratives remains challenging for knowledge models. In this work, we develop a series of knowledge models, DiffuCOMET, that leverage diffusion to learn to reconstruct the implicit semantic connections between narrative contexts and relevant commonsense knowledge. Across multiple diffusion steps, our method progressively refines a representation of commonsense facts that is anchored to a narrative, producing contextually-relevant and diverse commonsense inferences for an input context. To evaluate DiffuCOMET, we introduce new metrics for commonsense inference that more closely measure knowledge diversity and contextual relevance. Our results on two different benchmarks, ComFact and WebNLG+, show that knowledge generated by DiffuCOMET achieves a better trade-off between commonsense diversity, contextual relevance and alignment to known gold references, compared to basel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25193;&#23637;&#20102;&#23618;&#29702;&#35770;&#27169;&#22411;&#20174;&#35789;&#27719;&#27495;&#20041;&#21040;&#31687;&#31456;&#27495;&#20041;&#65292;&#36890;&#36807;&#35745;&#31639;&#26032;&#30340;&#19978;&#19979;&#25991;&#24615;&#24230;&#37327;&#65292;&#21457;&#29616;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#27604;&#20363;&#22823;&#24133;&#22686;&#21152;&#65292;&#24182;&#36890;&#36807;&#23558;Winograd Schema&#24314;&#27169;&#20026;Bell-CHSH&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#23618;&#29702;&#35770;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#25351;&#20195;&#27495;&#20041;&#30340;&#33258;&#28982;&#35821;&#35328;&#25361;&#25112;&#19978;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04505</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#27495;&#20041;&#30340;&#23618;&#29702;&#35770;&#27169;&#22411;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Developments in Sheaf-Theoretic Models of Natural Language Ambiguities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25193;&#23637;&#20102;&#23618;&#29702;&#35770;&#27169;&#22411;&#20174;&#35789;&#27719;&#27495;&#20041;&#21040;&#31687;&#31456;&#27495;&#20041;&#65292;&#36890;&#36807;&#35745;&#31639;&#26032;&#30340;&#19978;&#19979;&#25991;&#24615;&#24230;&#37327;&#65292;&#21457;&#29616;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#27604;&#20363;&#22823;&#24133;&#22686;&#21152;&#65292;&#24182;&#36890;&#36807;&#23558;Winograd Schema&#24314;&#27169;&#20026;Bell-CHSH&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#23618;&#29702;&#35770;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#25351;&#20195;&#27495;&#20041;&#30340;&#33258;&#28982;&#35821;&#35328;&#25361;&#25112;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#26159;&#25968;&#23398;&#23545;&#35937;&#65292;&#30001;&#22522;&#30784;&#26500;&#25104;&#30340;&#25299;&#25169;&#31354;&#38388;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#25968;&#25454;&#32452;&#25104;&#65292;&#20363;&#22914;&#23450;&#20041;&#22312;&#24320;&#38598;&#19978;&#30340;&#36830;&#32493;&#20989;&#25968;&#12290;&#23618;&#26368;&#21021;&#29992;&#20110;&#20195;&#25968;&#25299;&#25169;&#21644;&#36923;&#36753;&#20013;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#20063;&#29992;&#20110;&#24314;&#27169;&#29289;&#29702;&#23454;&#39564;&#21644;&#33258;&#28982;&#35821;&#35328;&#28040;&#23696;&#36807;&#31243;&#31561;&#20107;&#20214;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#20174;&#35789;&#27719;&#27495;&#20041;&#25193;&#23637;&#21040;&#30001;&#25351;&#20195;&#20135;&#29983;&#30340;&#31687;&#31456;&#27495;&#20041;&#12290;&#39318;&#20808;&#65292;&#23545;&#19968;&#32452;&#22522;&#26412;&#30340;&#25351;&#20195;&#31687;&#31456;&#25968;&#25454;&#35745;&#31639;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#19979;&#25991;&#24615;&#24230;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#27604;&#20363;&#26356;&#39640;&#65292;&#20026;82.9%&#65292;&#32780;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#26377;3.17%&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#21253;&#21547;&#25351;&#20195;&#27495;&#20041;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25361;&#25112;&#8212;&#8212;Winograd Schema&#24314;&#27169;&#20026;Bell-CHSH&#22330;&#26223;&#65292;&#20854;&#19978;&#19979;&#25991;&#27604;&#20363;&#20026;0.096&#12290;
&lt;/p&gt;
&lt;p&gt;
Sheaves are mathematical objects consisting of a base which constitutes a topological space and the data associated with each open set thereof, e.g. continuous functions defined on the open sets. Sheaves have originally been used in algebraic topology and logic. Recently, they have also modelled events such as physical experiments and natural language disambiguation processes. We extend the latter models from lexical ambiguities to discourse ambiguities arising from anaphora. To begin, we calculated a new measure of contextuality for a dataset of basic anaphoric discourses, resulting in a higher proportion of contextual models--82.9%--compared to previous work which only yielded 3.17% contextual models. Then, we show how an extension of the natural language processing challenge, known as the Winograd Schema, which involves anaphoric ambiguities can be modelled on the Bell-CHSH scenario with a contextual fraction of 0.096.
&lt;/p&gt;</description></item><item><title>&#21363;&#20351;&#26159;&#20219;&#21153;&#32422;&#26463;&#20063;&#20250;&#24433;&#21709;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#21363;&#20351;&#36825;&#20123;&#32422;&#26463;&#19982;&#35268;&#36991;&#26080;&#20851;&#65292;&#20063;&#20250;&#23548;&#33268;&#29616;&#26377;&#26816;&#27979;&#22120;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;</title><link>https://arxiv.org/abs/2311.08369</link><description>&lt;p&gt;
&#25351;&#20196;&#26041;&#24335;&#30340;&#37325;&#35201;&#24615;&#65306;&#21363;&#20351;&#20219;&#21153;&#32422;&#26463;&#20063;&#20250;&#24433;&#21709;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08369
&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#26159;&#20219;&#21153;&#32422;&#26463;&#20063;&#20250;&#24433;&#21709;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#21363;&#20351;&#36825;&#20123;&#32422;&#26463;&#19982;&#35268;&#36991;&#26080;&#20851;&#65292;&#20063;&#20250;&#23548;&#33268;&#29616;&#26377;&#26816;&#27979;&#22120;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28389;&#29992;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#24615;&#33021;&#21487;&#38752;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#12290;&#24403;&#29992;&#25143;&#25351;&#31034;LLMs&#29983;&#25104;&#25991;&#26412;&#26102;&#65292;&#25351;&#20196;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#21253;&#21547;&#19981;&#21516;&#30340;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#20026;LLM&#26816;&#27979;&#21019;&#24314;&#25968;&#25454;&#38598;&#26102;&#24182;&#27809;&#26377;&#28085;&#30422;&#36825;&#31181;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#27169;&#24335;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#20219;&#21153;&#23548;&#21521;&#30340;&#32422;&#26463;&#8212;&#8212;&#36825;&#20123;&#32422;&#26463;&#33258;&#28982;&#20250;&#21253;&#21547;&#22312;&#25351;&#20196;&#20013;&#65292;&#24182;&#19988;&#19982;&#26816;&#27979;&#35268;&#36991;&#26080;&#20851;&#8212;&#8212;&#20063;&#20250;&#23548;&#33268;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#22312;&#26816;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#36739;&#22823;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#20197;&#23398;&#29983;&#20316;&#25991;&#20889;&#20316;&#20026;&#29616;&#23454;&#39046;&#22495;&#65292;&#24182;&#26681;&#25454;&#20960;&#20010;&#22240;&#32032;&#25163;&#21160;&#21019;&#24314;&#22522;&#20110;&#20316;&#25991;&#36136;&#37327;&#30340;&#20219;&#21153;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24102;&#26377;&#36825;&#31181;&#32422;&#26463;&#30340;&#25351;&#20196;&#29983;&#25104;&#30340;&#25991;&#26412;&#19978;&#65292;&#24403;&#21069;&#26816;&#27979;&#22120;&#24615;&#33021;&#30340;&#26631;&#20934;&#20559;&#24046;&#65288;SD&#65289;&#26174;&#33879;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08369v2 Announce Type: replace  Abstract: To combat the misuse of Large Language Models (LLMs), many recent studies have presented LLM-generated-text detectors with promising performance. When users instruct LLMs to generate texts, the instruction can include different constraints depending on the user's need. However, most recent studies do not cover such diverse instruction patterns when creating datasets for LLM detection. In this paper, we find that even task-oriented constraints -- constraints that would naturally be included in an instruction and are not related to detection-evasion -- cause existing detectors to have a large variance in detection performance. We focus on student essay writing as a realistic domain and manually create task-oriented constraints based on several factors for essay quality. Our experiments show that the standard deviation (SD) of current detector performance on texts generated by an instruction with such a constraint is significantly large
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;</title><link>http://arxiv.org/abs/2401.05949</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36890;&#29992;&#28431;&#27934;&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#24357;&#21512;&#24046;&#36317;&#30340;&#33539;&#24335;&#65292;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#21442;&#25968;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#30340;&#20851;&#20999;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;ICLAttack&#65292;&#38024;&#23545;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#27745;&#26579;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;ICLAttack&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
&lt;/p&gt;</description></item><item><title>&#26681;&#25454; &#8220;&#19977;&#24605;&#32780;&#21518;&#35821;&#8221; &#34892;&#20026;&#21551;&#21457;&#65292;&#25552;&#20986;&#19968;&#31181;&#20004;&#38454;&#27573;&#23545;&#35805;&#20195;&#29702;&#29992;&#20110;&#29983;&#25104;&#24773;&#24863;&#23545;&#35805;&#65292;&#35813;&#20195;&#29702;&#22312;&#24773;&#24863;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#20445;&#25345;&#20102;&#35821;&#20041;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.04907</link><description>&lt;p&gt;
Think Twice&#65306;&#19968;&#31181;&#20154;&#31867;&#21270;&#30340;&#20004;&#38454;&#27573;&#23545;&#35805;&#20195;&#29702;&#29992;&#20110;&#29983;&#25104;&#24773;&#24863;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Think Twice: A Human-like Two-stage Conversational Agent for Emotional Response Generation. (arXiv:2301.04907v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04907
&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454; &#8220;&#19977;&#24605;&#32780;&#21518;&#35821;&#8221; &#34892;&#20026;&#21551;&#21457;&#65292;&#25552;&#20986;&#19968;&#31181;&#20004;&#38454;&#27573;&#23545;&#35805;&#20195;&#29702;&#29992;&#20110;&#29983;&#25104;&#24773;&#24863;&#23545;&#35805;&#65292;&#35813;&#20195;&#29702;&#22312;&#24773;&#24863;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#20445;&#25345;&#20102;&#35821;&#20041;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20154;&#31867;&#21270;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#30446;&#21069;&#24773;&#24863;&#23545;&#35805;&#26041;&#27861;&#37319;&#29992;&#32479;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#32852;&#21512;&#27169;&#22411;&#24773;&#24863;&#21644;&#35821;&#20041;&#12290;&#36825;&#31181;&#31574;&#30053;&#30001;&#20110;&#24773;&#24863;&#21644;&#35821;&#20041;&#20043;&#38388;&#30340;&#30456;&#20114;&#38480;&#21046;&#24448;&#24448;&#20250;&#20135;&#29983;&#23433;&#20840;&#30340;&#21709;&#24212;&#65292;&#24182;&#19988;&#38656;&#35201;&#32597;&#35265;&#30340;&#24773;&#24863;&#26631;&#27880;&#22823;&#35268;&#27169;&#23545;&#35805;&#35821;&#26009;&#24211;&#12290;&#21463;&#21040;&#20154;&#31867;&#23545;&#35805;&#20013;&#8220;&#19977;&#24605;&#32780;&#21518;&#35821;&#8221;&#30340;&#34892;&#20026;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#24773;&#24863;&#23545;&#35805;&#30340;&#20004;&#38454;&#27573;&#23545;&#35805;&#20195;&#29702;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#27809;&#26377;&#20351;&#29992;&#24773;&#24863;&#26631;&#27880;&#23545;&#35805;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#23545;&#35805;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#19978;&#19979;&#25991;&#35821;&#20041;&#30340;&#21407;&#22411;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;&#31532;&#19968;&#38454;&#27573;&#21407;&#22411;&#23558;&#36890;&#36807;&#19968;&#20010;&#21487;&#25511;&#30340;&#24773;&#24863;&#20248;&#21270;&#22120;&#19982;&#20849;&#24773;&#20551;&#35774;&#36827;&#34892;&#20462;&#25913;&#12290;&#22312;DailyDialog&#21644;EmpatheticDialogues&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#35805;&#20195;&#29702;&#22312;&#24773;&#24863;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;&#27604;&#36739;&#27169;&#22411;&#65292;&#24182;&#22312;&#33258;&#21160;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#20445;&#25345;&#20102;&#35821;&#20041;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Towards human-like dialogue systems, current emotional dialogue approaches jointly model emotion and semantics with a unified neural network. This strategy tends to generate safe responses due to the mutual restriction between emotion and semantics, and requires rare emotion-annotated large-scale dialogue corpus. Inspired by the "think twice" behavior in human dialogue, we propose a two-stage conversational agent for the generation of emotional dialogue. Firstly, a dialogue model trained without the emotion-annotated dialogue corpus generates a prototype response that meets the contextual semantics. Secondly, the first-stage prototype is modified by a controllable emotion refiner with the empathy hypothesis. Experimental results on the DailyDialog and EmpatheticDialogues datasets demonstrate that the proposed conversational outperforms the comparison models in emotion generation and maintains the semantic performance in automatic and human evaluations.
&lt;/p&gt;</description></item></channel></rss>