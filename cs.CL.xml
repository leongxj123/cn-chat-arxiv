<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;</title><link>https://rss.arxiv.org/abs/2312.05356</link><description>&lt;p&gt;
Neuron Patching: &#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#19982;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.05356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#12290;&#26356;&#26032;&#36825;&#20123;&#27169;&#22411;&#30340;&#26032;&#30693;&#35782;&#38750;&#24120;&#26114;&#36149;&#65292;&#36890;&#24120;&#38656;&#35201;&#20840;&#38754;&#23454;&#29616;&#20854;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;MENT&#65292;&#29992;&#20110;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#12290;&#22522;&#20110;&#29983;&#25104;&#24335;LLM&#30340;&#26426;&#21046;&#65292;MENT&#21487;&#20197;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#20196;&#29260;&#26102;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#65292;&#24182;&#36827;&#19968;&#27493;&#25903;&#25345;&#24120;&#35265;&#30340;&#32534;&#30721;&#20219;&#21153;&#12290;MENT&#20855;&#26377;&#39640;&#25928;&#12289;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#29305;&#28857;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#20462;&#34917;1&#25110;2&#20010;&#31070;&#32463;&#20803;&#26469;&#32416;&#27491;&#31070;&#32463;&#27169;&#22411;&#12290;&#20316;&#20026;&#31070;&#32463;&#20803;&#23618;&#38754;&#19978;&#29983;&#25104;&#27169;&#22411;&#32534;&#36753;&#30340;&#20808;&#39537;&#24037;&#20316;&#65292;&#25105;&#20204;&#35268;&#33539;&#20102;&#32534;&#36753;&#36807;&#31243;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#34913;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#32534;&#30721;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;API&#24207;&#21015;&#25512;&#33616;&#12289;&#34892;&#32423;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are successfully adopted in software engineering, especially in code generation. Updating these models with new knowledge is very expensive, and is often required to fully realize their value. In this paper, we propose a novel and effective model editing approach, \textsc{MENT}, to patch LLMs in coding tasks. Based on the mechanism of generative LLMs, \textsc{MENT} enables model editing in next-token predictions, and further supports common coding tasks. \textsc{MENT} is effective, efficient, and reliable. It can correct a neural model by patching 1 or 2 neurons. As the pioneer work on neuron-level model editing of generative models, we formalize the editing process and introduce the involved concepts. Besides, we also introduce new measures to evaluate its generalization ability, and build a benchmark for further study. Our approach is evaluated on three coding tasks, including API-seq recommendation, line-level code generation, and pseudocode-to-code transaction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#23500;&#21547;&#35821;&#20041;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#27604;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08492</link><description>&lt;p&gt;
&#23500;&#21547;&#35821;&#20041;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23569;&#26679;&#26412;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23500;&#21547;&#35821;&#20041;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#27604;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;RS-LLM&#65288;&#22522;&#20110;&#20016;&#23500;&#35821;&#20041;&#30340;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#20837;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#24341;&#20837;&#21508;&#31181;&#20013;&#25991;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24341;&#20837;&#23569;&#37327;&#29305;&#23450;&#30340;&#20013;&#25991;&#20016;&#23500;&#35821;&#20041;&#32467;&#26500;&#65292;LLMs&#22312;&#23569;&#26679;&#26412;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#20219;&#21153;&#19978;&#27604;&#22522;&#20110;BERT&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08492v1 Announce Type: new  Abstract: Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in speech to text (STT) and optical character recognition (OCR). Most of the existing CSC approaches relying on BERT architecture achieve excellent performance. However, limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications. In this paper, we explore using an in-context learning method named RS-LLM (Rich Semantic based LLMs) to introduce large language models (LLMs) as the foundation model. Besides, we study the impact of introducing various Chinese rich semantic information in our framework. We found that by introducing a small number of specific Chinese rich semantic structures, LLMs achieve better performance than the BERT-based model on few-shot CSC task. Furthermore, we conduct experiments on multiple datasets, and the experimental results verifie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#22312;&#22522;&#20110;&#22768;&#38899;&#30340;&#21333;&#35789;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#33258;&#21457;&#36830;&#25509;&#20004;&#20010;&#25110;&#19977;&#20010;&#21333;&#35789;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20250;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#25105;&#20204;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#24335;&#21644;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#37117;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.01626</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#30340;&#22522;&#30784;&#35821;&#27861;&#65306;&#33258;&#21457;&#32852;&#25509;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks. (arXiv:2305.01626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01626
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#22312;&#22522;&#20110;&#22768;&#38899;&#30340;&#21333;&#35789;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#33258;&#21457;&#36830;&#25509;&#20004;&#20010;&#25110;&#19977;&#20010;&#21333;&#35789;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20250;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#25105;&#20204;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#24335;&#21644;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#37117;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#30340;&#35745;&#31639;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#25991;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#26368;&#26222;&#36941;&#21644;&#22522;&#26412;&#30340;&#35821;&#27861;&#29305;&#24615;&#20043;&#19968;&#8212;&#8212;&#32852;&#25509;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21457;&#32852;&#25509;&#29616;&#35937;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#20010;&#21035;&#21333;&#35789;&#30340;&#22768;&#23398;&#35760;&#24405;&#19978;&#35757;&#32451;&#26102;&#65292;&#24320;&#22987;&#20135;&#29983;&#36755;&#20986;&#65292;&#36825;&#20123;&#36755;&#20986;&#23558;&#20004;&#20010;&#29978;&#33267;&#19977;&#20010;&#21333;&#35789;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#32780;&#19981;&#20250;&#25509;&#35302;&#21040;&#20855;&#26377;&#22810;&#20010;&#21333;&#35789;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#20004;&#20010;&#21333;&#35789;&#30340;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29615;&#22659;&#19979;&#35757;&#32451;&#30340;&#21407;&#22987;&#35821;&#38899;CNN&#20197;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#23427;&#19981;&#20165;&#23545;&#25105;&#20204;&#29702;&#35299;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#23398;&#20064;&#26041;&#24335;&#26377;&#24433;&#21709;&#65292;&#36824;&#23545;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational models of syntax are predominantly text-based. Here we propose that basic syntax can be modeled directly from raw speech in a fully unsupervised way. We focus on one of the most ubiquitous and basic properties of syntax -- concatenation. We introduce spontaneous concatenation: a phenomenon where convolutional neural networks (CNNs) trained on acoustic recordings of individual words start generating outputs with two or even three words concatenated without ever accessing data with multiple words in the input. Additionally, networks trained on two words learn to embed words into novel unobserved word combinations. To our knowledge, this is a previously unreported property of CNNs trained on raw speech in the Generative Adversarial Network setting and has implications both for our understanding of how these architectures learn as well as for modeling syntax and its evolution from raw acoustic inputs.
&lt;/p&gt;</description></item></channel></rss>