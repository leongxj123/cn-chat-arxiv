<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>AraTrust&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20449;&#35465;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20840;&#38754;&#20449;&#35465;&#35780;&#20272;&#22522;&#20934;&#30340;&#38382;&#39064;&#65292;&#24110;&#21161;&#20934;&#30830;&#35780;&#20272;&#21644;&#25552;&#39640;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09017</link><description>&lt;p&gt;
AraTrust&#65306;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#35465;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09017
&lt;/p&gt;
&lt;p&gt;
AraTrust&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20449;&#35465;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20840;&#38754;&#20449;&#35465;&#35780;&#20272;&#22522;&#20934;&#30340;&#38382;&#39064;&#65292;&#24110;&#21161;&#20934;&#30830;&#35780;&#20272;&#21644;&#25552;&#39640;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09017v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36805;&#36895;&#21457;&#23637;&#21644;&#24191;&#27867;&#25509;&#21463;&#20984;&#26174;&#20102;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;&#37492;&#20110;&#38463;&#25289;&#20271;&#35821;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#12289;&#25991;&#21270;&#20016;&#23500;&#24615;&#21644;&#22320;&#20301;&#19981;&#39640;&#65292;&#26377;&#24517;&#35201;&#19987;&#27880;&#20110;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#30340;&#20449;&#35465;&#35780;&#20272;&#22522;&#20934;&#26159;&#20934;&#30830;&#35780;&#20272;&#21644;&#25552;&#39640;&#22312;&#38463;&#25289;&#20271;&#35821;&#25552;&#31034;&#26102;LLMs&#30340;&#23433;&#20840;&#24615;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AraTrust 1&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#30340;&#20449;&#35465;&#22522;&#20934;&#12290;AraTrust &#21253;&#21547;&#20102;516&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#28041;&#21450;&#19982;&#30495;&#23454;&#24615;&#12289;&#36947;&#24503;&#12289;&#23433;&#20840;&#24615;&#12289;&#36523;&#20307;&#20581;&#24247;&#12289;&#24515;&#29702;&#20581;&#24247;&#12289;&#19981;&#20844;&#24179;&#34892;&#20026;&#12289;&#38750;&#27861;&#27963;&#21160;&#30456;&#20851;&#30340;&#22810;&#20010;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09017v1 Announce Type: new  Abstract: The swift progress and widespread acceptance of artificial intelligence (AI) systems highlight a pressing requirement to comprehend both the capabilities and potential risks associated with AI. Given the linguistic complexity, cultural richness, and underrepresented status of Arabic in AI research, there is a pressing need to focus on Large Language Models (LLMs) performance and safety for Arabic related tasks. Despite some progress in their development, there is a lack of comprehensive trustworthiness evaluation benchmarks which presents a major challenge in accurately assessing and improving the safety of LLMs when prompted in Arabic. In this paper, we introduce AraTrust 1, the first comprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises 516 human-written multiple-choice questions addressing diverse dimensions related to truthfulness, ethics, safety, physical health, mental health, unfairness, illegal activities
&lt;/p&gt;</description></item><item><title>&#31934;&#35843;&#35780;&#21028;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#21644;&#20844;&#24179;&#24615;&#19981;&#21450;GPT4&#12290;</title><link>https://arxiv.org/abs/2403.02839</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#21028;&#22120;&#30340;LLM&#30340;&#23454;&#35777;&#30740;&#31350;&#65306;&#31934;&#35843;&#35780;&#21028;&#22120;&#27169;&#22411;&#26159;&#29305;&#23450;&#20219;&#21153;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02839
&lt;/p&gt;
&lt;p&gt;
&#31934;&#35843;&#35780;&#21028;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#21644;&#20844;&#24179;&#24615;&#19981;&#21450;GPT4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35780;&#20272;&#20854;&#20182;LLM&#36136;&#37327;&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#35768;&#22810;&#30740;&#31350;&#37319;&#29992;&#19987;&#26377;&#30340;&#38381;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;GPT4&#65292;&#20316;&#20026;&#35780;&#20272;&#22120;&#12290;&#21478;&#22806;&#65292;&#20854;&#20182;&#30740;&#31350;&#21033;&#29992;&#24320;&#28304;LLM&#26469;&#31934;&#35843;&#35780;&#21028;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#30340;&#35780;&#21028;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;&#31934;&#35843;&#30340;&#35780;&#21028;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#38598;&#19978;&#33021;&#22815;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#36229;&#36807;GPT4&#65292;&#20294;&#23427;&#20204;&#26412;&#36136;&#19978;&#26159;&#29305;&#23450;&#20219;&#21153;&#30340;&#20998;&#31867;&#22120;&#65292;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#20844;&#24179;&#24615;&#36828;&#20302;&#20110;GPT4&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02839v1 Announce Type: new  Abstract: Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have employed proprietary close-source models, especially GPT4, as the evaluator. Alternatively, other works have fine-tuned judge models based on open-source LLMs as the evaluator. In this study, we conduct an empirical study of different judge models on their evaluation capability. Our findings indicate that although the fine-tuned judge models achieve high accuracy on in-domain test sets, even surpassing GPT4, they are inherently task-specific classifiers, and their generalizability and fairness severely underperform GPT4.
&lt;/p&gt;</description></item><item><title>ShieldLM&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#23433;&#20840;&#26816;&#27979;&#22120;&#65292;&#31526;&#21512;&#19968;&#33324;&#20154;&#31867;&#23433;&#20840;&#26631;&#20934;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#30340;&#26816;&#27979;&#35268;&#21017;&#65292;&#24182;&#25552;&#20379;&#20915;&#31574;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.16444</link><description>&lt;p&gt;
ShieldLM: &#20351;LLMs&#25104;&#20026;&#23545;&#40784;&#12289;&#21487;&#23450;&#21046;&#21644;&#21487;&#35299;&#37322;&#30340;&#23433;&#20840;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16444
&lt;/p&gt;
&lt;p&gt;
ShieldLM&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#23433;&#20840;&#26816;&#27979;&#22120;&#65292;&#31526;&#21512;&#19968;&#33324;&#20154;&#31867;&#23433;&#20840;&#26631;&#20934;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#30340;&#26816;&#27979;&#35268;&#21017;&#65292;&#24182;&#25552;&#20379;&#20915;&#31574;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#24615;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#65292;&#20294;&#22312;&#23545;LLMs&#30340;&#21709;&#24212;&#20013;&#26816;&#27979;&#23433;&#20840;&#38382;&#39064;&#30340;&#26041;&#27861;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#23545;&#40784;&#12289;&#21487;&#23450;&#21046;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ShieldLM&#65292;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#23433;&#20840;&#26816;&#27979;&#22120;&#65292;&#23427;&#19982;&#19968;&#33324;&#20154;&#31867;&#23433;&#20840;&#26631;&#20934;&#30456;&#31526;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#30340;&#26816;&#27979;&#35268;&#21017;&#65292;&#24182;&#20026;&#20854;&#20915;&#31574;&#25552;&#20379;&#35299;&#37322;&#12290;&#20026;&#20102;&#35757;&#32451;ShieldLM&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;14,387&#20010;&#26597;&#35810;-&#21709;&#24212;&#23545;&#30340;&#22823;&#22411;&#21452;&#35821;&#25968;&#25454;&#38598;&#65292;&#26681;&#25454;&#21508;&#31181;&#23433;&#20840;&#26631;&#20934;&#23545;&#21709;&#24212;&#30340;&#23433;&#20840;&#24615;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ShieldLM&#22312;&#22235;&#20010;&#27979;&#35797;&#38598;&#19978;&#36229;&#36234;&#20102;&#24378;&#22522;&#32447;&#65292;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#23450;&#21046;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#38500;&#20102;&#22312;&#26631;&#20934;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#22806;&#65292;ShieldLM&#36824;&#34987;&#35777;&#26126;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#20316;&#20026;&#20808;&#36827;LLMs&#30340;&#23433;&#20840;&#35780;&#20272;&#22120;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16444v1 Announce Type: new  Abstract: The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner. In this paper, we propose ShieldLM, an LLM-based safety detector, which aligns with general human safety standards, supports customizable detection rules, and provides explanations for its decisions. To train ShieldLM, we compile a large bilingual dataset comprising 14,387 query-response pairs, annotating the safety of responses based on various safety standards. Through extensive experiments, we demonstrate that ShieldLM surpasses strong baselines across four test sets, showcasing remarkable customizability and explainability. Besides performing well on standard detection datasets, ShieldLM has also been shown to be effective in real-world situations as a safety evaluator for advanced LLMs. We relea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MT-Bench-101&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#33021;&#21147;&#65292;&#26500;&#24314;&#20102;&#21253;&#21547;4208&#36718;&#23545;&#35805;&#25968;&#25454;&#30340;&#19977;&#32423;&#20998;&#23618;&#33021;&#21147;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#20102;21&#31181;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#19981;&#21516;&#23545;&#35805;&#36718;&#27425;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.14762</link><description>&lt;p&gt;
MT-Bench-101: &#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14762
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MT-Bench-101&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#33021;&#21147;&#65292;&#26500;&#24314;&#20102;&#21253;&#21547;4208&#36718;&#23545;&#35805;&#25968;&#25454;&#30340;&#19977;&#32423;&#20998;&#23618;&#33021;&#21147;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#20102;21&#31181;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#19981;&#21516;&#23545;&#35805;&#36718;&#27425;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22823;&#22823;&#22686;&#24378;&#20102;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#36718;&#23545;&#35805;&#25110;&#32773;&#25552;&#20379;&#31895;&#31890;&#24230;&#21644;&#19981;&#23436;&#25972;&#30340;&#22810;&#36718;&#23545;&#35805;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#30495;&#23454;&#23545;&#35805;&#30340;&#22797;&#26434;&#24615;&#21644;&#32454;&#24494;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MT-Bench-101&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#22810;&#36718;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;13&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;1388&#20010;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;4208&#36718;&#30340;&#19977;&#32423;&#20998;&#23618;&#33021;&#21147;&#20998;&#31867;&#12290;&#28982;&#21518;&#25105;&#20204;&#22522;&#20110;MT-Bench-101&#35780;&#20272;&#20102;21&#20010;&#27969;&#34892;&#30340;LLMs&#65292;&#20174;&#33021;&#21147;&#21644;&#20219;&#21153;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#35266;&#23519;&#21040;LLMs&#22312;&#23545;&#35805;&#36718;&#27425;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14762v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns with
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25910;&#38598;&#36731;&#37327;&#32423;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;Direct Preference Optimization&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#19978;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.10884</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#20559;&#22909;&#23545;&#40784;&#20462;&#22797;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#19978;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Multi-modal preference alignment remedies regression of visual instruction tuning on language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10884
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25910;&#38598;&#36731;&#37327;&#32423;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;Direct Preference Optimization&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#19978;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#34987;&#26399;&#26395;&#33021;&#22815;&#25903;&#25345;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#20132;&#25442;&#24335;&#22810;&#36718;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20351;&#29992;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65288;VQA&#65289;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;MLLMs&#21487;&#33021;&#20250;&#20986;&#29616;&#36864;&#21270;&#65292;&#22240;&#20026;VQA&#25968;&#25454;&#38598;&#32570;&#20047;&#21407;&#22987;&#25991;&#26412;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21518;&#32773;&#26159;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#65288;6k&#26465;&#35760;&#24405;&#65289;&#30340;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#31572;&#26696;&#30001;Gemini&#20197;&#32454;&#31890;&#24230;&#26041;&#24335;&#27880;&#37322;&#20102;5&#20010;&#36136;&#37327;&#25351;&#26631;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;&#26631;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#12289;&#25298;&#32477;&#25277;&#26679;&#12289;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#21644;SteerLM&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;DPO&#65292;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;6.73&#30340;MT-Bench&#20998;&#25968;&#65292;&#32780;Vicuna&#30340;6.57&#21644;LLaVA&#30340;5.99&#65292;&#23613;&#31649;&#25968;&#25454;&#35268;&#27169;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10884v1 Announce Type: cross  Abstract: In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#26041;&#27861;&#23545;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#30340;&#39118;&#38505;&#22240;&#32032;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25490;&#21517;&#65292;&#20197;&#25552;&#39640;&#23545;&#20854;&#29702;&#35299;&#12289;&#20998;&#31867;&#21644;&#20248;&#20808;&#32771;&#34385;&#39044;&#38450;&#21644;&#27835;&#30103;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.11517</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#39118;&#38505;&#22240;&#32032;&#20998;&#31867;&#19982;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
A Natural Language Processing-Based Classification and Mode-Based Ranking of Musculoskeletal Disorder Risk Factors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#26041;&#27861;&#23545;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#30340;&#39118;&#38505;&#22240;&#32032;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25490;&#21517;&#65292;&#20197;&#25552;&#39640;&#23545;&#20854;&#29702;&#35299;&#12289;&#20998;&#31867;&#21644;&#20248;&#20808;&#32771;&#34385;&#39044;&#38450;&#21644;&#27835;&#30103;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#65288;MSD&#65289;&#39118;&#38505;&#22240;&#32032;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#30456;&#32467;&#21512;&#12290;&#26088;&#22312;&#31934;&#32454;&#21270;&#29702;&#35299;&#12289;&#20998;&#31867;&#21644;&#20248;&#20808;&#32771;&#34385;&#38024;&#23545;&#24615;&#39044;&#38450;&#21644;&#27835;&#30103;&#12290;&#35780;&#20272;&#20102;&#20843;&#20010;NLP&#27169;&#22411;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#12289;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#36317;&#31163;&#24230;&#37327;&#23558;&#22240;&#32032;&#20998;&#31867;&#20026;&#20010;&#20154;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#24037;&#20316;&#22330;&#25152;&#12289;&#24515;&#29702;&#21644;&#32452;&#32455;&#31561;&#31867;&#21035;&#12290;BERT&#19982;&#20313;&#24358;&#30456;&#20284;&#24230;&#36798;&#21040;28%&#30340;&#20934;&#30830;&#29575;&#65307;&#21477;&#23376;&#36716;&#25442;&#22120;&#19982;&#27431;&#27663;&#12289;&#24067;&#38647;&#26354;&#33922;&#26031;&#21644;&#38389;&#21487;&#22827;&#26031;&#22522;&#36317;&#31163;&#24471;&#20998;&#20026;100%&#12290;&#36890;&#36807;10&#20493;&#20132;&#21449;&#39564;&#35777;&#65292;&#32479;&#35745;&#26816;&#39564;&#30830;&#20445;&#40065;&#26834;&#32467;&#26524;&#12290;&#35843;&#26597;&#25968;&#25454;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#30830;&#23450;&#20102;&#20005;&#37325;&#24615;&#31561;&#32423;&#65292;&#19982;&#25991;&#29486;&#30456;&#19968;&#33268;&#12290;"&#24037;&#20316;&#23039;&#21183;"&#26159;&#26368;&#20005;&#37325;&#30340;&#65292;&#20984;&#26174;&#20102;&#23039;&#21183;&#30340;&#20316;&#29992;&#12290;&#35843;&#26597;&#32467;&#26524;&#24378;&#35843;&#20102;"&#24037;&#20316;&#19981;&#31283;&#23450;&#24615;"&#12289;"&#24037;&#20316;&#21162;&#21147;&#21644;&#22238;&#25253;&#19981;&#24179;&#34913;"&#21644;"&#21592;&#24037;&#35774;&#26045;&#24046;"&#31561;&#22240;&#32032;&#30340;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11517v3 Announce Type: replace  Abstract: This research delves into Musculoskeletal Disorder (MSD) risk factors, using a blend of Natural Language Processing (NLP) and mode-based ranking. The aim is to refine understanding, classification, and prioritization for focused prevention and treatment. Eight NLP models are evaluated, combining pre-trained transformers, cosine similarity, and distance metrics to categorize factors into personal, biomechanical, workplace, psychological, and organizational classes. BERT with cosine similarity achieves 28% accuracy; sentence transformer with Euclidean, Bray-Curtis, and Minkowski distances scores 100%. With 10-fold cross-validation, statistical tests ensure robust results. Survey data and mode-based ranking determine severity hierarchy, aligning with the literature. "Working posture" is the most severe, highlighting posture's role. Survey insights emphasize "Job insecurity," "Effort reward imbalance," and "Poor employee facility" as sig
&lt;/p&gt;</description></item><item><title>Kun&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#36890;&#36807;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#25913;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#12290;&#23427;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#25552;&#39640;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20943;&#23569;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2401.06477</link><description>&lt;p&gt;
Kun: &#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#30340;&#20013;&#22269;&#33258;&#23545;&#40784;&#38382;&#39064;&#30340;&#31572;&#26696;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation. (arXiv:2401.06477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06477
&lt;/p&gt;
&lt;p&gt;
Kun&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#36890;&#36807;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#25913;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#12290;&#23427;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#25552;&#39640;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20943;&#23569;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Kun&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;Kun&#21033;&#29992;&#26469;&#33258;&#21566;&#36947;&#12289;&#23436;&#21367;&#21644;SkyPile&#31561;&#22810;&#20010;&#26469;&#28304;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#37319;&#29992;&#22522;&#20110;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#20013;&#25991;&#25351;&#23548;&#25968;&#25454;&#28857;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#23436;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#65292;&#26174;&#33879;&#20559;&#31163;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;6B&#21442;&#25968;&#30340;Yi&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;Kun&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#19988;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#23545;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#36825;&#31181;&#26041;&#27861;ological&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20013;&#25991;&#33258;&#23545;&#40784;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Kun, a novel approach for creating high-quality instruction-tuning datasets for large language models (LLMs) without relying on manual annotations. Adapting a self-training algorithm based on instruction back-translation and answer polishment, Kun leverages unlabelled data from diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial dataset of over a million Chinese instructional data points. This approach significantly deviates from traditional methods by using a self-curation process to refine and select the most effective instruction-output pairs. Our experiments with the 6B-parameter Yi model across various benchmarks demonstrate Kun's robustness and scalability. Our method's core contributions lie in its algorithmic advancement, which enhances data retention and clarity, and its innovative data generation approach that substantially reduces the reliance on costly and time-consuming manual annotations. This methodology presents a sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.07601</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#20449;&#21495;&#21644;&#24369;&#30417;&#30563;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#24230;&#20449;&#21495;&#20195;&#34920;&#20102;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#21592;&#36890;&#24120;&#29992;&#26469;&#35780;&#20272;&#22312;&#32447;&#20869;&#23481;&#30495;&#23454;&#24615;&#30340;&#19968;&#31995;&#21015;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#21487;&#20449;&#24230;&#20449;&#21495;&#25552;&#21462;&#30340;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#35757;&#32451;&#39640;&#20934;&#30830;&#29575;&#30340;&#29305;&#23450;&#20449;&#21495;&#25552;&#21462;&#22120;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#23545;&#25152;&#26377;&#21487;&#20449;&#24230;&#20449;&#21495;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#19968;&#32452;18&#20010;&#21487;&#20449;&#24230;&#20449;&#21495;&#26469;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20197;&#20135;&#29983;&#27599;&#20010;&#20449;&#21495;&#30340;&#24369;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#36825;&#20123;&#28508;&#22312;&#30340;&#22122;&#22768;&#26631;&#31614;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#32467;&#21512;&#20102;&#38646;-shot LLM&#21487;&#20449;&#24230;&#20449;&#21495;&#26631;&#27880;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#65292;&#32780;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#35757;&#32451;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credibility signals represent a wide range of heuristics that are typically used by journalists and fact-checkers to assess the veracity of online content. Automating the task of credibility signal extraction, however, is very challenging as it requires high-accuracy signal-specific extractors to be trained, while there are currently no sufficiently large datasets annotated with all credibility signals. This paper investigates whether large language models (LLMs) can be prompted effectively with a set of 18 credibility signals to produce weak labels for each signal. We then aggregate these potentially noisy labels using weak supervision in order to predict content veracity. We demonstrate that our approach, which combines zero-shot LLM credibility signal labeling and weak supervision, outperforms state-of-the-art classifiers on two misinformation datasets without using any ground-truth labels for training. We also analyse the contribution of the individual credibility signals towards p
&lt;/p&gt;</description></item></channel></rss>