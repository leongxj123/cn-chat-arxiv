<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21516;&#33021;&#21147;&#30340;&#32508;&#21512;&#20998;&#26512;&#25581;&#31034;&#20102;&#20854;&#21160;&#24577;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26631;&#24230;&#23450;&#24459;&#20013;&#30340;&#32570;&#22833;&#12290;</title><link>https://arxiv.org/abs/2404.01204</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#33021;&#21147;&#20998;&#26512;&#30340;&#24494;&#22937;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21516;&#33021;&#21147;&#30340;&#32508;&#21512;&#20998;&#26512;&#25581;&#31034;&#20102;&#20854;&#21160;&#24577;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26631;&#24230;&#23450;&#24459;&#20013;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#21453;&#26144;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#30340;&#26089;&#26399;&#25351;&#26631;&#26159;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#19968;&#20010;&#26680;&#24515;&#21407;&#21017;&#12290;&#29616;&#26377;&#30340;&#26631;&#24230;&#23450;&#24459;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#25439;&#22833;&#19982;&#35757;&#32451;&#28014;&#28857;&#25968;&#20043;&#38388;&#30340;&#24130;&#24459;&#30456;&#20851;&#24615;&#65292;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24403;&#21069;&#35757;&#32451;&#29366;&#24577;&#30340;&#37325;&#35201;&#25351;&#26631;&#21313;&#20998;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#21407;&#21017;&#21482;&#20851;&#27880;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#21387;&#32553;&#29305;&#24615;&#65292;&#23548;&#33268;&#19982;&#19979;&#28216;&#20219;&#21153;&#33021;&#21147;&#30340;&#25552;&#21319;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#19968;&#20123;&#21518;&#32493;&#30740;&#31350;&#35797;&#22270;&#23558;&#26631;&#24230;&#23450;&#24459;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#25351;&#26631;&#65288;&#22914;&#36229;&#21442;&#25968;&#65289;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#23545;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21508;&#31181;&#33021;&#21147;&#20043;&#38388;&#21160;&#24577;&#24046;&#24322;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#26412;&#25991;&#23545;&#21508;&#31181;&#39044;&#35757;&#32451;&#20013;&#38388;&#26816;&#26597;&#28857;&#19979;&#27169;&#22411;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01204v1 Announce Type: new  Abstract: Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26641;&#24179;&#22343;&#27861;&#26500;&#24314;&#38598;&#25104;&#35299;&#26512;&#22120;&#65292;&#31283;&#23450;&#24182;&#25552;&#21319;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#32447;</title><link>https://arxiv.org/abs/2403.00143</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#30340;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#65306;&#26641;&#24179;&#22343;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensemble-Based Unsupervised Discontinuous Constituency Parsing by Tree Averaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00143
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26641;&#24179;&#22343;&#27861;&#26500;&#24314;&#38598;&#25104;&#35299;&#26512;&#22120;&#65292;&#31283;&#23450;&#24182;&#25552;&#21319;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#30340;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#25105;&#20204;&#35266;&#23519;&#21040;&#20808;&#21069;&#21807;&#19968;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23545;&#29616;&#26377;&#19981;&#36830;&#32493;&#35299;&#26512;&#22120;&#30340;&#19981;&#21516;&#36816;&#34892;&#26500;&#24314;&#19968;&#20010;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#39044;&#27979;&#26641;&#26469;&#31283;&#23450;&#21644;&#25552;&#21319;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#19981;&#21516;&#30340;&#20108;&#20803;&#24615;&#21644;&#36830;&#32493;&#24615;&#35774;&#32622;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26641;&#24179;&#22343;&#35745;&#31639;&#22797;&#26434;&#24230;&#20998;&#26512;&#65288;&#20197;P&#21644;NP&#23436;&#20840;&#20026;&#21333;&#20301;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#30830;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#19968;&#20219;&#21153;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#23545;&#25152;&#26377;&#26679;&#26412;&#36816;&#34892;&#26102;&#38388;&#22343;&#21512;&#29702;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#32447;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00143v1 Announce Type: cross  Abstract: We address unsupervised discontinuous constituency parsing, where we observe a high variance in the performance of the only previous model. We propose to build an ensemble of different runs of the existing discontinuous parser by averaging the predicted trees, to stabilize and boost performance. To begin with, we provide comprehensive computational complexity analysis (in terms of P and NP-complete) for tree averaging under different setups of binarity and continuity. We then develop an efficient exact algorithm to tackle the task, which runs in a reasonable time for all samples in our experiments. Results on three datasets show our method outperforms all baselines in all metrics; we also provide in-depth analyses of our approach.
&lt;/p&gt;</description></item><item><title>DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17812</link><description>&lt;p&gt;
DropBP&#65306;&#36890;&#36807;&#20002;&#24323;&#21453;&#21521;&#20256;&#25773;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17812
&lt;/p&gt;
&lt;p&gt;
DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#27491;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#23618;&#27425;&#20002;&#24323;&#25216;&#26415;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20002;&#24323;&#26576;&#20123;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#22312;&#27491;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#20002;&#24323;&#23618;&#20250;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DropBP&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;DropBP&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#65292;&#19981;&#24433;&#21709;&#27491;&#21521;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;DropBP&#35745;&#31639;&#27599;&#20010;&#23618;&#30340;&#25935;&#24863;&#24615;&#20197;&#20998;&#37197;&#36866;&#24403;&#30340;&#20002;&#22833;&#29575;&#65292;&#20174;&#32780;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;DropBP&#26088;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#21152;&#36895;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#23436;&#20840;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17812v1 Announce Type: cross  Abstract: Training deep neural networks typically involves substantial computational costs during both forward and backward propagation. The conventional layer dropping techniques drop certain layers during training for reducing the computations burden. However, dropping layers during forward propagation adversely affects the training process by degrading accuracy. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy. DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation. Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process. DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#23398;&#29983;&#35299;&#31572;&#24120;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2402.01720</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Amharic Chatbot for FAQs in Universities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#23398;&#29983;&#35299;&#31572;&#24120;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#23398;&#29983;&#24120;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21521;&#31649;&#29702;&#21592;&#25110;&#25945;&#24072;&#23547;&#27714;&#24120;&#35265;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#36825;&#23545;&#21452;&#26041;&#26469;&#35828;&#37117;&#24456;&#32321;&#29712;&#65292;&#38656;&#35201;&#25214;&#21040;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#38463;&#22982;&#21704;&#25289;&#35821;&#20013;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#65292;&#20316;&#20026;&#34394;&#25311;&#21161;&#25163;&#22788;&#29702;&#38382;&#39064;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31243;&#24207;&#20351;&#29992;&#26631;&#35760;&#21270;&#12289;&#35268;&#33539;&#21270;&#12289;&#21435;&#38500;&#20572;&#29992;&#35789;&#21644;&#35789;&#24178;&#25552;&#21462;&#23545;&#38463;&#22982;&#21704;&#25289;&#35821;&#36755;&#20837;&#21477;&#23376;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#12290;&#37319;&#29992;&#20102;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#26469;&#20998;&#31867;&#26631;&#35760;&#21644;&#26816;&#32034;&#21512;&#36866;&#30340;&#22238;&#31572;&#65306;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#36890;&#36807;TensorFlow&#12289;Keras&#21644;NLTK&#23454;&#29616;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
University students often spend a considerable amount of time seeking answers to common questions from administrators or teachers. This can become tedious for both parties, leading to a need for a solution. In response, this paper proposes a chatbot model that utilizes natural language processing and deep learning techniques to answer frequently asked questions (FAQs) in the Amharic language. Chatbots are computer programs that simulate human conversation through the use of artificial intelligence (AI), acting as a virtual assistant to handle questions and other tasks. The proposed chatbot program employs tokenization, normalization, stop word removal, and stemming to analyze and categorize Amharic input sentences. Three machine learning model algorithms were used to classify tokens and retrieve appropriate responses: Support Vector Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented through TensorFlow, Keras, and NLTK. The deep learning model achieved the be
&lt;/p&gt;</description></item></channel></rss>