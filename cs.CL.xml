<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23545;&#31867;&#27604;&#25512;&#29702;&#12289;&#21453;&#24605;&#25512;&#29702;&#12289;&#21333;&#35789;&#39044;&#27979;&#21644;&#35821;&#27861;&#21028;&#26029;&#30340;&#34920;&#29616;&#21463;&#36741;&#21161;&#20219;&#21153;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#26041;&#27861;&#30340;&#20219;&#21153;&#38656;&#27714;&#36234;&#22823;&#65292;&#24615;&#33021;&#36234;&#20302;&#65292;&#36825;&#31181;"&#38656;&#27714;&#24046;&#36317;"&#22312;&#21442;&#25968;&#36739;&#23569;&#12289;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#23588;&#20026;&#26174;&#33879;</title><link>https://arxiv.org/abs/2404.02418</link><description>&lt;p&gt;
&#36741;&#21161;&#20219;&#21153;&#38656;&#27714;&#25513;&#30422;&#20102;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Auxiliary task demands mask the capabilities of smaller language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02418
&lt;/p&gt;
&lt;p&gt;
&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23545;&#31867;&#27604;&#25512;&#29702;&#12289;&#21453;&#24605;&#25512;&#29702;&#12289;&#21333;&#35789;&#39044;&#27979;&#21644;&#35821;&#27861;&#21028;&#26029;&#30340;&#34920;&#29616;&#21463;&#36741;&#21161;&#20219;&#21153;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#26041;&#27861;&#30340;&#20219;&#21153;&#38656;&#27714;&#36234;&#22823;&#65292;&#24615;&#33021;&#36234;&#20302;&#65292;&#36825;&#31181;"&#38656;&#27714;&#24046;&#36317;"&#22312;&#21442;&#25968;&#36739;&#23569;&#12289;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#23588;&#20026;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24515;&#29702;&#23398;&#23478;&#20204;&#23545;&#35748;&#30693;&#33021;&#21147;&#22914;&#35821;&#35328;&#29702;&#35299;&#25110;&#24515;&#28789;&#29702;&#35770;&#20309;&#26102;&#20986;&#29616;&#36827;&#34892;&#20102;&#20105;&#35770;&#12290;&#36825;&#20123;&#36777;&#35770;&#24120;&#24120;&#20851;&#27880;"&#20219;&#21153;&#38656;&#27714;"&#30340;&#27010;&#24565;--&#25191;&#34892;&#29305;&#23450;&#35780;&#20272;&#26102;&#25152;&#20276;&#38543;&#30340;&#36741;&#21161;&#25361;&#25112;--&#36825;&#20123;&#25361;&#25112;&#21487;&#33021;&#25513;&#30422;&#20102;&#20799;&#31461;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#24403;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#33021;&#21147;&#26102;&#65292;&#21516;&#26679;&#30340;&#38382;&#39064;&#20063;&#20250;&#20986;&#29616;&#65306;&#20219;&#21153;&#34920;&#29616;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#32467;&#21512;&#20102;&#27169;&#22411;&#35299;&#37322;&#21644;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#20854;&#21487;&#29992;&#36164;&#28304;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#31867;&#27604;&#25512;&#29702;&#12289;&#21453;&#24605;&#25512;&#29702;&#12289;&#21333;&#35789;&#39044;&#27979;&#21644;&#35821;&#27861;&#21028;&#26029;&#65292;&#20855;&#26377;&#26356;&#22823;&#20219;&#21153;&#38656;&#27714;&#30340;&#35780;&#20272;&#26041;&#27861;&#20250;&#27604;&#38477;&#20302;&#38656;&#27714;&#30340;&#35780;&#20272;&#24471;&#21040;&#26356;&#20302;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;"&#38656;&#27714;&#24046;&#36317;"&#22312;&#21442;&#25968;&#36739;&#23569;&#12289;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#26174;&#33879;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LM&#30340;&#24615;&#33021;&#19981;&#24212;&#34987;&#35299;&#37322;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02418v1 Announce Type: cross  Abstract: Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of "task demands" -- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability. The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model's underlying competence, combined with the model's ability to interpret and perform the task given its available resources. Here, we show that for analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands. This "demand gap" is most pronounced for models with fewer parameters and less training data. Our results illustrate that LM performance should not be interpret
&lt;/p&gt;</description></item><item><title>GigaPevt&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#19994;&#21307;&#30103;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;&#65292;&#22312;&#23545;&#35805;&#36136;&#37327;&#21644;&#24230;&#37327;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;1.18\%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16654</link><description>&lt;p&gt;
GigaPevt&#65306;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
GigaPevt: Multimodal Medical Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16654
&lt;/p&gt;
&lt;p&gt;
GigaPevt&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#19994;&#21307;&#30103;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;&#65292;&#22312;&#23545;&#35805;&#36136;&#37327;&#21644;&#24230;&#37327;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;1.18\%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#19968;&#20010;&#26234;&#33021;&#39640;&#25928;&#30340;&#21307;&#30103;&#21161;&#25163;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#12290;&#20027;&#35201;&#38480;&#21046;&#26469;&#33258;&#25968;&#25454;&#27169;&#24577;&#30340;&#31232;&#32570;&#24615;&#65292;&#38477;&#20302;&#20102;&#20840;&#38754;&#30340;&#24739;&#32773;&#24863;&#30693;&#12290;&#26412;&#28436;&#31034;&#35770;&#25991;&#20171;&#32461;&#20102;GigaPevt&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#21151;&#33021;&#21644;&#19987;&#19994;&#21307;&#30103;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23545;&#35805;&#36136;&#37327;&#21644;&#24230;&#37327;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#20351;&#24471;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.18\%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16654v1 Announce Type: cross  Abstract: Building an intelligent and efficient medical assistant is still a challenging AI problem. The major limitation comes from the data modality scarceness, which reduces comprehensive patient perception. This demo paper presents the GigaPevt, the first multimodal medical assistant that combines the dialog capabilities of large language models with specialized medical models. Such an approach shows immediate advantages in dialog quality and metric performance, with a 1.18\% accuracy improvement in the question-answering task.
&lt;/p&gt;</description></item><item><title>ArabicMMLU&#26159;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#30340;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#23398;&#26657;&#32771;&#35797;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#23545;35&#20010;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#22312;&#38463;&#25289;&#20271;&#35821;&#20013;&#24615;&#33021;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12840</link><description>&lt;p&gt;
ArabicMMLU&#65306;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#20013;&#30340;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12840
&lt;/p&gt;
&lt;p&gt;
ArabicMMLU&#26159;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#30340;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#23398;&#26657;&#32771;&#35797;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#23545;35&#20010;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#22312;&#38463;&#25289;&#20271;&#35821;&#20013;&#24615;&#33021;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#37325;&#28857;&#24050;&#32463;&#36716;&#21521;&#25512;&#29702;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#36825;&#24471;&#30410;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#37096;&#20998;&#22312;&#22823;&#37327;&#38463;&#25289;&#20271;&#25991;&#26412;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#20294;&#30001;&#20110;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#38463;&#25289;&#20271;&#35821;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ArabicMMLU&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#35328;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#25968;&#25454;&#26469;&#33258;&#20110;&#36328;&#36234;&#21271;&#38750;&#12289;&#40654;&#20961;&#29305;&#21644;&#28023;&#28286;&#22320;&#21306;&#19981;&#21516;&#22269;&#23478;&#25945;&#32946;&#27700;&#24179;&#30340;&#23398;&#26657;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#21253;&#25324;40&#20010;&#20219;&#21153;&#21644;14,575&#20010;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#65288;MSA&#65289;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#36890;&#36807;&#19982;&#35813;&#22320;&#21306;&#30340;&#27597;&#35821;&#32773;&#21512;&#20316;&#31934;&#24515;&#26500;&#24314;&#12290;&#25105;&#20204;&#23545;35&#20010;&#27169;&#22411;&#30340;&#20840;&#38754;&#35780;&#20272;&#26174;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#22909;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;BLOOMZ&#12289;mT0&#12289;LLama2&#21644;Fa&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12840v1 Announce Type: new  Abstract: The focus of language model evaluation has transitioned towards reasoning and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating their performance in Arabic remains challenging due to the limited availability of relevant datasets. To bridge this gap, we present ArabicMMLU, the first multi-task language understanding benchmark for Arabic language, sourced from school exams across diverse educational levels in different countries spanning North Africa, the Levant, and the Gulf regions. Our data comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard Arabic (MSA), and is carefully constructed by collaborating with native speakers in the region. Our comprehensive evaluations of 35 models reveal substantial room for improvement, particularly among the best open-source models. Notably, BLOOMZ, mT0, LLama2, and Fa
&lt;/p&gt;</description></item><item><title>C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03181</link><description>&lt;p&gt;
C-RAG: &#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#29983;&#25104;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03181
&lt;/p&gt;
&lt;p&gt;
C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#22791;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#38169;&#20301;&#12290;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RAG&#65289;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;RAG&#27169;&#22411;&#30340;&#29983;&#25104;&#39118;&#38505;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;RAG&#26159;&#21542;&#30830;&#23454;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#65292;2&#65289;&#22914;&#20309;&#23545;RAG&#21644;&#20256;&#32479;LLM&#30340;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#20197;&#21450;3&#65289;&#21738;&#20123;&#20805;&#20998;&#26465;&#20214;&#20351;&#24471;RAG&#27169;&#22411;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C-RAG&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;RAG&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;RAG&#27169;&#22411;&#25552;&#20379;&#20102;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#65292;&#24182;&#30830;&#20445;&#20102;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#33324;&#26377;&#30028;&#39118;&#38505;&#19979;&#30340;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
&lt;/p&gt;</description></item><item><title>ScreenQA&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;86K&#20010;&#38382;&#31572;&#23545;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;&#65292;&#26088;&#22312;&#35780;&#20272;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2209.08199</link><description>&lt;p&gt;
ScreenQA: &#31227;&#21160;&#24212;&#29992;&#25130;&#22270;&#19978;&#30340;&#22823;&#35268;&#27169;&#38382;&#31572;&#23545;
&lt;/p&gt;
&lt;p&gt;
ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08199
&lt;/p&gt;
&lt;p&gt;
ScreenQA&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;86K&#20010;&#38382;&#31572;&#23545;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;&#65292;&#26088;&#22312;&#35780;&#20272;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;ScreenQA&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#26469;&#29702;&#35299;&#23631;&#24149;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#23631;&#24149;&#25968;&#25454;&#38598;&#35201;&#20040;&#20391;&#37325;&#20110;&#32467;&#26500;&#21644;&#32452;&#20214;&#32423;&#21035;&#30340;&#29702;&#35299;&#65292;&#35201;&#20040;&#20391;&#37325;&#20110;&#20687;&#23548;&#33322;&#21644;&#20219;&#21153;&#23436;&#25104;&#20043;&#31867;&#30340;&#26356;&#39640;&#32423;&#21035;&#30340;&#32452;&#21512;&#20219;&#21153;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;86K&#20010;&#38382;&#31572;&#23545;&#26469;&#24357;&#21512;&#36825;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24076;&#26395;&#33021;&#22815;&#22522;&#20934;&#21270;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.08199v2 Announce Type: replace  Abstract: We present a new task and dataset, ScreenQA, for screen content understanding via question answering. The existing screen datasets are focused either on structure and component-level understanding, or on a much higher-level composite task such as navigation and task completion. We attempt to bridge the gap between these two by annotating 86K question-answer pairs over the RICO dataset in hope to benchmark the screen reading comprehension capacity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06461</link><description>&lt;p&gt;
&#20195;&#30721;&#20043;&#38388;&#30340;&#30028;&#38480;&#65306;&#25581;&#31034;&#26426;&#22120;&#21644;&#20154;&#31867;&#31243;&#24207;&#21592;&#20043;&#38388;&#19981;&#21516;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#27169;&#31946;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#28304;&#20195;&#30721;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#23548;&#33268;&#36719;&#20214;&#20135;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20195;&#30721;&#38271;&#24230;&#12289;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#33258;&#28982;&#24615;&#31561;&#23646;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#22266;&#26377;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#29305;&#21035;&#27880;&#24847;&#21040;&#65292;&#20195;&#30721;&#30340;&#32467;&#26500;&#20998;&#21106;&#26159;&#35782;&#21035;&#20854;&#26469;&#28304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#22411;&#26426;&#22120;&#29983;&#25104;&#20195;&#30721;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;DetectGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36328;&#25991;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#32654;&#22269;Twitter&#29992;&#25143;&#30456;&#27604;&#65292;&#20013;&#22269;&#26032;&#28010;&#24494;&#21338;&#29992;&#25143;&#22312;&#24773;&#24863;&#24378;&#24230;&#30340;&#21464;&#21270;&#21644;&#28608;&#21160;&#31243;&#24230;&#19978;&#26377;&#26356;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.05254</link><description>&lt;p&gt;
&#20013;&#32654;&#20004;&#22269;&#20043;&#38388;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#34920;&#36798;&#30340;&#20215;&#20540;&#21644;&#28608;&#21160;&#23545;&#27604;&#65306;&#19968;&#20010;&#36328;&#25991;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination. (arXiv:2401.05254v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36328;&#25991;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#32654;&#22269;Twitter&#29992;&#25143;&#30456;&#27604;&#65292;&#20013;&#22269;&#26032;&#28010;&#24494;&#21338;&#29992;&#25143;&#22312;&#24773;&#24863;&#24378;&#24230;&#30340;&#21464;&#21270;&#21644;&#28608;&#21160;&#31243;&#24230;&#19978;&#26377;&#26356;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#19978;&#20010;&#20307;&#30340;&#24773;&#24863;&#34920;&#36798;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35199;&#26041;&#29615;&#22659;&#20013;&#12290;&#19981;&#21516;&#25991;&#21270;&#20043;&#38388;&#23384;&#22312;&#30528;&#24341;&#21457;&#24773;&#24863;&#34920;&#36798;&#30340;&#37325;&#35201;&#24046;&#24322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32654;&#22269;Twitter&#21644;&#20013;&#22269;&#26032;&#28010;&#24494;&#21338;&#19978;&#30340;&#20004;&#20010;&#20027;&#35201;&#24773;&#24863;&#32500;&#24230;&#65288;&#20215;&#20540;&#21644;&#28608;&#21160;&#65289;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#20010;&#20307;&#20043;&#38388;&#30340;&#28608;&#21160;&#21644;&#20215;&#20540;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#24046;&#24322;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20851;&#20869;&#23481;&#19978;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#20004;&#20010;&#24179;&#21488;&#19978;&#30340;&#35789;&#35821;&#20351;&#29992;&#21644;&#35805;&#39064;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#65292;&#20197;&#35299;&#35835;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;Twitter&#29992;&#25143;&#26469;&#35828;&#65292;&#36127;&#38754;&#24773;&#32490;&#21644;&#27491;&#38754;&#24773;&#32490;&#20043;&#38388;&#30340;&#24773;&#24863;&#24378;&#24230;&#21464;&#21270;&#19981;&#22826;&#26126;&#26174;&#65292;&#32780;&#23545;&#20110;&#26032;&#28010;&#24494;&#21338;&#29992;&#25143;&#26469;&#35828;&#65292;&#20276;&#38543;&#30528;&#24773;&#24863;&#30340;&#19978;&#21319;&#65292;&#28608;&#21160;&#31243;&#24230;&#26377;&#26356;&#26126;&#26174;&#30340;&#21319;&#32423;&#12290;&#20174;&#35821;&#35328;&#29305;&#24449;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although affective expressions of individuals have been extensively studied using social media, research has primarily focused on the Western context. There are substantial differences among cultures that contribute to their affective expressions. This paper examines the differences between Twitter (X) in the United States and Sina Weibo posts in China on two primary dimensions of affect - valence and arousal. We study the difference in the functional relationship between arousal and valence (so-called V-shaped) among individuals in the US and China and explore the associated content differences. Furthermore, we correlate word usage and topics in both platforms to interpret their differences. We observe that for Twitter users, the variation in emotional intensity is less distinct between negative and positive emotions compared to Weibo users, and there is a sharper escalation in arousal corresponding with heightened emotions. From language features, we discover that affective expressio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#23558;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2310.11960</link><description>&lt;p&gt;
&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#65306;&#19968;&#31181;&#29992;&#20110;&#38271;&#24207;&#21015;&#30340;&#20998;&#27835;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences. (arXiv:2310.11960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#23558;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#33258;&#27880;&#24847;&#21147;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;Transformer&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#26469;&#20943;&#23569;&#27880;&#24847;&#21147;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;&#38271;&#24230;&#20026;n&#30340;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;&#36825;&#31181;&#20998;&#23618;&#26041;&#27861;&#23558;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20998;&#20026;O(log n)&#32423;&#30340;&#20998;&#36776;&#29575;&#65292;&#36739;&#36828;&#36317;&#31163;&#30340;&#32452;&#32676;&#36234;&#26469;&#36234;&#22823;&#65292;&#24182;&#23398;&#20064;&#35745;&#31639;&#32452;&#32676;&#25968;&#37327;&#30340;&#26435;&#37325;&#12290;&#22240;&#27492;&#65292;&#20197;&#39640;&#25928;&#20998;&#23618;&#30340;&#26041;&#24335;&#22312;&#36739;&#20302;&#30340;&#20998;&#36776;&#29575;&#20013;&#32771;&#34385;&#36828;&#31163;&#24444;&#27492;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#24635;&#20307;&#22797;&#26434;&#24230;&#20026;O(n)&#25110;O(n log n)&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}( \log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \
&lt;/p&gt;</description></item><item><title>&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06979</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#26159;&#36890;&#29992;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-Regressive Next-Token Predictors are Universal Learners. (arXiv:2309.06979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06979
&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#22312;&#36923;&#36753;&#21644;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#38750;&#20961;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#35757;&#32451;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#31616;&#21333;&#20219;&#21153;&#19978;&#30340;&#32593;&#32476;&#20013;&#20986;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#33258;&#22238;&#24402;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#22914;&#32447;&#24615;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#24403;&#20854;&#22312;&#24605;&#32500;&#38142;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#8212;&#8212;&#38271;&#24230;&#22797;&#26434;&#24230;&#65292;&#23427;&#34913;&#37327;&#20102;&#22312;&#36817;&#20284;&#26576;&#20010;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#24605;&#32500;&#38142;&#24207;&#21015;&#20013;&#25152;&#38656;&#30340;&#20013;&#38388;&#26631;&#35760;&#30340;&#25968;&#37327;&#65292;&#24182;&#20998;&#26512;&#20102;&#38271;&#24230;&#22797;&#26434;&#24230;&#21644;&#20854;&#20182;&#22797;&#26434;&#24615;&#27010;&#24565;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#31616;&#21333;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#22914;&#32447;&#24615;&#32593;&#32476;&#21644;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our resul
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#33258;&#21160;&#23398;&#29983;&#21453;&#39304;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#65292;&#20294;&#24341;&#20837;&#20102;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#8220;&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;&#8221;&#21644;&#24573;&#35270;&#38271;&#23614;&#20013;&#23569;&#25968;&#32676;&#20307;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.15334</link><description>&lt;p&gt;
&#19968;&#31181;&#36127;&#36131;&#20219;&#24320;&#21457;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#33258;&#21160;&#23398;&#29983;&#21453;&#39304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Responsible Development of Automated Student Feedback with Generative AI. (arXiv:2308.15334v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15334
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#33258;&#21160;&#23398;&#29983;&#21453;&#39304;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#65292;&#20294;&#24341;&#20837;&#20102;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#8220;&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;&#8221;&#21644;&#24573;&#35270;&#38271;&#23614;&#20013;&#23569;&#25968;&#32676;&#20307;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#23545;&#20110;&#25903;&#25345;&#23398;&#29983;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#29983;&#25104;AI&#23588;&#20854;&#26159;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20026;&#21521;&#23398;&#29983;&#25552;&#20379;&#21487;&#37325;&#22797;&#12289;&#21487;&#25193;&#23637;&#21644;&#21363;&#26102;&#29983;&#25104;&#30340;&#33258;&#21160;&#21453;&#39304;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20351;&#24471;&#20043;&#21069;&#31232;&#32570;&#19988;&#26114;&#36149;&#30340;&#23398;&#20064;&#36164;&#28304;&#21464;&#24471;&#20016;&#23500;&#36215;&#26469;&#12290;&#20174;&#25216;&#26415;&#35282;&#24230;&#32780;&#35328;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#21487;&#34892;&#30340;&#65292;&#24471;&#30410;&#20110;&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#65307;&#28982;&#32780;&#65292;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#20063;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#28508;&#22312;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#35748;&#30495;&#32771;&#34385;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#26368;&#20047;&#21619;&#30340;&#20219;&#21153;&#65307;&#20294;&#26159;&#36825;&#20063;&#21487;&#33021;&#23548;&#33268;&#8220;&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;&#8221;&#65292;&#21363;&#24573;&#35270;&#20102;&#38271;&#23614;&#20013;&#23569;&#25968;&#32676;&#20307;&#30340;&#38656;&#27714;&#65292;&#22240;&#20026;&#36825;&#20123;&#38656;&#27714;&#24456;&#38590;&#33258;&#21160;&#21270;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#33021;&#22815;&#20135;&#29983;&#26377;&#20215;&#20540;&#21644;&#30495;&#23454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing rich feedback to students is essential for supporting student learning. Recent advances in generative AI, particularly within large language modelling (LLM), provide the opportunity to deliver repeatable, scalable and instant automatically generated feedback to students, making abundant a previously scarce and expensive learning resource. Such an approach is feasible from a technical perspective due to these recent advances in Artificial Intelligence (AI) and Natural Language Processing (NLP); while the potential upside is a strong motivator, doing so introduces a range of potential ethical issues that must be considered as we apply these technologies. The attractiveness of AI systems is that they can effectively automate the most mundane tasks; but this risks introducing a "tyranny of the majority", where the needs of minorities in the long tail are overlooked because they are difficult to automate.  Developing machine learning models that can generate valuable and authentic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2308.07633</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#24778;&#20154;&#30340;&#25104;&#21151;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#20307;&#37327;&#21644;&#35745;&#31639;&#38656;&#27714;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#23454;&#38469;&#37096;&#32626;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#36825;&#20123;&#25361;&#25112;&#26085;&#30410;&#32039;&#36843;&#65292;&#27169;&#22411;&#21387;&#32553;&#39046;&#22495;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#19987;&#38376;&#38024;&#23545;LLMs&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#65292;&#20197;&#24212;&#23545;&#39640;&#25928;&#37096;&#32626;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22312;&#27599;&#31181;&#25216;&#26415;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;LLM&#30740;&#31350;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#29992;&#20110;&#35780;&#20272;&#25928;&#26524;&#30340;&#22522;&#20934;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of 
&lt;/p&gt;</description></item></channel></rss>