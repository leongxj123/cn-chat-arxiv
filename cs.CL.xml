<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>RigorLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21253;&#25324;&#33021;&#37327;&#25968;&#25454;&#22686;&#24378;&#12289;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#23433;&#20840;&#36755;&#20837;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13031</link><description>&lt;p&gt;
RigorLLM&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25269;&#24481;&#19981;&#33391;&#20869;&#23481;&#30340;&#40065;&#26834;&#38450;&#25252;&#26639;
&lt;/p&gt;
&lt;p&gt;
RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13031
&lt;/p&gt;
&lt;p&gt;
RigorLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21253;&#25324;&#33021;&#37327;&#25968;&#25454;&#22686;&#24378;&#12289;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#23433;&#20840;&#36755;&#20837;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#20013;&#20986;&#29616;&#30340;&#20559;&#35265;&#20197;&#21450;&#22312;&#24694;&#24847;&#36755;&#20837;&#19979;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#65292;&#37117;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#38450;&#25252;&#26639;&#65288;RigorLLM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#36890;&#36807;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#36827;&#34892;&#22522;&#20110;&#33021;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#12289;&#36890;&#36807;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38024;&#23545;&#36755;&#20837;&#20248;&#21270;&#23433;&#20840;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25105;&#20204;&#30340;&#25968;&#25454;&#22686;&#24378;&#23558;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#30340;&#22522;&#20110;&#34701;&#21512;&#30340;&#27169;&#22411;&#65292;RigorLLM&#20026;&#26377;&#23475;&#20869;&#23481;&#30340;&#35843;&#33410;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13031v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains. However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. Current mitigation strategies, while effective, are not resilient under adversarial attacks. This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation. Our experimental evalua
&lt;/p&gt;</description></item><item><title>MCFEND&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#22810;&#28304;&#26032;&#38395;&#25968;&#25454;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.09092</link><description>&lt;p&gt;
MCFEND&#65306;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09092
&lt;/p&gt;
&lt;p&gt;
MCFEND&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#22810;&#28304;&#26032;&#38395;&#25968;&#25454;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#26032;&#38395;&#22312;&#21508;&#20010;&#22312;&#32447;&#26469;&#28304;&#30340;&#26222;&#36941;&#20256;&#25773;&#23545;&#20844;&#20247;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#25968;&#25454;&#38598;&#20165;&#38480;&#20110;&#26469;&#33258;&#24494;&#21338;&#30340;&#26032;&#38395;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#34394;&#20551;&#26032;&#38395;&#22312;&#20869;&#23481;&#21644;&#31038;&#20250;&#32972;&#26223;&#31561;&#21508;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#22810;&#26679;&#24615;&#12290;&#20165;&#22312;&#21333;&#19968;&#26032;&#38395;&#26469;&#28304;&#19978;&#35757;&#32451;&#30340;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#36866;&#29992;&#20110;&#29616;&#23454;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#23398;&#20064;&#33258;&#19968;&#20010;&#22823;&#22411;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#25968;&#25454;&#38598;Weibo-21&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;F1&#20998;&#25968;&#65292;&#24403;&#27979;&#35797;&#25968;&#25454;&#25913;&#21464;&#20026;&#22810;&#28304;&#26032;&#38395;&#25968;&#25454;&#26102;&#65292;&#20174;0.943&#24613;&#21095;&#19979;&#38477;&#21040;0.470&#65292;&#26410;&#33021;&#35782;&#21035;&#36229;&#36807;&#19977;&#20998;&#20043;&#19968;&#30340;&#22810;&#28304;&#34394;&#20551;&#26032;&#38395;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;MCFEND&#65292;&#30001;&#25105;&#20204;&#20174;&#21508;&#31181;&#26469;&#28304;&#25910;&#38598;&#30340;&#26032;&#38395;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09092v1 Announce Type: cross  Abstract: The prevalence of fake news across various online sources has had a significant influence on the public. Existing Chinese fake news detection datasets are limited to news sourced solely from Weibo. However, fake news originating from multiple sources exhibits diversity in various aspects, including its content and social context. Methods trained on purely one single news source can hardly be applicable to real-world scenarios. Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese fake news detection dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source fake news. To address this limitation, we constructed the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND, which is composed of news we collected from diverse sources suc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#30784;&#27169;&#22411;API&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Aug-PE&#30340;&#22686;&#24378;PE&#31639;&#27861;&#65292;&#20197;&#20135;&#29983;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#65292;&#20026;&#35299;&#20915;&#31169;&#26377;&#25991;&#26412;&#25968;&#25454;&#20849;&#20139;&#19982;&#38544;&#31169;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21069;&#26223;&#21644;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.01749</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#30784;&#27169;&#22411;API&#29983;&#25104;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;2&#65306;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Synthetic Data via Foundation Model APIs 2: Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01749
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#30784;&#27169;&#22411;API&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Aug-PE&#30340;&#22686;&#24378;PE&#31639;&#27861;&#65292;&#20197;&#20135;&#29983;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#65292;&#20026;&#35299;&#20915;&#31169;&#26377;&#25991;&#26412;&#25968;&#25454;&#20849;&#20139;&#19982;&#38544;&#31169;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21069;&#26223;&#21644;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01749v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25277;&#35937;&#65306;&#30001;&#20110;&#23398;&#20064;&#31639;&#27861;&#30340;&#20986;&#29616;&#65292;&#25991;&#26412;&#25968;&#25454;&#21464;&#24471;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#20135;&#29983;&#30340;&#35768;&#22810;&#39640;&#36136;&#37327;&#25991;&#26412;&#25968;&#25454;&#26159;&#31169;&#23494;&#30340;&#65292;&#22240;&#27492;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#26080;&#27861;&#33258;&#30001;&#20849;&#20139;&#25110;&#20351;&#29992;&#12290;&#29983;&#25104;&#20855;&#26377;&#24418;&#24335;&#38544;&#31169;&#20445;&#35777;&#65288;&#21363;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#65289;&#30340;&#31169;&#23494;&#25991;&#26412;&#25968;&#25454;&#30340;&#21512;&#25104;&#21103;&#26412;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;DP&#24494;&#35843;&#65292;&#20197;&#29983;&#25104;DP&#21512;&#25104;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#19987;&#26377;LLM&#65288;&#20363;&#22914;GPT-3.5&#65289;&#24182;&#19981;&#21487;&#34892;&#65292;&#32780;&#19988;&#23545;&#20110;&#24320;&#28304;LLM&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;Lin&#31561;&#20154;&#65288;2024&#65289;&#26368;&#36817;&#24341;&#20837;&#20102;&#31169;&#26377;&#36827;&#21270;&#65288;PE&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21482;&#36890;&#36807;API&#35775;&#38382;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;PE&#31639;&#27861;&#65292;&#21517;&#20026;Aug-PE&#65292;&#36866;&#29992;&#20110;&#25991;&#26412;&#30340;&#22797;&#26434;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01749v1 Announce Type: new  Abstract: Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#20462;&#21098;&#31070;&#32463;&#20803;&#26469;&#23454;&#29616;&#21435;&#23398;&#20064;&#65292;&#21457;&#29616;LLMs&#20013;&#30340;&#31070;&#32463;&#20803;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01267</link><description>&lt;p&gt;
&#35299;&#21078;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#36873;&#25321;&#24615;&#20462;&#21098;&#23454;&#29616;&#26426;&#22120;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dissecting Language Models: Machine Unlearning via Selective Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01267
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#20462;&#21098;&#31070;&#32463;&#20803;&#26469;&#23454;&#29616;&#21435;&#23398;&#20064;&#65292;&#21457;&#29616;LLMs&#20013;&#30340;&#31070;&#32463;&#20803;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35774;&#35745;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#36873;&#25321;&#24615;&#20462;&#21098;&#26041;&#27861;&#65292;&#26681;&#25454;&#31070;&#32463;&#20803;&#23545;&#29305;&#23450;&#33021;&#21147;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#26469;&#31227;&#38500;&#31070;&#32463;&#20803;&#65292;&#32780;&#38750;&#25972;&#20307;&#32593;&#32476;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21024;&#38500;&#33021;&#22815;&#23454;&#29616;&#29305;&#23450;&#34892;&#20026;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#20013;&#30340;&#21069;&#39304;&#31070;&#32463;&#20803;&#21644;&#27880;&#24847;&#21147;&#31070;&#32463;&#20803;&#26159;&#19987;&#38376;&#21270;&#30340;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#23545;&#20110;&#29305;&#23450;&#20219;&#21153;&#65292;&#26576;&#20123;&#31070;&#32463;&#20803;&#27604;&#20854;&#20182;&#31070;&#32463;&#20803;&#26356;&#20026;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01267v1 Announce Type: cross  Abstract: Understanding and shaping the behaviour of Large Language Models (LLMs) is increasingly important as applications become more powerful and more frequently adopted. This paper introduces a machine unlearning method specifically designed for LLMs. We introduce a selective pruning method for LLMs that removes neurons based on their relative importance on a targeted capability compared to overall network performance. This approach is a compute- and data-efficient method for identifying and removing neurons that enable specific behaviours. Our findings reveal that both feed-forward and attention neurons in LLMs are specialized; that is, for specific tasks, certain neurons are more crucial than others.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;&#26694;&#26550;GenTKGQA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#21363;&#23376;&#22270;&#26816;&#32034;&#21644;&#31572;&#26696;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.16568</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20004;&#38454;&#27573;&#29983;&#25104;&#24335;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16568
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;&#26694;&#26550;GenTKGQA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#21363;&#23376;&#22270;&#26816;&#32034;&#21644;&#31572;&#26696;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;(TKGQA)&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#20219;&#21153;&#65292;&#22240;&#20026;&#38382;&#39064;&#20013;&#38544;&#34255;&#30528;&#26102;&#38388;&#32422;&#26463;&#65292;&#24182;&#19988;&#20174;&#21160;&#24577;&#32467;&#26500;&#21270;&#30693;&#35782;&#20013;&#23547;&#25214;&#31572;&#26696;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;TKGQA&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;&#26694;&#26550;GenTKGQA&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#24341;&#23548;LLMs&#22238;&#31572;&#26102;&#38388;&#24615;&#38382;&#39064;&#65306;&#23376;&#22270;&#26816;&#32034;&#21644;&#31572;&#26696;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#30340;&#22266;&#26377;&#30693;&#35782;&#26469;&#25366;&#25496;&#38382;&#39064;&#20013;&#30340;&#26102;&#38388;&#32422;&#26463;&#21644;&#32467;&#26500;&#38142;&#25509;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#20174;&#32780;&#32553;&#23567;&#20102;&#22312;&#26102;&#38388;&#21644;&#32467;&#26500;&#32500;&#24230;&#19978;&#30340;&#23376;&#22270;&#25628;&#32034;&#31354;&#38388;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#34394;&#25311;&#30693;&#35782;&#25351;&#31034;&#22120;&#26469;&#34701;&#21512;&#23376;&#22270;&#21644;&#25991;&#26412;&#34920;&#31034;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16568v1 Announce Type: new  Abstract: Temporal knowledge graph question answering (TKGQA) poses a significant challenge task, due to the temporal constraints hidden in questions and the answers sought from dynamic structured knowledge. Although large language models (LLMs) have made considerable progress in their reasoning ability over structured data, their application to the TKGQA task is a relatively unexplored area. This paper first proposes a novel generative temporal knowledge graph question answering framework, GenTKGQA, which guides LLMs to answer temporal questions through two phases: Subgraph Retrieval and Answer Generation. First, we exploit LLM's intrinsic knowledge to mine temporal constraints and structural links in the questions without extra training, thus narrowing down the subgraph search space in both temporal and structural dimensions. Next, we design virtual knowledge indicators to fuse the graph neural network signals of the subgraph and the text repres
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38750;&#23433;&#20840;&#20851;&#38190;&#30340;&#25112;&#30053;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#29615;&#22659;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHATATC&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#38598;&#23454;&#29616;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26597;&#35810;&#21644;&#21709;&#24212;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14850</link><description>&lt;p&gt;
CHATATC&#65306;&#29992;&#20110;&#25903;&#25345;&#25112;&#30053;&#31354;&#20013;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38750;&#23433;&#20840;&#20851;&#38190;&#30340;&#25112;&#30053;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#29615;&#22659;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHATATC&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#38598;&#23454;&#29616;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26597;&#35810;&#21644;&#21709;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#36890;&#36807;&#35832;&#22914;ChatGPT&#31561;&#20844;&#24320;&#21487;&#29992;&#24037;&#20855;&#24555;&#36895;&#36208;&#32418;&#12290;LLMs&#22312;&#20010;&#20154;&#21644;&#19987;&#19994;&#39046;&#22495;&#30340;&#24212;&#29992;&#24471;&#21040;&#25512;&#21160;&#65292;&#26159;&#30001;&#20110;&#20154;&#31867;&#29992;&#25143;&#19982;ChatGPT&#31561;&#35745;&#31639;&#26426;&#24212;&#29992;&#20043;&#38388;&#33258;&#28982;&#30340;&#20114;&#21160;&#65292;&#20197;&#21450;&#24378;&#22823;&#30340;&#25688;&#35201;&#21644;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#29983;&#25104;AI&#24037;&#20855;&#22914;&#20309;&#22312;&#38750;&#23433;&#20840;&#20851;&#38190;&#30340;&#25112;&#30053;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#29615;&#22659;&#20013;&#37096;&#32626;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;&#21253;&#21547;&#36229;&#36807;80,000&#20010;GDP&#23454;&#26045;&#12289;&#20462;&#35746;&#21644;&#21462;&#28040;&#30340;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#38598;&#65292;&#23545;CHATATC&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;CHATATC&#30340;&#26597;&#35810;&#21644;&#21709;&#24212;&#33021;&#21147;&#65292;&#35760;&#24405;&#20102;&#25104;&#21151;&#20043;&#22788;&#65288;&#20363;&#22914;&#65292;&#25552;&#20379;&#27491;&#30830;&#30340;GDP&#29575;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#21407;&#22240;&#65289;&#20197;&#21450;&#19981;&#36275;&#20043;&#22788;&#65288;&#20363;&#22914;&#65292;&#26368;&#39640;&#27700;&#24179;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14850v1 Announce Type: cross  Abstract: Generative artificial intelligence (AI) and large language models (LLMs) have gained rapid popularity through publicly available tools such as ChatGPT. The adoption of LLMs for personal and professional use is fueled by the natural interactions between human users and computer applications such as ChatGPT, along with powerful summarization and text generation capabilities. Given the widespread use of such generative AI tools, in this work we investigate how these tools can be deployed in a non-safety critical, strategic traffic flow management setting. Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations. We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;&#21313;&#31181;&#22823;&#23567;&#21464;&#20307;&#30340;&#22235;&#20010;&#24320;&#28304;MLLMs&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.14154</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14154
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;&#21313;&#31181;&#22823;&#23567;&#21464;&#20307;&#30340;&#22235;&#20010;&#24320;&#28304;MLLMs&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#22810;&#27169;&#24577;&#20449;&#24687;&#20132;&#27969;&#30340;&#20013;&#24515;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#29255;&#21644;&#35270;&#39057;&#65292;&#36825;&#20351;&#24471;&#26426;&#22120;&#38590;&#20197;&#29702;&#35299;&#22312;&#32447;&#31354;&#38388;&#20013;&#20132;&#20114;&#25152;&#20851;&#32852;&#30340;&#20449;&#24687;&#25110;&#24773;&#32490;&#12290;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#31867;&#24773;&#32490;&#21644;&#35832;&#22914;&#34394;&#20551;&#20449;&#24687;&#31561;&#22797;&#26434;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;MLLMs&#23545;&#22810;&#27169;&#24577;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;MM-Soc&#25972;&#21512;&#20102;&#33879;&#21517;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#34701;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;YouTube&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#38024;&#23545;&#20174;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#12289;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21040;&#31038;&#20132;&#19978;&#19979;&#25991;&#29983;&#25104;&#31561;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#24320;&#28304;MLLMs&#30340;&#21313;&#31181;&#19981;&#21516;&#35268;&#27169;&#21464;&#20307;&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20984;&#26174;&#20986;&#20102;&#23545;&#24615;&#33021;&#24179;&#34913;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14154v1 Announce Type: new  Abstract: Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2402.13463</link><description>&lt;p&gt;
RefuteBench&#65306;&#35780;&#20272;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39539;&#25351;&#20196;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#26085;&#30410;&#25193;&#22823;&#12290;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#26681;&#25454;&#27169;&#22411;&#30340;&#36755;&#20986;&#25552;&#20379;&#21453;&#39304;&#65292;&#24076;&#26395;&#24471;&#21040;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#30340;&#21453;&#39304;&#23436;&#25104;&#21709;&#24212;&#30340;&#21709;&#24212;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33021;&#21542;&#24688;&#24403;&#22320;&#21709;&#24212;&#29992;&#25143;&#30340;&#21453;&#39539;&#21453;&#39304;&#24182;&#22987;&#32456;&#25191;&#34892;&#19979;&#21435;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#20998;&#26512;&#12290;&#22522;&#20110;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;RefuteBench&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#38382;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#31561;&#20219;&#21153;&#12290;&#35780;&#20272;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#31215;&#26497;&#25509;&#21463;&#21453;&#39539;&#25351;&#20196;&#24418;&#24335;&#30340;&#21453;&#39304;&#65292;&#24182;&#26159;&#21542;&#33021;&#22815;&#22312;&#23545;&#35805;&#20013;&#22987;&#32456;&#36981;&#24490;&#29992;&#25143;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;&#20247;&#22810;LLMs&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;LLMs&#20542;&#21521;&#22266;&#25191;&#65292;&#21363;&#20542;&#21521;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#65292;&#32463;&#24120;&#26410;&#33021;&#36981;&#23432;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13463v1 Announce Type: cross  Abstract: The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the leng
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25552;&#31034;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25216;&#26415;GGPP&#12290;&#36890;&#36807;GGPP&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;LLMs&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#29305;&#23450;&#30340;&#38169;&#35823;&#31572;&#26696;&#65292;&#24182;&#24212;&#23545;&#25552;&#31034;&#20013;&#30340;&#26080;&#20851;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.07179</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#25552;&#31034;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25552;&#31034;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25216;&#26415;GGPP&#12290;&#36890;&#36807;GGPP&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;LLMs&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#29305;&#23450;&#30340;&#38169;&#35823;&#31572;&#26696;&#65292;&#24182;&#24212;&#23545;&#25552;&#31034;&#20013;&#30340;&#26080;&#20851;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#40065;&#26834;&#24615;&#22312;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#20351;&#29992;&#36805;&#36895;&#22686;&#38271;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#34987;&#35270;&#20026;&#25552;&#39640;&#20174;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;RAG-based LLMs&#30340;&#36755;&#20986;&#22914;&#20309;&#21463;&#21040;&#31245;&#26377;&#19981;&#21516;&#30340;&#36755;&#20837;&#24433;&#21709;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#25552;&#31034;&#20013;&#25554;&#20837;&#19968;&#20010;&#24456;&#30701;&#30340;&#21069;&#32512;&#20063;&#20250;&#23548;&#33268;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#20107;&#23454;&#27491;&#30830;&#31572;&#26696;&#30456;&#21435;&#29978;&#36828;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#36825;&#31867;&#21069;&#32512;&#23545;RAG&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Gradient Guided Prompt Perturbation&#65288;GGPP&#65289;&#30340;&#26032;&#22411;&#20248;&#21270;&#25216;&#26415;&#12290;GGPP&#22312;&#23558;RAG-based LLMs&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#29305;&#23450;&#38169;&#35823;&#31572;&#26696;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;&#23427;&#36824;&#21487;&#20197;&#24212;&#23545;&#25552;&#31034;&#20013;&#35831;&#27714;&#24573;&#30053;&#26080;&#20851;&#19978;&#19979;&#25991;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;LLMs&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;GGPP&#25200;&#21160;&#30340;&#25552;&#31034;&#20043;&#38388;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;&#24046;&#24322;&#26469;&#25552;&#20379;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.17505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
Arrows of Time for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#26041;&#21521;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#39044;&#27979;&#19979;&#19968;&#20010;&#35760;&#21495;&#21644;&#39044;&#27979;&#21069;&#19968;&#20010;&#35760;&#21495;&#26102;&#30340;&#24179;&#22343;&#23545;&#25968;&#22256;&#24785;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#26082;&#24494;&#22937;&#21448;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#26102;&#38388;&#31561;&#65289;&#19979;&#38750;&#24120;&#19968;&#33268;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#19981;&#24212;&#35813;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#22914;&#20309;&#20986;&#29616;&#22312;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#32771;&#34385;&#20013;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24102;&#26469;&#30340;&#19968;&#20123;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#32431;&#35821;&#35328;&#25688;&#35201;&#30340;&#25351;&#26631;&#27979;&#35797;&#24179;&#21488;APPLS&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;POMME&#26469;&#35780;&#20272;PLS&#20013;&#30340;&#25991;&#26412;&#31616;&#21270;&#12290;&#36890;&#36807;&#23545;&#25351;&#26631;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#25351;&#26631;&#26410;&#33021;&#22987;&#32456;&#25429;&#25417;&#21040;&#31616;&#21270;&#24230;&#12290;</title><link>https://arxiv.org/abs/2305.14341</link><description>&lt;p&gt;
APPLS: &#35780;&#20272;&#32431;&#35821;&#35328;&#25688;&#35201;&#30340;&#35780;&#20215;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
APPLS: Evaluating Evaluation Metrics for Plain Language Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#32431;&#35821;&#35328;&#25688;&#35201;&#30340;&#25351;&#26631;&#27979;&#35797;&#24179;&#21488;APPLS&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;POMME&#26469;&#35780;&#20272;PLS&#20013;&#30340;&#25991;&#26412;&#31616;&#21270;&#12290;&#36890;&#36807;&#23545;&#25351;&#26631;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#25351;&#26631;&#26410;&#33021;&#22987;&#32456;&#25429;&#25417;&#21040;&#31616;&#21270;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#20110;&#32431;&#35821;&#35328;&#25688;&#35201;&#65288;PLS&#65289;&#30340;&#27169;&#22411;&#26377;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#20294;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;PLS&#32570;&#20047;&#19987;&#38376;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#30001;&#20110;&#28041;&#21450;&#21040;&#29420;&#29305;&#30340;&#36716;&#25442;&#65288;&#20363;&#22914;&#65292;&#28155;&#21152;&#32972;&#26223;&#35299;&#37322;&#65292;&#21024;&#38500;&#19987;&#19994;&#26415;&#35821;&#65289;&#65292;&#22240;&#27492;&#23545;&#20110;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#30340;&#36866;&#29992;&#24615;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#20803;&#35780;&#20272;&#27979;&#35797;&#24179;&#21488;APPLS&#65292;&#26088;&#22312;&#35780;&#20272;PLS&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#26681;&#25454;&#20808;&#21069;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#23450;&#20041;&#20102;&#22235;&#20010;&#26631;&#20934;&#19978;&#30340;&#19968;&#32452;&#25200;&#21160;&#65292;PLS&#25351;&#26631;&#24212;&#35813;&#25429;&#25417;&#21040;&#65306;&#20449;&#24687;&#24615;&#12289;&#31616;&#21270;&#24230;&#12289;&#36830;&#36143;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#27979;&#35797;&#24179;&#21488;&#23545;&#25351;&#26631;&#36827;&#34892;&#20998;&#26512;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#25351;&#26631;&#26410;&#33021;&#22987;&#32456;&#25429;&#25417;&#21040;&#31616;&#21270;&#24230;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;POMME&#65292;&#26088;&#22312;&#35780;&#20272;PLS&#20013;&#25991;&#26412;&#31616;&#21270;&#65307;&#35813;&#25351;&#26631;&#26159;&#26681;&#25454;&#22495;&#20869;&#21644;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#26631;&#20934;&#21270;&#22256;&#24785;&#24230;&#24046;&#35745;&#31639;&#24471;&#21040;&#30340;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;POMME&#30340;&#25928;&#26524;&#65292;&#24182;&#19982;&#20854;&#20182;&#25351;&#26631;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there has been significant development of models for Plain Language Summarization (PLS), evaluation remains a challenge. PLS lacks a dedicated assessment metric, and the suitability of text generation evaluation metrics is unclear due to the unique transformations involved (e.g., adding background explanations, removing specialized terminology). To address these concerns, our study presents a granular meta-evaluation testbed, APPLS, designed to evaluate metrics for PLS. We define a set of perturbations along four criteria inspired by previous work that a PLS metric should capture: informativeness, simplification, coherence, and faithfulness. An analysis of metrics using our testbed reveals that current metrics fail to capture simplification consistently. In response, we introduce POMME, a new metric designed to assess text simplification in PLS; the metric is calculated as the normalized perplexity difference between an in-domain and out-of-domain language model. We demonstrate P
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#21253;&#25324;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.17432</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Video Understanding with Large Language Models: A Survey. (arXiv:2312.17432v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17432
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#21253;&#25324;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#35270;&#39057;&#24179;&#21488;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#19981;&#26029;&#22686;&#22810;&#65292;&#23545;&#29087;&#32451;&#30340;&#35270;&#39057;&#29702;&#35299;&#24037;&#20855;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#21152;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#25216;&#26415;&#36827;&#34892;&#35270;&#39057;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#20196;&#20154;&#24778;&#35766;&#65292;&#23588;&#20854;&#26159;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#26412;&#35843;&#26597;&#23545;Vid-LLMs&#30340;&#29420;&#29305;&#29305;&#28857;&#21644;&#33021;&#21147;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20998;&#20026;&#22235;&#31181;&#20027;&#35201;&#31867;&#22411;&#65306;&#22522;&#20110;LLM&#30340;&#35270;&#39057;&#20195;&#29702;&#12289;Vid-LLMs&#30340;&#39044;&#35757;&#32451;&#12289;Vid-LLMs&#30340;&#25351;&#20196;&#35843;&#25972;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#26597;&#23545;Vid-LLMs&#30340;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#21478;&#22806;&#65292;&#23427;&#36824;&#25506;&#35752;&#20102;Vid-LLMs&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of Large Language Models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of the recent advancements in video understanding harnessing the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into four main types: LLM-based Video Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods. Furthermore, this survey presents a comprehensive study of the tasks, datasets, and evaluation methodologies for Vid-LLMs. Additionally, it explores 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#36890;&#36807;&#26500;&#24314;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#21487;&#20197;&#32508;&#21512;&#20998;&#26512;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#20026;&#33647;&#29289;&#20877;&#23450;&#20301;&#30740;&#31350;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2309.03227</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#25581;&#31034;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#30340;&#25216;&#26415;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates. (arXiv:2309.03227v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#36890;&#36807;&#26500;&#24314;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#21487;&#20197;&#32508;&#21512;&#20998;&#26512;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#20026;&#33647;&#29289;&#20877;&#23450;&#20301;&#30740;&#31350;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20877;&#23450;&#20301;&#26159;&#19968;&#31181;&#21457;&#29616;&#29616;&#26377;&#33647;&#29289;&#26032;&#27835;&#30103;&#29992;&#36884;&#30340;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#36817;&#24180;&#26469;&#22312;&#35745;&#31639;&#31185;&#23398;&#25991;&#29486;&#20013;&#20351;&#29992;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#30340;&#25216;&#26415;&#28508;&#21147;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32508;&#21512;&#20998;&#26512;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#31561;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;s-BKG&#65289;&#65292;&#21253;&#25324;&#26469;&#33258;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30340;&#33647;&#29289;&#12289;&#30142;&#30149;&#21644;&#22522;&#22240;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35782;&#21035;&#22312;s-BKG&#20013;&#19982;&#30446;&#26631;&#30142;&#30149;&#20851;&#32852;&#26377;&#38480;&#20294;&#22312;&#31354;&#38388;&#19978;&#32039;&#23494;&#30456;&#37051;&#30340;&#33647;&#29289;&#20316;&#20026;&#28508;&#22312;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#33647;&#29289;&#19987;&#21033;&#20449;&#24687;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;p-BKG&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug repositioning-a promising strategy for discovering new therapeutic uses for existing drugs-has been increasingly explored in the computational science literature using biomedical databases. However, the technological potential of drug repositioning candidates has often been overlooked. This study presents a novel protocol to comprehensively analyse various sources such as pharmaceutical patents and biomedical databases, and identify drug repositioning candidates with both technological potential and scientific evidence. To this end, first, we constructed a scientific biomedical knowledge graph (s-BKG) comprising relationships between drugs, diseases, and genes derived from biomedical databases. Our protocol involves identifying drugs that exhibit limited association with the target disease but are closely located in the s-BKG, as potential drug candidates. We constructed a patent-informed biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information. Finally, we d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.12517</link><description>&lt;p&gt;
&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38024;&#23545;&#25991;&#26412;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#25351;&#20196;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#20110;&#22312;&#22823;&#35268;&#27169;&#25991;&#26723;&#38598;&#21512;&#20013;&#23450;&#20301;&#31526;&#21512;&#32473;&#23450;&#25551;&#36848;&#30340;&#25991;&#26412;&#65288;&#35821;&#20041;&#26816;&#32034;&#65289;&#24182;&#19981;&#36866;&#29992;&#12290;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#30340;&#30456;&#20284;&#24230;&#25628;&#32034;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#25191;&#34892;&#26816;&#32034;&#65292;&#20294;&#23884;&#20837;&#20013;&#30340;&#30456;&#20284;&#24230;&#23450;&#20041;&#19981;&#26126;&#30830;&#19988;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#23545;&#20110;&#35768;&#22810;&#29992;&#20363;&#26469;&#35828;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#37027;&#20040;&#65292;&#20160;&#20040;&#26159;&#26377;&#25928;&#26816;&#32034;&#30340;&#22909;&#30340;&#26597;&#35810;&#34920;&#31034;&#65311;&#25105;&#20204;&#30830;&#23450;&#20102;&#26681;&#25454;&#20869;&#23481;&#30340;&#25688;&#35201;&#25551;&#36848;&#26816;&#32034;&#21477;&#23376;&#30340;&#26126;&#30830;&#23450;&#20041;&#19988;&#19968;&#33268;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#25991;&#26412;&#23884;&#20837;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#27169;&#22411;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#36890;&#36807;&#25552;&#31034;LLM&#33719;&#24471;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#24456;&#23481;&#26131;&#20174;LLM&#20013;&#33719;&#24471;&#35757;&#32451;&#26448;&#26009;&#65292;&#20294;LLM&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval). Similarity search over embedding vectors does allow to perform retrieval by query, but the similarity reflected in the embedding is ill-defined and non-consistent, and is sub-optimal for many use cases. What, then, is a good query representation for effective retrieval?  We identify the well defined and consistent task of retrieving sentences based on abstract descriptions of their content. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM. While it is easy to source the training material from an LLM, the retrieval task cannot be performed by the LLM directly. This de
&lt;/p&gt;</description></item></channel></rss>