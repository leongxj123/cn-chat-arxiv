<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2404.02138</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Topic-based Watermarks for LLM-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#36755;&#20986;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#38590;&#20197;&#20998;&#36776;&#12290;&#27700;&#21360;&#31639;&#27861;&#26159;&#28508;&#22312;&#24037;&#20855;&#65292;&#36890;&#36807;&#22312;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#23884;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#21487;&#20197;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#22312;&#24050;&#30693;&#25915;&#20987;&#19979;&#32570;&#20047;&#20581;&#22766;&#24615;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;LLM&#27599;&#22825;&#29983;&#25104;&#25968;&#19975;&#20010;&#25991;&#26412;&#36755;&#20986;&#65292;&#27700;&#21360;&#31639;&#27861;&#38656;&#35201;&#35760;&#24518;&#27599;&#20010;&#36755;&#20986;&#25165;&#33021;&#35753;&#26816;&#27979;&#27491;&#24120;&#24037;&#20316;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;&#8220;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#8221;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
&lt;/p&gt;</description></item><item><title>IsoBench&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#19981;&#21516;&#21516;&#26500;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26356;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01266</link><description>&lt;p&gt;
IsoBench&#65306;&#22522;&#20110;&#21516;&#26500;&#34920;&#31034;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01266
&lt;/p&gt;
&lt;p&gt;
IsoBench&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#19981;&#21516;&#21516;&#26500;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26356;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#20165;&#25991;&#26412;&#25110;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#21516;&#26102;&#25552;&#31034;&#26102;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#26159;&#21542;&#20250;&#26681;&#25454;&#36755;&#20837;&#26041;&#24335;&#32780;&#25913;&#21464;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\textbf{IsoBench}$&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#30340;&#38382;&#39064;: &#25968;&#23398;&#12289;&#31185;&#23398;&#12289;&#31639;&#27861;&#21644;&#28216;&#25103;&#12290;&#27599;&#20010;&#31034;&#20363;&#21576;&#29616;&#20102;&#22810;&#20010;&#36755;&#20837;&#30340;&#21516;&#26500;&#34920;&#31034;&#65292;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#25968;&#23398;&#23637;&#31034;&#12290;IsoBench&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#21453;&#39304;&#65292;&#20197;&#35786;&#26029;&#30001;&#34920;&#31034;&#24418;&#24335;&#36896;&#25104;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#30456;&#21516;&#38382;&#39064;&#19978;&#65292;&#27169;&#22411;&#19968;&#36143;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;&#26368;&#31361;&#20986;&#30340;&#26159;&#65292;&#22312;&#25152;&#26377;IsoBench&#38382;&#39064;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;Claude-3 Opus&#22312;&#25552;&#20379;&#22270;&#20687;&#32780;&#19981;&#26159;&#25991;&#26412;&#26102;&#24615;&#33021;&#19979;&#38477;28.7&#20998;&#65307;&#21516;&#26679;&#65292;GPT-4 Turbo&#24615;&#33021;&#19979;&#38477;18.7&#20998;&#65292;Gemini Pro&#19979;&#38477;14.9&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01266v1 Announce Type: new  Abstract: Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#28041;&#21450;&#21040;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#31561;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.20329</link><description>&lt;p&gt;
ReALM: &#21442;&#32771;&#35299;&#26512;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
ReALM: Reference Resolution As Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#28041;&#21450;&#21040;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#31561;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#32771;&#35299;&#26512;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#29702;&#35299;&#21644;&#25104;&#21151;&#22788;&#29702;&#21508;&#31181;&#19978;&#19979;&#25991;&#33267;&#20851;&#37325;&#35201;&#12290; &#36825;&#31181;&#19978;&#19979;&#25991;&#26082;&#21253;&#25324;&#20808;&#21069;&#30340;&#23545;&#35805;&#65292;&#20063;&#21253;&#25324;&#19982;&#38750;&#23545;&#35805;&#23454;&#20307;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#29992;&#25143;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#25110;&#21518;&#21488;&#36816;&#34892;&#30340;&#23454;&#20307;&#12290; &#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#20204;&#22312;&#21442;&#32771;&#35299;&#26512;&#20013;&#30340;&#36816;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#23545;&#35805;&#23454;&#20307;&#65292;&#20173;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290; &#26412;&#25991;&#23637;&#31034;&#20102;LLMs&#22914;&#20309;&#34987;&#29992;&#26469;&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#20854;&#20013;&#28041;&#21450;&#23631;&#24149;&#19978;&#30340;&#36825;&#31181;&#23454;&#20307;&#31561;&#20256;&#32479;&#19978;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#21442;&#32771;&#35299;&#26512;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#31867;&#20284;&#21151;&#33021;&#30340;&#31995;&#32479;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20329v1 Announce Type: cross  Abstract: Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of re
&lt;/p&gt;</description></item><item><title>MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14624</link><description>&lt;p&gt;
MathVerse&#65306;&#24744;&#30340;&#22810;&#27169;&#24335;LLM&#26159;&#21542;&#30495;&#27491;&#30475;&#21040;&#20102;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#22270;&#34920;&#65311;
&lt;/p&gt;
&lt;p&gt;
MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14624
&lt;/p&gt;
&lt;p&gt;
MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#26410;&#20805;&#20998;&#35780;&#20272;&#21644;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#65292;&#23558;&#36807;&#22810;&#30340;&#35270;&#35273;&#20869;&#23481;&#34701;&#20837;&#25991;&#26412;&#38382;&#39064;&#20013;&#65292;&#36825;&#26377;&#21161;&#20110;MLLM&#22312;&#19981;&#30495;&#27491;&#35299;&#37322;&#36755;&#20837;&#22270;&#34920;&#30340;&#24773;&#20917;&#19979;&#25512;&#23548;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MathVerse&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#12290;&#25105;&#20204;&#31934;&#24515;&#25910;&#38598;&#20102;2,612&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#23398;&#31185;&#25968;&#23398;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#22270;&#34920;&#65292;&#26469;&#28304;&#20110;&#20844;&#24320;&#28192;&#36947;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#38382;&#39064;&#30001;&#20154;&#24037;&#27880;&#37322;&#32773;&#36716;&#21270;&#20026;&#20845;&#20010;&#19981;&#21516;&#29256;&#26412;&#65292;&#27599;&#20010;&#29256;&#26412;&#22312;&#22810;&#27169;&#24335;&#20013;&#25552;&#20379;&#19981;&#21516;&#31243;&#24230;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#20849;&#36129;&#29486;&#20102;15K&#20010;&#27979;&#35797;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;MathVerse&#33021;&#22815;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14624v1 Announce Type: cross  Abstract: The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to co
&lt;/p&gt;</description></item><item><title>TeaMs-RL&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30452;&#25509;&#29983;&#25104;&#22522;&#30784;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#20026;&#21333;&#19968;&#24494;&#35843;&#27493;&#39588;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.08694</link><description>&lt;p&gt;
TeaMs-RL&#65306;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25945;&#25480;LLMs&#26356;&#22909;&#22320;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08694
&lt;/p&gt;
&lt;p&gt;
TeaMs-RL&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30452;&#25509;&#29983;&#25104;&#22522;&#30784;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#20026;&#21333;&#19968;&#24494;&#35843;&#27493;&#39588;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#36890;&#24120;&#38754;&#20020;&#30528;&#22312;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#20013;&#23545;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#20005;&#37325;&#20381;&#36182;&#25110;&#19982;&#33258;&#25105;&#25351;&#23548;&#33539;&#24335;&#30456;&#20851;&#30340;&#39057;&#32321;&#19988;&#26114;&#36149;&#30340;&#22806;&#37096;&#26597;&#35810;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36716;&#21521;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;-- &#20294;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#20559;&#31163;&#20102;&#20856;&#22411;&#30340;RLHF&#65292;&#21518;&#32773;&#22312;&#25351;&#23548;&#25968;&#25454;&#35757;&#32451;&#21518;&#20248;&#21270;LLMs&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;RL&#30452;&#25509;&#29983;&#25104;&#21333;&#29420;&#36275;&#20197;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#30784;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;TeaMs-RL&#20351;&#29992;&#19968;&#31995;&#21015;&#25991;&#26412;&#25805;&#20316;&#21644;&#35268;&#21017;&#65292;&#20248;&#20808;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#21270;&#12290;&#23427;&#20419;&#36827;&#20102;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#32780;&#19981;&#36807;&#20110;&#20381;&#36182;&#22806;&#37096;&#20808;&#36827;&#27169;&#22411;&#65292;&#20026;&#21333;&#19968;&#24494;&#35843;&#27493;&#39588;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#28040;&#38500;&#20102;&#38543;&#21518;&#30340;RLHF&#38454;&#27573;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#65306;&#20943;&#23569;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08694v1 Announce Type: new  Abstract: The development of Large Language Models (LLMs) often confronts challenges stemming from the heavy reliance on human annotators in the reinforcement learning with human feedback (RLHF) framework, or the frequent and costly external queries tied to the self-instruct paradigm. In this work, we pivot to Reinforcement Learning (RL) -- but with a twist. Diverging from the typical RLHF, which refines LLMs following instruction data training, we use RL to directly generate the foundational instruction dataset that alone suffices for fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and rules, prioritizing the diversification of training datasets. It facilitates the generation of high-quality data without excessive reliance on external advanced models, paving the way for a single fine-tuning step and negating the need for subsequent RLHF stages. Our findings highlight key advantages of our approach: reduced need for human inv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02889</link><description>&lt;p&gt;
&#22312;&#23547;&#25214;&#30495;&#30456;&#65306;&#19968;&#31181;&#23457;&#38382;&#26041;&#27861;&#29992;&#20110;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
In Search of Truth: An Interrogation Approach to Hallucination Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02889
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#24182;&#19988;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#23427;&#20204;&#23545;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#24433;&#21709;&#21644;&#25972;&#21512;&#20173;&#28982;&#26377;&#38480;&#12290;&#19968;&#20010;&#38459;&#30861;&#23427;&#20204;&#24191;&#27867;&#24212;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#24187;&#35273;&#30340;&#21457;&#29983;&#65292;&#21363;LLMs&#21019;&#36896;&#20986;&#21548;&#36215;&#26469;&#30495;&#23454;&#20294;&#20559;&#31163;&#20107;&#23454;&#30495;&#30456;&#30340;&#31572;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#65292;&#36825;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;Llama-2&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26368;&#26032;LLMs&#30340;&#24187;&#35273;&#27700;&#24179;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#26816;&#27979;&#23427;&#20204;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#29305;&#23450;&#23454;&#39564;&#20013;&#35266;&#23519;&#21040;Llama-2&#36798;&#21040;62%&#30340;&#24187;&#35273;&#27700;&#24179;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27809;&#26377;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;87%&#30340;&#24179;&#34913;&#20934;&#30830;&#29575;&#65288;B-ACC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02889v1 Announce Type: new  Abstract: Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 62% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10892</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27700;&#21360;&#35777;&#26126;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;
&lt;/p&gt;
&lt;p&gt;
Proving membership in LLM pretraining data via data watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10892
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#30340;&#20316;&#21697;&#26159;&#21542;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#23454;&#29616;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#35775;&#38382;&#30340;&#21512;&#29702;&#26816;&#27979;&#65292;&#21069;&#25552;&#26159;&#29256;&#26435;&#25345;&#26377;&#20154;&#22312;&#20844;&#24320;&#21457;&#24067;&#20043;&#21069;&#36129;&#29486;&#20102;&#22810;&#20010;&#35757;&#32451;&#25991;&#26723;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#27700;&#21360;&#22788;&#29702;&#12290;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#25968;&#25454;&#27700;&#21360;&#65292;&#26816;&#27979;&#21487;&#20197;&#34987;&#26500;&#36896;&#20026;&#20551;&#35774;&#26816;&#39564;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#35823;&#26816;&#29575;&#30340;&#20445;&#35777;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#27700;&#21360;&#65306;&#19968;&#31181;&#25554;&#20837;&#38543;&#26426;&#24207;&#21015;&#65292;&#21478;&#19968;&#31181;&#38543;&#26426;&#29992;Unicode&#31867;&#20284;&#23383;&#31526;&#26367;&#25442;&#23383;&#31526;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;&#27700;&#21360;&#35774;&#35745;&#30340;&#19977;&#20010;&#26041;&#38754;--&#27700;&#21360;&#38271;&#24230;&#12289;&#22797;&#21046;&#27425;&#25968;&#21644;&#24178;&#25200;--&#22914;&#20309;&#24433;&#21709;&#20551;&#35774;&#26816;&#39564;&#30340;&#33021;&#21147;&#12290;&#25509;&#30528;&#30740;&#31350;&#20102;&#27700;&#21360;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#22914;&#20309;&#21464;&#21270;&#65306;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#20250;&#38477;&#20302;&#27700;&#21360;&#30340;&#24378;&#24230;&#65292;&#27700;&#21360;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10892v1 Announce Type: cross  Abstract: Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks
&lt;/p&gt;</description></item><item><title>G-SciEdBERT&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#24503;&#35821;&#31185;&#23398;&#25945;&#32946;BERT&#65292;&#29992;&#20110;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#20219;&#21153;&#30340;&#20070;&#38754;&#22238;&#31572;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#24503;&#35821;&#31185;&#23398;&#22238;&#31572;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#35780;&#20998;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;10%&#30340;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.06584</link><description>&lt;p&gt;
G-SciEdBERT: &#29992;&#20110;&#24503;&#35821;&#31185;&#23398;&#35780;&#20272;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06584
&lt;/p&gt;
&lt;p&gt;
G-SciEdBERT&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#24503;&#35821;&#31185;&#23398;&#25945;&#32946;BERT&#65292;&#29992;&#20110;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#20219;&#21153;&#30340;&#20070;&#38754;&#22238;&#31572;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#24503;&#35821;&#31185;&#23398;&#22238;&#31572;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#35780;&#20998;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;10%&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#20026;&#21508;&#31181;&#35821;&#35328;&#65288;&#20363;&#22914;&#24503;&#35821;&#20013;&#30340;&#24503;&#35821;BERT [G-BERT]&#65289;&#30340;&#33258;&#21160;&#35780;&#20998;&#31995;&#32479;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#33258;&#21160;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#38382;&#39064;&#30340;&#20070;&#38754;&#22238;&#31572;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26631;&#20934;&#30340;G-BERT&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#31185;&#23398;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#33021;&#19982;&#23398;&#29983;&#30340;&#20889;&#20316;&#39118;&#26684;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#24503;&#35821;&#31185;&#23398;&#25945;&#32946;BERT&#65288;G-SciEdBERT&#65289;&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#20219;&#21153;&#30340;&#20070;&#38754;&#22238;&#31572;&#12290;&#25105;&#20204;&#20351;&#29992;G-BERT&#65292;&#22312;5M&#20010;&#26631;&#35760;&#30340;PISA 2015&#22269;&#38469;&#23398;&#29983;&#35780;&#20272;&#30340;50K&#20010;&#24503;&#35821;&#31185;&#23398;&#22238;&#31572;&#35821;&#26009;&#24211;&#19978;&#23545;G-SciEdBERT&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;59&#20010;&#35780;&#20272;&#39033;&#30446;&#19978;&#23545;G-SciEdBERT&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#26816;&#26597;&#20102;&#35780;&#20998;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24615;&#33021;&#19982;G-BERT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;G-SciEdBERT&#22312;&#35780;&#20998;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#34920;&#26126;&#20854;&#35780;&#20998;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of natural language processing has paved the way for automated scoring systems in various languages, such as German (e.g., German BERT [G-BERT]). Automatically scoring written responses to science questions in German is a complex task and challenging for standard G-BERT as they lack contextual knowledge in the science domain and may be unaligned with student writing styles. This paper developed a contextualized German Science Education BERT (G-SciEdBERT), an innovative large language model tailored for scoring German-written responses to science tasks. Using G-BERT, we pre-trained G-SciEdBERT on a corpus of 50K German written science responses with 5M tokens to the Programme for International Student Assessment (PISA) 2015. We fine-tuned G-SciEdBERT on 59 assessment items and examined the scoring accuracy. We then compared its performance with G-BERT. Our findings reveal a substantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a 10% increase of quad
&lt;/p&gt;</description></item><item><title>CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2402.05374</link><description>&lt;p&gt;
CIC&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CIC: A framework for Culturally-aware Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05374
&lt;/p&gt;
&lt;p&gt;
CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VLPs&#65289;&#22914;BLIP&#20174;&#22270;&#20687;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#25152;&#25551;&#32472;&#30340;&#25991;&#21270;&#20803;&#32032;&#65288;&#20363;&#22914;&#20122;&#27954;&#25991;&#21270;&#32676;&#20307;&#30340;&#20256;&#32479;&#26381;&#35013;&#65289;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#24615;&#23383;&#24149;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;\textbf{&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#65288;CIC&#65289;}&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#20195;&#34920;&#19981;&#21516;&#25991;&#21270;&#30340;&#22270;&#20687;&#20013;&#29983;&#25104;&#23383;&#24149;&#24182;&#25551;&#36848;&#25991;&#21270;&#20803;&#32032;&#12290;&#21463;&#21040;&#23558;&#35270;&#35273;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#32452;&#21512;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#65288;1&#65289;&#26681;&#25454;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#38382;&#39064;&#20174;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#25552;&#21462;&#25991;&#21270;&#35270;&#35273;&#20803;&#32032;&#65292;&#65288;3&#65289;&#20351;&#29992;&#24102;&#26377;&#25552;&#31034;&#30340;LLMs&#29983;&#25104;&#25991;&#21270;&#24863;&#30693;&#23383;&#24149;&#12290;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#22823;&#23398;&#30340;45&#21517;&#21442;&#19982;&#32773;&#19978;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;UniMem&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#35760;&#24518;&#22686;&#24378;&#30340;&#35282;&#24230;&#37325;&#26032;&#21046;&#23450;&#20102;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;UniMix&#26469;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03009</link><description>&lt;p&gt;
UniMem&#65306;&#36808;&#21521;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
UniMem: Towards a Unified View of Long-Context Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;UniMem&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#35760;&#24518;&#22686;&#24378;&#30340;&#35282;&#24230;&#37325;&#26032;&#21046;&#23450;&#20102;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;UniMix&#26469;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#26159;&#38480;&#21046;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#33021;&#21147;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#34429;&#28982;&#23384;&#22312;&#21508;&#31181;&#33268;&#21147;&#20110;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#26159;&#23396;&#31435;&#22320;&#24320;&#21457;&#30340;&#65292;&#32570;&#20047;&#23545;&#23427;&#20204;&#30340;&#20248;&#28857;&#30340;&#31995;&#32479;&#20998;&#26512;&#21644;&#25972;&#21512;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UniMem&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20174;LLM&#30340;&#35760;&#24518;&#22686;&#24378;&#30340;&#35282;&#24230;&#37325;&#26032;&#21046;&#23450;&#20102;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#12290; UniMem&#30340;&#29305;&#28857;&#26159;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#20869;&#23384;&#31649;&#29702;&#65292;&#20869;&#23384;&#20889;&#20837;&#65292;&#20869;&#23384;&#35835;&#21462;&#21644;&#20869;&#23384;&#27880;&#20837;&#65292;&#20026;&#20102;&#29702;&#35299;&#21508;&#31181;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#25552;&#20379;&#20102;&#31995;&#32479;&#29702;&#35770;&#12290;&#25105;&#20204;&#22522;&#20110;UniMem&#37325;&#26032;&#21046;&#23450;&#20102;16&#31181;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;Transformer-XL&#65292;&#35760;&#24518;&#21270;Transformer&#65292;RMT&#21644;Longformer&#20013;&#30340;&#22235;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#23427;&#20204;&#30340;&#35774;&#35745;&#21407;&#21017;&#21644;&#20248;&#21183;&#12290;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UniMix&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-context processing is a critical ability that constrains the applicability of large language models. Although there exist various methods devoted to enhancing the long-context processing ability of large language models (LLMs), they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a unified framework that reformulates existing long-context methods from the view of memory augmentation of LLMs. UniMem is characterized by four key dimensions: Memory Management, Memory Writing, Memory Reading, and Memory Injection, providing a systematic theory for understanding various long-context methods. We reformulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an inn
&lt;/p&gt;</description></item><item><title>Bergeron&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#40774;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#21442;&#25968;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2312.00029</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#20934;&#26694;&#26550;&#25269;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00029
&lt;/p&gt;
&lt;p&gt;
Bergeron&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#40774;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#21442;&#25968;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#36827;&#23637;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#20195;&#23545;&#40784;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#23436;&#20840;&#38450;&#27490;&#22312;&#27169;&#22411;&#34987;&#33988;&#24847;&#25915;&#20987;&#26102;&#20135;&#29983;&#26377;&#23475;&#24212;&#23545;&#12290;&#20026;&#20102;&#24110;&#21161;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Bergeron&#65306;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;LLMs&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;&#65292;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#21442;&#25968;&#24494;&#35843;&#12290;Bergeron&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#65307;&#27425;&#35201;LLM&#27169;&#25311;&#21463;&#20445;&#25252;&#30340;&#20027;&#35201;LLM&#30340;&#33391;&#30693;&#12290;&#35813;&#26694;&#26550;&#22312;&#30417;&#35270;&#36755;&#20986;&#20197;&#26816;&#27979;&#20219;&#20309;&#26377;&#23475;&#20869;&#23481;&#30340;&#21516;&#26102;&#65292;&#26356;&#22909;&#22320;&#20445;&#25252;&#20027;&#35201;&#27169;&#22411;&#20813;&#21463;&#20837;&#20405;&#25915;&#20987;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#20351;&#29992;Bergeron&#26469;&#34917;&#20805;&#29616;&#26377;&#23545;&#40784;&#35757;&#32451;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00029v2 Announce Type: replace-cross  Abstract: Research into AI alignment has grown considerably since the recent introduction of increasingly capable Large Language Models (LLMs). Unfortunately, modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked. These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts. To help mitigate this issue, we introduce Bergeron: a framework designed to improve the robustness of LLMs against attacks without any additional parameter fine-tuning. Bergeron is organized into two tiers; with a secondary LLM emulating the conscience of a protected, primary LLM. This framework better safeguards the primary model against incoming attacks while monitoring its output for any harmful content. Empirical analysis shows that, by using Bergeron to complement models with existing alignment traini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.09101</link><description>&lt;p&gt;
&#26397;&#21521;&#22810;&#27493;&#25512;&#29702;&#30340;&#31572;&#26696;&#26657;&#20934;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified View of Answer Calibration for Multi-Step Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#25193;&#23637;&#20102;&#25913;&#36827;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#24120;&#23558;&#22810;&#27493;&#25512;&#29702;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#36335;&#24452;&#29983;&#25104;&#20197;&#29983;&#25104;&#25512;&#29702;&#36335;&#24452;&#65307;&#21644;&#31572;&#26696;&#26657;&#20934;&#21518;&#22788;&#29702;&#25512;&#29702;&#36335;&#24452;&#20197;&#33719;&#24471;&#26368;&#32456;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#32570;&#20047;&#23545;&#19981;&#21516;&#31572;&#26696;&#26657;&#20934;&#26041;&#27861;&#30340;&#31995;&#32479;&#20998;&#26512;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#22810;&#36335;&#24452;&#19978;&#30340;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21487;&#33021;&#21551;&#31034;&#20248;&#21270;&#22810;&#27493;&#25512;&#29702;&#31995;&#32479;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09101v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: path generation to generate the reasoning path(s); and answer calibration post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for opti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2305.13168</link><description>&lt;p&gt;
LLMs&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#65306;&#26368;&#26032;&#21151;&#33021;&#19982;&#26410;&#26469;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26500;&#24314;&#21644;&#25512;&#29702;&#20013;&#30340;&#25968;&#37327;&#21270;&#21644;&#36136;&#21270;&#35780;&#20272;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#37325;&#28857;&#20851;&#27880;&#28085;&#30422;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#38382;&#31572;&#22235;&#20010;&#20856;&#22411;&#20219;&#21153;&#65292;&#20174;&#32780;&#20840;&#38754;&#25506;&#32034;&#20102;LLMs&#22312;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#32463;&#39564;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;GPT-4&#20026;&#20195;&#34920;&#30340;LLMs&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#32780;&#19981;&#26159;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#34429;&#28982;GPT-4&#22312;&#19982;KG&#26500;&#24314;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#36824;&#25193;&#23637;&#21040;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#34394;&#25311;&#30693;&#35782;&#25552;&#21462;&#30340;&#26500;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#30340;&#26041;&#24335;&#65292;&#23558;&#30693;&#35782;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13444</link><description>&lt;p&gt;
&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65306;&#22522;&#20110;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption. (arXiv:2401.13444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#30340;&#26041;&#24335;&#65292;&#23558;&#30693;&#35782;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26356;&#26032;&#23427;&#20204;&#30340;&#30693;&#35782;&#38754;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#24403;&#38754;&#23545;&#19981;&#29087;&#24713;&#30340;&#26597;&#35810;&#26102;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#20102;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;LLMs&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23558;LLMs&#35270;&#20026;&#20027;&#35201;&#30340;&#20915;&#31574;&#32773;&#65292;&#23545;&#20854;&#33021;&#21147;&#25552;&#20986;&#20102;&#36739;&#39640;&#30340;&#35201;&#27714;&#12290;&#23545;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#19988;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#30340;LLMs&#26469;&#35828;&#65292;&#36825;&#26159;&#19981;&#22826;&#21512;&#36866;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#20026;&#26680;&#24515;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65288;CGPE&#65289;&#65292;&#23427;&#23558;&#30693;&#35782;&#24211;&#19982;LLMs&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#23545;&#27169;&#22411;&#30340;&#33021;&#21147;&#35201;&#27714;&#36739;&#20302;&#12290;&#21463;&#20154;&#31867;&#25163;&#21160;&#26816;&#32034;&#30693;&#35782;&#30340;&#26041;&#27861;&#21551;&#21457;&#65292;CGPE&#21033;&#29992;&#38382;&#39064;&#20013;&#30340;&#20449;&#24687;&#20316;&#20026;&#32447;&#32034;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#30693;&#35782;&#24211;&#20013;&#25152;&#38656;&#30340;&#30693;&#35782;&#36335;&#24452;&#12290;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CGPE&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#29992;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#19988;&#24615;&#33021;&#36739;&#24046;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, large language models (LLMs) have showcased remarkable capabilities. However, updating their knowledge poses challenges, potentially leading to inaccuracies when confronted with unfamiliar queries. While integrating knowledge graphs with LLMs has been explored, existing approaches treat LLMs as primary decision-makers, imposing high demands on their capabilities. This is particularly unsuitable for LLMs with lower computational costs and relatively poorer performance. In this paper, we introduce a Clue-Guided Path Exploration framework (CGPE) that efficiently merges a knowledge base with an LLM, placing less stringent requirements on the model's capabilities. Inspired by the method humans use to manually retrieve knowledge, CGPE employs information from the question as clues to systematically explore the required knowledge path within the knowledge base. Experiments on open-source datasets reveal that CGPE outperforms previous methods and is highly applicable to LLMs w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#25991;&#26412;&#22810;&#26679;&#24615;&#28608;&#21169;&#26041;&#27861;&#23545;LLM&#25991;&#26412;&#22686;&#24378;&#20013;&#29983;&#25104;&#25991;&#26412;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#31105;&#24524;&#35789;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20351;&#29992;&#20808;&#21069;&#21019;&#24314;&#30340;&#25913;&#20889;&#20316;&#20026;&#25552;&#31034;&#26102;&#65292;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.06643</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#28608;&#21169;&#23545;LLM&#25991;&#26412;&#22686;&#24378;&#20013;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation. (arXiv:2401.06643v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#25991;&#26412;&#22810;&#26679;&#24615;&#28608;&#21169;&#26041;&#27861;&#23545;LLM&#25991;&#26412;&#22686;&#24378;&#20013;&#29983;&#25104;&#25991;&#26412;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#31105;&#24524;&#35789;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20351;&#29992;&#20808;&#21069;&#21019;&#24314;&#30340;&#25913;&#20889;&#20316;&#20026;&#25552;&#31034;&#26102;&#65292;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25968;&#25454;&#22686;&#24378;&#20219;&#21153;&#20013;&#25214;&#21040;&#20102;&#24212;&#29992;&#65292;&#20854;&#20013;&#23569;&#37327;&#25991;&#26412;&#26679;&#26412;&#34987;LLM&#25913;&#20889;&#65292;&#28982;&#21518;&#29992;&#20110;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#19981;&#21516;&#25552;&#31034;&#12289;&#31181;&#23376;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#12289;&#36807;&#28388;&#26041;&#27861;&#25110;&#27169;&#22411;&#35774;&#32622;&#23545;&#25913;&#20889;&#25968;&#25454;&#65288;&#21644;&#19979;&#28216;&#27169;&#22411;&#65289;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#20247;&#21253;&#20013;&#24050;&#32463;&#24314;&#31435;&#33391;&#22909;&#30340;&#19977;&#31181;&#25991;&#26412;&#22810;&#26679;&#24615;&#28608;&#21169;&#26041;&#27861;&#65306;&#31105;&#24524;&#35789;&#12289;&#36890;&#36807;&#20808;&#21069;&#24322;&#24120;&#35299;&#30340;&#25552;&#31034;&#21644;&#36890;&#36807;&#20808;&#21069;&#24322;&#24120;&#35299;&#30340;&#38142;&#25509;&#12290;&#20351;&#29992;&#36825;&#20123;&#28608;&#21169;&#26041;&#27861;&#20316;&#20026;&#25351;&#23548;LLM&#22686;&#34917;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25506;&#27979;&#23427;&#20204;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;5&#31181;&#19981;&#21516;&#30340;LLM&#21644;6&#20010;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#31105;&#24524;&#35789;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20351;&#29992;&#20808;&#21069;&#21019;&#24314;&#30340;&#25913;&#20889;&#20316;&#20026;&#25552;&#31034;&#26102;&#65292;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune the model. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: taboo words, hints by previous outlier solutions, and chaining on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts' lexical diversity and downstream model performance. We compare the effects over 5 different LLMs and 6 datasets. We show that diversity is most increased by taboo words, while downstream model performance is highest when previously created paraphrases are used as hint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;33881&#20221;&#19987;&#21033;&#25991;&#20214;&#30340;&#26679;&#26412;&#65292;&#23558;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#38416;&#37322;&#20026;&#30693;&#35782;&#22270;&#35889;&#65292;&#20174;&#32780;&#25581;&#31034;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2312.06355</link><description>&lt;p&gt;
&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Linguistic and Structural Basis of Engineering Design Knowledge. (arXiv:2312.06355v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;33881&#20221;&#19987;&#21033;&#25991;&#20214;&#30340;&#26679;&#26412;&#65292;&#23558;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#38416;&#37322;&#20026;&#30693;&#35782;&#22270;&#35889;&#65292;&#20174;&#32780;&#25581;&#31034;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#21697;&#25551;&#36848;&#26159;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#20027;&#35201;&#36733;&#20307;&#65292;&#26082;&#26159;&#35774;&#35745;&#36807;&#31243;&#30340;&#20135;&#29289;&#65292;&#20063;&#26159;&#39537;&#21160;&#35774;&#35745;&#36807;&#31243;&#30340;&#22240;&#32032;&#12290;&#23613;&#31649;&#29289;&#21697;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#20869;&#28085;&#36827;&#34892;&#25551;&#36848;&#65292;&#20294;&#35774;&#35745;&#36807;&#31243;&#38656;&#35201;&#19968;&#31181;&#25551;&#36848;&#26469;&#20307;&#29616;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#65292;&#36825;&#36890;&#36807;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#23433;&#25490;&#22312;&#25991;&#26412;&#20013;&#34920;&#29616;&#20986;&#26469;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20174;&#21508;&#31181;&#25991;&#26412;&#20013;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#29983;&#25104;&#20307;&#29616;&#26126;&#30830;&#30340;&#24037;&#31243;&#35774;&#35745;&#20107;&#23454;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;&#26412;&#20307;&#35770;&#35774;&#35745;&#29702;&#35770;&#24456;&#23569;&#33021;&#25351;&#23548;&#30446;&#21069;&#20165;&#38480;&#20110;&#26500;&#24605;&#21644;&#23398;&#20064;&#30446;&#30340;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#20174;33881&#20221;&#19987;&#21033;&#25991;&#20214;&#30340;&#22823;&#26679;&#26412;&#20013;&#23558;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#38416;&#37322;&#20026;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#30740;&#31350;&#36825;&#20123;&#30693;&#35782;&#22270;&#35889;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#29702;&#35299;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artefact descriptions are the primary carriers of engineering design knowledge that is both an outcome and a driver of the design process. While an artefact could be described in different connotations, the design process requires a description to embody engineering design knowledge, which is expressed in the text through intricate placement of entities and relationships. As large-language models learn from all kinds of text merely as a sequence of characters/tokens, these are yet to generate text that embodies explicit engineering design facts. Existing ontological design theories are less likely to guide the large-language models whose applications are currently limited to ideation and learning purposes. In this article, we explicate engineering design knowledge as knowledge graphs from a large sample of 33,881 patent documents. We examine the constituents of these knowledge graphs to understand the linguistic and structural basis of engineering design knowledge. In terms of linguist
&lt;/p&gt;</description></item><item><title>ArcheType&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.18208</link><description>&lt;p&gt;
ArcheType&#65306;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. (arXiv:2310.18208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18208
&lt;/p&gt;
&lt;p&gt;
ArcheType&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35821;&#20041;&#21015;&#31867;&#22411;&#27880;&#37322;&#65288;CTA&#65289;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#32570;&#28857;&#65306;&#23427;&#20204;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#26102;&#22266;&#23450;&#30340;&#35821;&#20041;&#31867;&#22411;&#65307;&#38656;&#35201;&#22823;&#37327;&#30340;&#27599;&#20010;&#31867;&#22411;&#30340;&#35757;&#32451;&#26679;&#26412;&#24182;&#20135;&#29983;&#22823;&#37327;&#36816;&#34892;&#26102;&#25512;&#26029;&#25104;&#26412;&#65307;&#21363;&#20351;&#31867;&#22411;&#20445;&#25345;&#19981;&#21464;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#21487;&#33021;&#22312;&#35780;&#20272;&#26032;&#25968;&#25454;&#38598;&#26102;&#19979;&#38477;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;CTA&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ArcheType&#65292;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#37319;&#26679;&#12289;&#25552;&#31034;&#24207;&#21015;&#21270;&#12289;&#27169;&#22411;&#26597;&#35810;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#20174;&#32780;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#20840;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#35299;&#20915;&#21015;&#31867;&#22411;&#27880;&#37322;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#21035;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20986;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#25552;&#20379;&#20102;&#26368;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type and incur large run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve column type annotation problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes new state-of-the-art performance on both zero-shot and fine-tuned C
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15238</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#23398;&#20064;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Using Generated Privileged Information by Text-to-Image Diffusion Models. (arXiv:2309.15238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#26159;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#39069;&#22806;&#30340;&#25968;&#25454;&#34920;&#31034;&#20013;&#33719;&#30410;&#65292;&#36825;&#34987;&#31216;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#19981;&#30475;&#21040;&#39069;&#22806;&#34920;&#31034;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#21487;&#33719;&#24471;&#29305;&#26435;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20154;&#24037;&#29305;&#26435;&#20449;&#24687;&#12290;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#21407;&#22987;&#25991;&#26412;&#26679;&#26412;&#36827;&#19968;&#27493;&#29992;&#20110;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#26469;&#35757;&#32451;&#22810;&#27169;&#24577;&#25945;&#24072;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#22810;&#27169;&#24577;&#25945;&#24072;&#30340;&#30693;&#35782;&#34987;&#33976;&#39311;&#21040;&#22522;&#20110;&#25991;&#26412;&#30340;&#65288;&#21333;&#27169;&#24577;&#65289;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#31216;&#20026;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65288;LUGPI&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Using Privileged Information is a particular type of knowledge distillation where the teacher model benefits from an additional data representation during training, called privileged information, improving the student model, which does not see the extra representation. However, privileged information is rarely available in practice. To this end, we propose a text classification framework that harnesses text-to-image diffusion models to generate artificial privileged information. The generated images and the original text samples are further used to train multimodal teacher models based on state-of-the-art transformer-based architectures. Finally, the knowledge from multimodal teachers is distilled into a text-based (unimodal) student. Hence, by employing a generative model to produce synthetic data as privileged information, we guide the training of the student model. Our framework, called Learning Using Generated Privileged Information (LUGPI), yields noticeable performance g
&lt;/p&gt;</description></item><item><title>Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;&#65292;&#22312;&#20943;&#23567;&#27169;&#22411;&#38544;&#34255;&#22823;&#23567;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.16475</link><description>&lt;p&gt;
Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Transformer Compression via Subspace Projection. (arXiv:2308.16475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16475
&lt;/p&gt;
&lt;p&gt;
Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;&#65292;&#22312;&#20943;&#23567;&#27169;&#22411;&#38544;&#34255;&#22823;&#23567;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TCSP&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#30340;&#38544;&#34255;&#22823;&#23567;&#26469;&#21387;&#32553;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#25972;&#20010;&#36716;&#25442;&#27169;&#22411;&#25237;&#24433;&#21040;&#19968;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#30697;&#38453;&#19982;&#20943;&#23567;&#32500;&#24230;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#20010;&#23376;&#31354;&#38388;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#19981;&#21516;&#23618;&#27425;&#30340;&#37319;&#26679;&#25968;&#25454;&#23454;&#20363;&#30340;&#29305;&#24449;&#30697;&#38453;&#20998;&#35299;&#20026;&#19968;&#20010;&#25237;&#24433;&#30697;&#38453;&#12290;&#20026;&#20102;&#35780;&#20272;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#24212;&#29992;TCSP&#26469;&#21387;&#32553;T5&#21644;BERT&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TCSP&#22312;&#20445;&#35777;&#26368;&#22810;1.6%&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;44%&#30340;&#21387;&#32553;&#27604;&#65292;&#36229;&#36807;&#25110;&#32773;&#36798;&#21040;&#20102;&#20808;&#21069;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;TCSP&#36824;&#19982;&#20854;&#20182;&#30446;&#26631;&#36807;&#28388;&#22120;&#21644;&#27880;&#24847;&#21147;&#22836;&#22823;&#23567;&#21387;&#32553;&#30340;&#26041;&#27861;&#30456;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TCSP, a novel method for compressing a transformer model by focusing on reducing the hidden size of the model. By projecting the whole transform model into a subspace, we enable matrix operations between the weight matrices in the model and features in a reduced-dimensional space, leading to significant reductions in model parameters and computing resources. To establish this subspace, we decompose the feature matrix, derived from different layers of sampled data instances, into a projection matrix. For evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a compression ratio of 44\% with at most 1.6\% degradation in accuracy, surpassing or matching prior compression methods. Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.
&lt;/p&gt;</description></item><item><title>RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07922</link><description>&lt;p&gt;
RAVEN&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07922
&lt;/p&gt;
&lt;p&gt;
RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;ATLAS&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#39044;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#38271;&#24230;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RAVEN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#19978;&#19979;&#25991;&#31034;&#20363;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#27169;&#22411;&#20462;&#25913;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RAVEN&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#26126;&#26174;&#20248;&#20110;ATLAS&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;&#22810;&#35821;&#31181;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#21512;&#25104;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#32534;&#30721;&#30340;&#35821;&#38899;&#29305;&#24449;&#30340;&#37327;&#21270;&#34920;&#31034;&#35821;&#38899;&#38899;&#39057;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#20266;&#25991;&#26412;&#26469;&#24314;&#31435;&#32479;&#19968;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#21518;&#36890;&#36807;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#27169;&#22411;&#65292;&#21033;&#29992;&#21333;&#20803;&#21040;&#21333;&#20803;&#32763;&#35793;&#30446;&#26631;&#23558;&#21475;&#35821;&#35821;&#35328;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24314;&#31435;&#23545;&#21475;&#35821;&#35821;&#35328;&#30340;&#29702;&#35299;&#24182;&#23558;&#20854;&#30456;&#20851;&#32852;&#21040;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2308.01831</link><description>&lt;p&gt;
&#36890;&#36807;&#32479;&#19968;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#20197;&#21450;&#21333;&#20803;&#21040;&#21333;&#20803;&#30340;&#32763;&#35793;&#23454;&#29616;&#22810;&#23545;&#22810;&#21475;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Many-to-Many Spoken Language Translation via Unified Speech and Text Representation Learning with Unit-to-Unit Translation. (arXiv:2308.01831v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;&#22810;&#35821;&#31181;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#21512;&#25104;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#32534;&#30721;&#30340;&#35821;&#38899;&#29305;&#24449;&#30340;&#37327;&#21270;&#34920;&#31034;&#35821;&#38899;&#38899;&#39057;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#20266;&#25991;&#26412;&#26469;&#24314;&#31435;&#32479;&#19968;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#21518;&#36890;&#36807;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#27169;&#22411;&#65292;&#21033;&#29992;&#21333;&#20803;&#21040;&#21333;&#20803;&#32763;&#35793;&#30446;&#26631;&#23558;&#21475;&#35821;&#35821;&#35328;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24314;&#31435;&#23545;&#21475;&#35821;&#35821;&#35328;&#30340;&#29702;&#35299;&#24182;&#23558;&#20854;&#30456;&#20851;&#32852;&#21040;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#23398;&#20064;&#22810;&#35821;&#31181;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#32479;&#19968;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#35821;&#38899;&#21512;&#25104;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#38899;&#21333;&#20803;&#34920;&#31034;&#22810;&#35821;&#31181;&#35821;&#38899;&#38899;&#39057;&#65292;&#36825;&#20123;&#35821;&#38899;&#21333;&#20803;&#26159;&#20174;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#32534;&#30721;&#30340;&#35821;&#38899;&#29305;&#24449;&#30340;&#37327;&#21270;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#38899;&#39057;&#35270;&#20026;&#20266;&#25991;&#26412;&#24182;&#19987;&#27880;&#20110;&#20854;&#35821;&#35328;&#20869;&#23481;&#65292;&#20174;&#32780;&#26500;&#24314;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#32479;&#19968;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#35821;&#31181;&#25968;&#25454;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#21333;&#20803;&#21040;&#21333;&#20803;&#32763;&#35793;&#65288;UTUT&#65289;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23558;&#32534;&#30721;&#22120;&#19982;&#28304;&#35821;&#35328;&#26631;&#35760;&#21644;&#35299;&#30721;&#22120;&#19982;&#30446;&#26631;&#35821;&#35328;&#26631;&#35760;&#30456;&#20851;&#32852;&#65292;&#20248;&#21270;&#27169;&#22411;&#20197;&#23558;&#21475;&#35821;&#35821;&#35328;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24314;&#31435;&#23545;&#21475;&#35821;&#35821;&#35328;&#30340;&#29702;&#35299;&#20197;&#21450;&#22914;&#20309;&#23558;&#20854;&#19982;&#19981;&#21516;&#35821;&#35328;&#30456;&#20851;&#32852;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a method to learn unified representations of multilingual speech and text with a single model, especially focusing on the purpose of speech synthesis. We represent multilingual speech audio with speech units, the quantized representations of speech features encoded from a self-supervised speech model. Therefore, we can focus on their linguistic content by treating the audio as pseudo text and can build a unified representation of speech and text. Then, we propose to train an encoder-decoder structured model with a Unit-to-Unit Translation (UTUT) objective on multilingual data. Specifically, by conditioning the encoder with the source language token and the decoder with the target language token, the model is optimized to translate the spoken language into that of the target language, in a many-to-many language translation setting. Therefore, the model can build the knowledge of how spoken languages are comprehended and how to relate them to different languages
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LoraHub&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.13269</link><description>&lt;p&gt;
LoraHub: &#36890;&#36807;&#21160;&#24577;LoRA&#32452;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#20219;&#21153;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition. (arXiv:2307.13269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LoraHub&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24120;&#24120;&#34987;&#29992;&#20110;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24494;&#35843;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;LoraHub&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#30446;&#30340;&#24615;&#32452;&#35013;&#22312;&#19981;&#21516;&#32473;&#23450;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#30340;&#25112;&#30053;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#20165;&#20973;&#20511;&#26469;&#33258;&#26032;&#20219;&#21153;&#30340;&#20960;&#20010;&#31034;&#20363;&#65292;LoraHub&#21487;&#20197;&#28789;&#27963;&#22320;&#32452;&#21512;&#22810;&#20010;LoRA&#27169;&#22359;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#32452;&#21512;&#26082;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20063;&#19981;&#38656;&#35201;&#26799;&#24230;&#12290;&#25105;&#20204;&#20174;Big-Bench Hard&#65288;BBH&#65289;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#20986;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#22312;&#27599;&#20010;&#25512;&#29702;&#36755;&#20837;&#26049;&#36793;&#19981;&#38656;&#35201;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#22521;&#32946;&#19968;&#20010;LoRA&#31038;&#21306;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#20854;&#20013;&#20998;&#20139;&#20182;&#20204;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a strategic framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a novel task, LoraHub enables the fluid combination of multiple LoRA modules, eradicating the need for human expertise. Notably, the composition requires neither additional model parameters nor gradients. Our empirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest that LoraHub can effectively mimic the performance of in-context learning in few-shot scenarios, excluding the necessity of in-context examples alongside each inference input. A significant contribution of our research is the fostering of a community for LoRA, where users can share their trained LoRA module
&lt;/p&gt;</description></item><item><title>IncDSI&#26159;&#19968;&#31181;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#25913;&#21464;&#32593;&#32476;&#21442;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#36895;&#24230;&#65292;&#33021;&#22815;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2307.10323</link><description>&lt;p&gt;
IncDSI&#65306;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
IncDSI: Incrementally Updatable Document Retrieval. (arXiv:2307.10323v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10323
&lt;/p&gt;
&lt;p&gt;
IncDSI&#26159;&#19968;&#31181;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#25913;&#21464;&#32593;&#32476;&#21442;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#36895;&#24230;&#65292;&#33021;&#22815;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;iable&#25628;&#32034;&#32034;&#24341;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#25991;&#26723;&#26816;&#32034;&#33539;&#20363;&#65292;&#23427;&#23558;&#25991;&#26723;&#35821;&#26009;&#24211;&#30340;&#20449;&#24687;&#32534;&#30721;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#20013;&#65292;&#24182;&#30452;&#25509;&#23558;&#26597;&#35810;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#22312;&#35757;&#32451;&#27169;&#22411;&#20043;&#21518;&#28155;&#21152;&#26032;&#25991;&#26723;&#24182;&#19981;&#23481;&#26131;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IncDSI&#65292;&#19968;&#31181;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#30340;&#26041;&#27861;&#65288;&#27599;&#20010;&#25991;&#26723;&#32422;20-50&#27627;&#31186;&#65289;&#65292;&#32780;&#26080;&#38656;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#65288;&#29978;&#33267;&#37096;&#20998;&#25968;&#25454;&#38598;&#65289;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#28155;&#21152;&#25991;&#26723;&#30340;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#22312;&#32593;&#32476;&#21442;&#25968;&#19978;&#36827;&#34892;&#26368;&#23567;&#25913;&#21464;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#34429;&#28982;&#36895;&#24230;&#26356;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;IncDSI&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Differentiable Search Index is a recently proposed paradigm for document retrieval, that encodes information about a corpus of documents within the parameters of a neural network and directly maps queries to corresponding documents. These models have achieved state-of-the-art performances for document retrieval across many benchmarks. These kinds of models have a significant limitation: it is not easy to add new documents after a model is trained. We propose IncDSI, a method to add documents in real time (about 20-50ms per document), without retraining the model on the entire dataset (or even parts thereof). Instead we formulate the addition of documents as a constrained optimization problem that makes minimal changes to the network parameters. Although orders of magnitude faster, our approach is competitive with re-training the model on the whole dataset and enables the development of document retrieval systems that can be updated with new information in real-time. Our code for IncDSI
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20174;&#24418;&#24335;&#21270;&#30340;&#35282;&#24230;&#23545;Byte-Pair&#32534;&#30721;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36845;&#20195;&#36138;&#23146;&#29256;&#26412;&#26159;&#23545;&#26368;&#20248;&#21512;&#24182;&#24207;&#21015;&#30340;&#36817;&#20284;&#35299;&#65292;&#24182;&#20248;&#21270;&#20102;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.16837</link><description>&lt;p&gt;
Byte-Pair&#32534;&#30721;&#30340;&#24418;&#24335;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Formal Perspective on Byte-Pair Encoding. (arXiv:2306.16837v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20174;&#24418;&#24335;&#21270;&#30340;&#35282;&#24230;&#23545;Byte-Pair&#32534;&#30721;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36845;&#20195;&#36138;&#23146;&#29256;&#26412;&#26159;&#23545;&#26368;&#20248;&#21512;&#24182;&#24207;&#21015;&#30340;&#36817;&#20284;&#35299;&#65292;&#24182;&#20248;&#21270;&#20102;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Byte-Pair&#32534;&#30721;&#65288;BPE&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25968;&#25454;&#26631;&#35760;&#31639;&#27861;&#65292;&#23613;&#31649;&#26368;&#21021;&#26159;&#20316;&#20026;&#19968;&#31181;&#21387;&#32553;&#26041;&#27861;&#32780;&#35774;&#35745;&#30340;&#12290;BPE&#34920;&#38754;&#19978;&#30475;&#36215;&#26469;&#26159;&#19968;&#31181;&#36138;&#23146;&#31639;&#27861;&#65292;&#20294;&#26159;BPE&#23547;&#27714;&#35299;&#20915;&#30340;&#24213;&#23618;&#20248;&#21270;&#38382;&#39064;&#23578;&#26410;&#26126;&#30830;&#12290;&#25105;&#20204;&#23558;BPE&#24418;&#24335;&#21270;&#20026;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23376;&#27169;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36845;&#20195;&#36138;&#23146;&#29256;&#26412;&#26159;&#19968;&#20010;&#23545;&#20110;&#26368;&#20248;&#21512;&#24182;&#24207;&#21015;&#30340;$\frac{1}{{\sigma(\boldsymbol{\mu}^\star)}}(1-e^{-{\sigma(\boldsymbol{\mu}^\star)}})$-&#36817;&#20284;&#35299;&#65292;&#20854;&#20013;${\sigma(\boldsymbol{\mu}^\star)}$&#26159;&#30456;&#23545;&#20110;&#26368;&#20248;&#21512;&#24182;&#24207;&#21015;$\boldsymbol{\mu}^\star$&#30340;&#24635;&#21521;&#21518;&#26354;&#29575;&#12290;&#32463;&#39564;&#35777;&#36817;&#20284;&#35299;&#30340;&#19979;&#30028;&#32422;&#20026;$\approx 0.37$&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#24555;&#30340;BPE&#23454;&#29616;&#65292;&#23558;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$\mathcal{O}\left(N M\right)$&#20248;&#21270;&#20026;$\mathcal{O}\left(N \log M\right)$&#65292;&#20854;&#20013;$N$&#26159;&#24207;&#21015;&#38271;&#24230;&#65292;$M$&#26159;&#21512;&#24182;&#27425;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#26292;&#21147;&#25628;&#32034;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method. BPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down. We formalize BPE as a combinatorial optimization problem. Via submodular functions, we prove that the iterative greedy version is a $\frac{1}{{\sigma(\boldsymbol{\mu}^\star)}}(1-e^{-{\sigma(\boldsymbol{\mu}^\star)}})$-approximation of an optimal merge sequence, where ${\sigma(\boldsymbol{\mu}^\star)}$ is the total backward curvature with respect to the optimal merge sequence $\boldsymbol{\mu}^\star$. Empirically the lower bound of the approximation is $\approx 0.37$.  We provide a faster implementation of BPE which improves the runtime complexity from $\mathcal{O}\left(N M\right)$ to $\mathcal{O}\left(N \log M\right)$, where $N$ is the sequence length and $M$ is the merge count. Finally, we optimize the brute
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20294;&#26410;&#26469;&#21313;&#24180;&#38543;&#30528;&#38556;&#30861;&#34987;&#20811;&#26381;&#65292;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2303.07103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Could a Large Language Model be Conscious?. (arXiv:2303.07103v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20294;&#26410;&#26469;&#21313;&#24180;&#38543;&#30528;&#38556;&#30861;&#34987;&#20811;&#26381;&#65292;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26222;&#36941;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24863;&#30693;&#25110;&#24847;&#35782;&#12290;&#25105;&#20204;&#26159;&#21542;&#24212;&#35813;&#35748;&#30495;&#32771;&#34385;&#36825;&#20010;&#24819;&#27861;&#65311;&#26412;&#25991;&#23558;&#20998;&#26512;&#25903;&#25345;&#21644;&#21453;&#23545;&#36825;&#20010;&#24819;&#27861;&#30340;&#26368;&#26377;&#21147;&#30340;&#29702;&#30001;&#12290;&#26681;&#25454;&#24847;&#35782;&#31185;&#23398;&#20013;&#30340;&#20027;&#27969;&#20551;&#35774;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20363;&#22914;&#32570;&#20047;&#24490;&#29615;&#22788;&#29702;&#12289;&#20840;&#23616;&#30340;&#24037;&#20316;&#31354;&#38388;&#21644;&#32479;&#19968;&#30340;&#26234;&#33021;&#26426;&#26500;&#31561;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#38556;&#30861;&#22312;&#26410;&#26469;&#21313;&#24180;&#24038;&#21491;&#37117;&#21487;&#33021;&#34987;&#20811;&#26381;&#12290;&#20316;&#32773;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#34429;&#28982;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#36739;&#23567;&#65292;&#20294;&#25105;&#20204;&#24212;&#35813;&#35748;&#30495;&#32771;&#34385;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has recently been widespread discussion of whether large language models might be sentient or conscious. Should we take this idea seriously? I will break down the strongest reasons for and against. Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models: for example, their lack of recurrent processing, a global workspace, and unified agency. At the same time, it is quite possible that these obstacles will be overcome in the next decade or so. I conclude that while it is somewhat unlikely that current large language models are conscious, we should take seriously the possibility that successors to large language models may be conscious in the not-too-distant future.
&lt;/p&gt;</description></item></channel></rss>