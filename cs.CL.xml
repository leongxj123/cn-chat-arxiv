<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11322</link><description>&lt;p&gt;
&#20351;&#29992;StateFlow&#22686;&#24378;LLM&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#36890;&#36807;&#29366;&#24577;&#39537;&#21160;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#36235;&#21183;&#26085;&#30410;&#26126;&#26174;&#65292;&#20363;&#22914;&#38656;&#35201;&#19968;&#31995;&#21015;&#25805;&#20316;&#21644;&#19982;&#24037;&#20855;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;StateFlow&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#27714;&#35299;&#33539;&#24335;&#65292;&#23558;&#30001;LLM&#25903;&#25345;&#30340;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#12290;&#36890;&#36807;&#27491;&#30830;&#26500;&#24314;&#29366;&#24577;&#21644;&#23450;&#20041;&#29366;&#24577;&#36716;&#25442;&#65292;StateFlow&#30830;&#23450;&#20102;&#20219;&#21153;&#27714;&#35299;&#30340;&#36827;&#23637;&#65292;&#30830;&#20445;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;LLM&#22312;&#25972;&#20010;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#30340;&#21709;&#24212;&#12290;&#22312;&#27599;&#20010;&#29366;&#24577;&#20013;&#65292;StateFlow&#20801;&#35768;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#19981;&#20165;&#21253;&#25324;&#26681;&#25454;&#29305;&#23450;&#25552;&#31034;&#25351;&#23548;&#29983;&#25104;LLM&#21709;&#24212;&#65292;&#36824;&#21253;&#25324;&#26681;&#25454;&#38656;&#35201;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#12290;&#29366;&#24577;&#36716;&#25442;&#30001;LLM&#20570;&#20986;&#30340;&#29305;&#23450;&#35268;&#21017;&#25110;&#20915;&#31574;&#25511;&#21046;&#65292;&#20801;&#35768;&#36890;&#36807;&#20219;&#21153;&#30340;&#39044;&#23450;&#20041;StateFlow&#27169;&#22411;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#37327;&#21270;&#24615;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.08564</link><description>&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Non-discrimination Criteria for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#37327;&#21270;&#24615;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32463;&#21382;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#36234;&#26469;&#36234;&#26222;&#36941;&#22320;&#25552;&#20379;&#32473;&#20844;&#20247;&#20351;&#29992;&#65292;&#20154;&#20204;&#24320;&#22987;&#25285;&#24515;&#22312;&#24212;&#29992;&#20013;&#24310;&#32493;&#21644;&#25918;&#22823;&#26377;&#23475;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#21487;&#33021;&#23545;&#20854;&#38024;&#23545;&#30340;&#20010;&#20154;&#36896;&#25104;&#20260;&#23475;&#21644;&#38480;&#21046;&#65292;&#26080;&#35770;&#26159;&#30001;&#35823;&#20256;&#36824;&#26159;&#27495;&#35270;&#25152;&#26500;&#25104;&#12290;&#35782;&#21035;&#24615;&#21035;&#20559;&#35265;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#30340;&#31038;&#20250;&#26500;&#36896;&#65292;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21457;&#29616;&#21644;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19977;&#20010;&#26469;&#33258;&#20998;&#31867;&#30340;&#33879;&#21517;&#38750;&#27495;&#35270;&#26631;&#20934;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31867;&#27604;&#65292;&#21363;&#29420;&#31435;&#24615;&#12289;&#20998;&#31163;&#24615;&#21644;&#20805;&#20998;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#20123;&#26631;&#20934;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#27599;&#20010;&#26631;&#20934;&#30340;&#25552;&#31034;&#65292;&#37325;&#28857;&#20851;&#27880;&#32844;&#19994;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#20855;&#20307;&#21033;&#29992;&#21307;&#23398;&#27979;&#35797;&#26469;&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32972;&#26223;&#20013;&#24341;&#20837;&#22522;&#26412;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08564v1 Announce Type: cross  Abstract: Within recent years, generative AI, such as large language models, has undergone rapid development. As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications. Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination. Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in generative language models. In particular, we derive generative AI analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency. To demonstrate these criteria in action, we design prompts for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the generative AI context. 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.05720</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05720
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#25688;&#35201;&#26159;&#36890;&#36807;&#24635;&#32467;&#20020;&#24202;&#35760;&#24405;&#32780;&#29983;&#25104;&#30340;&#24120;&#35265;&#20020;&#24202;&#25991;&#20214;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#23454;&#38469;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#65288;&#22914;BHC&#21512;&#25104;&#65289;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#23637;&#31034;&#12290;&#20026;&#20102;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;BHC&#21512;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;MIMIC-IV&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#23553;&#35013;&#20102;&#20020;&#24202;&#35760;&#24405;&#21644;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#23545;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#36890;&#29992;LLMs&#21644;&#19977;&#20010;&#21307;&#30103;&#39046;&#22495;&#36866;&#24212;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#36827;&#20174;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;BHC&#12290;&#25105;&#20204;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;BHC&#65292;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#65288;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#21644;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#24212;&#29992;&#20110;&#19977;&#20010;&#24320;&#28304;LLMs&#65288;Clinical-T5-Large&#65292;Llama2-13B&#65292;FLAN-UL2&#65289;&#21644;&#20004;&#20010;&#19987;&#26377;LLMs&#65288;GPT-3.5&#65292;GPT-4&#65289;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#36866;&#37197;&#22120;&#65288;IVA&#65289;&#65292;&#29992;&#20110;&#22312;LLMs&#20013;&#22686;&#24378;&#23545;&#32454;&#31890;&#24230;&#35270;&#35273;&#20803;&#32032;&#30340;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#20102;&#38271;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#35270;&#35273;&#28165;&#26224;&#24230;&#38477;&#20302;&#21644;&#26080;&#20851;&#35270;&#35273;&#20196;&#29260;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13546</link><description>&lt;p&gt;
LLMs&#19982;&#38271;&#35270;&#39057;&#30456;&#36935;&#65306;&#22312;LLMs&#20013;&#21033;&#29992;&#20114;&#21160;&#24335;&#35270;&#35273;&#36866;&#37197;&#22120;&#25512;&#36827;&#38271;&#35270;&#39057;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13546
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#36866;&#37197;&#22120;&#65288;IVA&#65289;&#65292;&#29992;&#20110;&#22312;LLMs&#20013;&#22686;&#24378;&#23545;&#32454;&#31890;&#24230;&#35270;&#35273;&#20803;&#32032;&#30340;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#20102;&#38271;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#35270;&#35273;&#28165;&#26224;&#24230;&#38477;&#20302;&#21644;&#26080;&#20851;&#35270;&#35273;&#20196;&#29260;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#35270;&#39057;&#29702;&#35299;&#26159;&#22810;&#23186;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#20013;&#19968;&#39033;&#37325;&#35201;&#19988;&#25345;&#32493;&#25361;&#25112;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#29702;&#35299;&#35270;&#39057;&#25104;&#20026;&#19968;&#31181;&#26032;&#20852;&#19988;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#39057;&#20196;&#29260;&#25968;&#37327;&#24222;&#22823;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35270;&#35273;&#28165;&#26224;&#24230;&#38477;&#20302;&#65292;&#36824;&#38754;&#20020;&#30528;&#22312;&#22238;&#31572;&#35270;&#39057;&#30456;&#20851;&#38382;&#39064;&#26102;&#20986;&#29616;&#26080;&#20851;&#35270;&#35273;&#20196;&#29260;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#36866;&#37197;&#22120;(IVA)&#65292;&#26088;&#22312;&#22686;&#24378;&#19982;&#32454;&#31890;&#24230;&#35270;&#35273;&#20803;&#32032;&#30340;&#20132;&#20114;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#22240;&#26524;&#21464;&#25442;&#22120;&#23558;&#38271;&#35270;&#39057;&#36716;&#25442;&#20026;&#26102;&#38388;&#35270;&#39057;&#20196;&#29260;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#19982;&#35270;&#39057;&#35828;&#26126;&#19968;&#36215;&#36755;&#20837;LLMs&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#38598;&#25104;&#20102;IVA&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#26102;&#38388;&#24103;&#36873;&#25321;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13546v1 Announce Type: new  Abstract: Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MetaTree&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;</title><link>https://arxiv.org/abs/2402.03774</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#23398;&#20064;&#20915;&#31574;&#26641;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning a Decision Tree Algorithm with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MetaTree&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#22240;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#32780;&#38395;&#21517;&#12290;&#20256;&#32479;&#19978;&#65292;&#20915;&#31574;&#26641;&#26159;&#36890;&#36807;&#36882;&#24402;&#31639;&#27861;&#26500;&#24314;&#30340;&#65292;&#22312;&#26641;&#30340;&#27599;&#20010;&#33410;&#28857;&#19978;&#23558;&#25968;&#25454;&#36827;&#34892;&#20998;&#21306;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#26368;&#20339;&#20998;&#21306;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38024;&#23545;&#23616;&#37096;&#27573;&#20248;&#21270;&#30340;&#20915;&#31574;&#26641;&#21487;&#33021;&#26080;&#27861;&#24102;&#26469;&#20840;&#23616;&#27010;&#25324;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MetaTree&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36807;&#28388;&#36755;&#20986;&#26469;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#24378;&#22823;&#30340;&#20998;&#31867;&#20915;&#31574;&#26641;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#25311;&#21512;&#36138;&#23146;&#20915;&#31574;&#26641;&#21644;&#20248;&#21270;&#20915;&#31574;&#26641;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;MetaTree&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;&#36825;&#31181;&#35757;&#32451;&#20351;MetaTree&#19981;&#20165;&#21487;&#20197;&#27169;&#25311;&#36825;&#20123;&#31639;&#27861;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19978;&#19979;&#25991;&#26234;&#33021;&#22320;&#35843;&#25972;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#30340;&#27010;&#25324;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.
&lt;/p&gt;</description></item><item><title>DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02563</link><description>&lt;p&gt;
DefInt&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02563
&lt;/p&gt;
&lt;p&gt;
DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22914;&#36830;&#38145;&#25512;&#29702;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#26631;&#35760;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24040;&#22823;&#35299;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#23454;&#38469;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#29305;&#21035;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65288;DefInt&#65289;&#65292;&#20197;&#37322;&#25918;&#28151;&#21512;LLMs&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;DefInt&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;&#25512;&#29702;&#24605;&#36335;&#65292;&#31867;&#20284;&#20110;&#8220;&#31995;&#32479;1&#8221;&#20135;&#29983;&#30340;&#24555;&#36895;&#30452;&#35273;&#12290;&#22914;&#26524;&#36825;&#20123;&#30452;&#35273;&#34987;&#35748;&#20026;&#20302;&#32622;&#20449;&#24230;&#65292;&#21017;DefInt&#23558;&#35843;&#29992;&#25918;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#24605;&#25512;&#29702;&#20316;&#20026;&#8220;&#31995;&#32479;2&#8221;&#30340;&#24178;&#39044;&#65292;&#21487;&#20197;&#35206;&#30422;&#40664;&#35748;&#24605;&#32771;&#24182;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;DefInt&#35770;&#25991;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
&lt;/p&gt;</description></item><item><title>Monkey&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#35814;&#32454;&#30340;&#35270;&#35273;&#25429;&#25417;&#21644;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.06607</link><description>&lt;p&gt;
Monkey: &#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#25991;&#26412;&#26631;&#31614;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06607
&lt;/p&gt;
&lt;p&gt;
Monkey&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#35814;&#32454;&#30340;&#35270;&#35273;&#25429;&#25417;&#21644;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#22312;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#21644;&#35814;&#32454;&#22330;&#26223;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Monkey&#26469;&#22686;&#24378;LMM&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;Monkey&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#21010;&#20998;&#20026;&#32479;&#19968;&#30340;&#34917;&#19969;&#26469;&#22788;&#29702;&#22270;&#20687;&#65292;&#27599;&#20010;&#34917;&#19969;&#30340;&#22823;&#23567;&#19982;&#21407;&#26469;&#35757;&#32451;&#33391;&#22909;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20351;&#29992;&#30340;&#22823;&#23567;(&#20363;&#22914;448x448)&#30456;&#21305;&#37197;&#12290;&#37197;&#22791;&#20102;&#27599;&#20010;&#34917;&#19969;&#30340;&#36866;&#37197;&#22120;&#65292;Monkey&#21487;&#20197;&#22788;&#29702;&#39640;&#36798;1344x896&#20687;&#32032;&#30340;&#26356;&#39640;&#20998;&#36776;&#29575;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#35270;&#35273;&#20449;&#24687;&#30340;&#35814;&#32454;&#25429;&#25417;&#12290;&#20854;&#27425;&#65292;&#23427;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#65292;&#20016;&#23500;&#20102;&#22330;&#26223;-&#23545;&#35937;&#20851;&#32852;&#30340;&#19978;&#19979;&#25991;&#12290;&#36825;&#31181;&#20004;&#37096;&#20998;&#31574;&#30053;&#30830;&#20445;&#20102;&#20174;&#29983;&#25104;&#25968;&#25454;&#20013;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#65306;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#20801;&#35768;&#23545;&#35270;&#35273;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#25429;&#25417;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20840;&#38754;&#25551;&#36848;&#30340;&#25928;&#26524;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06607v3 Announce Type: replace-cross  Abstract: Large Multimodal Models (LMMs) have shown promise in vision-language tasks but struggle with high-resolution input and detailed scene understanding. Addressing these challenges, we introduce Monkey to enhance LMM capabilities. Firstly, Monkey processes input images by dividing them into uniform patches, each matching the size (e.g., 448x448) used in the original training of the well-trained vision encoder. Equipped with individual adapter for each patch, Monkey can handle higher resolutions up to 1344x896 pixels, enabling the detailed capture of complex visual information. Secondly, it employs a multi-level description generation method, enriching the context for scene-object associations. This two-part strategy ensures more effective learning from generated data: the higher resolution allows for a more detailed capture of visuals, which in turn enhances the effectiveness of comprehensive descriptions. Extensive ablative result
&lt;/p&gt;</description></item><item><title>LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#23637;&#31034;&#20986;&#20102;&#20851;&#38190;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#65292;&#36825;&#20026;&#24314;&#27169;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#22312;&#31454;&#20105;&#32972;&#26223;&#19979;&#30340;LLMs&#28508;&#21147;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2310.05746</link><description>&lt;p&gt;
&#35753;&#34892;&#21160;&#32988;&#20110;&#38596;&#36777;&#65306;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#20013;&#30340;&#25112;&#30053;&#35268;&#21010;&#19982;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05746
&lt;/p&gt;
&lt;p&gt;
LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#23637;&#31034;&#20986;&#20102;&#20851;&#38190;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#65292;&#36825;&#20026;&#24314;&#27169;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#22312;&#31454;&#20105;&#32972;&#26223;&#19979;&#30340;LLMs&#28508;&#21147;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#28982;&#32780;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#20110;&#38745;&#24577;&#22522;&#20934;&#12290;&#35780;&#20272;&#36825;&#19968;&#28857;&#38656;&#35201;&#27979;&#35797;&#25112;&#30053;&#25512;&#29702;&#33021;&#21147;&#30340;&#29615;&#22659;&#65292;&#36825;&#31181;&#29615;&#22659;&#38656;&#35201;&#22312;&#21160;&#24577;&#30340;&#31454;&#20105;&#22330;&#26223;&#20013;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AucArena&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#25311;&#25293;&#21334;&#30340;&#26032;&#39062;&#35780;&#20272;&#22871;&#20214;&#65292;&#36873;&#25321;&#36825;&#20010;&#35774;&#32622;&#26159;&#22240;&#20026;&#23427;&#38750;&#24120;&#19981;&#21487;&#39044;&#27979;&#65292;&#28041;&#21450;&#19982;&#36164;&#28304;&#21644;&#39118;&#38505;&#31649;&#29702;&#30456;&#20851;&#30340;&#35768;&#22810;&#25216;&#33021;&#65292;&#21516;&#26102;&#20063;&#26131;&#20110;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLM&#39537;&#21160;&#31454;&#26631;&#20195;&#29702;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#20182;&#20204;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;LLM&#20855;&#26377;&#25293;&#21334;&#21442;&#19982;&#30340;&#20851;&#38190;&#25216;&#33021;&#65292;&#22914;&#39044;&#31639;&#31649;&#29702;&#21644;&#30446;&#26631;&#36981;&#20174;&#65292;&#36825;&#20123;&#25216;&#33021;&#20250;&#38543;&#30528;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#25913;&#36827;&#32780;&#25552;&#39640;&#12290;&#36825;&#31361;&#20986;&#20102;LLM&#22312;&#24314;&#27169;&#31454;&#25216;&#32972;&#26223;&#19979;&#30340;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05746v2 Announce Type: replace-cross  Abstract: Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM p
&lt;/p&gt;</description></item><item><title>SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2401.13463</link><description>&lt;p&gt;
SpeechDPR: &#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering. (arXiv:2401.13463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13463
&lt;/p&gt;
&lt;p&gt;
SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#38382;&#31572;(SQA)&#26159;&#26426;&#22120;&#36890;&#36807;&#22312;&#32473;&#23450;&#21475;&#35821;&#27573;&#33853;&#20013;&#25214;&#21040;&#31572;&#26696;&#33539;&#22260;&#26469;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#36807;&#21435;&#30340;SQA&#26041;&#27861;&#27809;&#26377;&#20351;&#29992;ASR&#65292;&#20197;&#36991;&#20813;&#35782;&#21035;&#38169;&#35823;&#21644;&#35789;&#27719;&#22806;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#24320;&#25918;&#39046;&#22495;SQA(openSQA)&#38382;&#39064;&#20013;&#65292;&#26426;&#22120;&#38656;&#35201;&#39318;&#20808;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24050;&#30693;&#30340;&#29992;&#20110;openSQA&#38382;&#39064;&#26816;&#32034;&#32452;&#20214;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;SpeechDPR&#12290;SpeechDPR&#36890;&#36807;&#20174;&#26080;&#30417;&#30563;ASR(UASR)&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;(TDR)&#30340;&#32423;&#32852;&#27169;&#22411;&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#23398;&#20064;&#21477;&#23376;&#32423;&#35821;&#20041;&#34920;&#31034;&#12290;&#19981;&#38656;&#35201;&#25163;&#21160;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#32423;&#32852;&#30340;UASR&#21644;TDR&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#26174;&#33879;&#25552;&#39640;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robus
&lt;/p&gt;</description></item><item><title>TrustLLM&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#21487;&#20449;&#24615;&#21407;&#21017;&#30340;&#25552;&#20986;&#12289;&#24314;&#31435;&#22522;&#20934;&#30340;&#26041;&#27861;&#12289;&#35780;&#20272;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#25361;&#25112;&#30340;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.05561</link><description>&lt;p&gt;
TrustLLM: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
TrustLLM: Trustworthiness in Large Language Models. (arXiv:2401.05561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05561
&lt;/p&gt;
&lt;p&gt;
TrustLLM&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#21487;&#20449;&#24615;&#21407;&#21017;&#30340;&#25552;&#20986;&#12289;&#24314;&#31435;&#22522;&#20934;&#30340;&#26041;&#27861;&#12289;&#35780;&#20272;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#25361;&#25112;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#22312;&#21487;&#20449;&#24615;&#26041;&#38754;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;LLMs&#30340;&#21487;&#20449;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#35805;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TrustLLM&#65292;&#23427;&#26159;&#23545;LLMs&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#32500;&#24230;&#30340;&#21487;&#20449;&#24615;&#21407;&#21017;&#12289;&#24314;&#31435;&#22522;&#20934;&#12289;&#35780;&#20272;&#21644;&#20998;&#26512;&#20027;&#27969;LLMs&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#24320;&#25918;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#28085;&#30422;&#20843;&#20010;&#19981;&#21516;&#32500;&#24230;&#30340;&#21487;&#20449;LLMs&#21407;&#21017;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#19968;&#20010;&#36328;&#20845;&#20010;&#32500;&#24230;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#24615;&#21644;&#26426;&#22120;&#20262;&#29702;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;TrustLLM&#20013;&#23637;&#31034;&#20102;&#19968;&#20010;&#35780;&#20272;16&#20010;&#20027;&#27969;LLMs&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;30&#22810;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our find
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;GPT-4V&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20174;&#24120;&#35782;&#30693;&#35782;&#12289;&#32454;&#31890;&#24230;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#20840;&#38754;&#30693;&#35782;&#19982;&#20915;&#31574;&#29702;&#25454;&#19977;&#20010;&#26041;&#38754;&#23545;&#20854;&#33021;&#21147;&#36827;&#34892;&#20102;&#28145;&#20837;&#32771;&#23519;&#12290;</title><link>http://arxiv.org/abs/2311.07536</link><description>&lt;p&gt;
&#23545;GPT-4V&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering. (arXiv:2311.07536v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;GPT-4V&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20174;&#24120;&#35782;&#30693;&#35782;&#12289;&#32454;&#31890;&#24230;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#20840;&#38754;&#30693;&#35782;&#19982;&#20915;&#31574;&#29702;&#25454;&#19977;&#20010;&#26041;&#38754;&#23545;&#20854;&#33021;&#21147;&#36827;&#34892;&#20102;&#28145;&#20837;&#32771;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#27169;&#22411;&#65288;MLMs&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#35270;&#35273;&#29702;&#35299;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#39046;&#22495;&#25552;&#20379;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30495;&#27491;&#30340;&#25361;&#25112;&#22312;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;VQA&#20219;&#21153;&#65292;&#24182;&#19981;&#20165;&#38656;&#35201;&#35782;&#21035;&#35270;&#35273;&#20803;&#32032;&#65292;&#36824;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#35270;&#35273;&#20449;&#24687;&#24182;&#32467;&#21512;&#20016;&#23500;&#30340;&#23398;&#20064;&#30693;&#35782;&#24211;&#12290;&#20026;&#20102;&#25581;&#31034;MLMs&#29305;&#21035;&#26159;&#26032;&#24341;&#20837;&#30340;GPT-4V&#30340;&#36825;&#20123;&#33021;&#21147;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#65306;1&#65289;&#24120;&#35782;&#30693;&#35782;&#65292;&#35780;&#20272;&#27169;&#22411;&#29702;&#35299;&#35270;&#35273;&#32447;&#32034;&#24182;&#36830;&#25509;&#21040;&#36890;&#29992;&#30693;&#35782;&#30340;&#33021;&#21147;&#65307;2&#65289;&#32454;&#31890;&#24230;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#27979;&#35797;&#27169;&#22411;&#20174;&#22270;&#20687;&#20013;&#25512;&#29702;&#20986;&#20855;&#20307;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20854;&#22312;&#21508;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#65307;3&#65289;&#20840;&#38754;&#30693;&#35782;&#19982;&#20915;&#31574;&#29702;&#25454;&#65292;&#26816;&#26597;&#27169;&#22411;&#25552;&#20379;&#36923;&#36753;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of multimodal large models (MLMs) has significantly advanced the field of visual understanding, offering remarkable capabilities in the realm of visual question answering (VQA). Yet, the true challenge lies in the domain of knowledge-intensive VQA tasks, which necessitate not just recognition of visual elements, but also a deep comprehension of the visual information in conjunction with a vast repository of learned knowledge. To uncover such capabilities of MLMs, particularly the newly introduced GPT-4V, we provide an in-depth evaluation from three perspectives: 1) Commonsense Knowledge, which assesses how well models can understand visual cues and connect to general knowledge; 2) Fine-grained World Knowledge, which tests the model's skill in reasoning out specific knowledge from images, showcasing their proficiency across various specialized fields; 3) Comprehensive Knowledge with Decision-making Rationales, which examines model's capability to provide logical explanatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GPT-4&#39046;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#8220;&#36866;&#24212;-&#26816;&#32034;-&#20462;&#35746;&#8221;&#30340;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#29983;&#25104;&#20869;&#23481;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03328</link><description>&lt;p&gt;
&#25226;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#36866;&#24212;&#37325;&#26032;&#34920;&#36848;&#20026;&#36866;&#24212;-&#26816;&#32034;-&#20462;&#35746;
&lt;/p&gt;
&lt;p&gt;
Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise. (arXiv:2310.03328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GPT-4&#39046;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#8220;&#36866;&#24212;-&#26816;&#32034;-&#20462;&#35746;&#8221;&#30340;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#29983;&#25104;&#20869;&#23481;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#19968;&#33324;&#39046;&#22495;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#22312;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;&#20013;&#22269;&#27861;&#24459;&#65289;&#29983;&#25104;&#38169;&#35823;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#36825;&#36890;&#24120;&#26159;&#30001;&#20110;&#27809;&#26377;&#21253;&#21547;&#36825;&#26679;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#24471;GPT-4&#26080;&#27861;&#33719;&#21462;&#39046;&#22495;&#20869;&#30340;&#30693;&#35782;&#12290;&#19968;&#20010;&#32039;&#36843;&#30340;&#25361;&#25112;&#26159;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#19978;&#32487;&#32493;&#35757;&#32451;&#22914;&#27492;&#22823;&#35268;&#27169;&#30340;LLM&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#8220;&#36866;&#24212;-&#26816;&#32034;-&#20462;&#35746;&#8221;&#30340;&#36807;&#31243;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GPT-4&#39046;&#22495;&#36866;&#24212;&#26694;&#26550;&#12290;&#21021;&#22987;&#27493;&#39588;&#26159;&#36890;&#36807;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#19978;&#32487;&#32493;&#23398;&#20064;&#65292;&#23558;&#19968;&#20010;&#32463;&#27982;&#23454;&#24800;&#30340;7B LLM&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#35299;&#20915;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#36866;&#24212;&#30340;LLM&#26681;&#25454;&#20219;&#21153;&#26597;&#35810;&#29983;&#25104;&#19968;&#20010;&#21021;&#31295;&#31572;&#26696;&#12290;&#28982;&#21518;&#65292;&#21021;&#31295;&#31572;&#26696;&#23558;&#29992;&#20110;&#20174;&#22806;&#37096;&#26816;&#32034;&#25903;&#25345;&#35777;&#25454;&#30340;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) like GPT-4 have recently demonstrated astonishing zero-shot capabilities in general domain tasks, they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data.  This paper introduces a simple and effective domain adaptation framework for GPT-4 by reformulating generation as an \textbf{adapt-retrieve-revise} process. The initial step is to \textbf{adapt} an affordable 7B LLM to the target domain by continuing learning on in-domain data. When solving a task, we leverage the adapted LLM to generate a draft answer given a task query. Then, the draft answer will be used to \textbf{retrieve} supporting evidence candidates from an externa
&lt;/p&gt;</description></item><item><title>KLoB&#26159;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#20107;&#23454;&#30693;&#35782;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16535</link><description>&lt;p&gt;
KLoB: &#19968;&#31181;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models. (arXiv:2309.16535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16535
&lt;/p&gt;
&lt;p&gt;
KLoB&#26159;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#20107;&#23454;&#30693;&#35782;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23450;&#20301;&#28982;&#21518;&#32534;&#36753;&#30340;&#33539;&#24335;&#24050;&#32463;&#25104;&#20026;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23450;&#20301;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#22320;&#25214;&#21040;&#23884;&#20837;&#25152;&#38656;&#30693;&#35782;&#30340;&#30830;&#20999;&#21442;&#25968;&#36824;&#32570;&#20047;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#23545;&#20107;&#23454;&#30693;&#35782;&#30340;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#20294;&#27809;&#26377;&#25552;&#20379;&#19968;&#31181;&#27979;&#35797;&#20551;&#35774;&#30340;&#26041;&#27861;&#20197;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#35752;&#35770;&#21644;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KLoB&#65292;&#19968;&#20010;&#35780;&#20272;&#21487;&#38752;&#30340;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#24212;&#28385;&#36275;&#30340;&#19977;&#20010;&#22522;&#26412;&#23646;&#24615;&#30340;&#22522;&#20934;&#12290;KLoB&#21487;&#20316;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#24182;&#20026;&#37325;&#26032;&#35780;&#20272;&#20107;&#23454;&#30693;&#35782;&#30340;&#23616;&#37096;&#24615;&#20551;&#35774;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;\url{https://github.com/juyiming/KLoB}&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Locate-Then-Edit paradigm has emerged as one of the main approaches in changing factual knowledge stored in the Language models. However, there is a lack of research on whether present locating methods can pinpoint the exact parameters embedding the desired knowledge. Moreover, although many researchers have questioned the validity of locality hypothesis of factual knowledge, no method is provided to test the a hypothesis for more in-depth discussion and research. Therefore, we introduce KLoB, a benchmark examining three essential properties that a reliable knowledge locating method should satisfy. KLoB can serve as a benchmark for evaluating existing locating methods in language models, and can contributes a method to reassessing the validity of locality hypothesis of factual knowledge. Our is publicly available at \url{https://github.com/juyiming/KLoB}.
&lt;/p&gt;</description></item><item><title>Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15806</link><description>&lt;p&gt;
Lyra: &#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#30340;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15806
&lt;/p&gt;
&lt;p&gt;
Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#25506;&#32034;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#24187;&#35273;&#30340;&#20943;&#36731;&#21644;&#36890;&#36807;&#35777;&#26126;&#22120;&#38169;&#35823;&#28040;&#24687;&#30340;&#32454;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;LLMs&#22312;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Lyra&#65292;&#19968;&#31181;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#20462;&#27491;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65306;&#24037;&#20855;&#20462;&#27491;&#65288;TC&#65289;&#21644;&#29468;&#24819;&#20462;&#27491;&#65288;CC&#65289;&#12290;&#20026;&#20102;&#22312;&#24418;&#24335;&#35777;&#26126;&#30340;&#21518;&#22788;&#29702;&#20013;&#23454;&#29616;&#24037;&#20855;&#20462;&#27491;&#65292;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#35777;&#26126;&#24037;&#20855;&#65288;&#22914;Sledgehammer&#65289;&#26469;&#25351;&#23548;&#26367;&#25442;&#19981;&#27491;&#30830;&#30340;&#24037;&#20855;&#12290;&#24037;&#20855;&#20462;&#27491;&#26174;&#33879;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29468;&#24819;&#20462;&#27491;&#65292;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#26426;&#21046;&#65292;&#26088;&#22312;&#19982;&#35777;&#26126;&#22120;&#20114;&#21160;&#65292;&#36890;&#36807;&#35777;&#26126;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#36827;&#19968;&#27493;&#23436;&#21892;&#24418;&#24335;&#35777;&#26126;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13840</link><description>&lt;p&gt;
&#36229;&#36234;&#35268;&#27169;&#65306;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#35777;&#26126;&#20102;LLMs&#26159;&#22312;&#24418;&#24335;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;
&lt;/p&gt;
&lt;p&gt;
Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#39044;&#20808;&#35757;&#32451;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36235;&#21183;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#24378;&#22823;&#30340;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#23427;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#27010;&#24565;&#65292;&#23578;&#26410;&#23436;&#20840;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;Task2Vec&#22810;&#26679;&#24615;&#31995;&#25968;&#26469;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#24418;&#24335;&#26041;&#38754;&#65292;&#36229;&#36234;&#35268;&#27169;&#26412;&#36523;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30340;&#24418;&#24335;&#22810;&#26679;&#24615;&#39640;&#20110;&#29702;&#35770;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#31435;&#23545;&#22810;&#26679;&#24615;&#31995;&#25968;&#30340;&#20449;&#24515;&#65292;&#25105;&#20204;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#35813;&#31995;&#25968;&#19982;&#22810;&#26679;&#24615;&#30340;&#30452;&#35266;&#23646;&#24615;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#65292;&#38543;&#30528;&#28508;&#22312;&#27010;&#24565;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#22686;&#21152;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22810;&#26679;&#24615;&#31995;&#25968;&#26159;&#21487;&#38752;&#30340;&#65292;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#65292;&#24182;&#25512;&#27979;&#23427;&#21487;&#20197;&#20316;&#20026;&#39044;&#35757;&#32451;LLMs&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04928</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20381;&#36182;&#20110;&#20855;&#26377;&#32473;&#23450;&#21629;&#21517;&#23454;&#20307;&#30340;&#22823;&#37327;&#27880;&#37322;&#25991;&#26412;&#65292;&#20854;&#21019;&#24314;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#25552;&#21462;&#26032;&#23454;&#20307;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#27880;&#37322;&#20219;&#21153;&#21644;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65288;&#26631;&#35760;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#25110;&#19981;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#65289;&#65292;&#24182;&#22312;&#26356;&#22810;&#30340;&#25968;&#25454;&#38598;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#23398;&#20064;&#21040;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;9&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;NER&#12289;&#19968;&#27425;&#26679;&#26412;NER&#12289;10&#27425;&#26679;&#26412;NER&#21644;100&#27425;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;&#20998;&#21035;&#20026;35.44&#65285;&#12289;50.10&#65285;&#12289;69.94&#65285;&#21644;79.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
&lt;/p&gt;</description></item></channel></rss>