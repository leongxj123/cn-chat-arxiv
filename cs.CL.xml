<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#29992;&#20110;&#32842;&#22825;&#35760;&#24405;&#25688;&#35201;&#21270;&#65292;&#35813;&#31574;&#30053;&#39318;&#20808;&#32467;&#21512;&#20102;&#25277;&#21462;&#21644;&#29983;&#25104;&#24335;&#25688;&#35201;&#21270;&#25216;&#26415;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#32842;&#22825;&#35760;&#24405;&#25688;&#35201;&#21270;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01510</link><description>&lt;p&gt;
&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#29992;&#20110;&#32842;&#22825;&#35760;&#24405;&#25688;&#35201;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Strategy for Chat Transcript Summarization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01510
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#29992;&#20110;&#32842;&#22825;&#35760;&#24405;&#25688;&#35201;&#21270;&#65292;&#35813;&#31574;&#30053;&#39318;&#20808;&#32467;&#21512;&#20102;&#25277;&#21462;&#21644;&#29983;&#25104;&#24335;&#25688;&#35201;&#21270;&#25216;&#26415;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#32842;&#22825;&#35760;&#24405;&#25688;&#35201;&#21270;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#21270;&#26159;&#23558;&#19968;&#27573;&#25991;&#26412;&#21387;&#32553;&#25104;&#36739;&#23569;&#21477;&#23376;&#30340;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20869;&#23481;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#32842;&#22825;&#35760;&#24405;&#26159;&#23458;&#25143;&#65288;&#26469;&#30005;&#32773;&#65289;&#21644;&#23458;&#26381;&#20154;&#21592;&#20043;&#38388;&#30340;&#25968;&#23383;&#25110;&#22312;&#32447;&#23545;&#35805;&#30340;&#25991;&#26412;&#21103;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24320;&#21457;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#32467;&#21512;&#25277;&#21462;&#21644;&#29983;&#25104;&#24335;&#25688;&#35201;&#21270;&#25216;&#26415;&#65292;&#23545;&#32570;&#20047;&#26631;&#28857;&#25110;&#26410;&#26631;&#28857;&#30340;&#32842;&#22825;&#35760;&#24405;&#36827;&#34892;&#21387;&#32553;&#65292;&#20135;&#29983;&#26356;&#26131;&#35835;&#30340;&#24102;&#26631;&#28857;&#25688;&#35201;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#25688;&#35201;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;&#24191;&#27867;&#30340;&#27979;&#35797;&#12289;&#35780;&#20272;&#12289;&#27604;&#36739;&#21644;&#39564;&#35777;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#27809;&#26377;&#25163;&#21160;&#29983;&#25104;&#30340;&#21442;&#32771;&#25688;&#35201;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#32842;&#22825;&#35760;&#24405;&#25688;&#35201;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization is the process of condensing a piece of text to fewer sentences, while still preserving its content. Chat transcript, in this context, is a textual copy of a digital or online conversation between a customer (caller) and agent(s). This paper presents an indigenously (locally) developed hybrid method that first combines extractive and abstractive summarization techniques in compressing ill-punctuated or un-punctuated chat transcripts to produce more readable punctuated summaries and then optimizes the overall quality of summarization through reinforcement learning. Extensive testing, evaluations, comparisons, and validation have demonstrated the efficacy of this approach for large-scale deployment of chat transcript summarization, in the absence of manually generated reference (annotated) summaries.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.14814</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The opportunities and risks of large language models in mental health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14814
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#21457;&#29983;&#29575;&#27491;&#22312;&#19978;&#21319;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#29616;&#26377;&#30340;&#24515;&#29702;&#20445;&#20581;&#27169;&#24335;&#26080;&#27861;&#20805;&#20998;&#25193;&#23637;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#20855;&#26377;&#21019;&#36896;&#26032;&#39062;&#12289;&#22823;&#35268;&#27169;&#35299;&#20915;&#26041;&#26696;&#20197;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#30340;&#25215;&#35834;&#24863;&#21040;&#20048;&#35266;&#12290;&#23613;&#31649;&#23427;&#20204;&#36824;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;LLMs&#24050;&#34987;&#24212;&#29992;&#20110;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#24050;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#21033;&#29992;LLMs&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25945;&#32946;&#12289;&#35780;&#20272;&#21644;&#24178;&#39044;&#30340;&#21162;&#21147;&#65292;&#24182;&#31361;&#20986;&#20102;&#27599;&#20010;&#39046;&#22495;&#20013;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#30340;&#20851;&#38190;&#26426;&#20250;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#25152;&#20276;&#38543;&#30340;&#39118;&#38505;&#65292;&#24182;&#40723;&#21169;&#37319;&#29992;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#36843;&#20999;&#38656;&#27714;&#24517;&#39035;&#19982;&#36127;&#36131;&#20219;&#30340;&#24515;&#29702;&#20581;&#24247;LLMs&#30340;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#30456;&#24179;&#34913;&#12290;&#29305;&#21035;&#20851;&#38190;&#30340;&#26159;&#30830;&#20445;&#24515;&#29702;&#20581;&#24247;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;(&#20154;&#31867;+AI)&#26041;&#27861;HyEnA&#65292;&#29992;&#20110;&#20174;&#24847;&#35265;&#25991;&#26412;&#20013;&#25552;&#21462;&#35770;&#28857;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#21270;&#22788;&#29702;&#36895;&#24230;&#21644;&#20154;&#31867;&#29702;&#35299;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#20844;&#27665;&#21453;&#39304;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#35206;&#30422;&#29575;&#21644;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09713</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35770;&#35777;&#25366;&#25496;&#30340;&#28151;&#21512;&#26234;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Intelligence Method for Argument Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09713
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;(&#20154;&#31867;+AI)&#26041;&#27861;HyEnA&#65292;&#29992;&#20110;&#20174;&#24847;&#35265;&#25991;&#26412;&#20013;&#25552;&#21462;&#35770;&#28857;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#21270;&#22788;&#29702;&#36895;&#24230;&#21644;&#20154;&#31867;&#29702;&#35299;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#20844;&#27665;&#21453;&#39304;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#35206;&#30422;&#29575;&#21644;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35843;&#26597;&#24037;&#20855;&#33021;&#22815;&#25910;&#38598;&#20844;&#27665;&#21453;&#39304;&#24847;&#35265;&#35821;&#26009;&#24211;&#12290;&#20174;&#24222;&#22823;&#19988;&#22024;&#26434;&#30340;&#24847;&#35265;&#38598;&#20013;&#25552;&#21462;&#20851;&#38190;&#35770;&#28857;&#26377;&#21161;&#20110;&#24555;&#36895;&#20934;&#30830;&#22320;&#29702;&#35299;&#24847;&#35265;&#12290;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#35770;&#28857;&#65292;&#20294;(1)&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#36739;&#39640;&#30340;&#27880;&#37322;&#25104;&#26412;; (2)&#23545;&#24050;&#30693;&#35266;&#28857;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#23545;&#26032;&#39062;&#35266;&#28857;&#25928;&#26524;&#27424;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyEnA&#65292;&#19968;&#31181;&#28151;&#21512;(&#20154;&#31867;+AI)&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20027;&#35266;&#25991;&#26412;&#20013;&#25552;&#21462;&#35770;&#28857;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#21270;&#22788;&#29702;&#30340;&#36895;&#24230;&#21644;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#27665;&#21453;&#39304;&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#20102;HyEnA&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#26041;&#38754;&#65292;&#19982;&#19968;&#32452;&#21508;&#31181;&#24847;&#35265;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;HyEnA&#22312;&#39640;&#35206;&#30422;&#29575;&#21644;&#20934;&#30830;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#35777;&#23454;&#20102;&#20154;&#31867;&#27934;&#23519;&#30340;&#24517;&#35201;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;HyEnA&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#21147;&#24037;&#20316;&#37327;&#65292;&#19988;&#19981;&#20250;&#29306;&#29298;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09713v1 Announce Type: new  Abstract: Large-scale survey tools enable the collection of citizen feedback in opinion corpora. Extracting the key arguments from a large and noisy set of opinions helps in understanding the opinions quickly and accurately. Fully automated methods can extract arguments but (1) require large labeled datasets that induce large annotation costs and (2) work well for known viewpoints, but not for novel points of view. We propose HyEnA, a hybrid (human + AI) method for extracting arguments from opinionated texts, combining the speed of automated processing with the understanding and reasoning capabilities of humans. We evaluate HyEnA on three citizen feedback corpora. We find that, on the one hand, HyEnA achieves higher coverage and precision than a state-of-the-art automated method when compared to a common set of diverse opinions, justifying the need for human insight. On the other hand, HyEnA requires less human effort and does not compromise quali
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLix&#30340;&#26032;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#35843;&#25972;&#65292;&#36890;&#36807;&#20851;&#32852;&#27599;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#29305;&#24449;&#19982;&#20854;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17934</link><description>&lt;p&gt;
&#29992;&#29305;&#24449;&#21270;&#20302;&#31209;&#28151;&#21512;&#36827;&#34892;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17934
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLix&#30340;&#26032;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#35843;&#25972;&#65292;&#36890;&#36807;&#20851;&#32852;&#27599;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#29305;&#24449;&#19982;&#20854;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#25968;&#21313;&#29978;&#33267;&#25968;&#30334;&#31181;&#20154;&#31867;&#35821;&#35328;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#36890;&#36807;&#21482;&#35843;&#25972;&#23569;&#37327;&#21442;&#25968;&#26174;&#33879;&#20943;&#23569;&#20102;&#36866;&#24212;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#20687; LoRA&#65288;Hu &#31561;&#20154;&#65292;2022&#65289;&#36825;&#26679;&#30340; PEFT &#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#28151;&#21512;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#65292;&#21407;&#22240;&#22312;&#20110;&#26377;&#38480;&#30340;&#21442;&#25968;&#23481;&#37327;&#21644;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36127;&#38754;&#20114;&#30456;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#21270;&#20302;&#31209;&#28151;&#21512;&#65288;FLix&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#35843;&#25972;&#30340;&#26032;&#22411; PEFT &#26041;&#27861;&#12290;FLix&#23558;&#27599;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#29305;&#24449;&#65288;&#20363;&#22914;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#25110;&#20219;&#21153;&#65289;&#19982;&#20854;&#33258;&#24049;&#30340;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#21442;&#25968;&#30456;&#20851;&#32852;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#32452;&#21512;&#29305;&#23450;&#20110;&#29305;&#24449;&#30340;&#21442;&#25968;&#65292;FLix&#33021;&#22815;&#36866;&#24212;&#22810;&#31181;&#25968;&#25454;&#38598;&#28151;&#21512;&#65292;&#24182;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FLix &#21487;&#20197;&#22312;&#25552;&#20379;&#26356;&#22909;&#24615;&#33021;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#36866;&#24212;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17934v1 Announce Type: cross  Abstract: Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive. Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters. However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets. In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning. FLix associates each unique dataset feature, such as the dataset's language or task, with its own low-rank weight update parameters. By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets. Our experiments show that FLix leads t
&lt;/p&gt;</description></item><item><title>ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15220</link><description>&lt;p&gt;
ChunkAttention: &#20855;&#26377;&#21069;&#32512;&#24863;&#30693;KV&#32531;&#23384;&#21644;&#20004;&#38454;&#27573;&#20998;&#21306;&#30340;&#39640;&#25928;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15220
&lt;/p&gt;
&lt;p&gt;
ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;&#38271;&#24207;&#21015;&#26469;&#35828;&#26159;&#25512;&#29702;&#24310;&#36831;&#30340;&#19968;&#20010;&#26174;&#33879;&#26469;&#28304;&#12290;&#22312;&#22810;&#31199;&#25143;LLMs&#26381;&#21153;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;LLM&#35831;&#27714;&#22312;&#21069;&#32512;&#20013;&#20849;&#20139;&#31995;&#32479;&#25552;&#31034;&#30340;&#27010;&#29575;&#65292;&#21487;&#20197;&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25805;&#20316;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChunkAttention&#65292;&#19968;&#31181;&#20855;&#26377;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#26816;&#27979;&#22810;&#20010;&#35831;&#27714;&#20043;&#38388;&#21305;&#37197;&#30340;&#25552;&#31034;&#21069;&#32512;&#65292;&#24182;&#20849;&#20139;&#23427;&#20204;&#30340;&#38190;/&#20540;&#24352;&#37327;&#20197;&#25913;&#36827;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#25972;&#20307;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#26469;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;KV&#32531;&#23384;&#20043;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#33258;&#27880;&#24847;&#21147;&#20869;&#26680;&#65292;&#20854;&#20013;&#23454;&#29616;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
&lt;/p&gt;</description></item><item><title>DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02563</link><description>&lt;p&gt;
DefInt&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02563
&lt;/p&gt;
&lt;p&gt;
DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22914;&#36830;&#38145;&#25512;&#29702;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#26631;&#35760;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24040;&#22823;&#35299;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#23454;&#38469;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#29305;&#21035;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65288;DefInt&#65289;&#65292;&#20197;&#37322;&#25918;&#28151;&#21512;LLMs&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;DefInt&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;&#25512;&#29702;&#24605;&#36335;&#65292;&#31867;&#20284;&#20110;&#8220;&#31995;&#32479;1&#8221;&#20135;&#29983;&#30340;&#24555;&#36895;&#30452;&#35273;&#12290;&#22914;&#26524;&#36825;&#20123;&#30452;&#35273;&#34987;&#35748;&#20026;&#20302;&#32622;&#20449;&#24230;&#65292;&#21017;DefInt&#23558;&#35843;&#29992;&#25918;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#24605;&#25512;&#29702;&#20316;&#20026;&#8220;&#31995;&#32479;2&#8221;&#30340;&#24178;&#39044;&#65292;&#21487;&#20197;&#35206;&#30422;&#40664;&#35748;&#24605;&#32771;&#24182;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;DefInt&#35770;&#25991;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
&lt;/p&gt;</description></item><item><title>KGLens &#26159;&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;&#30693;&#35782;&#22270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#30340;&#26694;&#26550;&#65292;&#24110;&#21161;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;&#30693;&#35782;&#22270;&#30340;&#30693;&#35782;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2312.11539</link><description>&lt;p&gt;
KGLens&#65306;&#19968;&#20010;&#21442;&#25968;&#21270;&#30693;&#35782;&#22270;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#30693;&#36947;&#21644;&#19981;&#30693;&#36947;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11539
&lt;/p&gt;
&lt;p&gt;
KGLens &#26159;&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;&#30693;&#35782;&#22270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#30340;&#26694;&#26550;&#65292;&#24110;&#21161;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;&#30693;&#35782;&#22270;&#30340;&#30693;&#35782;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#26159;&#35780;&#20272;&#20107;&#23454;&#24615;&#24182;&#35782;&#21035;LLMs&#30340;&#30693;&#35782;&#30450;&#28857;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23558;KGs&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#39640;&#25928;&#35780;&#20272;&#36825;&#20123;&#24191;&#27867;&#19988;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KGLens--&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;KGs&#21644;LLMs&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#65292;&#24182;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;KGs&#30340;&#30693;&#35782;&#32570;&#38519;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;KGLens&#20855;&#26377;&#19968;&#20010;&#22270;&#24341;&#23548;&#30340;&#38382;&#39064;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23558;KGs&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21442;&#25968;&#21270;KG&#32467;&#26500;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#21152;&#24555;KG&#30340;&#36941;&#21382;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Wikidata&#30340;&#19977;&#20010;&#39046;&#22495;&#29305;&#23450;KG&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20123;KG&#21253;&#25324;&#36229;&#36807;19,000&#26465;&#36793;&#65292;700&#20010;&#20851;&#31995;&#21644;21,000&#20010;&#23454;&#20307;&#12290;&#25105;&#20204;&#36328;&#36234;8&#20010;LLMs&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;KGLens&#19981;&#20165;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11539v2 Announce Type: replace  Abstract: Measuring the alignment between a Knowledge Graph (KG) and Large Language Models (LLMs) is an effective method to assess the factualness and identify the knowledge blind spots of LLMs. However, this approach encounters two primary challenges including the translation of KGs into natural language and the efficient evaluation of these extensive and complex structures. In this paper, we present KGLens--a novel framework aimed at measuring the alignment between KGs and LLMs, and pinpointing the LLMs' knowledge deficiencies relative to KGs. KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed sampling strategy based on parameterized KG structure to expedite KG traversal. We conducted experiments using three domain-specific KGs from Wikidata, which comprise over 19,000 edges, 700 relations, and 21,000 entities. Our analysis across eight LLMs reveals that KGLens not only
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11864</link><description>&lt;p&gt;
&#36890;&#36807;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#21387;&#32553;&#21040;&#20855;&#26377;&#23567;&#20110;&#21313;&#20159;&#21442;&#25968;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#65292;&#23558;&#25512;&#29702;&#36807;&#31243;&#23553;&#35013;&#20026;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;EoTD&#25968;&#25454;&#38598;&#26469;&#23545;SLMs&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;SLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36825;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#65288;&#21253;&#25324;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#31243;&#24207;&#21644;&#24605;&#32500;&#26041;&#31243;&#65289;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;EoTD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;ETD&#20351;&#36825;&#20123;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANOLA QA&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#20351;&#29992;&#22810;&#31890;&#24230;&#31572;&#26696;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20016;&#23500;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#31890;&#24230;&#29256;&#26412;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...</title><link>http://arxiv.org/abs/2401.04695</link><description>&lt;p&gt;
&#32553;&#23567;&#30693;&#35782;&#35780;&#20272;&#24046;&#36317;&#65306;&#22810;&#23618;&#27425;&#31572;&#26696;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers. (arXiv:2401.04695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANOLA QA&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#20351;&#29992;&#22810;&#31890;&#24230;&#31572;&#26696;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20016;&#23500;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#31890;&#24230;&#29256;&#26412;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#24615;&#38382;&#39064;&#36890;&#24120;&#21487;&#20197;&#20197;&#19981;&#21516;&#23618;&#27425;&#30340;&#31890;&#24230;&#27491;&#30830;&#22238;&#31572;&#12290;&#20363;&#22914;&#65292;&#8220;1961&#24180;8&#26376;4&#26085;&#8221;&#21644;&#8220;1961&#24180;&#8221;&#37117;&#26159;&#23545;&#8220;&#24052;&#25289;&#20811;&#183;&#22885;&#24052;&#39532;&#26159;&#22312;&#20160;&#20040;&#26102;&#20505;&#20986;&#29983;&#30340;&#65311;&#8221;&#36825;&#20010;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#38382;&#31572;&#35780;&#20272;&#21327;&#35758;&#24182;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#36825;&#19968;&#28857;&#65292;&#32780;&#26159;&#23558;&#39044;&#27979;&#30340;&#31572;&#26696;&#19982;&#21333;&#19968;&#31890;&#24230;&#23618;&#27425;&#30340;&#31572;&#26696;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GRANOLA QA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#20854;&#20013;&#39044;&#27979;&#30340;&#31572;&#26696;&#26681;&#25454;&#20934;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#19982;&#19968;&#32452;&#22810;&#31890;&#24230;&#31572;&#26696;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20016;&#23500;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#22810;&#31890;&#24230;&#31572;&#26696;&#65292;&#24182;&#21019;&#24314;&#20102;GRANOLA-EQ&#65292;&#19968;&#20010;&#22810;&#31890;&#24230;&#29256;&#26412;&#30340;EntityQuestions&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;GRANOLA-EQ&#19978;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#35299;&#30721;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;Decoding with Response Aggregation (DRAG)&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#23558;&#21709;&#24212;&#30340;&#31890;&#24230;&#19982;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Factual questions typically can be answered correctly at different levels of granularity. For example, both ``August 4, 1961'' and ``1961'' are correct answers to the question ``When was Barack Obama born?''. Standard question answering (QA) evaluation protocols, however, do not explicitly take this into account and compare a predicted answer against answers of a single granularity level. In this work, we propose GRANOLA QA, a novel evaluation setting where a predicted answer is evaluated in terms of accuracy and informativeness against a set of multi-granularity answers. We present a simple methodology for enriching existing datasets with multi-granularity answers, and create GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We evaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm, called Decoding with Response Aggregation (DRAG), that is geared towards aligning the response granularity with the model's uncertainty. Our experiments show th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#65292;&#21253;&#25324;&#19981;&#21516;&#25216;&#26415;&#30340;&#27010;&#36848;&#21644;&#27604;&#36739;&#12289;&#31639;&#27861;&#35780;&#20272;&#26041;&#27861;&#12289;&#24212;&#29992;&#22330;&#26223;&#20197;&#21450;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.07913</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Text Watermarking in the Era of Large Language Models. (arXiv:2312.07913v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#65292;&#21253;&#25324;&#19981;&#21516;&#25216;&#26415;&#30340;&#27010;&#36848;&#21644;&#27604;&#36739;&#12289;&#31639;&#27861;&#35780;&#20272;&#26041;&#27861;&#12289;&#24212;&#29992;&#22330;&#26223;&#20197;&#21450;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#22312;&#29256;&#26435;&#20445;&#25252;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28982;&#32780;&#20854;&#33021;&#21147;&#21644;&#24212;&#29992;&#22330;&#26223;&#19968;&#30452;&#21463;&#38480;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#20026;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30340;&#36827;&#27493;&#25171;&#24320;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#36890;&#36807;&#20854;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#22686;&#24378;&#20102;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#20351;&#29992;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#26469;&#20445;&#25252;&#33258;&#36523;&#30340;&#29256;&#26435;&#12290;&#26412;&#25991;&#23545;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30340;&#29616;&#29366;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#26041;&#38754;&#65306;&#65288;1&#65289;&#19981;&#21516;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30340;&#27010;&#36848;&#21644;&#27604;&#36739;&#65307;&#65288;2&#65289;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21253;&#25324;&#25104;&#21151;&#29575;&#12289;&#23545;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#12289;&#40065;&#26834;&#24615;&#21644;&#38450;&#31713;&#25913;&#24615;&#65307;&#65288;3&#65289;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30340;&#28508;&#22312;&#24212;&#29992;&#22330;&#26223;&#65307;&#65288;4&#65289;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text watermarking algorithms play a crucial role in the copyright protection of textual content, yet their capabilities and application scenarios have been limited historically. The recent developments in large language models (LLMs) have opened new opportunities for the advancement of text watermarking techniques. LLMs not only enhance the capabilities of text watermarking algorithms through their text understanding and generation abilities but also necessitate the use of text watermarking algorithms for their own copyright protection. This paper conducts a comprehensive survey of the current state of text watermarking technology, covering four main aspects: (1) an overview and comparison of different text watermarking techniques; (2) evaluation methods for text watermarking algorithms, including their success rates, impact on text quality, robustness, and unforgeability; (3) potential application scenarios for text watermarking technology; (4) current challenges and future directions
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#23545;&#35805;&#25688;&#35201;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#29992;&#25143;&#30340;&#29305;&#23450;&#20852;&#36259;&#65292;&#32780;&#25351;&#23548;&#23545;&#35805;&#25688;&#35201;&#30340;&#24341;&#20837;&#21487;&#20197;&#24110;&#21161;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#26469;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26597;&#35810;&#25688;&#35201;&#19977;&#20803;&#32452;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#26469;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10981</link><description>&lt;p&gt;
&#20351;&#29992;&#26597;&#35810;&#32858;&#21512;&#30340;&#25351;&#23548;&#24615;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Instructive Dialogue Summarization with Query Aggregations. (arXiv:2310.10981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10981
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#23545;&#35805;&#25688;&#35201;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#29992;&#25143;&#30340;&#29305;&#23450;&#20852;&#36259;&#65292;&#32780;&#25351;&#23548;&#23545;&#35805;&#25688;&#35201;&#30340;&#24341;&#20837;&#21487;&#20197;&#24110;&#21161;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#26469;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26597;&#35810;&#25688;&#35201;&#19977;&#20803;&#32452;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#26469;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#23545;&#35805;&#25688;&#35201;&#26041;&#27861;&#30452;&#25509;&#29983;&#25104;&#25688;&#35201;&#65292;&#19981;&#32771;&#34385;&#29992;&#25143;&#30340;&#29305;&#23450;&#20852;&#36259;&#12290;&#36825;&#22312;&#29992;&#25143;&#26356;&#21152;&#20851;&#27880;&#29305;&#23450;&#20027;&#39064;&#25110;&#26041;&#38754;&#30340;&#24773;&#20917;&#19979;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#38543;&#30528;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25351;&#23548;&#23545;&#35805;&#26469;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#38598;&#12290;&#20026;&#20102;&#20811;&#26381;&#25351;&#23548;&#24615;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#26041;&#27861;&#26469;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#25688;&#35201;&#19977;&#20803;&#32452;&#12290;&#36825;&#20010;&#36807;&#31243;&#21253;&#25324;&#20197;&#25688;&#35201;&#20026;&#38170;&#28857;&#30340;&#26597;&#35810;&#29983;&#25104;&#12289;&#26597;&#35810;&#36807;&#28388;&#21644;&#22522;&#20110;&#26597;&#35810;&#30340;&#25688;&#35201;&#29983;&#25104;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;InstructDS&#65288;&#25351;&#23548;&#24615;&#23545;&#35805;&#25688;&#35201;&#65289;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#23545;&#35805;&#25688;&#35201;&#21644;&#23545;&#35805;&#38405;&#35835;&#29702;&#35299;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional dialogue summarization methods directly generate summaries and do not consider user's specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce instruction-tuning to dialogues to expand the capability set of dialogue summarization models. To overcome the scarcity of instructive dialogue summarization data, we propose a three-step approach to synthesize high-quality query-based summarization triples. This process involves summary-anchored query generation, query filtering, and query-based summary generation. By training a unified model called InstructDS (Instructive Dialogue Summarization) on three summarization datasets with multi-purpose instructive triples, we expand the capability of dialogue summarization models. We evaluate our method on four datasets, including dialogue summarization and dialogue reading comprehension. Experimental re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;VoIP&#36890;&#20449;&#39046;&#22495;&#20013;&#25506;&#32034;&#20102;&#22768;&#23398;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#22768;&#23398;&#25351;&#26631;&#65292;&#25581;&#31034;&#20102;&#35821;&#38899;&#22686;&#24378;&#23545;VoIP&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07161</link><description>&lt;p&gt;
VoIP&#24179;&#21488;&#19978;&#35821;&#38899;&#22686;&#24378;&#30340;&#24515;&#29702;&#22768;&#23398;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms. (arXiv:2310.07161v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;VoIP&#36890;&#20449;&#39046;&#22495;&#20013;&#25506;&#32034;&#20102;&#22768;&#23398;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#22768;&#23398;&#25351;&#26631;&#65292;&#25581;&#31034;&#20102;&#35821;&#38899;&#22686;&#24378;&#23545;VoIP&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;VoIP&#65288;&#20114;&#32852;&#32593;&#35821;&#38899;&#20256;&#36755;&#21327;&#35758;&#65289;&#36890;&#20449;&#20013;&#65292;&#30001;&#22768;&#23398;&#36716;&#25442;&#24341;&#20837;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#36827;&#34892;&#20005;&#26684;&#30340;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#23545;&#19987;&#26377;&#21457;&#36865;&#31471;&#38477;&#22122;&#25928;&#26524;&#30340;&#25506;&#32034;&#65292;&#23545;Google Meets&#21644;Zoom&#31561;&#24179;&#21488;&#36827;&#34892;&#20102;&#32454;&#33268;&#35780;&#20272;&#12290;&#30740;&#31350;&#21033;&#29992;Deep Noise Suppression&#65288;DNS&#65289;2020&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#20102;&#38024;&#23545;&#21508;&#31181;&#38477;&#22122;&#35774;&#32622;&#21644;&#25509;&#25910;&#22120;&#25509;&#21475;&#30340;&#32467;&#26500;&#21270;&#32771;&#23519;&#12290;&#36890;&#36807;&#23558;Oaxaca&#20998;&#35299;&#24341;&#20837;&#21040;&#22768;&#23398;-&#35821;&#38899;&#25200;&#21160;&#20998;&#26512;&#20013;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#35770;&#30340;&#21019;&#26032;&#65292;&#35813;&#20998;&#35299;&#36890;&#24120;&#26159;&#32463;&#27982;&#35745;&#37327;&#23398;&#24037;&#20855;&#65292;&#22312;&#27492;&#22788;&#37325;&#26032;&#29992;&#20110;&#20998;&#26512;VoIP&#31995;&#32479;&#20013;&#30340;&#22768;&#23398;-&#35821;&#38899;&#25200;&#21160;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30830;&#23450;&#36825;&#20123;&#36716;&#25442;&#30340;&#24433;&#21709;&#65292;&#21033;&#29992;&#24515;&#29702;&#22768;&#23398;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;PESQ&#21644;STOI&#65292;&#26469;&#25552;&#20379;&#23545;&#35821;&#38899;&#25913;&#21464;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25152;&#33719;&#24471;&#30340;&#35266;&#28857;&#31361;&#20986;&#26174;&#31034;&#20102;VoIP&#24433;&#21709;&#30340;&#22768;&#23398;&#21160;&#21147;&#23398;&#30340;&#22797;&#26434;&#26223;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via the Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were harnessed to furnish a comprehensive understanding of speech alterations. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07865</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25512;&#25991;/&#29992;&#25143;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22788;&#29702;&#25991;&#26412;&#22823;&#25968;&#25454;&#22320;&#29702;&#26631;&#35760;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#20272;&#35745;&#22352;&#26631;&#23545;&#65288;&#32463;&#24230;&#65292;&#32428;&#24230;&#65289;&#21644;&#20108;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#33539;&#22260;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#12290;&#24615;&#33021;&#25351;&#26631;&#34920;&#26126;&#65292;&#23545;&#20110;&#22312;&#25512;&#25991;&#20869;&#23481;&#21644;&#20803;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;30&#20844;&#37324;&#65292;&#32654;&#22269;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;15&#20844;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
&lt;/p&gt;</description></item></channel></rss>