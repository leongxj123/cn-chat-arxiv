<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PATCH&#65292;&#29992;&#20110;&#23558;&#24515;&#29702;&#27979;&#37327;&#39046;&#22495;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#30340;&#27979;&#37327;&#36136;&#37327;&#12289;&#39033;&#30446;&#32423;&#21035;&#35780;&#20272;&#21644;&#21442;&#32771;&#20154;&#32676;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01799</link><description>&lt;p&gt;
PATCH -- &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#27979;&#37327;&#36741;&#21161;&#22522;&#20934;&#27979;&#35797;&#65306;&#25968;&#23398;&#33021;&#21147;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PATCH&#65292;&#29992;&#20110;&#23558;&#24515;&#29702;&#27979;&#37327;&#39046;&#22495;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#30340;&#27979;&#37327;&#36136;&#37327;&#12289;&#39033;&#30446;&#32423;&#21035;&#35780;&#20272;&#21644;&#21442;&#32771;&#20154;&#32676;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#30340;&#22823;&#22411;&#65288;&#22810;&#27169;&#24577;&#65289;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22522;&#20934;&#27979;&#35797;&#30528;&#37325;&#20110;&#34913;&#37327;LLMs&#30340;&#23398;&#26415;&#33021;&#21147;&#65292;&#36890;&#24120;&#20063;&#23545;&#27604;&#36739;&#27169;&#22411;&#24615;&#33021;&#19982;&#20154;&#31867;&#32771;&#35797;&#32773;&#24863;&#20852;&#36259;&#12290;&#23613;&#31649;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#23545;LLMs&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#26377;&#38382;&#39064;&#30340;&#27979;&#37327;&#36136;&#37327;&#65288;&#20363;&#22914;&#65292;&#23427;&#20204;&#26159;&#21542;&#20197;&#21487;&#38752;&#30340;&#26041;&#24335;&#34913;&#37327;&#25152;&#38656;&#30340;&#20869;&#23481;&#65311;&#65289;&#12289;&#32570;&#20047;&#39033;&#30446;&#32423;&#21035;&#30340;&#36136;&#37327;&#35780;&#20272;&#65288;&#20363;&#22914;&#65292;&#26377;&#20123;&#39033;&#30446;&#26159;&#21542;&#27604;&#20854;&#20182;&#26356;&#37325;&#35201;&#25110;&#26356;&#22256;&#38590;&#65311;&#65289;&#20197;&#21450;&#20154;&#31867;&#20154;&#21475;&#21442;&#29031;&#27169;&#31946;&#65288;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#19982;&#35841;&#36827;&#34892;&#27604;&#36739;&#65311;&#65289;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#8212;&#8212;&#19968;&#38376;&#33268;&#21147;&#20110;&#27979;&#37327;&#28508;&#22312;&#21464;&#37327;&#22914;&#23398;&#26415;&#33021;&#21147;&#30340;&#39046;&#22495;&#8212;&#8212;&#26469;&#36827;&#34892;LLMs&#22522;&#20934;&#27979;&#35797;&#30340;&#24515;&#29702;&#27979;&#37327;&#36741;&#21161;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#19977;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PATCH&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#27979;&#37327;&#36741;&#21161;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01799v1 Announce Type: new  Abstract: Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers. While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics - a field dedicated to the measurement of latent variables like academic proficiency - into LLM benchmarking. We make three primary contributions. First, we introduce PATCH: a novel framework for Psychometrics-AssisTed benCHmarking of LLMs. PATCH addresses 
&lt;/p&gt;</description></item><item><title>&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#39044;&#31639;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#36873;&#25321;&#25490;&#21517;&#27425;&#20110;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.00725</link><description>&lt;p&gt;
&#36234;&#22823;&#36234;&#22909;&#21527;&#65311;&#36890;&#36807;&#39044;&#31639;&#37325;&#26032;&#20998;&#37197;&#25913;&#36827;LLM&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
The Larger the Better? Improved LLM Code-Generation via Budget Reallocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00725
&lt;/p&gt;
&lt;p&gt;
&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#39044;&#31639;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#36873;&#25321;&#25490;&#21517;&#27425;&#20110;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27604;&#36739;&#23567;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20063;&#38656;&#35201;&#26356;&#22810;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#23601;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#20004;&#20010;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;&#39044;&#31639;&#19979;&#36816;&#34892;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#65288;&#20363;&#22914;&#65292;&#35745;&#31639;&#36164;&#28304;&#65292;&#36816;&#34892;&#26102;&#38388;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#22823;&#23567;&#30340;&#20195;&#30721;&#29983;&#25104;LLMs&#65292;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#20363;&#22914;&#36816;&#34892;&#19968;&#20010;70B&#27169;&#22411;&#19968;&#27425;&#19982;&#20174;13B&#27169;&#22411;&#29983;&#25104;&#20116;&#20010;&#36755;&#20986;&#24182;&#36873;&#25321;&#19968;&#20010;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26631;&#20934;&#21333;&#20803;&#27979;&#35797;&#35774;&#32622;&#20013;&#65292;&#21453;&#22797;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#26368;&#39640;&#21487;&#36798;15%&#30340;&#22686;&#30410;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#36739;&#23567;&#27169;&#22411;&#20013;&#22522;&#20110;&#25490;&#21517;&#30340;&#20505;&#36873;&#36873;&#25321;&#34920;&#29616;&#19981;&#21450;&#26469;&#33258;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;&#36739;&#23567;&#27169;&#22411;&#32780;&#38750;&#36739;&#22823;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00725v1 Announce Type: cross  Abstract: It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the imp
&lt;/p&gt;</description></item><item><title>AutoRE &#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF&#30340;&#26032;&#39062;&#20851;&#31995;&#25277;&#21462;&#33539;&#24335;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#22312;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;</title><link>https://arxiv.org/abs/2403.14888</link><description>&lt;p&gt;
AutoRE&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
AutoRE: Document-Level Relation Extraction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14888
&lt;/p&gt;
&lt;p&gt;
AutoRE &#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF&#30340;&#26032;&#39062;&#20851;&#31995;&#25277;&#21462;&#33539;&#24335;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#22312;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#24322;&#24120;&#33021;&#21147;&#65292;&#36825;&#28608;&#21169;&#30528;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#23427;&#20204;&#36827;&#34892;&#20449;&#24687;&#25277;&#21462;(IE)&#20219;&#21153;&#65292;&#21253;&#25324;&#20851;&#31995;&#25277;&#21462;(RE)&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;(SentRE)&#20219;&#21153;&#65292;&#36825;&#36890;&#24120;&#28085;&#30422;&#20102;&#21333;&#20010;&#21477;&#23376;&#20869;&#30340;&#19968;&#32452;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26041;&#27861;&#37319;&#29992;&#23558;&#20851;&#31995;&#20316;&#20026;&#20505;&#36873;&#36873;&#25321;&#38598;&#25104;&#21040;&#25552;&#31034;&#27169;&#26495;&#20013;&#30340;&#26041;&#24335;&#65292;&#23548;&#33268;&#22312;&#22788;&#29702;&#20998;&#24067;&#22312;&#32473;&#23450;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#26102;&#25928;&#29575;&#20302;&#19979;&#65292;&#24615;&#33021;&#20122;&#20248;&#65292;&#24182;&#22312;&#22788;&#29702;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;(DocRE)&#20219;&#21153;&#26102;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoRE&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;DocRE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF(Re
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14888v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional abilities in comprehending and generating text, motivating numerous researchers to utilize them for Information Extraction (IE) purposes, including Relation Extraction (RE). Nonetheless, most existing methods are predominantly designed for Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a restricted set of relations and triplet facts within a single sentence. Furthermore, certain approaches resort to treating relations as candidate choices integrated into prompt templates, leading to inefficient processing and suboptimal performance when tackling Document-Level Relation Extraction (DocRE) tasks, which entail handling multiple relations and triplet facts distributed across a given document, posing distinct challenges. To overcome these limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel RE extraction paradigm named RHF (Re
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#33609;&#31295;&#39564;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39069;&#22806;&#30340;&#22681;&#38047;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#33609;&#31295;&#26631;&#35760;</title><link>https://arxiv.org/abs/2403.10444</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#26368;&#20339;&#22359;&#32423;&#33609;&#31295;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimal Block-Level Draft Verification for Accelerating Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#33609;&#31295;&#39564;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39069;&#22806;&#30340;&#22681;&#38047;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#33609;&#31295;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#24050;&#34987;&#35777;&#26126;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26080;&#25439;&#21152;&#36895;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290; &#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#31639;&#27861;&#39318;&#20808;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#27169;&#22411;&#36215;&#33609;&#19968;&#22359;&#26631;&#35760;&#12290;&#36825;&#20123;&#26631;&#35760;&#28982;&#21518;&#30001;&#22823;&#22411;&#27169;&#22411;&#24182;&#34892;&#39564;&#35777;&#65292;&#21482;&#26377;&#19968;&#37096;&#20998;&#26631;&#35760;&#23558;&#34987;&#20445;&#30041;&#65292;&#20197;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#36981;&#24490;&#22823;&#22411;&#27169;&#22411;&#30340;&#20998;&#24067;&#12290; &#22312;&#20197;&#24448;&#30340;&#25152;&#26377;&#25512;&#27979;&#35299;&#30721;&#24037;&#20316;&#20013;&#65292;&#36215;&#33609;&#39564;&#35777;&#26159;&#29420;&#31435;&#22320;&#36880;&#20010;&#26631;&#35760;&#25191;&#34892;&#30340;&#12290; &#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#36215;&#33609;&#39564;&#35777;&#31639;&#27861;&#65292;&#21487;&#25552;&#20379;&#39069;&#22806;&#30340;&#22681;&#38047;&#21152;&#36895;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36215;&#33609;&#26631;&#35760;&#12290; &#25105;&#20204;&#39318;&#20808;&#23558;&#36215;&#33609;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#19968;&#20010;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290; &#22359;&#32423;&#21046;&#23450;&#20801;&#35768;&#25105;&#20204;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#36215;&#33609;&#39564;&#35777;&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#36215;&#33609;&#20013;&#39044;&#26399;&#33719;&#24471;&#26356;&#22810;&#25509;&#21463;&#30340;&#26631;&#35760;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10444v1 Announce Type: cross  Abstract: Speculative decoding has shown to be an effective method for lossless acceleration of large language models (LLMs) during inference. In each iteration, the algorithm first uses a smaller model to draft a block of tokens. The tokens are then verified by the large model in parallel and only a subset of tokens will be kept to guarantee that the final output follows the distribution of the large model. In all of the prior speculative decoding works, the draft verification is performed token-by-token independently. In this work, we propose a better draft verification algorithm that provides additional wall-clock speedup without incurring additional computation cost and draft tokens. We first formulate the draft verification step as a block-level optimal transport problem. The block-level formulation allows us to consider a wider range of draft verification algorithms and obtain a higher number of accepted tokens in expectation in one draft 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#22836;&#30340;&#25805;&#20316;&#65292;&#25581;&#31034;&#20102;&#32467;&#21512;&#20102;&#21477;&#27861;&#20381;&#36182;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#35821;&#20041;&#24863;&#24212;&#22836;&#30340;&#20986;&#29616;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13055</link><description>&lt;p&gt;
&#35782;&#21035;&#35821;&#20041;&#24863;&#24212;&#22836;&#20197;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Identifying Semantic Induction Heads to Understand In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#22836;&#30340;&#25805;&#20316;&#65292;&#25581;&#31034;&#20102;&#32467;&#21512;&#20102;&#21477;&#27861;&#20381;&#36182;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#35821;&#20041;&#24863;&#24212;&#22836;&#30340;&#20986;&#29616;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#25512;&#29702;&#36923;&#36753;&#30340;&#19981;&#36879;&#26126;&#24615;&#24341;&#21457;&#20102;&#23545;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#65292;&#25105;&#20204;&#23545;&#27880;&#24847;&#21147;&#22836;&#30340;&#25805;&#20316;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#26159;&#21542;&#32534;&#30721;&#20102;&#33258;&#28982;&#35821;&#35328;&#20013;&#23384;&#22312;&#30340;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#65306;&#20174;&#21477;&#23376;&#20013;&#35299;&#26512;&#30340;&#21477;&#27861;&#20381;&#36182;&#21644;&#30693;&#35782;&#22270;&#20013;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#27880;&#24847;&#21147;&#22836;&#34920;&#29616;&#20986;&#19968;&#31181;&#27169;&#24335;&#65292;&#21363;&#24403;&#20851;&#27880;&#22836;&#26631;&#35760;&#26102;&#65292;&#23427;&#20204;&#20250;&#22238;&#24518;&#36215;&#23614;&#26631;&#35760;&#65292;&#24182;&#22686;&#21152;&#36825;&#20123;&#23614;&#26631;&#35760;&#30340;&#36755;&#20986;&#36923;&#36753;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#35821;&#20041;&#24863;&#24212;&#22836;&#30340;&#21046;&#23450;&#19982;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#20986;&#29616;&#23384;&#22312;&#23494;&#20999;&#20851;&#32852;&#12290;&#35821;&#20041;&#27880;&#24847;&#21147;&#22836;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#25105;&#20204;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13055v1 Announce Type: cross  Abstract: Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Vec2Text&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#30340;&#23041;&#32961;&#20197;&#21450;&#22914;&#20309;&#32531;&#35299;&#65292;&#36890;&#36807;&#23545;&#36317;&#31163;&#24230;&#37327;&#12289;&#27744;&#21270;&#20989;&#25968;&#12289;&#29942;&#39048;&#39044;&#35757;&#32451;&#31561;&#26041;&#38754;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#33719;&#24471;&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#20013;&#25991;&#26412;&#21487;&#24674;&#22797;&#24615;&#21644;&#26816;&#32034;&#25928;&#26524;&#26435;&#34913;&#20851;&#38190;&#20803;&#32032;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12784</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#32531;&#35299;Vec2Text&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Vec2Text&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#30340;&#23041;&#32961;&#20197;&#21450;&#22914;&#20309;&#32531;&#35299;&#65292;&#36890;&#36807;&#23545;&#36317;&#31163;&#24230;&#37327;&#12289;&#27744;&#21270;&#20989;&#25968;&#12289;&#29942;&#39048;&#39044;&#35757;&#32451;&#31561;&#26041;&#38754;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#33719;&#24471;&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#20013;&#25991;&#26412;&#21487;&#24674;&#22797;&#24615;&#21644;&#26816;&#32034;&#25928;&#26524;&#26435;&#34913;&#20851;&#38190;&#20803;&#32032;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Vec2Text&#25216;&#26415;&#65292;&#19968;&#31181;&#29992;&#20110;&#21453;&#36716;&#25991;&#26412;&#23884;&#20837;&#30340;&#25216;&#26415;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#20013;&#23384;&#22312;&#20005;&#37325;&#38544;&#31169;&#38382;&#39064;&#30340;&#25285;&#24551;&#65292;&#21253;&#25324;&#37027;&#20123;&#20351;&#29992;OpenAI&#21644;Cohere&#25552;&#20379;&#30340;&#25991;&#26412;&#23884;&#20837;&#30340;&#31995;&#32479;&#12290;&#36825;&#31181;&#23041;&#32961;&#26469;&#33258;&#20110;&#19968;&#20010;&#24694;&#24847;&#25915;&#20987;&#32773;&#36890;&#36807;&#35775;&#38382;&#25991;&#26412;&#23884;&#20837;&#26469;&#37325;&#26500;&#21407;&#22987;&#25991;&#26412;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20351;&#29992;Vec2Text&#24674;&#22797;&#25991;&#26412;&#30340;&#23884;&#20837;&#27169;&#22411;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#28041;&#21450;&#36317;&#31163;&#24230;&#37327;&#12289;&#27744;&#21270;&#20989;&#25968;&#12289;&#29942;&#39048;&#39044;&#35757;&#32451;&#12289;&#21152;&#22122;&#22768;&#35757;&#32451;&#12289;&#23884;&#20837;&#37327;&#21270;&#21644;&#23884;&#20837;&#32500;&#24230;&#31561;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#22312;&#21407;&#22987;Vec2Text&#35770;&#25991;&#20013;&#23578;&#26410;&#34987;&#35752;&#35770;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#22240;&#32032;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#26088;&#22312;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#24433;&#21709;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#20013;&#25991;&#26412;&#21487;&#24674;&#22797;&#24615;&#21644;&#26816;&#32034;&#25928;&#26524;&#20043;&#38388;&#26435;&#34913;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12784v1 Announce Type: cross  Abstract: The introduction of Vec2Text, a technique for inverting text embeddings, has raised serious privacy concerns within dense retrieval systems utilizing text embeddings, including those provided by OpenAI and Cohere. This threat comes from the ability for a malicious attacker with access to text embeddings to reconstruct the original text.   In this paper, we investigate various aspects of embedding models that could influence the recoverability of text using Vec2Text. Our exploration involves factors such as distance metrics, pooling functions, bottleneck pre-training, training with noise addition, embedding quantization, and embedding dimensions -- aspects not previously addressed in the original Vec2Text paper. Through a thorough analysis of these factors, our aim is to gain a deeper understanding of the critical elements impacting the trade-offs between text recoverability and retrieval effectiveness in dense retrieval systems. This a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Chain-of-Layer&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;&#23454;&#20307;&#38598;&#20013;&#24402;&#32435;&#20998;&#31867;&#20307;&#31995;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38598;&#25104;&#30340;&#25490;&#21517;&#36807;&#28388;&#22120;&#26469;&#20943;&#23569;&#38169;&#35823;&#65292;Chain-of-Layer&#22312;&#22235;&#20010;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07386</link><description>&lt;p&gt;
Chain-of-Layer&#65306;&#36890;&#36807;&#26377;&#38480;&#31034;&#20363;&#36845;&#20195;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20307;&#31995;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Chain-of-Layer&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;&#23454;&#20307;&#38598;&#20013;&#24402;&#32435;&#20998;&#31867;&#20307;&#31995;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38598;&#25104;&#30340;&#25490;&#21517;&#36807;&#28388;&#22120;&#26469;&#20943;&#23569;&#38169;&#35823;&#65292;Chain-of-Layer&#22312;&#22235;&#20010;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20998;&#31867;&#20307;&#31995;&#24402;&#32435;&#23545;&#20110;&#32593;&#32476;&#25628;&#32034;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#38382;&#31572;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#25163;&#21160;&#25972;&#29702;&#20998;&#31867;&#20307;&#31995;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#25104;&#26412;&#65292;&#22240;&#27492;&#33258;&#21160;&#26500;&#24314;&#20998;&#31867;&#20307;&#31995;&#38750;&#24120;&#26377;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Chain-of-Layer&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;&#23454;&#20307;&#38598;&#20013;&#24402;&#32435;&#20998;&#31867;&#20307;&#31995;&#12290;Chain-of-Layer&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#27599;&#19968;&#23618;&#36873;&#25321;&#30456;&#20851;&#20505;&#36873;&#23454;&#20307;&#65292;&#24182;&#36880;&#27493;&#20174;&#19978;&#21040;&#19979;&#26500;&#24314;&#20998;&#31867;&#20307;&#31995;&#12290;&#20026;&#20102;&#20943;&#23569;&#38169;&#35823;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#38598;&#25104;&#30340;&#25490;&#21517;&#36807;&#28388;&#22120;&#65292;&#22312;&#27599;&#19968;&#27425;&#36845;&#20195;&#20013;&#20943;&#23569;&#29983;&#25104;&#30340;&#34394;&#26500;&#20869;&#23481;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Chain-of-Layer&#22312;&#22235;&#20010;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic taxonomy induction is crucial for web search, recommendation systems, and question answering. Manual curation of taxonomies is expensive in terms of human effort, making automatic taxonomy construction highly desirable. In this work, we introduce Chain-of-Layer which is an in-context learning framework designed to induct taxonomies from a given set of entities. Chain-of-Layer breaks down the task into selecting relevant candidate entities in each layer and gradually building the taxonomy from top to bottom. To minimize errors, we introduce the Ensemble-based Ranking Filter to reduce the hallucinated content generated at each iteration. Through extensive experiments, we demonstrate that Chain-of-Layer achieves state-of-the-art performance on four real-world benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#30340;&#20869;&#23384;&#38656;&#27714;&#22686;&#21152;&#21644;&#25512;&#26029;&#36895;&#24230;&#21463;&#38480;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02750</link><description>&lt;p&gt;
KIVI&#65306;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#30340;&#20869;&#23384;&#38656;&#27714;&#22686;&#21152;&#21644;&#25512;&#26029;&#36895;&#24230;&#21463;&#38480;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#26381;&#21153;&#38656;&#35201;&#23558;&#35768;&#22810;&#35831;&#27714;&#25209;&#37327;&#22788;&#29702;&#20197;&#20943;&#23569;&#27599;&#20010;&#35831;&#27714;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#20197;&#36991;&#20813;&#37325;&#26032;&#35745;&#31639;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#26174;&#33879;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#25104;&#20026;&#36895;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#26032;&#29942;&#39048;&#12290;&#36825;&#31181;&#20869;&#23384;&#38656;&#27714;&#38543;&#30528;&#25209;&#22788;&#29702;&#22823;&#23567;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;KV&#32531;&#23384;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;GPU&#30340;SRAM&#24517;&#39035;&#20174;&#20027;GPU&#20869;&#23384;&#20013;&#21152;&#36733;&#25972;&#20010;KV&#32531;&#23384;&#20197;&#29983;&#25104;&#27599;&#20010;&#26631;&#35760;&#65292;&#23548;&#33268;&#35745;&#31639;&#26680;&#24515;&#22312;&#27492;&#36807;&#31243;&#20013;&#22788;&#20110;&#31354;&#38386;&#29366;&#24577;&#12290;&#20943;&#23567;KV&#32531;&#23384;&#22823;&#23567;&#30340;&#19968;&#20010;&#30452;&#25509;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#37327;&#21270;&#65292;&#36890;&#36807;&#20943;&#23569;KV&#32531;&#23384;&#25152;&#38656;&#30340;&#24635;&#23383;&#33410;&#25968;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;KV&#32531;&#23384;&#20803;&#32032;&#20998;&#24067;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#20197;&#20102;&#35299;KV&#32531;&#23384;&#37327;&#21270;&#30340;&#38590;&#24230;&#21644;&#38480;&#21046;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#20803;&#32032;&#20998;&#24067;&#30740;&#31350;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU's SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribut
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#21407;&#22987;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#38405;&#35835;&#29702;&#35299;&#25991;&#26412;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24615;&#33021;&#22987;&#32456;&#24471;&#21040;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2309.09530</link><description>&lt;p&gt;
&#36890;&#36807;&#38405;&#35835;&#29702;&#35299;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models via Reading Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09530
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#21407;&#22987;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#38405;&#35835;&#29702;&#35299;&#25991;&#26412;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24615;&#33021;&#22987;&#32456;&#24471;&#21040;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#35821;&#26009;&#24211;&#19978;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#21407;&#22987;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#36171;&#20104;&#27169;&#22411;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#26497;&#22823;&#22320;&#25439;&#23475;&#20102;&#20854;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#21463;&#20154;&#31867;&#36890;&#36807;&#38405;&#35835;&#29702;&#35299;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#21363;&#38405;&#35835;&#21518;&#32451;&#20064;&#25552;&#39640;&#22522;&#20110;&#25152;&#23398;&#30693;&#35782;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21407;&#22987;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#38405;&#35835;&#29702;&#35299;&#25991;&#26412;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#27599;&#20010;&#21407;&#22987;&#25991;&#26412;&#37117;&#20250;&#34987;&#19968;&#31995;&#21015;&#19982;&#20854;&#20869;&#23481;&#30456;&#20851;&#30340;&#20219;&#21153;&#20016;&#23500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#21487;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#65288;&#29983;&#29289;&#21307;&#23398;&#12289;&#37329;&#34701;&#21644;&#27861;&#24459;&#65289;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#21319;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;7B&#35821;&#35328;&#27169;&#22411;&#22312;&#31454;&#20105;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#19982;&#35268;&#27169;&#26356;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#65288;&#22914;BloombergGPT-50B&#65289;&#30456;&#23218;&#32654;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09530v2 Announce Type: replace  Abstract: We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on the learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#35821;&#20041;&#32593;&#32476;&#65292;&#20026;&#21697;&#29260;&#24418;&#35937;&#30340;&#26356;&#22909;&#25506;&#32034;&#21644;&#36830;&#25509;&#24615;&#30340;&#25913;&#36827;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16228</link><description>&lt;p&gt;
&#21697;&#29260;&#32593;&#32476;&#22686;&#24378;&#22120;&#65306;&#25552;&#21319;&#21697;&#29260;&#36830;&#25509;&#24615;&#30340;&#26032;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Brand Network Booster: A New System for Improving Brand Connectivity. (arXiv:2309.16228v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#35821;&#20041;&#32593;&#32476;&#65292;&#20026;&#21697;&#29260;&#24418;&#35937;&#30340;&#26356;&#22909;&#25506;&#32034;&#21644;&#36830;&#25509;&#24615;&#30340;&#25913;&#36827;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#35821;&#20041;&#32593;&#32476;&#65292;&#20026;&#21697;&#29260;&#24418;&#35937;&#30340;&#26356;&#22909;&#25506;&#32034;&#21644;&#36830;&#25509;&#24615;&#30340;&#25913;&#36827;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;&#22312;&#32593;&#32476;&#20998;&#26512;&#26041;&#38754;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#25193;&#23637;&#29256;&#30340;&#26368;&#22823;&#36830;&#25509;&#24230;&#25913;&#36827;&#38382;&#39064;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20854;&#20013;&#21253;&#25324;&#32771;&#34385;&#25932;&#23545;&#33410;&#28857;&#12289;&#32422;&#26463;&#39044;&#31639;&#21644;&#21152;&#26435;&#32593;&#32476;&#30340;&#21487;&#33021;&#24615; - &#36890;&#36807;&#28155;&#21152;&#38142;&#25509;&#25110;&#22686;&#21152;&#29616;&#26377;&#36830;&#25509;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#36830;&#25509;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#32467;&#21512;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#23637;&#31034;&#36825;&#20010;&#26032;&#31995;&#32479;&#65292;&#24182;&#35752;&#35770;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#23545;&#20110;&#32593;&#32476;&#23398;&#32773;&#21644;&#25903;&#25345;&#24066;&#22330;&#33829;&#38144;&#21644;&#20256;&#25773;&#31649;&#29702;&#32773;&#30340;&#25112;&#30053;&#20915;&#31574;&#36807;&#31243;&#37117;&#24456;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new decision support system offered for an in-depth analysis of semantic networks, which can provide insights for a better exploration of a brand's image and the improvement of its connectivity. In terms of network analysis, we show that this goal is achieved by solving an extended version of the Maximum Betweenness Improvement problem, which includes the possibility of considering adversarial nodes, constrained budgets, and weighted networks - where connectivity improvement can be obtained by adding links or increasing the weight of existing connections. We present this new system together with two case studies, also discussing its performance. Our tool and approach are useful both for network scholars and for supporting the strategic decision-making processes of marketing and communication managers.
&lt;/p&gt;</description></item><item><title>Belebele&#26159;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23567;&#22411;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2308.16884</link><description>&lt;p&gt;
Belebele&#22522;&#20934;&#25968;&#25454;&#38598;&#65306;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#24182;&#34892;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants. (arXiv:2308.16884v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16884
&lt;/p&gt;
&lt;p&gt;
Belebele&#26159;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23567;&#22411;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Belebele&#65292;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#22522;&#20934;&#30340;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#65292;&#20351;&#24471;&#21487;&#20197;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#22522;&#20110;Flores-200&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#20010;&#30701;&#31687;&#25991;&#31456;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#20010;&#22810;&#36873;&#31572;&#26696;&#12290;&#38382;&#39064;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#65292;&#20197;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#27700;&#24179;&#30340;&#27169;&#22411;&#12290;&#21333;&#29420;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#24050;&#32463;&#36275;&#22815;&#22256;&#38590;&#65292;&#21487;&#20197;&#25361;&#25112;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#30001;&#20110;&#23436;&#20840;&#24182;&#34892;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#30452;&#25509;&#27604;&#36739;&#25152;&#26377;&#35821;&#35328;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#20294;&#23567;&#22411;MLMs&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much small
&lt;/p&gt;</description></item><item><title>LyricWhiz&#26159;&#19968;&#31181;&#40065;&#26834;&#12289;&#22810;&#35821;&#35328;&#12289;&#38646;&#23556;&#20987;&#30340;&#33258;&#21160;&#27468;&#35789;&#36716;&#24405;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#20316;&#20026;"&#32819;&#26421;"&#21644;GPT-4&#20316;&#20026;"&#22823;&#33041;"&#65292;&#23427;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#27468;&#35789;&#36716;&#24405;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.17103</link><description>&lt;p&gt;
LyricWhiz: &#36890;&#36807;&#21521;ChatGPT&#32819;&#35821;&#36827;&#34892;&#40065;&#26834;&#30340;&#22810;&#35821;&#35328;&#38646;&#23556;&#20987;&#27468;&#35789;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT. (arXiv:2306.17103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17103
&lt;/p&gt;
&lt;p&gt;
LyricWhiz&#26159;&#19968;&#31181;&#40065;&#26834;&#12289;&#22810;&#35821;&#35328;&#12289;&#38646;&#23556;&#20987;&#30340;&#33258;&#21160;&#27468;&#35789;&#36716;&#24405;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#20316;&#20026;"&#32819;&#26421;"&#21644;GPT-4&#20316;&#20026;"&#22823;&#33041;"&#65292;&#23427;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#27468;&#35789;&#36716;&#24405;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LyricWhiz&#30340;&#40065;&#26834;&#12289;&#22810;&#35821;&#35328;&#12289;&#38646;&#23556;&#20987;&#30340;&#33258;&#21160;&#27468;&#35789;&#36716;&#24405;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27969;&#27966;&#22914;&#25671;&#28378;&#21644;&#37329;&#23646;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#30340;&#20840;&#26032;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Whisper&#65292;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#21450;GPT-4&#65292;&#24403;&#20170;&#26368;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;Whisper&#20805;&#24403;&#8220;&#32819;&#26421;&#8221;&#65292;&#36127;&#36131;&#36716;&#24405;&#35821;&#38899;&#65292;&#32780;GPT-4&#21017;&#20316;&#20026;&#8220;&#22823;&#33041;&#8221;&#65292;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#24615;&#33021;&#30340;&#19978;&#19979;&#25991;&#36755;&#20986;&#36873;&#25321;&#21644;&#26657;&#27491;&#30340;&#27880;&#37322;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;LyricWhiz&#22312;&#33521;&#35821;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#24405;&#22810;&#31181;&#35821;&#35328;&#30340;&#27468;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;LyricWhiz&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;CC-BY-NC-SA&#29256;&#26435;&#35768;&#21487;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;MTG-Jamendo&#65292;&#24182;&#25552;&#20379;&#20102;h
&lt;/p&gt;
&lt;p&gt;
We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today's most performant chat-based large language model. In the proposed method, Whisper functions as the "ear" by transcribing the audio, while GPT-4 serves as the "brain," acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copyright license, based on MTG-Jamendo, and offer a h
&lt;/p&gt;</description></item></channel></rss>