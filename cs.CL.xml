<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;SAMMO&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32534;&#35793;&#26102;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02319</link><description>&lt;p&gt;
Prompt&#20316;&#20026;&#31243;&#24207;&#65306;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39640;&#25928;&#32534;&#35793;&#26102;Prompt&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SAMMO&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32534;&#35793;&#26102;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29616;&#22312;&#33021;&#22788;&#29702;&#26356;&#38271;&#26356;&#22797;&#26434;&#30340;&#36755;&#20837;&#65292;&#36825;&#20419;&#36827;&#20102;&#26356;&#22797;&#26434;&#25552;&#31034;&#30340;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25552;&#31034;&#36890;&#24120;&#38656;&#35201;&#19968;&#20123;&#35843;&#25972;&#20197;&#25552;&#39640;&#37096;&#32626;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#38543;&#30528;&#25552;&#31034;&#22797;&#26434;&#24230;&#21644;LLM&#24378;&#24230;&#30340;&#22686;&#21152;&#65292;&#35768;&#22810;&#25552;&#31034;&#20248;&#21270;&#25216;&#26415;&#24050;&#19981;&#20877;&#36275;&#22815;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SAMMO&#65292;&#19968;&#20010;&#29992;&#20110;&#20803;&#25552;&#31034;&#31243;&#24207;&#30340;{\em &#32534;&#35793;&#26102;}&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#25552;&#31034;&#34920;&#31034;&#20026;&#32467;&#26500;&#21270;&#23545;&#35937;&#65292;&#20801;&#35768;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#25628;&#32034;&#19968;&#32452;&#20016;&#23500;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;SAMMO&#25512;&#24191;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#25351;&#20196;&#35843;&#25972;&#12289;RAG&#31649;&#32447;&#35843;&#25972;&#21644;&#25552;&#31034;&#21387;&#32553;&#26041;&#38754;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#25918;&#25152;&#26377;&#20195;&#30721;&#20379;&#22823;&#23478;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02319v1 Announce Type: cross  Abstract: Large language models (LLMs) can now handle longer and more complex inputs, which facilitate the use of more elaborate prompts. However, prompts often require some tuning to improve performance for deployment. Recent work has proposed automatic prompt optimization methods, but as prompt complexity and LLM strength increase, many prompt optimization techniques are no longer sufficient and a new approach is needed to optimize {\em meta prompt programs}. To address this, we introduce SAMMO, a framework for {\em compile-time} optimizations of metaprompt programs, which represent prompts as structured objects that allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs.   We make all code available open-sou
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20391;&#37325;&#20110;&#36776;&#35782;&#20167;&#24680;&#35328;&#35770;&#30340;&#26263;&#31034;&#30446;&#26631;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#30446;&#26631;&#36328;&#24230;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#65292;&#30446;&#30340;&#26159;&#35782;&#21035;&#26356;&#21152;&#24494;&#22937;&#30340;&#20167;&#24680;&#35328;&#35770;&#24182;&#22686;&#24378;&#23545;&#25968;&#23383;&#24179;&#21488;&#19978;&#26377;&#23475;&#20869;&#23481;&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.19836</link><description>&lt;p&gt;
&#38544;&#24335;&#26377;&#23475;&#20869;&#23481;&#30340;&#30446;&#26631;&#36328;&#24230;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Target Span Detection for Implicit Harmful Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19836
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20391;&#37325;&#20110;&#36776;&#35782;&#20167;&#24680;&#35328;&#35770;&#30340;&#26263;&#31034;&#30446;&#26631;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#30446;&#26631;&#36328;&#24230;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#65292;&#30446;&#30340;&#26159;&#35782;&#21035;&#26356;&#21152;&#24494;&#22937;&#30340;&#20167;&#24680;&#35328;&#35770;&#24182;&#22686;&#24378;&#23545;&#25968;&#23383;&#24179;&#21488;&#19978;&#26377;&#23475;&#20869;&#23481;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36776;&#35782;&#20167;&#24680;&#35328;&#35770;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;&#27492;&#31867;&#35328;&#35770;&#24615;&#36136;&#30340;&#20851;&#38190;&#19968;&#27493;&#65292;&#26368;&#32456;&#26377;&#21161;&#20110;&#25913;&#36827;&#22312;&#32447;&#35770;&#22363;&#19978;&#20882;&#29359;&#24615;&#24086;&#23376;&#30340;&#26816;&#27979;&#12290;&#22312;&#32447;&#24179;&#21488;&#19978;&#35768;&#22810;&#26377;&#23475;&#20869;&#23481;&#20351;&#29992;&#38544;&#21547;&#35821;&#35328;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#33030;&#24369;&#21644;&#21463;&#20445;&#25252;&#32676;&#20307;&#65292;&#20363;&#22914;&#20351;&#29992;&#21051;&#26495;&#30340;&#29305;&#24449;&#32780;&#38750;&#26126;&#31034;&#30340;&#30446;&#26631;&#21517;&#31216;&#65292;&#36825;&#20351;&#24471;&#26816;&#27979;&#21644;&#20943;&#36731;&#20854;&#35821;&#35328;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#36776;&#35782;&#20167;&#24680;&#35328;&#35770;&#30340;&#26263;&#31034;&#30446;&#26631;&#65292;&#36825;&#23545;&#35782;&#21035;&#26356;&#21152;&#24494;&#22937;&#30340;&#20167;&#24680;&#35328;&#35770;&#21450;&#22686;&#24378;&#25968;&#23383;&#24179;&#21488;&#19978;&#26377;&#23475;&#20869;&#23481;&#30340;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26088;&#22312;&#35782;&#21035;&#21363;&#20351;&#26410;&#26126;&#31034;&#30340;&#30446;&#26631;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#33879;&#21517;&#30340;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65288;SBIC&#12289;DynaHate&#21644;IHC&#65289;&#20013;&#25910;&#38598;&#24182;&#26631;&#27880;&#30446;&#26631;&#36328;&#24230;&#12290;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#21512;&#24182;&#38598;&#21512;&#21629;&#21517;&#20026;&#38544;&#21547;-&#30446;&#26631;-&#36328;&#24230;&#12290;&#36825;&#19968;&#38598;&#21512;&#26159;&#36890;&#36807;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#27861;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19836v1 Announce Type: new  Abstract: Identifying the targets of hate speech is a crucial step in grasping the nature of such speech and, ultimately, in improving the detection of offensive posts on online forums. Much harmful content on online platforms uses implicit language especially when targeting vulnerable and protected groups such as using stereotypical characteristics instead of explicit target names, making it harder to detect and mitigate the language. In this study, we focus on identifying implied targets of hate speech, essential for recognizing subtler hate speech and enhancing the detection of harmful content on digital platforms. We define a new task aimed at identifying the targets even when they are not explicitly stated. To address that task, we collect and annotate target spans in three prominent implicit hate speech datasets: SBIC, DynaHate, and IHC. We call the resulting merged collection Implicit-Target-Span. The collection is achieved using an innovat
&lt;/p&gt;</description></item><item><title>LlamaFactory&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.13372</link><description>&lt;p&gt;
LlamaFactory&#65306;100&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13372
&lt;/p&gt;
&lt;p&gt;
LlamaFactory&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#24494;&#35843;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#27169;&#22411;&#19978;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#38750;&#24179;&#20961;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LlamaFactory&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19968;&#22871;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#20869;&#32622;&#30340;Web UI LlamaBoard &#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#65292;&#26080;&#38656;&#32534;&#30721;&#12290;&#25105;&#20204;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#24050;&#21457;&#24067;&#22312; https://github.com/hiyouga/LLaMA-Factory&#65292;&#24182;&#24050;&#33719;&#24471;&#36229;&#36807;13,000&#39063;&#26143;&#21644;1,600&#20010;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13372v1 Announce Type: new  Abstract: Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks.
&lt;/p&gt;</description></item><item><title>MIntRec2.0&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#21644;&#23545;&#35805;&#20013;&#22330;&#22806;&#26816;&#27979;&#25361;&#25112;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;30&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#30340;1,245&#20010;&#23545;&#35805;&#21644;15,040&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#36924;&#30495;&#30340;&#22330;&#22806;&#26679;&#26412;&#65292;&#24182;&#20016;&#23500;&#20102;&#21457;&#35328;&#32773;&#20449;&#24687;&#20197;&#25903;&#25345;&#22810;&#26041;&#23545;&#35805;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.10943</link><description>&lt;p&gt;
MIntRec2.0&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#21644;&#23545;&#35805;&#20013;&#22330;&#22806;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10943
&lt;/p&gt;
&lt;p&gt;
MIntRec2.0&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#21644;&#23545;&#35805;&#20013;&#22330;&#22806;&#26816;&#27979;&#25361;&#25112;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;30&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#30340;1,245&#20010;&#23545;&#35805;&#21644;15,040&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#36924;&#30495;&#30340;&#22330;&#22806;&#26679;&#26412;&#65292;&#24182;&#20016;&#23500;&#20102;&#21457;&#35328;&#32773;&#20449;&#24687;&#20197;&#25903;&#25345;&#22810;&#26041;&#23545;&#35805;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#25972;&#21512;&#26469;&#33258;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#30340;&#38750;&#35821;&#35328;&#24418;&#24335;&#65292;&#20197;&#22686;&#24378;&#23545;&#20154;&#31867;&#24847;&#22270;&#30340;&#29702;&#35299;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#19978;&#21463;&#38480;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22810;&#36718;&#23545;&#35805;&#20114;&#21160;&#20013;&#20986;&#29616;&#30340;&#22330;&#22806;&#26679;&#26412;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MIntRec2.0&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26041;&#23545;&#35805;&#20013;&#30340;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;1,245&#20010;&#23545;&#35805;&#65292;15,040&#20010;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#22312;30&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#30340;&#26032;&#24847;&#22270;&#20998;&#31867;&#20013;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#38500;&#20102;9,304&#20010;&#22330;&#20869;&#26679;&#26412;&#22806;&#65292;&#36824;&#21253;&#25324;5,736&#20010;&#20986;&#29616;&#22312;&#22810;&#36718;&#19978;&#19979;&#25991;&#20013;&#30340;&#22330;&#22806;&#26679;&#26412;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#33258;&#28982;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#27599;&#20010;&#35805;&#35821;&#20013;&#21457;&#35328;&#32773;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#20016;&#23500;&#20102;&#23427;&#22312;&#22810;&#26041;&#23545;&#35805;&#30740;&#31350;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10943v1 Announce Type: cross  Abstract: Multimodal intent recognition poses significant challenges, requiring the incorporation of non-verbal modalities from real-world contexts to enhance the comprehension of human intentions. Existing benchmark datasets are limited in scale and suffer from difficulties in handling out-of-scope samples that arise in multi-turn conversational interactions. We introduce MIntRec2.0, a large-scale benchmark dataset for multimodal intent recognition in multi-party conversations. It contains 1,245 dialogues with 15,040 samples, each annotated within a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope samples, it also includes 5,736 out-of-scope samples appearing in multi-turn contexts, which naturally occur in real-world scenarios. Furthermore, we provide comprehensive information on the speakers in each utterance, enriching its utility for multi-party conversational research. We establish a general framework supporting the o
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#65288;CoAT&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#35757;&#32451;&#22330;&#26223;&#65292;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#21033;&#29992;&#31867;&#27604;&#25512;&#29702;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;CoAT&#65292;&#39044;&#35757;&#32451;&#30340;transformers&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28436;&#31034;&#20013;&#30340;&#26032;&#28508;&#22312;&#27010;&#24565;&#65292;&#20351;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#20989;&#25968;&#21464;&#25442;&#26356;&#21152; robust&#12290;</title><link>https://arxiv.org/abs/2403.09703</link><description>&lt;p&gt;
&#27010;&#24565;&#24863;&#30693;&#25968;&#25454;&#26500;&#24314;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Concept-aware Data Construction Improves In-context Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#65288;CoAT&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#35757;&#32451;&#22330;&#26223;&#65292;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#21033;&#29992;&#31867;&#27604;&#25512;&#29702;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;CoAT&#65292;&#39044;&#35757;&#32451;&#30340;transformers&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28436;&#31034;&#20013;&#30340;&#26032;&#28508;&#22312;&#27010;&#24565;&#65292;&#20351;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#20989;&#25968;&#21464;&#25442;&#26356;&#21152; robust&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#34920;&#29616;&#20026;LMs&#33021;&#22815;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25191;&#34892;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#26377;&#20851;&#31574;&#21010;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#30340;&#24037;&#20316;&#20551;&#23450;ICL&#26159;&#30001;&#20110;&#24040;&#22823;&#30340;&#36807;&#21442;&#25968;&#21270;&#25110;&#22810;&#20219;&#21153;&#35757;&#32451;&#35268;&#27169;&#23548;&#33268;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#24037;&#20316;&#23558;ICL&#33021;&#21147;&#24402;&#22240;&#20110;&#27010;&#24565;&#30456;&#20851;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22312;&#23567;&#35268;&#27169;&#12289;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#20102;&#21151;&#33021;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09703v1 Announce Type: cross  Abstract: Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs' ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges from a vast over-parametrization or the scale of multi-task training. However, recent theoretical work attributes the ICL ability to concept-dependent training data and creates functional in-context learners even in small-scale, synthetic settings.   In this work, we practically explore this newly identified axis of ICL quality. We propose Concept-aware Training (CoAT), a framework for constructing training scenarios that make it beneficial for the LM to learn to utilize the analogical reasoning concepts from demonstrations. We find that by using CoAT, pre-trained transformers can learn to better utilise new latent concepts from demonstrations and that such ability makes ICL more robust to the functio
&lt;/p&gt;</description></item><item><title>Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03640</link><description>&lt;p&gt;
Apollo&#65306;&#36731;&#37327;&#32423;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65306;&#35753;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#26222;&#24800;60&#20159;&#20154;
&lt;/p&gt;
&lt;p&gt;
Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03640
&lt;/p&gt;
&lt;p&gt;
Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#29699;&#21307;&#23398;&#30693;&#35782;&#30340;&#24222;&#22823;&#23384;&#20648;&#24211;&#20027;&#35201;&#26159;&#20197;&#33521;&#35821;&#20026;&#20027;&#65292;&#20294;&#22312;&#20256;&#36882;&#37327;&#36523;&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#26041;&#38754;&#65292;&#26412;&#22320;&#35821;&#35328;&#23545;&#20110;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#23558;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#20154;&#32676;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#28085;&#30422;&#20840;&#29699;61&#20159;&#20154;&#21475;&#30340;&#20845;&#31181;&#26368;&#24120;&#29992;&#35821;&#35328;&#30340;&#21307;&#23398;LLMs&#12290;&#36825;&#19968;&#21162;&#21147;&#26368;&#32456;&#20419;&#25104;&#20102;ApolloCorpora&#22810;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#38598;&#21644;XMedBench&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#22312;&#22810;&#35821;&#35328;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21457;&#24067;&#30340;Apollo&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#30456;&#23545;&#36739;&#23567;&#23610;&#23544;&#65288;&#21363;0.5B&#12289;1.8B&#12289;2B&#12289;6B&#21644;7B&#65289;&#19978;&#21462;&#24471;&#20102;&#19982;&#21516;&#31561;&#22823;&#23567;&#27169;&#22411;&#26368;&#20339;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;Apollo-7B&#26159;&#36804;&#20170;&#20026;&#27490;&#36798;&#21040;70B&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#36739;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03640v1 Announce Type: cross  Abstract: Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.02990</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#30340;&#25968;&#25454;&#22686;&#24378;&#65306;&#25968;&#25454;&#35270;&#35282;&#12289;&#23398;&#20064;&#33539;&#24335;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02990
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#35757;&#32451;&#26679;&#26412;&#22810;&#26679;&#21270;&#32780;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21450;&#20854;&#20182;&#39046;&#22495;&#20013;&#23427;&#20204;&#25552;&#20379;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#20174;&#25968;&#25454;&#35270;&#35282;&#21644;&#23398;&#20064;&#35270;&#35282;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20174;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#31561;&#12290;&#26412;&#35843;&#26597;&#31361;&#26174;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#26088;&#22312;&#20316;&#20026;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02990v1 Announce Type: cross  Abstract: In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65292;&#20174;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#35282;&#24230;&#36830;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.19350</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65292;&#20174;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#35282;&#24230;&#36830;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21033;&#29992;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#21644;&#25512;&#26029;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#36339;QA&#26041;&#38754;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;PLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#24515;&#29702;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#65292;&#36755;&#20837;&#25991;&#27573;&#20013;&#30340;&#26174;&#24335;&#20449;&#24687;&#19982;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#26410;&#33021;&#20805;&#20998;&#20851;&#27880;&#20174;&#20154;&#31867;&#35748;&#30693;&#30740;&#31350;&#30340;&#35282;&#24230;&#38142;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#22522;&#20110;PLMs&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#65288;PEI&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#25552;&#31034;&#36830;&#25509;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#65292;&#19982;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#23545;&#40784;&#65292;&#29992;&#20110;&#22810;&#36339;QA&#12290;&#25105;&#20204;&#23558;&#36755;&#20837;&#25991;&#27573;&#35270;&#20026;&#26174;&#24335;&#30693;&#35782;&#65292;&#21033;&#29992;&#23427;&#20204;&#36890;&#36807;&#32479;&#19968;&#25552;&#31034;&#25512;&#23548;&#38544;&#24335;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19350v1 Announce Type: new  Abstract: Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to simulate human reasoning and inference processes, achieving proficient performance in multi-hop QA. However, a gap persists between PLMs' reasoning abilities and those of humans when tackling complex problems. Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading. Nevertheless, current research has given insufficient attention to linking input passages and PLMs' pre-training-based knowledge from the perspective of human cognition studies. In this study, we introduce a \textbf{P}rompting \textbf{E}xplicit and \textbf{I}mplicit knowledge (PEI) framework, which uses prompts to connect explicit and implicit knowledge, aligning with human reading process for multi-hop QA. We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified prompt reason
&lt;/p&gt;</description></item><item><title>JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17887</link><description>&lt;p&gt;
JMLR&#65306;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#20197;&#22686;&#24378;&#25512;&#29702;&#21644;&#19987;&#19994;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17887
&lt;/p&gt;
&lt;p&gt;
JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31934;&#20934;&#21307;&#23398;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#21307;&#30103;&#26381;&#21153;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#20851;&#38190;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#20013;&#21457;&#25381;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36825;&#20123;&#31995;&#32479;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#21516;&#26102;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#21644;LLM&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#65288;JMLR&#65289;&#30340;&#26041;&#27861;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#22312;&#22788;&#29702;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#37319;&#29992;&#21516;&#27493;&#35757;&#32451;&#26426;&#21046;&#65292;JMLR&#20943;&#23569;&#20102;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17887v1 Announce Type: new  Abstract: With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems. To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase. This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks. By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering question
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;Llama-2-70B&#22686;&#24378;MultiWOZ&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#65292;&#26377;&#25928;&#35299;&#20915;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#30340;&#38386;&#32842;&#24178;&#25200;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#25215;&#35748;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#24182;&#25512;&#21160;&#20219;&#21153;&#30340;&#36827;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.15248</link><description>&lt;p&gt;
Chitchat&#20316;&#20026;&#24178;&#25200;&#65306;&#21521;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#28155;&#21152;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15248
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;Llama-2-70B&#22686;&#24378;MultiWOZ&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#65292;&#26377;&#25928;&#35299;&#20915;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#30340;&#38386;&#32842;&#24178;&#25200;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#25215;&#35748;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#24182;&#25512;&#21160;&#20219;&#21153;&#30340;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#65288;TOD&#65289;&#20013;&#65292;&#20154;&#31867;&#29992;&#25143;&#33258;&#28982;&#20250;&#24341;&#20837;&#36229;&#20986;&#20219;&#21153;&#33539;&#22260;&#30340;&#38386;&#32842;&#65292;&#24178;&#25200;&#20102;&#23545;&#35805;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;Llama-2-70B&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;MultiWOZ&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#65292;&#36825;&#26159;TOD&#20013;&#20856;&#22411;&#30340;&#38386;&#32842;&#24178;&#25200;&#30340;&#19968;&#20010;&#20363;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#20004;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#27492;&#28155;&#21152;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#20165;&#22312;TOD&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21478;&#19968;&#20010;&#22312;TOD&#19978;&#36827;&#34892;&#21021;&#27493;&#38386;&#32842;&#20132;&#20114;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#35757;&#32451;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22312;&#21516;&#19968;&#36718;&#20013;&#25345;&#32493;&#25215;&#35748;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#24182;&#25104;&#21151;&#25512;&#21160;&#20219;&#21153;&#30340;&#36827;&#34892;&#65292;&#36825;&#24471;&#21040;&#20102;&#20154;&#31867;&#35780;&#20272;&#30340;&#30830;&#35748;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#24341;&#20837;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15248v1 Announce Type: new  Abstract: During task-oriented dialogues (TODs), human users naturally introduce chitchat that is beyond the immediate scope of the task, interfering with the flow of the conversation. To address this issue without the need for expensive manual data creation, we use few-shot prompting with Llama-2-70B to enhance the MultiWOZ dataset with user backstories, a typical example of chitchat interference in TODs. We assess the impact of this addition by testing two models: one trained solely on TODs and another trained on TODs with a preliminary chitchat interaction. Our analysis reveals that our enriched dataset poses a significant challenge to these systems. Moreover, we demonstrate that our dataset can be effectively used for training purposes, enabling a system to consistently acknowledge the user's backstory while also successfully moving the task forward in the same turn, as confirmed by human evaluation. These findings highlight the benefits of ge
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#39046;&#22495;&#21644;&#38271;&#24230;&#30340;&#21512;&#25104;NLI&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;NLI&#27169;&#22411;&#22312;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12368</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;NLI&#27169;&#22411;&#39046;&#22495;&#27867;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A synthetic data approach for domain generalization of NLI models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#39046;&#22495;&#21644;&#38271;&#24230;&#30340;&#21512;&#25104;NLI&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;NLI&#27169;&#22411;&#22312;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20173;&#28982;&#26159;LLMs&#30340;&#19968;&#20010;&#37325;&#35201;&#22522;&#20934;&#20219;&#21153;&#12290; NLI&#25968;&#25454;&#38598;&#26159;&#36801;&#31227;&#23398;&#20064;&#21040;&#20854;&#20182;&#35821;&#20041;&#20219;&#21153;&#30340;&#36339;&#26495;&#65292;&#32780;NLI&#27169;&#22411;&#26159;&#35782;&#21035;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#24544;&#23454;&#24615;&#30340;&#26631;&#20934;&#24037;&#20855;&#12290; &#20170;&#22825;&#26377;&#20960;&#20010;&#22823;&#35268;&#27169;&#30340;NLI&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;&#36825;&#20123;&#38598;&#21512;&#19978;&#36827;&#34892;&#29228;&#22369;&#65292;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290; &#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20998;&#24067;/&#39046;&#22495;&#25968;&#25454;&#19978;&#30340;&#23454;&#38469;&#24615;&#33021;&#23578;&#19981;&#24456;&#28165;&#26970;&#12290; &#25105;&#20204;&#23545;NLI&#27169;&#22411;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#38271;&#24230;&#29983;&#25104;&#21512;&#25104;NLI&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#20123;&#25968;&#25454;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#34987;&#29616;&#26377;&#35757;&#32451;&#38598;&#35206;&#30422;&#12290; &#29983;&#25104;&#30340;&#31034;&#20363;&#20855;&#26377;&#26377;&#24847;&#20041;&#30340;&#21069;&#25552;&#65292;&#20551;&#35774;&#20197;&#21019;&#36896;&#24615;&#30340;&#26041;&#24335;&#24418;&#25104;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#22320;&#23545;&#20960;&#20010;&#21069;&#25552;&#26631;&#35760;&#36827;&#34892;&#32534;&#36753;&#65292;&#26631;&#31614;&#30340;&#20934;&#30830;&#29575;&#24456;&#39640;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65288;685K&#20010;&#21512;&#25104;&#31034;&#20363;&#65289;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12368v1 Announce Type: new  Abstract: Natural Language Inference (NLI) remains an important benchmark task for LLMs. NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text. There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections. Yet their realistic performance on out-of-distribution/domain data is less well-understood. We present an in-depth exploration of the problem of domain generalization of NLI models. We demonstrate a new approach for generating synthetic NLI data in diverse domains and lengths, so far not covered by existing training sets. The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy. We show that models trained on this data ($685$K synthetic examples) have the b
&lt;/p&gt;</description></item><item><title>LLMs&#22312;NLG&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23384;&#22312;&#28151;&#28102;&#19981;&#21516;&#35780;&#20272;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#20998;&#31867;&#31995;&#32479;&#21644;&#38024;&#23545;&#19981;&#21516;LLMs&#35780;&#20272;&#34892;&#20026;&#30340;&#25200;&#21160;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;LLMs&#22266;&#26377;&#30340;&#28151;&#28102;&#38382;&#39064;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.12055</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#26159;&#21542;&#28151;&#28102;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#36136;&#37327;&#26631;&#20934;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLM-based Evaluators Confusing NLG Quality Criteria?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12055
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;NLG&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23384;&#22312;&#28151;&#28102;&#19981;&#21516;&#35780;&#20272;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#20998;&#31867;&#31995;&#32479;&#21644;&#38024;&#23545;&#19981;&#21516;LLMs&#35780;&#20272;&#34892;&#20026;&#30340;&#25200;&#21160;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;LLMs&#22266;&#26377;&#30340;&#28151;&#28102;&#38382;&#39064;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#19981;&#21516;&#20219;&#21153;&#30340;NLG&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#20284;&#20046;&#28151;&#28102;&#20102;&#19981;&#21516;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#36991;&#20813;&#29616;&#26377;NLG&#36136;&#37327;&#26631;&#20934;&#20013;&#19981;&#19968;&#33268;&#27010;&#24565;&#21270;&#21644;&#27169;&#31946;&#34920;&#36798;&#30340;&#38382;&#39064;&#26412;&#36523;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#23618;&#27425;&#20998;&#31867;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#20808;&#21069;&#30740;&#31350;&#30340;11&#20010;&#24120;&#35265;&#26041;&#38754;&#30340;&#30456;&#24212;&#19981;&#21516;&#26631;&#20934;&#12290;&#21463;&#34892;&#20026;&#27979;&#35797;&#21551;&#21457;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;18&#31181;&#38024;&#23545;&#19981;&#21516;LLMs&#35780;&#20272;&#34892;&#20026;&#30340;&#26041;&#38754;&#23450;&#21521;&#25200;&#21160;&#25915;&#20987;&#65292;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#36229;&#20986;&#20998;&#31867;&#31995;&#32479;&#25351;&#23548;&#33539;&#22260;&#30340;&#20154;&#31867;&#27880;&#37322;&#65292;&#20197;&#39564;&#35777;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#22266;&#26377;&#30340;&#28151;&#28102;&#38382;&#39064;&#65292;&#20197;&#21450;&#20854;&#20182;&#20540;&#24471;&#20851;&#27880;&#30340;&#29616;&#35937;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12055v1 Announce Type: new  Abstract: Some prior work has shown that LLMs perform well in NLG evaluation for different tasks. However, we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability. For further verification, we first consider avoiding issues of inconsistent conceptualization and vague expression in existing NLG quality criteria themselves. So we summarize a clear hierarchical classification system for 11 common aspects with corresponding different criteria from previous studies involved. Inspired by behavioral testing, we elaborately design 18 types of aspect-targeted perturbation attacks for fine-grained analysis of the evaluation behaviors of different LLMs. We also conduct human annotations beyond the guidance of the classification system to validate the impact of the perturbations. Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further researc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#26694;&#26550;&#65288;LogicCheckGPT&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.11622</link><description>&lt;p&gt;
&#36923;&#36753;&#38381;&#29615;&#65306;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11622
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#26694;&#26550;&#65288;LogicCheckGPT&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#24187;&#35273;&#19968;&#30452;&#26159;&#38459;&#30861;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#26356;&#24191;&#27867;&#24212;&#29992;&#30340;&#36719;&#32907;&#12290;&#23545;&#35937;&#24187;&#35273;&#26159;&#25351;LVLMs&#22312;&#22270;&#20687;&#20013;&#22768;&#31216;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#25351;&#23548;&#35843;&#25972;&#21644;&#22522;&#20110;&#22806;&#37096;&#27169;&#22411;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#22806;&#37096;&#27169;&#22411;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26410;&#28145;&#20837;&#25506;&#35752;&#30340;&#39046;&#22495;&#65292;&#21363;&#21033;&#29992;LVLM&#26412;&#36523;&#26469;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#26679;&#30340;&#30452;&#35273;&#65292;&#21363;LVLM&#20542;&#21521;&#20110;&#23545;&#23384;&#22312;&#30340;&#23545;&#35937;&#20570;&#20986;&#36923;&#36753;&#19968;&#33268;&#30340;&#21453;&#24212;&#65292;&#20294;&#23545;&#24187;&#35273;&#23545;&#35937;&#20570;&#20986;&#19981;&#19968;&#33268;&#30340;&#21453;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#23545;&#35937;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#36731;&#26694;&#26550;&#65292;&#21363;LogicCheckGPT&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36923;&#36753;&#19968;&#33268;&#24615;&#25506;&#27979;&#26469;&#25552;&#20986;&#20855;&#26377;&#36923;&#36753;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11622v1 Announce Type: cross  Abstract: Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical corr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#26469;&#30740;&#31350;LLM&#21644;&#20154;&#31867;&#35009;&#21028;&#30340;&#20559;&#35265;&#65292;&#25581;&#31034;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24378;&#35843;&#35780;&#20272;&#29616;&#26377;LLM&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10669</link><description>&lt;p&gt;
&#20154;&#31867;&#36824;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35009;&#21028;&#65311;&#19968;&#39033;&#20851;&#20110;&#21028;&#20915;&#20559;&#35265;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Humans or LLMs as the Judge? A Study on Judgement Biases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10669
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#26469;&#30740;&#31350;LLM&#21644;&#20154;&#31867;&#35009;&#21028;&#30340;&#20559;&#35265;&#65292;&#25581;&#31034;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24378;&#35843;&#35780;&#20272;&#29616;&#26377;LLM&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#35009;&#21028;&#65288;&#21363;&#20154;&#31867;&#21644;LLM&#20316;&#20026;&#35009;&#21028;&#65289;&#26469;&#35780;&#20272;&#29616;&#26377;LLM&#24615;&#33021;&#30340;&#20570;&#27861;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21516;&#26102;&#21487;&#33021;&#24341;&#20837;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#36136;&#30097;&#35780;&#20272;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;LLM&#21644;&#20154;&#31867;&#35009;&#21028;&#30340;5&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;142&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#28041;&#21450;&#20462;&#35746;&#30340;&#24067;&#21346;&#22982;&#20998;&#31867;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#25104;&#21315;&#19978;&#19975;&#27425;&#30340;&#20154;&#31867;&#21644;LLM&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#22312;&#19981;&#21516;&#31243;&#24230;&#19978;&#37117;&#23481;&#26131;&#21463;&#21040;&#24178;&#25200;&#65292;&#21363;&#20351;&#26368;&#23574;&#31471;&#30340;&#35009;&#21028;&#20063;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#20182;&#20204;&#30340;&#24369;&#28857;&#23545;LLM&#35009;&#21028;&#36827;&#34892;&#25915;&#20987;&#12290;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#25552;&#37266;&#31038;&#32676;&#20851;&#20110;&#20154;&#31867;&#21644;LLM&#20316;&#20026;&#35009;&#21028;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#20197;&#21450;&#21457;&#23637;&#30340;&#32039;&#36843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10669v1 Announce Type: new  Abstract: Adopting human and large language models (LLM) as judges (\textit{a.k.a} human- and LLM-as-a-judge) for evaluating the performance of existing LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLM judges, questioning the reliability of the evaluation results. In this paper, we propose a novel framework for investigating 5 types of biases for LLM and human judges. We curate a dataset with 142 samples referring to the revised Bloom's Taxonomy and conduct thousands of human and LLM evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases. We further exploit their weakness and conduct attacks on LLM judges. We hope that our work can notify the community of the vulnerability of human- and LLM-as-a-judge against perturbations, as well as the urgency of develop
&lt;/p&gt;</description></item><item><title>AI&#21307;&#38498;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#23454;&#26102;&#20132;&#20114;&#24335;&#35786;&#26029;&#29615;&#22659;&#65292;&#36890;&#36807;&#19982;LLMs&#30340;&#20132;&#20114;&#35780;&#20272;&#21644;&#21327;&#20316;&#65292;&#25552;&#39640;&#20020;&#24202;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09742</link><description>&lt;p&gt;
AI&#21307;&#38498;&#65306;&#29992;&#20110;&#20020;&#24202;&#35786;&#26029;&#30340;LLMs&#20316;&#20026;&#23454;&#20064;&#21307;&#29983;&#30340;&#20132;&#20114;&#24335;&#35780;&#20272;&#21644;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09742
&lt;/p&gt;
&lt;p&gt;
AI&#21307;&#38498;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#23454;&#26102;&#20132;&#20114;&#24335;&#35786;&#26029;&#29615;&#22659;&#65292;&#36890;&#36807;&#19982;LLMs&#30340;&#20132;&#20114;&#35780;&#20272;&#21644;&#21327;&#20316;&#65292;&#25552;&#39640;&#20020;&#24202;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#26631;&#24535;&#30528;&#37325;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#36776;&#21035;&#21644;&#38382;&#31572;&#20219;&#21153;&#65292;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#20854;&#20132;&#20114;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;AI&#21307;&#38498;&#65292;&#19968;&#20010;&#26088;&#22312;&#26500;&#24314;&#23454;&#26102;&#20132;&#20114;&#24335;&#35786;&#26029;&#29615;&#22659;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#27169;&#25311;&#36807;&#31243;&#65292;&#25105;&#20204;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#21307;&#30103;&#35760;&#24405;&#65292;&#21019;&#24314;&#20102;&#24739;&#32773;&#12289;&#26816;&#26597;&#32773;&#21644;&#21307;&#30103;&#20027;&#20219;&#20195;&#29702;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;AI&#21307;&#38498;&#36827;&#34892;LLMs&#30340;&#20132;&#20114;&#35780;&#20272;&#21644;&#21327;&#20316;&#12290;&#21021;&#22987;&#38454;&#27573;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35270;&#22270;&#21307;&#23398;&#35780;&#20272;&#65288;MVME&#65289;&#22522;&#20934;&#65292;&#20854;&#20013;&#21508;&#31181;LLMs&#20316;&#20026;&#23454;&#20064;&#21307;&#29983;&#36827;&#34892;&#20132;&#20114;&#24335;&#35786;&#26029;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21327;&#20316;&#26426;&#21046;&#65292;&#28041;&#21450;&#21307;&#30103;&#20027;&#20219;&#30340;&#30417;&#30563;&#19979;&#30340;&#36845;&#20195;&#35752;&#35770;&#21644;&#20105;&#35758;&#35299;&#20915;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09742v1 Announce Type: new  Abstract: The incorporation of Large Language Models (LLMs) in healthcare marks a significant advancement. However, the application has predominantly been limited to discriminative and question-answering tasks, which does not fully leverage their interactive potential. To address this limitation, our paper presents AI Hospital, a framework designed to build a real-time interactive diagnosis environment. To simulate the procedure, we collect high-quality medical records to create patient, examiner, and medical director agents. AI Hospital is then utilized for the interactive evaluation and collaboration of LLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmark where various LLMs serve as intern doctors for interactive diagnosis. Subsequently, to improve diagnostic accuracy, we introduce a collaborative mechanism that involves iterative discussions and a dispute resolution process under the supervision of the medical director. I
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08114</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Preference Learning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#24494;&#35843;&#25216;&#26415;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23545;&#20110;&#23545;&#40784;&#36825;&#20123;&#27169;&#22411;&#26469;&#35828;&#65292;&#26368;&#20851;&#38190;&#30340;&#32771;&#34385;&#26159;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#20154;&#21147;&#36164;&#28304;&#65292;&#25110;&#32773;&#22312;LLM&#26412;&#36523;&#34987;&#29992;&#20316;oracle&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#27169;&#22411;&#36164;&#28304;&#12290;&#20174;&#20154;&#31867;&#25110;AI&#20559;&#22909;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF / RLAIF&#65289;&#26159;&#36825;&#31181;&#25216;&#26415;&#26368;&#31361;&#20986;&#30340;&#20363;&#23376;&#65292;&#20294;&#23427;&#24448;&#24448;&#22797;&#26434;&#19988;&#19981;&#31283;&#23450;&#12290;&#26368;&#36817;&#65292;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#26356;&#31616;&#21333;&#21644;&#26356;&#31283;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;DPO&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#29109;&#21644;DPO&#20248;&#21270;&#30340;&#38544;&#24335;&#20559;&#22909;&#27169;&#22411;&#30340;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#23454;&#29992;&#37319;&#38598;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.
&lt;/p&gt;</description></item><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item><item><title>AutoMix&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#25321;&#26356;&#22823;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#33258;&#25105;&#39564;&#35777;&#21644;&#20803;&#39564;&#35777;&#22120;&#25552;&#39640;&#20102;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#30340;&#20248;&#21270;&#65292;&#23454;&#39564;&#35777;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26368;&#22810;86%.</title><link>https://arxiv.org/abs/2310.12963</link><description>&lt;p&gt;
AutoMix: &#33258;&#21160;&#28151;&#21512;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AutoMix: Automatically Mixing Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.12963
&lt;/p&gt;
&lt;p&gt;
AutoMix&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#25321;&#26356;&#22823;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#33258;&#25105;&#39564;&#35777;&#21644;&#20803;&#39564;&#35777;&#22120;&#25552;&#39640;&#20102;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#30340;&#20248;&#21270;&#65292;&#23454;&#39564;&#35777;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26368;&#22810;86%.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29616;&#22312;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#20113;API&#25552;&#20379;&#21830;&#33719;&#24471;&#12290;&#34429;&#28982;&#36825;&#31181;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#36873;&#25321;&#65292;&#20294;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#36873;&#39033;&#20197;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoMix&#65292;&#19968;&#31181;&#26681;&#25454;&#36739;&#23567;LM&#30340;&#36755;&#20986;&#30340;&#36817;&#20284;&#27491;&#30830;&#24615;&#26469;&#31574;&#30053;&#24615;&#22320;&#23558;&#26597;&#35810;&#36335;&#30001;&#21040;&#26356;&#22823;LM&#30340;&#26041;&#27861;&#12290;AutoMix&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#23569;&#37327;&#26679;&#26412;&#30340;&#33258;&#25105;&#39564;&#35777;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20272;&#35745;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#32780;&#26080;&#38656;&#35757;&#32451;&#12290;&#37492;&#20110;&#39564;&#35777;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#65292;&#25105;&#20204;&#22312;AutoMix&#20013;&#20351;&#29992;&#20102;&#20803;&#39564;&#35777;&#22120;&#26469;&#25552;&#39640;&#36825;&#20123;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;LLAMA2-13B&#21644;GPT-4&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;AutoMix&#36229;&#36234;&#20102;&#24050;&#24314;&#31435;&#30340;&#22522;&#32447;&#65292;&#27599;&#21333;&#20301;&#25104;&#26412;&#30340;&#22686;&#37327;&#25928;&#30410;&#25552;&#39640;&#20102;&#26368;&#22810;86%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.c&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.12963v3 Announce Type: replace  Abstract: Large language models (LLMs) are now available from cloud API providers in various sizes and configurations. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta-verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13B and GPT-4, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 86%. Our code and data are available at https://github.c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#24314;&#27169;&#65292;&#36890;&#36807;&#22312;&#20449;&#24565;&#29366;&#24577;&#36319;&#36394;&#20013;&#24341;&#20837;&#24773;&#24863;&#26816;&#27979;&#23454;&#29616;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#31471;&#21040;&#31471;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#32467;&#26524;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#29992;&#25143;&#30340;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#22238;&#24212;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#65292;&#23545;&#20110;&#25552;&#39640;&#22238;&#24212;&#30340;&#20849;&#40483;&#31243;&#24230;&#20855;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2401.13789</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Emotion Detection and Task-Oriented Dialogue Modeling. (arXiv:2401.13789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#24314;&#27169;&#65292;&#36890;&#36807;&#22312;&#20449;&#24565;&#29366;&#24577;&#36319;&#36394;&#20013;&#24341;&#20837;&#24773;&#24863;&#26816;&#27979;&#23454;&#29616;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#31471;&#21040;&#31471;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#32467;&#26524;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#29992;&#25143;&#30340;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#22238;&#24212;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#65292;&#23545;&#20110;&#25552;&#39640;&#22238;&#24212;&#30340;&#20849;&#40483;&#31243;&#24230;&#20855;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#24773;&#24863;&#26816;&#27979;&#65288;ED&#65289;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#25110;&#32773;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#39033;&#29420;&#31435;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#26080;&#32541;&#22320;&#32479;&#19968;ED&#21644;TOD&#24314;&#27169;&#21487;&#20197;&#24102;&#26469;&#30456;&#20114;&#30340;&#22909;&#22788;&#65292;&#22240;&#27492;&#26159;&#19968;&#31181;&#20540;&#24471;&#32771;&#34385;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23558;ED&#21253;&#21547;&#22312;&#20449;&#24565;&#29366;&#24577;&#36319;&#36394;&#20013;&#65292;&#24182;&#20381;&#36182;&#20110;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26469;&#25193;&#23637;SimpleToD&#36825;&#20010;&#31471;&#21040;&#31471;&#30340;TOD&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-2&#21644;Llama-2&#22312;EmoWOZ&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#24773;&#24863;&#36827;&#34892;&#27880;&#37322;&#30340;MultiWOZ&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;ED&#21644;&#20219;&#21153;&#32467;&#26524;&#30340;&#24615;&#33021;&#26222;&#36941;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#29992;&#25143;&#30340;&#24773;&#24863;&#20026;&#31995;&#32479;&#30340;&#22238;&#24212;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#36827;&#19968;&#27493;&#25913;&#21892;&#22238;&#24212;&#30340;&#20849;&#40483;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In current text-based task-oriented dialogue (TOD) systems, user emotion detection (ED) is often overlooked or is typically treated as a separate and independent task, requiring additional training. In contrast, our work demonstrates that seamlessly unifying ED and TOD modeling brings about mutual benefits, and is therefore an alternative to be considered. Our method consists in augmenting SimpleToD, an end-to-end TOD system, by extending belief state tracking to include ED, relying on a single language model. We evaluate our approach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ annotated with emotions. Our results reveal a general increase in performance for ED and task results. Our findings also indicate that user emotions provide useful contextual conditioning for system responses, and can be leveraged to further refine responses in terms of empathy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#23398;&#38382;&#31572;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.16035</link><description>&lt;p&gt;
MedEdit&#65306;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases. (arXiv:2309.16035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#23398;&#38382;&#31572;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34429;&#28982;&#22312;&#19968;&#33324;&#39046;&#22495;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#38754;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24448;&#24448;&#20316;&#20026;&#8220;&#40657;&#30418;&#8221;&#36816;&#20316;&#65292;&#38590;&#20197;&#20462;&#25913;&#20854;&#34892;&#20026;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#26088;&#22312;&#25913;&#36827;LLM&#30340;&#21709;&#24212;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26816;&#32034;&#31574;&#30053;&#65292;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;LLM&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#12290;&#36890;&#36807;&#23545;MedQA-SMILE&#25968;&#25454;&#38598;&#36827;&#34892;&#21307;&#23398;QA&#30340;&#37325;&#28857;&#30740;&#31350;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#26816;&#32034;&#27169;&#22411;&#21644;&#21521;LLM&#25552;&#20379;&#30340;&#20107;&#23454;&#25968;&#37327;&#23545;&#20854;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#32534;&#36753;&#21518;&#30340;Vicuna&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20174;44.46&#65285;&#25552;&#39640;&#21040;48.54&#65285;&#12290;&#36825;&#39033;&#24037;&#20316;&#20984;&#26174;&#20102;&#27169;&#22411;&#32534;&#36753;&#25913;&#21892;LLM&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20026;&#32531;&#35299;&#40657;&#30418;LLM&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks like medical question answering (QA). Moreover, they tend to function as "black-boxes," making it challenging to modify their behavior. Addressing this, our study delves into model editing utilizing in-context learning, aiming to improve LLM responses without the need for fine-tuning or retraining. Specifically, we propose a comprehensive retrieval strategy to extract medical facts from an external knowledge base, and then we incorporate them into the query prompt for the LLM. Focusing on medical QA using the MedQA-SMILE dataset, we evaluate the impact of different retrieval models and the number of facts provided to the LLM. Notably, our edited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%. This work underscores the potential of model editing to enhance LLM performance, offering a practical approach to mitigate the challenges of black-box LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#30340;&#24555;&#36895;&#23567;&#22411;BERT&#27169;&#22411;&#12290;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#36739;&#23567;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;RoBERTa&#27169;&#22411;&#30456;&#24403;&#30340;95%&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12568</link><description>&lt;p&gt;
&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#30340;&#23567;&#22411;&#24555;&#36895;BERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Small and Fast BERT for Chinese Medical Punctuation Restoration. (arXiv:2308.12568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12568
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#30340;&#24555;&#36895;&#23567;&#22411;BERT&#27169;&#22411;&#12290;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#36739;&#23567;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;RoBERTa&#27169;&#22411;&#30456;&#24403;&#30340;95%&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#21548;&#20889;&#20013;&#65292;&#27809;&#26377;&#26126;&#30830;&#26631;&#28857;&#31526;&#21495;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#23548;&#33268;&#20102;&#23545;&#21548;&#20889;&#25253;&#21578;&#30340;&#35823;&#35299;&#12290;&#20026;&#20102;&#20351;&#29992;ASR&#25552;&#20379;&#31934;&#30830;&#21644;&#26131;&#25026;&#30340;&#20020;&#24202;&#25253;&#21578;&#65292;&#38656;&#35201;&#36827;&#34892;&#33258;&#21160;&#26631;&#28857;&#20462;&#22797;&#12290;&#32771;&#34385;&#21040;&#23454;&#38469;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#24555;&#36895;&#36731;&#37327;&#32423;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65288;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#65289;&#26469;&#25552;&#28860;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#26631;&#28857;&#20462;&#22797;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25552;&#28860;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;RoBERTa&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;10%&#30340;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;95%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In clinical dictation, utterances after automatic speech recognition (ASR) without explicit punctuation marks may lead to the misunderstanding of dictated reports. To give a precise and understandable clinical report with ASR, automatic punctuation restoration is required. Considering a practical scenario, we propose a fast and light pre-trained model for Chinese medical punctuation restoration based on 'pretraining and fine-tuning' paradigm. In this work, we distill pre-trained models by incorporating supervised contrastive learning and a novel auxiliary pre-training task (Punctuation Mark Prediction) to make it well-suited for punctuation restoration. Our experiments on various distilled models reveal that our model can achieve 95% performance while 10% model size relative to state-of-the-art Chinese RoBERTa.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.10635</link><description>&lt;p&gt;
SciBench: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#23637;&#22312;&#35768;&#22810;&#25968;&#23398;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#22823;&#22810;&#21482;&#21253;&#21547;&#21021;&#39640;&#20013;&#31185;&#30446;&#30340;&#38382;&#39064;&#65292;&#20165;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#22871;&#20214;SciBench&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#27979;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#25152;&#38656;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;SciBench&#21253;&#21547;&#20004;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#24320;&#25918;&#38598;&#65292;&#21253;&#25324;&#20174;&#25968;&#23398;&#12289;&#21270;&#23398;&#21644;&#29289;&#29702;&#25945;&#31185;&#20070;&#20013;&#25688;&#24405;&#30340;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#65292;&#20197;&#21450;&#19968;&#20010;&#23553;&#38381;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25968;&#23398;&#26412;&#31185;&#32771;&#35797;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of deli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;GPT-4&#35299;&#20915;&#26356;&#22797;&#26434;&#21644;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MathChat&#30340;&#23545;&#35805;&#24335;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#24182;&#22312;&#22256;&#38590;&#39640;&#20013;&#31454;&#36187;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.01337</link><description>&lt;p&gt;
&#22522;&#20110;GPT-4&#30340;&#22797;&#26434;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Challenging Math Problem Solving with GPT-4. (arXiv:2306.01337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;GPT-4&#35299;&#20915;&#26356;&#22797;&#26434;&#21644;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MathChat&#30340;&#23545;&#35805;&#24335;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#24182;&#22312;&#22256;&#38590;&#39640;&#20013;&#31454;&#36187;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26159;&#19968;&#39033;&#26377;&#36259;&#30340;&#30740;&#31350;&#65292;&#32771;&#34385;&#21040;&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#25968;&#23398;&#38382;&#39064;&#30340;&#20016;&#23500;&#24615;&#12290;&#34429;&#28982;&#20043;&#21069;&#26377;&#20960;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#35299;&#20915;&#21021;&#31561;&#25968;&#23398;&#38382;&#39064;&#65292;&#20294;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;GPT-4&#35299;&#20915;&#26356;&#22797;&#26434;&#21644;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#30340;&#21069;&#27839;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-4&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20123;&#26159;&#20174;&#29616;&#26377;&#24037;&#20316;&#20013;&#25913;&#32534;&#32780;&#26469;&#30340;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;MathChat&#65292;&#36825;&#26159;&#26412;&#30740;&#31350;&#26032;&#25552;&#20986;&#30340;&#19968;&#31181;&#23545;&#35805;&#24335;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;MATH&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#39640;&#20013;&#31454;&#36187;&#38382;&#39064;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#23545;&#35805;&#24335;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Employing Large Language Models (LLMs) to address mathematical problems is an intriguing research endeavor, considering the abundance of math problems expressed in natural language across numerous science and engineering fields. While several prior works have investigated solving elementary mathematics using LLMs, this work explores the frontier of using GPT-4 for solving more complex and challenging math problems. We evaluate various ways of using GPT-4. Some of them are adapted from existing work, and one is \MathChat, a conversational problem-solving framework newly proposed in this work. We perform the evaluation on difficult high school competition problems from the MATH dataset, which shows the advantage of the proposed conversational approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#25552;&#31034;&#20301;&#32622;&#23545;&#20110;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20855;&#26377;&#23454;&#36136;&#24615;&#24433;&#21709;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#20301;&#32622;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#65292;&#25552;&#31034;&#20301;&#32622;&#20248;&#21270;&#24212;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.14493</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;NLU&#20219;&#21153;&#20013;&#25552;&#31034;&#20301;&#32622;&#30830;&#23454;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Prompt position really matters in few-shot and zero-shot NLU tasks. (arXiv:2305.14493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#25552;&#31034;&#20301;&#32622;&#23545;&#20110;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20855;&#26377;&#23454;&#36136;&#24615;&#24433;&#21709;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#20301;&#32622;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#65292;&#25552;&#31034;&#20301;&#32622;&#20248;&#21270;&#24212;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21560;&#24341;&#20102;&#20247;&#22810;&#30740;&#31350;&#32773;&#30340;&#20851;&#27880;&#12290;&#20294;&#26159;&#65292;&#26377;&#25928;&#25552;&#31034;&#27169;&#26495;&#30340;&#24320;&#21457;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#31034;&#35789;&#27719;&#36873;&#25321;&#25110;&#20445;&#30041;&#25552;&#31034;&#20301;&#32622;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#25552;&#31034;&#20301;&#32622;&#36873;&#39033;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#37327;&#21270;&#20102;&#25552;&#31034;&#20301;&#32622;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#23454;&#36136;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#20301;&#32622;&#23545;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#25552;&#31034;&#20301;&#32622;&#20248;&#21270;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#19982;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#37325;&#24515;&#24182;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based models have made remarkable advancements in the fields of zero-shot and few-shot learning, attracting a lot of attention from researchers. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary selection or embedding initialization with the reserved prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position option for natural language understanding tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt position used in prior studies is often sub-optimal for both zero-shot and few-shot settings. These findings suggest prompt position optimisation as an interesting research direction alongside the existing focus on prompt engineering.
&lt;/p&gt;</description></item></channel></rss>