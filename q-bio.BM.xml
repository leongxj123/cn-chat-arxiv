<rss version="2.0"><channel><title>Chat Arxiv q-bio.BM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio.BM</description><item><title>DPLM&#26159;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#34507;&#30333;&#36136;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18567</link><description>&lt;p&gt;
&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Are Versatile Protein Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18567
&lt;/p&gt;
&lt;p&gt;
DPLM&#26159;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#34507;&#30333;&#36136;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25193;&#25955;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;DPLM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23545;&#34507;&#30333;&#36136;&#24207;&#21015;&#20855;&#26377;&#24378;&#22823;&#30340;&#29983;&#25104;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#31181;&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#31163;&#25955;&#25193;&#25955;&#27010;&#29575;&#26694;&#26550;&#20013;&#20174;&#36827;&#21270;&#35268;&#27169;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#39044;&#35757;&#32451;&#21487;&#25193;&#23637;&#30340;DPLM&#65292;&#36825;&#20026;&#34507;&#30333;&#36136;&#30340;&#35821;&#35328;&#24314;&#27169;&#25552;&#20379;&#20102;&#22522;&#26412;&#26041;&#27861;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;DPLM&#23637;&#31034;&#20102;&#29983;&#25104;&#20986;&#31526;&#21512;&#32467;&#26500;&#30340;&#12289;&#26032;&#39062;&#30340;&#12289;&#22810;&#26679;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25193;&#25955;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20351;&#24471;DPLM&#23545;&#34507;&#30333;&#36136;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35299;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20248;&#31168;&#30340;&#34920;&#31034;&#23398;&#20064;&#32773;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#19982;ESM2&#65288;Lin et al., 2022&#65289;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;DPLM&#21487;&#20197;&#38024;&#23545;&#21508;&#31181;&#38656;&#27714;&#36827;&#34892;&#23450;&#21046;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#30340;&#23454;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18567v1 Announce Type: new  Abstract: This paper introduces diffusion protein language model (DPLM), a versatile protein language model that demonstrates strong generative and predictive capabilities for protein sequences. We first pre-train scalable DPLMs from evolutionary-scale protein sequences within a generative self-supervised discrete diffusion probabilistic framework, which generalizes language modeling for proteins in a principled way. After pre-training, DPLM exhibits the ability to generate structurally plausible, novel, and diverse protein sequences for unconditional generation. We further demonstrate the proposed diffusion generative pre-training makes DPLM possess a better understanding of proteins, making it a superior representation learner, which can be fine-tuned for various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022). Moreover, DPLM can be tailored for various needs, which showcases its prowess of conditional generation in several ways
&lt;/p&gt;</description></item></channel></rss>