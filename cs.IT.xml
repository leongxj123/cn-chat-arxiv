<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#23454;&#29616;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#29992;&#20110;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.03106</link><description>&lt;p&gt;
&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Rotation Invariant Quantization for Model Compression. (arXiv:2303.03106v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#23454;&#29616;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#29992;&#20110;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#21387;&#32553;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#12289;&#28040;&#32791;&#20869;&#23384;&#30340;&#27169;&#22411;&#37096;&#32626;&#21040;&#20869;&#23384;&#36164;&#28304;&#26377;&#38480;&#35774;&#22791;&#19978;&#30340;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;NN&#27169;&#22411;&#21387;&#32553;&#30340;&#36895;&#29575;-&#22833;&#30495;&#26435;&#34913;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#37327;&#21270;&#25972;&#20010;NN&#27169;&#22411;&#65292;&#22312;&#27599;&#20010;&#23618;&#27425;&#19978;&#24471;&#21040;&#19981;&#21516;&#30340;&#36895;&#29575;&#65292;&#21363;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26059;&#36716;&#19981;&#21464;&#37327;&#26041;&#27861;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23545;RIQ&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;RIQ&#22312;&#39044;&#35757;&#32451;&#30340;VGG&#31264;&#23494;&#21644;&#20462;&#21098;&#27169;&#22411;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;19.4&#20493;&#21644;52.9&#20493;&#30340;&#21387;&#32553;&#27604;&#65292;&#31934;&#24230;&#38477;&#20302;&#23567;&#20110;0.4%&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;\url{https://github.com/ehaleva/RIQ}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training Neural Network (NN) model compression is an attractive approach for deploying large, memory-consuming models on devices with limited memory resources. In this study, we investigate the rate-distortion tradeoff for NN model compression. First, we suggest a Rotation-Invariant Quantization (RIQ) technique that utilizes a single parameter to quantize the entire NN model, yielding a different rate at each layer, i.e., mixed-precision quantization. Then, we prove that our rotation-invariant approach is optimal in terms of compression. We rigorously evaluate RIQ and demonstrate its capabilities on various models and tasks. For example, RIQ facilitates $\times 19.4$ and $\times 52.9$ compression ratios on pre-trained VGG dense and pruned models, respectively, with $&lt;0.4\%$ accuracy degradation. Code is available in \url{https://github.com/ehaleva/RIQ}.
&lt;/p&gt;</description></item></channel></rss>