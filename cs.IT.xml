<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TREET&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Donsker-Vardhan&#34920;&#31034;&#27861;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#31283;&#23450;&#36807;&#31243;&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20272;&#35745;TE&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#20248;&#21270;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#21644;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06919</link><description>&lt;p&gt;
TREET: &#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
TREET: TRansfer Entropy Estimation via Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TREET&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Donsker-Vardhan&#34920;&#31034;&#27861;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#31283;&#23450;&#36807;&#31243;&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20272;&#35745;TE&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#20248;&#21270;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#21644;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#36755;&#29109;&#65288;TE&#65289;&#26159;&#20449;&#24687;&#35770;&#20013;&#25581;&#31034;&#36807;&#31243;&#20043;&#38388;&#20449;&#24687;&#27969;&#21160;&#26041;&#21521;&#30340;&#24230;&#37327;&#65292;&#23545;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREET&#30340;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#31283;&#23450;&#36807;&#31243;&#30340;TE&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;Donsker-Vardhan&#65288;DV&#65289;&#34920;&#31034;&#27861;&#23545;TE&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31070;&#32463;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;TREET&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#20102;&#22686;&#21152;&#20854;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#34920;&#31034;&#24341;&#29702;&#30340;&#20272;&#35745;TE&#20248;&#21270;&#26041;&#26696;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#26469;&#20248;&#21270;&#20855;&#26377;&#35760;&#24518;&#24615;&#30340;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#65292;&#36825;&#26159;&#20449;&#24687;&#35770;&#20013;&#30340;&#19968;&#20010;&#20856;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer entropy (TE) is a measurement in information theory that reveals the directional flow of information between processes, providing valuable insights for a wide range of real-world applications. This work proposes Transfer Entropy Estimation via Transformers (TREET), a novel transformer-based approach for estimating the TE for stationary processes. The proposed approach employs Donsker-Vardhan (DV) representation to TE and leverages the attention mechanism for the task of neural estimation. We propose a detailed theoretical and empirical study of the TREET, comparing it to existing methods. To increase its applicability, we design an estimated TE optimization scheme that is motivated by the functional representation lemma. Afterwards, we take advantage of the joint optimization scheme to optimize the capacity of communication channels with memory, which is a canonical optimization problem in information theory, and show the memory capabilities of our estimator. Finally, we apply
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#65292;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#24182;&#21457;&#29616;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.04161</link><description>&lt;p&gt;
&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#35268;&#33539;&#20998;&#26512;&#26694;&#26550;&#65306;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30740;&#31350;Transformer&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#65292;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#24182;&#21457;&#29616;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#27169;&#22411;&#22312;&#27492;&#36807;&#31243;&#20013;&#36890;&#36807;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#65292;&#20801;&#35768;&#29702;&#35770;&#21644;&#31995;&#32479;&#23454;&#39564;&#26469;&#30740;&#31350;Transformer&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24314;&#27169;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#28304;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#31995;&#32479;&#22320;&#30740;&#31350;&#25968;&#25454;&#20998;&#24067;&#29305;&#24615;&#12289;Transformer&#26550;&#26500;&#12289;&#23398;&#21040;&#30340;&#20998;&#24067;&#21644;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#65292;&#36825;&#21462;&#20915;&#20110;&#20855;&#20307;&#30340;&#25968;&#25454;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains. Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance. In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data chara
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#22312;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#24847;&#21619;&#30528;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20989;&#25968;&#26063;&#65292;&#24182;&#35752;&#35770;&#20102;&#27809;&#26377;&#22122;&#22768;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14474</link><description>&lt;p&gt;
&#27827;&#24029;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limits to Reservoir Learning. (arXiv:2307.14474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14474
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#22312;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#24847;&#21619;&#30528;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20989;&#25968;&#26063;&#65292;&#24182;&#35752;&#35770;&#20102;&#27809;&#26377;&#22122;&#22768;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#26469;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#65288;IPC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#20449;&#21495;&#38598;&#21512;&#21040;&#23436;&#25972;&#20989;&#25968;&#22522;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;IPC&#26469;&#34913;&#37327;&#22122;&#22768;&#19979;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#65288;&#19968;&#31181;&#29305;&#27530;&#30340;&#24490;&#29615;&#32593;&#32476;&#65289;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;IPC&#22312;&#31995;&#32479;&#23610;&#23544;n&#19978;&#26159;&#19968;&#20010;&#22810;&#39033;&#24335;&#65292;&#21363;&#20351;&#32771;&#34385;&#21040;n&#20010;&#36755;&#20986;&#20449;&#21495;&#30340;$2^n$&#20010;&#21487;&#33021;&#30340;&#36880;&#28857;&#20056;&#31215;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#36864;&#21270;&#24847;&#21619;&#30528;&#22312;&#20648;&#27700;&#24211;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20648;&#27700;&#24211;&#25152;&#34920;&#31034;&#30340;&#20989;&#25968;&#26063;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#27809;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#19968;&#38598;&#21512;&#30340;$2^n$&#20010;&#20989;&#25968;&#22312;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we bound a machine's ability to learn based on computational limitations implied by physicality. We start by considering the information processing capacity (IPC), a normalized measure of the expected squared error of a collection of signals to a complete basis of functions. We use the IPC to measure the degradation under noise of the performance of reservoir computers, a particular kind of recurrent network, when constrained by physical considerations. First, we show that the IPC is at most a polynomial in the system size $n$, even when considering the collection of $2^n$ possible pointwise products of the $n$ output signals. Next, we argue that this degradation implies that the family of functions represented by the reservoir requires an exponential number of samples to learn in the presence of the reservoir's noise. Finally, we conclude with a discussion of the performance of the same collection of $2^n$ functions without noise when being used for binary classification
&lt;/p&gt;</description></item></channel></rss>