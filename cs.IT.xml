<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#19979;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#37327;$O(\lambda/n)$&#26469;&#19978;&#30028;&#20272;&#35745;&#27867;&#21270;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.14658</link><description>&lt;p&gt;
&#20851;&#20110;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the tightness of information-theoretic bounds on generalization error of learning algorithms. (arXiv:2303.14658v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#19979;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#37327;$O(\lambda/n)$&#26469;&#19978;&#30028;&#20272;&#35745;&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Russo&#21644;Xu&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35777;&#26126;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#36890;&#36807;&#20449;&#24687;&#24230;&#37327;&#36827;&#34892;&#19978;&#30028;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25910;&#25947;&#36895;&#24230;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#8220;&#24930;&#8221;&#30340;&#65292;&#22240;&#20026;&#23427;&#30340;&#26399;&#26395;&#25910;&#25947;&#36895;&#24230;&#30340;&#24418;&#24335;&#20026;$O(\sqrt{\lambda/n})$&#65292;&#20854;&#20013;$\lambda$&#26159;&#19968;&#20123;&#20449;&#24687;&#29702;&#35770;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#35777;&#26126;&#20102;&#26681;&#21495;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#25910;&#25947;&#36895;&#24230;&#24930;&#65292;&#21487;&#20197;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#20351;&#29992;&#36825;&#20010;&#30028;&#38480;&#26469;&#24471;&#21040;$O(\lambda/n)$&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36798;&#21040;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#21363;&#25152;&#35859;&#30340;$(\eta,c)$-&#20013;&#24515;&#26465;&#20214;&#12290;&#22312;&#36825;&#20010;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#30340;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent line of works, initiated by Russo and Xu, has shown that the generalization error of a learning algorithm can be upper bounded by information measures. In most of the relevant works, the convergence rate of the expected generalization error is in the form of $O(\sqrt{\lambda/n})$ where $\lambda$ is some information-theoretic quantities such as the mutual information or conditional mutual information between the data and the learned hypothesis. However, such a learning rate is typically considered to be ``slow", compared to a ``fast rate" of $O(\lambda/n)$ in many learning scenarios. In this work, we first show that the square root does not necessarily imply a slow rate, and a fast rate result can still be obtained using this bound under appropriate assumptions. Furthermore, we identify the critical conditions needed for the fast rate generalization error, which we call the $(\eta,c)$-central condition. Under this condition, we give information-theoretic bounds on the generaliz
&lt;/p&gt;</description></item></channel></rss>