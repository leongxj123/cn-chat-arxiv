<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TREET&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Donsker-Vardhan&#34920;&#31034;&#27861;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#31283;&#23450;&#36807;&#31243;&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20272;&#35745;TE&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#20248;&#21270;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#21644;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06919</link><description>&lt;p&gt;
TREET: &#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
TREET: TRansfer Entropy Estimation via Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TREET&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Donsker-Vardhan&#34920;&#31034;&#27861;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#31283;&#23450;&#36807;&#31243;&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20272;&#35745;TE&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#20248;&#21270;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#21644;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#36755;&#29109;&#65288;TE&#65289;&#26159;&#20449;&#24687;&#35770;&#20013;&#25581;&#31034;&#36807;&#31243;&#20043;&#38388;&#20449;&#24687;&#27969;&#21160;&#26041;&#21521;&#30340;&#24230;&#37327;&#65292;&#23545;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREET&#30340;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#31283;&#23450;&#36807;&#31243;&#30340;TE&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;Donsker-Vardhan&#65288;DV&#65289;&#34920;&#31034;&#27861;&#23545;TE&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31070;&#32463;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;TREET&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#20102;&#22686;&#21152;&#20854;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#34920;&#31034;&#24341;&#29702;&#30340;&#20272;&#35745;TE&#20248;&#21270;&#26041;&#26696;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#26469;&#20248;&#21270;&#20855;&#26377;&#35760;&#24518;&#24615;&#30340;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#65292;&#36825;&#26159;&#20449;&#24687;&#35770;&#20013;&#30340;&#19968;&#20010;&#20856;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer entropy (TE) is a measurement in information theory that reveals the directional flow of information between processes, providing valuable insights for a wide range of real-world applications. This work proposes Transfer Entropy Estimation via Transformers (TREET), a novel transformer-based approach for estimating the TE for stationary processes. The proposed approach employs Donsker-Vardhan (DV) representation to TE and leverages the attention mechanism for the task of neural estimation. We propose a detailed theoretical and empirical study of the TREET, comparing it to existing methods. To increase its applicability, we design an estimated TE optimization scheme that is motivated by the functional representation lemma. Afterwards, we take advantage of the joint optimization scheme to optimize the capacity of communication channels with memory, which is a canonical optimization problem in information theory, and show the memory capabilities of our estimator. Finally, we apply
&lt;/p&gt;</description></item></channel></rss>