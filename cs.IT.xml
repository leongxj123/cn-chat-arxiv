<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#20197;&#32553;&#23567;&#27169;&#25311;&#19982;&#30495;&#23454;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20998;&#24067;&#40065;&#26834;&#20540;&#36845;&#20195;&#8221;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.16589</link><description>&lt;p&gt;
&#20855;&#26377;&#29983;&#25104;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#21487;&#30097;&#20215;&#26684;
&lt;/p&gt;
&lt;p&gt;
The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model. (arXiv:2305.16589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#20197;&#32553;&#23567;&#27169;&#25311;&#19982;&#30495;&#23454;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20998;&#24067;&#40065;&#26834;&#20540;&#36845;&#20195;&#8221;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#20197;&#20943;&#23569;&#22312;&#23454;&#36341;&#20013;&#30340;&#27169;&#25311;&#19982;&#30495;&#23454;&#24046;&#36317;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RMDPs&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#22312;&#37096;&#32626;&#29615;&#22659;&#33853;&#22312;&#39044;&#23450;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20869;&#26102;&#65292;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#20102;&#19968;&#20123;&#21162;&#21147;&#65292;&#20294;RMDPs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#65292;&#26080;&#35770;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26159;&#20160;&#20040;&#12290;&#19981;&#28165;&#26970;&#20998;&#24067;&#40065;&#26834;&#24615;&#19982;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#26159;&#21542;&#20855;&#26377;&#32479;&#35745;&#23398;&#19978;&#30340;&#24433;&#21709;&#12290;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#26681;&#25454;&#21517;&#20041;MDP&#32472;&#21046;&#26679;&#26412;&#65292;&#25105;&#20204;&#23558;&#25551;&#36848;RMDPs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24403;&#30001;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#36317;&#31163;&#25110;$\chi^2$&#20998;&#27495;&#25351;&#23450;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26102;&#12290;&#22312;&#36825;&#37324;&#30740;&#31350;&#30340;&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#40065;&#26834;&#20540;&#36845;&#20195;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25972;&#20010;&#33539;&#22260;&#20869;&#37117;&#26159;&#36817;&#20046;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates model robustness in reinforcement learning (RL) to reduce the sim-to-real gap in practice. We adopt the framework of distributionally robust Markov decision processes (RMDPs), aimed at learning a policy that optimizes the worst-case performance when the deployed environment falls within a prescribed uncertainty set around the nominal MDP. Despite recent efforts, the sample complexity of RMDPs remained mostly unsettled regardless of the uncertainty set in use. It was unclear if distributional robustness bears any statistical consequences when benchmarked against standard RL.  Assuming access to a generative model that draws samples based on the nominal MDP, we characterize the sample complexity of RMDPs when the uncertainty set is specified via either the total variation (TV) distance or $\chi^2$ divergence. The algorithm studied here is a model-based method called {\em distributionally robust value iteration}, which is shown to be near-optimal for the full range
&lt;/p&gt;</description></item></channel></rss>