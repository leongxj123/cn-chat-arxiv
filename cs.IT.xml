<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24615;Gram-Schmidt&#26041;&#27861;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21644;&#21435;&#38500;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#25552;&#21462;&#25968;&#25454;&#20013;&#30340;&#32447;&#24615;&#29305;&#24449;&#24182;&#21435;&#38500;&#38750;&#32447;&#24615;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2311.09386</link><description>&lt;p&gt;
&#36229;&#36234;PCA&#65306;&#19968;&#31181;&#27010;&#29575;&#24615;Gram-Schmidt&#26041;&#27861;&#30340;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24615;Gram-Schmidt&#26041;&#27861;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21644;&#21435;&#38500;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#25552;&#21462;&#25968;&#25454;&#20013;&#30340;&#32447;&#24615;&#29305;&#24449;&#24182;&#21435;&#38500;&#38750;&#32447;&#24615;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#22312;&#25968;&#25454;&#20013;&#23384;&#22312;&#38750;&#32447;&#24615;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#27010;&#29575;&#24615;Gram-Schmidt (GS)&#31867;&#22411;&#30340;&#27491;&#20132;&#21270;&#36807;&#31243;&#26469;&#26816;&#27979;&#21644;&#26144;&#23556;&#20986;&#20887;&#20313;&#32500;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#19968;&#26063;&#20989;&#25968;&#19978;&#24212;&#29992;GS&#36807;&#31243;&#65292;&#35813;&#26063;&#20989;&#25968;&#39044;&#35745;&#25429;&#25417;&#21040;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#26032;&#30340;&#22823;&#26041;&#24046;&#26041;&#21521;&#65292;&#25110;&#32773;&#23558;&#36825;&#20123;&#20381;&#36182;&#24615;&#20174;&#20027;&#25104;&#20998;&#20013;&#21435;&#38500;&#12290;&#22312;&#21069;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29109;&#20943;&#23569;&#30340;&#20449;&#24687;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#21518;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25152;&#24471;&#31639;&#27861;&#22312;&#25152;&#36873;&#25321;&#20989;&#25968;&#26063;&#30340;&#32447;&#24615;&#24352;&#25104;&#31354;&#38388;&#20013;&#21487;&#20197;&#26816;&#27979;&#21644;&#21435;&#38500;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#12290;&#20004;&#31181;&#25552;&#20986;&#30340;&#26041;&#27861;&#37117;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#32447;&#24615;&#29305;&#24449;&#24182;&#21435;&#38500;&#38750;&#32447;&#24615;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear feature extraction at the presence of nonlinear dependencies among the data is a fundamental challenge in unsupervised learning. We propose using a probabilistic Gram-Schmidt (GS) type orthogonalization process in order to detect and map out redundant dimensions. Specifically, by applying the GS process over a family of functions which presumably captures the nonlinear dependencies in the data, we construct a series of covariance matrices that can either be used to identify new large-variance directions, or to remove those dependencies from the principal components. In the former case, we provide information-theoretic guarantees in terms of entropy reduction. In the latter, we prove that under certain assumptions the resulting algorithms detect and remove nonlinear dependencies whenever those dependencies lie in the linear span of the chosen function family. Both proposed methods extract linear features from the data while removing nonlinear redundancies. We provide simulation r
&lt;/p&gt;</description></item></channel></rss>