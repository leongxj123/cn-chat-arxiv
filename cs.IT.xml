<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#35813;&#35770;&#25991;&#23548;&#20986;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#29992;&#20110;&#21051;&#30011;&#31616;&#21333;&#20108;&#20803;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65288;&#20056;&#27861;&#24120;&#25968;&#29420;&#31435;&#20110;$p$&#12289;$q$&#21644;&#25152;&#26377;&#38169;&#35823;&#21442;&#25968;&#65289;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35774;&#32622;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.16981</link><description>&lt;p&gt;
&#31616;&#21333;&#20108;&#20803;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Sample Complexity of Simple Binary Hypothesis Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16981
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23548;&#20986;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#29992;&#20110;&#21051;&#30011;&#31616;&#21333;&#20108;&#20803;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65288;&#20056;&#27861;&#24120;&#25968;&#29420;&#31435;&#20110;$p$&#12289;$q$&#21644;&#25152;&#26377;&#38169;&#35823;&#21442;&#25968;&#65289;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35774;&#32622;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#30340;&#20108;&#20803;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26159;&#21306;&#20998;&#20004;&#20010;&#20998;&#24067;$p$&#21644;$q$&#25152;&#38656;&#30340;&#26368;&#23567;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#25968;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#20043;&#19968;&#36827;&#34892;&#65306;(i) &#26080;&#20808;&#39564;&#35774;&#32622;&#65292;&#31867;&#22411;-I&#38169;&#35823;&#26368;&#22823;&#20026;$\alpha$&#65292;&#31867;&#22411;-II&#38169;&#35823;&#26368;&#22823;&#20026;$\beta$; &#25110;&#32773; (ii) &#36125;&#21494;&#26031;&#35774;&#32622;&#65292;&#36125;&#21494;&#26031;&#38169;&#35823;&#26368;&#22823;&#20026;$\delta$&#65292;&#20808;&#39564;&#20998;&#24067;&#20026;$(\alpha, 1-\alpha)$&#12290; &#36804;&#20170;&#20026;&#27490;&#65292;&#21482;&#22312;$\alpha = \beta$&#65288;&#26080;&#20808;&#39564;&#65289;&#25110;$\alpha = 1/2$&#65288;&#36125;&#21494;&#26031;&#65289;&#26102;&#30740;&#31350;&#20102;&#27492;&#38382;&#39064;&#65292;&#24182;&#19988;&#24050;&#30693;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#29992;$p$&#21644;$q$&#20043;&#38388;&#30340;Hellinger&#25955;&#24230;&#26469;&#21051;&#30011;&#65292;&#30452;&#21040;&#20056;&#27861;&#24120;&#25968;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#29992;&#26469;&#21051;&#30011;&#26679;&#26412;&#22797;&#26434;&#24230;&#65288;&#20056;&#27861;&#24120;&#25968;&#29420;&#31435;&#20110;$p$&#12289;$q$&#21644;&#25152;&#26377;&#38169;&#35823;&#21442;&#25968;&#65289;&#65292;&#36866;&#29992;&#20110;&#65306;(i) &#20808;&#39564;&#35774;&#32622;&#20013;&#25152;&#26377;$0 \le \alpha, \beta \le 1/8$&#65307;&#20197;&#21450; (ii) &#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#25152;&#26377;$\delta \le \alpha/4$&#12290; &#29305;&#21035;&#22320;&#65292;&#35813;&#20844;&#24335;&#36866;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16981v1 Announce Type: cross  Abstract: The sample complexity of simple binary hypothesis testing is the smallest number of i.i.d. samples required to distinguish between two distributions $p$ and $q$ in either: (i) the prior-free setting, with type-I error at most $\alpha$ and type-II error at most $\beta$; or (ii) the Bayesian setting, with Bayes error at most $\delta$ and prior distribution $(\alpha, 1-\alpha)$. This problem has only been studied when $\alpha = \beta$ (prior-free) or $\alpha = 1/2$ (Bayesian), and the sample complexity is known to be characterized by the Hellinger divergence between $p$ and $q$, up to multiplicative constants. In this paper, we derive a formula that characterizes the sample complexity (up to multiplicative constants that are independent of $p$, $q$, and all error parameters) for: (i) all $0 \le \alpha, \beta \le 1/8$ in the prior-free setting; and (ii) all $\delta \le \alpha/4$ in the Bayesian setting. In particular, the formula admits eq
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#20013;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14587</link><description>&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#20855;&#26377;&#30072;&#21464;&#29575;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint Communication and Computation Framework for Goal-Oriented Semantic Communication with Distortion Rate Resilience. (arXiv:2309.14587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#20013;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#35821;&#20041;&#36890;&#20449;&#30340;&#30740;&#31350;&#20027;&#35201;&#32771;&#34385;&#20934;&#30830;&#24615;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;&#31995;&#32479;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#24726;&#35770;&#65306;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24212;&#35813;&#36890;&#36807;&#35757;&#32451;&#33258;&#28982;&#22320;&#20986;&#29616;&#65292;&#32780;&#19981;&#26159;&#30001;&#32593;&#32476;&#32422;&#26463;&#25152;&#20915;&#23450;&#12290;&#37492;&#20110;&#36825;&#20010;&#22256;&#22659;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#30001;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#24182;&#20998;&#26512;&#23398;&#20064;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21407;&#22987;&#25968;&#25454;&#21644;&#30072;&#21464;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#20808;&#20272;&#35745;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#21464;&#24471;&#21487;&#34892;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#25311;&#21644;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research efforts on semantic communication have mostly considered accuracy as a main problem for optimizing goal-oriented communication systems. However, these approaches introduce a paradox: the accuracy of artificial intelligence (AI) tasks should naturally emerge through training rather than being dictated by network constraints. Acknowledging this dilemma, this work introduces an innovative approach that leverages the rate-distortion theory to analyze distortions induced by communication and semantic compression, thereby analyzing the learning process. Specifically, we examine the distribution shift between the original data and the distorted data, thus assessing its impact on the AI model's performance. Founding upon this analysis, we can preemptively estimate the empirical accuracy of AI tasks, making the goal-oriented semantic communication problem feasible. To achieve this objective, we present the theoretical foundation of our approach, accompanied by simulations and ex
&lt;/p&gt;</description></item></channel></rss>