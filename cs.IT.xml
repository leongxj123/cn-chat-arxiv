<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#22312;&#36830;&#32493;&#30340;&#20248;&#21270;&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20248;&#20540;&#25110;&#35299;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.09570</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20445;&#30495;&#24230;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21450;&#36328;&#20219;&#21153;&#21487;&#36716;&#31227;&#30340;&#26368;&#22823;&#20540;&#29109;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#22312;&#36830;&#32493;&#30340;&#20248;&#21270;&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20248;&#20540;&#25110;&#35299;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#35774;&#35745;&#32773;&#38754;&#20020;&#19968;&#31995;&#21015;&#20248;&#21270;&#20219;&#21153;&#65292;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26114;&#36149;&#35780;&#20272;&#30340;&#40657;&#30418;&#20989;&#25968;&#24418;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#38656;&#35201;&#33719;&#21462;&#19981;&#21516;&#20219;&#21153;&#30340;&#26368;&#20248;&#20540;&#25110;&#35299;&#30340;&#20449;&#24687;&#21644;&#36890;&#36807;&#21442;&#25968;&#30340;&#36716;&#31227;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09570v1 Announce Type: new  Abstract: In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of black-box functions that are costly to evaluate. For example, the designer may need to tune the hyperparameters of neural network models for different learning tasks over time. Rather than evaluating the objective function for each candidate solution, the designer may have access to approximations of the objective functions, for which higher-fidelity evaluations entail a larger cost. Existing multi-fidelity black-box optimization strategies select candidate solutions and fidelity levels with the goal of maximizing the information accrued about the optimal value or solution for the current task. Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the 
&lt;/p&gt;</description></item></channel></rss>