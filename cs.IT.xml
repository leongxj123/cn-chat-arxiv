<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#36830;&#32493;&#20989;&#25968;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#30340;&#20005;&#26684;&#35777;&#26126;&#65292;&#22635;&#34917;&#20102;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#25104;&#21151;&#30340;&#29702;&#35770;&#32570;&#21475;</title><link>https://arxiv.org/abs/2007.15776</link><description>&lt;p&gt;
&#29992;&#20110;&#27969;&#24418;&#19978;&#20989;&#25968;&#36924;&#36817;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Random Vector Functional Link Networks for Function Approximation on Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2007.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#36830;&#32493;&#20989;&#25968;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#30340;&#20005;&#26684;&#35777;&#26126;&#65292;&#22635;&#34917;&#20102;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#25104;&#21151;&#30340;&#29702;&#35770;&#32570;&#21475;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
feed-forward&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36895;&#24230;&#22240;&#24930;&#32780;&#33879;&#21517;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#24050;&#32463;&#25104;&#20026;&#29942;&#39048;&#25968;&#21313;&#24180;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#23581;&#35797;&#24341;&#20837;&#38543;&#26426;&#24615;&#26469;&#20943;&#23569;&#23398;&#20064;&#38656;&#27714;&#12290;&#22522;&#20110;Igelnik&#21644;Pao&#30340;&#21407;&#22987;&#26500;&#36896;&#65292;&#20855;&#26377;&#38543;&#26426;&#36755;&#20837;&#21040;&#38544;&#34255;&#23618;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#32570;&#20047;&#24517;&#35201;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#29702;&#35770;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#65288;&#26356;&#27491;&#30340;&#65289;&#20005;&#26684;&#35777;&#26126;&#65292;&#35777;&#26126;Igelnik&#21644;Pao&#30340;&#26500;&#36896;&#26159;&#19968;&#20010;&#36830;&#32493;&#20989;&#25968;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#36924;&#36817;&#35823;&#24046;&#20687;&#28176;&#36817;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2007.15776v3 Announce Type: replace-cross  Abstract: The learning speed of feed-forward neural networks is notoriously slow and has presented a bottleneck in deep learning applications for several decades. For instance, gradient-based learning algorithms, which are used extensively to train neural networks, tend to work slowly when all of the network parameters must be iteratively tuned. To counter this, both researchers and practitioners have tried introducing randomness to reduce the learning requirement. Based on the original construction of Igelnik and Pao, single layer neural-networks with random input-to-hidden layer weights and biases have seen success in practice, but the necessary theoretical justification is lacking. In this paper, we begin to fill this theoretical gap. We provide a (corrected) rigorous proof that the Igelnik and Pao construction is a universal approximator for continuous functions on compact domains, with approximation error decaying asymptotically lik
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#39640;&#30340;&#25209;&#35780;&#21147;&#37327;&#23558;&#20154;&#31867;&#35302;&#25720;&#19982;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#22256;&#24785;&#24230;&#27979;&#35797;&#21644;&#32479;&#35745;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#20986;&#21487;&#33021;&#34987;&#32534;&#36753;&#30340;&#37096;&#20998;&#65292;&#24182;&#25506;&#35752;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21450;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12747</link><description>&lt;p&gt;
&#20351;&#29992;&#26356;&#39640;&#30340;&#25209;&#35780;&#21147;&#37327;&#23558;&#20154;&#31867;&#35302;&#25720;&#19982;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#31163;&#65306;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Separating the Human Touch from AI-Generated Text using Higher Criticism: An Information-Theoretic Approach. (arXiv:2308.12747v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#39640;&#30340;&#25209;&#35780;&#21147;&#37327;&#23558;&#20154;&#31867;&#35302;&#25720;&#19982;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#22256;&#24785;&#24230;&#27979;&#35797;&#21644;&#32479;&#35745;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#20986;&#21487;&#33021;&#34987;&#32534;&#36753;&#30340;&#37096;&#20998;&#65292;&#24182;&#25506;&#35752;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21450;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#19968;&#31687;&#32473;&#23450;&#25991;&#31456;&#26159;&#23436;&#20840;&#30001;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#30340;&#65292;&#36824;&#26159;&#21253;&#21547;&#20102;&#19968;&#20123;&#30001;&#20854;&#20182;&#20316;&#32773;&#65288;&#21487;&#33021;&#26159;&#20154;&#31867;&#65289;&#36827;&#34892;&#20102;&#37325;&#35201;&#32534;&#36753;&#30340;&#26367;&#20195;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#28041;&#21450;&#23545;&#20010;&#21035;&#21477;&#23376;&#25110;&#20854;&#20182;&#25991;&#26412;&#21333;&#20301;&#36215;&#28304;&#36827;&#34892;&#30340;&#22810;&#20010;&#22256;&#24785;&#24230;&#27979;&#35797;&#65292;&#23558;&#36825;&#20123;&#22810;&#20010;&#27979;&#35797;&#32467;&#21512;&#36215;&#26469;&#20351;&#29992;&#26356;&#39640;&#30340;&#25209;&#35780;&#21147;&#37327;&#65288;HC&#65289;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#22815;&#35782;&#21035;&#20986;&#21487;&#33021;&#34987;&#32534;&#36753;&#30340;&#37096;&#20998;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#20102;&#23545;&#25968;&#22256;&#24785;&#24230;&#25910;&#25947;&#21040;&#20132;&#21449;&#29109;&#29575;&#30340;&#21551;&#21457;&#65292;&#20197;&#21450;&#19968;&#20010;&#32479;&#35745;&#27169;&#22411;&#26469;&#25551;&#36848;&#34987;&#32534;&#36753;&#30340;&#25991;&#26412;&#65292;&#21363;&#21477;&#23376;&#20027;&#35201;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#65292;&#20294;&#21487;&#33021;&#26377;&#23569;&#25968;&#21477;&#23376;&#26159;&#36890;&#36807;&#20854;&#20182;&#26426;&#21046;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#24433;&#21709;&#20854;&#25104;&#21151;&#30340;&#22240;&#32032;&#12290;&#36825;&#20010;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#21487;&#33021;&#20250;&#25913;&#21892;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to determine whether a given article was entirely written by a generative language model versus an alternative situation in which the article includes some significant edits by a different author, possibly a human. Our process involves many perplexity tests for the origin of individual sentences or other text atoms, combining these multiple tests using Higher Criticism (HC). As a by-product, the method identifies parts suspected to be edited. The method is motivated by the convergence of the log-perplexity to the cross-entropy rate and by a statistical model for edited text saying that sentences are mostly generated by the language model, except perhaps for a few sentences that might have originated via a different mechanism. We demonstrate the effectiveness of our method using real data and analyze the factors affecting its success. This analysis raises several interesting open challenges whose resolution may improve the method's effectiveness.
&lt;/p&gt;</description></item></channel></rss>