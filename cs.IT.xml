<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BASS&#30340;&#22522;&#20110;&#24191;&#25773;&#30340;&#23376;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.13779</link><description>&lt;p&gt;
&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#36890;&#20449;&#25104;&#26412;&#65306;&#29992;&#20110;&#26080;&#32447;&#32593;&#32476;&#30340;&#22522;&#20110;&#24191;&#25773;&#30340;&#23376;&#22270;&#37319;&#26679;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Faster Convergence with Less Communication: Broadcast-Based Subgraph Sampling for Decentralized Learning over Wireless Networks. (arXiv:2401.13779v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BASS&#30340;&#22522;&#20110;&#24191;&#25773;&#30340;&#23376;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20849;&#35782;&#30340;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(D-SGD)&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#32593;&#32476;&#20195;&#29702;&#20043;&#38388;&#30340;&#21435;&#20013;&#24515;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;D-SGD&#30340;&#19968;&#20010;&#20851;&#38190;&#37096;&#20998;&#26159;&#22522;&#20110;&#20849;&#35782;&#30340;&#27169;&#22411;&#24179;&#22343;&#65292;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#21644;&#34701;&#21512;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#20849;&#35782;&#24179;&#22343;&#65292;&#36890;&#20449;&#21327;&#35843;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#30830;&#23450;&#33410;&#28857;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#35775;&#38382;&#20449;&#36947;&#65292;&#24182;&#23558;&#20449;&#24687;&#20256;&#36755;&#65288;&#25110;&#25509;&#25910;&#65289;&#32473;&#65288;&#25110;&#20174;&#65289;&#37051;&#23621;&#33410;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BASS&#30340;&#22522;&#20110;&#24191;&#25773;&#30340;&#23376;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#26088;&#22312;&#21152;&#24555;D-SGD&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#32771;&#34385;&#27599;&#36718;&#36845;&#20195;&#30340;&#23454;&#38469;&#36890;&#20449;&#25104;&#26412;&#12290;BASS&#21019;&#24314;&#19968;&#32452;&#28151;&#21512;&#30697;&#38453;&#20505;&#36873;&#39033;&#65292;&#34920;&#31034;&#22522;&#30784;&#25299;&#25169;&#30340;&#31232;&#30095;&#23376;&#22270;&#12290;&#22312;&#27599;&#20010;&#20849;&#35782;&#36845;&#20195;&#20013;&#65292;&#23558;&#37319;&#26679;&#19968;&#20010;&#28151;&#21512;&#30697;&#38453;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#29305;&#23450;&#30340;&#35843;&#24230;&#20915;&#31574;&#65292;&#28608;&#27963;&#22810;&#20010;&#26080;&#30896;&#25758;&#30340;&#33410;&#28857;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consensus-based decentralized stochastic gradient descent (D-SGD) is a widely adopted algorithm for decentralized training of machine learning models across networked agents. A crucial part of D-SGD is the consensus-based model averaging, which heavily relies on information exchange and fusion among the nodes. Specifically, for consensus averaging over wireless networks, communication coordination is necessary to determine when and how a node can access the channel and transmit (or receive) information to (or from) its neighbors. In this work, we propose $\texttt{BASS}$, a broadcast-based subgraph sampling method designed to accelerate the convergence of D-SGD while considering the actual communication cost per iteration. $\texttt{BASS}$ creates a set of mixing matrix candidates that represent sparser subgraphs of the base topology. In each consensus iteration, one mixing matrix is sampled, leading to a specific scheduling decision that activates multiple collision-free subsets of node
&lt;/p&gt;</description></item></channel></rss>