<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;</title><link>https://arxiv.org/abs/2402.10686</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#21644;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65306;&#20449;&#24687;&#35770;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10686
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#20013;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#36807;&#24230;&#33258;&#20449;&#26469;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#20869;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#65288;LiRA&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20801;&#35768;&#30740;&#31350;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#30001;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#38598;&#24341;&#36215;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#30446;&#26631;&#27169;&#22411;&#30340;&#26657;&#20934;&#27700;&#24179;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#20174;&#30446;&#26631;&#27169;&#22411;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#36880;&#28176;&#20943;&#23569;&#65306;&#32622;&#20449;&#21521;&#37327;&#65288;CV&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#36755;&#20986;&#27010;&#29575;&#21521;&#37327;&#34987;&#21457;&#24067;&#65307;&#30495;&#23454;&#26631;&#31614;&#32622;&#20449;&#24230;&#65288;TLC&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#21482;&#26377;&#27169;&#22411;&#20998;&#37197;&#32473;&#30495;&#23454;&#26631;&#31614;&#30340;&#27010;&#29575;&#26159;&#21487;&#29992;&#30340;&#65307;&#20197;&#21450;&#20915;&#31574;&#38598;&#65288;DS&#65289;&#25259;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10686v1 Announce Type: cross  Abstract: In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the state-of-the-art likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in 
&lt;/p&gt;</description></item></channel></rss>