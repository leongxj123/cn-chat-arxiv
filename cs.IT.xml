<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02636</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#23398;&#20064;&#29420;&#31435;&#30340;&#22240;&#26524;&#26426;&#21046;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Learn Independent Causal Mechanisms?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#19981;&#24120;&#35265;&#30340;&#29615;&#22659;&#35774;&#32622;&#25110;&#20998;&#24067;&#21464;&#21270;&#30340;&#20219;&#21153;&#20013;&#65292;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#30446;&#21069;&#36890;&#24120;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#33030;&#24369;&#30340;&#65292;&#22240;&#20026;&#20219;&#21153;&#30340;&#33539;&#22260;&#21487;&#33021;&#26080;&#27861;&#39044;&#27979;&#25110;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#19988;&#20351;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#39069;&#22806;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#37027;&#20123;&#23398;&#20064;&#25277;&#35937;&#21464;&#37327;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#31995;&#32479;&#65292;&#22914;&#22240;&#26524;&#27169;&#22411;&#65292;&#21487;&#20197;&#34920;&#29616;&#20986;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#26356;&#24378;&#31283;&#20581;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#23384;&#22312;&#24182;&#20351;&#29992;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65288;ICMs&#65289;&#65292;&#34920;&#31034;&#21482;&#31232;&#30095;&#20132;&#20114;&#30340;&#39640;&#23618;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#20004;&#20010;&#27010;&#24565;&#65292;&#22312;LLMs&#20013;&#23398;&#20064;ICMs&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;&#22810;&#20010;&#31232;&#30095;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#30340;&#26032;LLM&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability. This issue has usually been alleviated by feeding more training data into the LLM. However, this method is brittle, as the scope of tasks may not be readily predictable or may evolve, and updating the model with new data generally requires extensive additional training. By contrast, systems, such as causal models, that learn abstract variables and causal relationships can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language
&lt;/p&gt;</description></item></channel></rss>