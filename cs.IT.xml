<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#20855;&#26377;&#26497;&#23567;&#22343;&#26041;&#35823;&#24046;&#65292;&#21487;&#20197;&#33719;&#24471;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#31361;&#30772;&#20102;&#20165;&#20570;&#27425;&#39640;&#26031;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.15602</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26497;&#23567;&#21270;&#26368;&#20248;&#24615;&#65306;&#36229;&#36234;&#23494;&#24230;&#19979;&#30028;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#20855;&#26377;&#26497;&#23567;&#22343;&#26041;&#35823;&#24046;&#65292;&#21487;&#20197;&#33719;&#24471;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#31361;&#30772;&#20102;&#20165;&#20570;&#27425;&#39640;&#26031;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#38750;&#21442;&#25968;&#32479;&#35745;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#22823;&#26679;&#26412;&#22330;&#26223;&#19979;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#25277;&#26679;&#30340;&#28176;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#26680;&#30340;&#24471;&#20998;&#20272;&#35745;&#22120;&#21487;&#20197;&#23454;&#29616;&#23545; $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$ &#30340;&#24471;&#20998;&#20989;&#25968;&#30340;&#26368;&#20248;&#22343;&#26041;&#35823;&#24046;&#20026; $\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$&#65292;&#20854;&#20013; $n$ &#21644; $d$ &#20998;&#21035;&#20195;&#34920;&#26679;&#26412;&#22823;&#23567;&#21644;&#32500;&#24230;&#65292;$t$ &#22312;&#19978;&#19979;&#21463;&#21040; $n$ &#30340;&#22810;&#39033;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988; $p_0$ &#26159;&#20219;&#24847;&#27425;&#20122;&#39640;&#26031;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#36825;&#23548;&#33268;&#22312;&#20165;&#36827;&#34892;&#27425;&#39640;&#26031;&#20551;&#35774;&#26102;&#65292;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#20026; $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$&#12290;&#22914;&#26524;&#27492;&#22806;&#65292;$p_0$ &#23646;&#20110; $\beta\le 2$ &#30340; $\beta$-Sobolev&#31354;&#38388;&#30340;&#38750;&#21442;&#25968;&#26063;&#65292;&#36890;&#36807;&#37319;&#29992;&#26089;&#20572;&#31574;&#30053;&#65292;&#25105;&#20204;&#24471;&#21040;&#35813;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#30340;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#20026; $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15602v1 Announce Type: cross  Abstract: We study the asymptotic error of score-based diffusion model sampling in large-sample scenarios from a non-parametric statistics perspective. We show that a kernel-based score estimator achieves an optimal mean square error of $\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$ for the score function of $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$, where $n$ and $d$ represent the sample size and the dimension, $t$ is bounded above and below by polynomials of $n$, and $p_0$ is an arbitrary sub-Gaussian distribution. As a consequence, this yields an $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$ upper bound for the total variation error of the distribution of the sample generated by the diffusion model under a mere sub-Gaussian assumption. If in addition, $p_0$ belongs to the nonparametric family of the $\beta$-Sobolev space with $\beta\le 2$, by adopting an early stopping strategy, we obtain that the diffusion
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#20110;&#25490;&#21015;&#19981;&#21464;&#25110;&#32773;&#21487;&#21152;&#21487;&#20998;&#30340;&#20998;&#35010;&#20989;&#25968;&#65292;&#39640;&#25928;&#30340;&#26080;&#20559;&#31232;&#30095;&#21270;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.14925</link><description>&lt;p&gt;
&#39640;&#25928;&#26080;&#20559;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Unbiased Sparsification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#20110;&#25490;&#21015;&#19981;&#21464;&#25110;&#32773;&#21487;&#21152;&#21487;&#20998;&#30340;&#20998;&#35010;&#20989;&#25968;&#65292;&#39640;&#25928;&#30340;&#26080;&#20559;&#31232;&#30095;&#21270;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#21521;&#37327;$p\in \mathbb{R}^n$&#30340;&#26080;&#20559;$m$-&#31232;&#30095;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#24179;&#22343;&#20540;&#20026;$p$&#65292;&#26368;&#22810;&#26377;$m&lt;n$&#20010;&#38750;&#38646;&#22352;&#26631;&#30340;&#38543;&#26426;&#21521;&#37327;$Q\in \mathbb{R}^n&#12290; &#26080;&#20559;&#31232;&#30095;&#21270;&#21487;&#20197;&#21387;&#32553;&#21407;&#22987;&#21521;&#37327;&#32780;&#19981;&#24341;&#20837;&#20559;&#24046;&#65307;&#23427;&#20986;&#29616;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#65292;&#27604;&#22914;&#32852;&#37030;&#23398;&#20064;&#21644;&#37319;&#26679;&#31232;&#30095;&#27010;&#29575;&#20998;&#24067;&#12290; &#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#26080;&#20559;&#31232;&#30095;&#21270;&#36824;&#24212;&#35813;&#26368;&#23567;&#21270;&#19968;&#20010;&#24230;&#37327;$Q$&#19982;&#21407;&#22987;$p$&#20043;&#38388;&#36317;&#31163;&#26377;&#22810;&#36828;&#30340;&#20998;&#35010;&#20989;&#25968;$\mathsf{Div}(Q,p)$&#30340;&#26399;&#26395;&#20540;&#12290; &#22914;&#26524;$Q$&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#26159;&#26368;&#20248;&#30340;&#65292;&#37027;&#20040;&#25105;&#20204;&#31216;&#20043;&#20026;&#39640;&#25928;&#12290; &#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#25551;&#36848;&#20102;&#23545;&#20110;&#26082;&#26159;&#25490;&#21015;&#19981;&#21464;&#21448;&#26159;&#21487;&#21152;&#21487;&#20998;&#30340;&#20998;&#35010;&#20989;&#25968;&#30340;&#39640;&#25928;&#26080;&#20559;&#31232;&#30095;&#21270;&#12290; &#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25490;&#21015;&#19981;&#21464;&#20998;&#35010;&#20989;&#25968;&#30340;&#34920;&#24449;&#23545;&#20110;&#20998;&#35010;&#20989;&#25968;&#30340;&#36873;&#25321;&#26159;&#20581;&#22766;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#38024;&#23545;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#30340;&#26368;&#20248;$Q$&#30340;&#31867;&#19982;&#25105;&#20204;&#30340;&#31867;&#37325;&#21512;&#20102;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14925v1 Announce Type: cross  Abstract: An unbiased $m$-sparsification of a vector $p\in \mathbb{R}^n$ is a random vector $Q\in \mathbb{R}^n$ with mean $p$ that has at most $m&lt;n$ nonzero coordinates. Unbiased sparsification compresses the original vector without introducing bias; it arises in various contexts, such as in federated learning and sampling sparse probability distributions. Ideally, unbiased sparsification should also minimize the expected value of a divergence function $\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If $Q$ is optimal in this sense, then we call it efficient. Our main results describe efficient unbiased sparsifications for divergences that are either permutation-invariant or additively separable. Surprisingly, the characterization for permutation-invariant divergences is robust to the choice of divergence function, in the sense that our class of optimal $Q$ for squared Euclidean distance coincides with our class of op
&lt;/p&gt;</description></item></channel></rss>