<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#23398;&#20064;&#33021;&#21147;&#26159;&#19968;&#31181;&#24230;&#37327;&#27169;&#22411;&#26377;&#25928;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#21028;&#26029;&#26159;&#21542;&#38656;&#35201;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#25110;&#32773;&#23547;&#25214;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17332</link><description>&lt;p&gt;
&#23398;&#20064;&#33021;&#21147;&#65306;&#27169;&#22411;&#26377;&#25928;&#32500;&#24230;&#30340;&#24230;&#37327;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning Capacity: A Measure of the Effective Dimensionality of a Model. (arXiv:2305.17332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17332
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#33021;&#21147;&#26159;&#19968;&#31181;&#24230;&#37327;&#27169;&#22411;&#26377;&#25928;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#21028;&#26029;&#26159;&#21542;&#38656;&#35201;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#25110;&#32773;&#23547;&#25214;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#28909;&#21147;&#23398;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#27491;&#24335;&#23545;&#24212;&#20851;&#31995;&#65292;&#23558;&#26679;&#26412;&#25968;&#37327;&#35270;&#20026;&#21453;&#28201;&#24230;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#8220;&#23398;&#20064;&#33021;&#21147;&#8221;&#65292;&#36825;&#26159;&#27169;&#22411;&#26377;&#25928;&#32500;&#24230;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#35768;&#22810;&#22312;&#20856;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#23398;&#20064;&#33021;&#21147;&#20165;&#21344;&#21442;&#25968;&#25968;&#37327;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#21462;&#20915;&#20110;&#29992;&#20110;&#35757;&#32451;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#19978;&#19982;&#20174;PAC-Bayesian&#26694;&#26550;&#33719;&#24471;&#30340;&#33021;&#21147;&#27010;&#24565;&#19968;&#33268;&#12290;&#23398;&#20064;&#33021;&#21147;&#20316;&#20026;&#27979;&#35797;&#35823;&#24046;&#30340;&#20989;&#25968;&#19981;&#20250;&#20986;&#29616;&#21452;&#23792;&#19979;&#38477;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#22312;&#38750;&#24120;&#23567;&#21644;&#38750;&#24120;&#22823;&#30340;&#26679;&#26412;&#22823;&#23567;&#22788;&#39281;&#21644;&#65292;&#36825;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#35828;&#26126;&#26159;&#21542;&#24212;&#35813;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#25110;&#32773;&#23547;&#25214;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23398;&#20064;&#33021;&#21147;&#26469;&#29702;&#35299;&#26377;&#25928;&#32500;&#25968;&#65292;&#21363;&#20351;&#26159;&#38750;&#21442;&#25968;&#27169;&#22411;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We exploit a formal correspondence between thermodynamics and inference, where the number of samples can be thought of as the inverse temperature, to define a "learning capacity'' which is a measure of the effective dimensionality of a model. We show that the learning capacity is a tiny fraction of the number of parameters for many deep networks trained on typical datasets, depends upon the number of samples used for training, and is numerically consistent with notions of capacity obtained from the PAC-Bayesian framework. The test error as a function of the learning capacity does not exhibit double descent. We show that the learning capacity of a model saturates at very small and very large sample sizes; this provides guidelines, as to whether one should procure more data or whether one should search for new architectures, to improve performance. We show how the learning capacity can be used to understand the effective dimensionality, even for non-parametric models such as random fores
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25299;&#23637;&#20102;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;(PINN) &#26469;&#35299;&#20915;&#27714;&#35299;&#38556;&#30861;&#29289;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#25968;&#20540;&#26041;&#27861;&#30340;&#38590;&#24230;&#36739;&#22823;&#65292;&#20294;&#20316;&#32773;&#36890;&#36807;&#23545;&#22810;&#31181;&#24773;&#20917;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;PINN&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03552</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#27714;&#35299;&#38556;&#30861;&#29289;&#30456;&#20851;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
A physics-informed neural network framework for modeling obstacle-related equations. (arXiv:2304.03552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25299;&#23637;&#20102;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;(PINN) &#26469;&#35299;&#20915;&#27714;&#35299;&#38556;&#30861;&#29289;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#25968;&#20540;&#26041;&#27861;&#30340;&#38590;&#24230;&#36739;&#22823;&#65292;&#20294;&#20316;&#32773;&#36890;&#36807;&#23545;&#22810;&#31181;&#24773;&#20917;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;PINN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#29992;&#20110;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#12288;&#30340;&#30740;&#31350;&#21017;&#26159;&#36817;&#24180;&#26469;&#30340;&#28909;&#28857;&#65292;&#23588;&#20854;&#22312;&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65288;&#22914;TensorFlow&#25110;PyTorch&#65289;&#30340;&#25903;&#25345;&#19979;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#21487;&#36890;&#36807;&#35299;&#26512;&#31232;&#30095;&#19988;&#22122;&#22768;&#25968;&#25454;&#26469;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#23558;&#25299;&#23637;PINN&#26469;&#27714;&#35299;&#38556;&#30861;&#29289;&#30456;&#20851;PDE&#65292;&#36825;&#31867;&#26041;&#31243;&#38590;&#24230;&#36739;&#22823;&#65292;&#38656;&#35201;&#21487;&#20197;&#24471;&#21040;&#20934;&#30830;&#35299;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#20316;&#32773;&#22312;&#27491;&#24120;&#21644;&#19981;&#35268;&#21017;&#30340;&#38556;&#30861;&#24773;&#20917;&#19979;&#65292;&#23545;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;PDE&#30340;&#22810;&#20010;&#22330;&#26223;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PINNs&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been highly successful in some applications. Nevertheless, its use for solving partial differential equations (PDEs) has only been of recent interest with current state-of-the-art machine learning libraries, e.g., TensorFlow or PyTorch. Physics-informed neural networks (PINNs) are an attractive tool for solving partial differential equations based on sparse and noisy data. Here extend PINNs to solve obstacle-related PDEs which present a great computational challenge because they necessitate numerical methods that can yield an accurate approximation of the solution that lies above a given obstacle. The performance of the proposed PINNs is demonstrated in multiple scenarios for linear and nonlinear PDEs subject to regular and irregular obstacles.
&lt;/p&gt;</description></item></channel></rss>