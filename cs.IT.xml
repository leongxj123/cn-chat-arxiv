<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26684;&#28857;&#21464;&#25442;&#32534;&#30721;&#65288;LTC&#65289;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#20013;&#37319;&#29992;&#26684;&#28857;&#37327;&#21270;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#21387;&#32553;&#20013;&#25509;&#36817;&#36895;&#29575;&#22833;&#30495;&#26497;&#38480;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.07320</link><description>&lt;p&gt;
&#29992;&#26684;&#28857;&#21464;&#25442;&#32534;&#30721;&#25509;&#36817;&#31070;&#32463;&#21387;&#32553;&#20013;&#30340;&#36895;&#29575;&#22833;&#30495;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07320
&lt;/p&gt;
&lt;p&gt;
&#26684;&#28857;&#21464;&#25442;&#32534;&#30721;&#65288;LTC&#65289;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#20013;&#37319;&#29992;&#26684;&#28857;&#37327;&#21270;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#21387;&#32553;&#20013;&#25509;&#36817;&#36895;&#29575;&#22833;&#30495;&#26497;&#38480;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21387;&#32553;&#22312;&#35774;&#35745;&#20855;&#26377;&#33391;&#22909;&#36895;&#29575;&#22833;&#30495;&#65288;RD&#65289;&#24615;&#33021;&#20294;&#22797;&#26434;&#24230;&#20302;&#30340;&#26377;&#25439;&#21387;&#32553;&#22120;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#31070;&#32463;&#21387;&#32553;&#35774;&#35745;&#28041;&#21450;&#23558;&#28304;&#36716;&#25442;&#20026;&#28508;&#21464;&#37327;&#65292;&#28982;&#21518;&#33293;&#20837;&#20026;&#25972;&#25968;&#24182;&#36827;&#34892;&#29109;&#32534;&#30721;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#26576;&#20123;&#28304;&#19978;&#30340;&#19968;&#27425;&#24615;&#24773;&#20917;&#19979;&#26159;&#26368;&#20339;&#30340;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#22312;i.i.d.&#24207;&#21015;&#19978;&#23427;&#26159;&#39640;&#24230;&#27425;&#20248;&#30340;&#65292;&#20107;&#23454;&#19978;&#24635;&#26159;&#24674;&#22797;&#21407;&#22987;&#28304;&#24207;&#21015;&#30340;&#26631;&#37327;&#37327;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20122;&#20248;&#36234;&#24615;&#26159;&#30001;&#20110;&#28508;&#31354;&#38388;&#20013;&#37327;&#21270;&#26041;&#26696;&#30340;&#36873;&#25321;&#65292;&#32780;&#38750;&#21464;&#25442;&#35774;&#35745;&#25152;&#33268;&#12290;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#20013;&#37319;&#29992;&#26684;&#28857;&#37327;&#21270;&#32780;&#38750;&#26631;&#37327;&#37327;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26684;&#28857;&#21464;&#25442;&#32534;&#30721;&#65288;Lattice Transform Coding&#65292;LTC&#65289;&#33021;&#22815;&#22312;&#21508;&#20010;&#32500;&#24230;&#19978;&#24674;&#22797;&#26368;&#20339;&#30690;&#37327;&#37327;&#21270;&#65292;&#24182;&#22312;&#21512;&#29702;&#30340;&#22797;&#26434;&#24230;&#19979;&#25509;&#36817;&#28176;&#36817;&#21487;&#23454;&#29616;&#30340;&#36895;&#29575;&#22833;&#30495;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07320v1 Announce Type: cross  Abstract: Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal in a one-shot sense on certain sources, we show that it is highly sub-optimal on i.i.d. sequences, and in fact always recovers scalar quantization of the original source sequence. We demonstrate that the sub-optimality is due to the choice of quantization scheme in the latent space, and not the transform design. By employing lattice quantization instead of scalar quantization in the latent space, we demonstrate that Lattice Transform Coding (LTC) is able to recover optimal vector quantization at various dimensions and approach the asymptotically-achievable rate-distortion function at reasonable complexity. 
&lt;/p&gt;</description></item></channel></rss>