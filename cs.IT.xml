<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#36827;&#34892;&#20809;&#23398;&#31995;&#32479;&#38750;&#32447;&#24615;&#36890;&#36947;&#34917;&#20607;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;Transformer&#30340;&#35760;&#24518;&#20851;&#27880;&#33021;&#21147;&#21644;&#24182;&#34892;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#20449;&#24687;&#25513;&#30721;&#65292;&#29992;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13119</link><description>&lt;p&gt;
&#21033;&#29992;Transformer&#36827;&#34892;&#20809;&#23398;&#31995;&#32479;&#38750;&#32447;&#24615;&#36890;&#36947;&#34917;&#20607;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Transformers for Nonlinear Channel Compensation in Optical Systems. (arXiv:2304.13119v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#36827;&#34892;&#20809;&#23398;&#31995;&#32479;&#38750;&#32447;&#24615;&#36890;&#36947;&#34917;&#20607;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;Transformer&#30340;&#35760;&#24518;&#20851;&#27880;&#33021;&#21147;&#21644;&#24182;&#34892;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#20449;&#24687;&#25513;&#30721;&#65292;&#29992;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#36890;&#36947;&#22343;&#34913;&#26041;&#27861;&#65292;&#29992;&#20110;&#30456;&#24178;&#38271;&#36317;&#31163;&#20256;&#36755;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#30452;&#25509;&#20851;&#27880;&#19968;&#31995;&#21015;&#31526;&#21495;&#20043;&#38388;&#30340;&#35760;&#24518;&#65292;&#22240;&#27492;Transformer&#21487;&#20197;&#19982;&#24182;&#34892;&#32467;&#26500;&#26377;&#25928;&#22320;&#37197;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#37096;&#20998;&#30340;Transformer&#23454;&#29616;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#22343;&#34913;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#19981;&#21516;&#36229;&#21442;&#25968;&#33539;&#22260;&#20869;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#22788;&#29702;&#31526;&#21495;&#22359;&#65292;&#24182;&#20180;&#32454;&#36873;&#25321;&#35201;&#19968;&#36215;&#22788;&#29702;&#30340;&#32534;&#30721;&#22120;&#36755;&#20986;&#23376;&#38598;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#25200;&#21160;&#29702;&#35770;&#30340;&#29289;&#29702;&#23398;&#20449;&#24687;&#25513;&#30721;&#65292;&#29992;&#20110;&#38477;&#20302;Transformer&#38750;&#32447;&#24615;&#22343;&#34913;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new nonlinear channel equalization method for the coherent long-haul transmission based on Transformers. We show that due to their capability to attend directly to the memory across a sequence of symbols, Transformers can be used effectively with a parallelized structure. We present an implementation of encoder part of Transformer for nonlinear equalization and analyze its performance over a wide range of different hyper-parameters. It is shown that by processing blocks of symbols at each iteration and carefully selecting subsets of the encoder's output to be processed together, an efficient nonlinear compensation can be achieved. We also propose the use of a physic-informed mask inspired by nonlinear perturbation theory for reducing the computational complexity of Transformer nonlinear equalization.
&lt;/p&gt;</description></item></channel></rss>