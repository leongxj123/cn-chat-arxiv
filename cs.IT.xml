<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65306;&#20998;&#23618;&#29420;&#31435;&#23376;&#27169;&#22411;&#35757;&#32451;&#65288;HIST&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20840;&#23616;&#27169;&#22411;&#21010;&#20998;&#20026;&#19981;&#30456;&#20132;&#30340;&#23376;&#27169;&#22411;&#65292;&#24182;&#22312;&#20998;&#23618;&#32467;&#26500;&#20013;&#20998;&#24067;&#65292;&#20197;&#38477;&#20302;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#23384;&#20648;&#36127;&#25285;&#65292;&#21516;&#26102;&#33410;&#32422;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2310.17890</link><description>&lt;p&gt;
&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23376;&#27169;&#22411;&#21010;&#20998;&#65306;&#31639;&#27861;&#35774;&#35745;&#19982;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Submodel Partitioning in Hierarchical Federated Learning: Algorithm Design and Convergence Analysis. (arXiv:2310.17890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65306;&#20998;&#23618;&#29420;&#31435;&#23376;&#27169;&#22411;&#35757;&#32451;&#65288;HIST&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20840;&#23616;&#27169;&#22411;&#21010;&#20998;&#20026;&#19981;&#30456;&#20132;&#30340;&#23376;&#27169;&#22411;&#65292;&#24182;&#22312;&#20998;&#23618;&#32467;&#26500;&#20013;&#20998;&#24067;&#65292;&#20197;&#38477;&#20302;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#23384;&#20648;&#36127;&#25285;&#65292;&#21516;&#26102;&#33410;&#32422;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#30456;&#36739;&#20256;&#32479;&#30340;&#8220;&#26143;&#22411;&#25299;&#25169;&#8221;&#26550;&#26500;&#30340;&#32852;&#37030;&#23398;&#20064;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#19978;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#26102;&#65292;HFL&#20173;&#28982;&#20250;&#23545;&#36793;&#32536;&#35774;&#22791;&#36896;&#25104;&#37325;&#22823;&#30340;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#23384;&#20648;&#36127;&#25285;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#29420;&#31435;&#23376;&#27169;&#22411;&#35757;&#32451;&#65288;HIST&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#20998;&#23618;&#22330;&#26223;&#19979;&#30340;&#36825;&#20123;&#38382;&#39064;&#12290;HIST&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20998;&#23618;&#29256;&#26412;&#30340;&#27169;&#22411;&#21010;&#20998;&#65292;&#21363;&#22312;&#27599;&#19968;&#36718;&#20013;&#23558;&#20840;&#23616;&#27169;&#22411;&#21010;&#20998;&#20026;&#19981;&#30456;&#20132;&#30340;&#23376;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#32454;&#32990;&#20013;&#65292;&#20351;&#24471;&#27599;&#20010;&#32454;&#32990;&#21482;&#36127;&#36131;&#35757;&#32451;&#20840;&#27169;&#22411;&#30340;&#19968;&#20010;&#21010;&#20998;&#12290;&#36825;&#26679;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#33410;&#30465;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#21516;&#26102;&#20943;&#36731;&#25972;&#20010;&#20998;&#23618;&#32467;&#26500;&#20013;&#30340;&#36890;&#20449;&#36127;&#36733;&#12290;&#25105;&#20204;&#23545;HIST&#22312;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#19979;&#30340;&#25910;&#25947;&#24615;&#34892;&#20026;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical federated learning (HFL) has demonstrated promising scalability advantages over the traditional "star-topology" architecture-based federated learning (FL). However, HFL still imposes significant computation, communication, and storage burdens on the edge, especially when training a large-scale model over resource-constrained Internet of Things (IoT) devices. In this paper, we propose hierarchical independent submodel training (HIST), a new FL methodology that aims to address these issues in hierarchical settings. The key idea behind HIST is a hierarchical version of model partitioning, where we partition the global model into disjoint submodels in each round, and distribute them across different cells, so that each cell is responsible for training only one partition of the full model. This enables each client to save computation/storage costs while alleviating the communication loads throughout the hierarchy. We characterize the convergence behavior of HIST for non-convex 
&lt;/p&gt;</description></item></channel></rss>