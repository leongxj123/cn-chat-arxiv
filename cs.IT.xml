<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#26143;&#24231;&#20013;&#30340;&#36335;&#30001;&#65292;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#26368;&#20339;&#36335;&#24452;&#65292;&#24182;&#22312;&#22312;&#32447;&#38454;&#27573;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#36335;&#30001;&#12290;</title><link>https://arxiv.org/abs/2402.17666</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20998;&#24067;&#24335;&#21355;&#26143;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#26143;&#24231;&#20013;&#30340;&#36335;&#30001;&#65292;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#26368;&#20339;&#36335;&#24452;&#65292;&#24182;&#22312;&#22312;&#32447;&#38454;&#27573;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#36335;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#26143;&#24231;&#65288;LSatCs&#65289;&#20013;&#36335;&#30001;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MA-DRL&#65289;&#26041;&#27861;&#12290;&#27599;&#20010;&#21355;&#26143;&#26159;&#19968;&#20010;&#29420;&#31435;&#30340;&#20915;&#31574;&#21046;&#23450;&#26234;&#33021;&#20307;&#65292;&#20855;&#26377;&#23545;&#29615;&#22659;&#30340;&#37096;&#20998;&#30693;&#35782;&#65292;&#24182;&#21463;&#21040;&#38468;&#36817;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25903;&#25345;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#20171;&#32461;&#30340;Q-routing&#35299;&#20915;&#26041;&#26696;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#23558;&#20854;&#25193;&#23637;&#20026;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#32593;&#32476;&#21644;&#20132;&#36890;&#21464;&#21270;&#65292;&#24182;&#22522;&#20110;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#19968;&#20010;&#20381;&#36182;&#20840;&#23616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23398;&#20064;&#22312;&#27599;&#20010;&#21487;&#33021;&#20301;&#32622;&#21644;&#25317;&#22581;&#32423;&#21035;&#19978;&#30340;&#26368;&#20339;&#36335;&#24452;&#30340;&#31163;&#32447;&#25506;&#32034;&#23398;&#20064;&#38454;&#27573;&#65307;&#65288;2&#65289;&#19968;&#20010;&#24102;&#26377;&#26412;&#22320;&#12289;&#26426;&#36733;&#12289;&#39044;&#35757;&#32451;DNN&#30340;&#22312;&#32447;&#24320;&#21457;&#38454;&#27573;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;MA-DRL&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#31163;&#32447;&#23398;&#20064;&#26368;&#20339;&#36335;&#30001;&#65292;&#28982;&#21518;&#21152;&#36733;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#36335;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17666v1 Announce Type: new  Abstract: This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents. Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online.
&lt;/p&gt;</description></item></channel></rss>