<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#35757;&#32451;&#20998;&#24067;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#19981;&#21305;&#37197;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07171</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#20998;&#24067;&#22810;&#26679;&#21270;&#23454;&#29616;&#32852;&#37030;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Federated Generalization via Information-Theoretic Distribution Diversification. (arXiv:2310.07171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07171
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#35757;&#32451;&#20998;&#24067;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#19981;&#21305;&#37197;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#22312;&#26080;&#38656;&#30452;&#25509;&#25968;&#25454;&#20849;&#20139;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#21516;&#27169;&#22411;&#35757;&#32451;&#30340;&#33021;&#21147;&#32780;&#26085;&#30410;&#31361;&#20986;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24040;&#22823;&#24046;&#24322;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25361;&#25112;&#65292;&#23545;FL&#30340;&#27867;&#21270;&#33021;&#21147;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#24403;&#24182;&#38750;&#25152;&#26377;&#23458;&#25143;&#31471;&#37117;&#21442;&#19982;&#35757;&#32451;&#36807;&#31243;&#26102;&#65292;&#24773;&#20917;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#36825;&#26159;&#30001;&#20110;&#19981;&#31283;&#23450;&#30340;&#32593;&#32476;&#36830;&#25509;&#25110;&#26377;&#38480;&#30340;&#35745;&#31639;&#33021;&#21147;&#32780;&#24120;&#35265;&#12290;&#36825;&#21487;&#33021;&#26497;&#22823;&#22320;&#22797;&#26434;&#21270;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#22823;&#37327;&#30740;&#31350;&#38598;&#20013;&#22312;&#28041;&#21450;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#24046;&#36317;&#38382;&#39064;&#19978;&#65292;&#20294;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#38750;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#21364;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic genera
&lt;/p&gt;</description></item></channel></rss>