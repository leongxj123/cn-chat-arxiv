<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#24314;&#27169;&#20026;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#23548;&#33268;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#25509;&#36817;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20248;&#20449;&#24687;&#29942;&#39048;&#35299;&#26102;&#12290;</title><link>http://arxiv.org/abs/2305.11957</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#25506;&#32034;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards understanding neural collapse in supervised contrastive learning with the information bottleneck method. (arXiv:2305.11957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#24314;&#27169;&#20026;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#23548;&#33268;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#25509;&#36817;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20248;&#20449;&#24687;&#29942;&#39048;&#35299;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26159;&#25351;&#22312;&#36229;&#20986;&#24615;&#33021;&#24179;&#21488;&#35757;&#32451;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#21518;&#19968;&#23618;&#28608;&#27963;&#30340;&#20960;&#20309;&#23398;&#34920;&#29616;&#12290;&#30446;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26159;&#21542;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#27867;&#21270;&#65292;&#22914;&#26524;&#26159;&#65292;&#36229;&#20986;&#24615;&#33021;&#24179;&#21488;&#30340;&#35757;&#32451;&#22914;&#20309;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#12290;&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#24314;&#27169;&#20026;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#65292;&#20197;&#25506;&#31350;&#26159;&#21542;&#23384;&#22312;&#36825;&#26679;&#19968;&#31181;&#32039;&#20945;&#30340;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#20854;&#19982;&#27867;&#21270;&#24615;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#23548;&#33268;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#25509;&#36817;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20248;&#20449;&#24687;&#29942;&#39048;&#35299;&#26102;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#23545;&#27604;&#25439;&#22833;&#30446;&#26631;&#29420;&#31435;&#35757;&#32451;&#30340;&#20004;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#32447;&#24615;&#21487;&#35782;&#21035;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#24471;&#21040;&#30340;&#34920;&#31034;&#31561;&#25928;&#20110;&#30697;&#38453;&#21464;&#25442;&#12290;&#25105;&#20204;&#21033;&#29992;&#32447;&#24615;&#21487;&#35782;&#21035;&#24615;&#26469;&#36817;&#20284;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#12290;&#36825;&#20010;&#36817;&#20284;&#34920;&#26126;&#65292;&#24403;&#31867;&#24179;&#22343;&#20540;&#30456;&#31561;&#26102;&#65292;&#26368;&#20248;&#35299;&#38750;&#24120;&#25509;&#36817;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural collapse describes the geometry of activation in the final layer of a deep neural network when it is trained beyond performance plateaus. Open questions include whether neural collapse leads to better generalization and, if so, why and how training beyond the plateau helps. We model neural collapse as an information bottleneck (IB) problem in order to investigate whether such a compact representation exists and discover its connection to generalization. We demonstrate that neural collapse leads to good generalization specifically when it approaches an optimal IB solution of the classification problem. Recent research has shown that two deep neural networks independently trained with the same contrastive loss objective are linearly identifiable, meaning that the resulting representations are equivalent up to a matrix transformation. We leverage linear identifiability to approximate an analytical solution of the IB problem. This approximation demonstrates that when class means exh
&lt;/p&gt;</description></item></channel></rss>