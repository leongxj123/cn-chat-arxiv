<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20302;&#32500;&#25688;&#35201;&#32479;&#35745;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#26368;&#23567;&#21270;&#21518;&#39564;&#29109;&#26469;&#33719;&#21462;&#26368;&#20248;&#25688;&#35201;&#32479;&#35745;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#36341;&#24314;&#35758;&#21644;&#31034;&#20363;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.02340</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#21518;&#39564;&#29109;&#20135;&#29983;&#20102;&#26368;&#20248;&#25688;&#35201;&#32479;&#35745;&#37327;
&lt;/p&gt;
&lt;p&gt;
Minimising the Expected Posterior Entropy Yields Optimal Summary Statistics. (arXiv:2206.02340v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02340
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20302;&#32500;&#25688;&#35201;&#32479;&#35745;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#26368;&#23567;&#21270;&#21518;&#39564;&#29109;&#26469;&#33719;&#21462;&#26368;&#20248;&#25688;&#35201;&#32479;&#35745;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#36341;&#24314;&#35758;&#21644;&#31034;&#20363;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20302;&#32500;&#25688;&#35201;&#32479;&#35745;&#37327;&#23545;&#20110;&#39640;&#25928;&#65288;&#26080;&#20284;&#28982;&#65289;&#25512;&#26029;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#31867;&#21035;&#30340;&#25688;&#35201;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#23545;&#20110;&#27491;&#30830;&#20998;&#26512;&#38477;&#32500;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#22312;&#27169;&#22411;&#30340;&#20808;&#39564;&#39044;&#27979;&#20998;&#24067;&#19979;&#26368;&#23567;&#21270;&#26399;&#26395;&#21518;&#39564;&#29109;&#65288;EPE&#65289;&#26469;&#33719;&#21462;&#25688;&#35201;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#31561;&#25928;&#20110;&#25110;&#26159;&#26368;&#23567;&#21270;EPE&#30340;&#29305;&#27530;&#25110;&#26497;&#38480;&#24773;&#20917;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33719;&#21462;&#26368;&#23567;&#21270;EPE&#30340;&#39640;&#20445;&#30495;&#25688;&#35201;&#65307;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#22522;&#20934;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#26082;&#25552;&#20379;&#20102;&#33719;&#21462;&#26377;&#25928;&#25688;&#35201;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#21448;&#20026;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#20855;&#20307;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting low-dimensional summary statistics from large datasets is essential for efficient (likelihood-free) inference. We characterise different classes of summaries and demonstrate their importance for correctly analysing dimensionality reduction algorithms. We propose obtaining summaries by minimising the expected posterior entropy (EPE) under the prior predictive distribution of the model. Many existing methods are equivalent to or are special or limiting cases of minimising the EPE. We develop a method to obtain high-fidelity summaries that minimise the EPE; we apply it to benchmark and real-world examples. We both offer a unifying perspective for obtaining informative summaries and provide concrete recommendations for practitioners.
&lt;/p&gt;</description></item></channel></rss>