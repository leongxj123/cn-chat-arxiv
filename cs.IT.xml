<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;</title><link>https://arxiv.org/abs/2402.10686</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#21644;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65306;&#20449;&#24687;&#35770;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10686
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#20013;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#36807;&#24230;&#33258;&#20449;&#26469;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#20869;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#65288;LiRA&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20801;&#35768;&#30740;&#31350;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#30001;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#38598;&#24341;&#36215;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#30446;&#26631;&#27169;&#22411;&#30340;&#26657;&#20934;&#27700;&#24179;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#20174;&#30446;&#26631;&#27169;&#22411;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#36880;&#28176;&#20943;&#23569;&#65306;&#32622;&#20449;&#21521;&#37327;&#65288;CV&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#36755;&#20986;&#27010;&#29575;&#21521;&#37327;&#34987;&#21457;&#24067;&#65307;&#30495;&#23454;&#26631;&#31614;&#32622;&#20449;&#24230;&#65288;TLC&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#21482;&#26377;&#27169;&#22411;&#20998;&#37197;&#32473;&#30495;&#23454;&#26631;&#31614;&#30340;&#27010;&#29575;&#26159;&#21487;&#29992;&#30340;&#65307;&#20197;&#21450;&#20915;&#31574;&#38598;&#65288;DS&#65289;&#25259;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10686v1 Announce Type: cross  Abstract: In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the state-of-the-art likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25439;&#22833;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#35770;&#25991;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.12696</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#23398;&#20064;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Disentanglement Learning via Topology. (arXiv:2308.12696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25439;&#22833;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#35770;&#25991;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TopDis&#65288;&#25299;&#25169;&#35299;&#32544;&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;&#22810;&#23610;&#24230;&#25299;&#25169;&#25439;&#22833;&#39033;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#35299;&#32544;&#26159;&#25968;&#25454;&#34920;&#31034;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#32423;&#35748;&#30693;&#30340;&#23454;&#29616;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#22522;&#20110;VAE&#30340;&#26368;&#26032;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#28508;&#21464;&#37327;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#24635;&#20307;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#35299;&#32544;&#12290;&#25105;&#20204;&#20174;&#20998;&#26512;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#23646;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#35299;&#32544;&#65292;&#29305;&#21035;&#26159;&#20248;&#21270;&#25968;&#25454;&#27969;&#24418;&#36941;&#21382;&#30340;&#25299;&#25169;&#30456;&#20284;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25299;&#25169;&#25439;&#22833;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#65292;&#22914;MIG&#12289;FactorVAE&#24471;&#20998;&#12289;SAP&#24471;&#20998;&#21644;DCI&#35299;&#32544;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art method based on VAE minimizes the total correlation of the joint distribution of latent variables. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement. Our experiments have shown that the proposed topological loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results. Our method works in an unsupervised m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.04723</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#20869;&#37096;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25552;&#39640;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#23646;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#21363;&#32473;&#23450;&#25991;&#26412;&#26679;&#26412;&#23884;&#20837;&#38598;&#21512;&#19979;&#30340;&#27969;&#24418;&#30340;&#20869;&#37096;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#27969;&#30021;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#22312;&#20960;&#20010;&#22522;&#20110;&#23383;&#27597;&#30340;&#35821;&#35328;&#20013;&#32422;&#20026; $9$&#65292;&#32780;&#20013;&#25991;&#32422;&#20026; $7$&#65292;&#32780;&#27599;&#31181;&#35821;&#35328;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#36739;&#20302;&#65292;&#24046;&#32422; $1.5$&#65292;&#24182;&#19988;&#26377;&#26126;&#26174;&#30340;&#32479;&#35745;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
&lt;/p&gt;</description></item></channel></rss>