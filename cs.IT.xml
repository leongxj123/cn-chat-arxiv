<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#23545;&#40784;&#26469;&#32531;&#35299;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#24182;&#23454;&#29616;&#20102;&#33021;&#25928;&#39640;&#12289;&#24310;&#36831;&#20302;&#30340;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#12290;</title><link>https://arxiv.org/abs/2403.16986</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#30340;&#21160;&#24577;&#30456;&#23545;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Dynamic Relative Representations for Goal-Oriented Semantic Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#23545;&#40784;&#26469;&#32531;&#35299;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#24182;&#23454;&#29616;&#20102;&#33021;&#25928;&#39640;&#12289;&#24310;&#36831;&#20302;&#30340;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#30340;6G&#26080;&#32447;&#32593;&#32476;&#20013;&#65292;&#36890;&#20449;&#30340;&#35821;&#20041;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#23558;&#21457;&#25381;&#22522;&#30784;&#20316;&#29992;&#65292;&#23558;&#21547;&#20041;&#21644;&#30456;&#20851;&#24615;&#32435;&#20837;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#24403;&#35774;&#22791;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#36923;&#36753;&#25110;&#20869;&#37096;&#34920;&#31034;&#26102;&#65292;&#20250;&#20986;&#29616;&#38556;&#30861;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#21487;&#33021;&#21361;&#21450;&#29702;&#35299;&#12290;&#22312;&#28508;&#22312;&#31354;&#38388;&#36890;&#20449;&#20013;&#65292;&#36825;&#19968;&#25361;&#25112;&#34920;&#29616;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#26102;&#39640;&#32500;&#34920;&#31034;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26469;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#23545;&#40784;&#32531;&#35299;&#35821;&#20041;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20248;&#21270;&#31574;&#30053;&#65292;&#20197;&#35843;&#25972;&#30456;&#23545;&#34920;&#31034;&#12289;&#36890;&#20449;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#23454;&#29616;&#33021;&#25928;&#39640;&#12289;&#24310;&#36831;&#20302;&#30340;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32531;&#35299;&#20013;&#36215;&#20316;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16986v1 Announce Type: cross  Abstract: In future 6G wireless networks, semantic and effectiveness aspects of communications will play a fundamental role, incorporating meaning and relevance into transmissions. However, obstacles arise when devices employ diverse languages, logic, or internal representations, leading to semantic mismatches that might jeopardize understanding. In latent space communication, this challenge manifests as misalignment within high-dimensional representations where deep neural networks encode data. This paper presents a novel framework for goal-oriented semantic communication, leveraging relative representations to mitigate semantic mismatches via latent space alignment. We propose a dynamic optimization strategy that adapts relative representations, communication parameters, and computation resources for energy-efficient, low-latency, goal-oriented semantic communications. Numerical results demonstrate our methodology's effectiveness in mitigating
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#29289;&#29702;&#23618;&#36890;&#20449;&#21151;&#33021;&#30340;&#23454;&#29992;&#35774;&#22791;&#38388;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#32467;&#21512;&#20449;&#36947;&#22122;&#22768;&#20197;&#22686;&#24378;&#38887;&#24615;&#65292;&#37319;&#29992;VQ-VAE&#23454;&#29616;&#39640;&#25928;&#31283;&#20581;&#30340;&#36890;&#20449;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#25552;&#21319;&#36890;&#29992;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11656</link><description>&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#29289;&#29702;&#23618;&#36890;&#20449;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Integrating Pre-Trained Language Model with Physical Layer Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11656
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#29289;&#29702;&#23618;&#36890;&#20449;&#21151;&#33021;&#30340;&#23454;&#29992;&#35774;&#22791;&#38388;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#32467;&#21512;&#20449;&#36947;&#22122;&#22768;&#20197;&#22686;&#24378;&#38887;&#24615;&#65292;&#37319;&#29992;VQ-VAE&#23454;&#29616;&#39640;&#25928;&#31283;&#20581;&#30340;&#36890;&#20449;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#25552;&#21319;&#36890;&#29992;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#22791;&#38388;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#65292;&#35774;&#22791;&#30452;&#25509;&#36890;&#36807;&#23884;&#20837;&#24335;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;&#35821;&#35328;&#27169;&#22411;&#65289;&#20132;&#25442;&#20449;&#24687;&#65292;&#38656;&#35201;&#24378;&#22823;&#12289;&#39640;&#25928;&#19988;&#36890;&#29992;&#30340;&#36890;&#20449;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#26694;&#26550;&#19982;&#29616;&#26377;&#26080;&#32447;&#31995;&#32479;&#38598;&#25104;&#24182;&#26377;&#25928;&#31649;&#29702;&#22122;&#22768;&#21644;&#27604;&#29305;&#35823;&#24046;&#37117;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#35774;&#22791;&#38388;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#29289;&#29702;&#23618;&#36890;&#20449;&#21151;&#33021;&#65292;&#24182;&#36890;&#36807;&#38142;&#36335;&#32423;&#27169;&#25311;&#22120;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#32467;&#21512;&#20449;&#36947;&#22122;&#22768;&#20197;&#22686;&#24378;&#38887;&#24615;&#65292;&#37319;&#29992;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#23454;&#29616;&#39640;&#25928;&#31283;&#20581;&#30340;&#36890;&#20449;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#32534;&#30721;-&#35299;&#30721;Transformer&#25552;&#21319;&#36890;&#29992;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#36890;&#20449;&#22330;&#26223;&#30340;&#27169;&#25311;&#20013;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23637;&#29616;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11656v1 Announce Type: cross  Abstract: The burgeoning field of on-device AI communication, where devices exchange information directly through embedded foundation models, such as language models (LMs), requires robust, efficient, and generalizable communication frameworks. However, integrating these frameworks with existing wireless systems and effectively managing noise and bit errors pose significant challenges. In this work, we introduce a practical on-device AI communication framework, integrated with physical layer (PHY) communication functions, demonstrated through its performance on a link-level simulator. Our framework incorporates end-to-end training with channel noise to enhance resilience, incorporates vector quantized variational autoencoders (VQ-VAE) for efficient and robust communication, and utilizes pre-trained encoder-decoder transformers for improved generalization capabilities. Simulations, across various communication scenarios, reveal that our framework
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/n)$&#30340;&#19978;&#30028;&#65292;&#20026;&#25105;&#20204;&#23545;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07025</link><description>&lt;p&gt;
&#22343;&#22330;&#26497;&#38480;&#19979;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Generalization Error of Graph Neural Networks in the Mean-field Regime
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07025
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/n)$&#30340;&#19978;&#30028;&#65292;&#20026;&#25105;&#20204;&#23545;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#21363;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#65306;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#30740;&#31350;&#20043;&#21069;&#65292;&#20851;&#20110;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#27867;&#21270;&#35823;&#24046;&#30340;&#29616;&#26377;&#30028;&#38480;&#32570;&#20047;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#24615;&#33021;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#26159;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25512;&#23548;&#20986;&#19978;&#30028;&#65292;&#20197;&#35780;&#20272;&#36825;&#20123;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20197;$O(1/n)$&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$n$&#26159;&#22270;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#36825;&#20123;&#19978;&#30028;&#20026;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#20174;&#32780;&#23545;&#25105;&#20204;&#30340;&#29702;&#35299;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a theoretical framework for assessing the generalization error of graph classification tasks via graph neural networks in the over-parameterized regime, where the number of parameters surpasses the quantity of data points. We explore two widely utilized types of graph neural networks: graph convolutional neural networks and message passing graph neural networks. Prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance. Our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these graph neural networks. We establish upper bounds with a convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These upper bounds offer a theoretical assurance of the networks' performance on unseen data in the challenging over-parameterized regime and overall contribute to our under
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;</title><link>http://arxiv.org/abs/2106.02797</link><description>&lt;p&gt;
&#31070;&#32463;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Neural Distributed Source Coding. (arXiv:2106.02797v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;(DSC)&#26159;&#22312;&#27809;&#26377;&#30456;&#20114;&#20851;&#32852;&#30340;&#36793;&#38469;&#20449;&#24687;&#21487;&#20379;&#35299;&#30721;&#22120;&#20351;&#29992;&#30340;&#24773;&#20917;&#19979;&#23545;&#36755;&#20837;&#36827;&#34892;&#32534;&#30721;&#30340;&#20219;&#21153;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Slepian&#21644;Wolf&#22312;1973&#24180;&#35777;&#26126;&#65292;&#27809;&#26377;&#35775;&#38382;&#36793;&#38469;&#20449;&#24687;&#30340;&#32534;&#30721;&#22120;&#21487;&#20197;&#28176;&#36817;&#22320;&#23454;&#29616;&#19982;&#36793;&#38469;&#20449;&#24687;&#21487;&#29992;&#24773;&#20917;&#19979;&#30456;&#21516;&#30340;&#21387;&#32553;&#29575;&#12290;&#34429;&#28982;&#22312;&#36825;&#20010;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#20294;&#23454;&#36341;&#20013;&#30340;DSC&#19968;&#30452;&#23616;&#38480;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#29305;&#23450;&#30340;&#30456;&#20851;&#32467;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#30456;&#20851;&#32467;&#26500;&#19981;&#21487;&#30693;&#19988;&#33021;&#22815;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#30340;&#26377;&#25439;DSC&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#28304;&#27169;&#22411;&#65292;&#32780;&#26159;&#21033;&#29992;&#26465;&#20214;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VQ-VAE)&#26469;&#23398;&#20064;&#20998;&#24067;&#24335;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;(PSNR)&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed source coding (DSC) is the task of encoding an input in the absence of correlated side information that is only available to the decoder. Remarkably, Slepian and Wolf showed in 1973 that an encoder without access to the side information can asymptotically achieve the same compression rate as when the side information is available to it. While there is vast prior work on this topic, practical DSC has been limited to synthetic datasets and specific correlation structures. Here we present a framework for lossy DSC that is agnostic to the correlation structure and can scale to high dimensions. Rather than relying on hand-crafted source modeling, our method utilizes a conditional Vector-Quantized Variational Autoencoder (VQ-VAE) to learn the distributed encoder and decoder. We evaluate our method on multiple datasets and show that our method can handle complex correlations and achieves state-of-the-art PSNR.
&lt;/p&gt;</description></item></channel></rss>