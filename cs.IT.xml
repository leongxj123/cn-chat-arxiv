<rss version="2.0"><channel><title>Chat Arxiv cs.IT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IT</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#25968;&#25454;&#39537;&#21160;&#36870;&#26368;&#20248;&#25511;&#21046;&#26041;&#26696;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#21644;&#38543;&#26426;&#31995;&#32479;&#19979;&#30340;&#25104;&#26412;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13928</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#21644;&#38543;&#26426;&#31995;&#32479;&#30340;&#20984;&#25968;&#25454;&#39537;&#21160;&#36870;&#26368;&#20248;&#25511;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Convex Data-Driven Inverse Optimal Control for Nonlinear, Non-stationary and Stochastic Systems. (arXiv:2306.13928v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#25968;&#25454;&#39537;&#21160;&#36870;&#26368;&#20248;&#25511;&#21046;&#26041;&#26696;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#21644;&#38543;&#26426;&#31995;&#32479;&#19979;&#30340;&#25104;&#26412;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#35770;&#36848;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#22495;&#30340;&#36870;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#30446;&#30340;&#26159;&#20174;&#35266;&#27979;&#20540;&#20013;&#25512;&#26029;&#20986;&#39537;&#21160;&#26234;&#33021;&#20307;&#34892;&#21160;&#30340;&#25104;&#26412;&#65292;&#21363;&#20351;&#36825;&#20010;&#25104;&#26412;&#26159;&#38750;&#20984;&#21644;&#38750;&#24179;&#31283;&#30340;&#65292;&#21516;&#26102;&#21463;&#21040;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#21644;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#26469;&#23454;&#29616;&#25104;&#26412;&#20272;&#35745;&#65292;&#21363;&#20351;&#20195;&#29702;&#25104;&#26412;&#19981;&#26159;&#20984;&#30340;&#65292;&#26412;&#25991;&#20063;&#33021;&#22815;&#29983;&#25104;&#20984;&#38382;&#39064;&#12290;&#20026;&#20102;&#24471;&#20986;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#20197;&#38543;&#26426;&#31574;&#30053;&#20026;&#20915;&#31574;&#21464;&#37327;&#30340;&#26377;&#38480;&#26102;&#22495;&#21069;&#21521;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#26368;&#20248;&#35299;&#30340;&#26174;&#24335;&#34920;&#36798;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#36716;&#21270;&#20026;&#31639;&#27861;&#27969;&#31243;&#65292;&#24182;&#36890;&#36807;&#34394;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;&#30828;&#20214;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#26377;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with a finite-horizon inverse control problem, which has the goal of inferring, from observations, the possibly non-convex and non-stationary cost driving the actions of an agent. In this context, we present a result that enables cost estimation by solving an optimization problem that is convex even when the agent cost is not and when the underlying dynamics is nonlinear, non-stationary and stochastic. To obtain this result, we also study a finite-horizon forward control problem that has randomized policies as decision variables. For this problem, we give an explicit expression for the optimal solution. Moreover, we turn our findings into algorithmic procedures and we show the effectiveness of our approach via both in-silico and experimental validations with real hardware. All the experiments confirm the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;&#65292;&#38024;&#23545;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#26500;&#36896;&#20102;&#19968;&#31181;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2304.08172</link><description>&lt;p&gt;
&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Pointwise convergence theorem of gradient descent in sparse deep neural network. (arXiv:2304.08172v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;&#65292;&#38024;&#23545;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#26500;&#36896;&#20102;&#19968;&#31181;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#29702;&#35770;&#32467;&#26500;&#36880;&#28176;&#24471;&#21040;&#20102;&#38416;&#26126;&#12290;Imaizumi-Fukumizu&#65288;2019&#65289;&#21644;Suzuki&#65288;2019&#65289;&#25351;&#20986;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#20026;&#38750;&#20809;&#28369;&#20989;&#25968;&#26102;&#65292;DNN&#30340;&#23398;&#20064;&#33021;&#21147;&#20248;&#20110;&#20808;&#21069;&#30340;&#29702;&#35770;&#12290;&#28982;&#32780;&#65292;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#20247;&#22810;&#30740;&#31350;&#23581;&#35797;&#22312;&#27809;&#26377;&#20219;&#20309;&#32479;&#35745;&#35770;&#35777;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25968;&#23398;&#30740;&#31350;&#65292;&#25506;&#31350;&#30495;&#27491;&#33021;&#22815;&#24341;&#21457;&#26799;&#24230;&#19979;&#38477;&#30340;DNN&#26550;&#26500;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#24615;&#65292;&#36825;&#19968;&#23581;&#35797;&#20284;&#20046;&#26356;&#36148;&#36817;&#23454;&#38469;DNN&#12290;&#26412;&#25991;&#23558;&#30446;&#26631;&#20989;&#25968;&#38480;&#21046;&#20026;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#65292;&#24182;&#22312;ReLU-DNN&#20013;&#26500;&#36896;&#20102;&#19968;&#20010;&#31232;&#30095;&#19988;&#20855;&#26377;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#20013;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theoretical structure of deep neural network (DNN) has been clarified gradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the learning ability of DNN is superior to the previous theories when the target function is non-smooth functions. However, as far as the author is aware, none of the numerous works to date attempted to mathematically investigate what kind of DNN architectures really induce pointwise convergence of gradient descent (without any statistical argument), and this attempt seems to be closer to the practical DNNs. In this paper we restrict target functions to non-smooth indicator functions, and construct a deep neural network inducing pointwise convergence provided by gradient descent process in ReLU-DNN. The DNN has a sparse and a special shape, with certain variable transformations.
&lt;/p&gt;</description></item></channel></rss>