<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#24320;&#21457;&#25351;&#20196;&#39537;&#21160;&#28216;&#25103;&#24341;&#25806;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21019;&#24314;&#28216;&#25103;&#65292;&#20174;&#32780;&#38477;&#20302;&#28216;&#25103;&#24320;&#21457;&#30340;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00276</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25351;&#20196;&#39537;&#21160;&#28216;&#25103;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Instruction-Driven Game Engines on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00276
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#24320;&#21457;&#25351;&#20196;&#39537;&#21160;&#28216;&#25103;&#24341;&#25806;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21019;&#24314;&#28216;&#25103;&#65292;&#20174;&#32780;&#38477;&#20302;&#28216;&#25103;&#24320;&#21457;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Instruction-Driven Game Engine (IDGE) &#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36981;&#24490;&#33258;&#30001;&#24418;&#24335;&#30340;&#28216;&#25103;&#35268;&#21017;&#24182;&#33258;&#21160;&#29983;&#25104;&#28216;&#25103;&#36807;&#31243;&#26469;&#20351;&#28216;&#25103;&#24320;&#21457;&#27665;&#20027;&#21270;&#12290;IDGE&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#21457;&#20986;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#21019;&#24314;&#28216;&#25103;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#28216;&#25103;&#24320;&#21457;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#23558;IDGE&#30340;&#23398;&#20064;&#36807;&#31243;&#35270;&#20026;&#19979;&#19968;&#20010;&#29366;&#24577;&#39044;&#27979;&#20219;&#21153;&#65292;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#39044;&#27979;&#29609;&#23478;&#34892;&#21160;&#32473;&#20986;&#30340;&#28216;&#25103;&#29366;&#24577;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28216;&#25103;&#29366;&#24577;&#30340;&#35745;&#31639;&#24517;&#39035;&#20934;&#30830;&#65307;&#21542;&#21017;&#65292;&#36731;&#24494;&#30340;&#38169;&#35823;&#21487;&#33021;&#20250;&#30772;&#22351;&#28216;&#25103;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20197;&#35838;&#31243;&#26041;&#24335;&#35757;&#32451;IDGE&#65292;&#36880;&#28176;&#22686;&#21152;&#27169;&#22411;&#23545;&#22797;&#26434;&#22330;&#26223;&#30340;&#25509;&#35302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00276v1 Announce Type: new  Abstract: The Instruction-Driven Game Engine (IDGE) project aims to democratize game development by enabling a large language model (LLM) to follow free-form game rules and autonomously generate game-play processes. The IDGE allows users to create games by issuing simple natural language instructions, which significantly lowers the barrier for game development. We approach the learning process for IDGEs as a Next State Prediction task, wherein the model autoregressively predicts in-game states given player actions. It is a challenging task because the computation of in-game states must be precise; otherwise, slight errors could disrupt the game-play. To address this, we train the IDGE in a curriculum manner that progressively increases the model's exposure to complex scenarios.   Our initial progress lies in developing an IDGE for Poker, a universally cherished card game. The engine we've designed not only supports a wide range of poker variants b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#29420;&#31435;&#20110;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#31283;&#20581;&#21453;&#21521;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#26377;&#25928;&#22788;&#29702;&#23545;&#25239;&#20928;&#21270;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16067</link><description>&lt;p&gt;
&#38024;&#23545;&#23545;&#25239;&#20928;&#21270;&#30340;&#24378;&#22823;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust Diffusion Models for Adversarial Purification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16067
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#29420;&#31435;&#20110;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#31283;&#20581;&#21453;&#21521;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#26377;&#25928;&#22788;&#29702;&#23545;&#25239;&#20928;&#21270;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#30340;&#23545;&#25239;&#20928;&#21270;&#65288;AP&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26368;&#26377;&#21147;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#23545;&#23545;&#25239;&#25915;&#20987;&#24182;&#19981;&#31283;&#20581;&#36825;&#19968;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25193;&#25955;&#36807;&#31243;&#24456;&#23481;&#26131;&#30772;&#22351;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#21453;&#21521;&#36807;&#31243;&#21518;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#20294;&#19982;&#21407;&#22987;&#36755;&#20837;&#22270;&#20687;&#23436;&#20840;&#19981;&#21516;&#65292;&#23548;&#33268;&#26631;&#20934;&#31934;&#24230;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#28982;&#32780;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#31105;&#27490;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#23545;&#25239;&#24341;&#23548;&#30340;&#31283;&#20581;&#21453;&#21521;&#36807;&#31243;&#65292;&#23427;&#29420;&#31435;&#20110;&#32473;&#23450;&#30340;&#39044;&#35757;&#32451;DMs&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;DMs&#12290;&#36825;&#31181;&#24378;&#22823;&#30340;&#24341;&#23548;&#19981;&#20165;&#21487;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#20928;&#21270;&#31034;&#20363;&#20445;&#30041;&#26356;&#22810;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#36824;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16067v1 Announce Type: cross  Abstract: Diffusion models (DMs) based adversarial purification (AP) has shown to be the most powerful alternative to adversarial training (AT). However, these methods neglect the fact that pre-trained diffusion models themselves are not robust to adversarial attacks as well. Additionally, the diffusion process can easily destroy semantic information and generate a high quality image but totally different from the original input image after the reverse process, leading to degraded standard accuracy. To overcome these issues, a natural idea is to harness adversarial training strategy to retrain or fine-tune the pre-trained diffusion model, which is computationally prohibitive. We propose a novel robust reverse process with adversarial guidance, which is independent of given pre-trained DMs and avoids retraining or fine-tuning the DMs. This robust guidance can not only ensure to generate purified examples retaining more semantic content but also m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#20197;&#23558;&#31532;&#20108;&#27169;&#24577;&#65288;&#38750;RGB&#65289;&#32435;&#20837;NeRFs&#20013;&#65292;&#36890;&#36807;&#36873;&#25321;&#28909;&#25104;&#20687;&#20316;&#20026;&#31532;&#20108;&#27169;&#24577;&#26469;&#25361;&#25112;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#30340;&#25972;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.11865</link><description>&lt;p&gt;
&#21033;&#29992;&#28909;&#25104;&#20687;&#25506;&#32034;&#22810;&#27169;&#24577;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#24182;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#20197;&#23558;&#31532;&#20108;&#27169;&#24577;&#65288;&#38750;RGB&#65289;&#32435;&#20837;NeRFs&#20013;&#65292;&#36890;&#36807;&#36873;&#25321;&#28909;&#25104;&#20687;&#20316;&#20026;&#31532;&#20108;&#27169;&#24577;&#26469;&#25361;&#25112;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#22312;&#19968;&#32452;RGB&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#36805;&#36895;&#21457;&#23637;&#20026;&#26032;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#12290;&#26412;&#25991;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#23545;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#65288;&#22914;NeRFs&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22914;&#20309;&#23558;&#31532;&#20108;&#27169;&#24577;&#65288;&#38750;RGB&#65289;&#32435;&#20837;NeRFs&#20013;&#65306;&#65288;1&#65289;&#29420;&#31435;&#22320;&#20174;&#22836;&#35757;&#32451;&#27599;&#31181;&#27169;&#24577;&#65307;&#65288;2&#65289;&#22312;RGB&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#27169;&#24577;&#19978;&#36827;&#34892;&#24494;&#35843;&#65307;&#65288;3&#65289;&#28155;&#21152;&#31532;&#20108;&#20998;&#25903;&#65307;&#65288;4&#65289;&#28155;&#21152;&#19968;&#20010;&#21333;&#29420;&#30340;&#32452;&#20214;&#26469;&#39044;&#27979;&#65288;&#39068;&#33394;&#65289;&#39069;&#22806;&#27169;&#24577;&#30340;&#20540;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#28909;&#25104;&#20687;&#20316;&#20026;&#31532;&#20108;&#27169;&#24577;&#65292;&#22240;&#20026;&#20174;&#36752;&#23556;&#24230;&#26469;&#30475;&#65292;&#23427;&#19982;RGB&#26377;&#24456;&#22823;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#23558;&#20854;&#25972;&#21512;&#21040;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11865v1 Announce Type: cross  Abstract: Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard for the task of novel view synthesis when trained on a set of RGB images. In this paper, we conduct a comprehensive evaluation of neural scene representations, such as NeRFs, in the context of multi-modal learning. Specifically, we present four different strategies of how to incorporate a second modality, other than RGB, into NeRFs: (1) training from scratch independently on both modalities; (2) pre-training on RGB and fine-tuning on the second modality; (3) adding a second branch; and (4) adding a separate component to predict (color) values of the additional modality. We chose thermal imaging as second modality since it strongly differs from RGB in terms of radiosity, making it challenging to integrate into neural scene representations. For the evaluation of the proposed strategies, we captured a new publicly available multi-view dataset, ThermalMix, consisti
&lt;/p&gt;</description></item><item><title>&#33258;&#26412;&#30740;&#31350;&#21457;&#29616;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24613;&#24615;&#39635;&#32454;&#32990;&#30333;&#34880;&#30149;&#20998;&#31867;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#20026;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#19988;&#33410;&#32422;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.05379</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#23454;&#20363;&#23398;&#20064;&#29992;&#20110;&#24613;&#24615;&#39635;&#32454;&#32990;&#30333;&#34880;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05379
&lt;/p&gt;
&lt;p&gt;
&#33258;&#26412;&#30740;&#31350;&#21457;&#29616;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24613;&#24615;&#39635;&#32454;&#32990;&#30333;&#34880;&#30149;&#20998;&#31867;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#20026;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#19988;&#33410;&#32422;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30142;&#30149;&#35786;&#26029;&#20351;&#29992;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20381;&#36182;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#27169;&#22411;&#35757;&#32451;&#12290;&#24613;&#24615;&#39635;&#32454;&#32990;&#30333;&#34880;&#30149;&#65288;AML&#65289;&#31561;&#30142;&#30149;&#30001;&#20110;&#22312;&#21333;&#20010;&#32454;&#32990;&#27700;&#24179;&#19978;&#31232;&#32570;&#19988;&#26114;&#36149;&#30340;&#26631;&#27880;&#32780;&#38754;&#20020;&#25361;&#25112;&#12290;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#35299;&#20915;&#20102;&#24369;&#26631;&#35760;&#22330;&#26223;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#29992;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#30340;&#24378;&#22823;&#32534;&#30721;&#22120;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#22522;&#20110;MIL&#30340;AML&#20122;&#22411;&#20998;&#31867;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20174;&#34880;&#28034;&#29255;&#20013;&#21435;&#38500;&#20102;&#32534;&#30721;&#22120;&#35757;&#32451;&#26399;&#38388;&#30340;&#26631;&#35760;&#25968;&#25454;&#38656;&#27714;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;SSL&#26041;&#27861;SimCLR&#12289;SwAV&#21644;DINO&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#30417;&#30563;&#39044;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;SSL&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;SSL&#22312;MIL&#20013;&#30340;&#28508;&#21147;&#12290;&#36825;&#19968;&#31361;&#30772;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#19988;&#33410;&#32422;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05379v1 Announce Type: cross  Abstract: Automated disease diagnosis using medical image analysis relies on deep learning, often requiring large labeled datasets for supervised model training. Diseases like Acute Myeloid Leukemia (AML) pose challenges due to scarce and costly annotations on a single-cell level. Multiple Instance Learning (MIL) addresses weakly labeled scenarios but necessitates powerful encoders typically trained with labeled data. In this study, we explore Self-Supervised Learning (SSL) as a pre-training approach for MIL-based AML subtype classification from blood smears, removing the need for labeled data during encoder training. We investigate the three state-of-the-art SSL methods SimCLR, SwAV, and DINO, and compare their performance against supervised pre-training. Our findings show that SSL-pretrained encoders achieve comparable performance, showcasing the potential of SSL in MIL. This breakthrough offers a cost-effective and data-efficient solution, pr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20934;&#30830;&#22320;&#25191;&#34892;&#29702;&#35770;&#29289;&#29702;&#30740;&#31350;&#35770;&#25991;&#20013;&#20851;&#38190;&#30340;Hartree-Fock&#26041;&#27861;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.03154</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#22810;&#20307;&#29289;&#29702;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Quantum Many-Body Physics Calculations with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03154
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20934;&#30830;&#22320;&#25191;&#34892;&#29702;&#35770;&#29289;&#29702;&#30740;&#31350;&#35770;&#25991;&#20013;&#20851;&#38190;&#30340;Hartree-Fock&#26041;&#27861;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#39046;&#22495;&#65288;&#21253;&#25324;&#25968;&#23398;&#21644;&#31185;&#23398;&#25512;&#29702;&#65289;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#21069;&#25152;&#26410;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;LLMs&#21487;&#20197;&#20934;&#30830;&#22320;&#25191;&#34892;&#29702;&#35770;&#29289;&#29702;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#20851;&#38190;&#35745;&#31639;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#37327;&#23376;&#29289;&#29702;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#36817;&#20284;&#26041;&#27861;&#65306;Hartree-Fock&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#20998;&#26512;&#24615;&#30340;&#22810;&#27493;&#35745;&#31639;&#65292;&#23548;&#20986;&#36817;&#20284;&#21704;&#23494;&#39039;&#37327;&#21644;&#30456;&#24212;&#30340;&#33258;&#27965;&#26041;&#31243;&#12290;&#20026;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#35745;&#31639;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#27493;&#25552;&#31034;&#27169;&#26495;&#65292;&#23558;&#20998;&#26512;&#35745;&#31639;&#25286;&#20998;&#20026;&#26631;&#20934;&#27493;&#39588;&#65292;&#24182;&#20026;&#38382;&#39064;&#29305;&#23450;&#20449;&#24687;&#30041;&#20986;&#21344;&#20301;&#31526;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-4&#22312;&#25191;&#34892;&#36807;&#21435;&#21313;&#24180;&#30340;15&#31687;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#35745;&#31639;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20462;&#27491;&#20013;&#38388;&#27493;&#39588;&#65292;&#23427;&#21487;&#20197;&#27491;&#30830;&#22320;&#25512;&#23548;&#20986;&#26368;&#32456;&#30340;Hartree-Fock&#21704;&#23494;&#39039;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03154v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated an unprecedented ability to perform complex tasks in multiple domains, including mathematical and scientific reasoning. We demonstrate that with carefully designed prompts, LLMs can accurately carry out key calculations in research papers in theoretical physics. We focus on a broadly used approximation method in quantum physics: the Hartree-Fock method, requiring an analytic multi-step calculation deriving approximate Hamiltonian and corresponding self-consistency equations. To carry out the calculations using LLMs, we design multi-step prompt templates that break down the analytic calculation into standardized steps with placeholders for problem-specific information. We evaluate GPT-4's performance in executing the calculation for 15 research papers from the past decade, demonstrating that, with correction of intermediate steps, it can correctly derive the final Hartree-Fock Hamiltonian i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#23545;&#27604;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14874</link><description>&lt;p&gt;
&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65306;&#21033;&#29992;&#23545;&#27604;&#35299;&#30721;&#21644;&#33976;&#39311;&#25552;&#21319;LLM&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#23545;&#27604;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#36739;&#23567;&#30340;&#19994;&#20313;&#27169;&#22411;&#25110;&#38544;&#34255;&#29366;&#24577;&#24046;&#24322;&#20998;&#26512;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;DCD&#37319;&#29992;&#20102;&#23545;&#27604;&#24335;&#24605;&#32500;&#24341;&#23548;&#21644;&#20808;&#36827;&#30340;&#33976;&#39311;&#25216;&#26415;&#65292;&#21253;&#25324;Dropout&#21644;&#37327;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#23545;&#27604;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#21518;&#32773;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#21644;&#19994;&#20313;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;&#36890;&#36807;&#23558;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;DCD&#28040;&#38500;&#20102;&#23545;&#19994;&#20313;&#27169;&#22411;&#30340;&#38656;&#27714;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DCD&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#21508;&#31181;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;GSM8K&#21644;StrategyQA&#25968;&#25454;&#38598;&#20013;&#22343;&#36229;&#36807;&#20102;CD&#21644;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14874v1 Announce Type: cross  Abstract: We propose a straightforward approach called Distillation Contrastive Decoding (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during inference. In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought Prompting and advanced distillation techniques, including Dropout and Quantization. This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands. By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage. Our evaluations demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.14609</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Complex Qeury Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14609
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#31572;&#26696;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#25191;&#34892;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;&#30340;&#33021;&#21147;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22522;&#20110;&#22270;&#25512;&#29702;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#21644;&#36923;&#36753;&#26597;&#35810;&#34920;&#31034;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25214;&#21040;&#36923;&#36753;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#26597;&#35810;&#21333;&#20010;&#30693;&#35782;&#22270;&#35889;&#19978;&#65292;&#24182;&#19981;&#33021;&#24212;&#29992;&#20110;&#22810;&#20010;&#22270;&#24418;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#20849;&#20139;&#24102;&#26377;&#25935;&#24863;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#21487;&#33021;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#65292;&#20351;&#24471;&#20849;&#20139;&#21644;&#26500;&#24314;&#19968;&#20010;&#32858;&#21512;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#25512;&#29702;&#20197;&#26816;&#32034;&#26597;&#35810;&#31572;&#26696;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#19981;&#28165;&#26970;&#22914;&#20309;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#26597;&#35810;&#12290;&#19968;&#20010;&#23454;&#20307;&#21487;&#33021;&#28041;&#21450;&#21040;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#23545;&#20110;&#21457;&#29616;&#30693;&#35782;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14609v1 Announce Type: cross  Abstract: Complex logical query answering is a challenging task in knowledge graphs (KGs) that has been widely studied. The ability to perform complex logical reasoning is essential and supports various graph reasoning-based downstream tasks, such as search engines. Recent approaches are proposed to represent KG entities and logical queries into embedding vectors and find answers to logical queries from the KGs. However, existing proposed methods mainly focus on querying a single KG and cannot be applied to multiple graphs. In addition, directly sharing KGs with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated KG for reasoning to retrieve query answers. Thus, it remains unknown how to answer queries on multi-source KGs. An entity can be involved in various knowledge graphs and reasoning on multiple KGs and answering complex queries on multi-source KGs is important in discovering knowledge 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#20928;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AToP&#65289;&#27969;&#31243;&#65292;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#25200;&#21160;&#30772;&#22351;&#21644;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#24494;&#35843;&#20928;&#21270;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.16352</link><description>&lt;p&gt;
&#23545;&#20928;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AToP&#65289;&#65306;&#25552;&#21319;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16352
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#20928;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AToP&#65289;&#27969;&#31243;&#65292;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#25200;&#21160;&#30772;&#22351;&#21644;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#24494;&#35843;&#20928;&#21270;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#35748;&#20026;&#26131;&#21463;&#35774;&#35745;&#31934;&#33391;&#30340;&#23545;&#25239;&#25915;&#20987;&#24433;&#21709;&#12290;&#22522;&#20110;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#30340;&#26368;&#25104;&#21151;&#38450;&#24481;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#29305;&#23450;&#25915;&#20987;&#19979;&#30340;&#26368;&#20339;&#40065;&#26834;&#24615;&#65292;&#20294;&#26080;&#27861;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#30693;&#25915;&#20987;&#12290;&#22522;&#20110;&#23545;&#25239;&#20928;&#21270;&#65288;AP&#65289;&#30340;&#21478;&#19968;&#26377;&#25928;&#38450;&#24481;&#25216;&#26415;&#21487;&#20197;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#26080;&#27861;&#23454;&#29616;&#26368;&#20339;&#40065;&#26834;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;&#26631;&#20934;&#20934;&#30830;&#24615;&#38477;&#32423;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27969;&#31243;&#65292;&#31216;&#20026;&#23545;&#20928;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AToP&#65289;&#65292;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65306;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#65288;RT&#65289;&#30772;&#22351;&#25200;&#21160;&#65292;&#20197;&#36991;&#20813;&#23545;&#24050;&#30693;&#25915;&#20987;&#30340;&#36807;&#24230;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26410;&#30693;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#27867;&#21270;&#65307;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#23545;&#20928;&#21270;&#22120;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65288;FT&#65289;&#65292;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16352v2 Announce Type: replace-cross  Abstract: The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness. Meanwhile, both methods share one common limitation on the degraded standard accuracy. To mitigate these issues, we propose a novel pipeline called Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss. RT is essential to avoid overlearning to known attacks resulting in the robustness generalization to unseen attacks and FT is essential for the improvement of robustness. To evaluate our method in an e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20197;&#33258;&#25105;&#23637;&#31034;&#29702;&#35770;&#20026;&#25351;&#23548;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#21382;&#21490;&#23545;&#35805;&#20998;&#25104;&#21512;&#29702;&#21644;&#29702;&#24615;&#30340;&#21477;&#23376;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#38416;&#26126;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#22686;&#24378;&#21516;&#29702;&#24515;&#22238;&#24212;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.08702</link><description>&lt;p&gt;
&#29702;&#24615;&#24773;&#24863;&#65306;&#20197;&#33258;&#25105;&#23637;&#31034;&#29702;&#35770;&#20026;&#25351;&#23548;&#30340;&#22686;&#24378;&#22411;&#21516;&#29702;&#24515;&#22238;&#24212;&#29983;&#25104;&#30340;LLM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided by Self-presentation Theory. (arXiv:2312.08702v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20197;&#33258;&#25105;&#23637;&#31034;&#29702;&#35770;&#20026;&#25351;&#23548;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#21382;&#21490;&#23545;&#35805;&#20998;&#25104;&#21512;&#29702;&#21644;&#29702;&#24615;&#30340;&#21477;&#23376;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#38416;&#26126;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#22686;&#24378;&#21516;&#29702;&#24515;&#22238;&#24212;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#20934;&#30830;&#34920;&#36798;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#23545;&#20110;&#21516;&#29702;&#24515;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#23545;&#35805;&#26412;&#36523;&#30340;&#29702;&#24615;&#34920;&#36798;&#21644;&#21512;&#29702;&#30340;&#34920;&#29616;&#26041;&#38754;&#65292;&#21364;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#65292;&#32780;&#36825;&#20123;&#26159;&#35748;&#30693;&#21516;&#29702;&#24515;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#31038;&#20250;&#23398;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#33258;&#25105;&#23637;&#31034;&#29702;&#35770;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#21382;&#21490;&#23545;&#35805;&#20998;&#25104;&#21512;&#29702;&#21644;&#29702;&#24615;&#30340;&#21477;&#23376;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#38416;&#26126;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#23545;&#35805;&#20013;&#30340;&#29702;&#24615;&#20449;&#24687;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#19988;&#20808;&#21069;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#22806;&#37096;&#30693;&#35782;&#23384;&#22312;&#35821;&#20041;&#30683;&#30462;&#21644;&#29421;&#31364;&#35270;&#37326;&#30340;&#38480;&#21046;&#12290;&#32771;&#34385;&#21040;LLM&#22312;&#26234;&#33021;&#20195;&#29702;&#39046;&#22495;&#30340;&#21331;&#36234;&#34920;&#29616;&#65292;&#25105;&#20204;&#37319;&#29992;LLaMA2-70b&#20316;&#20026;&#29702;&#24615;&#22823;&#33041;&#26469;&#20998;&#26512;&#28145;&#36828;&#30340;&#36923;&#36753;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having the ability to empathize is crucial for accurately representing human behavior during conversations. Despite numerous research aim to improve the cognitive capability of models by incorporating external knowledge, there has been limited attention on the sensible and rational expression of the conversation itself, which are crucial components of the cognitive empathy. Guided by self-presentation theory in sociology, we have designed an innovative categorical approach that segregates historical dialogues into sensible and rational sentences and subsequently elucidate the context through the designed attention mechanism. However, the rational information within the conversation is restricted and the external knowledge used in previous methods have limitations of semantic contradiction and narrow vision field. Considering the impressive performance of LLM in the domain of intelligent agent. We employ LLaMA2-70b as a rational brain to analyze the profound logical information maintain
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20943;&#23569;&#26799;&#24230;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#21512;&#24182;&#30340;&#24615;&#33021;&#24182;&#23545;&#36229;&#21442;&#25968;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12808</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Model Merging by Uncertainty-Based Gradient Matching. (arXiv:2310.12808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20943;&#23569;&#26799;&#24230;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#21512;&#24182;&#30340;&#24615;&#33021;&#24182;&#23545;&#36229;&#21442;&#25968;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;&#30340;&#21152;&#26435;&#24179;&#22343;&#26469;&#21512;&#24182;&#65292;&#20294;&#20026;&#20160;&#20040;&#20250;&#36215;&#20316;&#29992;&#65292;&#20160;&#20040;&#24773;&#20917;&#19979;&#20250;&#22833;&#36133;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#21152;&#26435;&#24179;&#22343;&#30340;&#19981;&#20934;&#30830;&#24615;&#19982;&#26799;&#24230;&#19981;&#21305;&#37197;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20943;&#23569;&#19981;&#21305;&#37197;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#31181;&#32852;&#31995;&#36824;&#25581;&#31034;&#20102;&#20854;&#20182;&#26041;&#26696;&#65288;&#22914;&#24179;&#22343;&#20540;&#12289;&#20219;&#21153;&#31639;&#26415;&#21644;Fisher&#21152;&#26435;&#24179;&#22343;&#65289;&#20013;&#30340;&#38544;&#21547;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#38754;&#37117;&#22312;&#24615;&#33021;&#21644;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#26041;&#38754;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained on different datasets can be merged by a weighted-averaging of their parameters, but why does it work and when can it fail? Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by reducing the mismatch. The connection also reveals implicit assumptions in other schemes such as averaging, task arithmetic, and Fisher-weighted averaging. Our new method gives consistent improvements for large language models and vision transformers, both in terms of performance and robustness to hyperparameters.
&lt;/p&gt;</description></item><item><title>OpsEval&#26159;&#19968;&#20010;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26377;&#32447;&#32593;&#32476;&#25805;&#20316;&#12289;5G&#36890;&#20449;&#25805;&#20316;&#21644;&#25968;&#25454;&#24211;&#25805;&#20316;&#31561;&#20851;&#38190;&#22330;&#26223;&#19979;&#30340;&#33021;&#21147;&#27700;&#24179;&#65292;&#20026;&#25552;&#20379;&#38024;&#23545;AIOps&#23450;&#21046;&#30340;LLMs&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.07637</link><description>&lt;p&gt;
OpsEval: &#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models. (arXiv:2310.07637v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07637
&lt;/p&gt;
&lt;p&gt;
OpsEval&#26159;&#19968;&#20010;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26377;&#32447;&#32593;&#32476;&#25805;&#20316;&#12289;5G&#36890;&#20449;&#25805;&#20316;&#21644;&#25968;&#25454;&#24211;&#25805;&#20316;&#31561;&#20851;&#38190;&#22330;&#26223;&#19979;&#30340;&#33021;&#21147;&#27700;&#24179;&#65292;&#20026;&#25552;&#20379;&#38024;&#23545;AIOps&#23450;&#21046;&#30340;LLMs&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#22312;&#32763;&#35793;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#31561;NLP&#30456;&#20851;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;LLMs&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;AIOps&#65288;&#38754;&#21521;IT&#36816;&#32500;&#30340;&#20154;&#24037;&#26234;&#33021;&#65289;&#20013;&#65292;&#30001;&#20110;&#20854;&#20808;&#36827;&#30340;&#20449;&#24687;&#27719;&#24635;&#12289;&#25253;&#21578;&#20998;&#26512;&#21644;API&#35843;&#29992;&#33021;&#21147;&#32780;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;LLMs&#22312;AIOps&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#23578;&#26410;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#24341;&#23548;&#38024;&#23545;AIOps&#23450;&#21046;&#30340;LLMs&#30340;&#20248;&#21270;&#12290;&#19982;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#35780;&#20272;&#32593;&#32476;&#37197;&#32622;&#31561;&#29305;&#23450;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;OpsEval&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;LLMs&#35774;&#35745;&#30340;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;&#12290;OpsEval&#39318;&#27425;&#23545;LLMs&#22312;&#19977;&#20010;&#20851;&#38190;&#22330;&#26223;&#65288;&#26377;&#32447;&#32593;&#32476;&#25805;&#20316;&#12289;5G&#36890;&#20449;&#25805;&#20316;&#21644;&#25968;&#25454;&#24211;&#25805;&#20316;&#65289;&#20197;&#21450;&#19981;&#21516;&#30340;&#33021;&#21147;&#27700;&#24179;&#65288;&#30693;&#35782;&#22238;&#24518;&#12289;&#20998;&#26512;&#24605;&#32771;&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable capabilities in NLP-related tasks such as translation, summarizing, and generation. The application of LLMs in specific areas, notably AIOps (Artificial Intelligence for IT Operations), holds great potential due to their advanced abilities in information summarizing, report analyzing, and ability of API calling. Nevertheless, the performance of current LLMs in AIOps tasks is yet to be determined. Furthermore, a comprehensive benchmark is required to steer the optimization of LLMs tailored for AIOps. Compared with existing benchmarks that focus on evaluating specific fields like network configuration, in this paper, we present \textbf{OpsEval}, a comprehensive task-oriented AIOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs' proficiency in three crucial scenarios (Wired Network Operation, 5G Communication Operation, and Database Operation) at various ability levels (knowledge recall, analytical thinking, an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#31934;&#24230;&#19978;&#21482;&#25439;&#22833;$\Theta(\alpha)$&#21644;$O(\sqrt{\alpha})$&#65292;&#23545;&#24212;&#19981;&#21516;&#30340;&#20844;&#27491;&#32422;&#26463;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.11892</link><description>&lt;p&gt;
&#20851;&#20110;&#21463;&#24694;&#24847;&#22122;&#22768;&#24433;&#21709;&#30340;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11892
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#31934;&#24230;&#19978;&#21482;&#25439;&#22833;$\Theta(\alpha)$&#21644;$O(\sqrt{\alpha})$&#65292;&#23545;&#24212;&#19981;&#21516;&#30340;&#20844;&#27491;&#32422;&#26463;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#35757;&#32451;&#25968;&#25454;&#20013;&#24494;&#23567;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#12290;Konstantinov&#21644;Lampert (2021)&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#36127;&#38754;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#19981;&#24179;&#34913;&#30340;&#32676;&#32452;&#22823;&#23567;&#19979;&#23384;&#22312;&#19968;&#20123;&#25968;&#25454;&#20998;&#24067;&#65292;&#20219;&#20309;&#36866;&#24403;&#30340;&#23398;&#20064;&#22120;&#37117;&#20250;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#33030;&#24369;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#20048;&#35266;&#30340;&#35266;&#28857;&#65292;&#22914;&#26524;&#20801;&#35768;&#38543;&#26426;&#20998;&#31867;&#22120;&#65292;&#21017;&#24773;&#20917;&#26356;&#21152;&#32454;&#33268;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#21482;&#20250;&#20135;&#29983;$\Theta(\alpha)$&#30340;&#31934;&#24230;&#25439;&#22833;&#65292;&#20854;&#20013;$\alpha$&#26159;&#24694;&#24847;&#22122;&#22768;&#29575;&#65292;&#29978;&#33267;&#21487;&#20197;&#19982;&#27809;&#26377;&#20844;&#27491;&#32422;&#26463;&#30340;&#24773;&#20917;&#23436;&#20840;&#21305;&#37197;&#12290;&#23545;&#20110;&#26426;&#20250;&#22343;&#31561;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#21482;&#20250;&#20135;&#29983;$O(\sqrt{\alpha})$&#30340;&#25439;&#22833;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21305;&#37197;&#30340;$\Omega(\sqrt{\alpha})$&#30340;&#19979;&#30028;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Konstantinov&#21644;Lampert (2021)&#31034;&#33539;&#20102;&#23545;&#20110;&#36866;&#24403;&#30340;&#23398;&#20064;&#22120;&#65292;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#31934;&#24230;&#25439;&#22833;&#37117;&#26159;$\Omega(1)$&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.01001</link><description>&lt;p&gt;
DiffLoad:&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#26426;&#32452;&#25237;&#20837;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#39640;&#26031;&#20284;&#28982;&#26041;&#27861;&#30340;&#65292;&#23427;&#26088;&#22312;&#22312;&#32473;&#23450;&#30340;&#21327;&#21464;&#37327;&#19979;&#20934;&#30830;&#20272;&#35745;&#20998;&#24067;&#26399;&#26395;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#21644;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;Seq2seq&#32467;&#26500;&#26469;&#20272;&#35745;&#26412;&#20307;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#40065;&#26834;&#30340;&#21152;&#24615;&#26607;&#35199;&#20998;&#24067;&#26469;&#20272;&#35745;&#29289;&#35937;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#31361;&#21464;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#26694;&#26550;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#25581;&#31034;&#20102;&#24403;&#20154;&#31867;&#22312;&#20915;&#31574;&#20013;&#36807;&#24230;&#20381;&#36182;AI&#26102;&#65292;&#25913;&#21892;&#20449;&#20219;&#21487;&#33021;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#26377;&#36259;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.08804</link><description>&lt;p&gt;
&#20851;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#19982;&#20934;&#30830;&#24615;&#30340;&#30456;&#20114;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making. (arXiv:2304.08804v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#26694;&#26550;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#25581;&#31034;&#20102;&#24403;&#20154;&#31867;&#22312;&#20915;&#31574;&#20013;&#36807;&#24230;&#20381;&#36182;AI&#26102;&#65292;&#25913;&#21892;&#20449;&#20219;&#21487;&#33021;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#65292;&#23558;&#20154;&#31867;&#32622;&#20110;&#20915;&#31574;&#29615;&#36335;&#20013;&#22830;&#30340;&#20027;&#35201;&#25215;&#35834;&#26159;&#65292;&#20182;&#20204;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#31526;&#21512;&#20854;&#27491;&#30830;&#30340;&#21644;&#35206;&#30422;&#20854;&#38169;&#35823;&#30340;&#24314;&#35758;&#26469;&#34917;&#20805;AI&#31995;&#32479;&#12290;&#28982;&#32780;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#30475;&#21040;&#20154;&#31867;&#20542;&#21521;&#20110;&#36807;&#24230;&#25110;&#19981;&#36275;&#22320;&#20381;&#36182;AI&#24314;&#35758;&#65292;&#36825;&#24847;&#21619;&#30528;&#20182;&#20204;&#35201;&#20040;&#20381;&#20174;&#38169;&#35823;&#30340;&#24314;&#35758;&#65292;&#35201;&#20040;&#35206;&#30422;&#27491;&#30830;&#30340;&#24314;&#35758;&#12290;&#36825;&#31181;&#20381;&#36182;&#34892;&#20026;&#23545;&#20915;&#31574;&#20934;&#30830;&#24615;&#26377;&#23475;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#24182;&#20998;&#26512;&#20102;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#26694;&#26550;&#65292;&#20351;&#36825;&#31181;&#30456;&#20114;&#20851;&#31995;&#26356;&#21152;&#20855;&#20307;&#21270;&#12290;&#35813;&#26694;&#26550;&#24110;&#21161;&#25105;&#20204;&#35299;&#37322;&#21644;&#27604;&#36739;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#33719;&#24471;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#24178;&#39044;&#65288;&#20363;&#22914;&#35299;&#37322;&#65289;&#24433;&#21709;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20174;&#26694;&#26550;&#20013;&#25512;&#20986;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#23646;&#24615;&#65306;&#65288;i&#65289;&#24403;&#20154;&#31867;&#19981;&#36275;&#22320;&#20381;&#36182;AI&#24314;&#35758;&#26102;&#65292;&#25913;&#21892;&#20449;&#20219;&#23558;&#26174;&#30528;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#20182;&#20204;&#36807;&#24230;&#20381;&#36182;&#26102;&#65292;&#20449;&#20219;&#30340;&#25913;&#21892;&#21364;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In AI-assisted decision-making, a central promise of putting a human in the loop is that they should be able to complement the AI system by adhering to its correct and overriding its mistaken recommendations. In practice, however, we often see that humans tend to over- or under-rely on AI recommendations, meaning that they either adhere to wrong or override correct recommendations. Such reliance behavior is detrimental to decision-making accuracy. In this work, we articulate and analyze the interdependence between reliance behavior and accuracy in AI-assisted decision-making, which has been largely neglected in prior work. We also propose a visual framework to make this interdependence more tangible. This framework helps us interpret and compare empirical findings, as well as obtain a nuanced understanding of the effects of interventions (e.g., explanations) in AI-assisted decision-making. Finally, we infer several interesting properties from the framework: (i) when humans under-rely o
&lt;/p&gt;</description></item></channel></rss>