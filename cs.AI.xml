<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#24182;&#35299;&#20915;&#20102;AI&#29983;&#25104;&#30340;&#38754;&#23380;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#22120;&#29992;&#20110;&#39044;&#27979;&#38754;&#37096;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#21435;&#20559;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01002</link><description>&lt;p&gt;
AI&#29983;&#25104;&#30340;&#38754;&#23380;&#25670;&#33073;&#20102;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
AI-generated faces free from racial and gender stereotypes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01002
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#24182;&#35299;&#20915;&#20102;AI&#29983;&#25104;&#30340;&#38754;&#23380;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#22120;&#29992;&#20110;&#39044;&#27979;&#38754;&#37096;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#21435;&#20559;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;Stable Diffusion&#20043;&#31867;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;AI&#27169;&#22411;&#27599;&#22825;&#37117;&#34987;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20154;&#23545;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#25918;&#22823;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#25552;&#20986;&#20102;&#20851;&#20999;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#20219;&#24847;&#32473;&#23450;&#38754;&#37096;&#22270;&#20687;&#30340;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#24180;&#40836;&#32452;&#65292;&#24182;&#23637;&#31034;&#20854;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21033;&#29992;&#36825;&#20010;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#23545;Stable Diffusion&#22312;&#20845;&#31181;&#31181;&#26063;&#12289;&#20004;&#31181;&#24615;&#21035;&#12289;&#20116;&#20010;&#24180;&#40836;&#32452;&#12289;32&#20010;&#32844;&#19994;&#21644;&#20843;&#20010;&#23646;&#24615;&#19978;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;&#26367;&#20195;&#26041;&#26696;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;Stable Diffusion&#22312;&#25551;&#32472;&#21516;&#19968;&#31181;&#26063;&#30340;&#20010;&#20307;&#26102;&#30456;&#20284;&#31243;&#24230;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#20986;&#39640;&#24230;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#20363;&#22914;&#65292;&#23558;&#22823;&#22810;&#25968;&#20013;&#19996;&#30007;&#24615;&#25551;&#32472;&#20026;&#30382;&#32932;&#40669;&#40657;&#12289;&#30041;&#30528;&#32993;&#23376;&#12289;&#25140;&#30528;&#20256;&#32479;&#22836;&#39280;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#22686;&#21152;&#38754;&#37096;&#22810;&#26679;&#24615;&#30340;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative AI models such as Stable Diffusion are used daily by millions worldwide. However, many have raised concerns regarding how these models amplify racial and gender stereotypes. To study this phenomenon, we develop a classifier to predict the race, gender, and age group of any given face image, and show that it achieves state-of-the-art performance. Using this classifier, we quantify biases in Stable Diffusion across six races, two genders, five age groups, 32 professions, and eight attributes. We then propose novel debiasing solutions that outperform state-of-the-art alternatives. Additionally, we examine the degree to which Stable Diffusion depicts individuals of the same race as being similar to one another. This analysis reveals a high degree of stereotyping, e.g., depicting most middle eastern males as being dark-skinned, bearded, and wearing a traditional headdress. We address these limitations by proposing yet another novel solution that increases facial div
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PromptCodec&#65292;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#24449;&#24863;&#30693;&#25552;&#31034;&#32534;&#30721;&#22120;&#30340;&#39640;&#20445;&#30495;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#29305;&#24449;&#34920;&#31034;&#12289;&#33258;&#36866;&#24212;&#29305;&#24449;&#21152;&#26435;&#34701;&#21512;&#21644;&#25928;&#29575;&#20248;&#21270;&#26469;&#35299;&#20915;&#39640;&#21387;&#32553;&#29575;&#19979;&#30340;&#39640;&#20445;&#30495;&#38899;&#39057;&#37325;&#24314;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02702</link><description>&lt;p&gt;
PromptCodec: &#20351;&#29992;&#22522;&#20110;&#33258;&#36866;&#24212;&#29305;&#24449;&#24863;&#30693;&#30340;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#39640;&#20445;&#30495;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
PromptCodec: High-Fidelity Neural Speech Codec using Disentangled Representation Learning based Adaptive Feature-aware Prompt Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PromptCodec&#65292;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#24449;&#24863;&#30693;&#25552;&#31034;&#32534;&#30721;&#22120;&#30340;&#39640;&#20445;&#30495;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#29305;&#24449;&#34920;&#31034;&#12289;&#33258;&#36866;&#24212;&#29305;&#24449;&#21152;&#26435;&#34701;&#21512;&#21644;&#25928;&#29575;&#20248;&#21270;&#26469;&#35299;&#20915;&#39640;&#21387;&#32553;&#29575;&#19979;&#30340;&#39640;&#20445;&#30495;&#38899;&#39057;&#37325;&#24314;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#36817;&#26469;&#22312;&#29983;&#25104;&#35821;&#38899;&#24314;&#27169;&#39046;&#22495;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#65292;&#20363;&#22914;&#35821;&#38899;&#36716;&#25442;&#12289;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#31561;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#21387;&#32553;&#29575;&#19979;&#30830;&#20445;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#30340;&#39640;&#20445;&#30495;&#38899;&#39057;&#37325;&#24314;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PromptCodec&#65292;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#24449;&#24863;&#30693;&#25552;&#31034;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#31471;&#21040;&#31471;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#26469;&#33258;&#25552;&#31034;&#32534;&#30721;&#22120;&#30340;&#39069;&#22806;&#29305;&#24449;&#34920;&#31034;&#65292;PromptCodec&#21487;&#20197;&#20998;&#37197;&#38656;&#35201;&#22788;&#29702;&#30340;&#35821;&#38899;&#20449;&#24687;&#24182;&#22686;&#24378;&#20854;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#29305;&#24449;&#21152;&#26435;&#34701;&#21512;&#26041;&#27861;&#65292;&#20197;&#25972;&#21512;&#19981;&#21516;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#36317;&#31163;&#30340;&#26032;&#39062;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#20248;&#21270;PromptCodec&#30340;&#32534;&#30721;&#22120;&#20197;&#30830;&#20445;&#20854;&#25928;&#29575;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02702v1 Announce Type: cross  Abstract: Neural speech codec has recently gained widespread attention in generative speech modeling domains, like voice conversion, text-to-speech synthesis, etc. However, ensuring high-fidelity audio reconstruction of speech codecs under high compression rates remains an open and challenging issue. In this paper, we propose PromptCodec, a novel end-to-end neural speech codec model using disentangled representation learning based feature-aware prompt encoders. By incorporating additional feature representations from prompt encoders, PromptCodec can distribute the speech information requiring processing and enhance its capabilities. Moreover, a simple yet effective adaptive feature weighted fusion approach is introduced to integrate features of different encoders. Meanwhile, we propose a novel disentangled representation learning strategy based on cosine distance to optimize PromptCodec's encoders to ensure their efficiency, thereby further impr
&lt;/p&gt;</description></item><item><title>Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11585</link><description>&lt;p&gt;
Linguacodus&#65306;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#20013;&#36827;&#34892;&#21464;&#38761;&#24615;&#20195;&#30721;&#29983;&#25104;&#30340;&#21327;&#21516;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11585
&lt;/p&gt;
&lt;p&gt;
Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26080;&#32541;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Linguacodus&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#37096;&#32626;&#19968;&#20010;&#21160;&#24577;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#39640;&#32423;&#25968;&#25454;&#22609;&#24418;&#25351;&#20196;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36845;&#20195;&#22320;&#36716;&#25442;&#20026;&#20195;&#30721;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;Linguacodus&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#35780;&#20272;&#21508;&#31181;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20026;&#29305;&#23450;&#20219;&#21153;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#65292;&#24182;&#38416;&#26126;&#20102;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#20195;&#30721;&#12290;Linguacodus&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#20219;&#21153;&#25551;&#36848;&#21644;&#21487;&#25191;&#34892;&#20195;&#30721;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#23545;&#25512;&#36827;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#31561;&#20215;&#20110;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.17805</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks and Arithmetic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17805
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#31561;&#20215;&#20110;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#34920;&#24449;&#20102;&#36981;&#24490;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#19981;&#38480;&#20110;&#32858;&#21512;-&#32452;&#21512;GNN&#25110;&#20854;&#20182;&#29305;&#23450;&#31867;&#22411;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20351;&#29992;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#20934;&#30830;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#32467;&#26524;&#20013;&#65292;&#32593;&#32476;&#30340;&#28608;&#27963;&#20989;&#25968;&#25104;&#20026;&#30005;&#36335;&#20013;&#30340;&#38376;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#24120;&#25968;&#28145;&#24230;&#30005;&#36335;&#21644;&#32593;&#32476;&#23478;&#26063;&#22343;&#25104;&#31435;&#65292;&#26080;&#35770;&#26159;&#22312;&#19968;&#33268;&#36824;&#26159;&#38750;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#25152;&#26377;&#24120;&#35265;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17805v1 Announce Type: cross  Abstract: We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types. We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#20851;&#27880;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#29702;&#35299;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.17304</link><description>&lt;p&gt;
&#25506;&#31350;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#23616;&#21644;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Probing Multimodal Large Language Models for Global and Local Semantic Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17304
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#20851;&#27880;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#29702;&#35299;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#23558;&#20854;&#20248;&#31168;&#30340;&#34920;&#31034;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#24577;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#21033;&#29992;&#22270;&#20687;&#25551;&#36848;&#23545;&#40784;&#25968;&#25454;&#38598;&#35757;&#32451;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;MLLMs&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#23436;&#25972;&#30340;&#22270;&#20687;&#20449;&#24687;&#65292;&#21363;&#20840;&#23616;&#20449;&#24687;&#65292;&#25110;&#32773;&#23427;&#20204;&#21482;&#33021;&#25429;&#25417;&#19968;&#20123;&#23616;&#37096;&#23545;&#35937;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#21487;&#20197;&#32534;&#30721;&#26356;&#22810;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#20854;&#34920;&#31034;&#21521;&#37327;&#22312;&#35270;&#35273;-&#35821;&#35328;&#34164;&#28085;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#19981;&#26159;&#39030;&#23618;&#12290;&#25105;&#20204;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#36827;&#19968;&#27493;&#25506;&#31350;&#27169;&#22411;&#30340;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#19987;&#27880;&#20110;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#20943;&#24369;&#20102;&#23545;&#20840;&#23616;&#20449;&#24687;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17304v1 Announce Type: cross  Abstract: The success of large language models has inspired researchers to transfer their exceptional representing ability to other modalities. Several recent works leverage image-caption alignment datasets to train multimodal large language models (MLLMs), which achieve state-of-the-art performance on image-to-text tasks. However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local object information. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models for local semantic representation through object detection tasks. And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#24335;LLM&#21644;&#31526;&#21512;&#39044;&#27979;&#25216;&#26415;&#30340;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15368</link><description>&lt;p&gt;
&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#30340;&#25216;&#26415;&#23454;&#29616;&#35821;&#35328;&#25351;&#23548;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#23433;&#20840;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#24335;LLM&#21644;&#31526;&#21512;&#39044;&#27979;&#25216;&#26415;&#30340;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#35821;&#35328;&#25351;&#23548;&#26426;&#22120;&#20154;&#22242;&#38431;&#30340;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#12290;&#20219;&#21153;&#29992;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#34920;&#31034;&#65292;&#35201;&#27714;&#26426;&#22120;&#20154;&#22312;&#21508;&#31181;&#20301;&#32622;&#21644;&#35821;&#20041;&#23545;&#35937;&#19978;&#24212;&#29992;&#23427;&#20204;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#31227;&#21160;&#12289;&#25805;&#20316;&#21644;&#24863;&#30693;&#65289;&#12290;&#26368;&#36817;&#20960;&#31687;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35774;&#35745;&#26377;&#25928;&#30340;&#22810;&#26426;&#22120;&#20154;&#35745;&#21010;&#26469;&#35299;&#20915;&#31867;&#20284;&#30340;&#35268;&#21010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#20219;&#21153;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#24335;LLM&#30340;&#35268;&#21010;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#26469;&#23454;&#29616;&#30340;&#65292;CP&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#23545;&#20854;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25512;&#29702;&#12290;CP&#20801;&#35768;&#25152;&#25552;&#20986;&#30340;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#20197;&#20998;&#24067;&#26041;&#24335;&#25512;&#29702;&#20854;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#22312;&#20805;&#20998;&#20449;&#20219;&#26102;&#33021;&#22815;&#20570;&#20986;&#20010;&#21035;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15368v1 Announce Type: cross  Abstract: This paper addresses task planning problems for language-instructed robot teams. Tasks are expressed in natural language (NL), requiring the robots to apply their capabilities (e.g., mobility, manipulation, and sensing) at various locations and semantic objects. Several recent works have addressed similar planning problems by leveraging pre-trained Large Language Models (LLMs) to design effective multi-robot plans. However, these approaches lack mission performance and safety guarantees. To address this challenge, we introduce a new decentralized LLM-based planner that is capable of achieving high mission success rates. This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in black-box models. CP allows the proposed multi-robot planner to reason about its inherent uncertainty in a decentralized fashion, enabling robots to make individual decisions when they are sufficiently ce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04838</link><description>&lt;p&gt;
PaDeLLM-NER&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24182;&#34892;&#35299;&#30721;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;LLMs&#30340;&#39640;&#24310;&#36831;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#39034;&#24207;&#35299;&#30721;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;NER&#30340;&#25152;&#26377;&#26631;&#31614;&#21644;&#25552;&#21450;&#65292;&#26174;&#33879;&#22686;&#21152;&#20102;&#24207;&#21015;&#38271;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PaDeLLM-NER&#65288;Parallel Decoding in LLM for NE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#27169;&#22359;&#25110;&#26550;&#26500;&#20462;&#25913;&#21363;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#12290;PaDeLLM-NER&#20801;&#35768;&#21516;&#26102;&#35299;&#30721;&#25152;&#26377;&#25552;&#21450;&#65292;&#20174;&#32780;&#20943;&#23569;&#29983;&#25104;&#24310;&#36831;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PaDeLLM-NER&#30340;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#23545;&#33521;&#35821;&#21644;&#20013;&#25991;&#26469;&#35828;&#27604;&#33258;&#22238;&#24402;&#26041;&#27861;&#24555;1.76&#21040;10.22&#20493;&#12290;&#19982;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#32500;&#25345;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
&lt;/p&gt;</description></item><item><title>HEAM&#26159;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;&#20869;&#23384;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#23558;3D&#22534;&#21472;DRAM&#19982;DIMM&#38598;&#25104;&#65292;&#29992;&#20110;&#21152;&#36895;&#22788;&#29702;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#23884;&#20837;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.04032</link><description>&lt;p&gt;
HEAM: &#20351;&#29992;&#22788;&#29702;-&#20869;&#23384;&#36827;&#34892;&#25955;&#21015;&#23884;&#20837;&#21152;&#36895;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HEAM : Hashed Embedding Acceleration using Processing-In-Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04032
&lt;/p&gt;
&lt;p&gt;
HEAM&#26159;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;&#20869;&#23384;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#23558;3D&#22534;&#21472;DRAM&#19982;DIMM&#38598;&#25104;&#65292;&#29992;&#20110;&#21152;&#36895;&#22788;&#29702;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#23884;&#20837;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#25968;&#25454;&#20013;&#24515;&#20013;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#25191;&#34892;&#23884;&#20837;&#25805;&#20316;&#26102;&#38656;&#35201;&#22823;&#23481;&#37327;&#30340;&#20869;&#23384;&#21644;&#39640;&#24102;&#23485;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;DIMM-based&#36817;&#20869;&#23384;&#22788;&#29702;&#25216;&#26415;&#25110;&#24341;&#20837;3D&#22534;&#21472;DRAM&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#21644;&#25193;&#23637;&#20869;&#23384;&#24102;&#23485;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#22788;&#29702;&#26085;&#30410;&#25193;&#22823;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22823;&#23567;&#26102;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#25512;&#33616;&#27169;&#22411;&#24050;&#32463;&#22686;&#38271;&#21040;&#36229;&#36807;&#25968;&#21313;TB&#30340;&#22823;&#23567;&#65292;&#23548;&#33268;&#22312;&#20256;&#32479;&#21333;&#33410;&#28857;&#25512;&#26029;&#26381;&#21153;&#22120;&#19978;&#39640;&#25928;&#36816;&#34892;&#21464;&#24471;&#22256;&#38590;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#31639;&#27861;&#26041;&#27861;&#26469;&#20943;&#23567;&#23884;&#20837;&#34920;&#23481;&#37327;&#65292;&#20294;&#36890;&#24120;&#20250;&#23548;&#33268;&#20869;&#23384;&#35775;&#38382;&#22686;&#21152;&#25110;&#20869;&#23384;&#36164;&#28304;&#21033;&#29992;&#20302;&#25928;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;HEAM&#65292;&#19968;&#31181;&#24322;&#26500;&#20869;&#23384;&#26550;&#26500;&#65292;&#23558;3D&#22534;&#21472;DRAM&#19982;DIMM&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#21152;&#36895;&#32452;&#21512;&#23884;&#20837;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's data centers, personalized recommendation systems face challenges such as the need for large memory capacity and high bandwidth, especially when performing embedding operations. Previous approaches have relied on DIMM-based near-memory processing techniques or introduced 3D-stacked DRAM to address memory-bound issues and expand memory bandwidth. However, these solutions fall short when dealing with the expanding size of personalized recommendation systems. Recommendation models have grown to sizes exceeding tens of terabytes, making them challenging to run efficiently on traditional single-node inference servers. Although various algorithmic methods have been proposed to reduce embedding table capacity, they often result in increased memory access or inefficient utilization of memory resources. This paper introduces HEAM, a heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to accelerate recommendation systems in which compositional embedding is util
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;SNN&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29305;&#24449;&#25552;&#21462;&#21644;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.16141</link><description>&lt;p&gt;
SNNs&#20013;&#22522;&#20110;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;&#20462;&#21098;&#26041;&#27861;&#65292;&#21463;&#21040;&#20851;&#38190;&#24615;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis. (arXiv:2311.16141v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;SNN&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29305;&#24449;&#25552;&#21462;&#21644;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#33410;&#33021;&#21644;&#26080;&#20056;&#27861;&#29305;&#24615;&#65292;SNNs&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28145;&#24230;SNNs&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#38271;&#32473;&#27169;&#22411;&#37096;&#32626;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#32593;&#32476;&#20462;&#21098;&#36890;&#36807;&#21387;&#32553;&#32593;&#32476;&#35268;&#27169;&#26469;&#20943;&#23569;&#27169;&#22411;&#37096;&#32626;&#30340;&#30828;&#20214;&#36164;&#28304;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SNN&#20462;&#21098;&#26041;&#27861;&#30001;&#20110;&#20462;&#21098;&#36845;&#20195;&#22686;&#21152;&#20102;SNNs&#30340;&#35757;&#32451;&#38590;&#24230;&#65292;&#23548;&#33268;&#20462;&#21098;&#25104;&#26412;&#39640;&#26114;&#19988;&#24615;&#33021;&#25439;&#22833;&#20005;&#37325;&#12290;&#26412;&#25991;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#29992;&#20110;SNN&#20462;&#21098;&#30340;&#20877;&#29983;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#24182;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SNN&#20013;&#29992;&#20110;&#20851;&#38190;&#24615;&#30340;&#20302;&#25104;&#26412;&#24230;&#37327;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#22312;&#20462;&#21098;&#21518;&#23545;&#25152;&#20462;&#21098;&#32467;&#26500;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#24182;&#20877;&#29983;&#37027;&#20123;&#20855;&#26377;&#36739;&#39640;&#20851;&#38190;&#24615;&#30340;&#32467;&#26500;&#65292;&#20197;&#33719;&#21462;&#20851;&#38190;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have gained considerable attention due to the energy-efficient and multiplication-free characteristics. The continuous growth in scale of deep SNNs poses challenges for model deployment. Network pruning reduces hardware resource requirements of model deployment by compressing the network scale. However, existing SNN pruning methods cause high pruning costs and performance loss because the pruning iterations amplify the training difficulty of SNNs. In this paper, inspired by the critical brain hypothesis in neuroscience, we propose a regeneration mechanism based on the neuron criticality for SNN pruning to enhance feature extraction and accelerate the pruning process. Firstly, we propose a low-cost metric for the criticality in SNNs. Then, we re-rank the pruned structures after pruning and regenerate those with higher criticality to obtain the critical network. Our method achieves higher performance than the current state-of-the-art (SOTA) method with up t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Magmaw&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#36827;&#34892;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#25239;&#25915;&#20987;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#29983;&#25104;&#36890;&#29992;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#25915;&#20987;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#23545;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30340;&#38887;&#24615;&#12290;&#20351;&#29992;&#23454;&#26102;&#26080;&#32447;&#25915;&#20987;&#24179;&#21488;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.00207</link><description>&lt;p&gt;
Magmaw: &#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Magmaw: Modality-Agnostic Adversarial Attacks on Machine Learning-Based Wireless Communication Systems. (arXiv:2311.00207v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Magmaw&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#36827;&#34892;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#25239;&#25915;&#20987;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#29983;&#25104;&#36890;&#29992;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#25915;&#20987;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#23545;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30340;&#38887;&#24615;&#12290;&#20351;&#29992;&#23454;&#26102;&#26080;&#32447;&#25915;&#20987;&#24179;&#21488;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21512;&#24182;&#31471;&#21040;&#31471;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#25152;&#26377;&#29289;&#29702;&#23618;&#27169;&#22359;&#20197;&#23454;&#29616;&#32852;&#21512;&#25910;&#21457;&#22120;&#20248;&#21270;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#38024;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#31995;&#32479;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24182;&#26410;&#25552;&#20379;&#21253;&#25324;&#28304;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#12289;&#20849;&#21516;&#30340;&#29289;&#29702;&#23618;&#32452;&#20214;&#21644;&#26080;&#32447;&#39046;&#22495;&#32422;&#26463;&#22312;&#20869;&#30340;&#20840;&#38754;&#35270;&#35282;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Magmaw&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#38024;&#23545;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#20256;&#36755;&#30340;&#20219;&#20309;&#22810;&#27169;&#24577;&#20449;&#21495;&#29983;&#25104;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#19979;&#28216;&#24212;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#24341;&#20837;&#20102;&#26032;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#25915;&#20987;&#23545;&#29616;&#26377;&#24191;&#27867;&#20351;&#29992;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#25200;&#21160;&#20449;&#21495;&#20943;&#27861;&#38450;&#24481;&#26041;&#27861;&#30340;&#38887;&#24615;&#12290;&#20026;&#20102;&#27010;&#24565;&#35777;&#26126;&#65292;&#25105;&#20204;&#20351;&#29992;&#36719;&#20214;&#23450;&#20041;&#26080;&#32447;&#30005;&#31995;&#32479;&#26500;&#24314;&#20102;&#19968;&#20010;&#23454;&#26102;&#26080;&#32447;&#25915;&#20987;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has been instrumental in enabling joint transceiver optimization by merging all physical layer blocks of the end-to-end wireless communication systems. Although there have been a number of adversarial attacks on ML-based wireless systems, the existing methods do not provide a comprehensive view including multi-modality of the source data, common physical layer components, and wireless domain constraints. This paper proposes Magmaw, the first black-box attack methodology capable of generating universal adversarial perturbations for any multimodal signal transmitted over a wireless channel. We further introduce new objectives for adversarial attacks on ML-based downstream applications. The resilience of the attack to the existing widely used defense methods of adversarial training and perturbation signal subtraction is experimentally verified. For proof-of-concept evaluation, we build a real-time wireless attack platform using a software-defined radio system. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#35782;&#21035;&#24615;&#30340;DF&#35299;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2304.03365</link><description>&lt;p&gt;
&#22870;&#21169;&#36716;&#31227;&#30340;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Decision-Focused Learning for Reward Transfer. (arXiv:2304.03365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#35782;&#21035;&#24615;&#30340;DF&#35299;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20915;&#31574;&#37325;&#28857;&#65288;Decision-focused&#65292;DF&#65289;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#34987;&#20171;&#32461;&#20026;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#19987;&#27880;&#20110;&#23398;&#20064;&#26368;&#26377;&#21033;&#20110;&#33719;&#24471;&#39640;&#25253;&#37228;&#30340;MDP&#21160;&#24577;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#19987;&#27880;&#20110;&#30452;&#25509;&#20248;&#21270;&#25253;&#37228;&#26469;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#20294;&#20174;MLE&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#19981;&#22815;&#20934;&#30830;&#65292;&#22240;&#27492;&#21487;&#33021;&#23545;&#22870;&#21169;&#20989;&#25968;&#30340;&#21464;&#21270;&#24456;&#33030;&#24369;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;DF&#35299;&#30340;&#38750;&#35782;&#21035;&#24615;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29609;&#20855;&#31034;&#20363;&#21644;&#21307;&#30103;&#27169;&#25311;&#22120;&#19978;&#23637;&#31034;&#20102;RDF&#26174;&#30528;&#22686;&#21152;&#20102;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused (DF) model-based reinforcement learning has recently been introduced as a powerful algorithm which can focus on learning the MDP dynamics which are most relevant for obtaining high rewards. While this approach increases the performance of agents by focusing the learning towards optimizing for the reward directly, it does so by learning less accurate dynamics (from a MLE standpoint), and may thus be brittle to changes in the reward function. In this work, we develop the robust decision-focused (RDF) algorithm which leverages the non-identifiability of DF solutions to learn models which maximize expected returns while simultaneously learning models which are robust to changes in the reward function. We demonstrate on a variety of toy example and healthcare simulators that RDF significantly increases the robustness of DF to changes in the reward function, without decreasing the overall return the agent obtains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.09010</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#25351;&#25968;&#26631;&#20934;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39118;&#38505;&#20013;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#21644;&#31995;&#32479;&#21442;&#25968;&#25200;&#21160;&#30340;&#24433;&#21709;&#32780;&#19981;&#22815;&#31283;&#20581;&#12290;&#22240;&#27492;,&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#65292;&#26679;&#26412;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#27169;&#22411;&#39118;&#38505;&#25935;&#24863;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#21464;&#20307;&#65292;&#20854;&#23454;&#29616;&#36807;&#31243;&#31867;&#20284;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#25968;&#26631;&#20934;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#31574;&#30053;&#39118;&#38505;&#25935;&#24863;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#33945;&#29305;&#21345;&#32599;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#22312;&#32447;(&#26102;&#38388;&#24046;&#20998;)&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#25968;&#26631;&#20934;&#30340;&#20351;&#29992;&#33021;&#22815;&#25512;&#24191;&#24120;&#29992;&#30340;&#29305;&#23450;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#22312;&#25670;&#21160;&#26438;&#21644;&#25670;&#25670;&#26438;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#23454;&#29616;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
&lt;/p&gt;</description></item></channel></rss>