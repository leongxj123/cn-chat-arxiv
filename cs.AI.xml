<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#21644;GAN&#32593;&#32476;&#65292;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#20174;EEG&#20449;&#21495;&#20013;&#24674;&#22797;&#20986;&#21508;&#31181;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#32467;&#21512;&#23545;&#25239;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#65292;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.10115</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#21644;GAN&#20174;EEG&#35760;&#24405;&#20013;&#29983;&#25104;&#35270;&#35273;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#21644;GAN&#32593;&#32476;&#65292;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#20174;EEG&#20449;&#21495;&#20013;&#24674;&#22797;&#20986;&#21508;&#31181;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#32467;&#21512;&#23545;&#25239;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#65292;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#24863;&#30693;&#24615;&#33041;&#35299;&#30721;&#39046;&#22495;&#30340;&#19968;&#20010;&#29616;&#20195;&#30740;&#31350;&#25361;&#25112;&#65292;&#21363;&#20351;&#29992;&#23545;&#25239;&#24335;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20174;EEG&#20449;&#21495;&#20013;&#21512;&#25104;&#22270;&#20687;&#12290;&#20855;&#20307;&#30446;&#26631;&#26159;&#21033;&#29992;&#20027;&#20307;&#35266;&#30475;&#22270;&#20687;&#26102;&#33719;&#24471;&#30340;EEG&#35760;&#24405;&#37325;&#26032;&#21019;&#24314;&#23646;&#20110;&#21508;&#31181;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#29983;&#25104;EEG&#32534;&#30721;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;GAN&#32593;&#32476;&#30340;&#29983;&#25104;&#22120;&#32452;&#20214;&#30340;&#36755;&#20837;&#12290;&#38500;&#20102;&#23545;&#25239;&#25439;&#22833;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10115v1 Announce Type: new  Abstract: In this study, we tackle a modern research challenge within the field of perceptual brain decoding, which revolves around synthesizing images from EEG signals using an adversarial deep learning framework. The specific objective is to recreate images belonging to various object categories by leveraging EEG recordings obtained while subjects view those images. To achieve this, we employ a Transformer-encoder based EEG encoder to produce EEG encodings, which serve as inputs to the generator component of the GAN network. Alongside the adversarial loss, we also incorporate perceptual loss to enhance the quality of the generated images.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25506;&#27979;&#20276;&#38543;&#31639;&#23376;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Fourier&#22522;&#19978;&#36827;&#34892;&#25237;&#24433;&#26469;&#36924;&#36817;&#19968;&#31867;&#38750;&#33258;&#20276;&#38543;&#30340;&#26080;&#38480;&#32500;&#32039;&#31639;&#23376;&#65292;&#24182;&#24212;&#29992;&#20110;&#24674;&#22797;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#31639;&#23376;&#30340;&#26684;&#26519;&#20989;&#25968;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35797;&#22270;&#22635;&#34917;&#31639;&#23376;&#23398;&#20064;&#29702;&#35770;&#19982;&#23454;&#36341;&#24046;&#36317;&#30340;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2401.17739</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#20276;&#38543;&#31639;&#23376;&#30340;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Operator learning without the adjoint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25506;&#27979;&#20276;&#38543;&#31639;&#23376;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Fourier&#22522;&#19978;&#36827;&#34892;&#25237;&#24433;&#26469;&#36924;&#36817;&#19968;&#31867;&#38750;&#33258;&#20276;&#38543;&#30340;&#26080;&#38480;&#32500;&#32039;&#31639;&#23376;&#65292;&#24182;&#24212;&#29992;&#20110;&#24674;&#22797;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#31639;&#23376;&#30340;&#26684;&#26519;&#20989;&#25968;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35797;&#22270;&#22635;&#34917;&#31639;&#23376;&#23398;&#20064;&#29702;&#35770;&#19982;&#23454;&#36341;&#24046;&#36317;&#30340;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#23376;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#20010;&#35868;&#22242;&#65306;&#22914;&#20309;&#22312;&#27809;&#26377;&#25506;&#27979;&#20276;&#38543;&#31639;&#23376;&#30340;&#24773;&#20917;&#19979;&#20174;&#25968;&#25454;&#20013;&#24674;&#22797;&#38750;&#33258;&#20276;&#38543;&#31639;&#23376;&#65311;&#30446;&#21069;&#30340;&#23454;&#38469;&#26041;&#27861;&#34920;&#26126;&#65292;&#22312;&#20165;&#20351;&#29992;&#30001;&#31639;&#23376;&#30340;&#27491;&#21521;&#20316;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#31639;&#23376;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#20276;&#38543;&#31639;&#23376;&#12290;&#28982;&#32780;&#65292;&#20197;&#30452;&#35266;&#30340;&#26041;&#24335;&#30475;&#65292;&#20284;&#20046;&#26377;&#24517;&#35201;&#37319;&#26679;&#20276;&#38543;&#31639;&#23376;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37096;&#20998;&#35299;&#37322;&#20102;&#36825;&#20010;&#35868;&#22242;&#65292;&#36890;&#36807;&#35777;&#26126;&#22312;&#19981;&#26597;&#35810;&#20276;&#38543;&#31639;&#23376;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;Fourier&#22522;&#19978;&#36827;&#34892;&#25237;&#24433;&#26469;&#36924;&#36817;&#19968;&#31867;&#38750;&#33258;&#20276;&#38543;&#30340;&#26080;&#38480;&#32500;&#32039;&#31639;&#23376;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#32467;&#26524;&#24212;&#29992;&#20110;&#24674;&#22797;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#31639;&#23376;&#30340;&#26684;&#26519;&#20989;&#25968;&#65292;&#24182;&#23548;&#20986;&#19968;&#20010;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20294;&#25105;&#20204;&#30340;&#26159;&#31532;&#19968;&#20010;&#35797;&#22270;&#22635;&#34917;&#29702;&#35770;&#19982;&#23454;&#36341;&#24046;&#36317;&#30340;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a mystery at the heart of operator learning: how can one recover a non-self-adjoint operator from data without probing the adjoint? Current practical approaches suggest that one can accurately recover an operator while only using data generated by the forward action of the operator without access to the adjoint. However, naively, it seems essential to sample the action of the adjoint. In this paper, we partially explain this mystery by proving that without querying the adjoint, one can approximate a family of non-self-adjoint infinite-dimensional compact operators via projection onto a Fourier basis. We then apply the result to recovering Green's functions of elliptic partial differential operators and derive an adjoint-free sample complexity bound. While existing theory justifies low sample complexity in operator learning, ours is the first adjoint-free analysis that attempts to close the gap between theory and practice.
&lt;/p&gt;</description></item><item><title>GDL-DS&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20855;&#26377;&#20998;&#24067;&#36716;&#25442;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#30340;&#20449;&#24687;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2310.08677</link><description>&lt;p&gt;
GDL-DS: &#20998;&#24067;&#36716;&#25442;&#19979;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GDL-DS: A Benchmark for Geometric Deep Learning under Distribution Shifts. (arXiv:2310.08677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08677
&lt;/p&gt;
&lt;p&gt;
GDL-DS&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20855;&#26377;&#20998;&#24067;&#36716;&#25442;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#30340;&#20449;&#24687;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;(GDL)&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#25797;&#38271;&#23545;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#30340;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#20854;&#22312;&#22788;&#29702;&#20998;&#24067;&#36716;&#25442;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#35768;&#22810;&#30456;&#20851;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GDL-DS&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;GDL&#27169;&#22411;&#22312;&#20855;&#26377;&#20998;&#24067;&#36716;&#25442;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#31890;&#23376;&#29289;&#29702;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#21040;&#29983;&#29289;&#21270;&#23398;&#30340;&#19981;&#21516;&#31185;&#23398;&#39046;&#22495;&#65292;&#24182;&#21253;&#25324;&#21508;&#31181;&#20998;&#24067;&#36716;&#25442;&#65292;&#21253;&#25324;&#26465;&#20214;&#12289;&#21327;&#21464;&#21644;&#27010;&#24565;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26469;&#33258;&#36229;&#20986;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#30340;&#20449;&#24687;&#35775;&#38382;&#30340;&#19977;&#20010;&#32423;&#21035;&#65292;&#21253;&#25324;&#27809;&#26377;&#36229;&#20986;&#20998;&#24067;&#30340;&#20449;&#24687;&#12289;&#21482;&#26377;&#24102;&#26631;&#31614;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#21644;&#24102;&#26377;&#23569;&#25968;&#26631;&#31614;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#28041;&#21450;30&#20010;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#35780;&#20272;3&#31181;&#20449;&#24687;&#35775;&#38382;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning (GDL) has gained significant attention in various scientific fields, chiefly for its proficiency in modeling data with intricate geometric structures. Yet, very few works have delved into its capability of tackling the distribution shift problem, a prevalent challenge in many relevant applications. To bridge this gap, we propose GDL-DS, a comprehensive benchmark designed for evaluating the performance of GDL models in scenarios with distribution shifts. Our evaluation datasets cover diverse scientific domains from particle physics and materials science to biochemistry, and encapsulate a broad spectrum of distribution shifts including conditional, covariate, and concept shifts. Furthermore, we study three levels of information access from the out-of-distribution (OOD) testing data, including no OOD information, only OOD features without labels, and OOD features with a few labels. Overall, our benchmark results in 30 different experiment settings, and evaluates 3 
&lt;/p&gt;</description></item><item><title>Soda&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#21151;&#33021;&#24615;&#32534;&#31243;&#35821;&#35328;&#65292;&#29992;&#20110;&#25551;&#36848;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#22788;&#29702;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#23450;&#20041;&#27169;&#22411;&#21270;&#22797;&#26434;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.01961</link><description>&lt;p&gt;
Soda:&#19968;&#31181;&#29992;&#20110;&#25551;&#36848;&#20197;&#20154;&#20026;&#20013;&#24515;&#38382;&#39064;&#30340;&#38754;&#21521;&#23545;&#35937;&#30340;&#21151;&#33021;&#24615;&#32534;&#31243;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Soda: An Object-Oriented Functional Language for Specifying Human-Centered Problems. (arXiv:2310.01961v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01961
&lt;/p&gt;
&lt;p&gt;
Soda&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#21151;&#33021;&#24615;&#32534;&#31243;&#35821;&#35328;&#65292;&#29992;&#20110;&#25551;&#36848;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#22788;&#29702;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#23450;&#20041;&#27169;&#22411;&#21270;&#22797;&#26434;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Soda&#65288;Symbolic Objective Descriptive Analysis&#65289;&#65292;&#19968;&#31181;&#35821;&#35328;&#65292;&#26377;&#21161;&#20110;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#22788;&#29702;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#24182;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#26816;&#26597;&#23427;&#20204;&#27491;&#30830;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#35821;&#35328;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#26159;&#30001;&#23545;&#35745;&#31639;&#26426;&#31995;&#32479;&#22797;&#26434;&#35201;&#27714;&#36827;&#34892;&#25551;&#36848;&#30340;&#35774;&#35745;&#25152;&#28608;&#21457;&#30340;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#23450;&#20041;&#26469;&#24314;&#27169;&#36825;&#20123;&#35201;&#27714;&#26102;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#23646;&#24615;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#23427;&#26377;&#21161;&#20110;&#20197;&#26356;&#36879;&#26126;&#21644;&#26356;&#23569;&#20986;&#38169;&#30340;&#26041;&#24335;&#25551;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Soda (Symbolic Objective Descriptive Analysis), a language that helps to treat qualities and quantities in a natural way and greatly simplifies the task of checking their correctness. We present key properties for the language motivated by the design of a descriptive language to encode complex requirements on computer systems, and we explain how these key properties must be addressed to model these requirements with simple definitions. We give an overview of a tool that helps to describe problems in an easy way that we consider more transparent and less error-prone.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.11655</link><description>&lt;p&gt;
&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19982;&#24378;&#30423;&#21453;&#39304;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#30830;&#23450;&#24615;&#28436;&#21270;&#21644;&#19981;&#21487;&#35266;&#27979;&#30340;&#29366;&#24577;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20027;&#35201;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#22312;&#27599;&#19968;&#36718;&#33719;&#24471;&#30340;&#22870;&#21169;&#26159;&#36873;&#25321;&#34892;&#21160;&#30340;&#30701;&#26399;&#22870;&#21169;&#21644;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#31243;&#24230;&#65288;&#21363;&#36890;&#36807;&#20854;&#29366;&#24577;&#27979;&#37327;&#65289;&#30340;&#20989;&#25968;&#12290;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24179;&#21488;&#20174;&#29992;&#25143;&#23545;&#29305;&#23450;&#31867;&#22411;&#20869;&#23481;&#30340;&#21442;&#19982;&#20013;&#33719;&#24471;&#30340;&#22870;&#21169;&#19981;&#20165;&#21462;&#20915;&#20110;&#20855;&#20307;&#20869;&#23481;&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#36824;&#21462;&#20915;&#20110;&#29992;&#25143;&#19982;&#24179;&#21488;&#19978;&#20854;&#20182;&#31867;&#22411;&#20869;&#23481;&#20114;&#21160;&#21518;&#20854;&#20559;&#22909;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#955;&#8712;[0,1]&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#22240;&#20808;&#21069;&#20869;&#23481;&#28040;&#36153;&#32780;&#24555;&#36895;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CP&#20998;&#35299;&#31639;&#27861;DL-CPALS&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#26377;&#21033;&#30340;&#21021;&#22987;&#21270;&#20540;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#20449;&#36947;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.13947</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#27861;&#29992;&#20110;&#24352;&#37327;CP&#20998;&#35299;&#21450;&#20854;&#22312;&#22823;&#35268;&#27169;MIMO&#20449;&#36947;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep-Learning-Aided Alternating Least Squares for Tensor CP Decomposition and Its Application to Massive MIMO Channel Estimation. (arXiv:2305.13947v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CP&#20998;&#35299;&#31639;&#27861;DL-CPALS&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#26377;&#21033;&#30340;&#21021;&#22987;&#21270;&#20540;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#20449;&#36947;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CANDECOMP/PARAFAC (CP)&#20998;&#35299;&#26159;&#22312;&#22810;&#22495;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#31995;&#32479;&#20013;&#34920;&#36848;&#25509;&#25910;&#21040;&#30340;&#24352;&#37327;&#20449;&#21495;&#30340;&#26368;&#24120;&#29992;&#27169;&#22411;&#65292;&#22240;&#20026;&#25509;&#25910;&#26426;&#36890;&#24120;&#20250;&#23558;&#26469;&#33258;&#19981;&#21516;&#20256;&#36755;&#36335;&#24452;&#25110;&#29992;&#25143;&#30340;&#32452;&#20214;&#30456;&#21152;&#12290;&#20026;&#20102;&#23454;&#29616;&#20934;&#30830;&#21644;&#20302;&#24310;&#36831;&#30340;&#20449;&#36947;&#20272;&#35745;&#65292;&#38656;&#35201;&#22909;&#30340;&#12289;&#24555;&#36895;&#30340;CP&#20998;&#35299;&#31639;&#27861;&#12290;CP&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;CPALS&#65289;&#26159;&#35745;&#31639;CP&#20998;&#35299;&#30340;&#20027;&#21147;&#31639;&#27861;&#12290;&#20294;&#26159;&#65292;&#23427;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#21021;&#22987;&#21270;&#65292;&#33391;&#22909;&#30340;&#36215;&#22987;&#20540;&#21487;&#20197;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#19982;CPALS&#35299;&#32806;&#65292;&#24182;&#19981;&#19968;&#23450;&#26377;&#21033;&#20110;&#35299;&#20915;CP&#20998;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#31639;&#27861;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CPALS&#65288;DL-CPALS&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29983;&#25104;&#26377;&#21033;&#30340;&#21021;&#22987;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;DL-CPALS&#23558;DNN&#21644;CPALS&#38598;&#25104;&#21040;&#27169;&#22411;&#22522;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
CANDECOMP/PARAFAC (CP) decomposition is the mostly used model to formulate the received tensor signal in a multi-domain massive multiple-input multiple-output (MIMO) system, as the receiver generally sums the components from different paths or users. To achieve accurate and low-latency channel estimation, good and fast CP decomposition algorithms are desired. The CP alternating least squares (CPALS) is the workhorse algorithm for calculating the CP decomposition. However, its performance depends on the initializations, and good starting values can lead to more efficient solutions. Existing initialization strategies are decoupled from the CPALS and are not necessarily favorable for solving the CP decomposition. To enhance the algorithm's speed and accuracy, this paper proposes a deep-learning-aided CPALS (DL-CPALS) method that uses a deep neural network (DNN) to generate favorable initializations. The proposed DL-CPALS integrates the DNN and CPALS to a model-based deep learning paradigm
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#22312;&#22522;&#20110;&#22768;&#38899;&#30340;&#21333;&#35789;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#33258;&#21457;&#36830;&#25509;&#20004;&#20010;&#25110;&#19977;&#20010;&#21333;&#35789;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20250;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#25105;&#20204;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#24335;&#21644;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#37117;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.01626</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#30340;&#22522;&#30784;&#35821;&#27861;&#65306;&#33258;&#21457;&#32852;&#25509;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks. (arXiv:2305.01626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01626
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#22312;&#22522;&#20110;&#22768;&#38899;&#30340;&#21333;&#35789;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#33258;&#21457;&#36830;&#25509;&#20004;&#20010;&#25110;&#19977;&#20010;&#21333;&#35789;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20250;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#25105;&#20204;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#24335;&#21644;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#37117;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#30340;&#35745;&#31639;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#25991;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#26368;&#26222;&#36941;&#21644;&#22522;&#26412;&#30340;&#35821;&#27861;&#29305;&#24615;&#20043;&#19968;&#8212;&#8212;&#32852;&#25509;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21457;&#32852;&#25509;&#29616;&#35937;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#20010;&#21035;&#21333;&#35789;&#30340;&#22768;&#23398;&#35760;&#24405;&#19978;&#35757;&#32451;&#26102;&#65292;&#24320;&#22987;&#20135;&#29983;&#36755;&#20986;&#65292;&#36825;&#20123;&#36755;&#20986;&#23558;&#20004;&#20010;&#29978;&#33267;&#19977;&#20010;&#21333;&#35789;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#32780;&#19981;&#20250;&#25509;&#35302;&#21040;&#20855;&#26377;&#22810;&#20010;&#21333;&#35789;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#20004;&#20010;&#21333;&#35789;&#30340;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29615;&#22659;&#19979;&#35757;&#32451;&#30340;&#21407;&#22987;&#35821;&#38899;CNN&#20197;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#23427;&#19981;&#20165;&#23545;&#25105;&#20204;&#29702;&#35299;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#23398;&#20064;&#26041;&#24335;&#26377;&#24433;&#21709;&#65292;&#36824;&#23545;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational models of syntax are predominantly text-based. Here we propose that basic syntax can be modeled directly from raw speech in a fully unsupervised way. We focus on one of the most ubiquitous and basic properties of syntax -- concatenation. We introduce spontaneous concatenation: a phenomenon where convolutional neural networks (CNNs) trained on acoustic recordings of individual words start generating outputs with two or even three words concatenated without ever accessing data with multiple words in the input. Additionally, networks trained on two words learn to embed words into novel unobserved word combinations. To our knowledge, this is a previously unreported property of CNNs trained on raw speech in the Generative Adversarial Network setting and has implications both for our understanding of how these architectures learn as well as for modeling syntax and its evolution from raw acoustic inputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#23558;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;Soft Actor-Critic&#26041;&#27861;&#35843;&#25972;&#20026;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;Q&#20540;&#20302;&#20272;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;Atari&#28216;&#25103;&#21644;&#22823;&#35268;&#27169;MOBA&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.10081</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#31163;&#25955;&#22411;Soft Actor-Critic&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revisiting Discrete Soft Actor-Critic. (arXiv:2209.10081v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#23558;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;Soft Actor-Critic&#26041;&#27861;&#35843;&#25972;&#20026;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;Q&#20540;&#20302;&#20272;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;Atari&#28216;&#25103;&#21644;&#22823;&#35268;&#27169;MOBA&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;Soft Actor-Critic&#26041;&#27861;&#65288;SAC&#65289;&#35843;&#25972;&#20026;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#30340;SAC&#26041;&#27861;&#65292;&#24182;&#28145;&#20837;&#29702;&#35299;&#20102;&#22312;&#31163;&#25955;&#35774;&#32622;&#19979;&#20854;Q&#20540;&#20302;&#20272;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29109;&#24809;&#32602;&#21644;&#20855;&#26377;Q-clip&#30340;&#21452;&#24179;&#22343;Q-learning&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#21253;&#25324;Atari&#28216;&#25103;&#21644;&#19968;&#20010;&#22823;&#35268;&#27169;MOBA&#28216;&#25103;&#22312;&#20869;&#30340;&#20856;&#22411;&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;: https://github.com/coldsummerday/Revisiting-Discrete-SAC.
&lt;/p&gt;
&lt;p&gt;
We study the adaption of soft actor-critic (SAC) from continuous action space to discrete action space. We revisit vanilla SAC and provide an in-depth understanding of its Q value underestimation and performance instability issues when applied to discrete settings. We thereby propose entropy-penalty and double average Q-learning with Q-clip to address these issues. Extensive experiments on typical benchmarks with discrete action space, including Atari games and a large-scale MOBA game, show the efficacy of our proposed method. Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.
&lt;/p&gt;</description></item></channel></rss>