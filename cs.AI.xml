<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01204</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Learning for Non-Sequential Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#20013;SSL&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#19978;&#19979;&#25991;&#21270;&#21644;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;SSL&#24050;&#25104;&#20026;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#26032;&#36235;&#21183;&#65292;&#36825;&#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#26126;&#30830;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#25551;&#36848;&#24615;&#30340;&#34920;&#31034;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#22238;&#39038;&#21644;&#24635;&#32467;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#65288;SSL4NS-TD&#65289;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;NS-TD&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#26041;&#27861;&#34987;&#20998;&#20026;&#19977;&#32452;&#8212;&#8212;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#24182;&#20171;&#32461;&#20102;&#27599;&#20010;&#26041;&#21521;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#21160;&#26426;&#21644;&#20248;&#28857;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#20171;&#32461;&#20102;SSL4NS-TD&#30340;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
&lt;/p&gt;</description></item><item><title>SPMamba&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#30340;&#26032;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#26367;&#25442;Transformer&#32452;&#20214;&#20026;Mamba&#27169;&#22359;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2404.02063</link><description>&lt;p&gt;
SPMamba&#65306;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#35821;&#38899;&#20998;&#31163;&#20013;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
SPMamba: State-space model is all you need in speech separation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02063
&lt;/p&gt;
&lt;p&gt;
SPMamba&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#30340;&#26032;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#26367;&#25442;Transformer&#32452;&#20214;&#20026;Mamba&#27169;&#22359;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#20998;&#31163;&#39046;&#22495;&#65292;CNN&#21644;Transformer&#27169;&#22411;&#37117;&#23637;&#31034;&#20102;&#31283;&#20581;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24341;&#36215;&#20102;&#30740;&#31350;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#23545;&#20110;&#38271;&#24207;&#21015;&#38899;&#39057;&#30340;&#24314;&#27169;&#33021;&#21147;&#26377;&#38480;&#65292;&#23548;&#33268;&#20998;&#31163;&#24615;&#33021;&#19981;&#20339;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21363;SPMamba&#12290;&#25105;&#20204;&#37319;&#29992;TF-GridNet&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#26694;&#26550;&#65292;&#24182;&#23558;&#20854;Transformer&#32452;&#20214;&#26367;&#25442;&#20026;&#19968;&#20010;&#21452;&#21521;Mamba&#27169;&#22359;&#65292;&#26088;&#22312;&#25429;&#33719;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;Mamba&#22522;&#20110;&#26041;&#27861;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#26041;&#38754;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02063v1 Announce Type: cross  Abstract: In speech separation, both CNN- and Transformer-based models have demonstrated robust separation capabilities, garnering significant attention within the research community. However, CNN-based methods have limited modelling capability for long-sequence audio, leading to suboptimal separation performance. Conversely, Transformer-based methods are limited in practical applications due to their high computational complexity. Notably, within computer vision, Mamba-based methods have been celebrated for their formidable performance and reduced computational requirements. In this paper, we propose a network architecture for speech separation using a state-space model, namely SPMamba. We adopt the TF-GridNet model as the foundational framework and substitute its Transformer component with a bidirectional Mamba module, aiming to capture a broader range of contextual information. Our experimental results reveal an important role in the performa
&lt;/p&gt;</description></item><item><title>Particip-AI &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;&#65292;&#24341;&#39046;&#20154;&#24037;&#26234;&#33021;&#30340;&#27665;&#20027;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.14791</link><description>&lt;p&gt;
Particip-AI: &#19968;&#31181;&#27665;&#20027;&#35843;&#26597;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14791
&lt;/p&gt;
&lt;p&gt;
Particip-AI &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;&#65292;&#24341;&#39046;&#20154;&#24037;&#26234;&#33021;&#30340;&#27665;&#20027;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#22914;ChatGPT&#65292;&#20284;&#20046;&#38477;&#20302;&#20102;&#20844;&#20247;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21450;&#21033;&#29992;&#20854;&#21147;&#37327;&#30340;&#38376;&#27099;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;&#21644;&#21457;&#23637;&#20173;&#25484;&#25569;&#22312;&#23569;&#25968;&#20154;&#25163;&#20013;&#65292;&#21457;&#23637;&#36895;&#24230;&#21152;&#24555;&#19988;&#32570;&#20047;&#39118;&#38505;&#35780;&#20272;&#12290;&#20316;&#20026;&#36808;&#21521;&#20154;&#24037;&#26234;&#33021;&#27665;&#20027;&#27835;&#29702;&#21644;&#39118;&#38505;&#35780;&#20272;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Particip-AI&#65292;&#19968;&#20010;&#26694;&#26550;&#29992;&#20110;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#23558;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#21450;&#20854;&#21361;&#23475;&#21644;&#30410;&#22788;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#25910;&#38598;&#20351;&#29992;&#24773;&#20917;&#26356;&#21152;&#32454;&#33268;&#21644;&#35814;&#32454;&#22320;&#30740;&#31350;&#20844;&#20247;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#22312;&#22791;&#36873;&#26041;&#26696;&#19979;&#65288;&#21363;&#24320;&#21457;&#21644;&#19981;&#24320;&#21457;&#19968;&#31181;&#20351;&#29992;&#24773;&#20917;&#65289;&#36827;&#34892;&#39118;&#38505;&#35780;&#20272;&#21576;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#21361;&#23475;&#65292;&#24182;&#36890;&#36807;&#20570;&#20986;&#23545;&#20854;&#21457;&#23637;&#30340;&#32467;&#35770;&#24615;&#36873;&#25321;&#38416;&#26126;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#25351;&#23548;&#27665;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#25215;&#35834;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;295&#20010;&#20154;&#21475;&#22810;&#26679;&#21270;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14791v1 Announce Type: cross  Abstract: General purpose AI, such as ChatGPT, seems to have lowered the barriers for the public to use AI and harness its power. However, the governance and development of AI still remain in the hands of a few, and the pace of development is accelerating without proper assessment of risks. As a first step towards democratic governance and risk assessment of AI, we introduce Particip-AI, a framework to gather current and future AI use cases and their harms and benefits from non-expert public. Our framework allows us to study more nuanced and detailed public opinions on AI through collecting use cases, surfacing diverse harms through risk assessment under alternate scenarios (i.e., developing and not developing a use case), and illuminating tensions over AI development through making a concluding choice on its development. To showcase the promise of our framework towards guiding democratic AI, we gather responses from 295 demographically diverse 
&lt;/p&gt;</description></item><item><title>User-LLM&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#20351;&#20854;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13598</link><description>&lt;p&gt;
User-LLM: &#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23454;&#29616;&#26377;&#25928;&#30340;LLM&#35821;&#22659;&#21270;
&lt;/p&gt;
&lt;p&gt;
User-LLM: Efficient LLM Contextualization with User Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13598
&lt;/p&gt;
&lt;p&gt;
User-LLM&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#20351;&#20854;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#22797;&#26434;&#19988;&#28508;&#22312;&#22024;&#26434;&#30340;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;User-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#26469;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#12290;&#36825;&#20123;&#23884;&#20837;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20174;&#21508;&#31181;&#29992;&#25143;&#20132;&#20114;&#20013;&#31934;&#28860;&#20986;&#26469;&#30340;&#65292;&#33021;&#22815;&#25429;&#25417;&#28508;&#22312;&#29992;&#25143;&#20559;&#22909;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#36719;&#25552;&#31034;&#23558;&#36825;&#20123;&#29992;&#25143;&#23884;&#20837;&#19982;LLMs&#38598;&#25104;&#36215;&#26469;&#65292;&#20351;LLMs&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#22312;MovieLens&#12289;&#20122;&#39532;&#36874;&#35780;&#35770;&#21644;&#35895;&#27468;&#26412;&#22320;&#35780;&#35770;&#31561;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#21644;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#29992;&#25143;&#30340;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#35821;&#22659;&#21270;&#65292;&#21516;&#26102;&#22312;&#35745;&#31639;&#19978;&#20063;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13598v1 Announce Type: cross  Abstract: Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorpora
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;, &#39640;&#20142;&#20102;&#22312;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#21644;&#35299;&#37322;&#24471;&#20986;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19978;&#30340;&#22256;&#38590;&#24615;</title><link>https://arxiv.org/abs/2402.09056</link><description>&lt;p&gt;
&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#21542;&#20934;&#30830;&#22320;&#34920;&#31034;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;, &#39640;&#20142;&#20102;&#22312;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#21644;&#35299;&#37322;&#24471;&#20986;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19978;&#30340;&#22256;&#38590;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19981;&#20165;&#24212;&#36820;&#22238;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36824;&#24212;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#24120;&#29992;&#20110;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#36817;&#24180;&#26469;&#65292;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#31561;&#26367;&#20195;&#26041;&#27861;&#20063;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#21518;&#32773;&#26412;&#36136;&#19978;&#25193;&#23637;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#26524;&#30340;&#20108;&#38454;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#20013;&#21487;&#20197;&#25552;&#21462;&#35748;&#35782;&#65288;&#21644;&#38543;&#26426;&#65289;&#19981;&#30830;&#23450;&#24615;&#30340;&#24230;&#37327;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;&#65292;&#24378;&#35843;&#20102;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#35299;&#37322;&#32467;&#26524;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#24615;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#35774;&#32622;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#35745;&#25968;&#30340;&#24191;&#27867;&#26041;&#27861;&#65292;&#36825;&#31687;&#35770;&#25991;&#20026;&#21487;&#36776;&#35782;&#24615;&#21644;&#25910;&#25947;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09056v1 Announce Type: new Abstract: Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#29992;&#20110;&#31435;&#20307;&#35270;&#35273;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#31435;&#20307;&#35270;&#35273;&#25552;&#20379;&#30340;&#39069;&#22806;&#35270;&#35282;&#21644;&#30452;&#25509;&#25512;&#27979;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#22312;6D&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#19988;&#21487;&#36866;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#23494;&#38598;&#29305;&#24449;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05610</link><description>&lt;p&gt;
&#25193;&#23637;&#29992;&#20110;&#31435;&#20307;&#35270;&#35273;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Extending 6D Object Pose Estimators for Stereo Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05610
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#29992;&#20110;&#31435;&#20307;&#35270;&#35273;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#31435;&#20307;&#35270;&#35273;&#25552;&#20379;&#30340;&#39069;&#22806;&#35270;&#35282;&#21644;&#30452;&#25509;&#25512;&#27979;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#22312;6D&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#19988;&#21487;&#36866;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#23494;&#38598;&#29305;&#24449;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#31283;&#20581;&#22320;&#20272;&#35745;&#29289;&#20307;&#30340;6D&#23039;&#24577;&#20173;&#28982;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30452;&#25509;&#20174;RGB&#22270;&#20687;&#20013;&#20351;&#29992;&#23494;&#38598;&#29305;&#24449;&#22238;&#24402;&#23039;&#24577;&#30340;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#31435;&#20307;&#35270;&#35273;&#25552;&#20379;&#20102;&#23545;&#29289;&#20307;&#30340;&#39069;&#22806;&#35270;&#35282;&#65292;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#23039;&#24577;&#27495;&#20041;&#21644;&#36974;&#25377;&#12290;&#27492;&#22806;&#65292;&#31435;&#20307;&#22270;&#20687;&#21487;&#20197;&#30452;&#25509;&#25512;&#27979;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#32780;&#21333;&#30446;&#35270;&#35273;&#21017;&#38656;&#35201;&#20869;&#32622;&#23545;&#35937;&#23610;&#23544;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#23558;&#26368;&#20808;&#36827;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#25193;&#23637;&#21040;&#31435;&#20307;&#35270;&#35273;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#19982;BOP&#20860;&#23481;&#30340;YCB-V&#25968;&#25454;&#38598;&#30340;&#31435;&#20307;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#31435;&#20307;&#35270;&#35273;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;6D&#23039;&#24577;&#20272;&#35745;&#31639;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#23494;&#38598;&#29305;&#24449;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the 6D pose of objects accurately, quickly, and robustly remains a difficult task. However, recent methods for directly regressing poses from RGB images using dense features have achieved state-of-the-art results. Stereo vision, which provides an additional perspective on the object, can help reduce pose ambiguity and occlusion. Moreover, stereo can directly infer the distance of an object, while mono-vision requires internalized knowledge of the object's size. To extend the state-of-the-art in 6D object pose estimation to stereo, we created a BOP compatible stereo version of the YCB-V dataset. Our method outperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo vision and can easily be adopted for other dense feature-based algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02636</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#23398;&#20064;&#29420;&#31435;&#30340;&#22240;&#26524;&#26426;&#21046;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Learn Independent Causal Mechanisms?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#19981;&#24120;&#35265;&#30340;&#29615;&#22659;&#35774;&#32622;&#25110;&#20998;&#24067;&#21464;&#21270;&#30340;&#20219;&#21153;&#20013;&#65292;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#30446;&#21069;&#36890;&#24120;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#33030;&#24369;&#30340;&#65292;&#22240;&#20026;&#20219;&#21153;&#30340;&#33539;&#22260;&#21487;&#33021;&#26080;&#27861;&#39044;&#27979;&#25110;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#19988;&#20351;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#39069;&#22806;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#37027;&#20123;&#23398;&#20064;&#25277;&#35937;&#21464;&#37327;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#31995;&#32479;&#65292;&#22914;&#22240;&#26524;&#27169;&#22411;&#65292;&#21487;&#20197;&#34920;&#29616;&#20986;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#26356;&#24378;&#31283;&#20581;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#23384;&#22312;&#24182;&#20351;&#29992;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65288;ICMs&#65289;&#65292;&#34920;&#31034;&#21482;&#31232;&#30095;&#20132;&#20114;&#30340;&#39640;&#23618;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#20004;&#20010;&#27010;&#24565;&#65292;&#22312;LLMs&#20013;&#23398;&#20064;ICMs&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;&#22810;&#20010;&#31232;&#30095;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#30340;&#26032;LLM&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability. This issue has usually been alleviated by feeding more training data into the LLM. However, this method is brittle, as the scope of tasks may not be readily predictable or may evolve, and updating the model with new data generally requires extensive additional training. By contrast, systems, such as causal models, that learn abstract variables and causal relationships can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#23545;&#38382;&#39064;&#27714;&#35299;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31616;&#27905;&#24615;&#19981;&#20165;&#38477;&#20302;&#20102;&#22238;&#31572;&#38271;&#24230;&#65292;&#19988;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#12290;&#28982;&#32780;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.05618</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#27714;&#35299;&#20013;&#65292;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models. (arXiv:2401.05618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#23545;&#38382;&#39064;&#27714;&#35299;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31616;&#27905;&#24615;&#19981;&#20165;&#38477;&#20302;&#20102;&#22238;&#31572;&#38271;&#24230;&#65292;&#19988;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#12290;&#28982;&#32780;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;(CCoT)&#25552;&#31034;&#12290;&#25105;&#20204;&#23558;&#26631;&#20934;&#30340;CoT&#21644;CCoT&#25552;&#31034;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#20102;&#35299;&#31616;&#27905;&#24615;&#23545;&#22238;&#31572;&#38271;&#24230;&#21644;&#27491;&#30830;&#31572;&#26696;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;(MCQA)&#22522;&#20934;&#30340;&#35780;&#20272;&#12290;CCoT&#23558;GPT-3.5&#21644;GPT-4&#30340;&#24179;&#22343;&#22238;&#31572;&#38271;&#24230;&#20998;&#21035;&#20943;&#23569;&#20102;48.70&#65285;&#65292;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#65292;&#24102;&#26377;CCoT&#30340;GPT-3.5&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;27.69&#65285;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;CCoT&#23548;&#33268;&#27599;&#20010;&#26631;&#35760;&#30340;&#25104;&#26412;&#24179;&#22343;&#38477;&#20302;&#20102;22.67&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20351;&#29992;CoT&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#26469;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;LLM&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#20026;&#30740;&#31350;LLM&#20013;&#36880;&#27493;&#25512;&#29702;&#30340;&#24418;&#25104;&#34892;&#20026;&#30340;AI&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#21327;&#35843;&#26080;&#20154;&#26426;&#22242;&#38431;&#25293;&#25668;&#22797;&#26434;&#20154;&#32676;&#30340;&#22810;&#26080;&#20154;&#26426;&#22810;&#28436;&#21592;&#35270;&#35282;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30446;&#26631;&#30340;&#35270;&#35282;&#35268;&#21010;&#22120;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.10863</link><description>&lt;p&gt;
&#36138;&#24515;&#35270;&#35282;&#65306;&#22810;&#26080;&#20154;&#26426;&#35270;&#37326;&#35268;&#21010;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#30340;&#21327;&#21516;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Greedy Perspectives: Multi-Drone View Planning for Collaborative Coverage in Cluttered Environments. (arXiv:2310.10863v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#21327;&#35843;&#26080;&#20154;&#26426;&#22242;&#38431;&#25293;&#25668;&#22797;&#26434;&#20154;&#32676;&#30340;&#22810;&#26080;&#20154;&#26426;&#22810;&#28436;&#21592;&#35270;&#35282;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30446;&#26631;&#30340;&#35270;&#35282;&#35268;&#21010;&#22120;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#22242;&#38431;&#30340;&#37096;&#32626;&#21487;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25293;&#25668;&#21160;&#24577;&#20154;&#32676;&#65288;&#28436;&#21592;&#65289;&#30340;&#22823;&#35268;&#27169;&#24433;&#20687;&#65292;&#29992;&#20110;&#22242;&#38431;&#36816;&#21160;&#21644;&#30005;&#24433;&#21046;&#20316;&#31561;&#26032;&#24212;&#29992;&#39046;&#22495;&#12290;&#20026;&#20102;&#23454;&#29616;&#35813;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#36890;&#36807;&#39034;&#24207;&#36138;&#24515;&#35268;&#21010;&#36827;&#34892;&#23376;&#27169;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#26080;&#20154;&#26426;&#22242;&#38431;&#20043;&#38388;&#36827;&#34892;&#25668;&#20687;&#26426;&#35270;&#37326;&#30340;&#21487;&#25193;&#23637;&#20248;&#21270;&#65292;&#20294;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#21327;&#21516;&#25928;&#26524;&#38754;&#20020;&#25361;&#25112;&#12290;&#38556;&#30861;&#29289;&#21487;&#33021;&#20135;&#29983;&#36974;&#25377;&#24182;&#22686;&#21152;&#26080;&#20154;&#26426;&#30896;&#25758;&#30340;&#20960;&#29575;&#65292;&#36825;&#21487;&#33021;&#36829;&#21453;&#36817;&#20284;&#26368;&#20248;&#24615;&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#22312;&#31264;&#23494;&#29615;&#22659;&#20013;&#21327;&#35843;&#26080;&#20154;&#26426;&#22242;&#38431;&#25293;&#25668;&#20154;&#32676;&#65292;&#38656;&#35201;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#35270;&#35282;&#35268;&#21010;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30446;&#26631;&#30340;&#22810;&#26080;&#20154;&#26426;&#22810;&#28436;&#21592;&#35270;&#35282;&#35268;&#21010;&#22120;&#65292;&#24182;&#19982;&#36138;&#24515;&#24418;&#25104;&#35268;&#21010;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#35752;&#36974;&#25377;&#21644;&#30896;&#25758;&#23545;&#25293;&#25668;&#24212;&#29992;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35780;&#20272;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deployment of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for novel applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can be used for scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a greedy formation planner. To evaluate performance,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#65292;&#23558;&#38745;&#24577;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#21253;&#21547;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00158</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#20998;&#31867;&#20013;&#30340;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Feedback-guided Data Synthesis for Imbalanced Classification. (arXiv:2310.00158v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#65292;&#23558;&#38745;&#24577;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#21253;&#21547;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29616;&#29366;&#26159;&#20351;&#29992;&#26469;&#33258;&#38271;&#23614;&#20998;&#24067;&#30340;&#30495;&#23454;&#22270;&#20687;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#29992;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#36825;&#20123;&#38745;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#25253;&#21578;&#20102;&#36866;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#24615;&#33021;&#25552;&#21319;&#21463;&#21040;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#19981;&#36275;&#30340;&#38480;&#21046;&#65292;&#36825;&#23558;&#20419;&#36827;&#29983;&#25104;&#26679;&#26412;&#30340;&#26377;&#29992;&#24615;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#22686;&#24378;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#12290;&#20026;&#20102;&#20351;&#35813;&#26694;&#26550;&#26377;&#25928;&#65292;&#25105;&#20204;&#21457;&#29616;&#26679;&#26412;&#24517;&#39035;&#25509;&#36817;&#25163;&#22836;&#20219;&#21153;&#30340;&#30495;&#23454;&#25968;&#25454;&#25903;&#25345;&#65292;&#24182;&#19988;&#20855;&#26377;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38271;&#23614;&#25968;&#25454;&#38598;&#65288;ImageNe...&#19978;&#39564;&#35777;&#20102;&#19977;&#20010;&#21453;&#39304;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current status quo in machine learning is to use static datasets of real images for training, which often come from long-tailed distributions. With the recent advances in generative models, researchers have started augmenting these static datasets with synthetic data, reporting moderate performance improvements on classification tasks. We hypothesize that these performance gains are limited by the lack of feedback from the classifier to the generative model, which would promote the usefulness of the generated samples to improve the classifier's performance. In this work, we introduce a framework for augmenting static datasets with useful synthetic samples, which leverages one-shot feedback from the classifier to drive the sampling of the generative model. In order for the framework to be effective, we find that the samples must be close to the support of the real data of the task at hand, and be sufficiently diverse. We validate three feedback criteria on a long-tailed dataset (ImageNe
&lt;/p&gt;</description></item><item><title>&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#21019;&#26032;&#28508;&#21147;&#65292;&#20294;&#20173;&#38754;&#20020;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.06799</link><description>&lt;p&gt;
&#24403;&#22320;&#29699;&#31185;&#23398;&#36935;&#35265;&#22522;&#30784;&#27169;&#22411;&#65306;&#36208;&#21521;&#36890;&#29992;&#22320;&#29699;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
When Geoscience Meets Foundation Models: Towards General Geoscience Artificial Intelligence System. (arXiv:2309.06799v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06799
&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#21019;&#26032;&#28508;&#21147;&#65292;&#20294;&#20173;&#38754;&#20020;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20195;&#34920;&#20102;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#23427;&#20204;&#20174;&#30334;&#19975;&#20159;&#23383;&#33410;&#30340;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25581;&#31034;&#20986;&#27934;&#23519;&#21147;&#12290;&#28789;&#27963;&#30340;&#20219;&#21153;&#35268;&#33539;&#12289;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#20197;&#21450;&#22810;&#27169;&#24577;&#30340;&#30693;&#35782;&#34920;&#31034;&#20351;&#24471;&#32508;&#21512;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22320;&#29699;&#31185;&#23398;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#20801;&#35768;&#35299;&#20915;&#19982;&#22320;&#29699;&#31995;&#32479;&#30456;&#20114;&#20316;&#29992;&#30456;&#20851;&#30340;&#22810;&#31181;&#39044;&#27979;&#12289;&#27169;&#25311;&#21644;&#20915;&#31574;&#25361;&#25112;&#12290;&#39046;&#22495;&#19987;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#20043;&#38388;&#30340;&#21512;&#20316;&#25512;&#21160;&#20102;&#36825;&#20123;&#23453;&#36149;&#24037;&#20855;&#22312;&#29702;&#35299;&#25105;&#20204;&#22320;&#29699;&#30340;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#26041;&#38754;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#22686;&#24378;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#23558;&#26377;&#21161;&#20110;&#25512;&#21160;&#22320;&#29699;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geoscience foundation models represent a revolutionary approach in the field of Earth sciences by integrating massive cross-disciplinary data to simulate and understand the Earth systems dynamics. As a data-centric artificial intelligence (AI) paradigm, they uncover insights from petabytes of structured and unstructured data. Flexible task specification, diverse inputs and outputs and multi-modal knowledge representation enable comprehensive analysis infeasible with individual data sources. Critically, the scalability and generalizability of geoscience models allow for tackling diverse prediction, simulation, and decision challenges related to Earth systems interactions. Collaboration between domain experts and computer scientists leads to innovations in these invaluable tools for understanding the past, present, and future of our planet. However, challenges remain in validation and verification, scale, interpretability, knowledge representation, and social bias. Going forward, enhanci
&lt;/p&gt;</description></item><item><title>NeFL&#26159;&#19968;&#20010;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07761</link><description>&lt;p&gt;
NeFL: &#38024;&#23545;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NeFL: Nested Federated Learning for Heterogeneous Clients. (arXiv:2308.07761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07761
&lt;/p&gt;
&lt;p&gt;
NeFL&#26159;&#19968;&#20010;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#25345;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#65288;&#21363;&#38459;&#22622;&#32773;&#65289;&#20250;&#20943;&#24930;&#24635;&#20307;&#35757;&#32451;&#26102;&#38388;&#24182;&#38477;&#20302;&#24615;&#33021;&#12290;&#31995;&#32479;&#30340;&#24322;&#26500;&#24615;&#65292;&#21253;&#25324;&#24322;&#26500;&#35745;&#31639;&#21644;&#32593;&#32476;&#24102;&#23485;&#65292;&#24050;&#32463;&#34987;&#29992;&#26469;&#20943;&#36731;&#38459;&#22622;&#32773;&#30340;&#24433;&#21709;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#27169;&#22411;&#26550;&#26500;&#26041;&#38754;&#30340;&#33258;&#30001;&#24230;&#36739;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#65288;NeFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#20998;&#25104;&#23376;&#27169;&#22411;&#12290;NeFL&#36890;&#36807;&#23558;&#27169;&#22411;&#35299;&#37322;&#20026;&#35299;&#20915;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#27493;&#38271;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#22810;&#20010;&#23376;&#27169;&#22411;&#26102;&#20986;&#29616;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#35299;&#32806;&#20102;&#19968;&#20123;&#21442;&#25968;&#12290;NeFL&#20351;&#36164;&#28304;&#21463;&#38480;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#20837;&#32852;&#37030;&#23398;&#20064;&#27969;&#31243;&#65292;&#24182;&#20351;&#27169;&#22411;&#33021;&#22815;&#34987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be 
&lt;/p&gt;</description></item><item><title>INFLECT-DGNN&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12289;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#21644;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#29992;&#20110;&#24433;&#21709;&#32773;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#21644;GNN&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08131</link><description>&lt;p&gt;
INFLECT-DGNN: &#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24433;&#21709;&#32773;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks. (arXiv:2307.08131v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08131
&lt;/p&gt;
&lt;p&gt;
INFLECT-DGNN&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12289;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#21644;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#29992;&#20110;&#24433;&#21709;&#32773;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#21644;GNN&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#32593;&#32476;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#12290;&#22312;&#25512;&#33616;&#21644;&#23450;&#21521;&#33829;&#38144;&#39046;&#22495;&#20013;&#65292;&#24433;&#21709;&#32773;&#26816;&#27979;&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#21160;&#24577;&#32593;&#32476;&#34920;&#31034;&#22823;&#22823;&#21463;&#30410;&#30340;&#39046;&#22495;&#65292;&#21407;&#22240;&#26159;&#19981;&#26029;&#21457;&#23637;&#30340;&#23458;&#25143;-&#21697;&#29260;&#20851;&#31995;&#12290;&#20026;&#20102;&#38416;&#36848;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;INFLECT-DGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#20197;&#21450;&#31934;&#24515;&#35774;&#35745;&#30340;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#30340;&#26032;&#26694;&#26550;&#12290;&#20026;&#20102;&#35780;&#20272;&#39044;&#27979;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#22478;&#24066;&#32593;&#32476;&#30340;&#29420;&#29305;&#20225;&#19994;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#20197;&#21033;&#28070;&#20026;&#39537;&#21160;&#30340;&#24433;&#21709;&#32773;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#20197;&#21450;GNN&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging network information for predictive modeling has become widespread in many domains. Within the realm of referral and targeted marketing, influencer detection stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the ongoing development of customer-brand relationships. To elaborate this idea, we introduce INFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph Neural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural Networks (RNN) with weighted loss functions, the Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted rolling-window strategy. To evaluate predictive performance, we utilize a unique corporate data set with networks of three cities and derive a profit-driven evaluation methodology for influencer prediction. Our results show how using RNN to encode temporal attributes alongside GNNs significantly improves predictive perform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#25972;&#21512;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#21644;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16143</link><description>&lt;p&gt;
&#20026;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#32780;&#36827;&#34892;&#30340;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications. (arXiv:2306.16143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#21457;&#39046;&#22495;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#25972;&#21512;&#29983;&#25104;&#24335;&#29992;&#25143;&#20307;&#39564;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#21644;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#20307;&#39564;&#65288;UX&#65289;&#26159;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#19987;&#27880;&#20110;&#25552;&#39640;&#31995;&#32479;&#29992;&#25143;&#30340;&#30452;&#35266;&#24615;&#12289;&#36879;&#26126;&#24230;&#12289;&#31616;&#27905;&#24615;&#21644;&#20449;&#20219;&#24230;&#12290;&#22823;&#22810;&#25968;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;UX&#30740;&#31350;&#37117;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21363;&#27809;&#26377;&#20851;&#27880;&#29992;&#25143;&#38656;&#27714;&#65292;&#24182;&#20165;&#20165;&#23558;&#39046;&#22495;&#29992;&#25143;&#29992;&#20110;&#21487;&#29992;&#24615;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#26356;&#20856;&#22411;&#30340;UX&#26041;&#27861;&#26159;&#20808;&#38024;&#23545;&#29992;&#25143;&#30340;&#21487;&#29992;&#24615;&#36827;&#34892;&#23450;&#21046;&#65292;&#32780;&#19981;&#26159;&#39318;&#20808;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;UX&#30740;&#31350;&#25972;&#21512;&#21040;&#24320;&#21457;&#39046;&#22495;NLP&#24212;&#29992;&#20013;&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#24335;UX&#30740;&#31350;&#23558;&#39046;&#22495;&#29992;&#25143;&#32435;&#20837;&#21407;&#22411;&#24320;&#21457;&#30340;&#21021;&#22987;&#38454;&#27573;&#65292;&#21363;&#26500;&#24605;&#21644;&#27010;&#24565;&#35780;&#20272;&#38454;&#27573;&#65292;&#20197;&#21450;&#26368;&#21518;&#19968;&#38454;&#27573;&#35780;&#20272;&#29992;&#25143;&#20215;&#20540;&#30340;&#21464;&#21270;&#12290;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#38024;&#23545;&#36807;&#31243;&#24037;&#19994;&#20013;&#26085;&#24120;&#25805;&#20316;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#20041;&#25628;&#32034;&#30340;&#23436;&#25972;&#21407;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
User experience (UX) is a part of human-computer interaction (HCI) research and focuses on increasing intuitiveness, transparency, simplicity, and trust for system users. Most of the UX research for machine learning (ML) or natural language processing (NLP) focuses on a data-driven methodology, i.e., it fails to focus on users' requirements, and engages domain users mainly for usability evaluation. Moreover, more typical UX methods tailor the systems towards user usability, unlike learning about the user needs first. The paper proposes a methodology for integrating generative UX research into developing domain NLP applications. Generative UX research employs domain users at the initial stages of prototype development, i.e., ideation and concept evaluation, and the last stage for evaluating the change in user value. In the case study, we report the full-cycle prototype development of a domain-specific semantic search for daily operations in the process industry. Our case study shows tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#27010;&#29575;&#35821;&#20041;&#26469;&#26597;&#35810;&#19981;&#19968;&#33268;&#30340;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09138</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#26597;&#35810;&#19981;&#19968;&#33268;&#30340;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
Exploiting Uncertainty for Querying Inconsistent Description Logics Knowledge Bases. (arXiv:2306.09138v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#27010;&#29575;&#35821;&#20041;&#26469;&#26597;&#35810;&#19981;&#19968;&#33268;&#30340;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#20041;Web&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#31649;&#29702;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#24517;&#35201;&#12290;&#36825;&#20123;&#30693;&#35782;&#24211;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#20854;&#20869;&#23481;&#32463;&#24120;&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#21333;&#29420;&#32771;&#34385;&#25110;&#32508;&#21512;&#32771;&#34385;&#26102;&#21487;&#33021;&#21253;&#21547;&#30456;&#20114;&#30683;&#30462;&#30340;&#25551;&#36848;&#12290;&#20256;&#32479;&#30340;&#25512;&#29702;&#31639;&#27861;&#26080;&#27861;&#22788;&#29702;&#19981;&#19968;&#33268;&#30340;&#30693;&#35782;&#24211;&#65292;&#38656;&#35201;&#36890;&#36807;&#35843;&#35797;&#30693;&#35782;&#24211;&#26469;&#28040;&#38500;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;DISPONTE&#30340;&#29616;&#26377;&#27010;&#29575;&#35821;&#20041;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#22312;&#19981;&#19968;&#33268;&#30340;&#30693;&#35782;&#24211;&#20013;&#36827;&#34892;&#26597;&#35810;&#12290;&#25105;&#20204;&#22312;TRILL&#21644;BUNDLE&#25512;&#29702;&#22120;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25552;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20462;&#22797;&#35821;&#20041;&#36827;&#34892;&#20102;&#27491;&#24335;&#27604;&#36739;&#65292;&#21518;&#32773;&#26159;&#22312;&#32771;&#34385;DL&#25512;&#29702;&#20219;&#21153;&#26102;&#26368;&#20026;&#25104;&#29087;&#30340;&#35821;&#20041;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The necessity to manage inconsistency in Description Logics Knowledge Bases~(KBs) has come to the fore with the increasing importance gained by the Semantic Web, where information comes from different sources that constantly change their content and may contain contradictory descriptions when considered either alone or together. Classical reasoning algorithms do not handle inconsistent KBs, forcing the debugging of the KB in order to remove the inconsistency. In this paper, we exploit an existing probabilistic semantics called DISPONTE to overcome this problem and allow queries also in case of inconsistent KBs. We implemented our approach in the reasoners TRILL and BUNDLE and empirically tested the validity of our proposal. Moreover, we formally compare the presented approach to that of the repair semantics, one of the most established semantics when considering DL reasoning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29615;&#22659;&#25216;&#26415;&#21644;&#26234;&#33021;&#22914;&#20309;&#25913;&#21892;&#27531;&#30142;&#20154;&#30340;&#25252;&#29702;&#38656;&#27714;&#21450;&#29983;&#27963;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10726</link><description>&lt;p&gt;
&#29615;&#22659;&#25216;&#26415;&#19982;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Ambient Technology &amp; Intelligence. (arXiv:2305.10726v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29615;&#22659;&#25216;&#26415;&#21644;&#26234;&#33021;&#22914;&#20309;&#25913;&#21892;&#27531;&#30142;&#20154;&#30340;&#25252;&#29702;&#38656;&#27714;&#21450;&#29983;&#27963;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#25105;&#20204;&#26377;&#24180;&#36731;&#20154;&#21644;&#32769;&#24180;&#20154;&#12289;&#26377;&#29305;&#27530;&#38656;&#27714;&#30340;&#20154;&#20197;&#21450;&#21487;&#20197;&#33258;&#29702;&#30340;&#20154;&#28151;&#21512;&#22312;&#19968;&#36215;&#12290;&#20840;&#29699;&#26377;&#36229;&#36807;10&#20159;&#20272;&#35745;&#26377;&#27531;&#30142;&#30340;&#20154;&#65292;&#21344;&#19990;&#30028;&#20154;&#21475;&#30340;&#32422;15&#65285;&#65292;&#20854;&#20013;15&#23681;&#21450;&#20197;&#19978;&#30340;&#20154;&#21475;&#21344;3.8&#65285;&#65288;&#32452;&#32455;&#65292;2011&#65289;&#12290;&#24739;&#26377;&#27531;&#30142;&#30340;&#20154;&#25968;&#22240;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#31561;&#22240;&#32032;&#32780;&#19978;&#21319;&#12290;&#36825;&#20123;&#21644;&#20854;&#20182;&#22240;&#32032;&#20351;&#24471;&#24403;&#20170;&#31038;&#20250;&#24613;&#38656;&#36866;&#24403;&#30340;&#25252;&#29702;&#35774;&#26045;&#12290;&#24314;&#31435;&#20102;&#20960;&#20010;&#25252;&#29702;&#35774;&#26045;&#26469;&#24110;&#21161;&#27531;&#30142;&#20154;&#36807;&#19978;&#26085;&#24120;&#29983;&#27963;&#65292;&#19981;&#20250;&#34987;&#31038;&#21306;&#25490;&#38500;&#22312;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, we have a mixture of young and older individuals, people with special needs, and people who can care for themselves. Over 1 billion people are estimated to be disabled; this figure corresponds to about 15% of the world's population, with 3.8% (approximately 190 million people) accounting for people aged 15 and up (Organization, 2011). The number of people with disabilities is upward due to the increase in chronic health conditions and many other things. These and other factors have made the need for proper care facilities urgent in today's society. Several care facilities are built to help people with disabilities live their everyday lives and not be left out of the community.
&lt;/p&gt;</description></item><item><title>DNN-Defender&#26159;&#19968;&#31181;&#22522;&#20110;DRAM&#30340;&#21463;&#23475;&#32773;&#37325;&#28857;&#38450;&#24481;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#37327;&#21270;DNN&#65292;&#21033;&#29992;&#20869;&#23384;&#20013;&#20132;&#25442;&#30340;&#28508;&#21147;&#20197;&#25269;&#24481;&#26377;&#38024;&#23545;&#24615;&#30340;&#20301;&#32763;&#36716;&#25915;&#20987;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.08034</link><description>&lt;p&gt;
DNN-Defender: &#19968;&#31181;&#29992;&#20110;&#23545;&#25239; Adversarial Weight Attack &#30340;&#20869;&#23384;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38450;&#24481;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
DNN-Defender: An in-DRAM Deep Neural Network Defense Mechanism for Adversarial Weight Attack. (arXiv:2305.08034v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08034
&lt;/p&gt;
&lt;p&gt;
DNN-Defender&#26159;&#19968;&#31181;&#22522;&#20110;DRAM&#30340;&#21463;&#23475;&#32773;&#37325;&#28857;&#38450;&#24481;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#37327;&#21270;DNN&#65292;&#21033;&#29992;&#20869;&#23384;&#20013;&#20132;&#25442;&#30340;&#28508;&#21147;&#20197;&#25269;&#24481;&#26377;&#38024;&#23545;&#24615;&#30340;&#20301;&#32763;&#36716;&#25915;&#20987;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#23433;&#20840;&#25935;&#24863;&#39046;&#22495;&#20013;&#30340;&#37096;&#32626;&#65292;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;DRAM&#30340;RowHammer&#28431;&#27934;&#65292;&#20197;&#30830;&#23450;&#24615;&#21644;&#31934;&#30830;&#24615;&#22320;&#32763;&#36716;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#26435;&#37325;&#30340;&#20301;&#65292;&#20174;&#32780;&#24433;&#21709;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#26159;&#22522;&#20110;&#36719;&#20214;&#30340;&#65292;&#20363;&#22914;&#37325;&#26500;&#26435;&#37325;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#24320;&#38144;&#25110;&#24615;&#33021;&#19979;&#38477;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#36890;&#29992;&#30828;&#20214;&#30340;&#21463;&#23475;&#32773;/&#25915;&#20987;&#32773;&#37325;&#28857;&#38450;&#24481;&#26426;&#21046;&#20250;&#23548;&#33268;&#26114;&#36149;&#30340;&#30828;&#20214;&#24320;&#38144;&#65292;&#24182;&#20445;&#30041;&#21463;&#23475;&#32773;&#21644;&#25915;&#20987;&#32773;&#34892;&#20043;&#38388;&#30340;&#31354;&#38388;&#36830;&#25509;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#38024;&#23545;&#37327;&#21270;DNN&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;DRAM&#30340;&#21463;&#23475;&#32773;&#37325;&#28857;&#38450;&#24481;&#26426;&#21046;&#65292;&#31216;&#20026;DNN-Defender&#65292;&#21033;&#29992;&#20102;&#20869;&#23384;&#20013;&#20132;&#25442;&#30340;&#28508;&#21147;&#20197;&#25269;&#24481;&#26377;&#38024;&#23545;&#24615;&#30340;&#20301;&#32763;&#36716;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DNN-Defender&#21487;&#20197;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
With deep learning deployed in many security-sensitive areas, machine learning security is becoming progressively important. Recent studies demonstrate attackers can exploit system-level techniques exploiting the RowHammer vulnerability of DRAM to deterministically and precisely flip bits in Deep Neural Networks (DNN) model weights to affect inference accuracy. The existing defense mechanisms are software-based, such as weight reconstruction requiring expensive training overhead or performance degradation. On the other hand, generic hardware-based victim-/aggressor-focused mechanisms impose expensive hardware overheads and preserve the spatial connection between victim and aggressor rows. In this paper, we present the first DRAM-based victim-focused defense mechanism tailored for quantized DNNs, named DNN-Defender that leverages the potential of in-DRAM swapping to withstand the targeted bit-flip attacks. Our results indicate that DNN-Defender can deliver a high level of protection dow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24230;&#37327;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.02128</link><description>&lt;p&gt;
&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#65306;&#22312;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#24230;&#37327;&#34892;&#20026;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
System Neural Diversity: Measuring Behavioral Heterogeneity in Multi-Agent Learning. (arXiv:2305.02128v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24230;&#37327;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31185;&#23398;&#25552;&#20379;&#20102;&#22810;&#26679;&#24615;&#20855;&#26377;&#38887;&#24615;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#24378;&#21046;&#35201;&#27714;&#21516;&#36136;&#24615;&#20197;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#30340;&#25928;&#29575;&#12290;&#24403;&#23398;&#20064;&#20195;&#29702;&#31995;&#32479;&#19981;&#21463;&#21516;&#36136;&#31574;&#30053;&#30340;&#38480;&#21046;&#26102;&#65292;&#20010;&#20307;&#20195;&#29702;&#21487;&#33021;&#20250;&#21457;&#23637;&#20986;&#19981;&#21516;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#20135;&#29983;&#26377;&#21033;&#20110;&#31995;&#32479;&#30340;&#26032;&#20852;&#20114;&#34917;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#32570;&#20047;&#34913;&#37327;&#23398;&#20064;&#20195;&#29702;&#31995;&#32479;&#20013;&#34892;&#20026;&#22810;&#26679;&#24615;&#30340;&#24037;&#20855;&#24847;&#21619;&#30528;&#25105;&#20204;&#26080;&#27861;&#28145;&#20837;&#20102;&#35299;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#65288;SND&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#25506;&#35752;&#24182;&#35777;&#26126;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#23558;&#20854;&#19982;&#36328;&#23398;&#31185;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#26368;&#26032;&#34892;&#20026;&#22810;&#26679;&#24615;&#25351;&#26631;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary science provides evidence that diversity confers resilience. Yet, traditional multi-agent reinforcement learning techniques commonly enforce homogeneity to increase training sample efficiency. When a system of learning agents is not constrained to homogeneous policies, individual agents may develop diverse behaviors, resulting in emergent complementarity that benefits the system. Despite this feat, there is a surprising lack of tools that measure behavioral diversity in systems of learning agents. Such techniques would pave the way towards understanding the impact of diversity in collective resilience and performance. In this paper, we introduce System Neural Diversity (SND): a measure of behavioral heterogeneity for multi-agent systems where agents have stochastic policies. %over a continuous state space. We discuss and prove its theoretical properties, and compare it with alternate, state-of-the-art behavioral diversity metrics used in cross-disciplinary domains. Through
&lt;/p&gt;</description></item></channel></rss>