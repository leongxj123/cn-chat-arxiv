<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; NoFunEval&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#36825;&#20123;&#35201;&#27714;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;</title><link>https://rss.arxiv.org/abs/2401.15963</link><description>&lt;p&gt;
NoFunEval: &#26377;&#36259;&#30340;&#26159;&#65292;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#36229;&#20986;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#35201;&#27714;&#19978;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.15963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; NoFunEval&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#36825;&#20123;&#35201;&#27714;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;code LMs&#65289;&#30340;&#35780;&#20272;&#22522;&#20934;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;LMs&#26159;&#21542;&#33021;&#22815;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#19978;&#12290;&#22312;&#23454;&#38469;&#30340;&#36719;&#20214;&#24037;&#31243;&#20013;&#65292;&#24320;&#21457;&#20154;&#21592;&#20250;&#32771;&#34385;&#36229;&#20986;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;&#20182;&#20204;&#23545;&#20110;&#8220;&#22914;&#20309;&#8221;&#23454;&#29616;&#21151;&#33021;&#26377;&#30528;&#23545;&#25972;&#20307;&#31995;&#32479;&#35774;&#35745;&#30446;&#26631;&#65288;&#22914;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#65289;&#30340;&#35201;&#27714;&#12290;&#22914;&#26524;LMs&#33021;&#22815;&#23637;&#31034;&#23545;&#35201;&#27714;&#21644;&#20195;&#30721;&#35821;&#20041;&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#20182;&#20204;&#20063;&#20250;&#26356;&#21152;&#20449;&#20219;&#36825;&#20123;LMs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;NoFunEval&#26469;&#35780;&#20272;&#20195;&#30721;LMs&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26041;&#27861;Coding Concepts (CoCo)&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#20154;&#21592;&#21521;LMs&#20256;&#36798;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#23545;22&#20010;&#20195;&#30721;LMs&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26222;&#36941;&#34920;&#29616;&#19981;&#20339;&#65292;&#26263;&#31034;&#30528;&#23427;&#20204;&#22312;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing evaluation benchmarks of language models of code (code LMs) focus almost exclusively on whether the LMs can generate functionally-correct code. In real-world software engineering, developers think beyond functional correctness. They have requirements on "how" a functionality should be implemented to meet overall system design objectives like efficiency, security, and maintainability. They would also trust the code LMs more if the LMs demonstrate robust understanding of requirements and code semantics.   We propose a new benchmark NoFunEval to evaluate code LMs on non-functional requirements and simple classification instances for both functional and non-functional requirements. We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is that they generally falter when tested on our benchmark, hinting at fundamental blindspots in their tr
&lt;/p&gt;</description></item><item><title>ContactHandover&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25509;&#35302;&#24341;&#23548;&#30340;&#25235;&#21462;&#21644;&#29289;&#20307;&#36882;&#36865;&#38454;&#27573;&#26469;&#23454;&#29616;&#25104;&#21151;&#30340;&#29289;&#20307;&#36882;&#36865;&#12290;</title><link>https://arxiv.org/abs/2404.01402</link><description>&lt;p&gt;
ContactHandover: &#25509;&#35302;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
ContactHandover: Contact-Guided Robot-to-Human Object Handover
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01402
&lt;/p&gt;
&lt;p&gt;
ContactHandover&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25509;&#35302;&#24341;&#23548;&#30340;&#25235;&#21462;&#21644;&#29289;&#20307;&#36882;&#36865;&#38454;&#27573;&#26469;&#23454;&#29616;&#25104;&#21151;&#30340;&#29289;&#20307;&#36882;&#36865;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;&#26159;&#35768;&#22810;&#20154;&#26426;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#25104;&#21151;&#30340;&#36882;&#36865;&#38656;&#35201;&#26426;&#22120;&#20154;&#20445;&#25345;&#23545;&#29289;&#20307;&#30340;&#31283;&#23450;&#25235;&#21462;&#65292;&#21516;&#26102;&#30830;&#20445;&#20154;&#31867;&#20197;&#19968;&#31181;&#33258;&#28982;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#24335;&#25509;&#25910;&#29289;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ContactHandover&#65292;&#36825;&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#25509;&#35302;&#24341;&#23548;&#30340;&#25235;&#21462;&#38454;&#27573;&#21644;&#29289;&#20307;&#36882;&#36865;&#38454;&#27573;&#12290;&#22312;&#25235;&#21462;&#38454;&#27573;&#65292;ContactHandover&#39044;&#27979;&#26426;&#22120;&#20154;&#30340;6&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#21183;&#21644;&#20154;&#31867;&#25509;&#35302;&#28857;&#22312;&#29289;&#20307;&#19978;&#30340;3D&#21487;&#20379;&#24615;&#22270;&#12290;&#26426;&#22120;&#20154;&#30340;&#25235;&#21462;&#23039;&#21183;&#36890;&#36807;&#24809;&#32602;&#37027;&#20123;&#38459;&#30861;&#20154;&#31867;&#25509;&#35302;&#28857;&#30340;&#23039;&#21183;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#24182;&#25191;&#34892;&#25490;&#21517;&#26368;&#39640;&#30340;&#25235;&#21462;&#12290;&#22312;&#36882;&#36865;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#38752;&#36817;&#20154;&#31867;&#30340;&#25509;&#35302;&#28857;&#24182;&#26368;&#23567;&#21270;&#20154;&#31867;&#25163;&#33218;&#20851;&#33410;&#25197;&#30697;&#21644;&#20301;&#31227;&#26469;&#35745;&#31639;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#12290;&#25105;&#20204;&#22312;27&#31181;&#19981;&#21516;&#23478;&#29992;&#29289;&#21697;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01402v1 Announce Type: cross  Abstract: Robot-to-human object handover is an important step in many human robot collaboration tasks. A successful handover requires the robot to maintain a stable grasp on the object while making sure the human receives the object in a natural and easy-to-use manner. We propose ContactHandover, a robot to human handover system that consists of two phases: a contact-guided grasping phase and an object delivery phase. During the grasping phase, ContactHandover predicts both 6-DoF robot grasp poses and a 3D affordance map of human contact points on the object. The robot grasp poses are reranked by penalizing those that block human contact points, and the robot executes the highest ranking grasp. During the delivery phase, the robot end effector pose is computed by maximizing human contact points close to the human while minimizing the human arm joint torques and displacements. We evaluate our system on 27 diverse household objects and show that o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;LLM&#29983;&#25104;&#30340;&#34394;&#26500;&#20070;&#31821;&#25688;&#35201;&#36827;&#34892;&#20102;&#24544;&#23454;&#24615;&#21644;&#20869;&#23481;&#36873;&#25321;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#35780;&#20272;&#65292;&#24314;&#31435;&#20102;FABLES&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;26&#26412;&#20070;&#30340;3158&#20010;&#22768;&#26126;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#25104;&#21151;&#23545;LLM&#25688;&#35201;&#36827;&#34892;&#20102;&#22522;&#20110;&#24544;&#23454;&#24615;&#30340;&#25490;&#21517;</title><link>https://arxiv.org/abs/2404.01261</link><description>&lt;p&gt;
FABLES&#65306;&#35780;&#20272;&#20070;&#31821;&#25688;&#35201;&#20013;&#30340;&#24544;&#23454;&#24615;&#21644;&#20869;&#23481;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
FABLES: Evaluating faithfulness and content selection in book-length summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;LLM&#29983;&#25104;&#30340;&#34394;&#26500;&#20070;&#31821;&#25688;&#35201;&#36827;&#34892;&#20102;&#24544;&#23454;&#24615;&#21644;&#20869;&#23481;&#36873;&#25321;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#35780;&#20272;&#65292;&#24314;&#31435;&#20102;FABLES&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;26&#26412;&#20070;&#30340;3158&#20010;&#22768;&#26126;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#25104;&#21151;&#23545;LLM&#25688;&#35201;&#36827;&#34892;&#20102;&#22522;&#20110;&#24544;&#23454;&#24615;&#30340;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38271;&#25991;&#26412;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25216;&#26415;&#19978;&#21487;&#20197;&#24635;&#32467;&#38271;&#36798;100K&#20010;&#26631;&#35760;&#30340;&#20070;&#31821;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#25991;&#26723;&#30340;&#38271;&#24230;&#21644;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#23545;&#24544;&#23454;&#24615;&#31561;&#36755;&#20837;&#30456;&#20851;&#26041;&#38754;&#30340;&#35780;&#20272;&#12290;&#26412;&#25991;&#22312;&#34394;&#26500;&#20070;&#31821;&#30340;LLM&#29983;&#25104;&#25688;&#35201;&#19978;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#20154;&#31867;&#35780;&#20272;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;2023&#25110;2024&#24180;&#20986;&#29256;&#30340;&#20070;&#31821;&#25688;&#35201;&#65292;&#38599;&#20323;&#22312;&#36827;&#34892;&#27880;&#37322;&#20219;&#21153;&#20043;&#21069;&#24050;&#23436;&#20840;&#38405;&#35835;&#27599;&#26412;&#20070;&#30340;&#27880;&#37322;&#32773;&#26469;&#20943;&#23569;&#25104;&#26412;&#21644;&#35748;&#30693;&#36127;&#25285;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;FABLES&#25968;&#25454;&#38598;&#65292;&#23545;26&#26412;&#20070;&#30340;LLM&#29983;&#25104;&#25688;&#35201;&#20013;&#30340;3158&#20010;&#22768;&#26126;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#33457;&#36153;&#20102;5200&#32654;&#20803;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22522;&#20110;&#24544;&#23454;&#24615;&#23545;LLM&#25688;&#35201;&#36827;&#34892;&#25490;&#21517;&#65306;Claude-3-Opus&#22312;&#24544;&#23454;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#38381;&#28304;LLMs&#65292;&#32780;&#24320;&#28304;&#30340;Mixtral&#19982;GPT-3.5-Turbo&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01261v1 Announce Type: cross  Abstract: While long-context large language models (LLMs) can technically summarize book-length documents (&gt;100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#27969;&#23398;&#20064;&#20013;&#35745;&#31639;&#20915;&#31574;&#26641;&#26368;&#20339;&#20998;&#21106;&#28857;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27969;&#24335;&#35745;&#31639;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#39640;&#25928;&#36816;&#34892;</title><link>https://arxiv.org/abs/2403.19867</link><description>&lt;p&gt;
&#22312;&#27969;&#24335;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#25214;&#21040;&#20915;&#31574;&#26641;&#20998;&#21106;&#28857;
&lt;/p&gt;
&lt;p&gt;
Finding Decision Tree Splits in Streaming and Massively Parallel Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19867
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#27969;&#23398;&#20064;&#20013;&#35745;&#31639;&#20915;&#31574;&#26641;&#26368;&#20339;&#20998;&#21106;&#28857;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27969;&#24335;&#35745;&#31639;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#39640;&#25928;&#36816;&#34892;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#27969;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;&#20998;&#21106;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#35266;&#27979;&#25968;&#25454;&#27969;$x_i$&#21450;&#20854;&#26631;&#31614;$y_i$&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#23558;&#25968;&#25454;&#20998;&#20026;&#20004;&#32452;&#30340;&#26368;&#20339;&#20998;&#21106;&#28857;$j$&#65292;&#20351;&#24471;&#22343;&#26041;&#35823;&#24046;&#65288;&#22238;&#24402;&#38382;&#39064;&#65289;&#25110;&#35823;&#20998;&#31867;&#29575;&#65288;&#20998;&#31867;&#38382;&#39064;&#65289;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#24555;&#36895;&#30340;&#25968;&#25454;&#27969;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#20351;&#29992;&#20122;&#32447;&#24615;&#31354;&#38388;&#21644;&#23569;&#37327;&#27425;&#25968;&#30340;&#36941;&#21382;&#12290;&#36825;&#20123;&#31639;&#27861;&#36824;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#27169;&#22411;&#20013;&#12290;&#23613;&#31649;&#19981;&#33021;&#30452;&#25509;&#27604;&#36739;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;Domingos&#21644;Hulten&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65288;KDD 2000&#65289;&#30456;&#20114;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19867v1 Announce Type: cross  Abstract: In this work, we provide data stream algorithms that compute optimal splits in decision tree learning. In particular, given a data stream of observations $x_i$ and their labels $y_i$, the goal is to find the optimal split point $j$ that divides the data into two sets such that the mean squared error (for regression) or misclassification rate (for classification) is minimized. We provide various fast streaming algorithms that use sublinear space and a small number of passes for these problems. These algorithms can also be extended to the massively parallel computation model. Our work, while not directly comparable, complements the seminal work of Domingos and Hulten (KDD 2000).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;&#26694;&#26550;RPMArt&#65292;&#20027;&#35201;&#36129;&#29486;&#26159;&#33021;&#22815;&#31283;&#20581;&#22320;&#39044;&#27979;&#20851;&#33410;&#21442;&#25968;&#21644;&#21487;&#20449;&#28857;&#30340;RoArtNet&#12290;</title><link>https://arxiv.org/abs/2403.16023</link><description>&lt;p&gt;
RPMArt&#65306;&#38754;&#21521;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
RPMArt: Towards Robust Perception and Manipulation for Articulated Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16023
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;&#26694;&#26550;RPMArt&#65292;&#20027;&#35201;&#36129;&#29486;&#26159;&#33021;&#22815;&#31283;&#20581;&#22320;&#39044;&#27979;&#20851;&#33410;&#21442;&#25968;&#21644;&#21487;&#20449;&#28857;&#30340;RoArtNet&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#33410;&#23545;&#35937;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#24456;&#24120;&#35265;&#12290;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#26469;&#35828;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#34920;&#29616;&#20986;&#23545;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;&#25216;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20851;&#33410;&#23545;&#35937;&#26041;&#27861;&#19981;&#22815;&#35299;&#20915;&#28857;&#20113;&#20013;&#30340;&#22122;&#22768;&#38382;&#39064;&#65292;&#38590;&#20197;&#24357;&#21512;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;&#30340;&#26694;&#26550;&#65288;RPMArt&#65289;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#22914;&#20309;&#20174;&#22024;&#26434;&#30340;&#28857;&#20113;&#20013;&#20272;&#35745;&#20851;&#33410;&#21442;&#25968;&#24182;&#25805;&#20316;&#20851;&#33410;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#20581;&#22766;&#20851;&#33410;&#32593;&#32476;&#65288;RoArtNet&#65289;&#65292;&#36890;&#36807;&#23616;&#37096;&#29305;&#24449;&#23398;&#20064;&#21644;&#28857;&#20803;&#32452;&#25237;&#31080;&#33021;&#22815;&#31283;&#20581;&#22320;&#39044;&#27979;&#20851;&#33410;&#21442;&#25968;&#21644;&#21487;&#20449;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#33410;&#24863;&#30693;&#20998;&#31867;&#26041;&#26696;&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16023v1 Announce Type: cross  Abstract: Articulated objects are commonly found in daily life. It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications. However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between simulation and reality, thus limiting the practical deployment in real-world scenarios. To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud. Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting. Moreover, we introduce an articulation-aware classification scheme to enhance its ability
&lt;/p&gt;</description></item><item><title>Tastle&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#26694;&#26550;&#65292;&#37319;&#29992;&#24694;&#24847;&#20869;&#23481;&#38544;&#34255;&#21644;&#20869;&#23384;&#37325;&#26500;&#20197;&#21450;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.08424</link><description>&lt;p&gt;
Tastle: &#20026;&#33258;&#21160;&#36234;&#29425;&#25915;&#20987;&#24178;&#25200;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tastle: Distract Large Language Models for Automatic Jailbreak Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08424
&lt;/p&gt;
&lt;p&gt;
Tastle&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#26694;&#26550;&#65292;&#37319;&#29992;&#24694;&#24847;&#20869;&#23481;&#38544;&#34255;&#21644;&#20869;&#23384;&#37325;&#26500;&#20197;&#21450;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#22312;LLMs&#20844;&#24320;&#21457;&#24067;&#20043;&#21069;&#65292;&#20154;&#20204;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#23558;&#23427;&#20204;&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#23545;&#40784;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#20445;&#23427;&#20204;&#30340;&#26377;&#30410;&#24615;&#12289;&#35802;&#23454;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#32463;&#36807;&#32454;&#33268;&#23545;&#40784;&#30340;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25805;&#32437;&#65292;&#22914;&#36234;&#29425;&#65292;&#23548;&#33268;&#24847;&#22806;&#30340;&#34892;&#20026;&#12290;&#36234;&#29425;&#26159;&#26377;&#24847;&#24320;&#21457;&#24694;&#24847;&#25552;&#31034;&#65292;&#20174;LLM&#23433;&#20840;&#38480;&#21046;&#20013;&#36867;&#33073;&#20197;&#29983;&#25104;&#26410;&#32463;&#23457;&#26597;&#30340;&#26377;&#23475;&#20869;&#23481;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36234;&#29425;&#26041;&#27861;&#26469;&#23545;LLMs&#36827;&#34892;&#32418;&#38431;&#25915;&#20987;&#65292;&#20294;&#23427;&#20204;&#22312;&#25928;&#26524;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#36935;&#21040;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tastle&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;LLMs&#36827;&#34892;&#32418;&#38431;&#25915;&#20987;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#24694;&#24847;&#20869;&#23481;&#38544;&#34255;&#21644;&#20869;&#23384;&#37325;&#26500;&#65292;&#24182;&#32467;&#21512;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#26469;&#36234;&#29425;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08424v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved significant advances in recent days. Extensive efforts have been made before the public release of LLMs to align their behaviors with human values. The primary goal of alignment is to ensure their helpfulness, honesty and harmlessness. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. The jailbreak is to intentionally develop a malicious prompt that escapes from the LLM security restrictions to produce uncensored detrimental contents. Previous works explore different jailbreak methods for red teaming LLMs, yet they encounter challenges regarding to effectiveness and scalability. In this work, we propose Tastle, a novel black-box jailbreak framework for automated red teaming of LLMs. We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, mo
&lt;/p&gt;</description></item><item><title>SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06534</link><description>&lt;p&gt;
SARDet-100K: &#38754;&#21521;&#22823;&#35268;&#27169;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06534
&lt;/p&gt;
&lt;p&gt;
SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#29289;&#20307;&#26816;&#27979;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#19981;&#21487;&#26367;&#20195;&#30340;&#20840;&#22825;&#20505;&#25104;&#20687;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#20027;&#35201;&#21253;&#21547; &lt;2K &#24352;&#22270;&#20687;&#65292;&#19988;&#20165;&#21253;&#21547;&#21333;&#31867;&#21035;&#29289;&#20307;&#65289;&#21644;&#28304;&#20195;&#30721;&#19981;&#21487;&#35775;&#38382;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598; SARDet-100K &#32467;&#26524;&#26159;&#23545; 10 &#20010;&#29616;&#26377; SAR &#26816;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#20837;&#35843;&#30740;&#12289;&#25910;&#38598;&#21644;&#26631;&#20934;&#21270;&#30340;&#20135;&#29289;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SARDet-100K &#26159;&#26377;&#21490;&#20197;&#26469;&#31532;&#19968;&#20010;&#36798;&#21040; COCO &#27700;&#24179;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035; SAR &#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#20973;&#20511;&#36825;&#19968;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#24182;&#25581;&#31034;&#20102; SAR &#29289;&#20307;&#26816;&#27979;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06534v1 Announce Type: cross  Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising &lt;2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#27169;&#22359;&#65288;APM&#65289;&#65292;&#20026;SAM&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04164</link><description>&lt;p&gt;
ProMISe: &#20351;&#29992;SAM&#36827;&#34892;&#21487;&#25552;&#31034;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ProMISe: Promptable Medical Image Segmentation using SAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#27169;&#22359;&#65288;APM&#65289;&#65292;&#20026;SAM&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25552;&#20986;&#20102;Segment Anything Model (SAM)&#30340;&#24314;&#35758;&#65292;&#23545;SAM&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;(MIS)&#30340;&#24494;&#35843;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SAM&#27169;&#22411;&#30340;&#35268;&#27169;&#36739;&#22823;&#65292;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#22522;&#20110;&#24494;&#35843;&#30340;&#31574;&#30053;&#25104;&#26412;&#39640;&#65292;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#12289;&#29305;&#24449;&#25439;&#20260;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#23558;SAM&#36716;&#31227;&#21040;&#29305;&#23450;&#39046;&#22495;MIS&#30340;&#26041;&#27861;&#31105;&#29992;&#20102;&#27169;&#22411;&#30340;&#25552;&#31034;&#33021;&#21147;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#20351;&#29992;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#27169;&#22359;&#65288;APM&#65289;&#65292;&#20026;SAM&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#22495;&#20013;&#25552;&#20379;&#20102;&#20855;&#26377;&#27431;&#20960;&#37324;&#24503;&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#26679;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;SAM&#22312;MIS&#20013;&#38750;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#37327;&#27169;&#24335;&#31227;&#20301;&#65288;IPS&#65289;&#30340;&#26032;&#22411;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;SAM&#35843;&#25972;&#21040;&#29305;&#23450;&#21307;&#30103;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04164v1 Announce Type: cross  Abstract: With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for medical image segmentation (MIS) has become popular. However, due to the large size of the SAM model and the significant domain gap between natural and medical images, fine-tuning-based strategies are costly with potential risk of instability, feature damage and catastrophic forgetting. Furthermore, some methods of transferring SAM to a domain-specific MIS through fine-tuning strategies disable the model's prompting capability, severely limiting its utilization scenarios. In this paper, we propose an Auto-Prompting Module (APM), which provides SAM-based foundation model with Euclidean adaptive prompts in the target domain. Our experiments demonstrate that such adaptive prompts significantly improve SAM's non-fine-tuned performance in MIS. In addition, we propose a novel non-invasive method called Incremental Pattern Shifting (IPS) to adapt SAM to specific medica
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;Distributional Dispreference Optimization (D$^2$O)&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#31867;&#27491;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#40784;&#65292;&#20943;&#23569;&#20102;&#26377;&#23475;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2403.03419</link><description>&lt;p&gt;
&#21542;&#23450;&#21542;&#23450;&#65306;&#36890;&#36807;&#20998;&#24067;&#24335;&#21453;&#21916;&#22909;&#20248;&#21270;&#23454;&#29616;&#23545;&#40784;&#32780;&#26080;&#38656;&#20154;&#31867;&#27491;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03419
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;Distributional Dispreference Optimization (D$^2$O)&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#31867;&#27491;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#40784;&#65292;&#20943;&#23569;&#20102;&#26377;&#23475;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#35282;&#33394;&#65292;&#20294;&#20063;&#21487;&#33021;&#23384;&#22312;&#20256;&#25773;&#19981;&#36947;&#24503;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#23545;&#40784;&#25216;&#26415;&#34987;&#24341;&#20837;&#20197;&#24341;&#23548;LLM&#26397;&#30528;&#20154;&#31867;&#20559;&#22909;&#26041;&#21521;&#21457;&#23637;&#65292;&#24182;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#27491;&#36127;&#35757;&#32451;&#23545;&#65292;&#21463;&#21040;&#22024;&#26434;&#26631;&#31614;&#21644;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#21709;&#24212;&#25968;&#25454;&#20043;&#38388;&#30340;&#36793;&#32536;&#21306;&#21035;&#30340;&#22256;&#25200;&#12290;&#37492;&#20110;&#26368;&#36817;LLM&#22312;&#29983;&#25104;&#26377;&#29992;&#21709;&#24212;&#26041;&#38754;&#30340;&#39640;&#27700;&#24179;&#65292;&#26412;&#25991;&#23558;&#30740;&#31350;&#37325;&#28857;&#36716;&#21521;&#19968;&#20010;&#26032;&#30340;&#26041;&#21521;&#65306;&#20165;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#36127;&#26679;&#26412;&#26469;&#23454;&#29616;&#23545;&#40784;&#65292;&#20445;&#30041;&#26377;&#29992;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#26377;&#23475;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#21453;&#21916;&#22909;&#20248;&#21270;&#65288;D$^2$O&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#30340;&#21709;&#24212;&#19982;&#38750;&#39318;&#36873;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#25490;&#38500;&#26377;&#23475;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03419v1 Announce Type: cross  Abstract: Large language models (LLMs) have revolutionized the role of AI, yet also pose potential risks of propagating unethical content. Alignment technologies have been introduced to steer LLMs towards human preference, gaining increasing attention. Despite notable breakthroughs in this direction, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data. Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research focus: achieving alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness. For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between the generated responses and the dispreferred ones to effectively eschew harmful information. We theoretically demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CARE CA&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#26174;&#24335;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#12289;&#20197;&#21450;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18139</link><description>&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#27491;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Cause and Effect: Can Large Language Models Truly Understand Causality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CARE CA&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#26174;&#24335;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#12289;&#20197;&#21450;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#35299;&#35835;&#21644;&#35299;&#37322;&#35821;&#35328;&#25152;&#28041;&#21450;&#30340;&#22797;&#26434;&#22240;&#26524;&#20851;&#31995;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#26126;&#30830;&#25110;&#38544;&#21547;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#28982;&#32780;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#23558;&#20004;&#32773;&#32467;&#21512;&#36215;&#26469;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#22240;&#26524;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20855;&#26377;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#22686;&#24378;&#65288;CARE CA&#65289;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#22240;&#26524;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558; ConceptNet &#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#20013;&#30340;&#26126;&#30830;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#20197;&#21450;&#36890;&#36807;LLMs&#36827;&#34892;&#30340;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#19968;&#23618;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#19968;&#27493;&#31361;&#20986;LLMs&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;ConceptNet &#20013;&#30340;&#30693;&#35782;&#25552;&#39640;&#20102;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18139v1 Announce Type: cross  Abstract: With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Discussion&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#30340;&#36136;&#37327;</title><link>https://arxiv.org/abs/2402.16313</link><description>&lt;p&gt;
Chain-of-Discussion&#65306;&#22797;&#26434;&#35777;&#25454;&#38382;&#39064;&#22238;&#31572;&#30340;&#22810;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Discussion&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#38656;&#35201;&#27169;&#22411;&#25214;&#21040;&#36866;&#24403;&#30340;&#35777;&#25454;&#26469;&#24418;&#25104;&#21512;&#29702;&#12289;&#20840;&#38754;&#21644;&#26377;&#24110;&#21161;&#30340;&#31572;&#26696;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#36824;&#38656;&#35201;&#21442;&#19982;&#23545;&#19982;&#38382;&#39064;&#23494;&#20999;&#30456;&#20851;&#30340;&#28508;&#22312;&#22330;&#26223;&#36827;&#34892;&#28145;&#20837;&#35752;&#35770;&#12290;&#22312;&#26816;&#32034;&#27169;&#22359;&#30340;&#22686;&#24378;&#19979;&#65292;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#33021;&#22815;&#20135;&#29983;&#19968;&#33268;&#30340;&#31572;&#26696;&#65292;&#20294;&#22312;&#21487;&#38752;&#35777;&#25454;&#36873;&#25321;&#21644;&#28145;&#20837;&#38382;&#39064;&#20998;&#26512;&#26041;&#38754;&#20173;&#19981;&#22815;&#29702;&#24819;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Chain-of-Discussion&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#24320;&#28304;LLMs&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20026;&#24320;&#25918;&#24335;QA&#25552;&#20379;&#26356;&#27491;&#30830;&#12289;&#26356;&#20840;&#38754;&#30340;&#31572;&#26696;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20010;&#20307;&#19978;&#36824;&#19981;&#22815;&#24378;&#22823;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22810;&#20010;LLMs&#20043;&#38388;&#30340;&#35752;&#35770;&#23545;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;\url{https://github.com/kobaya}&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16313v1 Announce Type: cross  Abstract: Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \textbf{more correct} and \textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We release our data and code at \url{https://github.com/kobaya
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11161</link><description>&lt;p&gt;
PANDA&#65288;Pedantic ANswer-correctness Determination and Adjudication&#65289;&#65306;&#25913;&#36827;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;QA&#65289;&#21482;&#26377;&#22312;&#25105;&#20204;&#30693;&#36947;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#26102;&#25165;&#33021;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#26377;&#36259;&#30340;QA&#31034;&#20363;&#65292;&#24403;&#21069;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#65288;AC&#65289;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20887;&#38271;&#12289;&#33258;&#30001;&#26684;&#24335;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#32570;&#20047;&#25968;&#25454;&#21644;&#27169;&#22411;&#36807;&#22823;&#12290;&#22522;&#20110;LLM&#30340;&#35780;&#20998;&#22120;&#19982;&#20154;&#31867;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20294;&#36825;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#20165;&#22312;&#26377;&#38480;&#30340;QA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#28165;&#26224;&#30340;&#25351;&#21335;&#26469;&#35780;&#20272;&#20174;&#20154;&#31867;QA&#27604;&#36187;&#20013;&#37319;&#32435;&#30340;&#26426;&#22120;QA&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#30830;&#23450;&#21644;&#35009;&#20915;&#65288;Precise ANswer correctness Determination and Adjudication&#65292;PANDA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23567;&#24039;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;AC&#20998;&#31867;&#22120;&#65288;812 KB&#65289;&#65292;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11161v1 Announce Type: cross  Abstract: Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big. LLM based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing clear guidelines for evaluating machine QA adopted from human QA contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#36741;&#21161;&#25439;&#22833;&#20248;&#21270;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26426;&#22120;&#22270;&#20687;&#32534;&#30721;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#30340;&#36895;&#29575;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.08267</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#32534;&#30721;&#22120;&#21644;&#36741;&#21161;&#25439;&#22833;&#25913;&#36827;&#26426;&#22120;&#22270;&#20687;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08267
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#36741;&#21161;&#25439;&#22833;&#20248;&#21270;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26426;&#22120;&#22270;&#20687;&#32534;&#30721;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#30340;&#36895;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#22270;&#20687;&#32534;&#30721;&#65288;ICM&#65289;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#27169;&#22411;&#32780;&#19981;&#26159;&#20154;&#30524;&#35270;&#35273;&#26469;&#21387;&#32553;&#22270;&#20687;&#20197;&#20379;&#26426;&#22120;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#22312;ICM&#20013;&#65292;&#32534;&#30721;&#22120;&#35782;&#21035;&#21644;&#21387;&#32553;&#23545;&#20110;&#26426;&#22120;&#35782;&#21035;&#20219;&#21153;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23398;&#20064;&#22411;ICM&#26377;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#22522;&#20110;&#20219;&#21153;&#25439;&#22833;&#30340;&#21387;&#32553;&#27169;&#22411;&#20248;&#21270;&#21644;&#22522;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#30340;&#27604;&#29305;&#20998;&#37197;&#12290;&#36825;&#20123;&#26041;&#27861;&#20026;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#35782;&#21035;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#35782;&#21035;&#27169;&#22411;&#24456;&#28145;&#26102;&#65292;&#20351;&#29992;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#22522;&#20110;ROI&#30340;&#26041;&#27861;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#36890;&#24120;&#20250;&#22686;&#21152;&#39069;&#22806;&#24320;&#38144;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#22411;ICM&#27169;&#22411;&#30340;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#32534;&#30721;&#22120;&#24212;&#29992;&#36741;&#21161;&#25439;&#22833;&#26469;&#25552;&#39640;&#20854;&#35782;&#21035;&#33021;&#21147;&#21644;&#36895;&#29575;-&#22833;&#30495;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;27.7%&#21644;20.3%&#30340;Bjontegaard Delta&#36895;&#29575;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image coding for machines (ICM) aims to compress images for machine analysis using recognition models rather than human vision. Hence, in ICM, it is important for the encoder to recognize and compress the information necessary for the machine recognition task. There are two main approaches in learned ICM; optimization of the compression model based on task loss, and Region of Interest (ROI) based bit allocation. These approaches provide the encoder with the recognition capability. However, optimization with task loss becomes difficult when the recognition model is deep, and ROI-based methods often involve extra overhead during evaluation. In this study, we propose a novel training method for learned ICM models that applies auxiliary loss to the encoder to improve its recognition capability and rate-distortion performance. Our method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in object detection and semantic segmentation tasks, compared to the conventional training 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#27169;&#25311;&#25919;&#27835;&#36777;&#35770;&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#65292;&#23613;&#31649;&#34987;&#25351;&#23450;&#20174;&#29305;&#23450;&#30340;&#25919;&#27835;&#35266;&#28857;&#36827;&#34892;&#36777;&#35770;&#65292;LLMs&#20195;&#29702;&#26426;&#26500;&#20542;&#21521;&#20110;&#36981;&#24490;&#27169;&#22411;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36890;&#36807;&#33258;&#21160;&#33258;&#25105;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04049</link><description>&lt;p&gt;
&#35770;&#35821;&#26009;&#24211;&#27169;&#25311;&#36777;&#35770;&#20013;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Systematic Biases in LLM Simulations of Debates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#27169;&#25311;&#25919;&#27835;&#36777;&#35770;&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#65292;&#23613;&#31649;&#34987;&#25351;&#23450;&#20174;&#29305;&#23450;&#30340;&#25919;&#27835;&#35266;&#28857;&#36827;&#34892;&#36777;&#35770;&#65292;LLMs&#20195;&#29702;&#26426;&#26500;&#20542;&#21521;&#20110;&#36981;&#24490;&#27169;&#22411;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36890;&#36807;&#33258;&#21160;&#33258;&#25105;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20026;&#26500;&#24314;&#33021;&#22815;&#20934;&#30830;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#35745;&#31639;&#26426;&#27169;&#25311;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;LLMs&#26159;&#22797;&#26434;&#30340;&#32479;&#35745;&#23398;&#20064;&#22120;&#65292;&#27809;&#26377;&#30452;&#25509;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#20351;&#20854;&#23481;&#26131;&#20986;&#29616;&#24847;&#22806;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;LLMs&#22312;&#27169;&#25311;&#20154;&#31867;&#20114;&#21160;&#20013;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#20851;&#27880;LLMs&#22312;&#27169;&#25311;&#25919;&#27835;&#36777;&#35770;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#25351;&#23450;&#20174;&#29305;&#23450;&#30340;&#25919;&#27835;&#35266;&#28857;&#36827;&#34892;&#36777;&#35770;&#65292;LLMs&#20195;&#29702;&#26426;&#26500;&#20542;&#21521;&#20110;&#36981;&#24490;&#27169;&#22411;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#31181;&#20542;&#21521;&#23548;&#33268;&#20986;&#29616;&#34892;&#20026;&#27169;&#24335;&#65292;&#20284;&#20046;&#20559;&#31163;&#20102;&#20154;&#31867;&#20043;&#38388;&#24050;&#32463;&#30830;&#31435;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#33258;&#25105;&#20248;&#21270;&#26041;&#27861;&#21152;&#24378;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#25805;&#32437;LLMs&#20869;&#37096;&#30340;&#20559;&#35265;&#65292;&#24182;&#35777;&#26126;&#20195;&#29702;&#38543;&#21518;&#19982;&#36825;&#20123;&#35843;&#25972;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#22686;&#24378;&#29289;&#20214;&#23398;&#20064;&#22120;&#65288;MENOL&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23567;&#24050;&#30693;&#31867;&#21644;&#26410;&#30693;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#24341;&#20837;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26032;&#31867;&#21035;&#30340;&#21484;&#22238;&#29575;&#65292;&#24182;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.02026</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#22686;&#24378;&#29289;&#20214;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multimodal-Enhanced Objectness Learner for Corner Case Detection in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#22686;&#24378;&#29289;&#20214;&#23398;&#20064;&#22120;&#65288;MENOL&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23567;&#24050;&#30693;&#31867;&#21644;&#26410;&#30693;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#24341;&#20837;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26032;&#31867;&#21035;&#30340;&#21484;&#22238;&#29575;&#65292;&#24182;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#20307;&#26816;&#27979;&#30340;&#20808;&#21069;&#24037;&#20316;&#20013;&#65292;&#23553;&#38381;&#22330;&#26223;&#19979;&#30340;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#20294;&#22312;&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#24182;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#20854;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#19990;&#30028;&#38382;&#39064;&#26159;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#22312;&#36825;&#20123;&#26696;&#20363;&#20013;&#34920;&#29616;&#22256;&#38590;&#65292;&#36807;&#24230;&#20381;&#36182;&#35270;&#35273;&#22806;&#35266;&#65292;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20943;&#23567;&#24050;&#30693;&#31867;&#21644;&#26410;&#30693;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#24341;&#20837;&#22810;&#27169;&#24577;&#22686;&#24378;&#30340;&#29289;&#20214;&#23398;&#20064;&#22120;&#30340;&#27010;&#24565;&#12290;&#20511;&#21161;&#35270;&#35273;&#20013;&#24515;&#21644;&#22270;&#20687;-&#25991;&#26412;&#20004;&#31181;&#24418;&#24335;&#65292;&#25105;&#20204;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#23558;&#29289;&#20214;&#23398;&#20064;&#22120;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#31867;&#21035;&#24863;&#30693;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#29992;&#20110;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#22686;&#24378;&#29289;&#20214;&#23398;&#20064;&#22120;&#65288;MENOL&#65289;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26032;&#31867;&#21035;&#30340;&#21484;&#22238;&#29575;&#65292;&#24182;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#20165;&#20351;&#29992;5100&#20010;&#26631;&#31614;&#35757;&#32451;&#22270;&#20687;&#30340;CODA-val&#25968;&#25454;&#38598;&#19978;&#65292;MENOL&#23454;&#29616;&#20102;76.6%&#30340;mAR-corner&#21644;79.8%&#30340;mAR-agnostic&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous works on object detection have achieved high accuracy in closed-set scenarios, but their performance in open-world scenarios is not satisfactory. One of the challenging open-world problems is corner case detection in autonomous driving. Existing detectors struggle with these cases, relying heavily on visual appearance and exhibiting poor generalization ability. In this paper, we propose a solution by reducing the discrepancy between known and unknown classes and introduce a multimodal-enhanced objectness notion learner. Leveraging both vision-centric and image-text modalities, our semi-supervised learning framework imparts objectness knowledge to the student model, enabling class-aware detection. Our approach, Multimodal-Enhanced Objectness Learner (MENOL) for Corner Case Detection, significantly improves recall for novel classes with lower training costs. By achieving a 76.6% mAR-corner and 79.8% mAR-agnostic on the CODA-val dataset with just 5100 labeled training images, MEN
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;SLEEC&#65288;&#31038;&#20250;&#12289;&#27861;&#24459;&#12289;&#20262;&#29702;&#12289;&#31227;&#24773;&#12289;&#25991;&#21270;&#65289;&#35268;&#21017;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#25512;&#21160;AI&#31995;&#32479;&#36981;&#23432;&#20154;&#31867;&#32972;&#26223;&#30456;&#20851;&#35268;&#21017;&#30340;&#21046;&#23450;&#12289;&#39564;&#35777;&#21644;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2312.09699</link><description>&lt;p&gt;
&#31038;&#20250;&#12289;&#27861;&#24459;&#12289;&#20262;&#29702;&#12289;&#31227;&#24773;&#21644;&#25991;&#21270;&#35268;&#21017;&#65306;&#32534;&#21046;&#19982;&#25512;&#29702;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Social, Legal, Ethical, Empathetic, and Cultural Rules: Compilation and Reasoning (Extended Version)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;SLEEC&#65288;&#31038;&#20250;&#12289;&#27861;&#24459;&#12289;&#20262;&#29702;&#12289;&#31227;&#24773;&#12289;&#25991;&#21270;&#65289;&#35268;&#21017;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#25512;&#21160;AI&#31995;&#32479;&#36981;&#23432;&#20154;&#31867;&#32972;&#26223;&#30456;&#20851;&#35268;&#21017;&#30340;&#21046;&#23450;&#12289;&#39564;&#35777;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#22522;&#30784;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#23835;&#36215;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#30340;&#25285;&#24551;&#65292;&#36825;&#20123;&#24433;&#21709;&#26469;&#33258;&#20110;&#23427;&#20204;&#30340;&#34892;&#20026;&#25110;&#20915;&#31574;&#12290;&#36825;&#20123;&#31995;&#32479;&#24517;&#39035;&#34987;&#35774;&#35745;&#20026;&#36981;&#23432;&#23427;&#20204;&#23558;&#36816;&#20316;&#30340;&#20154;&#31867;&#32972;&#26223;&#12290;Townsend&#31561;&#20154;&#65288;2022&#65289;&#24341;&#20837;&#20102;SLEEC&#65288;&#31038;&#20250;&#12289;&#27861;&#24459;&#12289;&#20262;&#29702;&#12289;&#31227;&#24773;&#25110;&#25991;&#21270;&#65289;&#35268;&#21017;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20419;&#36827;AI&#22522;&#30784;&#21644;&#33258;&#20027;&#31995;&#32479;&#24212;&#36981;&#23432;&#35268;&#21017;&#30340;&#21046;&#23450;&#12289;&#39564;&#35777;&#21644;&#25191;&#34892;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#35770;&#26469;&#25581;&#31034;&#36825;&#20123;&#35268;&#21017;&#65292;&#35753;&#21746;&#23398;&#23478;&#12289;&#24459;&#24072;&#12289;&#39046;&#22495;&#19987;&#23478;&#21644;&#20854;&#20182;&#20154;&#29992;&#33258;&#28982;&#35821;&#35328;&#26469;&#21046;&#23450;&#36825;&#20123;&#35268;&#21017;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#35268;&#21017;&#22312;AI&#31995;&#32479;&#20013;&#26377;&#25928;&#20351;&#29992;&#65292;&#38656;&#35201;&#23558;&#36825;&#20123;&#35268;&#21017;&#31995;&#32479;&#22320;&#32763;&#35793;&#25104;&#25903;&#25345;&#33258;&#21160;&#25512;&#29702;&#30340;&#24418;&#24335;&#35821;&#35328;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;SLEEC&#35268;&#21017;&#27169;&#24335;&#36827;&#34892;&#20102;&#35821;&#35328;&#20998;&#26512;&#65292;&#36825;&#20351;&#24471;&#23558;SLEEC&#35268;&#21017;&#36716;&#25442;&#25104;c&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09699v2 Announce Type: replace  Abstract: The rise of AI-based and autonomous systems is raising concerns and apprehension due to potential negative repercussions stemming from their behavior or decisions. These systems must be designed to comply with the human contexts in which they will operate. To this extent, Townsend et al. (2022) introduce the concept of SLEEC (social, legal, ethical, empathetic, or cultural) rules that aim to facilitate the formulation, verification, and enforcement of the rules AI-based and autonomous systems should obey. They lay out a methodology to elicit them and to let philosophers, lawyers, domain experts, and others to formulate them in natural language. To enable their effective use in AI systems, it is necessary to translate these rules systematically into a formal language that supports automated reasoning. In this study, we first conduct a linguistic analysis of the SLEEC rules pattern, which justifies the translation of SLEEC rules into c
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.14142</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#32479;&#19968;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14142
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411; (CBM)&#65292;&#22312;&#20026;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26368;&#32456;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#32416;&#27491;&#19968;&#20010;&#39044;&#27979;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33016;&#37096;&#8221;&#65289;&#26080;&#27861;&#24110;&#21161;&#32416;&#27491;&#39640;&#24230;&#30456;&#20851;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33145;&#37096;&#8221;&#65289;&#65292;&#23548;&#33268;&#26368;&#32456;&#20934;&#30830;&#29575;&#19981;&#29702;&#24819;&#65307;&#23427;&#20204;&#26080;&#27861;&#33258;&#28982;&#22320;&#37327;&#21270;&#19981;&#21516;&#27010;&#24565;&#21644;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#23545;&#20110;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#26631;&#31614;&#8220;Kentucky Warbler&#8221;&#21644;&#27010;&#24565;&#8220;&#40657;&#33394;&#22068;&#24052;&#8221;&#30340;&#22270;&#20687;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#21478;&#19968;&#20010;&#27010;&#24565;&#8220;&#40657;&#33394;&#20896;&#8221;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#26356;&#28145;&#23618;&#27425;&#30340;&#27934;&#23519;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;Energy-based Concept Bottleneck Models&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
&lt;/p&gt;</description></item><item><title>TAT-LLM&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#32780;&#20026;&#20102;&#24212;&#23545;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TAT-LLM&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#36739;&#23567;LLM&#12290;</title><link>http://arxiv.org/abs/2401.13223</link><description>&lt;p&gt;
TAT-LLM: &#19968;&#31181;&#38024;&#23545;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data. (arXiv:2401.13223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13223
&lt;/p&gt;
&lt;p&gt;
TAT-LLM&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#32780;&#20026;&#20102;&#24212;&#23545;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TAT-LLM&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#36739;&#23567;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#38382;&#31572;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;Web&#19978;&#38750;&#24120;&#24120;&#35265;&#65288;&#22914;SEC&#25991;&#20214;&#65289;&#65292;&#36890;&#24120;&#38656;&#35201;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#22810;&#27493;&#39588;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38754;&#21521;&#34920;&#26684;&#21644;&#25991;&#26412;&#38382;&#31572;&#30340;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#25277;&#35937;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#19977;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#24182;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20221;&#25351;&#20196;&#26469;&#23454;&#20363;&#21270;&#35813;&#27969;&#27700;&#32447;&#24182;&#39564;&#35777;GPT-4&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#20687;GPT-4&#36825;&#26679;&#30340;&#22312;&#32447;LLM&#23384;&#22312;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#21508;&#31181;&#25361;&#25112;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#24320;&#21457;&#36739;&#23567;&#30340;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29616;&#26377;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#33258;&#21160;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;LLaMA 2&#36827;&#34892;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;TAT-LLM&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we address question answering (QA) over a hybrid of tabular and textual data that are very common content on the Web (e.g. SEC filings), where discrete reasoning capabilities are often required. Recently, large language models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning capabilities. We then consider harnessing the amazing power of LLMs to solve our task. We abstract a Step-wise Pipeline for tabular and textual QA, which consists of three key steps, including Extractor, Reasoner and Executor, and initially design an instruction to instantiate the pipeline and validate that GPT-4 outperforms all existing methods. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We develop a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.09769</link><description>&lt;p&gt;
&#36208;&#21521;&#24322;&#36136;&#22270;&#23398;&#20064;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#24322;&#36136;&#22270;&#65292;&#20854;&#20013;&#36830;&#25509;&#30340;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#25110;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#20204;&#20063;&#22312;&#19981;&#26029;&#21162;&#21147;&#25512;&#36827;&#20174;&#24322;&#36136;&#22270;&#20013;&#23398;&#20064;&#12290;&#34429;&#28982;&#26377;&#20851;&#35813;&#20027;&#39064;&#30340;&#35843;&#26597;&#23384;&#22312;&#65292;&#20294;&#23427;&#20204;&#21482;&#20851;&#27880;&#20110;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#32780;&#24573;&#30053;&#20102;&#24322;&#36136;&#22270;&#23398;&#20064;&#30340;&#20854;&#20182;&#23376;&#20027;&#39064;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;180&#22810;&#31687;&#35770;&#25991;&#65292;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#23618;&#27425;&#20998;&#31867;&#27861;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#65292;&#21253;&#25324;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01537</link><description>&lt;p&gt;
&#27450;&#39575;&#30340;&#33402;&#26415;&#65306;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#30340;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01537
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#34892;&#19994;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#39046;&#22495;&#27491;&#22312;&#32463;&#21382;&#22686;&#38271;&#30340;&#23454;&#26045;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;AI&#38450;&#24481;&#26426;&#21046;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26469;&#33258;&#19981;&#23436;&#20840;&#21487;&#20449;&#30340;&#31532;&#19977;&#26041;&#25552;&#20379;&#21830;&#30340;&#28508;&#22312;&#38544;&#34109;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21548;&#35273;&#21518;&#38376;&#21487;&#33021;&#20351;&#29992;&#26576;&#20123;&#20462;&#25913;&#20316;&#20026;&#20854;&#21551;&#21160;&#26426;&#21046;&#12290;DynamicTrigger&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#34987;&#24341;&#20837;&#65292;&#29992;&#20110;&#36827;&#34892;&#20351;&#29992;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#26469;&#30830;&#20445;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#27874;&#21160;&#30340;&#20449;&#21495;&#37319;&#26679;&#29575;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#22768;&#38899;&#35302;&#21457;&#22120;&#65288;&#27604;&#22914;&#25293;&#25163;&#22768;&#65289;&#23545;&#35828;&#35805;&#32773;&#36523;&#20221;&#36827;&#34892;&#25513;&#30422;&#65292;&#21487;&#20197;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65288;ASR&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#65292;DynamicTrigger&#22312;&#38544;&#34109;&#25915;&#20987;&#20013;&#26082;&#26377;&#25928;&#21448;&#38544;&#34109;&#65292;&#24182;&#22312;&#25915;&#20987;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;&#65292;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.16713</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Conditional Imputation for Healthcare Time-series. (arXiv:2312.16713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;&#65292;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#21307;&#30103;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#26465;&#20214;&#33258;&#27880;&#24847;&#21147;&#25554;&#34917;&#65288;CSAI&#65289;&#27169;&#22411;&#20197;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#20026;&#22522;&#30784;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32454;&#33410;&#30340;&#26465;&#20214;&#38544;&#34255;&#29366;&#24577;&#21021;&#22987;&#21270;&#26041;&#24335;&#12290;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#25554;&#34917;&#25216;&#26415;&#19981;&#21516;&#65292;&#23427;&#29305;&#21035;&#38024;&#23545;&#21307;&#30103;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;CSAI&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#32570;&#22833;&#25968;&#25454;&#30340;&#19981;&#21516;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel approach to addressing the challenge of missing data in multivariate time series, with a particular focus on the complexities of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model, grounded in a transformer-based framework, introduces a conditional hidden state initialization tailored to the intricacies of medical time series data. This methodology diverges from traditional imputation techniques by specifically targeting the imbalance in missing data distribution, a crucial aspect often overlooked in healthcare datasets. By integrating advanced knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to the distinct patterns of missing data in Electronic Health Records (EHRs).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#26377;&#25928;&#35843;&#33410;&#20102;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06286</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#34892;&#20026;&#25233;&#21046;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Suppressing Overestimation in Q-Learning through Adversarial Behaviors. (arXiv:2310.06286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#26377;&#25928;&#35843;&#33410;&#20102;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#31216;&#20026;&#34394;&#25311;&#23545;&#25239;&#24615;Q&#23398;&#20064;&#65288;DAQ&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#35843;&#33410;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;&#36890;&#36807;&#34394;&#25311;&#29609;&#23478;&#65292;&#23398;&#20064;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#19968;&#20010;&#21452;&#20154;&#38646;&#21644;&#21338;&#24328;&#12290;&#25152;&#25552;&#20986;&#30340;DAQ&#23558;&#20960;&#31181;Q&#23398;&#20064;&#30340;&#21464;&#20307;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#25511;&#21046;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#20363;&#22914;maxmin Q&#23398;&#20064;&#21644;minmax Q&#23398;&#20064;&#65288;&#26412;&#25991;&#25552;&#20986;&#65289;&#12290;&#36890;&#36807;&#34394;&#25311;&#23545;&#25239;&#24615;&#34892;&#20026;&#65292;&#25152;&#25552;&#20986;&#30340;DAQ&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#29616;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#35843;&#25972;&#23545;&#25239;&#24615;Q&#23398;&#20064;&#65292;&#20174;&#32508;&#21512;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;DAQ&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#23454;&#35777;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;DAQ&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias thourgh dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments.
&lt;/p&gt;</description></item><item><title>STAMP&#26159;&#19968;&#31181;&#22522;&#20110;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#21644;&#21487;&#24494;&#20223;&#30495;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.01775</link><description>&lt;p&gt;
STAMP&#65306;&#36890;&#36807;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#21487;&#24494;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent. (arXiv:2310.01775v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01775
&lt;/p&gt;
&lt;p&gt;
STAMP&#26159;&#19968;&#31181;&#22522;&#20110;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#21644;&#21487;&#24494;&#20223;&#30495;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25805;&#20316;&#20219;&#21153;&#65292;&#22914;&#20351;&#29992;&#24037;&#20855;&#25110;&#35013;&#37197;&#38646;&#20214;&#65292;&#24448;&#24448;&#38656;&#35201;&#31526;&#21495;&#21644;&#20960;&#20309;&#25512;&#29702;&#12290;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#39640;&#32423;&#20219;&#21153;&#24207;&#21015;&#36827;&#34892;&#26641;&#25628;&#32034;&#24182;&#26816;&#26597;&#36816;&#21160;&#23398;&#21644;&#21160;&#21147;&#23398;&#21487;&#34892;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#34429;&#28982;&#24615;&#33021;&#33391;&#22909;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#65292;&#22240;&#20026;&#20854;&#26102;&#38388;&#22797;&#26434;&#24615;&#38543;&#21487;&#33021;&#21160;&#20316;&#21644;&#29289;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21482;&#33021;&#25214;&#21040;&#21333;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#21487;&#34892;&#30340;&#35745;&#21010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stein&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;STAMP&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#24182;&#34892;&#21270;&#21644;&#21487;&#24494;&#20223;&#30495;&#26469;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#35745;&#21010;&#12290;STAMP&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#30340;TAMP&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#20197;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#35299;&#20915;&#30340;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65292;&#19968;&#31181;&#27010;&#29575;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning for many manipulation tasks, such as using tools or assembling parts, often requires both symbolic and geometric reasoning. Task and Motion Planning (TAMP) algorithms typically solve these problems by conducting a tree search over high-level task sequences while checking for kinematic and dynamic feasibility. While performant, most existing algorithms are highly inefficient as their time complexity grows exponentially with the number of possible actions and objects. Additionally, they only find a single solution to problems in which many feasible plans may exist. To address these limitations, we propose a novel algorithm called Stein Task and Motion Planning (STAMP) that leverages parallelization and differentiable simulation to efficiently search for multiple diverse plans. STAMP relaxes discrete-and-continuous TAMP problems into continuous optimization problems that can be solved using variational inference. Our algorithm builds upon Stein Variational Gradient Descent, a gra
&lt;/p&gt;</description></item><item><title>CyberForce&#26159;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#21327;&#21516;&#31169;&#23494;&#22320;&#30830;&#23450;&#36866;&#21512;&#32531;&#35299;&#21508;&#31181;&#38646;&#26085;&#25915;&#20987;&#30340;MTD&#25216;&#26415;&#12290;&#23427;&#25972;&#21512;&#20102;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#25110;&#24809;&#32602;FRL agent&#36873;&#25321;&#30340;MTD&#26426;&#21046;&#26469;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05978</link><description>&lt;p&gt;
CyberForce: &#19968;&#20010;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#32531;&#35299;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation. (arXiv:2308.05978v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05978
&lt;/p&gt;
&lt;p&gt;
CyberForce&#26159;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#21327;&#21516;&#31169;&#23494;&#22320;&#30830;&#23450;&#36866;&#21512;&#32531;&#35299;&#21508;&#31181;&#38646;&#26085;&#25915;&#20987;&#30340;MTD&#25216;&#26415;&#12290;&#23427;&#25972;&#21512;&#20102;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#25110;&#24809;&#32602;FRL agent&#36873;&#25321;&#30340;MTD&#26426;&#21046;&#26469;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#32593;(IoT)&#33539;&#20363;&#30340;&#25193;&#23637;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20294;&#26159;&#23545;&#20110;IoT&#35774;&#22791;&#23545;&#24694;&#24847;&#36719;&#20214;&#20107;&#20214;&#30340;&#33030;&#24369;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;(MTD)&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22686;&#24378;IoT&#35774;&#22791;&#30340;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#26032;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#21644;&#20195;&#29702;&#20154;&#23398;&#20064;&#21644;&#36873;&#25321;&#26377;&#25928;&#30340;MTD&#25216;&#26415;&#25152;&#38656;&#30340;&#26102;&#38388;&#20351;&#24471;&#36825;&#31181;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;IoT&#22330;&#26223;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CyberForce&#65292;&#19968;&#20010;&#37319;&#29992;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;(FRL)&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#38598;&#20307;&#19988;&#20445;&#23494;&#22320;&#30830;&#23450;&#36866;&#21512;&#32531;&#35299;&#21508;&#31181;&#38646;&#26085;&#25915;&#20987;&#30340;MTD&#25216;&#26415;&#12290;CyberForce&#32467;&#21512;&#20102;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#22870;&#21169;&#25110;&#24809;&#32602;FRL agent&#36873;&#25321;&#30340;MTD&#26426;&#21046;&#12290;&#35813;&#26694;&#26550;&#22312;&#19968;&#20010;&#30001;&#21313;&#21488;&#30495;&#23454;IoT&#24179;&#21488;&#35774;&#22791;&#32452;&#25104;&#30340;&#32852;&#37030;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#20845;&#20010;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expansion of the Internet-of-Things (IoT) paradigm is inevitable, but vulnerabilities of IoT devices to malware incidents have become an increasing concern. Recent research has shown that the integration of Reinforcement Learning with Moving Target Defense (MTD) mechanisms can enhance cybersecurity in IoT devices. Nevertheless, the numerous new malware attacks and the time that agents take to learn and select effective MTD techniques make this approach impractical for real-world IoT scenarios. To tackle this issue, this work presents CyberForce, a framework that employs Federated Reinforcement Learning (FRL) to collectively and privately determine suitable MTD techniques for mitigating diverse zero-day attacks. CyberForce integrates device fingerprinting and anomaly detection to reward or penalize MTD mechanisms chosen by an FRL-based agent. The framework has been evaluated in a federation consisting of ten devices of a real IoT platform. A pool of experiments with six malware samp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00071</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#26041;&#27861;&#29992;&#20110;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20351;&#29992;&#20102;&#21253;&#21547;&#22266;&#26377;&#20559;&#35265;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#25345;&#32493;&#31995;&#32479;&#24615;&#27495;&#35270;&#65292;&#22240;&#27492;&#65292;&#23457;&#26597;&#21644;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23558;&#20844;&#24179;&#24615;&#25972;&#21512;&#21040;&#23427;&#20204;&#30340;&#21457;&#23637;&#20013;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20844;&#27491;&#21644;&#26080;&#20559;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Vicuna-13B-v1.3&#30340;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#35266;&#23519;&#21040;&#20174;13B&#21040;33B&#30340;&#35268;&#27169;&#25193;&#23637;&#20250;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#29702;&#21487;&#33021;&#26159;&#20351;LLMs&#22312;&#21051;&#26495;&#21360;&#35937;&#31561;&#39046;&#22495;&#20219;&#21153;&#19978;&#36229;&#36234;&#35268;&#27169;&#23450;&#24459;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#36873;&#23450;&#30340;&#25512;&#29702;&#36861;&#36394;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#31361;&#20986;&#26174;&#31034;&#20102;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RL+&#27169;&#22411;&#25511;&#21046;&#26694;&#26550;&#20197;&#24320;&#21457;&#20986;&#21487;&#20197;&#26377;&#25928;&#21487;&#38752;&#22320;&#23398;&#20064;&#30340;&#20581;&#22766;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#26377;&#38480;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#29983;&#25104;&#30340;&#25353;&#38656;&#21442;&#32771;&#36816;&#21160;&#20998;&#25955; RL &#36807;&#31243;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#24314;&#27169;&#31616;&#21270;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#22312;&#36275;&#24335; locomotion &#19978;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#21644;&#24378;&#20581;&#65292;&#33021;&#27867;&#21270;&#21442;&#32771;&#36816;&#21160;&#24182;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.17842</link><description>&lt;p&gt;
RL+&#27169;&#22411;&#25511;&#21046;&#65306;&#20351;&#29992;&#25353;&#38656;&#26368;&#20248;&#25511;&#21046;&#23398;&#20064;&#22810;&#21151;&#33021;&#36275;&#24335; locomotion
&lt;/p&gt;
&lt;p&gt;
RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion. (arXiv:2305.17842v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RL+&#27169;&#22411;&#25511;&#21046;&#26694;&#26550;&#20197;&#24320;&#21457;&#20986;&#21487;&#20197;&#26377;&#25928;&#21487;&#38752;&#22320;&#23398;&#20064;&#30340;&#20581;&#22766;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#26377;&#38480;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#29983;&#25104;&#30340;&#25353;&#38656;&#21442;&#32771;&#36816;&#21160;&#20998;&#25955; RL &#36807;&#31243;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#24314;&#27169;&#31616;&#21270;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#22312;&#36275;&#24335; locomotion &#19978;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#21644;&#24378;&#20581;&#65292;&#33021;&#27867;&#21270;&#21442;&#32771;&#36816;&#21160;&#24182;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#21644;&#24378;&#20581;&#30340;&#36275;&#24335; locomotion&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#26377;&#38480;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#29983;&#25104;&#30340;&#25353;&#38656;&#21442;&#32771;&#36816;&#21160;&#26469;&#22686;&#24378; RL &#35757;&#32451;&#36807;&#31243;&#65292;&#35206;&#30422;&#20102;&#24191;&#27867;&#30340;&#36895;&#24230;&#21644;&#27493;&#24577;&#12290;&#36825;&#20123;&#21442;&#32771;&#36816;&#21160;&#20316;&#20026; RL &#31574;&#30053;&#27169;&#20223;&#30340;&#30446;&#26631;&#65292;&#23548;&#33268;&#24320;&#21457;&#20986;&#21487;&#26377;&#25928;&#21487;&#38752;&#22320;&#23398;&#20064;&#30340;&#20581;&#22766;&#25511;&#21046;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32771;&#34385;&#20840;&#36523;&#21160;&#21147;&#23398;&#65292;RL &#20811;&#26381;&#20102;&#24314;&#27169;&#31616;&#21270;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#30828;&#20214;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; RL &#35757;&#32451;&#36807;&#31243;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#30340;&#24378;&#20581;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#27867;&#21270;&#21442;&#32771;&#36816;&#21160;&#21644;&#22788;&#29702;&#21487;&#33021;&#23545;&#31616;&#21270;&#27169;&#22411;&#26500;&#25104;&#25361;&#25112;&#30340;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#20102; RL &#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This letter presents a control framework that combines model-based optimal control and reinforcement learning (RL) to achieve versatile and robust legged locomotion. Our approach enhances the RL training process by incorporating on-demand reference motions generated through finite-horizon optimal control, covering a broad range of velocities and gaits. These reference motions serve as targets for the RL policy to imitate, resulting in the development of robust control policies that can be learned efficiently and reliably. Moreover, by considering whole-body dynamics, RL overcomes the inherent limitations of modelling simplifications. Through simulation and hardware experiments, we demonstrate the robustness and controllability of the RL training process within our framework. Furthermore, our method demonstrates the ability to generalize reference motions and handle more complex locomotion tasks that may pose challenges for the simplified model, leveraging the flexibility of RL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Bug&#23450;&#20301;&#26041;&#27861;RLocator&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;Bug&#23450;&#20301;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05586</link><description>&lt;p&gt;
RLocator: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;Bug&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
RLocator: Reinforcement Learning for Bug Localization. (arXiv:2305.05586v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Bug&#23450;&#20301;&#26041;&#27861;RLocator&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;Bug&#23450;&#20301;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#32773;&#22312;&#20182;&#20204;&#30340;&#39033;&#30446;&#20013;&#33457;&#36153;&#20102;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#20462;&#22797;Bugs&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;Bug&#23450;&#20301;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20123;&#28304;&#20195;&#30721;&#25991;&#20214;&#21487;&#33021;&#26159;&#36127;&#36131;&#29305;&#23450;Bug&#30340;&#28304;&#22836;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;Bug&#23450;&#20301;&#12290;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#30452;&#25509;&#20248;&#21270;&#35780;&#20272;&#25351;&#26631;&#12290;&#30456;&#21453;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20250;&#23545;&#26816;&#32034;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Bug&#23450;&#20301;&#26041;&#27861;RLocator&#12290;&#25105;&#20204;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26469;&#20248;&#21270;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23545;Bug&#23450;&#20301;&#38382;&#39064;&#36827;&#34892;&#20844;&#24335;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35813;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#20845;&#31181;&#39640;&#24230;&#27969;&#34892;&#30340;Apache&#39033;&#30446;&#30340;8,316&#20010;Bug&#25253;&#21578;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;RLocator&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;Bug&#23450;&#20301;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software developers spend a significant portion of time fixing bugs in their projects. To streamline this process, bug localization approaches have been proposed to identify the source code files that are likely responsible for a particular bug. Prior work proposed several similarity-based machine-learning techniques for bug localization. Despite significant advances in these techniques, they do not directly optimize the evaluation measures. Instead, they use different metrics in the training and testing phases, which can negatively impact the model performance in retrieval tasks. In this paper, we propose RLocator, a Reinforcement Learning-based (RL) bug localization approach. We formulate the bug localization problem using a Markov Decision Process (MDP) to optimize the evaluation measures directly. We present the technique and experimentally evaluate it based on a benchmark dataset of 8,316 bug reports from six highly popular Apache projects. Our evaluation shows that RLocator achie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#24182;&#24341;&#20837;&#20102;&#20989;&#25968;&#36817;&#20284;&#26469;&#35299;&#20915;&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#21452;&#24179;&#22343;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.16715</link><description>&lt;p&gt;
&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization over General State and Action Spaces. (arXiv:2211.16715v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#24182;&#24341;&#20837;&#20102;&#20989;&#25968;&#36817;&#20284;&#26469;&#35299;&#20915;&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#21452;&#24179;&#22343;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#24322;&#24120;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20989;&#25968;&#36817;&#20284;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#21452;&#24179;&#22343;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#37117;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;RL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) problems over general state and action spaces are notoriously challenging. In contrast to the tableau setting, one can not enumerate all the states and then iteratively update the policies for each state. This prevents the application of many well-studied RL methods especially those with provable convergence guarantees. In this paper, we first present a substantial generalization of the recently developed policy mirror descent method to deal with general state and action spaces. We introduce new approaches to incorporate function approximation into this method, so that we do not need to use explicit policy parameterization at all. Moreover, we present a novel policy dual averaging method for which possibly simpler function approximation techniques can be applied. We establish linear convergence rate to global optimality or sublinear convergence to stationarity for these methods applied to solve different classes of RL problems under exact policy evaluation. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#25193;&#24352;&#32858;&#21512;&#30340;&#28145;&#24230;&#27531;&#24046;GCN&#26041;&#27861;&#36827;&#34892;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#28145;&#24230;&#21160;&#24577;&#27531;&#24046;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#32479;&#19968;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#35268;&#21017;&#22270;&#19978;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.11174</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#25193;&#24352;&#32858;&#21512;&#22312;&#28145;&#24230;&#27531;&#24046;GCN&#20013;&#36827;&#34892;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Overlapping Community Detection using Dynamic Dilated Aggregation in Deep Residual GCN. (arXiv:2210.11174v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#25193;&#24352;&#32858;&#21512;&#30340;&#28145;&#24230;&#27531;&#24046;GCN&#26041;&#27861;&#36827;&#34892;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#28145;&#24230;&#21160;&#24577;&#27531;&#24046;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#32479;&#19968;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#35268;&#21017;&#22270;&#19978;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#26159;&#22270;&#25366;&#25496;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#19968;&#20123;&#30740;&#31350;&#32771;&#34385;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#24212;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#33324;&#19981;&#35268;&#21017;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#23558;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#21160;&#24577;&#25193;&#24352;&#32858;&#21512;&#26426;&#21046;&#21644;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#21160;&#24577;&#27531;&#24046;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;DynaResGCN&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#20013;&#30340;&#37325;&#21472;&#31038;&#21306;&#12290;&#28145;&#24230;&#30340;DynaResGCN&#27169;&#22411;&#34987;&#29992;&#20316;&#32534;&#30721;&#22120;&#65292;&#32780;&#25105;&#20204;&#23558;&#20271;&#21162;&#21033;-&#27850;&#26494;&#65288;BP&#65289;&#27169;&#22411;&#20316;&#20026;&#35299;&#30721;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#26694;&#26550;&#24212;&#29992;&#22312;&#19968;&#20010;&#27809;&#26377;&#22522;&#20934;&#20540;&#30340;&#30740;&#31350;&#20027;&#39064;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#25317;&#26377;&#21487;&#38752;&#65288;&#25163;&#24037;&#26631;&#35760;&#65289;&#22522;&#20934;&#20540;&#30340;Facebook&#32593;&#32476;&#38598;&#21512;&#65292;&#20197;&#21450;&#19968;&#32452;&#20855;&#26377;&#32463;&#39564;&#24615;&#65288;&#38750;&#25163;&#24037;&#26631;&#35760;&#65289;&#22522;&#20934;&#20540;&#30340;&#38750;&#24120;&#22823;&#30340;&#21512;&#33879;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overlapping community detection is a key problem in graph mining. Some research has considered applying graph convolutional networks (GCN) to tackle the problem. However, it is still challenging to incorporate deep graph convolutional networks in the case of general irregular graphs. In this study, we design a deep dynamic residual graph convolutional network (DynaResGCN) based on our novel dynamic dilated aggregation mechanisms and a unified end-to-end encoder-decoder-based framework to detect overlapping communities in networks. The deep DynaResGCN model is used as the encoder, whereas we incorporate the Bernoulli-Poisson (BP) model as the decoder. Consequently, we apply our overlapping community detection framework in a research topics dataset without having ground truth, a set of networks from Facebook having a reliable (hand-labeled) ground truth, and in a set of very large co-authorship networks having empirical (not hand-labeled) ground truth. Our experimentation on these datase
&lt;/p&gt;</description></item></channel></rss>