<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>RepairAgent&#26159;&#39318;&#20010;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#31243;&#24207;&#26469;&#35299;&#20915;&#31243;&#24207;&#20462;&#22797;&#25361;&#25112;&#30340;&#24037;&#20316;&#65292;&#20854;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#19968;&#32452;&#26377;&#21161;&#20110;&#31243;&#24207;&#20462;&#22797;&#30340;&#24037;&#20855;&#20197;&#21450;&#21160;&#24577;&#26356;&#26032;&#30340;&#25552;&#31034;&#26684;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17134</link><description>&lt;p&gt;
RepairAgent&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#31243;&#24207;&#20462;&#22797;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
RepairAgent: An Autonomous, LLM-Based Agent for Program Repair
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17134
&lt;/p&gt;
&lt;p&gt;
RepairAgent&#26159;&#39318;&#20010;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#31243;&#24207;&#26469;&#35299;&#20915;&#31243;&#24207;&#20462;&#22797;&#25361;&#25112;&#30340;&#24037;&#20316;&#65292;&#20854;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#19968;&#32452;&#26377;&#21161;&#20110;&#31243;&#24207;&#20462;&#22797;&#30340;&#24037;&#20855;&#20197;&#21450;&#21160;&#24577;&#26356;&#26032;&#30340;&#25552;&#31034;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20943;&#36731;&#36719;&#20214;&#28431;&#27934;&#23545;&#31995;&#32479;&#21487;&#38752;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RepairAgent&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33258;&#20027;&#20195;&#29702;&#35299;&#20915;&#31243;&#24207;&#20462;&#22797;&#25361;&#25112;&#30340;&#24037;&#20316;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#29992;&#22266;&#23450;&#30340;&#25552;&#31034;&#25110;&#22312;&#22266;&#23450;&#30340;&#21453;&#39304;&#24490;&#29615;&#20013;&#25552;&#31034;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;LLM&#35270;&#20026;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#35268;&#21010;&#21644;&#25191;&#34892;&#20462;&#22797;&#25805;&#20316;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#36890;&#36807;&#35843;&#29992;&#36866;&#24403;&#30340;&#24037;&#20855;&#20462;&#22797;&#28431;&#27934;&#12290;RepairAgent&#33258;&#30001;&#22320;&#20132;&#32455;&#30528;&#25910;&#38598;&#26377;&#20851;&#28431;&#27934;&#30340;&#20449;&#24687;&#12289;&#25910;&#38598;&#20462;&#22797;&#26448;&#26009;&#20197;&#21450;&#39564;&#35777;&#20462;&#22797;&#36807;&#31243;&#65292;&#24182;&#26681;&#25454;&#25910;&#38598;&#21040;&#30340;&#20449;&#24687;&#21644;&#20808;&#21069;&#30340;&#20462;&#22797;&#23581;&#35797;&#21453;&#39304;&#20915;&#23450;&#35843;&#29992;&#21738;&#20123;&#24037;&#20855;&#12290;&#23454;&#29616;RepairAgent&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#19968;&#32452;&#26377;&#21161;&#20110;&#31243;&#24207;&#20462;&#22797;&#30340;&#24037;&#20855;&#65292;&#20197;&#21450;&#19968;&#20010;&#21160;&#24577;&#26356;&#26032;&#30340;&#25552;&#31034;&#26684;&#24335;&#65292;&#20351;LLM&#33021;&#22815;&#36827;&#34892;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17134v1 Announce Type: cross  Abstract: Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08773</link><description>&lt;p&gt;
Veagle: &#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Veagle: Advancements in Multimodal Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#35821;&#35328;&#21644;&#35270;&#35273;&#22914;&#20309;&#32467;&#21512;&#20135;&#29983;&#20102;&#27987;&#21402;&#20852;&#36259;&#65292;&#20174;&#32780;&#20652;&#29983;&#20102;&#26088;&#22312;&#26080;&#32541;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24310;&#20280;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#33539;&#22260;&#20174;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#21040;&#35270;&#35273;&#23450;&#20301;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#20934;&#30830;&#35299;&#37322;&#22270;&#20687;&#24182;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#38024;&#23545;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20013;&#35266;&#23519;&#21040;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;Veagle&#65292;&#34701;&#21512;&#20102;&#21463;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08773v1 Announce Type: cross  Abstract: Lately, researchers in artificial intelligence have been really interested in how language and vision come together, giving rise to the development of multimodal models that aim to seamlessly integrate textual and visual information. Multimodal models, an extension of Large Language Models (LLMs), have exhibited remarkable capabilities in addressing a diverse array of tasks, ranging from image captioning and visual question answering (VQA) to visual grounding. While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios. This paper introduces a novel approach to enhance the multimodal capabilities of existing models. In response to the limitations observed in current Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs), our proposed model Veagle, incorporates a unique mechanism inspired by th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;StreamingDialogue&#65292;&#36890;&#36807;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#65292;&#24182;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;</title><link>https://arxiv.org/abs/2403.08312</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#25439;&#22833;&#36827;&#34892;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;StreamingDialogue&#65306;&#38271;&#23545;&#35805;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08312
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;StreamingDialogue&#65292;&#36890;&#36807;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#65292;&#24182;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#26102;&#36935;&#21040;&#20102;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#23545;&#35805;&#19978;&#19979;&#25991;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#24182;&#19988;&#23545;&#35805;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;\textit{End-of-Utterance} (EoU) &#26377;&#32858;&#21512;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23558;EoU&#26631;&#35760;&#31216;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65288;conv-attn sinks&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;StreamingDialogue&#65292;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;conv-attn&#27785;&#28857;&#65292;&#24182;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20174;&#32780;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#27785;&#28857;&#25968;&#37327;&#65288;&#21363;&#35805;&#35821;&#25968;&#37327;&#65289;&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#12290;&#24403;&#21069;&#30340;LLMs&#24050;&#32463;&#23637;&#31034;&#20102;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#31383;&#21475;&#22823;&#23567;&#36798;&#21040;200k&#29978;&#33267;&#26356;&#22823;&#12290;&#36890;&#36807;&#23558;&#35805;&#35821;&#21387;&#32553;&#20026;EoUs&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08312v1 Announce Type: cross  Abstract: Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200k or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200k of utterances, resulting in a prolonged dialogue learning. In order to minimize informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#38024;&#23545;&#20154;&#31867;&#22270;&#20687;&#21512;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32654;&#23398;&#20998;&#25968;&#39044;&#27979;&#27169;&#22411;&#65292;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#30340;&#35270;&#35273;&#21560;&#24341;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#26377;&#29983;&#25104;&#30340;&#20154;&#31867;&#22270;&#20687;&#20013;&#20302;&#36136;&#37327;&#21306;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#65292;&#21516;&#26102;&#20063;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#27010;&#24565;&#35206;&#30422;&#24230;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.05125</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65306;&#20851;&#20110;&#20154;&#31867;&#22270;&#20687;&#21512;&#25104;&#30340;&#32463;&#39564;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text-to-Image Generative Models: An Empirical Study on Human Image Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#38024;&#23545;&#20154;&#31867;&#22270;&#20687;&#21512;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32654;&#23398;&#20998;&#25968;&#39044;&#27979;&#27169;&#22411;&#65292;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#30340;&#35270;&#35273;&#21560;&#24341;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#26377;&#29983;&#25104;&#30340;&#20154;&#31867;&#22270;&#20687;&#20013;&#20302;&#36136;&#37327;&#21306;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#65292;&#21516;&#26102;&#20063;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#27010;&#24565;&#35206;&#30422;&#24230;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#20154;&#31867;&#22270;&#20687;&#21512;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#35780;&#20272;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#26041;&#38754;&#65306;&#31532;&#19968;&#65292;&#19987;&#27880;&#20110;&#22270;&#20687;&#36136;&#37327;&#65292;&#22914;&#32654;&#23398;&#21644;&#36924;&#30495;&#24230;&#65307;&#31532;&#20108;&#65292;&#36890;&#36807;&#27010;&#24565;&#35206;&#30422;&#24230;&#21644;&#20844;&#24179;&#24615;&#26469;&#26816;&#26597;&#25991;&#26412;&#26465;&#20214;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32654;&#23398;&#20998;&#25968;&#39044;&#27979;&#27169;&#22411;&#65292;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#30340;&#35270;&#35273;&#21560;&#24341;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#26377;&#29983;&#25104;&#30340;&#20154;&#31867;&#22270;&#20687;&#20013;&#20302;&#36136;&#37327;&#21306;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;&#27010;&#24565;&#35206;&#30422;&#33539;&#22260;&#30340;&#25506;&#32034;&#35843;&#26597;&#20102;&#27169;&#22411;&#22312;&#20934;&#30830;&#35299;&#37322;&#21644;&#21576;&#29616;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#25105;&#20204;&#23545;&#20844;&#24179;&#24615;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20559;&#35265;&#65292;&#37325;&#28857;&#20851;&#27880;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#20154;&#31867;&#22270;&#20687;&#65292;&#20294;&#36825;&#31181;&#21452;&#37325;&#26041;&#38754;&#30340;&#26041;&#27861;&#26159;&#20026;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05125v1 Announce Type: cross  Abstract: In this paper, we present an empirical study introducing a nuanced evaluation framework for text-to-image (T2I) generative models, applied to human image synthesis. Our framework categorizes evaluations into two distinct groups: first, focusing on image qualities such as aesthetics and realism, and second, examining text conditions through concept coverage and fairness. We introduce an innovative aesthetic score prediction model that assesses the visual appeal of generated images and unveils the first dataset marked with low-quality regions in generated human images to facilitate automatic defect detection. Our exploration into concept coverage probes the model's effectiveness in interpreting and rendering text-based concepts accurately, while our analysis of fairness reveals biases in model outputs, with an emphasis on gender, race, and age. While our study is grounded in human imagery, this dual-faceted approach is designed with the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04306</link><description>&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Effectiveness Assessment of Recent Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#20986;&#29616;&#20195;&#34920;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#31243;&#24230;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#27969;&#34892;&#30340;LVLMs&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#22312;&#19987;&#19994;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#30340;&#20840;&#38754;&#27979;&#35797;&#24179;&#21488;&#65292;&#28085;&#30422;&#20845;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#26174;&#33879;&#12289;&#20266;&#35013;&#21644;&#36879;&#26126;&#29289;&#20307;&#26816;&#27979;&#65292;&#20197;&#21450;&#24687;&#32905;&#21644;&#30382;&#32932;&#30149;&#21464;&#26816;&#27979;&#65292;&#20197;&#21450;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#26368;&#36817;&#19977;&#31181;&#24320;&#28304;LVLMs--MiniGPT-v2&#12289;LLaVA-1.5&#21644;Shikra--&#22312;&#35270;&#35273;&#35782;&#21035;&#21644;&#23450;&#20301;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#25968;&#25454;&#30340;&#22330;&#26223;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01183</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#22330;&#26223;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01183
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#25968;&#25454;&#30340;&#22330;&#26223;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
21&#19990;&#32426;&#30340;&#29359;&#32618;&#20998;&#20026;&#34394;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#24050;&#32463;&#25104;&#20026;&#23545;&#21518;&#32773;&#20154;&#20204;&#31119;&#31049;&#21644;&#23433;&#20840;&#26500;&#25104;&#20840;&#29699;&#23041;&#32961;&#12290;&#23427;&#25552;&#20986;&#30340;&#25361;&#25112;&#24517;&#39035;&#36890;&#36807;&#32479;&#19968;&#30340;&#20840;&#29699;&#21512;&#20316;&#26469;&#38754;&#23545;&#65292;&#25105;&#20204;&#24517;&#39035;&#27604;&#20197;&#24448;&#26356;&#21152;&#20381;&#36182;&#33258;&#21160;&#21270;&#20294;&#20540;&#24471;&#20449;&#36182;&#30340;&#24037;&#20855;&#26469;&#24212;&#23545;&#32593;&#32476;&#29359;&#32618;&#26085;&#30410;&#22686;&#38271;&#30340;&#26412;&#36136;&#12290;&#27599;&#24180;&#26377;&#36229;&#36807;1000&#19975;&#36215;&#20799;&#31461;&#24615;&#34384;&#24453;&#25253;&#21578;&#25552;&#20132;&#32473;&#32654;&#22269;&#22269;&#23478;&#22833;&#36394;&#21644;&#34987;&#21093;&#21066;&#20799;&#31461;&#20013;&#24515;&#65292;&#36229;&#36807;80%&#26469;&#33258;&#32593;&#32476;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#35843;&#26597;&#20013;&#24515;&#21644;&#28165;&#38500;&#20013;&#24515;&#26080;&#27861;&#25163;&#21160;&#22788;&#29702;&#21644;&#27491;&#30830;&#35843;&#26597;&#25152;&#26377;&#22270;&#20687;&#12290;&#22522;&#20110;&#27492;&#65292;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#30340;&#21487;&#38752;&#33258;&#21160;&#21270;&#24037;&#20855;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22330;&#26223;&#35782;&#21035;&#20219;&#21153;&#23547;&#25214;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#65292;&#33021;&#22815;&#32452;&#32455;&#21644;&#20998;&#31867;&#20799;&#31461;&#24615;&#34384;&#24453;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#22312;&#25935;&#24863;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01183v1 Announce Type: cross  Abstract: Crime in the 21st century is split into a virtual and real world. However, the former has become a global menace to people's well-being and security in the latter. The challenges it presents must be faced with unified global cooperation, and we must rely more than ever on automated yet trustworthy tools to combat the ever-growing nature of online offenses. Over 10 million child sexual abuse reports are submitted to the US National Center for Missing &amp; Exploited Children every year, and over 80% originated from online sources. Therefore, investigation centers and clearinghouses cannot manually process and correctly investigate all imagery. In light of that, reliable automated tools that can securely and efficiently deal with this data are paramount. In this sense, the scene recognition task looks for contextual cues in the environment, being able to group and classify child sexual abuse data without requiring to be trained on sensitive 
&lt;/p&gt;</description></item><item><title>RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17257</link><description>&lt;p&gt;
RIME: &#20855;&#26377;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17257
&lt;/p&gt;
&lt;p&gt;
RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#23545;&#22870;&#21169;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;PbRL&#31639;&#27861;&#36807;&#24230;&#20381;&#36182;&#26469;&#33258;&#39046;&#22495;&#19987;&#23478;&#30340;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#23548;&#33268;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RIME&#65292;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#22024;&#26434;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#37492;&#21035;&#22120;&#65292;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#20197;&#36827;&#34892;&#20581;&#22766;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#36873;&#25321;&#19981;&#27491;&#30830;&#36896;&#25104;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#27492;&#22806;&#36824;&#33021;&#22635;&#34917;PbRL&#20013;&#20174;&#39044;&#35757;&#32451;&#21040;&#22312;&#32447;&#35757;&#32451;&#36807;&#28193;&#26102;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#21644;&#36816;&#21160;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RIME&#26174;&#33879;&#25552;&#21319;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#28909;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17257v1 Announce Type: cross  Abstract: Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm star
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#35757;&#32451;&#25968;&#25454;&#33021;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#27604;&#20363;&#30340;&#27700;&#21360;&#35757;&#32451;&#25991;&#26412;&#65292;&#20173;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#22320;&#26816;&#27979;&#20986;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.14904</link><description>&lt;p&gt;
&#25968;&#23383;&#27700;&#21360;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#25918;&#23556;&#24615;
&lt;/p&gt;
&lt;p&gt;
Watermarking Makes Language Models Radioactive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#35757;&#32451;&#25968;&#25454;&#33021;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#27604;&#20363;&#30340;&#27700;&#21360;&#35757;&#32451;&#25991;&#26412;&#65292;&#20173;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#22320;&#26816;&#27979;&#20986;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#26816;&#27979;&#21040;&#36825;&#31181;&#36755;&#20837;&#34987;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#25104;&#21592;&#25512;&#26029;&#21487;&#20197;&#20197;&#19968;&#23450;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#36825;&#31181;&#26816;&#27979;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;&#35757;&#32451;&#25968;&#25454;&#30041;&#19979;&#30340;&#30165;&#36857;&#27604;&#25104;&#21592;&#25512;&#26029;&#26356;&#23481;&#26131;&#26816;&#27979;&#19988;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#23558;&#27745;&#26579;&#27700;&#24179;&#19982;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#12289;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#27604;&#20363;&#21644;&#24494;&#35843;&#36807;&#31243;&#32852;&#31995;&#36215;&#26469;&#12290;&#29305;&#21035;&#26159;&#25105;&#20204;&#23637;&#31034;&#65292;&#21363;&#20351;&#21482;&#26377;5&#65285;&#30340;&#35757;&#32451;&#25991;&#26412;&#34987;&#25968;&#23383;&#27700;&#21360;&#26631;&#35760;&#65292;&#35757;&#32451;&#22312;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;&#21512;&#25104;&#25351;&#20196;&#19978;&#20173;&#28982;&#21487;&#20197;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#65288;p&#20540;&lt;1e-5&#65289;&#34987;&#26816;&#27979;&#21040;&#12290;&#22240;&#27492;&#65292;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;LLM&#27700;&#21360;&#25216;&#26415;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#30830;&#23450;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;LLM&#30340;&#36755;&#20986;&#26159;&#21542;&#34987;&#29992;&#26469;&#23545;&#21478;&#19968;&#20010;LLM&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14904v1 Announce Type: cross  Abstract: This paper investigates the radioactivity of LLM-generated texts, i.e. whether it is possible to detect that such input was used as training data. Conventional methods like membership inference can carry out this detection with some level of accuracy. We show that watermarked training data leaves traces easier to detect and much more reliable than membership inference. We link the contamination level to the watermark robustness, its proportion in the training set, and the fine-tuning process. We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (p-value &lt; 1e-5) even when as little as 5% of training text is watermarked. Thus, LLM watermarking, originally designed for detecting machine-generated text, gives the ability to easily identify if the outputs of a watermarked LLM were used to fine-tune another LLM.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.14744</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22478;&#24066;&#23621;&#27665;&#65306;&#29992;&#20110;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#30340;LLM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#28789;&#27963;&#39640;&#25928;&#30340;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#12290;LLMs&#36890;&#36807;&#39640;&#25928;&#22788;&#29702;&#35821;&#20041;&#25968;&#25454;&#24182;&#22312;&#24314;&#27169;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#20379;&#22810;&#21151;&#33021;&#24615;, &#20811;&#26381;&#20102;&#20197;&#24448;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#36843;&#20999;&#38656;&#27714;, &#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;: &#23558;LLMs&#19982;&#20016;&#23500;&#30340;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;, &#24320;&#21457;&#21487;&#38752;&#30340;&#27963;&#21160;&#29983;&#25104;&#31574;&#30053;, &#20197;&#21450;&#25506;&#32034;LLMs&#22312;&#22478;&#24066;&#31227;&#21160;&#20013;&#30340;&#24212;&#29992;&#12290;&#20854;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20195;&#29702;&#26694;&#26550;, &#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#20010;&#20307;&#27963;&#21160;&#27169;&#24335;&#21644;&#21160;&#26426;, &#21253;&#25324;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#33258;&#27965;&#26041;&#27861;&#21644;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;, &#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14744v1 Announce Type: new  Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This 
&lt;/p&gt;</description></item><item><title>LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14086</link><description>&lt;p&gt;
LexC-Gen: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21452;&#35821;&#35789;&#27719;&#34920;&#20026;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#29983;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14086
&lt;/p&gt;
&lt;p&gt;
LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#21294;&#20047;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#35760;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#36880;&#23383;&#32763;&#35793;&#26469;&#35299;&#20915;&#65292;&#28982;&#32780;&#65292;&#21452;&#35821;&#35789;&#20856;&#36890;&#24120;&#19982;&#20219;&#21153;&#25968;&#25454;&#26377;&#38480;&#30340;&#35789;&#27719;&#37325;&#21472;&#65292;&#23548;&#33268;&#32763;&#35793;&#35206;&#30422;&#21644;&#35789;&#20856;&#21033;&#29992;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LexC-Gen&#30340;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LexC-Gen&#39318;&#20808;&#20351;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#21333;&#35789;&#29983;&#25104;&#19982;&#35789;&#20856;&#20860;&#23481;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21333;&#35789;&#32763;&#35793;&#23558;&#20854;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;17&#31181;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;LexC-Gen&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#24615;&#33021;&#19978;&#19982;&#19987;&#23478;&#32763;&#35793;&#30340;&#40644;&#37329;&#25968;&#25454;&#31454;&#20105;&#21147;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#19978;&#24179;&#22343;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#21333;&#35789;&#32763;&#35793;&#26041;&#27861;&#25552;&#39640;&#20102;5.6&#21644;8.9&#20010;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14086v1 Announce Type: cross  Abstract: Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classificati
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#24895;&#26223;&#65292;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20026;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.12327</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#20132;&#27969;&#21527;&#65306;&#25506;&#32034;&#31454;&#20105;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21457;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#24895;&#26223;&#65292;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20026;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#20195;&#29702;&#20855;&#26377;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21644;&#31038;&#20250;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#30740;&#31350;LLM&#20195;&#29702;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#33258;&#21457;&#24314;&#31435;&#21512;&#20316;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#19981;&#20165;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#31454;&#20105;&#19982;&#21512;&#20316;&#30340;&#33021;&#21147;&#65292;&#20063;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#24895;&#26223;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#37027;&#20123;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#31038;&#20250;&#29616;&#35937;&#30340;&#27934;&#23519;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/wuzengqing001225/SABM_ShallWe &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12327v1 Announce Type: new  Abstract: Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics. However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied. To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings. This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science. Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena. The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#12289;&#22686;&#21152;&#20102;&#22238;&#25253;&#24182;&#31616;&#21270;&#20102;&#23454;&#29616;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09900</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Recurrent Reinforcement Learning with Memory Monoids
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#12289;&#22686;&#21152;&#20102;&#22238;&#25253;&#24182;&#31616;&#21270;&#20102;&#23454;&#29616;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20687;RNN&#21644;transformers&#36825;&#26679;&#30340;&#35760;&#24518;&#27169;&#22411;&#36890;&#36807;&#23558;&#36712;&#36857;&#26144;&#23556;&#21040;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#26469;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38271;&#24207;&#21015;&#30340;&#35268;&#27169;&#21270;&#22788;&#29702;&#33021;&#21147;&#24182;&#19981;&#29305;&#21035;&#22909;&#65292;&#23588;&#20854;&#26159;&#19982;&#19968;&#31867;&#26032;&#20852;&#30340;&#35760;&#24518;&#27169;&#22411;&#65288;&#26377;&#26102;&#31216;&#20026;&#32447;&#24615;&#24490;&#29615;&#27169;&#22411;&#65289;&#30456;&#27604;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#24490;&#29615;&#26356;&#26032;&#26159;&#19968;&#20010;&#21333;&#23376;&#65292;&#22240;&#27492;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20256;&#32479;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#21033;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#22686;&#21152;&#20102;&#22238;&#25253;&#65292;&#24182;&#31616;&#21270;&#20102;&#24490;&#29615;&#20002;&#22833;&#20989;&#25968;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09900v1 Announce Type: cross  Abstract: In RL, memory models such as RNNs and transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models. We discover that the recurrent update of these models is a monoid, leading us to formally define a novel memory monoid framework. We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies. Leveraging the properties of memory monoids, we propose a new batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#25552;&#39640;&#20102;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08918</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#22312;&#22270;&#19978;&#23398;&#20064;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21152;&#36895;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Graph Inference Acceleration by Learning MLPs on Graphs without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#25552;&#39640;&#20102;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#28040;&#24687;&#20256;&#36882;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#65292;&#27604;&#22914;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20174;GNNs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLPs&#65289;&#26469;&#21152;&#36895;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#26377;&#30417;&#30563;&#33976;&#39311;&#38480;&#21046;&#20102;&#23545;&#26410;&#35265;&#33410;&#28857;&#30340;&#27867;&#21270;&#65292;&#32780;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#36825;&#31181;&#24773;&#20917;&#24456;&#24120;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#29992;&#20110;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;SimMLP&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#40784;GNNs&#21644;MLPs&#20043;&#38388;&#30340;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#20043;&#38388;&#30340;&#31934;&#32454;&#21644;&#27867;&#21270;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#24179;&#20961;&#35299;&#30340;&#39118;&#38505;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08918v1 Announce Type: cross Abstract: Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph learning tasks, yet their reliance on message-passing constraints their deployment in latency-sensitive applications such as financial fraud detection. Recent works have explored distilling knowledge from GNNs to Multi-Layer Perceptrons (MLPs) to accelerate inference. However, this task-specific supervised distillation limits generalization to unseen nodes, which are prevalent in latency-sensitive applications. To this end, we present \textbf{\textsc{SimMLP}}, a \textbf{\textsc{Sim}}ple yet effective framework for learning \textbf{\textsc{MLP}}s on graphs without supervision, to enhance generalization. \textsc{SimMLP} employs self-supervised alignment between GNNs and MLPs to capture the fine-grained and generalizable correlation between node features and graph structures, and proposes two strategies to alleviate the risk of trivial solutions. Theoretically, w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#31687;&#29983;&#25104;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.06544</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26657;&#20934;&#38271;&#31687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Calibrating Long-form Generations from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#31687;&#29983;&#25104;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#38752;&#24615;&#65292;&#26657;&#20934;&#26159;&#24517;&#35201;&#30340; - &#27169;&#22411;&#30340;&#35780;&#20272;&#32622;&#20449;&#24230;&#24212;&#35813;&#19982;&#20854;&#21709;&#24212;&#27491;&#30830;&#24615;&#30340;&#23454;&#38469;&#21487;&#33021;&#24615;&#30456;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#21644;&#26657;&#20934;&#25351;&#26631;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#21709;&#24212;&#27491;&#30830;&#24615;&#30340;&#20108;&#20803;&#30495;/&#20551;&#35780;&#20272;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#38271;&#31687;&#29983;&#25104;&#20013;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#31572;&#26696;&#21487;&#33021;&#37096;&#20998;&#27491;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#20854;&#20013;LLMs&#30340;&#21709;&#24212;&#27491;&#30830;&#24615;&#21644;&#20851;&#32852;&#30340;&#32622;&#20449;&#27700;&#24179;&#37117;&#34987;&#35270;&#20026;&#19968;&#31995;&#21015;&#20998;&#25968;&#30340;&#20998;&#24067;&#12290;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#26469;&#31934;&#30830;&#35780;&#20272;LLM&#30340;&#26657;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#19968;&#33268;&#24615;&#21644;&#33258;&#35780;&#20272;&#30340;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21253;&#25324;&#38271;&#31687;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibratio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#23454;&#29616;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#23567;&#26356;&#25913;&#29616;&#26377;&#35268;&#33539;&#26469;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04660</link><description>&lt;p&gt;
&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness Through Artifact Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#23454;&#29616;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#23567;&#26356;&#25913;&#29616;&#26377;&#35268;&#33539;&#26469;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#20986;&#29616;&#32473;&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#38459;&#30861;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#22823;&#22810;&#25968;&#38450;&#24481;&#26041;&#27861;&#37117;&#25913;&#21464;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#24335;&#65288;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#25110;&#25512;&#29702;&#36807;&#31243;&#65288;&#22914;&#38543;&#26426;&#24179;&#28369;&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#27169;&#22411;&#20173;&#28982;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#22914;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#35937;&#26159;&#25353;&#29031;&#35268;&#33539;&#26469;&#35774;&#35745;&#65288;&#22914;&#26631;&#24535;&#35268;&#33539;&#65289;&#12290;&#20026;&#20102;&#25913;&#21892;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37325;&#26032;&#23450;&#20041;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#35268;&#33539;&#36827;&#34892;&#24494;&#23567;&#30340;&#26356;&#25913;&#65292;&#20197;&#38450;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#25105;&#20204;&#23558;&#33402;&#26415;&#35774;&#35745;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#36138;&#23146;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#22312;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#39046;&#22495;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#20854;&#33021;&#22815;&#25913;&#21464;&#20132;&#36890;&#26631;&#24535;&#20013;&#30340;&#35937;&#24418;&#22270;&#26631;&#65288;&#21363;&#26631;&#24535;&#20869;&#30340;&#31526;&#21495;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models' adversarial robustness, models remain highly susceptible to adversarial examples. Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness. Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples. We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BrainRGIN&#24314;&#27169;&#26550;&#26500;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26234;&#21147;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#24182;&#32467;&#21512;&#20102;&#32858;&#31867;&#23884;&#20837;&#12289;&#22270;&#21516;&#26500;&#32593;&#32476;&#12289;TopK&#27744;&#21270;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.03520</link><description>&lt;p&gt;
&#22823;&#33041;&#32593;&#32476;&#19982;&#26234;&#21147;&#65306;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BrainRGIN&#24314;&#27169;&#26550;&#26500;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26234;&#21147;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#24182;&#32467;&#21512;&#20102;&#32858;&#31867;&#23884;&#20837;&#12289;&#22270;&#21516;&#26500;&#32593;&#32476;&#12289;TopK&#27744;&#21270;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;rsfMRI&#65289;&#26159;&#19968;&#31181;&#30740;&#31350;&#22823;&#33041;&#21151;&#33021;&#21644;&#35748;&#30693;&#36807;&#31243;&#20851;&#31995;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25429;&#33719;&#22823;&#33041;&#30340;&#21151;&#33021;&#32452;&#32455;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#25110;&#21050;&#28608;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;BrainRGIN&#30340;&#26032;&#39062;&#24314;&#27169;&#26550;&#26500;&#65292;&#21033;&#29992;rsfMRI&#25512;&#23548;&#30340;&#38745;&#24577;&#21151;&#33021;&#32593;&#32476;&#36830;&#25509;&#30697;&#38453;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26234;&#21147;&#65288;&#27969;&#20307;&#12289;&#26230;&#20307;&#21644;&#24635;&#20307;&#26234;&#21147;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#23558;&#32858;&#31867;&#23884;&#20837;&#21644;&#22270;&#21516;&#26500;&#32593;&#32476;&#32435;&#20837;&#21040;&#22270;&#21367;&#31215;&#23618;&#20013;&#65292;&#20197;&#21453;&#26144;&#22823;&#33041;&#23376;&#32593;&#32476;&#32452;&#32455;&#30340;&#24615;&#36136;&#21644;&#39640;&#25928;&#32593;&#32476;&#34920;&#36798;&#65292;&#20877;&#36741;&#20197;TopK&#27744;&#21270;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03520v2 Announce Type: replace-cross  Abstract: Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful tool for investigating the relationship between brain function and cognitive processes as it allows for the functional organization of the brain to be captured without relying on a specific task or stimuli. In this paper, we present a novel modeling architecture called BrainRGIN for predicting intelligence (fluid, crystallized, and total intelligence) using graph neural networks on rsfMRI derived static functional network connectivity matrices. Extending from the existing graph convolution networks, our approach incorporates a clustering-based embedding and graph isomorphism network in the graph convolutional layer to reflect the nature of the brain sub-network organization and efficient network expression, in combination with TopK pooling and attention-based readout functions. We evaluated our proposed architecture on a large dataset, specifically the A
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21644;&#20195;&#29702;&#30340;&#34920;&#31034;&#65292;&#20351;&#29992;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#25429;&#25417;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#20013;&#30340;&#29616;&#35937;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#31354;&#38388;&#27169;&#24335;&#21644;&#24314;&#27169;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10300</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#20013;&#30340;&#29616;&#35937;&#26816;&#27979;&#30340;&#20855;&#26377;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#30340;&#20998;&#23618;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Framework with Spatio-Temporal Consistency Learning for Emergence Detection in Complex Adaptive Systems. (arXiv:2401.10300v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21644;&#20195;&#29702;&#30340;&#34920;&#31034;&#65292;&#20351;&#29992;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#25429;&#25417;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#20013;&#30340;&#29616;&#35937;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#31354;&#38388;&#27169;&#24335;&#21644;&#24314;&#27169;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30001;&#20132;&#20114;&#20195;&#29702;&#32452;&#25104;&#30340;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#65288;CAS&#65289;&#20013;&#65292;&#29616;&#35937;&#26159;&#19968;&#31181;&#20840;&#23616;&#23646;&#24615;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#24456;&#26222;&#36941;&#65292;&#20363;&#22914;&#32593;&#32476;&#23618;&#27425;&#30340;&#20132;&#36890;&#25317;&#22581;&#12290;&#26816;&#27979;&#23427;&#30340;&#24418;&#25104;&#21644;&#28040;&#25955;&#26377;&#21161;&#20110;&#30417;&#27979;&#31995;&#32479;&#30340;&#29366;&#24577;&#65292;&#24182;&#21457;&#20986;&#26377;&#23475;&#29616;&#35937;&#30340;&#35686;&#25253;&#20449;&#21495;&#12290;&#30001;&#20110;CAS&#27809;&#26377;&#38598;&#20013;&#24335;&#25511;&#21046;&#22120;&#65292;&#22522;&#20110;&#27599;&#20010;&#20195;&#29702;&#30340;&#23616;&#37096;&#35266;&#23519;&#26469;&#26816;&#27979;&#29616;&#35937;&#26159;&#21487;&#21462;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#19981;&#33021;&#25429;&#25417;&#19982;&#29616;&#35937;&#30456;&#20851;&#30340;&#31354;&#38388;&#27169;&#24335;&#65292;&#24182;&#19988;&#26080;&#27861;&#24314;&#27169;&#20195;&#29702;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#34920;&#31034;&#21644;&#20195;&#29702;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#22120;&#38024;&#23545;&#20195;&#29702;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#31995;&#32479;&#30340;&#22797;&#26434;&#28436;&#21270;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#36890;&#36807;&#20445;&#30041;&#26368;&#26032;100&#20010;&#20195;&#29702;&#30340;&#29366;&#24577;&#21644;&#21382;&#21490;&#29366;&#24577;&#26469;&#23398;&#20064;&#20195;&#29702;&#21644;&#31995;&#32479;&#30340;&#34920;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;
Emergence, a global property of complex adaptive systems (CASs) constituted by interactive agents, is prevalent in real-world dynamic systems, e.g., network-level traffic congestions. Detecting its formation and evaporation helps to monitor the state of a system, allowing to issue a warning signal for harmful emergent phenomena. Since there is no centralized controller of CAS, detecting emergence based on each agent's local observation is desirable but challenging. Existing works are unable to capture emergence-related spatial patterns, and fail to model the nonlinear relationships among agents. This paper proposes a hierarchical framework with spatio-temporal consistency learning to solve these two problems by learning the system representation and agent representations, respectively. Especially, spatio-temporal encoders are tailored to capture agents' nonlinear relationships and the system's complex evolution. Representations of the agents and the system are learned by preserving the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZoDiac&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#22312;&#21487;&#35757;&#32451;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27880;&#20837;&#27700;&#21360;&#65292;&#20174;&#32780;&#20351;&#27700;&#21360;&#33021;&#22815;&#22312;&#21463;&#25915;&#20987;&#26102;&#21487;&#38752;&#26816;&#27979;&#21040;&#65292;&#23545;&#26368;&#20808;&#36827;&#30340;&#27700;&#21360;&#25915;&#20987;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04247</link><description>&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#30340;&#40065;&#26834;&#22270;&#20687;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Robust Image Watermarking using Stable Diffusion. (arXiv:2401.04247v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZoDiac&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#22312;&#21487;&#35757;&#32451;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27880;&#20837;&#27700;&#21360;&#65292;&#20174;&#32780;&#20351;&#27700;&#21360;&#33021;&#22815;&#22312;&#21463;&#25915;&#20987;&#26102;&#21487;&#38752;&#26816;&#27979;&#21040;&#65292;&#23545;&#26368;&#20808;&#36827;&#30340;&#27700;&#21360;&#25915;&#20987;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#27700;&#21360;&#23545;&#20110;&#36861;&#36394;&#22270;&#20687;&#26469;&#28304;&#21644;&#22768;&#26126;&#25152;&#26377;&#26435;&#38750;&#24120;&#37325;&#35201;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;&#65289;&#30340;&#20986;&#29616;&#65292;&#33021;&#22815;&#21019;&#24314;&#34394;&#20551;&#20294;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#27700;&#21360;&#25104;&#20026;&#20102;&#23588;&#20026;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20351;&#29983;&#25104;&#30340;&#22270;&#20687;&#21487;&#38752;&#22320;&#36776;&#35748;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#27491;&#26159;&#36825;&#31181;&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#21487;&#20197;&#31227;&#38500;&#20351;&#29992;&#29616;&#26377;&#26041;&#27861;&#27880;&#20837;&#30340;&#27700;&#21360;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZoDiac&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#23558;&#27700;&#21360;&#27880;&#20837;&#21040;&#21487;&#35757;&#32451;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#22312;&#21463;&#25915;&#20987;&#26102;&#20173;&#28982;&#21487;&#20197;&#21487;&#38752;&#22320;&#22312;&#28508;&#22312;&#21521;&#37327;&#20013;&#26816;&#27979;&#21040;&#27700;&#21360;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598; MS-COCO&#12289;DiffusionDB &#21644; WikiArt &#19978;&#35780;&#20272;&#20102; ZoDiac&#65292;&#24182;&#21457;&#29616; ZoDiac &#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27700;&#21360;&#25915;&#20987;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#27700;&#21360;&#26816;&#27979;&#29575;&#36229;&#36807;98%&#65292;&#35823;&#25253;&#29575;&#20302;&#20110;6.4%&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31283;&#23450;&#25193;&#25955;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermarking images is critical for tracking image provenance and claiming ownership. With the advent of generative models, such as stable diffusion, able to create fake but realistic images, watermarking has become particularly important, e.g., to make generated images reliably identifiable. Unfortunately, the very same stable diffusion technology can remove watermarks injected using existing methods. To address this problem, we present a ZoDiac, which uses a pre-trained stable diffusion model to inject a watermark into the trainable latent space, resulting in watermarks that can be reliably detected in the latent vector, even when attacked. We evaluate ZoDiac on three benchmarks, MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against state-of-the-art watermark attacks, with a watermark detection rate over 98% and a false positive rate below 6.4%, outperforming state-of-the-art watermarking methods. Our research demonstrates that stable diffusion is a promising appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#22238;&#31572;&#30001;&#39640;&#32500;&#25968;&#25454;&#24341;&#36215;&#30340;&#22240;&#26524;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2401.01426</link><description>&lt;p&gt;
&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference. (arXiv:2401.01426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#22238;&#31572;&#30001;&#39640;&#32500;&#25968;&#25454;&#24341;&#36215;&#30340;&#22240;&#26524;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pearl&#30340;&#22240;&#26524;&#23618;&#27425;&#32467;&#26500;&#22312;&#35266;&#27979;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#38382;&#39064;&#20043;&#38388;&#24314;&#31435;&#20102;&#26126;&#30830;&#30340;&#20998;&#31163;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#35745;&#31639;&#21487;&#36776;&#35782;&#22240;&#26524;&#26597;&#35810;&#30340;&#22768;&#38899;&#21644;&#23436;&#25972;&#31639;&#27861;&#65292;&#22312;&#32473;&#23450;&#23618;&#27425;&#30340;&#22240;&#26524;&#32467;&#26500;&#21644;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#36739;&#20302;&#23618;&#27425;&#30340;&#23618;&#27425;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#31639;&#27861;&#20551;&#35774;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#25968;&#25454;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#23545;&#20110;&#22914;&#22270;&#20687;&#36825;&#26679;&#30340;&#39640;&#32500;&#21464;&#37327;&#26159;&#19968;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#20195;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21487;&#20197;&#34987;&#35757;&#32451;&#26469;&#23398;&#20064;&#22914;&#20309;&#20934;&#30830;&#22320;&#20174;&#36825;&#26679;&#30340;&#39640;&#32500;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#29305;&#21035;&#26159;&#38543;&#30528;&#22270;&#20687;&#22522;&#27169;&#22411;&#30340;&#26368;&#36817;&#20852;&#36215;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#22238;&#31572;&#24102;&#26377;&#36825;&#26679;&#39640;&#32500;&#25968;&#25454;&#30340;&#22240;&#26524;&#26597;&#35810;&#26159;&#38750;&#24120;&#26377;&#21560;&#24341;&#21147;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#35757;&#32451;&#31639;&#27861;&#65292;&#32473;&#23450;&#22240;&#26524;&#32467;&#26500;&#21644;&#39044;&#35757;&#32451;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#20272;&#35745;&#30001;&#39640;&#32500;&#25968;&#25454;&#24341;&#36215;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pearl's causal hierarchy establishes a clear separation between observational, interventional, and counterfactual questions. Researchers proposed sound and complete algorithms to compute identifiable causal queries at a given level of the hierarchy using the causal structure and data from the lower levels of the hierarchy. However, most of these algorithms assume that we can accurately estimate the probability distribution of the data, which is an impractical assumption for high-dimensional variables such as images. On the other hand, modern generative deep learning architectures can be trained to learn how to accurately sample from such high-dimensional distributions. Especially with the recent rise of foundation models for images, it is desirable to leverage pre-trained models to answer causal queries with such high-dimensional data. To address this, we propose a sequential training algorithm that, given the causal structure and a pre-trained conditional generative model, can train a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Prompt2Forget&#65288;P2F&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24536;&#35760;&#38544;&#31169;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;LLM&#26412;&#22320;&#38544;&#31169;&#25361;&#25112;&#12290;P2F&#26041;&#27861;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#29255;&#27573;&#24182;&#29983;&#25104;&#34394;&#26500;&#31572;&#26696;&#65292;&#27169;&#31946;&#21270;&#27169;&#22411;&#23545;&#21407;&#22987;&#36755;&#20837;&#30340;&#35760;&#24518;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;P2F&#20855;&#26377;&#24456;&#24378;&#30340;&#27169;&#31946;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#33258;&#36866;&#24212;&#20351;&#29992;&#65292;&#26080;&#38656;&#25163;&#21160;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2401.00870</link><description>&lt;p&gt;
&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24536;&#35760;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Teach Large Language Models to Forget Privacy. (arXiv:2401.00870v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Prompt2Forget&#65288;P2F&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24536;&#35760;&#38544;&#31169;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;LLM&#26412;&#22320;&#38544;&#31169;&#25361;&#25112;&#12290;P2F&#26041;&#27861;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#29255;&#27573;&#24182;&#29983;&#25104;&#34394;&#26500;&#31572;&#26696;&#65292;&#27169;&#31946;&#21270;&#27169;&#22411;&#23545;&#21407;&#22987;&#36755;&#20837;&#30340;&#35760;&#24518;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;P2F&#20855;&#26377;&#24456;&#24378;&#30340;&#27169;&#31946;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#33258;&#36866;&#24212;&#20351;&#29992;&#65292;&#26080;&#38656;&#25163;&#21160;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20445;&#25252;&#38544;&#31169;&#26041;&#27861;&#65292;&#22914;&#24046;&#20998;&#38544;&#31169;&#21644;&#21516;&#24577;&#21152;&#23494;&#65292;&#22312;&#21482;&#26377;&#40657;&#30418;API&#30340;&#29615;&#22659;&#19979;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#35201;&#27714;&#27169;&#22411;&#36879;&#26126;&#24615;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt2Forget&#65288;P2F&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;LLM&#26412;&#22320;&#38544;&#31169;&#25361;&#25112;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#23548;LLM&#24536;&#35760;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#23436;&#25972;&#38382;&#39064;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#29255;&#27573;&#65292;&#29983;&#25104;&#34394;&#26500;&#30340;&#31572;&#26696;&#65292;&#24182;&#20351;&#27169;&#22411;&#23545;&#21407;&#22987;&#36755;&#20837;&#30340;&#35760;&#24518;&#27169;&#31946;&#21270;&#12290;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#39046;&#22495;&#30340;&#21253;&#21547;&#38544;&#31169;&#25935;&#24863;&#20449;&#24687;&#30340;&#38382;&#39064;&#21019;&#24314;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;P2F&#23454;&#29616;&#20102;&#38646;-shot&#27867;&#21270;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#33258;&#36866;&#24212;&#65292;&#26080;&#38656;&#25163;&#21160;&#35843;&#25972;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;P2F&#20855;&#26377;&#24456;&#24378;&#30340;&#27169;&#31946;&#21270;LLM&#35760;&#24518;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#20219;&#20309;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven powerful, but the risk of privacy leakage remains a significant concern. Traditional privacy-preserving methods, such as Differential Privacy and Homomorphic Encryption, are inadequate for black-box API-only settings, demanding either model transparency or heavy computational resources. We propose Prompt2Forget (P2F), the first framework designed to tackle the LLM local privacy challenge by teaching LLM to forget. The method involves decomposing full questions into smaller segments, generating fabricated answers, and obfuscating the model's memory of the original input. A benchmark dataset was crafted with questions containing privacy-sensitive information from diverse fields. P2F achieves zero-shot generalization, allowing adaptability across a wide range of use cases without manual adjustments. Experimental results indicate P2F's robust capability to obfuscate LLM's memory, attaining a forgetfulness score of around 90\% without any utility los
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.02505</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning to Reach Goals via Diffusion. (arXiv:2310.02505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#23558;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#22122;&#22768;&#26144;&#23556;&#21040;&#30446;&#26631;&#27969;&#24418;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25918;&#22312;&#25193;&#25955;&#24314;&#27169;&#30340;&#32972;&#26223;&#19979;&#65292;&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;&#31867;&#20284;&#20110;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#21033;&#29992;&#39640;&#26031;&#22122;&#22768;&#21019;&#24314;&#38543;&#26426;&#36712;&#36857;&#65292;&#20351;&#20854;&#36828;&#31163;&#25968;&#25454;&#27969;&#24418;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#36828;&#31163;&#28508;&#22312;&#30446;&#26631;&#29366;&#24577;&#30340;&#36712;&#36857;&#12290;&#28982;&#21518;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#31867;&#20284;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#12290;&#36825;&#20010;&#31216;&#20026;Merlin&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#36873;&#25321;&#65292;&#29992;&#20110;&#21462;&#20195;&#25193;&#25955;&#20013;&#30340;&#39640;&#26031;&#22122;&#22768;&#27169;&#22411; - &#32531;&#20914;&#21306;&#20013;&#30340;&#21453;&#21521;&#25773;&#25918;&#65292;&#21453;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#31163;&#32447;&#30446;&#26631;&#36798;&#25104;&#20219;&#21153;&#19978;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02469</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#33391;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Good Privacy Protection Learners. (arXiv:2310.02469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#21019;&#24314;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#30340;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#65288;PII&#65289;&#12290;&#22312;&#27809;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#24494;&#35843; LLMs &#20250;&#23384;&#22312;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26032;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#27604;&#22914;&#35821;&#26009;&#24211;&#31574;&#23637;&#12289;&#22522;&#20110;&#24809;&#32602;&#30340;&#38750;&#27010;&#28982;&#24615;&#35757;&#32451;&#25439;&#22833;&#20197;&#21450;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;&#31561;&#31561;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65292;&#26174;&#31034;&#20986;&#24456;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33041;&#21551;&#21457;&#24335;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#32452;&#32455;&#35843;&#33410;&#32593;&#32476;&#23558;&#21333;&#19968;&#26377;&#38480;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#32452;&#32455;&#20026;&#20016;&#23500;&#30340;&#31232;&#30095;&#31070;&#32463;&#36335;&#24452;&#65292;&#20197;&#39640;&#25928;&#24212;&#23545;&#36882;&#22686;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#21487;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#20197;&#21450;&#27867;&#21270;&#30340;CIFAR100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#20248;&#21183;&#12289;&#33021;&#32791;&#21644;&#20869;&#23384;&#23481;&#37327;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.09550</link><description>&lt;p&gt;
&#20855;&#26377;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#30340;&#31070;&#32463;&#36335;&#24452;&#30340;&#33258;&#36866;&#24212;&#37325;&#32452;
&lt;/p&gt;
&lt;p&gt;
Adaptive Reorganization of Neural Pathways for Continual Learning with Spiking Neural Networks. (arXiv:2309.09550v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33041;&#21551;&#21457;&#24335;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#32452;&#32455;&#35843;&#33410;&#32593;&#32476;&#23558;&#21333;&#19968;&#26377;&#38480;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#32452;&#32455;&#20026;&#20016;&#23500;&#30340;&#31232;&#30095;&#31070;&#32463;&#36335;&#24452;&#65292;&#20197;&#39640;&#25928;&#24212;&#23545;&#36882;&#22686;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#21487;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#20197;&#21450;&#27867;&#21270;&#30340;CIFAR100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#20248;&#21183;&#12289;&#33021;&#32791;&#21644;&#20869;&#23384;&#23481;&#37327;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33041;&#21487;&#20197;&#33258;&#32452;&#32455;&#20986;&#20016;&#23500;&#22810;&#26679;&#30340;&#31232;&#30095;&#31070;&#32463;&#36335;&#24452;&#65292;&#36880;&#27493;&#25484;&#25569;&#25968;&#30334;&#20010;&#35748;&#30693;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#28145;&#24230;&#20154;&#24037;&#21644;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#26080;&#27861;&#20805;&#20998;&#33258;&#21160;&#35843;&#33410;&#32593;&#32476;&#20013;&#26377;&#38480;&#30340;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#38543;&#30528;&#20219;&#21153;&#22686;&#21152;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#33021;&#32791;&#19978;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33041;&#21551;&#21457;&#24335;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#32452;&#32455;&#35843;&#33410;&#32593;&#32476;&#23558;&#21333;&#19968;&#26377;&#38480;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SOR-SNN&#65289;&#37325;&#26032;&#32452;&#32455;&#20026;&#20016;&#23500;&#30340;&#31232;&#30095;&#31070;&#32463;&#36335;&#24452;&#65292;&#20197;&#39640;&#25928;&#24212;&#23545;&#36882;&#22686;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#21487;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#20248;&#21183;&#12289;&#33021;&#32791;&#21644;&#20869;&#23384;&#23481;&#37327;&#20248;&#21183;&#65292;&#21253;&#25324;&#20174;&#20799;&#31461;&#31616;&#21333;&#20219;&#21153;&#21040;&#22797;&#26434;&#20219;&#21153;&#12289;&#20197;&#21450;&#27867;&#21270;&#30340;CIFAR100&#21644;ImageNet&#25968;&#25454;&#38598;&#12290;&#23588;&#20854;&#26159;&#65292;SOR-SNN&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12289;&#33021;&#32791;&#21644;&#20869;&#23384;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human brain can self-organize rich and diverse sparse neural pathways to incrementally master hundreds of cognitive tasks. However, most existing continual learning algorithms for deep artificial and spiking neural networks are unable to adequately auto-regulate the limited resources in the network, which leads to performance drop along with energy consumption rise as the increase of tasks. In this paper, we propose a brain-inspired continual learning algorithm with adaptive reorganization of neural pathways, which employs Self-Organizing Regulation networks to reorganize the single and limited Spiking Neural Network (SOR-SNN) into rich sparse neural pathways to efficiently cope with incremental tasks. The proposed model demonstrates consistent superiority in performance, energy consumption, and memory capacity on diverse continual learning tasks ranging from child-like simple to complex tasks, as well as on generalized CIFAR100 and ImageNet datasets. In particular, the SOR-SNN mod
&lt;/p&gt;</description></item><item><title>&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03084</link><description>&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03084
&lt;/p&gt;
&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#30446;&#21069;&#35299;&#20915;&#22823;&#35268;&#27169;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;&#26412;&#25991;&#22312;CFR&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32431;CFR&#65288;PCFR&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;PCFR&#21487;&#20197;&#30475;&#20316;&#26159;CFR&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#30340;&#32467;&#21512;&#65292;&#32487;&#25215;&#20102;CFR&#30340;&#21453;&#20107;&#23454;&#36951;&#25022;&#65288;&#20540;&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#20351;&#29992;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#32780;&#19981;&#26159;&#36951;&#25022;&#21305;&#37197;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;PCFR&#21487;&#20197;&#23454;&#29616;Blackwell&#21487;&#36798;&#24615;&#65292;&#20351;PCFR&#33021;&#22815;&#19982;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#22312;&#20869;&#30340;&#20219;&#20309;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#32431;MCCFR&#65288;PMCCFR&#65289;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;PMCCFR&#30340;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#27604;MCCFR&#24555;&#19977;&#20493;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PMCCFR&#19981;&#36890;&#36807;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#36335;&#24452;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21160;&#31639;&#27861;&#65292;&#21463;&#21040;&#20102;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strat
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#39044;&#22788;&#29702;&#22120;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#21457;&#24037;&#31243;&#25216;&#26415;&#21644;&#20256;&#32479;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16361</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#39044;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Data Preprocessors. (arXiv:2308.16361v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16361
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#39044;&#22788;&#29702;&#22120;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#21457;&#24037;&#31243;&#25216;&#26415;&#21644;&#20256;&#32479;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;OpenAI&#30340;GPT&#31995;&#21015;&#21644;Meta&#30340;LLaMA&#21464;&#20307;&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#32463;&#36807;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;LLMs&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#21508;&#31181;&#20027;&#39064;&#19978;&#20154;&#31867;&#21270;&#30340;&#25991;&#26412;&#12290;&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;LLMs&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#26159;&#25968;&#25454;&#25366;&#25496;&#21644;&#20998;&#26512;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;&#22914;GPT-3.5&#12289;GPT-4&#21644;Vicuna-13B&#65289;&#22312;&#38169;&#35823;&#26816;&#27979;&#12289;&#25968;&#25454;&#25554;&#34917;&#12289;&#27169;&#24335;&#21305;&#37197;&#21644;&#23454;&#20307;&#21305;&#37197;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#38500;&#20102;&#23637;&#31034;LLMs&#30340;&#20869;&#22312;&#33021;&#21147;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#24320;&#38144;&#21644;&#25928;&#29575;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#21069;&#27839;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;&#19978;&#19979;&#25991;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#31561;&#20256;&#32479;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's LLaMA variants, have marked a significant advancement in artificial intelligence. Trained on vast amounts of text data, LLMs are capable of understanding and generating human-like text across a diverse range of topics. This study expands on the applications of LLMs, exploring their potential in data preprocessing, a critical stage in data mining and analytics applications. We delve into the applicability of state-of-the-art LLMs such as GPT-3.5, GPT-4, and Vicuna-13B for error detection, data imputation, schema matching, and entity matching tasks. Alongside showcasing the inherent capabilities of LLMs, we highlight their limitations, particularly in terms of computational expense and inefficiency. We propose an LLM-based framework for data preprocessing, which integrates cutting-edge prompt engineering techniques, coupled with traditional methods like contextualization and feature selection, to improve the perform
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#24182;&#35782;&#21035;&#30284;&#30151;&#26032;&#20122;&#22411;&#12290;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#24182;&#36827;&#34892;&#20122;&#22411;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.04075</link><description>&lt;p&gt;
&#22522;&#20110;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#30284;&#30151;&#26032;&#20122;&#22411;&#21644;&#27835;&#30103;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data. (arXiv:2307.04075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#24182;&#35782;&#21035;&#30284;&#30151;&#26032;&#20122;&#22411;&#12290;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#24182;&#36827;&#34892;&#20122;&#22411;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30284;&#30151;&#30340;&#39640;&#24322;&#36136;&#24615;&#21644;&#20020;&#24202;&#29305;&#24449;&#65292;&#19981;&#21516;&#30284;&#30151;&#20122;&#22411;&#20043;&#38388;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#21644;&#20020;&#24202;&#29305;&#24449;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#30284;&#30151;&#20122;&#22411;&#30340;&#35782;&#21035;&#21644;&#21457;&#29616;&#23545;&#20110;&#30284;&#30151;&#30340;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#39044;&#21518;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#65292;&#20174;&#32780;&#35782;&#21035;&#21644;&#34920;&#24449;&#30284;&#30151;&#20122;&#22411;&#12290;AMUCL&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#21644;&#32858;&#31867;&#65292;&#24182;&#35782;&#21035;&#26032;&#30340;&#30284;&#30151;&#20122;&#22411;&#12290;&#36825;&#31181;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#31354;&#38388;&#20013;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#32858;&#31867;&#20122;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omics data and clinical features among subtypes of different cancers. Therefore, the identification and discovery of cancer subtypes are crucial for the diagnosis, treatment, and prognosis of cancer. In this study, we proposed a generalization framework based on attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze cancer multi-omics data for the identification and characterization of cancer subtypes. AMUCL framework includes a unsupervised multi-head attention mechanism, which deeply extracts multi-omics data features. Importantly, a decoupled contrastive learning model (DMACL) based on a multi-head attention mechanism is proposed to learn multi-omics data features and clusters and identify new cancer subtypes. This unsupervised contrastive learning method clusters subtypes by calculating the similarity between samples in the feature space and sample
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26469;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2306.08157</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#36827;&#34892;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#22240;&#26524;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks. (arXiv:2306.08157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26469;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#37329;&#34701;&#21644;&#25237;&#36164;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#20854;&#29420;&#29305;&#30340;&#21306;&#22359;&#38142;&#30456;&#20851;&#29305;&#24615;&#65292;&#22914;&#38544;&#31169;&#12289;&#21435;&#20013;&#24515;&#21270;&#21644;&#19981;&#21487;&#36861;&#36394;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#20854;&#21463;&#27426;&#36814;&#30340;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#27874;&#21160;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#21152;&#23494;&#36135;&#24065;&#20173;&#28982;&#26159;&#19968;&#31181;&#39640;&#39118;&#38505;&#25237;&#36164;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;DBN&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20803;&#35774;&#32622;&#19979;&#27169;&#25311;&#22797;&#26434;&#31995;&#32479;&#65292;&#20197;&#39044;&#27979;&#20116;&#31181;&#27969;&#34892;&#21152;&#23494;&#36135;&#24065;&#30340;&#20215;&#26684;&#36816;&#21160;&#26041;&#21521;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrencies have gained popularity across various sectors, especially in finance and investment. The popularity is partly due to their unique specifications originating from blockchain-related characteristics such as privacy, decentralisation, and untraceability. Despite their growing popularity, cryptocurrencies remain a high-risk investment due to their price volatility and uncertainty. The inherent volatility in cryptocurrency prices, coupled with internal cryptocurrency-related factors and external influential global economic factors makes predicting their prices and price movement directions challenging. Nevertheless, the knowledge obtained from predicting the direction of cryptocurrency prices can provide valuable guidance for investors in making informed investment decisions. To address this issue, this paper proposes a dynamic Bayesian network (DBN) approach, which can model complex systems in multivariate settings, to predict the price movement direction of five popular a
&lt;/p&gt;</description></item><item><title>C-MCTS &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23433;&#20840;&#35780;&#21028;&#22120;&#36827;&#34892;&#25104;&#26412;&#20272;&#35745;&#65292;&#24182;&#22312;&#37096;&#32626;&#26399;&#38388;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35268;&#21010;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2305.16209</link><description>&lt;p&gt;
C-MCTS: &#23433;&#20840;&#35268;&#21010;&#19982;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
C-MCTS: Safe Planning with Monte Carlo Tree Search. (arXiv:2305.16209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16209
&lt;/p&gt;
&lt;p&gt;
C-MCTS &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23433;&#20840;&#35780;&#21028;&#22120;&#36827;&#34892;&#25104;&#26412;&#20272;&#35745;&#65292;&#24182;&#22312;&#37096;&#32626;&#26399;&#38388;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35268;&#21010;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#21487;&#20197;&#35299;&#20915;&#21463;&#32422;&#26463;&#30340;&#23433;&#20840;&#20915;&#31574;&#38382;&#39064;&#12290;&#23613;&#31649;CMDP&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;MCTS&#31561;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;CMDP&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#22312;&#25104;&#26412;&#26041;&#38754;&#20445;&#23432;&#34892;&#20107;&#65292;&#36890;&#36807;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#25104;&#26412;&#20272;&#35745;&#26469;&#36991;&#20813;&#36829;&#21453;&#32422;&#26463;&#65292;&#20294;&#36825;&#31181;&#20272;&#35745;&#23384;&#22312;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;MCTS&#65288;C-MCTS&#65289;&#65292;&#23427;&#20351;&#29992;&#20808;&#21069;&#22312;&#20195;&#29702;&#37096;&#32626;&#20043;&#21069;&#36890;&#36807;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#35757;&#32451;&#30340;&#23433;&#20840;&#35780;&#21028;&#22120;&#26469;&#20272;&#35745;&#25104;&#26412;&#12290;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#35780;&#21028;&#22120;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#12290;C-MCTS&#28385;&#36275;&#25104;&#26412;&#32422;&#26463;&#65292;&#20294;&#25805;&#20316;&#25509;&#36817;&#32422;&#26463;&#36793;&#30028;&#65292;&#27604;&#20197;&#24448;&#30340;&#24037;&#20316;&#33719;&#24471;&#26356;&#39640;&#30340;&#22870;&#21169;&#12290;&#20316;&#20026;&#19968;&#20010;&#24456;&#22909;&#30340;&#21103;&#20135;&#21697;&#65292;&#36825;&#20010;&#35268;&#21010;&#22120;&#22312;&#35268;&#21010;&#27493;&#39588;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#27169;&#22411;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
The Constrained Markov Decision Process (CMDP) formulation allows to solve safety-critical decision making tasks that are subject to constraints. While CMDPs have been extensively studied in the Reinforcement Learning literature, little attention has been given to sampling-based planning algorithms such as MCTS for solving them. Previous approaches perform conservatively with respect to costs as they avoid constraint violations by using Monte Carlo cost estimates that suffer from high variance. We propose Constrained MCTS (C-MCTS), which estimates cost using a safety critic that is trained with Temporal Difference learning in an offline phase prior to agent deployment. The critic limits exploration by pruning unsafe trajectories within MCTS during deployment. C-MCTS satisfies cost constraints but operates closer to the constraint boundary, achieving higher rewards than previous work. As a nice byproduct, the planner is more efficient w.r.t. planning steps. Most importantly, under model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#35780;&#20272;&#20219;&#21153;&#65292;&#21517;&#20026;ToMChallenges&#65292;&#20197;&#25506;&#32034;&#24515;&#26234;&#29702;&#35770;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#26234;&#29702;&#35770;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#31283;&#23450;&#22320;&#25191;&#34892;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15068</link><description>&lt;p&gt;
ToMChallenges: &#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#35780;&#20272;&#20219;&#21153;&#65292;&#29992;&#20110;&#25506;&#32034;&#24515;&#26234;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind. (arXiv:2305.15068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#35780;&#20272;&#20219;&#21153;&#65292;&#21517;&#20026;ToMChallenges&#65292;&#20197;&#25506;&#32034;&#24515;&#26234;&#29702;&#35770;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#26234;&#29702;&#35770;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#31283;&#23450;&#22320;&#25191;&#34892;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#26159;&#29702;&#35299;&#19981;&#21516;&#20010;&#20307;&#24515;&#26234;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#25191;&#34892;ToM&#20219;&#21153;&#23384;&#22312;&#28608;&#28872;&#30340;&#20105;&#35758;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25552;&#31034;&#26469;&#27979;&#35797;LLMs&#19978;&#30340;ToM&#65292;&#32467;&#26524;&#19981;&#19968;&#33268;&#65306;&#19968;&#20123;&#30740;&#31350;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23637;&#31034;ToM&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#25345;&#30456;&#21453;&#35266;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToMChallenges&#65292;&#19968;&#20010;&#22522;&#20110;Sally-Anne&#21644;Smarties&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#24515;&#26234;&#29702;&#35770;&#24182;&#21253;&#21547;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#35780;&#20998;&#22120;&#26469;&#31616;&#21270;&#31572;&#26696;&#35780;&#20272;&#36807;&#31243;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19977;&#20010;&#27169;&#22411;&#65306;davinci&#12289;turbo&#21644;gpt-4&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#21644;&#38169;&#35823;&#20998;&#26512;&#26174;&#31034;&#65292;LLMs&#22312;&#25552;&#31034;&#21644;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#19981;&#19968;&#33268;&#12290;&#23545;LLMs&#26469;&#35828;&#65292;&#31283;&#23450;&#22320;&#25191;&#34892;ToM&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM), the capacity to comprehend the mental states of distinct individuals, is essential for numerous practical applications. With the development of large language models (LLMs), there is a heated debate about whether they are able to perform ToM tasks. Previous studies have used different tasks and prompts to test the ToM on LLMs and the results are inconsistent: some studies asserted these models are capable of exhibiting ToM, while others suggest the opposite. In this study, We present ToMChallenges, a dataset for comprehensively evaluating the Theory of Mind based on the Sally-Anne and Smarties tests with a diverse set of tasks. In addition, we also propose an auto-grader to streamline the answer evaluation process. We tested three models: davinci, turbo, and gpt-4. Our evaluation results and error analyses show that LLMs have inconsistent behaviors across prompts and tasks. Performing the ToM tasks robustly remains a challenge for the LLMs. In addition, our paper 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35813;&#20998;&#25968;&#25429;&#25417;&#20102;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.09875</link><description>&lt;p&gt;
GREAT&#20998;&#25968;&#65306;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models. (arXiv:2304.09875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35813;&#20998;&#25968;&#25429;&#25417;&#20102;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23545;&#20110;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#32858;&#21512;&#19968;&#32452;&#25968;&#25454;&#26679;&#26412;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#32467;&#26524;&#19978;&#65292;&#20197;&#35780;&#20272;&#21644;&#25490;&#21517;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23616;&#37096;&#32479;&#35745;&#37327;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#20195;&#34920;&#22522;&#30784;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#30495;&#27491;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;GREAT&#20998;&#25968;&#27491;&#24335;&#20855;&#26377;&#19968;&#20010;&#20840;&#23616;&#32479;&#35745;&#37327;&#30340;&#29289;&#29702;&#24847;&#20041;&#65292;&#25429;&#25417;&#26469;&#33258;&#29983;&#25104;&#27169;&#22411;&#30340;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#12290;&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26679;&#26412;&#22343;&#20540;&#19982;&#30495;&#23454;&#22343;&#20540;&#20043;&#38388;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;GREAT&#20998;&#25968;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;&#20351;&#29992;GREAT&#20998;&#25968;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#39640;&#25928;&#32780;&#19988;&#35268;&#27169;&#21487;&#25193;&#23637;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current studies on adversarial robustness mainly focus on aggregating local robustness results from a set of data samples to evaluate and rank different models. However, the local statistics may not well represent the true global robustness of the underlying unknown data distribution. To address this challenge, this paper makes the first attempt to present a new framework, called GREAT Score , for global robustness evaluation of adversarial perturbation using generative models. Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean. GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.15613</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#26131;&#26631;&#31614;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Easy Label Projection for Cross-lingual Transfer. (arXiv:2211.15613v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35757;&#32451;&#25968;&#25454;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#24050;&#25104;&#20026;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#28041;&#21450;&#36328;&#24230;&#32423;&#21035;&#27880;&#37322;&#65288;&#20363;&#22914;&#20449;&#24687;&#25552;&#21462;&#25110;&#38382;&#39064;&#22238;&#31572;&#65289;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#26631;&#31614;&#25237;&#24433;&#27493;&#39588;&#65292;&#23558;&#24050;&#27880;&#37322;&#30340;&#36328;&#24230;&#26144;&#23556;&#21040;&#32763;&#35793;&#21518;&#30340;&#25991;&#26412;&#20013;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#23545;&#36825;&#31181;&#26041;&#27861;&#19982;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#30340;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#65288;QA&#65292;NER&#21644;&#20107;&#20214;&#25552;&#21462;&#65289;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#22635;&#34917;&#25991;&#29486;&#20013;&#30340;&#37325;&#35201;&#31354;&#30333;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature. Experimental results show that our optimized version of mark-then-translate, which we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2208.03923</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#20960;&#20309;&#35282;&#24230;&#29702;&#35299;VAEs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness of VAEs through the lens of local geometry. (arXiv:2208.03923v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#36827;&#34892;&#26080;&#30417;&#30563;&#25915;&#20987;&#26102;&#65292;&#23545;&#25163;&#20250;&#25214;&#21040;&#19968;&#20010;&#36755;&#20837;&#26679;&#26412;&#20013;&#30340;&#23567;&#25200;&#21160;&#65292;&#20174;&#32780;&#26174;&#30528;&#25913;&#21464;&#20854;&#28508;&#22312;&#31354;&#38388;&#32534;&#30721;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#19968;&#20010;&#22266;&#23450;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#24050;&#30693;&#30340;&#21407;&#22240;&#26159;&#28508;&#22312;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#19982;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#12290;&#22240;&#27492;&#65292;&#36755;&#20837;&#26679;&#26412;&#20013;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#20250;&#23558;&#20854;&#32534;&#30721;&#31227;&#21160;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20302;/&#38646;&#23494;&#24230;&#21306;&#22495;&#65292;&#20174;&#32780;&#20135;&#29983;&#26080;&#38480;&#21046;&#30340;&#29983;&#25104;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;&#32534;&#30721;&#22120;&#30340;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#27979;&#37327;&#23427;&#20174;&#36755;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#30340;&#24494;&#23567;&#28508;&#22312;&#20307;&#31215;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#34987;&#35270;&#20026;&#20998;&#26512;&#36755;&#20837;&#25200;&#21160;&#23548;&#33268;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#25928;&#26524;&#30340;&#38236;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an unsupervised attack on variational autoencoders (VAEs), an adversary finds a small perturbation in an input sample that significantly changes its latent space encoding, thereby compromising the reconstruction for a fixed decoder. A known reason for such vulnerability is the distortions in the latent space resulting from a mismatch between approximated latent posterior and a prior distribution. Consequently, a slight change in an input sample can move its encoding to a low/zero density region in the latent space resulting in an unconstrained generation. This paper demonstrates that an optimal way for an adversary to attack VAEs is to exploit a directional bias of a stochastic pullback metric tensor induced by the encoder and decoder networks. The pullback metric tensor of an encoder measures the change in infinitesimal latent volume from an input to a latent space. Thus, it can be viewed as a lens to analyse the effect of input perturbations leading to latent space distortions. We
&lt;/p&gt;</description></item></channel></rss>