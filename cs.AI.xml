<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18742</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Learning Dynamics of Alignment with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#24050;&#25104;&#20026;&#23433;&#20840;&#37096;&#32626;&#27169;&#22411;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#34429;&#28982;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29702;&#35770;&#19978;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#27491;&#24335;&#23637;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36824;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#29616;&#35937;&#65292;&#21363;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#26356;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24403;&#20195;LLMs&#21644;&#23545;&#40784;&#20219;&#21153;&#19978;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24378;&#21270;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#23545;&#40784;&#26041;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#20813;&#36131;&#22768;&#26126;&#65306;&#26412;&#25991;&#21253;&#21547;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11755</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#25552;&#31034;&#33258;&#21160;&#21270;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#30340;&#25552;&#31034;&#38598;&#25104;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38646;&#26679;&#26412;&#35782;&#21035;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20026;&#20102;&#33719;&#24471;&#36825;&#20123;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#20026;LLMs&#35774;&#35745;&#25552;&#31034;&#65292;&#20197;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;VLM&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#25163;&#21160;&#32534;&#20889;&#36825;&#20123;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#65292;&#32780;&#19988;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#19982;&#24863;&#20852;&#36259;&#31867;&#21035;&#30456;&#20851;&#30340;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#20219;&#21153;&#29305;&#23450;&#39118;&#26684;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#25490;&#38500;&#22312;&#24490;&#29615;&#20043;&#22806;&#65292;&#24182;&#23436;&#20840;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#30340;&#20803;&#25552;&#31034;&#65288;MPVR&#65289;&#12290;&#20165;&#20197;&#30446;&#26631;&#20219;&#21153;&#30340;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#24418;&#24335;&#20197;&#21450;&#19968;&#31995;&#21015;&#30456;&#20851;&#31867;&#21035;&#26631;&#31614;&#20316;&#20026;&#36755;&#20837;&#65292;MPVR&#33258;&#21160;&#20135;&#29983;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#31867;&#21035;&#25552;&#31034;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11755v1 Announce Type: cross  Abstract: Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of cat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#24182;&#25552;&#20986;&#19968;&#31181;&#20351;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.10795</link><description>&lt;p&gt;
&#20174;&#21333;&#35789;&#21040;&#36335;&#24452;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
From Words to Routes: Applying Large Language Models to Vehicle Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#24182;&#25552;&#20986;&#19968;&#31181;&#20351;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65288;&#20363;&#22914;&#25805;&#20316;&#21644;&#23548;&#33322;&#65289;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20854;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#12290;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#35753;&#25105;&#20204;&#24605;&#32771;&#65306;LLMs&#22312;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;VRPs&#65289;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#19977;&#27493;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;21&#31181;&#21333;&#36710;&#25110;&#22810;&#36710;&#36335;&#24452;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#22235;&#31181;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#27599;&#31181;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#20174;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#20195;&#30721;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#23545;&#20110;GPT-4&#25928;&#26524;&#26368;&#20339;&#65292;&#23454;&#29616;&#20102;56%&#30340;&#21487;&#34892;&#24615;&#65292;40%&#30340;&#20248;&#21270;&#24615;&#21644;53%&#30340;&#25928;&#29575;&#12290;&#31532;&#19977;&#65292;&#22522;&#20110;&#35266;&#23519;&#21040;LLMs&#21487;&#33021;&#26080;&#27861;&#22312;&#21021;&#22987;&#23581;&#35797;&#20013;&#25552;&#20379;&#27491;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;LLMs&#33021;&#22815;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10795v1 Announce Type: cross  Abstract: LLMs have shown impressive progress in robotics (e.g., manipulation and navigation) with natural language task descriptions. The success of LLMs in these tasks leads us to wonder: What is the ability of LLMs to solve vehicle routing problems (VRPs) with natural language task descriptions? In this work, we study this question in three steps. First, we construct a dataset with 21 types of single- or multi-vehicle routing problems. Second, we evaluate the performance of LLMs across four basic prompt paradigms of text-to-code generation, each involving different types of text input. We find that the basic prompt paradigm, which generates code directly from natural language task descriptions, performs the best for GPT-4, achieving 56% feasibility, 40% optimality, and 53% efficiency. Third, based on the observation that LLMs may not be able to provide correct solutions at the initial attempt, we propose a framework that enables LLMs to refin
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04202</link><description>&lt;p&gt;
&#24322;&#36136;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#20013;&#36947;&#24503;&#34892;&#20026;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#26085;&#30410;&#20851;&#27880;AI&#31995;&#32479;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#30340;&#38382;&#39064;&#31361;&#26174;&#20102;&#22312;&#20154;&#24037;&#20195;&#29702;&#20013;&#23884;&#20837;&#36947;&#24503;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#32463;&#39564;&#23398;&#20064;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#22810;&#20195;&#29702;&#65288;&#31038;&#20250;&#65289;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20135;&#29983;&#22797;&#26434;&#30340;&#32676;&#20307;&#23618;&#38754;&#29616;&#35937;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#27169;&#25311;&#30340;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#26469;&#30740;&#31350;&#29420;&#31435;&#23398;&#20064;&#20195;&#29702;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#23454;&#36341;&#20013;&#20195;&#29702;&#31038;&#20250;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36947;&#24503;&#24322;&#36136;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#65292;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#38754;&#23545;&#21518;&#26524;&#20027;&#20041;&#32773;&#65288;&#21363;&#20851;&#24515;&#38543;&#26102;&#38388;&#26368;&#22823;&#21270;&#26576;&#31181;&#32467;&#26524;&#65289;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#23545;&#25163;&#65288;&#21363;&#19987;&#27880;&#20110;&#31435;&#21363;&#36981;&#23432;&#29305;&#23450;&#35268;&#33539;&#65289; &#12290;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#36947;&#24503;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;GAN&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#24120;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#20934;&#30830;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26126;&#30830;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14049</link><description>&lt;p&gt;
&#29992;&#20110;&#26497;&#31471;&#25968;&#25454;&#32553;&#25918;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Models for Extreme Downscaling of Climate Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;GAN&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#24120;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#20934;&#30830;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26126;&#30830;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#25361;&#25112;&#38656;&#35201;&#20934;&#30830;&#21644;&#39640;&#20998;&#36776;&#29575;&#22320;&#26144;&#23556;&#27668;&#20505;&#21644;&#22825;&#27668;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#21482;&#33021;&#20197;&#38750;&#24120;&#31895;&#31961;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#25552;&#20379;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#26497;&#39640;&#30340;&#35745;&#31639;&#38656;&#27714;&#25152;&#33268;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21450;&#20854;&#21464;&#20307;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#25552;&#21319;&#33258;&#28982;&#22270;&#20687;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#22312;&#25913;&#36827;&#31185;&#23398;&#25968;&#25454;&#38598;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;GAN&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#32553;&#25918;&#26041;&#27861;&#65292;&#29992;&#20110;&#26497;&#31471;&#32553;&#25918;&#32593;&#26684;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20174;&#38750;&#24120;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#20934;&#30830;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14049v1 Announce Type: cross  Abstract: Addressing the challenges of climate change requires accurate and high-resolution mapping of climate and weather variables. However, many existing climate datasets, such as the gridded outputs of the state-of-the-art numerical climate models (e.g., general circulation models), are only available at very coarse spatial resolutions due to the model complexity and extremely high computational demand. Deep-learning-based methods, particularly generative adversarial networks (GANs) and their variants, have proved effective for refining natural images, and have shown great promise in improving scientific datasets. In this paper, we describe a conditional GAN-based geospatial downscaling method for extreme downscaling of gridded climate datasets. Compared to most existing methods, the method can generate high-resolution accurate climate datasets from very low-resolution inputs. More importantly, the method explicitly considers the uncertainty
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30524;&#31185;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#36828;&#31243;&#30524;&#31185;&#21672;&#35810;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21363;&#26102;&#21453;&#39304;&#29992;&#25143;&#25293;&#25668;&#30340;&#30524;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#29992;&#25143;&#25293;&#25668;&#22270;&#20687;&#36136;&#37327;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07118</link><description>&lt;p&gt;
&#19979;&#19968;&#20195;&#36828;&#31243;&#30524;&#31185;&#35786;&#30103;&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36136;&#37327;&#35780;&#20272;&#24110;&#21161;&#36828;&#31243;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#21672;&#35810;
&lt;/p&gt;
&lt;p&gt;
Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30524;&#31185;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#36828;&#31243;&#30524;&#31185;&#21672;&#35810;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21363;&#26102;&#21453;&#39304;&#29992;&#25143;&#25293;&#25668;&#30340;&#30524;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#29992;&#25143;&#25293;&#25668;&#22270;&#20687;&#36136;&#37327;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#26126;&#21644;&#20854;&#20182;&#30524;&#37096;&#30142;&#30149;&#26159;&#20840;&#29699;&#20851;&#27880;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#21360;&#24230;&#31561;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#36828;&#31243;&#30524;&#31185;&#35786;&#30103;&#25104;&#20026;&#19968;&#31181;&#29983;&#21629;&#32447;&#65292;&#24182;&#19988;&#26234;&#33021;&#25163;&#26426;&#30524;&#37096;&#25104;&#20687;&#30340; Grabi &#38468;&#20214;&#24471;&#21040;&#20102;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#25293;&#25668;&#30340;&#22270;&#29255;&#36136;&#37327;&#24448;&#24448;&#19981;&#22815;&#22909;&#65292;&#38656;&#35201;&#21307;&#29983;&#23457;&#26680;&#24182;&#19988;&#20250;&#24310;&#35823;&#26102;&#38388;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#33021;&#22815;&#27169;&#25311;&#21307;&#29983;&#30340;&#21028;&#26029;&#24182;&#19988;&#33021;&#22815;&#21363;&#26102;&#21453;&#39304;&#65292;&#25105;&#20204;&#23545;&#24739;&#32773;&#25293;&#25668;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23558;&#22797;&#26434;&#38382;&#39064;&#23618;&#27425;&#21270;&#65292;&#25105;&#20204;&#22312;&#27492;&#35299;&#20915;&#20102;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#37096;&#20998;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27010;&#24565;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blindness and other eye diseases are a global health concern, particularly in low- and middle-income countries like India. In this regard, during the COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi attachment for smartphone-based eye imaging gained in use. However, quality of user-captured image often remained inadequate, requiring clinician vetting and delays. In this backdrop, we propose an AI-based quality assessment system with instant feedback mimicking clinicians' judgments and tested on patient-captured images. Dividing the complex problem hierarchically, here we tackle a nontrivial part, and demonstrate a proof of the concept.
&lt;/p&gt;</description></item><item><title>LiRank&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#24212;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#24314;&#27169;&#25913;&#36827;&#21644;&#25216;&#26415;&#65292;&#36890;&#36807;A/B&#27979;&#35797;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06859</link><description>&lt;p&gt;
LiRank: &#39046;&#33521;&#30340;&#24037;&#19994;&#35268;&#27169;&#25490;&#21517;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LiRank: Industrial Large Scale Ranking Models at LinkedIn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06859
&lt;/p&gt;
&lt;p&gt;
LiRank&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#24212;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#24314;&#27169;&#25913;&#36827;&#21644;&#25216;&#26415;&#65292;&#36890;&#36807;A/B&#27979;&#35797;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LiRank&#65292;&#36825;&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#23558;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#20135;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#20010;&#24314;&#27169;&#25913;&#36827;&#65292;&#21253;&#25324;Residual DCN&#65292;&#23427;&#22312;&#33879;&#21517;&#30340;DCNv2&#26550;&#26500;&#20013;&#28155;&#21152;&#20102;&#27880;&#24847;&#21147;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#23558;SOTA&#26550;&#26500;&#32452;&#21512;&#21644;&#35843;&#20248;&#20197;&#21019;&#24314;&#32479;&#19968;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;Dense Gating&#12289;Transformers&#21644;Residual DCN&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;&#26657;&#20934;&#30340;&#26032;&#25216;&#26415;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25506;&#32034;/&#21033;&#29992;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#20135;&#29615;&#22659;&#12290;&#20026;&#20102;&#23454;&#29616;&#22823;&#35268;&#27169;&#25490;&#21517;&#27169;&#22411;&#30340;&#26377;&#25928;&#12289;&#29983;&#20135;&#32423;&#26381;&#21153;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20351;&#29992;&#37327;&#21270;&#21644;&#35789;&#27719;&#21387;&#32553;&#35757;&#32451;&#21644;&#21387;&#32553;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;Feed&#25490;&#21517;&#12289;&#32844;&#20301;&#25512;&#33616;&#21644;&#24191;&#21578;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#31561;&#22823;&#35268;&#27169;&#20351;&#29992;&#26696;&#20363;&#30340;&#37096;&#32626;&#35774;&#32622;&#32454;&#33410;&#12290;&#36890;&#36807;&#38416;&#26126;&#26368;&#26377;&#25928;&#30340;&#25216;&#26415;&#26041;&#27861;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#21508;&#31181;A/B&#27979;&#35797;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LiRank, a large-scale ranking framework at LinkedIn that brings to production state-of-the-art modeling architectures and optimization methods. We unveil several modeling improvements, including Residual DCN, which adds attention and residual connections to the famous DCNv2 architecture. We share insights into combining and tuning SOTA architectures to create a unified model, including Dense Gating, Transformers and Residual DCN. We also propose novel techniques for calibration and describe how we productionalized deep learning based explore/exploit methods. To enable effective, production-grade serving of large ranking models, we detail how to train and compress models using quantization and vocabulary compression. We provide details about the deployment setup for large-scale use cases of Feed ranking, Jobs Recommendations, and Ads click-through rate (CTR) prediction. We summarize our learnings from various A/B tests by elucidating the most effective technical approaches. T
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24773;&#24863;&#24605;&#32500;&#38142;&#65288;ECoT&#65289;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#24773;&#24863;&#26234;&#24935;&#20934;&#21017;&#23545;&#40784;&#65292;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.06836</link><description>&lt;p&gt;
&#36890;&#36807;&#24773;&#32490;&#24605;&#32500;&#38142;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#32490;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24773;&#24863;&#24605;&#32500;&#38142;&#65288;ECoT&#65289;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#24773;&#24863;&#26234;&#24935;&#20934;&#21017;&#23545;&#40784;&#65292;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#32490;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#23545;&#25506;&#32034;&#23427;&#20204;&#22312;&#24773;&#24863;&#26234;&#33021;&#20013;&#28508;&#21147;&#30340;&#22909;&#22855;&#24515;&#12290;&#28982;&#32780;&#65292;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#39046;&#22495;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#21644;&#24773;&#24863;&#29983;&#25104;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24773;&#24863;&#24605;&#32500;&#38142;&#65288;ECoT&#65289;&#30340;&#21363;&#25554;&#21363;&#29992;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#24773;&#24863;&#26234;&#21147;&#25351;&#21335;&#23545;&#40784;&#26469;&#22686;&#24378;LLMs&#22312;&#21508;&#31181;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#35780;&#20272;ECoT&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24773;&#24863;&#29983;&#25104;&#24471;&#20998;&#65288;EGS&#65289;&#30340;&#33258;&#21160;&#21270;&#22522;&#20110;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;EGS&#23558;&#25096;&#23572;&#26364;&#30340;&#24773;&#32490;&#26234;&#21147;&#29702;&#35770;&#20316;&#20026;&#20154;&#31867;&#19987;&#23478;&#20849;&#35782;&#65292;&#20026;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06836v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have shown remarkable performance in various emotion recognition tasks, thereby piquing the research community's curiosity for exploring their potential in emotional intelligence. However, several issues in the field of emotional generation tasks remain unresolved, including human preference alignment and emotional generation assessment. In this paper, we propose the Emotional Chain-of-Thought (ECoT), a plug-and-play prompting method that enhances the performance of LLMs on various emotional generation tasks by aligning with human emotional intelligence guidelines. To assess the reliability of ECoT, we propose an automated model-based evaluation method called Emotional Generation Score (EGS). EGS incorporates Goleman's Emotional Intelligence Theory as a consensus of human experts, providing a new perspective on the evaluation of emotional generation tasks. Extensive experimental results demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#65292;&#32780;&#22686;&#21152;&#30340;&#35757;&#32451;&#21442;&#25968;&#24456;&#23569;&#12290;</title><link>https://arxiv.org/abs/2310.05707</link><description>&lt;p&gt;
&#29992;&#35268;&#21010;&#26631;&#35760;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Guiding Language Model Math Reasoning with Planning Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#65292;&#32780;&#22686;&#21152;&#30340;&#35757;&#32451;&#21442;&#25968;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26469;&#22240;&#20854;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#65288;&#22914;&#24605;&#32500;&#38142;&#25512;&#29702;&#65289;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22686;&#24378;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#24573;&#35270;&#20102;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#32467;&#26500;&#21270;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;LLMs&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#20010;&#21035;&#25512;&#29702;&#27493;&#39588;&#65292;&#20294;&#22312;&#25972;&#20010;&#25512;&#29702;&#38142;&#19978;&#20445;&#25345;&#19968;&#33268;&#24615;&#26041;&#38754;&#21364;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#24320;&#22987;&#22788;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#65292;&#20316;&#20026;&#27169;&#22411;&#30340;&#24341;&#23548;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#23884;&#20837;&#28155;&#21152;&#21040;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#22686;&#21152;&#38750;&#24120;&#23567;&#65288;&#20165;&#20026;0.001%&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#23436;&#20840;&#24494;&#35843;&#25110;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#26041;&#26696;&#26469;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#31181;&#19981;&#21516;&#30340;LLMs&#65292;&#22312;&#19977;&#20010;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#26041;&#27861;&#65292;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce planning tokens at the start of each reasoning step, serving as a guide for the model, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. standard f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#8220;&#25105;&#26159;&#19968;&#20010;&#22855;&#24618;&#30340;&#25968;&#25454;&#38598;&#8221;&#65292;&#29992;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#22788;&#29702;&#20803;&#35821;&#35328;&#33258;&#25351;&#30340;&#38472;&#36848;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#22312;&#29983;&#25104;&#21644;&#39564;&#35777;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#37117;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.05300</link><description>&lt;p&gt;
&#25105;&#26159;&#19968;&#20010;&#22855;&#24618;&#30340;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#20803;&#35821;&#35328;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
I am a Strange Dataset: Metalinguistic Tests for Language Models. (arXiv:2401.05300v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#8220;&#25105;&#26159;&#19968;&#20010;&#22855;&#24618;&#30340;&#25968;&#25454;&#38598;&#8221;&#65292;&#29992;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#22788;&#29702;&#20803;&#35821;&#35328;&#33258;&#25351;&#30340;&#38472;&#36848;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#22312;&#29983;&#25104;&#21644;&#39564;&#35777;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#37117;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#28041;&#21450;&#20803;&#35821;&#35328;&#33258;&#25351;&#30340;&#38472;&#36848;&#65288;&#8220;&#26412;&#35770;&#25991;&#26377;&#20845;&#20010;&#37096;&#20998;&#12290;&#8221;&#65289;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#22788;&#29702;&#36825;&#26679;&#30340;&#35821;&#35328;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#8220;&#25105;&#26159;&#19968;&#20010;&#22855;&#24618;&#30340;&#25968;&#25454;&#38598;&#8221;&#65292;&#29992;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#21253;&#21547;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#20250;&#32487;&#32493;&#31867;&#20284;&#20110;&#8220;&#36825;&#20010;&#21477;&#23376;&#20013;&#20498;&#25968;&#31532;&#20108;&#20010;&#35789;&#26159;&#8221;&#30340;&#38472;&#36848;&#65288;&#27491;&#30830;&#30340;&#32487;&#32493;&#24212;&#35813;&#26159;&#8220;&#26159;&#8221;&#65289;&#12290;&#22312;&#39564;&#35777;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#20250;&#21028;&#26029;&#31867;&#20284;&#20110;&#8220;&#36825;&#20010;&#21477;&#23376;&#20013;&#20498;&#25968;&#31532;&#20108;&#20010;&#35789;&#26159;&#21477;&#23376;&#12290;&#8221;&#30340;&#38472;&#36848;&#30340;&#30495;&#23454;&#24615;&#65288;&#26159;&#20551;&#30340;&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26368;&#23567;&#24046;&#24322;&#30340;&#38750;&#33258;&#25351;&#20803;&#35821;&#35328;&#31034;&#20363;&#65292;&#26469;&#34917;&#20805;&#20027;&#25968;&#25454;&#38598;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22788;&#29702;&#20803;&#35821;&#35328;&#35821;&#35328;&#12290;&#25968;&#25454;&#38598;&#30001;&#19987;&#23478;&#25163;&#24037;&#21046;&#20316;&#65292;&#38750;&#19987;&#23478;&#26631;&#27880;&#21592;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#24320;&#28304;LLMs&#65288;&#20174;7B&#21040;70B&#30340;&#21442;&#25968;&#65289;&#20197;&#21450;&#36890;&#36807;API&#36827;&#34892;&#27979;&#35797;&#30340;&#38381;&#28304;LLMs&#12290;&#25152;&#26377;&#27169;&#22411;&#22312;&#20004;&#20010;&#23376;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#37117;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statements involving metalinguistic self-reference ("This paper has six sections.") are prevalent in many domains. Can large language models (LLMs) handle such language? In this paper, we present "I am a Strange Dataset", a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like "The penultimate word in this sentence is" (where a correct continuation is "is"). In verification, models judge the truth of statements like "The penultimate word in this sentence is sentence." (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and eve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;</title><link>http://arxiv.org/abs/2312.01185</link><description>&lt;p&gt;
&#26102;&#38388;&#20013;&#30340;&#28063;&#28458;&#65306;&#32654;&#22269;&#21382;&#21490;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22269;&#24773;&#21672;&#25991;&#25968;&#25454;&#38598;&#23545;&#32654;&#22269;&#21382;&#21490;&#30340;&#24635;&#20307;&#26102;&#38388;&#32447;&#21450;&#21672;&#25991;&#26412;&#36523;&#30340;&#29305;&#28857;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#65288;&#20063;&#26377;&#20123;&#19981;&#37027;&#20040;&#20196;&#20154;&#24778;&#35766;&#65289;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#65292;&#22914;BERT&#65288;DistilBERT&#65289;&#21644;GPT-2&#12290;&#34429;&#28982;&#24191;&#27867;&#35748;&#20026;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#26368;&#36866;&#21512;NLP&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;GPT-2&#32467;&#21512;UMAP&#31561;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#26356;&#24378;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#36825;&#20351;&#24471;GPT-2 + UMAP&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#23601;&#36275;&#22815;&#22909;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#26469;&#26816;&#27979;&#21738;&#20301;&#24635;&#32479;&#21457;&#34920;&#20102;&#21738;&#31687;&#28436;&#35762;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#65288;&#20934;&#30830;&#29575;&#20026;93\% - 95\%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#36816;&#34892;&#24773;&#20917;&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#20889;&#20316;&#24180;&#20221;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#20840;&#23616;&#25628;&#32034;&#30340;&#31639;&#27861;&#65288;UMCTS&#65289;&#29992;&#20110;&#26689;&#26550;&#32467;&#26500;&#23610;&#23544;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#26356;&#26032;&#36807;&#31243;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#20197;&#21450;&#20351;&#29992;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#26469;&#33719;&#24471;&#21512;&#36866;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.06045</link><description>&lt;p&gt;
&#22522;&#20110;&#21551;&#21457;&#24335;&#20840;&#23616;&#25628;&#32034;&#30340;&#26689;&#26550;&#32467;&#26500;&#23610;&#23544;&#20248;&#21270;&#38382;&#39064;&#30340;&#25913;&#36827;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;UMCTS&#65289;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Update Monte Carlo tree search (UMCTS) algorithm for heuristic global search of sizing optimization problems for truss structures. (arXiv:2309.06045v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#20840;&#23616;&#25628;&#32034;&#30340;&#31639;&#27861;&#65288;UMCTS&#65289;&#29992;&#20110;&#26689;&#26550;&#32467;&#26500;&#23610;&#23544;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#26356;&#26032;&#36807;&#31243;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#20197;&#21450;&#20351;&#29992;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#26469;&#33719;&#24471;&#21512;&#36866;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26689;&#26550;&#32467;&#26500;&#23610;&#23544;&#20248;&#21270;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36866;&#29992;&#20110;&#22788;&#29702;&#26080;&#26799;&#24230;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#20248;&#21270;&#31639;&#27861;&#8212;&#8212;&#26356;&#26032;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;UMCTS&#65289;&#65292;&#29992;&#20110;&#33719;&#24471;&#21512;&#36866;&#30340;&#26689;&#26550;&#32467;&#26500;&#35774;&#35745;&#12290;UMCTS&#26159;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#23558;&#26032;&#39062;&#30340;&#26356;&#26032;&#36807;&#31243;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#19982;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#30456;&#32467;&#21512;&#12290;&#26356;&#26032;&#36807;&#31243;&#24847;&#21619;&#30528;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#27599;&#20010;&#26500;&#20214;&#30340;&#26368;&#20339;&#25130;&#38754;&#31215;&#36890;&#36807;&#25628;&#32034;&#26641;&#30830;&#23450;&#65292;&#20854;&#21021;&#22987;&#29366;&#24577;&#26159;&#19978;&#19968;&#36718;&#30340;&#26368;&#32456;&#29366;&#24577;&#12290;&#22312;UMCTS&#31639;&#27861;&#20013;&#65292;&#24341;&#20837;&#20102;&#21152;&#36895;&#36873;&#25321;&#25104;&#21592;&#38754;&#31215;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#21152;&#36895;&#22120;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27599;&#20010;&#29366;&#24577;&#65292;&#24179;&#22343;&#22870;&#21169;&#34987;&#26368;&#20339;&#22870;&#21169;&#30340;&#27169;&#25311;&#36807;&#31243;&#20013;&#25910;&#38598;&#26469;&#30340;&#22870;&#21169;&#26367;&#20195;&#65292;&#30830;&#23450;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26356;&#26032;&#36807;&#31243;&#21644;MCTS&#65292;&#20197;&#21450;&#20351;&#29992;UCB&#26469;&#20248;&#21270;&#26689;&#26550;&#32467;&#26500;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sizing optimization of truss structures is a complex computational problem, and the reinforcement learning (RL) is suitable for dealing with multimodal problems without gradient computations. In this paper, a new efficient optimization algorithm called update Monte Carlo tree search (UMCTS) is developed to obtain the appropriate design for truss structures. UMCTS is an RL-based method that combines the novel update process and Monte Carlo tree search (MCTS) with the upper confidence bound (UCB). Update process means that in each round, the optimal cross-sectional area of each member is determined by search tree, and its initial state is the final state in the previous round. In the UMCTS algorithm, an accelerator for the number of selections for member area and iteration number is introduced to reduce the computation time. Moreover, for each state, the average reward is replaced by the best reward collected on the simulation process to determine the optimal solution. The proposed optim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#65292;&#20998;&#26512;&#24182;&#21387;&#32553;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15068</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Augmentation Framework for Anomaly Detection. (arXiv:2308.15068v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#65292;&#20998;&#26512;&#24182;&#21387;&#32553;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36890;&#24120;&#34987;&#25972;&#21512;&#21040;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#30340;&#24322;&#24120;&#25110;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#24322;&#24120;&#30340;&#26631;&#20934;&#22312;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#20998;&#24067;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#37325;&#26500;&#32593;&#32476;&#35757;&#32451;&#26377;&#36129;&#29486;&#30340;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#21387;&#32553;&#25104;&#20960;&#31181;&#26041;&#27861;&#65292;&#20174;&#32780;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#23558;&#36825;&#20010;&#26694;&#26550;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#26082;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#21448;&#36991;&#20813;&#23545;&#37325;&#26500;&#36807;&#31243;&#24341;&#20837;&#24178;&#25200;&#12290;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#30446;&#26631;&#30456;&#20851;&#25351;&#26631;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation methods are commonly integrated into the training of anomaly detection models. Previous approaches have primarily focused on replicating real-world anomalies or enhancing diversity, without considering that the standard of anomaly varies across different classes, potentially leading to a biased training distribution.This paper analyzes crucial traits of simulated anomalies that contribute to the training of reconstructive networks and condenses them into several methods, thus creating a comprehensive framework by selectively utilizing appropriate combinations.Furthermore, we integrate this framework with a reconstruction-based approach and concurrently propose a split training strategy that alleviates the issue of overfitting while avoiding introducing interference to the reconstruction process. The evaluations conducted on the MVTec anomaly detection dataset demonstrate that our method outperforms the previous state-of-the-art approach, particularly in terms of objec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.12754</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#22312;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#36890;&#36807;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Linear Feature Learning in Regression Through Regularisation. (arXiv:2307.12754v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;&#29305;&#24449;&#36873;&#25321;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#65292;&#38750;&#21442;&#25968;&#26041;&#27861;&#24120;&#24120;&#24456;&#38590;&#24212;&#23545;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#30456;&#20851;&#20449;&#24687;&#23384;&#22312;&#20110;&#25968;&#25454;&#30340;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#65292;&#21363;&#22810;&#25351;&#25968;&#27169;&#22411;&#12290;&#22914;&#26524;&#24050;&#30693;&#35813;&#23376;&#31354;&#38388;&#65292;&#23558;&#22823;&#22823;&#22686;&#24378;&#39044;&#27979;&#12289;&#35745;&#31639;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#39044;&#27979;&#30340;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#20272;&#35745;&#39044;&#27979;&#20989;&#25968;&#21644;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24182;&#21152;&#19978;&#20989;&#25968;&#23548;&#25968;&#30340;&#24809;&#32602;&#39033;&#65292;&#20197;&#20445;&#35777;&#20854;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;Hermite&#22810;&#39033;&#24335;&#30340;&#27491;&#20132;&#24615;&#21644;&#26059;&#36716;&#19981;&#21464;&#24615;&#29305;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;RegFeaL&#12290;&#36890;&#36807;&#21033;&#29992;&#26367;&#20195;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#26059;&#36716;&#25968;&#25454;&#20197;&#25913;&#21892;&#19982;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;POMDP&#20540;&#36845;&#20195;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#20256;&#32479;&#31526;&#21495;&#25216;&#26415;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#29366;&#24577;&#32622;&#20449;&#24230;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25240;&#25187;&#32047;&#31215;&#22238;&#25253;&#30340;&#36830;&#32493;&#29366;&#24577;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17639</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;POMDP&#20540;&#36845;&#20195;
&lt;/p&gt;
&lt;p&gt;
Point-based Value Iteration for Neuro-Symbolic POMDPs. (arXiv:2306.17639v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;POMDP&#20540;&#36845;&#20195;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#20256;&#32479;&#31526;&#21495;&#25216;&#26415;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#29366;&#24577;&#32622;&#20449;&#24230;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25240;&#25187;&#32047;&#31215;&#22238;&#25253;&#30340;&#36830;&#32493;&#29366;&#24577;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26159;&#32467;&#21512;&#20256;&#32479;&#31526;&#21495;&#25216;&#26415;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#26412;&#25991;&#32771;&#34385;&#20854;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#31526;&#21495;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;NS-POMDPs&#65289;&#65292;&#35813;&#27169;&#22411;&#25551;&#36848;&#20102;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#24863;&#30693;&#36830;&#32493;&#29366;&#24577;&#29615;&#22659;&#24182;&#36827;&#34892;&#31526;&#21495;&#20915;&#31574;&#30340;&#20195;&#29702;&#65292;&#24182;&#30740;&#31350;&#20102;&#20248;&#21270;&#25240;&#25187;&#32047;&#31215;&#22238;&#25253;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36830;&#32493;&#29366;&#24577;&#32622;&#20449;&#24230;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#27573;&#32447;&#24615;&#21644;&#20984;&#34920;&#31034;&#65288;P-PWLC&#65289;&#65292;&#36890;&#36807;&#35206;&#30422;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;&#22810;&#38754;&#20307;&#21644;&#20540;&#21521;&#37327;&#23454;&#29616;&#65292;&#24182;&#23558;Bellman backups&#25193;&#23637;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20540;&#20989;&#25968;&#30340;&#20984;&#24615;&#21644;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20540;&#36845;&#20195;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36830;&#32493;&#29366;&#24577;&#27169;&#22411;&#21644;&#31070;&#32463;&#24863;&#30693;&#26426;&#21046;&#30340;&#24213;&#23618;&#32467;&#26500;&#26469;&#20445;&#35777;&#26377;&#38480;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic artificial intelligence is an emerging area that combines traditional symbolic techniques with neural networks. In this paper, we consider its application to sequential decision making under uncertainty. We introduce neuro-symbolic partially observable Markov decision processes (NS-POMDPs), which model an agent that perceives a continuous-state environment using a neural network and makes decisions symbolically, and study the problem of optimising discounted cumulative rewards. This requires functions over continuous-state beliefs, for which we propose a novel piecewise linear and convex representation (P-PWLC) in terms of polyhedra covering the continuous-state space and value vectors, and extend Bellman backups to this representation. We prove the convexity and continuity of value functions and present two value iteration algorithms that ensure finite representability by exploiting the underlying structure of the continuous-state model and the neural perception mechani
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18381</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#30719;&#30707;&#20013;&#25552;&#28860;&#40644;&#37329;: &#22522;&#20110;&#20851;&#38190;&#26679;&#26412;&#36873;&#25321;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18381
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25928;&#29575;&#23398;&#20064;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25317;&#26377;&#22823;&#37327;&#22810;&#27169;&#22411;&#30340;&#29616;&#22312;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#25104;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#36807;&#31243;&#26412;&#36523;&#20173;&#28982;&#38750;&#24120;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#29992;&#20449;&#24687;&#29702;&#35770;&#26469;&#24314;&#27169;&#33976;&#39311;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#25454;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20197;&#20415;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26679;&#26412;&#36873;&#25321;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#20248;&#21270;&#36807;&#31243;&#12290;&#36825;&#31181;&#26032;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#25193;&#22823;&#29616;&#26377;&#31639;&#27861;&#33539;&#22260;&#20197;&#23545;&#26356;&#24222;&#22823;&#21644;&#22810;&#20803;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#20363;&#22914;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;0.04&#65285;&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#20445;&#25345;&#21487;&#27604;&#30340;&#33976;&#39311;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20854;&#36129;&#29486;&#21487;&#33021;&#20026;&#33976;&#39311;&#36807;&#31243;&#30340;&#21160;&#21147;&#23398;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;ASR&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#31561;&#25928;&#21442;&#25968;&#36716;&#25442;&#23454;&#29616;&#19981;&#21516;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#20114;&#36716;&#12290;&#21644;&#29616;&#26377;&#30340;SRP&#26041;&#27861;&#30456;&#27604;&#65292;ASR&#21487;&#20197;&#25104;&#21151;&#32771;&#34385;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#23454;&#29616;&#25512;&#29702;&#26399;&#38388;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.06345</link><description>&lt;p&gt;
ASR: &#20687;&#27880;&#24847;&#21147;&#19968;&#26679;&#30340;&#32467;&#26500;&#20877;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
ASR: Attention-alike Structural Re-parameterization. (arXiv:2304.06345v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06345
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;ASR&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#31561;&#25928;&#21442;&#25968;&#36716;&#25442;&#23454;&#29616;&#19981;&#21516;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#20114;&#36716;&#12290;&#21644;&#29616;&#26377;&#30340;SRP&#26041;&#27861;&#30456;&#27604;&#65292;ASR&#21487;&#20197;&#25104;&#21151;&#32771;&#34385;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#23454;&#29616;&#25512;&#29702;&#26399;&#38388;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#20877;&#21442;&#25968;&#21270;&#65288;SRP&#65289;&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#31561;&#25928;&#21442;&#25968;&#36716;&#25442;&#23454;&#29616;&#19981;&#21516;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#20114;&#36716;&#12290;&#35813;&#25216;&#26415;&#20351;&#24471;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36890;&#36807;&#36825;&#20123;&#36716;&#25442;&#20943;&#23569;&#24615;&#33021;&#25552;&#21319;&#30340;&#26032;&#22686;&#20195;&#20215;&#65292;&#20363;&#22914;&#21442;&#25968;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#22240;&#27492;SRP&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;SRP&#26041;&#27861;&#24050;&#25104;&#21151;&#32771;&#34385;&#20102;&#35768;&#22810;&#24120;&#29992;&#30340;&#26550;&#26500;&#65292;&#20363;&#22914;&#24402;&#19968;&#21270;&#12289;&#27744;&#21270;&#26041;&#27861;&#12289;&#22810;&#20998;&#25903;&#21367;&#31215;&#31561;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#33258;&#27880;&#24847;&#27169;&#22359;&#30001;&#20110;&#22312;&#25512;&#29702;&#26399;&#38388;&#36890;&#24120;&#20197;&#20056;&#27861;&#26041;&#24335;&#20316;&#29992;&#20110;&#39592;&#24178;&#32593;&#32476;&#24182;&#19988;&#27169;&#22359;&#30340;&#36755;&#20986;&#22312;&#25512;&#29702;&#26102;&#20381;&#36182;&#20110;&#36755;&#20837;&#65292;&#25152;&#20197;&#26080;&#27861;&#30452;&#25509;&#23454;&#29616;SRP&#65292;&#32780;&#36825;&#38480;&#21046;&#20102;SRP&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32479;&#35745;&#35282;&#24230;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
The structural re-parameterization (SRP) technique is a novel deep learning technique that achieves interconversion between different network architectures through equivalent parameter transformations. This technique enables the mitigation of the extra costs for performance improvement during training, such as parameter size and inference time, through these transformations during inference, and therefore SRP has great potential for industrial and practical applications. The existing SRP methods have successfully considered many commonly used architectures, such as normalizations, pooling methods, multi-branch convolution. However, the widely used self-attention modules cannot be directly implemented by SRP due to these modules usually act on the backbone network in a multiplicative manner and the modules' output is input-dependent during inference, which limits the application scenarios of SRP. In this paper, we conduct extensive experiments from a statistical perspective and discover
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.11760</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Fingerprinting Generative Adversarial Networks. (arXiv:2106.11760v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#30001;&#20110;&#21830;&#19994;GAN&#30340;&#29983;&#20135;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20154;&#21147;&#36164;&#28304;&#65292;&#22240;&#27492;&#36843;&#20999;&#38656;&#35201;&#29256;&#26435;&#20445;&#25252;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#12290;&#25105;&#20204;&#31361;&#30772;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#25152;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21019;&#36896;&#24615;&#22320;&#20174;&#30446;&#26631;GAN&#21644;&#20998;&#31867;&#22120;&#26500;&#24314;&#19968;&#20010;&#22797;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#36825;&#20010;&#22797;&#21512;&#27169;&#22411;&#20013;&#20135;&#29983;&#25351;&#32441;&#26679;&#26412;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#29256;&#26435;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#26696;&#21551;&#21457;&#20102;&#19968;&#20123;&#20855;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#25152;&#38656;&#35201;&#30340;&#19981;&#21516;&#23433;&#20840;&#35201;&#27714;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#35777;&#26126;&#35813;&#26041;&#26696;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have been widely used in various application scenarios. Since the production of a commercial GAN requires substantial computational and human resources, the copyright protection of GANs is urgently needed. In this paper, we present the first fingerprinting scheme for the Intellectual Property (IP) protection of GANs. We break through the stealthiness and robustness bottlenecks suffered by previous fingerprinting methods for classification models being naively transferred to GANs. Specifically, we innovatively construct a composite deep learning model from the target GAN and a classifier. Then we generate fingerprint samples from this composite model, and embed them in the classifier for effective ownership verification. This scheme inspires some concrete methodologies to practically protect the modern GAN models. Theoretical analysis proves that these methods can satisfy different security requirements necessary for IP protection. We also conduct 
&lt;/p&gt;</description></item></channel></rss>