<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35782;&#21035;&#37027;&#20123;&#22312;&#24494;&#35843;&#21518;&#26356;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26377;&#23475;&#35831;&#27714;&#30340;&#21709;&#24212;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.01099</link><description>&lt;p&gt;
&#20320;&#30340;&#8220;&#23433;&#20840;&#8221;&#25968;&#25454;&#20013;&#26377;&#20160;&#20040;&#65311;&#65306;&#35782;&#21035;&#30772;&#22351;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01099
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35782;&#21035;&#37027;&#20123;&#22312;&#24494;&#35843;&#21518;&#26356;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26377;&#23475;&#35831;&#27714;&#30340;&#21709;&#24212;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21363;&#20351;&#32463;&#36807;&#35843;&#25972;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#20063;&#23481;&#26131;&#34987;&#36234;&#29425;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#21482;&#26159;&#36827;&#19968;&#27493;&#20351;&#29992;&#33391;&#24615;&#25968;&#25454;&#65288;&#21363;&#27809;&#26377;&#26377;&#23475;&#20869;&#23481;&#30340;&#25968;&#25454;&#65289;&#23545;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20250;&#23548;&#33268;&#23433;&#20840;&#24615;&#22823;&#24133;&#19979;&#38477;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#33391;&#24615;&#24494;&#35843;&#19981;&#32463;&#24847;&#38388;&#23548;&#33268;&#36234;&#29425;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#35270;&#35282;&#34920;&#24449;&#24494;&#35843;&#25968;&#25454;&#65306;&#34920;&#31034;&#21644;&#26799;&#24230;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#26377;&#23475;&#31034;&#20363;&#24182;&#36828;&#31163;&#33391;&#24615;&#31034;&#20363;&#30340;&#25968;&#25454;&#28857;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#26356;&#26377;&#21487;&#33021;&#22312;&#24494;&#35843;&#21518;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#12290;&#20165;&#20165;&#35757;&#32451;100&#20010;&#36825;&#20123;&#30475;&#20284;&#33391;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#23601;&#21487;&#20197;&#20351;&#24494;&#35843;&#27169;&#22411;&#32943;&#23450;&#22320;&#22238;&#24212;&#36229;&#36807;70&#65285;&#30340;&#34987;&#27979;&#35797;&#30340;&#26377;&#23475;&#35831;&#27714;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01099v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to &gt; 70% of tested harmful requests, compared to &lt;
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoSumm&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30446;&#26631;&#23454;&#29616;&#20102;&#26080;&#21442;&#32771;&#25688;&#35201;&#30340;&#31934;&#28860;&#29983;&#25104;&#22120;</title><link>https://arxiv.org/abs/2403.13780</link><description>&lt;p&gt;
&#26080;&#21442;&#32771;&#25688;&#35201;&#30340;&#20449;&#24687;&#35770;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Distillation for Reference-less Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13780
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoSumm&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30446;&#26631;&#23454;&#29616;&#20102;&#26080;&#21442;&#32771;&#25688;&#35201;&#30340;&#31934;&#28860;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33258;&#21160;&#25688;&#35201;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#19987;&#26377;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#65292;&#25110;&#32773;&#20174;&#23427;&#20204;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoSumm&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30446;&#26631;&#36827;&#34892;&#31934;&#28860;&#24378;&#22823;&#30340;&#25688;&#35201;&#29983;&#25104;&#22120;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;LLM&#30340;&#33021;&#21147;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13780v1 Announce Type: new  Abstract: The current winning recipe for automatic summarization is using proprietary large-scale language models (LLMs) such as ChatGPT as is, or imitation learning from them as teacher models. While increasingly ubiquitous dependence on such large-scale language models is convenient, there remains an important question of whether small-scale models could have achieved competitive results, if we were to seek an alternative learning method -- that allows for a more cost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a novel framework to distill a powerful summarizer based on the information-theoretic objective for summarization, without relying on either the LLM's capability or human-written references. To achieve this, we first propose a novel formulation of the desiderata of summarization (saliency, faithfulness and brevity) through the lens of mutual information between the original document and the summary. Based on thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#22797;&#27979;&#37327;&#30340;&#35270;&#35273;&#27169;&#25311;&#37327;&#34920;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21709;&#24212;&#39118;&#26684;&#65288;RP&#65289;&#34920;&#24449;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;VAS&#20013;&#22788;&#29702;RP&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.10136</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;&#27169;&#25311;&#37327;&#34920;&#23545;&#37325;&#22797;&#27979;&#37327;&#30340;&#21709;&#24212;&#39118;&#26684;&#36827;&#34892;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Response Style Characterization for Repeated Measures Using the Visual Analogue Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#22797;&#27979;&#37327;&#30340;&#35270;&#35273;&#27169;&#25311;&#37327;&#34920;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21709;&#24212;&#39118;&#26684;&#65288;RP&#65289;&#34920;&#24449;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;VAS&#20013;&#22788;&#29702;RP&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#25253;&#21578;&#27979;&#37327;&#65288;&#20363;&#22914;&#65292;&#21033;&#20811;&#29305;&#37327;&#34920;&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#20027;&#35266;&#20581;&#24247;&#24863;&#30693;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#31934;&#30830;&#19988;&#20415;&#20110;&#35780;&#20272;&#20154;&#20204;&#24863;&#21463;&#30340;&#33021;&#21147;&#65292;&#35270;&#35273;&#27169;&#25311;&#37327;&#34920;&#65288;VAS&#65289;&#65292;&#19968;&#31181;&#28369;&#21160;&#26465;&#37327;&#34920;&#65292;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#20250;&#21463;&#21040;&#21709;&#24212;&#39118;&#26684;&#65288;RS&#65289;&#30340;&#24433;&#21709;&#65292;RS&#26159;&#19968;&#31181;&#29992;&#25143;&#20381;&#36182;&#30340;&#31995;&#32479;&#24615;&#20542;&#21521;&#65292;&#26080;&#35770;&#38382;&#21367;&#35828;&#26126;&#22914;&#20309;&#37117;&#20250;&#21457;&#29983;&#12290;&#23613;&#31649;&#22312;&#20010;&#20307;&#38388;&#20998;&#26512;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#20294;&#23545;VAS&#20013;RS&#65288;&#34920;&#31034;&#20026;&#21709;&#24212;&#21078;&#38754;&#65288;RP&#65289;&#65289;&#30340;&#22788;&#29702;&#24182;&#26410;&#21463;&#21040;&#36275;&#22815;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20027;&#35201;&#29992;&#20110;&#20010;&#20307;&#20869;&#30417;&#27979;&#19988;&#19981;&#22826;&#21463;RP&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;VAS&#27979;&#37327;&#36890;&#24120;&#38656;&#35201;&#23545;&#21516;&#19968;&#38382;&#21367;&#39033;&#30446;&#36827;&#34892;&#37325;&#22797;&#33258;&#25105;&#25253;&#21578;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#22312;&#21033;&#20811;&#29305;&#37327;&#34920;&#19978;&#24212;&#29992;&#20256;&#32479;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RP&#34920;&#24449;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#37325;&#22797;&#27979;&#37327;&#30340;VAS&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10136v1 Announce Type: cross  Abstract: Self-report measures (e.g., Likert scales) are widely used to evaluate subjective health perceptions. Recently, the visual analog scale (VAS), a slider-based scale, has become popular owing to its ability to precisely and easily assess how people feel. These data can be influenced by the response style (RS), a user-dependent systematic tendency that occurs regardless of questionnaire instructions. Despite its importance, especially in between-individual analysis, little attention has been paid to handling the RS in the VAS (denoted as response profile (RP)), as it is mainly used for within-individual monitoring and is less affected by RP. However, VAS measurements often require repeated self-reports of the same questionnaire items, making it difficult to apply conventional methods on a Likert scale. In this study, we developed a novel RP characterization method for various types of repeatedly measured VAS data. This approach involves t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#12289;&#24182;&#21457;&#39044;&#27979;&#21644;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.06906</link><description>&lt;p&gt;
&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22312;&#32771;&#34385;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#25512;&#36831;&#22810;&#20301;&#19987;&#23478;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06906
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#12289;&#24182;&#21457;&#39044;&#27979;&#21644;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25512;&#36831;&#65288;L2D&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#31995;&#32479;&#20013;&#23558;&#20915;&#31574;&#25512;&#36831;&#32473;&#20154;&#31867;&#65292;&#20174;&#32780;&#22312;&#20154;&#31867;&#26356;&#26377;&#21487;&#33021;&#27491;&#30830;&#26102;&#25512;&#36831;&#20915;&#31574;&#12290;&#29616;&#26377;L2D&#30740;&#31350;&#24573;&#35270;&#20102;&#38459;&#30861;&#20854;&#23454;&#38469;&#37319;&#29992;&#30340;&#30495;&#23454;&#31995;&#32479;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21363;&#65306;&#24573;&#35270;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#65292;&#20854;&#20013;&#31532;1&#31867;&#21644;&#31532;2&#31867;&#38169;&#35823;&#30340;&#25104;&#26412;&#19981;&#21516;&#65307;&#35201;&#27714;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#24182;&#21457;&#20154;&#31867;&#39044;&#27979;&#65307;&#19981;&#22788;&#29702;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#12290;DeCCaF&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;L2D&#26041;&#27861;&#65292;&#37319;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#24314;&#27169;&#20154;&#31867;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#20943;&#23569;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#65292;&#24182;&#20351;&#29992;&#32422;&#26463;&#32534;&#31243;&#26469;&#20840;&#23616;&#26368;&#23567;&#21270;&#38169;&#35823;&#25104;&#26412;&#65292;&#21516;&#26102;&#32771;&#34385;&#24037;&#20316;&#37327;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#31995;&#21015;&#20013;&#27979;&#35797;&#20102;DeCCaF
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06906v1 Announce Type: cross  Abstract: Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#24335;&#65292;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21512;&#25104;&#20013;&#30340;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#32972;&#26223;&#29983;&#25104;&#23548;&#33268;&#30340;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05053</link><description>&lt;p&gt;
PrimeComposer&#65306;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#30340;&#24555;&#36895;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#27861;&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#24335;&#65292;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21512;&#25104;&#20013;&#30340;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#32972;&#26223;&#29983;&#25104;&#23548;&#33268;&#30340;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21512;&#25104;&#28041;&#21450;&#23558;&#32473;&#23450;&#23545;&#35937;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#29305;&#23450;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#12290;&#30446;&#21069;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#20960;&#20010;&#37319;&#26679;&#22120;&#20013;&#32452;&#21512;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#24341;&#23548;&#29983;&#25104;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26435;&#37325;&#26469;&#33258;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#65292;&#23427;&#20204;&#30340;&#32452;&#21512;&#23548;&#33268;&#22312;&#21512;&#25104;&#20013;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#36807;&#22810;&#20851;&#27880;&#32972;&#26223;&#29983;&#25104;&#65292;&#21363;&#20351;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#36825;&#20123;&#38382;&#39064;&#24694;&#21270;&#12290;&#36825;&#19981;&#20165;&#20943;&#24930;&#20102;&#25512;&#29702;&#36895;&#24230;&#65292;&#36824;&#25439;&#23475;&#20102;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36824;&#22312;&#36807;&#28193;&#21306;&#22495;&#24341;&#20837;&#20102;&#19981;&#38656;&#35201;&#30340;&#20266;&#24433;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#21512;&#25104;&#24418;&#24335;&#21270;&#20026;&#19968;&#39033;&#22522;&#20110;&#20027;&#39064;&#30340;&#23616;&#37096;&#32534;&#36753;&#20219;&#21153;&#65292;&#20165;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#32534;&#36753;&#21518;&#30340;&#21069;&#26223;&#19982;&#22122;&#22768;&#32972;&#26223;&#30456;&#32467;&#21512;&#65292;&#20197;&#20445;&#25345;&#22330;&#26223;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#21097;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;tr
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05053v1 Announce Type: cross  Abstract: Image composition involves seamlessly integrating given objects into a specific visual context. The current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion in synthesis and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only slows down inference but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster tr
&lt;/p&gt;</description></item><item><title>MOKA&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#24320;&#25918;&#35789;&#27719;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.03174</link><description>&lt;p&gt;
MOKA&#65306;&#22522;&#20110;&#26631;&#35760;&#30340;&#35270;&#35273;&#25552;&#31034;&#23454;&#29616;&#24320;&#25918;&#35789;&#27719;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03174
&lt;/p&gt;
&lt;p&gt;
MOKA&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#24320;&#25918;&#35789;&#27719;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#30340;&#27867;&#21270;&#35201;&#27714;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#28041;&#21450;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#29615;&#22659;&#20197;&#21450;&#20219;&#21153;&#30446;&#26631;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MOKA&#65288;Marking Open-vocabulary Keypoint Affordances&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#26469;&#35299;&#20915;&#30001;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#25551;&#36848;&#25351;&#23450;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03174v1 Announce Type: cross  Abstract: Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question. In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions. At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world. By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from br
&lt;/p&gt;</description></item><item><title>SoftTiger&#26159;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#20020;&#24202;&#31508;&#35760;&#30340;&#32467;&#26500;&#21270;&#65292;&#23454;&#29616;&#20102;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.00868</link><description>&lt;p&gt;
SoftTiger: &#29992;&#20110;&#21307;&#30103;&#24037;&#20316;&#27969;&#30340;&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SoftTiger: A Clinical Foundation Model for Healthcare Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00868
&lt;/p&gt;
&lt;p&gt;
SoftTiger&#26159;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#20020;&#24202;&#31508;&#35760;&#30340;&#32467;&#26500;&#21270;&#65292;&#23454;&#29616;&#20102;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#24182;&#20171;&#32461;&#20102;SoftTiger&#65292;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CLaM&#65289;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#12290;&#20020;&#24202;&#31508;&#35760;&#30340;&#21465;&#36848;&#24615;&#21644;&#38750;&#32467;&#26500;&#21270;&#29305;&#24615;&#26159;&#21307;&#30103;&#26234;&#33021;&#21270;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#25353;&#29031;&#22269;&#38469;&#20114;&#25805;&#20316;&#24615;&#26631;&#20934;&#23558;&#20020;&#24202;&#31508;&#35760;&#32467;&#26500;&#21270;&#20026;&#20020;&#24202;&#25968;&#25454;&#65292;&#28041;&#21450;&#22269;&#38469;&#24739;&#32773;&#25688;&#35201;&#12289;&#20020;&#24202;&#21360;&#35937;&#21644;&#21307;&#30103;&#25509;&#35302;&#19977;&#20010;&#20851;&#38190;&#23376;&#20219;&#21153;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21644;&#39564;&#35777;&#30340;&#20020;&#24202;&#25968;&#25454;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30446;&#26631;&#27169;&#22411;&#39318;&#20808;&#33021;&#22815;&#25903;&#25345;&#22522;&#26412;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#22914;&#32553;&#20889;&#25193;&#23637;&#21644;&#26102;&#38388;&#20449;&#24687;&#25552;&#21462;&#65292;&#28982;&#21518;&#23398;&#20064;&#25191;&#34892;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#65292;&#22914;&#21360;&#35937;&#21644;&#25509;&#35302;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21307;&#30103;&#27169;&#22411;&#20013;&#30340;&#19968;&#20123;&#24314;&#27169;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00868v1 Announce Type: cross  Abstract: We release and introduce SoftTiger, a clinical large language model (CLaM) designed as a foundation model for healthcare workflows. The narrative and unstructured nature of clinical notes is a major obstacle for healthcare intelligentization. We address a critical problem of structuring clinical notes into clinical data, according to international interoperability standards. We collect and annotate data for three critical subtasks, namely, international patient summary, clinical impression and medical encounter. We then supervised fine-tuned a state-of-the-art LLM using public and credentialed clinical data. The training is orchestrated in a way that the target model can first support basic clinical tasks such as abbreviation expansion and temporal information extraction, and then learn to perform more complex downstream clinical tasks such as impression and encounter summary. Moreover, we address, several modeling challenges in the he
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#36712;&#36857;&#20013;&#23398;&#20064;&#35268;&#21010;&#34892;&#21160;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#20851;&#27880;&#22312;&#21442;&#25968;&#26410;&#25552;&#20379;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#32423;&#21035;&#30340;&#36319;&#36394;&#36136;&#37327;&#20197;&#21450;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10726</link><description>&lt;p&gt;
&#20174;&#29366;&#24577;&#36712;&#36857;&#20013;&#23398;&#20064;&#35268;&#21010;&#34892;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Planning Action Models from State Traces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#36712;&#36857;&#20013;&#23398;&#20064;&#35268;&#21010;&#34892;&#21160;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#20851;&#27880;&#22312;&#21442;&#25968;&#26410;&#25552;&#20379;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#32423;&#21035;&#30340;&#36319;&#36394;&#36136;&#37327;&#20197;&#21450;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20174;&#29366;&#24577;&#36712;&#36857;&#20013;&#23398;&#20064;STRIPS&#39046;&#22495;&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#20174;&#35201;&#23398;&#20064;&#30340;&#34892;&#21160;&#30340;&#21517;&#31216;&#21644;&#21442;&#25968;&#24320;&#22987;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#21807;&#19968;&#20219;&#21153;&#26159;&#25512;&#26029;&#32473;&#23450;&#34892;&#21160;&#30340;&#21069;&#25552;&#26465;&#20214;&#21644;&#25928;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#23398;&#20064;&#26102;&#26410;&#25552;&#20379;&#23398;&#20064;&#34892;&#21160;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#26681;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#23450;&#20041;&#20102;&#20004;&#20010;&#32423;&#21035;&#30340;&#36319;&#36394;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#22312;&#19968;&#20010;&#32423;&#21035;(L1)&#20013;&#65292;&#36712;&#36857;&#20013;&#30340;&#29366;&#24577;&#34987;&#26631;&#35760;&#20026;&#34892;&#21160;&#21517;&#31216;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#20986;&#34892;&#21160;&#30340;&#25968;&#37327;&#21644;&#21517;&#31216;&#65292;&#20294;&#25105;&#20204;&#20173;&#38656;&#35201;&#24324;&#28165;&#21442;&#25968;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#12290;&#22312;&#21478;&#19968;&#20010;&#32423;&#21035;(L2)&#20013;&#65292;&#29366;&#24577;&#36824;&#39069;&#22806;&#26631;&#35760;&#26377;&#26500;&#25104;&#30456;&#24212;&#22522;&#20110;&#23545;&#35937;&#30340;&#34892;&#21160;&#30340;&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20173;&#28982;&#38656;&#35201;&#25512;&#26029;&#23398;&#20064;&#34892;&#21160;&#20013;&#21442;&#25968;&#30340;&#31867;&#22411;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10726v1 Announce Type: new  Abstract: Previous STRIPS domain model acquisition approaches that learn from state traces start with the names and parameters of the actions to be learned. Therefore their only task is to deduce the preconditions and effects of the given actions. In this work, we explore learning in situations when the parameters of learned actions are not provided. We define two levels of trace quality based on which information is provided and present an algorithm for each. In one level (L1), the states in the traces are labeled with action names, so we can deduce the number and names of the actions, but we still need to work out the number and types of parameters. In the other level (L2), the states are additionally labeled with objects that constitute the parameters of the corresponding grounded actions. Here we still need to deduce the types of the parameters in the learned actions. We experimentally evaluate the proposed algorithms and compare them with the
&lt;/p&gt;</description></item><item><title>Cloud Kitchen&#24179;&#21488;&#21033;&#29992;&#22522;&#20110;&#35268;&#21010;&#30340;&#22797;&#21512;AI&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#27969;&#31243;&#65292;&#36890;&#36807;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21644;&#30495;&#23454;&#21382;&#21490;&#25968;&#25454;&#38598;&#38477;&#20302;&#24310;&#36831;&#37197;&#36865;&#37327;&#65292;&#25552;&#39640;&#39038;&#23458;&#28385;&#24847;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.10725</link><description>&lt;p&gt;
&#20113;&#21416;&#25151;&#65306;&#20351;&#29992;&#22522;&#20110;&#35268;&#21010;&#30340;&#22797;&#21512;AI&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Cloud Kitchen: Using Planning-based Composite AI to Optimize Food Delivery Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10725
&lt;/p&gt;
&lt;p&gt;
Cloud Kitchen&#24179;&#21488;&#21033;&#29992;&#22522;&#20110;&#35268;&#21010;&#30340;&#22797;&#21512;AI&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#27969;&#31243;&#65292;&#36890;&#36807;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21644;&#30495;&#23454;&#21382;&#21490;&#25968;&#25454;&#38598;&#38477;&#20302;&#24310;&#36831;&#37197;&#36865;&#37327;&#65292;&#25552;&#39640;&#39038;&#23458;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#39135;&#21697;&#37197;&#36865;&#24066;&#22330;&#20026;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26381;&#21153;&#25552;&#20379;&#20102;&#35768;&#22810;&#26426;&#20250;&#65292;&#21487;&#20197;&#25552;&#39640;&#20840;&#29699;&#20379;&#39184;&#25928;&#29575;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Cloud Kitchen&#24179;&#21488;&#65292;&#20316;&#20026;&#19968;&#20010;&#20915;&#31574;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#20855;&#26377;&#39135;&#21697;&#37197;&#36865;&#26381;&#21153;&#30340;&#39184;&#21381;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#27169;&#25311;&#22120;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#35813;&#24179;&#21488;&#30001;&#19968;&#20010;Technology-Specific Bridge&#65288;TSB&#65289;&#32452;&#25104;&#65292;&#29992;&#20110;&#19982;&#39184;&#21381;&#25110;&#27169;&#25311;&#22120;&#36827;&#34892;&#36890;&#20449;&#12290;TSB&#20351;&#29992;PDDL&#27169;&#22411;&#34920;&#31034;&#23884;&#20837;&#22312;Unified Planning Framework&#65288;UPF&#65289;&#20013;&#30340;&#20915;&#31574;&#12290;&#20915;&#31574;&#28041;&#21450;&#23558;&#39038;&#23458;&#35746;&#21333;&#20998;&#37197;&#21040;&#36710;&#36742;&#20197;&#21450;&#20915;&#23450;&#20197;&#20160;&#20040;&#39034;&#24207;&#20026;&#39038;&#23458;&#25552;&#20379;&#26381;&#21153;&#65288;&#23545;&#20110;&#27599;&#36742;&#36710;&#65289;&#65292;&#36825;&#26159;&#36890;&#36807;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRPTW&#65289;&#26469;&#23436;&#25104;&#30340;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#35299;&#20915;&#26159;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#24179;&#21488;&#21046;&#23450;&#30340;&#20915;&#31574;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#21382;&#21490;&#25968;&#25454;&#38598;&#20943;&#23569;&#24310;&#36831;&#37197;&#36865;&#37327;&#26469;&#25552;&#39640;&#39038;&#23458;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10725v1 Announce Type: new  Abstract: The global food delivery market provides many opportunities for AI-based services that can improve the efficiency of feeding the world. This paper presents the Cloud Kitchen platform as a decision-making tool for restaurants with food delivery and a simulator to evaluate the impact of the decisions. The platform consists of a Technology-Specific Bridge (TSB) that provides an interface for communicating with restaurants or the simulator. TSB uses a PDDL model to represent decisions embedded in the Unified Planning Framework (UPF). Decision-making, which concerns allocating customers' orders to vehicles and deciding in which order the customers will be served (for each vehicle), is done via a Vehicle Routing Problem with Time Windows (VRPTW), an efficient tool for this problem. We show that decisions made by our platform can improve customer satisfaction by reducing the number of delayed deliveries using a real-world historical dataset.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20013;&#36896;&#25104;&#30340;&#24378;&#24187;&#35273;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21542;&#23450;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#32780;&#26080;&#38656;&#20351;&#29992;&#31232;&#30095;&#36127;&#25968;&#25454;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.10543</link><description>&lt;p&gt;
&#28040;&#38500;&#21542;&#23450;&#23548;&#33268;&#30340;&#24378;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Strong hallucinations from negation and how to fix them
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10543
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20013;&#36896;&#25104;&#30340;&#24378;&#24187;&#35273;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21542;&#23450;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#32780;&#26080;&#38656;&#20351;&#29992;&#31232;&#30095;&#36127;&#25968;&#25454;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#22312;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26377;&#26102;&#20250;&#25552;&#20379;&#30001;&#20110;&#36923;&#36753;&#19981;&#36830;&#36143;&#32780;&#19981;&#21487;&#33021;&#25104;&#31435;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#21709;&#24212;&#20026;\textit{&#24378;&#24187;&#35273;}&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#28304;&#20110;LM&#35745;&#31639;&#20854;&#20869;&#37096;&#34920;&#31034;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#20174;&#36825;&#20123;&#34920;&#31034;&#20013;&#20135;&#29983;&#30340;&#36755;&#20986;&#12290;&#37325;&#28857;&#20851;&#27880;&#21542;&#23450;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#21542;&#23450;&#19981;&#26159;&#20316;&#20026;&#28508;&#22312;&#34920;&#31034;&#30340;&#21478;&#19968;&#20010;&#20803;&#32032;&#65292;&#32780;&#26159;&#20316;&#20026;\textit{LM&#28508;&#22312;&#34920;&#31034;&#19978;&#30340;&#19968;&#20010;&#25805;&#20316;&#65292;&#32422;&#26463;&#23427;&#20204;&#21487;&#33021;&#30340;&#28436;&#21464;&#26041;&#24335;}&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#22312;&#24102;&#21542;&#23450;&#30340;&#22635;&#31354;&#25552;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#23545;&#31232;&#30095;&#36127;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10543v1 Announce Type: cross  Abstract: Despite great performance on many tasks, language models (LMs) still struggle with reasoning, sometimes providing responses that cannot possibly be true because they stem from logical incoherence. We call such responses \textit{strong hallucinations} and prove that they follow from an LM's computation of its internal representations for logical operators and outputs from those representations. Focusing on negation, we provide a novel solution in which negation is treated not as another element of a latent representation, but as \textit{an operation over an LM's latent representations that constrains how they may evolve}. We show that our approach improves model performance in cloze prompting and natural language inference tasks with negation without requiring training on sparse negative data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65292;&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06871</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive Generative Models for Reranking Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65292;&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#37325;&#26032;&#25490;&#24207;&#36890;&#36807;&#24314;&#27169;&#39033;&#30446;&#20043;&#38388;&#30340;&#20869;&#37096;&#30456;&#20851;&#24615;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#37325;&#26032;&#25490;&#24207;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22312;&#25490;&#21015;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#25506;&#32034;&#26368;&#20339;&#24207;&#21015;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#29983;&#25104;&#22120;-&#35780;&#20272;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#29983;&#25104;&#22120;&#29983;&#25104;&#22810;&#20010;&#21487;&#34892;&#24207;&#21015;&#65292;&#35780;&#20272;&#22120;&#22522;&#20110;&#20272;&#35745;&#30340;&#21015;&#34920;&#24471;&#20998;&#36873;&#25321;&#26368;&#20339;&#24207;&#21015;&#12290;&#29983;&#25104;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#29983;&#25104;&#22120;&#20989;&#25968;&#12290;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#37319;&#29992;&#33258;&#22238;&#24402;&#31574;&#30053;&#36827;&#34892;&#24207;&#21015;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#24037;&#19994;&#31995;&#32479;&#20013;&#37096;&#32626;&#33258;&#22238;&#24402;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65288;NAR4Rec&#65289;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#19982;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;m
&lt;/p&gt;
&lt;p&gt;
In a multi-stage recommendation system, reranking plays a crucial role by modeling the intra-list correlations among items.The key challenge of reranking lies in the exploration of optimal sequences within the combinatorial space of permutations. Recent research proposes a generator-evaluator learning paradigm, where the generator generates multiple feasible sequences and the evaluator picks out the best sequence based on the estimated listwise score. Generator is of vital importance, and generative models are well-suited for the generator function. Current generative models employ an autoregressive strategy for sequence generation. However, deploying autoregressive models in real-time industrial systems is challenging. Hence, we propose a Non-AutoRegressive generative model for reranking Recommendation (NAR4Rec) designed to enhance efficiency and effectiveness. To address challenges related to sparse training samples and dynamic candidates impacting model convergence, we introduce a m
&lt;/p&gt;</description></item><item><title>PsySafe&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#26234;&#33021;&#20307;&#24515;&#29702;&#23398;&#65292;&#25581;&#31034;&#26234;&#33021;&#20307;&#30340;&#40657;&#26263;&#24515;&#29702;&#29366;&#24577;&#23545;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2401.11880</link><description>&lt;p&gt;
PsySafe&#65306;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23433;&#20840;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#30340;&#32508;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11880
&lt;/p&gt;
&lt;p&gt;
PsySafe&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#26234;&#33021;&#20307;&#24515;&#29702;&#23398;&#65292;&#25581;&#31034;&#26234;&#33021;&#20307;&#30340;&#40657;&#26263;&#24515;&#29702;&#29366;&#24577;&#23545;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#21152;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21518;&#65292;&#23637;&#29616;&#20986;&#20102;&#38598;&#20307;&#26234;&#33021;&#30340;&#28145;&#36828;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26234;&#33021;&#34987;&#24694;&#24847;&#20351;&#29992;&#21487;&#33021;&#24102;&#26469;&#37325;&#22823;&#39118;&#38505;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#21019;&#26032;&#30340;&#35270;&#35282;&#25506;&#32034;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#21457;&#29616;&#26234;&#33021;&#20307;&#30340;&#40657;&#26263;&#24515;&#29702;&#29366;&#24577;&#26500;&#25104;&#20102;&#23545;&#23433;&#20840;&#30340;&#37325;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#26234;&#33021;&#20307;&#24515;&#29702;&#23398;&#20026;&#22522;&#30784;&#30340;&#32508;&#21512;&#26694;&#26550;&#65288;PsySafe&#65289;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65306;&#39318;&#20808;&#65292;&#35782;&#21035;&#26234;&#33021;&#20307;&#20013;&#30340;&#40657;&#26263;&#20154;&#26684;&#29305;&#24449;&#22914;&#20309;&#23548;&#33268;&#39118;&#38505;&#34892;&#20026;&#65307;&#20854;&#27425;&#65292;&#20174;&#24515;&#29702;&#21644;&#34892;&#20026;&#35282;&#24230;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65307;&#31532;&#19977;&#65292;&#21046;&#23450;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11880v2 Announce Type: replace-cross  Abstract: Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit profound capabilities in collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. In this paper, we explore these concerns through the innovative lens of agent psychology, revealing that the dark psychological states of agents constitute a significant threat to safety. To tackle these concerns, we propose a comprehensive framework (PsySafe) grounded in agent psychology, focusing on three key areas: firstly, identifying how dark personality traits in agents can lead to risky behaviors; secondly, evaluating the safety of multi-agent systems from the psychological and behavioral perspectives, and thirdly, devising effective strategies to mitigate these risks. Our experiments reveal
&lt;/p&gt;</description></item><item><title>CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.05544</link><description>&lt;p&gt;
CodePrompt&#65306;&#36890;&#36807;Prompt&#23398;&#20064;&#30340;&#30693;&#35782;&#29305;&#24449;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning. (arXiv:2401.05544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05544
&lt;/p&gt;
&lt;p&gt;
CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CodeBERT&#65289;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;CodeBERT&#30340;&#25991;&#26412;&#23884;&#20837;&#33021;&#21147;&#21644;"[CLS]"&#21477;&#23376;&#23884;&#20837;&#20449;&#24687;&#20316;&#20026;&#19979;&#28216;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#35821;&#20041;&#34920;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#26469;&#25552;&#21462;&#26377;&#25928;&#29305;&#24449;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#33021;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CodePrompt&#65292;&#36890;&#36807;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have explored the potential of utilizing pre-trained language models, such as CodeBERT, to improve source code-related tasks. Previous studies have mainly relied on CodeBERT's text embedding capability and the `[CLS]' sentence embedding information as semantic representations for fine-tuning downstream source code-related tasks. However, these methods require additional neural network layers to extract effective features, resulting in higher computational costs. Furthermore, existing approaches have not leveraged the rich knowledge contained in both source code and related text, which can lead to lower accuracy. This paper presents a novel approach, CodePrompt, which utilizes rich knowledge recalled from a pre-trained model by prompt learning and an attention mechanism to improve source code-related classification tasks. Our approach initially motivates the language model with prompt information to retrieve abundant knowledge associated with the input as representative feat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;212&#20010;&#30495;&#23454;&#30340;&#24694;&#24847;&#26381;&#21153;&#65288;Malla&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22320;&#19979;&#24066;&#22330;&#30340;&#25193;&#25955;&#21644;&#23545;&#20844;&#20849;LLM&#26381;&#21153;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#30340;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2401.03315</link><description>&lt;p&gt;
Malla: &#25581;&#31192;&#29616;&#23454;&#19990;&#30028;&#20013;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#24694;&#24847;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Malla: Demystifying Real-world Large Language Model Integrated Malicious Services. (arXiv:2401.03315v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;212&#20010;&#30495;&#23454;&#30340;&#24694;&#24847;&#26381;&#21153;&#65288;Malla&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22320;&#19979;&#24066;&#22330;&#30340;&#25193;&#25955;&#21644;&#23545;&#20844;&#20849;LLM&#26381;&#21153;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#30340;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22320;&#19979;&#21033;&#29992;&#65292;&#20063;&#31216;&#20026;Malla&#65292;&#27491;&#22312;&#22686;&#21152;&#65292;&#21152;&#21095;&#20102;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#65292;&#24182;&#23545;LLMs&#25216;&#26415;&#30340;&#21487;&#20449;&#24230;&#25552;&#20986;&#20102;&#30097;&#38382;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#21162;&#21147;&#21435;&#20102;&#35299;&#36825;&#31181;&#26032;&#22411;&#32593;&#32476;&#29359;&#32618;&#30340;&#35268;&#27169;&#12289;&#24433;&#21709;&#21644;&#25216;&#26415;&#12290;&#26412;&#25991;&#26159;&#31532;&#19968;&#27425;&#23545;212&#20010;&#30495;&#23454;&#30340;Malla&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22320;&#19979;&#24066;&#22330;&#30340;&#25193;&#25955;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#24320;&#20102;Malla&#29983;&#24577;&#31995;&#32479;&#65292;&#25581;&#31034;&#20102;&#20854;&#26174;&#33879;&#30340;&#22686;&#38271;&#23545;&#24403;&#20170;&#20844;&#20849;LLM&#26381;&#21153;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;212&#20010;Mallas&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;8&#20010;&#21518;&#31471;LLMs&#65292;&#20197;&#21450;182&#20010;&#32469;&#36807;&#20844;&#20849;LLM API&#20445;&#25252;&#25514;&#26045;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;Mallas&#20351;&#29992;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#28389;&#29992;&#26410;&#32463;&#23457;&#26597;&#30340;LLMs&#21644;&#36890;&#36807;&#36234;&#29425;&#25552;&#31034;&#21033;&#29992;&#20844;&#20849;LLM API&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;Malla&#29359;&#32618;&#34892;&#20026;&#30340;&#23454;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The underground exploitation of large language models (LLMs) for malicious services (i.e., Malla) is witnessing an uptick, amplifying the cyber threat landscape and posing questions about the trustworthiness of LLM technologies. However, there has been little effort to understand this new cybercrime, in terms of its magnitude, impact, and techniques. In this paper, we conduct the first systematic study on 212 real-world Mallas, uncovering their proliferation in underground marketplaces and exposing their operational modalities. Our study discloses the Malla ecosystem, revealing its significant growth and impact on today's public LLM services. Through examining 212 Mallas, we uncovered eight backend LLMs used by Mallas, along with 182 prompts that circumvent the protective measures of public LLM APIs. We further demystify the tactics employed by Mallas, including the abuse of uncensored LLMs and the exploitation of public LLM APIs through jailbreak prompts. Our findings enable a better 
&lt;/p&gt;</description></item><item><title>PromptBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;&#65292;&#21253;&#25324;&#20102;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#31561;&#32452;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#21644;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>http://arxiv.org/abs/2312.07910</link><description>&lt;p&gt;
PromptBench&#65306;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07910
&lt;/p&gt;
&lt;p&gt;
PromptBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;&#65292;&#21253;&#25324;&#20102;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#31561;&#32452;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#21644;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#23545;&#20110;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PromptBench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#32479;&#19968;&#24211;&#12290;&#23427;&#30001;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;&#21644;&#25193;&#23637;&#65306;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#12290;PromptBench&#26088;&#22312;&#25104;&#20026;&#19968;&#20010;&#24320;&#25918;&#12289;&#36890;&#29992;&#21644;&#28789;&#27963;&#30340;&#20195;&#30721;&#24211;&#65292;&#20197;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#65292;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#21644;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/microsoft/promptbench&#19978;&#25214;&#21040;&#65292;&#24182;&#23558;&#25345;&#32493;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30446;&#26631;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;Nash&#22343;&#34913;&#24182;&#20855;&#26377;&#29305;&#23450;&#20215;&#20540;&#33539;&#22260;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.00582</link><description>&lt;p&gt;
&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20197;&#23454;&#29616;&#20219;&#24847;Nash&#22343;&#34913;&#21644;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value. (arXiv:2311.00582v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30446;&#26631;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;Nash&#22343;&#34913;&#24182;&#20855;&#26377;&#29305;&#23450;&#20215;&#20540;&#33539;&#22260;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20301;&#21892;&#24847;&#30340;&#28216;&#25103;&#35774;&#35745;&#32773;&#25110;&#24694;&#24847;&#30340;&#23545;&#25163;&#20462;&#25913;&#20102;&#19968;&#20010;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#20415;&#19968;&#20010;&#30446;&#26631;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#30340;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;Nash&#22343;&#34913;&#65292;&#24182;&#19988;&#22312;&#30446;&#26631;&#33539;&#22260;&#20869;&#20855;&#26377;&#20215;&#20540;&#65292;&#20197;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#33021;&#22815;&#23433;&#35013;&#20026;&#26576;&#20010;&#28216;&#25103;&#30340;&#21807;&#19968;&#22343;&#34913;&#30340;&#31574;&#30053;&#37197;&#32622;&#30340;&#38598;&#21512;&#65292;&#24182;&#24314;&#31435;&#20102;&#25104;&#21151;&#23433;&#35013;&#30340;&#20805;&#20998;&#21644;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#19968;&#20010;&#24102;&#26377;&#32447;&#24615;&#32422;&#26463;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#28982;&#21518;&#36827;&#34892;&#38543;&#26426;&#25200;&#21160;&#65292;&#26469;&#33719;&#24471;&#19968;&#20010;&#25104;&#26412;&#36817;&#20046;&#26368;&#20248;&#30340;&#20462;&#25913;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost. We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation. We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22235;&#31181;&#25945;&#23398;&#25351;&#23548;&#31574;&#30053;&#23545;&#23398;&#20064;&#32773;&#22312;&#20351;&#29992;LLM&#26102;&#30340;&#34920;&#29616;&#21644;&#24863;&#30693;&#25928;&#26524;&#65292;&#21457;&#29616;&#30452;&#25509;LLM&#31572;&#26696;&#25552;&#39640;&#20102;&#34920;&#29616;&#65292;&#32780;&#25913;&#36827;&#23398;&#29983;&#35299;&#20915;&#26041;&#26696;&#21017;&#22686;&#21152;&#20102;&#23545;LLM&#30340;&#20449;&#20219;&#24230;&#12290;&#21516;&#26102;&#65292;&#32467;&#26500;&#21270;&#25351;&#23548;&#20063;&#20943;&#23569;&#20102;&#38543;&#26426;&#26597;&#35810;&#21644;&#23398;&#29983;&#22797;&#21046;&#31896;&#36148;&#38382;&#39064;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2310.13712</link><description>&lt;p&gt;
&#23398;&#20064;&#32773;&#20351;&#29992;LLM&#26102;&#25351;&#23548;&#21644;&#20132;&#20114;&#31574;&#30053;&#23545;&#23398;&#20064;&#32773;&#30340;&#34920;&#29616;&#21644;&#24863;&#30693;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Guidance and Interaction Strategies for LLM Use on Learner Performance and Perception. (arXiv:2310.13712v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22235;&#31181;&#25945;&#23398;&#25351;&#23548;&#31574;&#30053;&#23545;&#23398;&#20064;&#32773;&#22312;&#20351;&#29992;LLM&#26102;&#30340;&#34920;&#29616;&#21644;&#24863;&#30693;&#25928;&#26524;&#65292;&#21457;&#29616;&#30452;&#25509;LLM&#31572;&#26696;&#25552;&#39640;&#20102;&#34920;&#29616;&#65292;&#32780;&#25913;&#36827;&#23398;&#29983;&#35299;&#20915;&#26041;&#26696;&#21017;&#22686;&#21152;&#20102;&#23545;LLM&#30340;&#20449;&#20219;&#24230;&#12290;&#21516;&#26102;&#65292;&#32467;&#26500;&#21270;&#25351;&#23548;&#20063;&#20943;&#23569;&#20102;&#38543;&#26426;&#26597;&#35810;&#21644;&#23398;&#29983;&#22797;&#21046;&#31896;&#36148;&#38382;&#39064;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#25945;&#23460;&#35268;&#27169;&#21644;&#25945;&#24072;&#36164;&#28304;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25945;&#23398;&#21161;&#25163;&#21487;&#20197;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#25506;&#32034;&#23427;&#20204;&#22312;&#25945;&#32946;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#19981;&#20165;&#22312;&#20110;&#30830;&#23450;LLMs&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19988;&#22312;&#20110;&#35782;&#21035;&#23398;&#20064;&#32773;&#19982;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#20114;&#21160;&#32454;&#24494;&#24046;&#21035;&#65292;&#36825;&#20250;&#24433;&#21709;&#23398;&#20064;&#32773;&#30340;&#21442;&#19982;&#21644;&#25104;&#26524;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#22530;&#65288;N=145&#65289;&#21644;Prolific&#19978;&#36827;&#34892;&#20102;&#19968;&#39033;&#24418;&#25104;&#24615;&#30740;&#31350;&#65288;N=356&#65289;&#65292;&#20197;&#25506;&#32034;&#22235;&#31181;&#25945;&#23398;&#25351;&#23548;&#31574;&#30053;&#23545;&#23398;&#20064;&#32773;&#22312;LLMs&#19978;&#30340;&#34920;&#29616;&#12289;&#33258;&#20449;&#24515;&#21644;&#20449;&#20219;&#24230;&#30340;&#24433;&#21709;&#12290;&#30452;&#25509;&#30340;LLM&#31572;&#26696;&#31245;&#24494;&#25552;&#39640;&#20102;&#34920;&#29616;&#65292;&#32780;&#25913;&#36827;&#23398;&#29983;&#30340;&#35299;&#20915;&#26041;&#26696;&#22686;&#21152;&#20102;&#20449;&#20219;&#24230;&#12290;&#32467;&#26500;&#21270;&#25351;&#23548;&#20943;&#23569;&#20102;&#38543;&#26426;&#26597;&#35810;&#21644;&#23398;&#29983;&#23558;&#20316;&#19994;&#38382;&#39064;&#22797;&#21046;&#31896;&#36148;&#32473;LLM&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20984;&#26174;&#20102;t
&lt;/p&gt;
&lt;p&gt;
Personalized chatbot-based teaching assistants can be crucial in addressing increasing classroom sizes, especially where direct teacher presence is limited. Large language models (LLMs) offer a promising avenue, with increasing research exploring their educational utility. However, the challenge lies not only in establishing the efficacy of LLMs but also in discerning the nuances of interaction between learners and these models, which impact learners' engagement and results. We conducted a formative study in an undergraduate computer science classroom (N=145) and a controlled experiment on Prolific (N=356) to explore the impact of four pedagogically informed guidance strategies on the learners' performance, confidence and trust in LLMs. Direct LLM answers marginally improved performance, while refining student solutions fostered trust. Structured guidance reduced random queries as well as instances of students copy-pasting assignment questions to the LLM. Our work highlights the role t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#26694;&#26550;CausalReinforceNet&#65292;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20197;&#25552;&#39640;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#20132;&#26131;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09462</link><description>&lt;p&gt;
&#19968;&#20010;&#36171;&#20104;&#22240;&#26524;&#20998;&#26512;&#33021;&#21147;&#30340;&#22686;&#24378;&#23398;&#20064;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65306;&#22686;&#24378;&#33258;&#21160;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
A Framework for Empowering Reinforcement Learning Agents with Causal Analysis: Enhancing Automated Cryptocurrency Trading. (arXiv:2310.09462v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#26694;&#26550;CausalReinforceNet&#65292;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20197;&#25552;&#39640;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#20132;&#26131;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#20132;&#26131;&#26041;&#27861;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#20013;&#24320;&#21457;&#30408;&#21033;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#38024;&#23545;&#20116;&#31181;&#28909;&#38376;&#30340;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#65288;&#21363;&#27604;&#29305;&#24065;&#20197;&#22806;&#30340;&#21152;&#23494;&#36135;&#24065;&#65289;&#65306;&#24065;&#23433;&#24065;&#12289;&#20197;&#22826;&#22346;&#12289;&#33713;&#29305;&#24065;&#12289;&#29790;&#27874;&#24065;&#21644;&#27888;&#36798;&#24065;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CausalReinforceNet&#65292;&#19968;&#20010;&#34987;&#26500;&#24314;&#20026;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#26694;&#26550;&#12290;&#20316;&#20026;&#20132;&#26131;&#31995;&#32479;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;CausalReinforceNet&#26694;&#26550;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#22312;&#29305;&#24449;&#24037;&#31243;&#36807;&#31243;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#35782;&#21035;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#21464;&#21160;&#30340;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#23558;&#27010;&#29575;&#24615;&#20215;&#26684;&#26041;&#21521;&#20449;&#21495;&#32435;&#20837;&#26694;&#26550;&#20013;&#65292;&#20197;&#22686;&#24378;&#20132;&#26131;&#31995;&#32479;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in artificial intelligence-enhanced trading methods, developing a profitable automated trading system remains challenging in the rapidly evolving cryptocurrency market. This study aims to address these challenges by developing a reinforcement learning-based automated trading system for five popular altcoins~(cryptocurrencies other than Bitcoin): Binance Coin, Ethereum, Litecoin, Ripple, and Tether. To this end, we present CausalReinforceNet, a framework framed as a decision support system. Designed as the foundational architecture of the trading system, the CausalReinforceNet framework enhances the capabilities of the reinforcement learning agent through causal analysis. Within this framework, we use Bayesian networks in the feature engineering process to identify the most relevant features with causal relationships that influence cryptocurrency price movements. Additionally, we incorporate probabilistic price direction signals from dynamic Bayesian networks to enhance
&lt;/p&gt;</description></item><item><title>SLAN&#26159;&#19968;&#31181;&#26080;&#38656;&#25554;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24314;&#27169;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#65292;&#21033;&#29992;&#21160;&#24577;&#36866;&#24212;&#30340;LSTM&#26550;&#26500;&#26469;&#25429;&#25417;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#23616;&#37096;&#25688;&#35201;&#65292;&#24182;&#22312;&#25972;&#20010;&#35266;&#27979;&#26399;&#38388;&#32500;&#25345;&#19968;&#20010;&#20840;&#23616;&#25688;&#35201;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.08698</link><description>&lt;p&gt;
&#26080;&#38656;&#25554;&#20540;&#30340;&#24314;&#27169;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Modelling Irregularly Sampled Time Series Without Imputation. (arXiv:2309.08698v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08698
&lt;/p&gt;
&lt;p&gt;
SLAN&#26159;&#19968;&#31181;&#26080;&#38656;&#25554;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24314;&#27169;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#65292;&#21033;&#29992;&#21160;&#24577;&#36866;&#24212;&#30340;LSTM&#26550;&#26500;&#26469;&#25429;&#25417;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#23616;&#37096;&#25688;&#35201;&#65292;&#24182;&#22312;&#25972;&#20010;&#35266;&#27979;&#26399;&#38388;&#32500;&#25345;&#19968;&#20010;&#20840;&#23616;&#25688;&#35201;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#65288;ISTS&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23384;&#22312;&#32570;&#22833;&#20540;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#36716;&#25442;&#20026;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#26469;&#22788;&#29702;ISTS&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#23384;&#22312;&#28508;&#22312;&#30340;&#32570;&#22833;&#26426;&#21046;&#65292;&#23548;&#33268;&#20102;&#19981;&#24076;&#26395;&#30340;&#20559;&#24046;&#21644;&#27425;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SLAN&#65288;Switch LSTM Aggregate Network&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#32452;LSTM&#23545;ISTS&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#26080;&#38656;&#25554;&#20540;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#28508;&#22312;&#36807;&#31243;&#30340;&#20551;&#35774;&#12290;&#23427;&#26681;&#25454;&#27979;&#37327;&#20256;&#24863;&#22120;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20854;&#26550;&#26500;&#12290;SLAN&#21033;&#29992;&#19981;&#35268;&#21017;&#24615;&#20449;&#24687;&#26126;&#30830;&#25429;&#25417;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#23616;&#37096;&#25688;&#35201;&#65292;&#24182;&#22312;&#25972;&#20010;&#35266;&#27979;&#26399;&#38388;&#32500;&#25345;&#19968;&#20010;&#20840;&#23616;&#25688;&#35201;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SLAN&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;MIMIC-III&#12289;Physionet 2012&#21644;Physionet 2019&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Rohit102497/SLAN&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling irregularly-sampled time series (ISTS) is challenging because of missing values. Most existing methods focus on handling ISTS by converting irregularly sampled data into regularly sampled data via imputation. These models assume an underlying missing mechanism leading to unwanted bias and sub-optimal performance. We present SLAN (Switch LSTM Aggregate Network), which utilizes a pack of LSTMs to model ISTS without imputation, eliminating the assumption of any underlying process. It dynamically adapts its architecture on the fly based on the measured sensors. SLAN exploits the irregularity information to capture each sensor's local summary explicitly and maintains a global summary state throughout the observational period. We demonstrate the efficacy of SLAN on publicly available datasets, namely, MIMIC-III, Physionet 2012 and Physionet 2019. The code is available at https://github.com/Rohit102497/SLAN.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#26469;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.07215</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#19982;&#33258;&#36866;&#24212;&#26680;&#24515;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Quantization-aware Training with Adaptive Coreset Selection. (arXiv:2306.07215v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#26469;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#37327;&#30340;&#22686;&#21152;&#65292;&#22686;&#21152;&#20102;&#23545;&#26377;&#25928;&#27169;&#22411;&#37096;&#32626;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#26159;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26435;&#37325;&#21644;&#28608;&#27963;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;QAT&#26041;&#27861;&#38656;&#35201;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36825;&#20250;&#23548;&#33268;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21644;&#39640;&#33021;&#32791;&#12290;&#26680;&#24515;&#38598;&#36873;&#25321;&#26159;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#20887;&#20313;&#24615;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#25928;&#35757;&#32451;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#25552;&#39640;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#22522;&#20110;QAT&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25351;&#26631;&#65306;&#35823;&#24046;&#21521;&#37327;&#20998;&#25968;&#21644;&#19981;&#19968;&#33268;&#20998;&#25968;&#65292;&#29992;&#20110;&#37327;&#21270;&#35757;&#32451;&#36807;&#31243;&#20013;&#27599;&#20010;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;ACS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expanding model size and computation of deep neural networks (DNNs) have increased the demand for efficient model deployment methods. Quantization-aware training (QAT) is a representative model compression method to leverage redundancy in weights and activations. However, most existing QAT methods require end-to-end training on the entire dataset, which suffers from long training time and high energy costs. Coreset selection, aiming to improve data efficiency utilizing the redundancy of training data, has also been widely used for efficient training. In this work, we propose a new angle through the coreset selection to improve the training efficiency of quantization-aware training. Based on the characteristics of QAT, we propose two metrics: error vector score and disagreement score, to quantify the importance of each sample during training. Guided by these two metrics of importance, we proposed a quantization-aware adaptive coreset selection (ACS) method to select the data for the
&lt;/p&gt;</description></item><item><title>NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14405</link><description>&lt;p&gt;
NeuralMatrix: &#23558;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#31227;&#21160;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference. (arXiv:2305.14405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14405
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NeuralMatrix&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#22810;&#21151;&#33021;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#22522;&#20110;ASIC&#30340;&#21152;&#36895;&#22120;&#30340;&#19987;&#29992;&#24615;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;CPU&#21644;GPU&#31561;&#36890;&#29992;&#22788;&#29702;&#22120;&#30456;&#27604;&#30340;&#24212;&#29992;&#29305;&#23450;&#21152;&#36895;&#27700;&#24179;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;DNN&#35745;&#31639;&#20013;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36816;&#31639;&#26144;&#23556;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#21450;&#20351;&#29992;GEMM&#21152;&#36895;&#22120;&#23545;DNN&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19977;&#31181;&#27969;&#34892;&#31867;&#21035;&#30340;&#21508;&#31181;DNN&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65288;&#21363;CNN&#65292;Transformers&#21644;GNN&#65289;&#20316;&#20026;&#31034;&#20363;&#30340;&#25903;&#25745;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;DNN&#36716;&#25442;&#20026;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21518;&#20165;&#20250;&#20986;&#29616;&#39640;&#36798;2.02&#65285;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#65292;&#21516;&#26102;&#23558;&#21534;&#21520;&#37327;&#19982;&#21151;&#29575;&#30340;&#27604;&#20540;&#19982;CPU&#21644;GPU&#30456;&#27604;&#25552;&#39640;&#20102;113&#20493;&#21040;19.44&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce NeuralMatrix, a novel framework that enables the computation of versatile deep neural networks (DNNs) on a single general matrix multiplication (GEMM) accelerator. The proposed approach overcomes the specificity limitations of ASIC-based accelerators while achieving application-specific acceleration levels compared to general-purpose processors such as CPUs and GPUs. We address the challenges of mapping both linear and nonlinear operations in DNN computation to general matrix multiplications and the impact of using a GEMM accelerator on DNN inference accuracy. Extensive experiments are conducted on various DNN models from three popular categories (i.e., CNN, Transformers, and GNN) as illustrative backbone models. Our results demonstrate that DNNs suffer only up to a 2.02% accuracy loss after being converted to general matrix multiplication, while achieving 113x to 19.44x improvements in throughput per power compared to CPUs and GPUs.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00050</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24320;&#21551;&#22240;&#26524;&#30740;&#31350;&#30340;&#26032;&#31687;&#31456;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00050
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#33021;&#21147;&#22791;&#21463;&#20105;&#35758;&#65292;&#24182;&#19988;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#21307;&#23398;&#12289;&#31185;&#23398;&#12289;&#27861;&#24459;&#21644;&#25919;&#31574;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#21450;&#20854;&#22240;&#26524;&#25512;&#29702;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#24314;&#26500;&#21644;&#27979;&#37327;&#25928;&#24230;&#23041;&#32961;&#12290;&#22522;&#20110;GPT-3.5&#21644;4&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22240;&#26524;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LLMs&#23637;&#31034;&#20102;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#35299;&#37322;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.  Crucially, LLMs perform these causal tasks while relying on sources of knowledg
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#34920;&#31034;&#21644;&#20215;&#20540;&#25439;&#22833;&#65292;&#20351;&#26827;&#30424;&#28216;&#25103;&#24615;&#33021;&#33719;&#24471;&#20102;&#39640;&#36798;180 Elo&#28857;&#30340;&#22686;&#24378;</title><link>http://arxiv.org/abs/2304.14918</link><description>&lt;p&gt;
&#34920;&#31034;&#24456;&#37325;&#35201;&#65306;&#26827;&#30424;&#28216;&#25103;&#23545;&#35270;&#35273;Transformer&#25552;&#20986;&#20102;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Representation Matters: The Game of Chess Poses a Challenge to Vision Transformers. (arXiv:2304.14918v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14918
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#34920;&#31034;&#21644;&#20215;&#20540;&#25439;&#22833;&#65292;&#20351;&#26827;&#30424;&#28216;&#25103;&#24615;&#33021;&#33719;&#24471;&#20102;&#39640;&#36798;180 Elo&#28857;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#24050;&#32463;&#25104;&#20026;&#20102;&#8220;AI&#30340;&#29790;&#22763;&#20891;&#20992;&#8221;&#65292;&#20294;&#27809;&#26377;&#20154;&#25361;&#25112;&#23427;&#20204;&#21435;&#25484;&#25569;&#35937;&#26827;&#36825;&#20010;&#32463;&#20856;&#30340;AI&#22522;&#20934;&#12290;&#20294;&#20165;&#20165;&#20351;&#29992;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#22312;AlphaZero&#20013;&#26080;&#27861;&#25484;&#25569;&#35937;&#26827;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;ViT&#22826;&#24930;&#20102;&#12290;&#21363;&#20351;&#20351;&#29992;MobileNet&#21644;NextViT&#30340;&#32452;&#21512;&#20351;&#23427;&#20204;&#26356;&#26377;&#25928;&#65292;&#20063;&#26080;&#27861;&#20987;&#36133;&#23454;&#38469;&#19978;&#26356;&#37325;&#35201;&#30340;&#19996;&#35199;&#65306;&#31616;&#21333;&#25913;&#21464;&#36755;&#20837;&#34920;&#31034;&#21644;&#20215;&#20540;&#25439;&#22833;&#65292;&#20174;&#32780;&#33719;&#24471;&#39640;&#36798;180 Elo&#28857;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformers have gained the reputation as the "Swiss army knife of AI", no one has challenged them to master the game of chess, one of the classical AI benchmarks. Simply using vision transformers (ViTs) within AlphaZero does not master the game of chess, mainly because ViTs are too slow. Even making them more efficient using a combination of MobileNet and NextViT does not beat what actually matters: a simple change of the input representation and value loss, resulting in a greater boost of up to 180 Elo points over AlphaZero.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03997</link><description>&lt;p&gt;
REDf&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network. (arXiv:2304.03997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19990;&#30028;&#21521;&#26356;&#21487;&#25345;&#32493;&#30340;&#33021;&#28304;&#26410;&#26469;&#21457;&#23637;&#65292;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#28304;&#32435;&#20837;&#30005;&#32593;&#30340;&#38598;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38388;&#27463;&#24615;&#20351;&#30005;&#32593;&#31649;&#29702;&#21644;&#30830;&#20445;&#31283;&#23450;&#30340;&#30005;&#21147;&#20379;&#24212;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#33021;&#37327;&#38656;&#27714;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#26469;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#33021;&#47071;&#38656;&#27714;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20123;&#32593;&#32476;&#29305;&#21035;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#22235;&#20010;&#21382;&#21490;&#33021;&#37327;&#38656;&#27714;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#30340;&#33021;&#28304;&#20998;&#37197;&#20844;&#21496;&#65292;&#21253;&#25324;&#32654;&#22269;&#30005;&#21147;&#12289;Commonwealth Edison&#12289;Dayton Power and Light&#20197;&#21450;&#23486;&#22805;&#27861;&#23612;&#20122;-&#26032;&#27901;&#35199;-&#39532;&#37324;&#20848;&#20114;&#32852;&#32593;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;REDf&#27169;&#22411;&#19982;&#20854;&#20182;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;REDf&#27169;&#22411;&#22312;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12289;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#20915;&#23450;&#31995;&#25968;&#31561;&#20934;&#30830;&#24230;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;REDf&#21487;&#20197;&#20316;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#30340;&#21487;&#38752;&#24037;&#20855;&#65292;&#24182;&#25552;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#32435;&#20837;&#26234;&#33021;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future. However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity. In this paper, we propose a deep learning-based approach for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. We use long short-term memory networks, which are well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four datasets of historical energy demand data from different energy distribution companies including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is also compared with two 
&lt;/p&gt;</description></item></channel></rss>