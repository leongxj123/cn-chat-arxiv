<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01607</link><description>&lt;p&gt;
&#20855;&#26377;&#24517;&#35201;&#22238;&#28335;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
Natural Counterfactuals With Necessary Backtracking
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#25552;&#20379;&#35299;&#37322;&#21644;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;Judea Pearl&#30340;&#30740;&#31350;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#24456;&#20248;&#38597;&#65292;&#20294;&#20854;&#29983;&#25104;&#21453;&#20107;&#23454;&#24773;&#26223;&#24448;&#24448;&#38656;&#35201;&#36807;&#20110;&#33073;&#31163;&#23454;&#38469;&#24773;&#26223;&#30340;&#24178;&#39044;&#65292;&#22240;&#27492;&#38590;&#20197;&#23454;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26694;&#26550;&#21644;&#19968;&#31181;&#26681;&#25454;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#25913;&#36827;&#65292;&#20801;&#35768;&#23545;&#22240;&#26524;&#21069;&#32622;&#21464;&#37327;&#36827;&#34892;&#25913;&#21464;&#20197;&#26368;&#23567;&#21270;&#19982;&#23454;&#38469;&#24773;&#26223;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#24615;&#20934;&#21017;&#20801;&#35768;&#20294;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;LCG&#65292;&#36890;&#36807;&#27169;&#25311;&#21508;&#31181;&#36719;&#20214;&#36807;&#31243;&#27169;&#22411;&#20197;&#21450;&#21033;&#29992;&#21327;&#20316;&#21644;&#25216;&#26415;&#25552;&#39640;&#20195;&#30721;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15852</link><description>&lt;p&gt;
&#24403;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#36935;&#19978;&#36719;&#20214;&#24320;&#21457;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
When LLM-based Code Generation Meets the Software Development Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15852
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;LCG&#65292;&#36890;&#36807;&#27169;&#25311;&#21508;&#31181;&#36719;&#20214;&#36807;&#31243;&#27169;&#22411;&#20197;&#21450;&#21033;&#29992;&#21327;&#20316;&#21644;&#25216;&#26415;&#25552;&#39640;&#20195;&#30721;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#36807;&#31243;&#27169;&#22411;&#22312;&#20419;&#36827;&#36719;&#20214;&#22242;&#38431;&#20869;&#21327;&#20316;&#19982;&#27807;&#36890;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#30340;&#24320;&#21457;&#20219;&#21153;&#26041;&#38754;&#25285;&#24403;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LCG&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#21040;&#25104;&#29087;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#21551;&#21457;&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#12290;LCG&#21033;&#29992;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20195;&#29702;&#26469;&#27169;&#25311;&#21508;&#31181;&#36719;&#20214;&#36807;&#31243;&#27169;&#22411;&#65292;&#21363;LCGWaterfall&#12289;LCGTDD&#21644;LCGScrum&#12290;&#27599;&#20010;&#27169;&#22411;&#20026;LLM&#20195;&#29702;&#20998;&#37197;&#29305;&#23450;&#35282;&#33394;&#65292;&#22914;&#38656;&#27714;&#24037;&#31243;&#24072;&#12289;&#26550;&#26500;&#24072;&#12289;&#24320;&#21457;&#20154;&#21592;&#12289;&#27979;&#35797;&#20154;&#21592;&#21644;Scrum Master&#65292;&#21453;&#26144;&#20102;&#20856;&#22411;&#30340;&#24320;&#21457;&#27963;&#21160;&#21644;&#27807;&#36890;&#27169;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#24605;&#32500;&#38142;&#21644;&#25552;&#31034;&#32452;&#21512;&#25216;&#26415;&#36827;&#34892;&#21327;&#20316;&#65292;&#20195;&#29702;&#19981;&#26029;&#23436;&#21892;&#33258;&#36523;&#20197;&#25552;&#39640;&#20195;&#30721;&#36136;&#37327;&#12290;&#22312;GPT3.5&#20316;&#20026;&#22522;&#30784;LLM&#21644;&#22522;&#20934;(GPT)&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LCG&#22312;&#22235;&#20010;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#34920;&#29616;&#65306;HumanEval&#12289;HumanEval-ET&#12289;MBPP&#21644;MBPP-ET&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15852v1 Announce Type: cross  Abstract: Software process models play a pivotal role in fostering collaboration and communication within software teams, enabling them to tackle intricate development tasks effectively. This paper introduces LCG, a code generation framework inspired by established software engineering practices. LCG leverages multiple Large Language Model (LLM) agents to emulate various software process models, namely LCGWaterfall, LCGTDD, and LCGScrum. Each model assigns LLM agents specific roles such as requirement engineer, architect, developer, tester, and scrum master, mirroring typical development activities and communication patterns. Through collaborative efforts utilizing chain-of-thought and prompt composition techniques, the agents continuously refine themselves to enhance code quality. Utilizing GPT3.5 as the underlying LLM and baseline (GPT), we evaluate LCG across four code generation benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Results
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;$h$-PMD&#65292;&#23427;&#36890;&#36807;&#22312;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#32467;&#21512;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#21644;&#21069;&#30651;&#28145;&#24230;$h&#65292;&#20197;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.14156</link><description>&lt;p&gt;
&#20855;&#26377;&#21069;&#30651;&#29305;&#24615;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Mirror Descent with Lookahead
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;$h$-PMD&#65292;&#23427;&#36890;&#36807;&#22312;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#32467;&#21512;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#21644;&#21069;&#30651;&#28145;&#24230;$h&#65292;&#20197;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#65288;PMD&#65289;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#31639;&#27861;&#26694;&#26550;&#65292;&#21253;&#25324;&#20960;&#31181;&#37325;&#35201;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#22914;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65288;&#22914;TRPO&#21644;PPO&#65289;&#30456;&#32852;&#31995;&#12290;PMD&#21487;&#20197;&#30475;&#20316;&#26159;&#23454;&#29616;&#27491;&#21017;&#21270;1&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#30340;&#36719;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;1&#27493;&#36138;&#24515;&#31574;&#30053;&#21487;&#33021;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#26368;&#36817;&#22312;RL&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#22914;AlphaGo&#21644;AlphaZero&#24050;&#32463;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#22810;&#27493;&#39588;&#65292;&#36138;&#24515;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#23427;&#20204;&#30340;1&#27493;&#39588;&#23545;&#24212;&#29289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;PMD&#31639;&#27861;&#65292;&#31216;&#20026;$h$-PMD&#65292;&#23427;&#23558;&#20855;&#26377;&#21069;&#30651;&#28145;&#24230;$h$&#30340;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#32467;&#21512;&#21040;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#25240;&#25187;&#22240;&#23376;&#20026;$\gamma$&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;$h$-PMD&#21487;&#20197;&#25512;&#24191;&#26631;&#20934;&#30340;PMD&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14156v1 Announce Type: cross  Abstract: Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\gamma$, we show that $h$-PMD which generalizes the standard PMD enj
&lt;/p&gt;</description></item><item><title>ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;</title><link>https://arxiv.org/abs/2403.05266</link><description>&lt;p&gt;
ERBench&#65306;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05266
&lt;/p&gt;
&lt;p&gt;
ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24187;&#35273;&#22522;&#20934;&#35201;&#20040;&#26159;&#38745;&#24577;&#30340;&#65292;&#35201;&#20040;&#32570;&#20047;&#21487;&#35843;&#25972;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#29616;&#26377;&#30340;&#20851;&#31995;&#25968;&#25454;&#24211;&#26159;&#26500;&#24314;&#22522;&#20934;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ERBench&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#36716;&#25442;&#20026;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#65288;ER&#65289;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#24211;&#27169;&#24335;&#12289;&#35760;&#24405;&#21644;&#21151;&#33021;&#20381;&#36182;&#26469;&#26500;&#24314;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22806;&#38190;&#32422;&#26463;&#26469;&#36830;&#25509;&#20851;&#31995;&#21644;&#26500;&#24314;&#22810;&#36339;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20219;&#24847;&#22797;&#26434;&#65292;&#29992;&#20110;&#35843;&#35797;LLMs&#30340;&#20013;&#38388;&#31572;&#26696;&#12290;&#26368;&#21518;&#65292;ERBench&#25903;&#25345;&#25345;&#32493;&#35780;&#20272;&#65292;&#22810;&#27169;&#24577;qu
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05266v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal qu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.04690</link><description>&lt;p&gt;
&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;: &#22312;&#32447;&#31243;&#22359;&#32423;&#21035;&#20943;&#23569;&#33258;&#27880;&#24847;&#21147;&#30340;O(n^2)&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04690
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37051;&#22495;&#27880;&#24847;&#21147;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#26631;&#35760;&#30340;&#27880;&#24847;&#21147;&#33539;&#22260;&#20026;&#20854;&#26368;&#36817;&#30340;&#37051;&#23621;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#25104;&#26412;&#12290;&#35813;&#38480;&#21046;&#30001;&#31383;&#21475;&#22823;&#23567;&#21644;&#25193;&#24352;&#22240;&#23376;&#21442;&#25968;&#21270;&#65292;&#20171;&#20110;&#32447;&#24615;&#25237;&#24433;&#21644;&#33258;&#27880;&#24847;&#21147;&#20043;&#38388;&#32472;&#21046;&#20102;&#21487;&#33021;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#35889;&#12290;&#37051;&#22495;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#26356;&#19968;&#33324;&#22320;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#38271;&#26399;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#39640;&#31209;&#30340;&#31354;&#38388;&#65288;2-D&#21644;3-D&#65289;&#65292;&#20419;&#20351;&#24320;&#21457;&#23450;&#21046;&#20869;&#26680;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#20869;&#26680;&#22312;&#21151;&#33021;&#25110;&#24615;&#33021;&#26041;&#38754;&#21463;&#38480;&#65292;&#22914;&#26524;&#19981;&#26159;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#37051;&#22495;&#27880;&#24847;&#21147;&#21487;&#20197;&#34920;&#31034;&#20026;&#25209;&#37327;&#21270;&#30340;GEMM&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#24182;&#20026;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#23454;&#29616;&#23427;&#12290;&#19982;&#29616;&#26377;&#30340;&#31616;&#21333;&#20869;&#26680;&#30456;&#27604;&#65292;&#36825;&#20123;&#20869;&#26680;&#24179;&#22343;&#25552;&#20379;&#20102;&#20998;&#21035;&#26159;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;&#20840;&#31934;&#24230;&#24310;&#36831;&#25913;&#36827;&#20998;&#21035;&#20026;895%&#21644;272%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04690v1 Announce Type: cross  Abstract: Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. 
&lt;/p&gt;</description></item><item><title>NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15393</link><description>&lt;p&gt;
NeuralThink: &#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#36827;&#34892;&#22806;&#25512;&#30340;&#31639;&#27861;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15393
&lt;/p&gt;
&lt;p&gt;
NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25797;&#38271;&#27169;&#24335;&#35782;&#21035;&#65292;&#20294;&#22312;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#26041;&#24335;&#19978;&#22788;&#29702;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#24605;&#32500;&#26041;&#27861;&#23637;&#29616;&#20102;&#23398;&#20064;&#21487;&#20197;&#22806;&#25512;&#30340;&#31639;&#27861;&#30340;&#28508;&#21147;&#65306;&#22312;&#36739;&#23567;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#24182;&#22312;&#36739;&#22823;&#30340;&#29615;&#22659;&#20013;&#25191;&#34892;&#23398;&#21040;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23616;&#38480;&#20110;&#23545;&#31216;&#20219;&#21153;&#65292;&#21363;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#30456;&#21516;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; NeuralThink&#65292;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#20854;&#20013;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#23545;&#31216;&#20219;&#21153;&#22806;&#25512;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NeuralThink &#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#19968;&#30452;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15393v1 Announce Type: cross  Abstract: While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;&#23427;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07963</link><description>&lt;p&gt;
SMX: &#19987;&#23478;&#36845;&#20195;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
SMX: Sequential Monte Carlo Planning for Expert Iteration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07963
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;&#23427;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#33021;&#22815;&#22312;&#20915;&#31574;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#21033;&#29992;&#35268;&#21010;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26641;&#29366;&#25628;&#32034;&#26041;&#27861;&#21644;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25628;&#32034;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#36136;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#34429;&#28982;&#23454;&#36341;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#37096;&#20998;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20173;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35745;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;SMX&#22522;&#20110;&#25511;&#21046;&#20316;&#20026;&#25512;&#26029;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#21463;&#30410;&#20110;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23427;&#22522;&#20110;&#37319;&#26679;&#30340;&#25628;&#32034;&#26041;&#27861;&#20351;&#20854;&#36866;&#24212;&#20855;&#26377;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;SMX&#20801;&#35768;&#39640;&#24230;&#24182;&#34892;&#21270;&#24182;&#21487;&#20197;&#36816;&#34892;&#20110;&#21508;&#31867;&#35745;&#31639;&#26426;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing agents that can leverage planning abilities during their decision and learning processes is critical to the advancement of Artificial Intelligence. Recent works have demonstrated the effectiveness of combining tree-based search methods and self-play learning mechanisms. Yet, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they still demand extensive computational resources, which hinders their applicability. In this paper, we introduce SMX, a model-based planning algorithm that utilises scalable Sequential Monte Carlo methods to create an effective self-learning mechanism. Grounded in the theoretical framework of control as inference, SMX benefits from robust theoretical underpinnings. Its sampling-based search approach makes it adaptable to environments with both discrete and continuous action spaces. Furthermore, SMX allows for high parallelisation and can run on h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.06255</link><description>&lt;p&gt;
&#36827;&#21462;&#30340;&#40077;&#21187;&#36890;&#36807;&#25552;&#31034;&#23545;&#25239;&#35843;&#25972;&#25269;&#21046;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#32469;&#36807;&#20869;&#32622;&#30340;&#23433;&#20840;&#25514;&#26045;&#24182;&#25552;&#20379;&#21361;&#38505;&#25110;&#38750;&#27861;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36234;&#29425;&#34892;&#20026;&#12290;&#20026;&#20102;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38598;&#20013;&#22312;&#20869;&#23481;&#36807;&#28388;&#25110;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning&#65288;PAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#25105;&#20204;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20132;&#26367;&#26356;&#26032;&#25915;&#20987;&#21644;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25552;&#31034;&#35843;&#25972;&#30340;&#35282;&#24230;&#23454;&#26045;&#38450;&#24481;&#30340;&#20154;&#12290;&#19968;&#26086;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;LLMs&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25269;&#24481;&#36234;&#29425;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
&lt;/p&gt;</description></item><item><title>DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.05421</link><description>&lt;p&gt;
DiffTOP: &#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05421
&lt;/p&gt;
&lt;p&gt;
DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffTOP&#65292;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#65292;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#29983;&#25104;&#21160;&#20316;&#12290;&#36712;&#36857;&#20248;&#21270;&#26159;&#19968;&#31181;&#22312;&#25511;&#21046;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#30001;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#25439;&#22833;&#23545;&#20110;&#36712;&#36857;&#20248;&#21270;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#36712;&#36857;&#20248;&#21270;&#30340;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#12290;DiffTOP&#35299;&#20915;&#20102;&#20043;&#21069;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#22240;&#20026;DiffTOP&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#25439;&#22833;&#30452;&#25509;&#26368;&#22823;&#21270;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;DiffTOP&#22312;&#26631;&#20934;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#22871;&#20214;&#20013;&#36827;&#34892;&#20102;&#27169;&#20223;&#23398;&#20064;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#22312;&#20449;&#20219;&#28216;&#25103;&#20013;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#20559;&#35265;&#21644;&#23545;&#20195;&#29702;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.04559</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Model Agents Simulate Human Trust Behaviors?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04559
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#22312;&#20449;&#20219;&#28216;&#25103;&#20013;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#20559;&#35265;&#21644;&#23545;&#20195;&#29702;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#32463;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#20316;&#20026;&#27169;&#25311;&#24037;&#20855;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#22312;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;LLM&#20195;&#29702;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20154;&#31867;&#20114;&#21160;&#20013;&#26368;&#20851;&#38190;&#30340;&#34892;&#20026;&#20043;&#19968;&#65292;&#20449;&#20219;&#65292;&#26088;&#22312;&#35843;&#26597;LLM&#20195;&#29702;&#26159;&#21542;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#65292;&#22312;&#34987;&#34892;&#20026;&#32463;&#27982;&#23398;&#24191;&#27867;&#25509;&#21463;&#30340;&#20449;&#20219;&#28216;&#25103;&#26694;&#26550;&#19979;&#65292;LLM&#20195;&#29702;&#36890;&#24120;&#34920;&#29616;&#20986;&#20449;&#20219;&#34892;&#20026;&#65292;&#31216;&#20026;&#20195;&#29702;&#20449;&#20219;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#20195;&#29702;&#22312;&#20449;&#20219;&#34892;&#20026;&#26041;&#38754;&#19982;&#20154;&#31867;&#20855;&#26377;&#36739;&#39640;&#30340;&#34892;&#20026;&#19968;&#33268;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;LLM&#20195;&#29702;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20195;&#29702;&#20449;&#20219;&#20013;&#30340;&#20559;&#35265;&#20197;&#21450;&#20195;&#29702;&#20449;&#20219;&#22312;&#23545;&#20195;&#29702;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#26041;&#38754;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21253;&#25324;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#22312;&#20869;&#30340;&#26465;&#20214;&#19979;&#20195;&#29702;&#20449;&#20219;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strate
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#65292;&#35813;&#21161;&#25163;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20026;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.03941</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Discovery of the Hidden World with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#65292;&#35813;&#21161;&#25163;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20026;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#36215;&#28304;&#20110;&#20174;&#24050;&#30693;&#20107;&#23454;&#21644;&#35266;&#23519;&#20013;&#21457;&#29616;&#26032;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#27979;&#37327;&#21464;&#37327;&#65292;&#36890;&#24120;&#30001;&#20154;&#31867;&#19987;&#23478;&#25552;&#20379;&#65292;&#20197;&#25214;&#21040;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#22240;&#26524;&#21464;&#37327;&#36890;&#24120;&#26080;&#27861;&#33719;&#21462;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#20026;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#39640;&#32423;&#38544;&#34255;&#21464;&#37327;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#12290;COAT&#23558;LLMs&#20316;&#20026;&#22240;&#32032;&#25552;&#20379;&#22120;&#24341;&#20837;&#65292;&#25552;&#21462;&#20986;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;LLMs&#36824;&#21487;&#20197;&#34987;&#25351;&#31034;&#25552;&#20379;&#29992;&#20110;&#25910;&#38598;&#25968;&#25454;&#20540;&#65288;&#20363;&#22914;&#27880;&#37322;&#26631;&#20934;&#65289;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#24182;&#23558;&#21407;&#22987;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#19968;&#27493;&#35299;&#26512;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#27880;&#37322;&#25968;&#25454;&#23558;&#34987;&#36755;&#20837;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;LoT&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#21319;&#20027;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LoT&#33021;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.02769</link><description>&lt;p&gt;
&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;: &#26131;&#20110;&#27169;&#20223;&#30340;&#21487;&#25512;&#24191;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02769
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;LoT&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#21319;&#20027;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LoT&#33021;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21363;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;Learning from Teaching&#65292;&#31616;&#31216;LoT&#65289;&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#12290;&#21463;&#21040;&#20154;&#31867;&#25429;&#25417;&#31616;&#26126;&#25277;&#35937;&#27169;&#24335;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#25512;&#24191;&#30340;&#20851;&#31995;&#26356;&#23481;&#26131;&#25945;&#25480;&#12290;LoT&#36890;&#36807;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#20010;&#27010;&#24565;&#65292;&#36890;&#36807;&#25552;&#20379;&#21453;&#39304;&#26469;&#35757;&#32451;&#20027;&#27169;&#22411;&#21644;&#25913;&#36827;&#20027;&#27169;&#22411;&#65292;&#20197;&#25429;&#25417;&#26356;&#22810;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24341;&#20837;LoT&#30456;&#27604;&#20165;&#22312;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#22909;&#22788;&#12290;&#36825;&#34920;&#26126;&#20102;LoT&#22312;&#35782;&#21035;&#21487;&#25512;&#24191;&#20449;&#24687;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data. It suggests the effectiveness of LoT in identifying generalizable information without falling into th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#30495;&#23454;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#25351;&#20986;&#20102;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#30340;&#38556;&#30861;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#20851;&#27880;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.02420</link><description>&lt;p&gt;
2024&#24180;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Factuality of Large Language Models in the Year 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#30495;&#23454;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#25351;&#20986;&#20102;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#30340;&#38556;&#30861;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#20851;&#27880;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23588;&#20854;&#26159;&#22312;&#32842;&#22825;&#26041;&#38754;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#21518;&#65292;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#22320;&#26041;&#30452;&#25509;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#65292;&#20351;&#20154;&#20204;&#20174;&#25628;&#32034;&#12289;&#25552;&#21462;&#21644;&#25972;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#30340;&#36807;&#31243;&#20013;&#24471;&#21040;&#35299;&#33073;&#12290;&#28982;&#32780;&#65292;&#24456;&#22810;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#22238;&#31572;&#26159;&#38169;&#35823;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;LLM&#30495;&#23454;&#24615;&#30340;&#30740;&#31350;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#26088;&#22312;&#25214;&#20986;&#20027;&#35201;&#25361;&#25112;&#21450;&#20854;&#21407;&#22240;&#65292;&#24182;&#25351;&#20986;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#20998;&#26512;&#24320;&#25918;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#38754;&#20020;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#36824;&#23637;&#26395;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2312.04828</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;
&lt;/p&gt;
&lt;p&gt;
Human-Readable Fingerprint for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#21644;&#37197;&#22871;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#35768;&#21487;&#35777;&#65292;&#20445;&#25252;LLM&#30340;&#29256;&#26435;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#30340;&#21442;&#25968;&#20462;&#25913;&#65292;&#30830;&#23450;LLM&#30340;&#21407;&#22987;&#22522;&#26412;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;LLM&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#22320;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#65292;&#32780;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#27169;&#22411;&#25910;&#25947;&#21518;&#65292;LLM&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#20445;&#25345;&#31283;&#23450;&#65292;&#36890;&#36807;&#21518;&#32493;&#30340;&#35757;&#32451;&#27493;&#39588;&#65292;&#21253;&#25324;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;RLHF&#65292;&#20960;&#20046;&#27809;&#26377;&#25200;&#21160;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#36275;&#22815;&#26465;&#20214;&#12290;&#36890;&#36807;&#32487;&#32493;&#35757;&#32451;LLM&#24182;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#39033;&#26469;&#25512;&#24320;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#21521;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#24517;&#35201;&#24615;&#65292;&#32467;&#26524;&#20351;&#24471;&#27169;&#22411;&#21463;&#25439;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26041;&#21521;&#23481;&#26131;&#21463;&#21040;&#31616;&#21333;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22914;&#32500;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2312.02119</link><description>&lt;p&gt;
&#25915;&#20987;&#26641;&#65306;&#33258;&#21160;&#30772;&#35299;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02119
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#20173;&#22312;&#29983;&#25104;&#26377;&#23475;&#12289;&#24102;&#20559;&#35265;&#21644;&#26377;&#27602;&#20869;&#23481;&#65292;&#36825;&#19968;&#28857;&#30001;&#20154;&#20026;&#35774;&#35745;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#26222;&#36941;&#23384;&#22312;&#24471;&#20197;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#65292;&#20165;&#38656;&#35201;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;TAP&#21033;&#29992;LLM&#26469;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#36845;&#20195;&#22320;&#20248;&#21270;&#20505;&#36873;&#65288;&#25915;&#20987;&#65289;&#25552;&#31034;&#65292;&#30452;&#21040;&#29983;&#25104;&#30340;&#25552;&#31034;&#20043;&#19968;&#36234;&#29425;&#30446;&#26631;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#22312;&#23558;&#25552;&#31034;&#21457;&#36865;&#32473;&#30446;&#26631;&#20043;&#21069;&#65292;TAP&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#24182;&#31227;&#38500;&#21487;&#33021;&#19981;&#20250;&#23548;&#33268;&#36234;&#29425;&#30340;&#25552;&#31034;&#12290;&#20351;&#29992;&#24605;&#32500;&#26641;&#25512;&#29702;&#20351;TAP&#33021;&#22815;&#22312;&#22823;&#37327;&#25552;&#31034;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#32780;&#20462;&#21098;&#21017;&#20943;&#23569;&#20102;&#21457;&#36865;&#32473;&#30446;&#26631;&#30340;&#24635;&#26597;&#35810;&#25968;&#37327;&#12290;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;TAP&#29983;&#25104;&#30340;&#25552;&#31034;&#36234;&#29425;&#20102;&#36229;&#36807;80%&#30340;&#26368;&#20808;&#36827;LLMs&#65288;&#21253;&#25324;GPT4&#21644;GPT4-Turbo&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02119v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thought reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10862</link><description>&lt;p&gt;
&#22522;&#20110;&#21098;&#26525;&#30340;&#20445;&#25252;: &#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#23545;&#40784;&#30340;LLMs&#30340;&#36234;&#29425;&#25269;&#25239;&#21147;
&lt;/p&gt;
&lt;p&gt;
Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#35825;&#20351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#21644;&#36829;&#27861;&#20869;&#23481;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21098;&#26525;LLM&#21442;&#25968;&#22810;&#36798;20&#65285;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#23427;&#20204;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#24182;&#19988;&#19981;&#25439;&#23475;&#20854;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21098;&#26525;&#21518;&#35266;&#23519;&#21040;&#30340;&#22686;&#24378;&#23433;&#20840;&#24615;&#19982;&#27169;&#22411;&#30340;&#21021;&#22987;&#23433;&#20840;&#35757;&#32451;&#27700;&#24179;&#30456;&#20851;&#65292;&#36825;&#26263;&#31034;&#21098;&#26525;&#30340;&#25928;&#26524;&#21487;&#33021;&#26356;&#26222;&#36941;&#65292;&#20063;&#21487;&#33021;&#36866;&#29992;&#20110;&#36229;&#20986;&#23433;&#20840;&#24615;&#33539;&#30068;&#30340;&#20854;&#20182;LLM&#34892;&#20026;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#12289;&#25554;&#20837;&#21040;&#21313;&#20010;&#19981;&#21516;&#36234;&#29425;&#25552;&#31034;&#20013;&#30340;225&#20010;&#26377;&#23475;&#20219;&#21153;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;LLMs&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#36234;&#29425;&#25552;&#31034;&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#65288;&#22914;LLaMA-2 Chat&#65292;Vicuna&#21644;Mistral Instruct&#65289;&#20855;&#26377;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05975</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25366;&#25496;&#29992;&#25143;&#30340;&#24847;&#22270;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;ICLRec&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32858;&#31867;&#26469;&#25552;&#21462;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#23613;&#31649;&#23427;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#22797;&#26434;&#21644;&#32321;&#29712;&#30340;&#20132;&#26367;&#20248;&#21270;&#38382;&#39064;&#65292;&#23548;&#33268;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#24191;&#20041;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#26694;&#26550;&#20013;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#32463;&#24120;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32858;&#31867;&#20250;&#24433;&#21709;&#22823;&#35268;&#27169;&#34892;&#19994;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24847;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;ELCRec&#65292;&#23427;&#23558;&#34920;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26694;&#26550;&#20013;&#36827;&#34892;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#24182;&#37319;&#29992;&#20998;&#35789;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#25216;&#24039;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#25506;&#32034;&#30340;&#38590;&#24230;&#65292;&#24182;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04459</link><description>&lt;p&gt;
&#23376;&#35789;&#20316;&#20026;&#25216;&#24039;&#65306;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#35789;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning. (arXiv:2309.04459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#24182;&#37319;&#29992;&#20998;&#35789;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#25216;&#24039;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#25506;&#32034;&#30340;&#38590;&#24230;&#65292;&#24182;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#20855;&#26377;&#22256;&#38590;&#65292;&#22240;&#20026;&#38656;&#35201;&#36890;&#36807;&#38271;&#26399;&#30340;&#12289;&#21327;&#35843;&#30340;&#34892;&#21160;&#24207;&#21015;&#25165;&#33021;&#33719;&#24471;&#20219;&#20309;&#22870;&#21169;&#12290;&#32780;&#19988;&#65292;&#22312;&#36830;&#32493;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#65292;&#21487;&#33021;&#30340;&#34892;&#21160;&#25968;&#37327;&#26159;&#26080;&#31351;&#22810;&#30340;&#65292;&#36825;&#21482;&#20250;&#22686;&#21152;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#19968;&#31867;&#26041;&#27861;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#25910;&#38598;&#30340;&#20132;&#20114;&#25968;&#25454;&#20013;&#24418;&#25104;&#26102;&#38388;&#19978;&#24310;&#20280;&#30340;&#34892;&#21160;&#65292;&#36890;&#24120;&#31216;&#20026;&#25216;&#24039;&#65292;&#24182;&#22312;&#36825;&#20010;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#30340;&#20248;&#21270;&#12290;&#36890;&#24120;&#36825;&#26679;&#30340;&#26041;&#27861;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#38656;&#35201;&#19968;&#20010;&#28459;&#38271;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#24320;&#22987;&#20043;&#21069;&#24418;&#25104;&#25216;&#24039;&#12290;&#37492;&#20110;&#20808;&#21069;&#30340;&#35777;&#25454;&#34920;&#26126;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24182;&#19981;&#38656;&#35201;&#23436;&#25972;&#30340;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#24039;&#29983;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#65292;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20511;&#37492;&#26469;&#30340;&#20998;&#35789;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration in sparse-reward reinforcement learning is difficult due to the requirement of long, coordinated sequences of actions in order to achieve any reward. Moreover, in continuous action spaces there are an infinite number of possible actions, which only increases the difficulty of exploration. One class of methods designed to address these issues forms temporally extended actions, often called skills, from interaction data collected in the same domain, and optimizes a policy on top of this new action space. Typically such methods require a lengthy pretraining phase, especially in continuous action spaces, in order to form the skills before reinforcement learning can begin. Given prior evidence that the full range of the continuous action space is not required in such tasks, we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#21487;&#20197;&#28165;&#38500;&#38544;&#24418;&#22270;&#20687;&#27700;&#21360;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#21270;&#20877;&#29983;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;&#25152;&#26377;&#38544;&#24418;&#27700;&#21360;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#24182;&#38024;&#23545;&#19968;&#31181;&#20855;&#26377;&#24377;&#24615;&#30340;&#27700;&#21360;RivaGAN&#65292;&#20877;&#29983;&#25915;&#20987;&#21487;&#20197;&#21435;&#38500;93-99%&#30340;&#27700;&#21360;&#12290;</title><link>http://arxiv.org/abs/2306.01953</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#24335;AI&#65292;&#35777;&#26126;&#20102;&#38544;&#24418;&#22270;&#20687;&#27700;&#21360;&#26159;&#21487;&#28165;&#38500;&#30340;
&lt;/p&gt;
&lt;p&gt;
Invisible Image Watermarks Are Provably Removable Using Generative AI. (arXiv:2306.01953v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01953
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#21487;&#20197;&#28165;&#38500;&#38544;&#24418;&#22270;&#20687;&#27700;&#21360;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#21270;&#20877;&#29983;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;&#25152;&#26377;&#38544;&#24418;&#27700;&#21360;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#24182;&#38024;&#23545;&#19968;&#31181;&#20855;&#26377;&#24377;&#24615;&#30340;&#27700;&#21360;RivaGAN&#65292;&#20877;&#29983;&#25915;&#20987;&#21487;&#20197;&#21435;&#38500;93-99%&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24418;&#27700;&#21360;&#36890;&#36807;&#23884;&#20837;&#21482;&#26377;&#26435;&#21033;&#25317;&#26377;&#32773;&#21487;&#20197;&#26816;&#27979;&#21040;&#30340;&#38544;&#34255;&#20449;&#24687;&#26469;&#20445;&#25252;&#22270;&#20687;&#30340;&#29256;&#26435;&#12290;&#23427;&#20204;&#36824;&#38450;&#27490;&#20154;&#20204;&#28389;&#29992;&#30001;AI&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#21270;&#20877;&#29983;&#25915;&#20987;&#26469;&#28165;&#38500;&#36825;&#20123;&#38544;&#24418;&#27700;&#21360;&#12290;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#39318;&#20808;&#21521;&#22270;&#20687;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#30772;&#22351;&#27700;&#21360;&#65292;&#28982;&#21518;&#37325;&#24314;&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#22270;&#20687;&#38477;&#22122;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#23454;&#20363;&#21270;&#12290;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#38544;&#24418;&#27700;&#21360;&#37117;&#23481;&#26131;&#21463;&#21040;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#12290;&#23545;&#20110;&#19968;&#20010;&#29305;&#21035;&#26377;&#24377;&#24615;&#30340;&#27700;&#21360;RivaGAN&#65292;&#20877;&#29983;&#25915;&#20987;&#21487;&#20197;&#21435;&#38500;93-99%&#30340;&#38544;&#24418;&#27700;&#21360;&#65292;&#32780;&#22522;&#32447;&#25915;&#20987;&#21482;&#33021;&#21435;&#38500;&#19981;&#36229;&#36807;3%&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#19981;&#35201;&#27714;&#24102;&#27700;&#21360;&#30340;&#22270;&#20687;&#19982;&#21407;&#22987;&#22270;&#20687;&#30456;&#21516;&#65292;&#20445;&#25345;&#22270;&#20687;&#35821;&#20041;&#30456;&#20284;&#30340;&#27700;&#21360;&#21487;&#33021;&#26159;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invisible watermarks safeguard images' copyright by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models. We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and empirical results, we show that all invisible watermarks are vulnerable to the proposed attack. For a particularly resilient watermark, RivaGAN, regeneration attacks remove 93-99% of the invisible watermarks while the baseline attacks remove no more than 3%. However, if we do not require the watermarked image to look the same as the original one, watermarks that keep the image semantically similar can be an alternative
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01762</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#25552;&#32431;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#37096;&#32626;&#20026;&#21508;&#31181;&#26085;&#24120;&#26381;&#21153;&#65292;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#36867;&#36991;&#25915;&#20987;&#26159;&#26368;&#26222;&#36941;&#30340;&#19968;&#31181;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25110;&#21033;&#29992;&#22823;&#37327;&#28165;&#27905;&#25968;&#25454;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#37096;&#32626;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23545;&#22312;&#32447;&#26381;&#21153;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#24403;&#26816;&#27979;&#21040;&#26576;&#31181;&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#26102;&#65292;&#26381;&#21153;&#25552;&#20379;&#32773;&#21482;&#33021;&#33719;&#24471;&#26377;&#38480;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#32780;&#22823;&#37327;&#30340;&#28165;&#27905;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#21517;&#20026;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#26088;&#22312;&#24555;&#36895;&#38450;&#24481;&#20855;&#26377;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#31034;&#20363;&#38480;&#21046;&#30340;&#21407;&#22987;&#26381;&#21153;&#27169;&#22411;&#30340;&#26576;&#31181;&#25915;&#20987;&#12290;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#36716;&#31227;&#23398;&#20064;&#33391;&#22909;&#21021;&#22987;&#21270;&#30340;&#36890;&#29992;&#36235;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;Transformer&#26469;&#25552;&#32431;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#39044;&#35757;&#32451;&#30340;Transformer&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#40723;&#21169;&#25552;&#32431;&#21518;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#25509;&#36817;&#28165;&#26224;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RaPiD&#22312;&#38450;&#24481;&#21508;&#31181;&#20855;&#26377;&#38480;&#25968;&#25454;&#30340;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#65292;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01158</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#30693;&#35782;&#30340;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge. (arXiv:2306.01158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#65292;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#23398;&#32773;&#20204;&#25552;&#20986;&#20102;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#30340;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#32452;&#21512;&#36215;&#26469;&#20197;&#34893;&#29983;&#20986;&#21487;&#20197;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#22522;&#30784;&#27169;&#22359;&#36890;&#24120;&#26159;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#65292;&#20063;&#20801;&#35768;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#30340;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#32570;&#20047;&#22788;&#29702;&#21644;&#25972;&#21512;&#22810;&#31181;&#31867;&#22411;&#20449;&#24687;&#65288;&#30693;&#35782;&#65289;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21017;&#65292;&#23376;&#30446;&#26631;&#21644;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#36825;&#31181;&#26032;&#30340;&#26694;&#26550;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#30340;&#21464;&#20307;&#65292;&#21363;&#22686;&#24378;&#35760;&#24518;&#30340;&#20210;&#35009;&#22120;&#65292;&#23427;&#22686;&#21152;&#20102;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#24050;&#26377;&#30340;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#65292;&#21516;&#26102;&#20063;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to mitigate some of the inefficiencies of Reinforcement Learning (RL), modular approaches composing different decision-making policies to derive agents capable of performing a variety of tasks have been proposed. The modules at the basis of these architectures are generally reusable, also allowing for "plug-and-play" integration. However, such solutions still lack the ability to process and integrate multiple types of information (knowledge), such as rules, sub-goals, and skills. We propose Augmented Modular Reinforcement Learning (AMRL) to address these limitations. This new framework uses an arbitrator to select heterogeneous modules and seamlessly incorporate different types of knowledge. Additionally, we introduce a variation of the selection mechanism, namely the Memory-Augmented Arbitrator, which adds the capability of exploiting temporal information. We evaluate the proposed mechanisms on established as well as new environments and benchmark them against prominent deep 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#20998;&#23618;&#26550;&#26500;&#65292;&#21033;&#29992;&#26102;&#38388;&#25277;&#35937;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36830;&#32493;&#25511;&#21046;&#65292;&#20855;&#26377;&#33410;&#33021;&#12289;&#25345;&#32493;&#25506;&#32034;&#12289;&#20943;&#23569;&#20915;&#31574;&#12289;&#20943;&#23569;&#25238;&#21160;&#21644;&#22686;&#21152;&#21160;&#20316;&#37325;&#22797;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.18701</link><description>&lt;p&gt;
&#39640;&#25928;&#36830;&#32493;&#25511;&#21046;&#30340;&#26102;&#38388;&#20998;&#23618;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Temporally Layered Architecture for Efficient Continuous Control. (arXiv:2305.18701v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18701
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#20998;&#23618;&#26550;&#26500;&#65292;&#21033;&#29992;&#26102;&#38388;&#25277;&#35937;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36830;&#32493;&#25511;&#21046;&#65292;&#20855;&#26377;&#33410;&#33021;&#12289;&#25345;&#32493;&#25506;&#32034;&#12289;&#20943;&#23569;&#20915;&#31574;&#12289;&#20943;&#23569;&#25238;&#21160;&#21644;&#22686;&#21152;&#21160;&#20316;&#37325;&#22797;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#20998;&#23618;&#26550;&#26500;&#65288;TLA&#65289;&#65292;&#29992;&#20110;&#20855;&#26377;&#26368;&#23567;&#33021;&#37327;&#28040;&#32791;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#25511;&#21046;&#12290;TLA&#23558;&#24555;&#36895;&#31574;&#30053;&#21644;&#24930;&#36895;&#31574;&#30053;&#23618;&#21472;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#26102;&#38388;&#25277;&#35937;&#65292;&#20351;&#27599;&#20010;&#23618;&#21487;&#20197;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#20511;&#37492;&#20102;&#20154;&#33041;&#30340;&#33410;&#33021;&#26426;&#21046;&#65292;&#26681;&#25454;&#29615;&#22659;&#38656;&#27714;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#25191;&#34892;&#21160;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;TLA&#38500;&#20102;&#33410;&#33021;&#20043;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#35768;&#22810;&#39069;&#22806;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#25345;&#32493;&#25506;&#32034;&#12289;&#20943;&#23569;&#38656;&#27714;&#20915;&#31574;&#12289;&#20943;&#23569;&#25238;&#21160;&#21644;&#22686;&#21152;&#21160;&#20316;&#37325;&#22797;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;TLA&#22312;&#22810;&#20010;&#37325;&#35201;&#25351;&#26631;&#19978;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22810;&#30446;&#26631;&#35780;&#20998;&#26469;&#23450;&#24615;&#35780;&#20272;&#36830;&#32493;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;TLA&#20855;&#26377;&#26174;&#33879;&#26356;&#22909;&#30340;&#35780;&#20998;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#31639;&#27861;&#22312;&#24930;&#36895;&#31574;&#30053;&#21644;
&lt;/p&gt;
&lt;p&gt;
We present a temporally layered architecture (TLA) for temporally adaptive control with minimal energy expenditure. The TLA layers a fast and a slow policy together to achieve temporal abstraction that allows each layer to focus on a different time scale. Our design draws on the energy-saving mechanism of the human brain, which executes actions at different timescales depending on the environment's demands. We demonstrate that beyond energy saving, TLA provides many additional advantages, including persistent exploration, fewer required decisions, reduced jerk, and increased action repetition. We evaluate our method on a suite of continuous control tasks and demonstrate the significant advantages of TLA over existing methods when measured over multiple important metrics. We also introduce a multi-objective score to qualitatively assess continuous control policies and demonstrate a significantly better score for TLA. Our training algorithm uses minimal communication between the slow and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05368</link><description>&lt;p&gt;
GNNs: &#21487;&#20197;&#26356;&#24378;&#12289;&#26356;&#26032;&#12289;&#26356;&#24555;
&lt;/p&gt;
&lt;p&gt;
GNNs,You can be Stronger,Deeper and Faster. (arXiv:2305.05368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#21487;&#20197;&#20174;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#36890;&#36807;&#38598;&#25104;&#37051;&#23621;&#33410;&#28857;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#34920;&#29616;&#20986;&#33394;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#24615;&#33021;&#20250;&#38543;&#30528;&#23618;&#25968;&#22686;&#21152;&#32780;&#36880;&#28176;&#38477;&#20302;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#8212;&#8212;k&#36339;&#23376;&#22270;&#32858;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;&#28145;&#23618;GNN&#34920;&#29616;&#36880;&#28176;&#36864;&#21270;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21253;&#25324;&#32858;&#21512;&#23376;&#22270;&#30340;&#37325;&#21472;&#20197;&#21450;&#22522;&#20110;&#27531;&#24046;&#30340;GNN&#23454;&#38469;&#19978;&#21033;&#29992;&#20102;1&#21040;k&#36339;&#23376;&#22270;&#32858;&#21512;&#32467;&#26524;&#26469;&#25552;&#39640;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#36890;&#36807;&#29702;&#35770;&#25512;&#23548;&#35777;&#26126;&#20854;&#27604;&#20043;&#21069;&#30340;&#27531;&#24046;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#21487;&#20197;&#21033;&#29992;1&#21040;k&#36339;&#36291;&#23376;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs), a type of neural network that can learn from graph-structured data and learn the representation of nodes by aggregating their neighbors, have shown excellent performance in downstream tasks.However, it is known that the performance of graph neural networks (GNNs) degrades gradually as the number of layers increases. Based on k-hop subgraph aggregation, which is a new concept, we propose a new perspective to understand the expressive power of GNN.From this perspective, we reveal the potential causes of the performance degradation of the deep traditional GNN - aggregated subgraph overlap, and the fact that the residual-based graph neural networks in fact exploit the aggregation results of 1 to k hop subgraphs to improve the effectiveness.Further, we propose a new sampling-based node-level residual module named SDF, which is shown by theoretical derivation to obtain a superior expressive power compared to previous residual methods by using information from 1 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26234;&#33021;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#30693;&#35782;&#31561;&#20215;&#24615;&#65292;&#25552;&#20986;&#20102;&#22312;&#27169;&#22411;&#19982;&#29289;&#29702;&#31995;&#32479;&#20043;&#38388;&#21516;&#27493;&#30693;&#35782;&#30340;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2204.07481</link><description>&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#30693;&#35782;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
Knowledge Equivalence in Digital Twins of Intelligent Systems. (arXiv:2204.07481v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26234;&#33021;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#30693;&#35782;&#31561;&#20215;&#24615;&#65292;&#25552;&#20986;&#20102;&#22312;&#27169;&#22411;&#19982;&#29289;&#29702;&#31995;&#32479;&#20043;&#38388;&#21516;&#27493;&#30693;&#35782;&#30340;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#21253;&#21547;&#20102;&#23545;&#25152;&#30740;&#31350;&#30340;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#23454;&#26102;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#27169;&#25311;&#26469;&#20248;&#21270;&#29289;&#29702;&#19990;&#30028;&#12290;&#28982;&#32780;&#65292;&#25968;&#23383;&#23402;&#29983;&#25152;&#20570;&#30340;&#20998;&#26512;&#21482;&#26377;&#22312;&#27169;&#22411;&#19982;&#23454;&#38469;&#29289;&#29702;&#19990;&#30028;&#31561;&#20215;&#30340;&#24773;&#20917;&#19979;&#25165;&#26159;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#12290;&#22312;&#29289;&#29702;&#31995;&#32479;&#26159;&#26234;&#33021;&#21644;&#33258;&#20027;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#25345;&#36825;&#26679;&#19968;&#20010;&#31561;&#20215;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#26234;&#33021;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#65292;&#20854;&#20013;&#31995;&#32479;&#20855;&#26377;&#30693;&#35782;&#24863;&#30693;&#33021;&#21147;&#20294;&#33021;&#21147;&#26377;&#38480;&#12290;&#25968;&#23383;&#23402;&#29983;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#31215;&#32047;&#26356;&#22810;&#30693;&#35782;&#65292;&#20174;&#20803;&#23618;&#38754;&#19978;&#25913;&#36827;&#20102;&#29289;&#29702;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#22312;&#34394;&#25311;&#31354;&#38388;&#20013;&#22797;&#21046;&#36825;&#26679;&#19968;&#20010;&#26234;&#33021;&#29289;&#29702;&#31995;&#32479;&#30340;&#30693;&#35782;&#24863;&#30693;&#33021;&#21147;&#38656;&#35201;&#37319;&#29992;&#26032;&#39062;&#30340;&#31561;&#20215;&#24615;&#32500;&#25252;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#22411;&#19982;&#29289;&#29702;&#31995;&#32479;&#20043;&#38388;&#21516;&#27493;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#31561;&#20215;&#24615;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
A digital twin contains up-to-date data-driven models of the physical world being studied and can use simulation to optimise the physical world. However, the analysis made by the digital twin is valid and reliable only when the model is equivalent to the physical world. Maintaining such an equivalent model is challenging, especially when the physical systems being modelled are intelligent and autonomous. The paper focuses in particular on digital twin models of intelligent systems where the systems are knowledge-aware but with limited capability. The digital twin improves the acting of the physical system at a meta-level by accumulating more knowledge in the simulated environment. The modelling of such an intelligent physical system requires replicating the knowledge-awareness capability in the virtual space. Novel equivalence maintaining techniques are needed, especially in synchronising the knowledge between the model and the physical system. This paper proposes the notion of knowled
&lt;/p&gt;</description></item></channel></rss>