<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ChatTracer&#26159;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#23454;&#26102;&#34013;&#29273;&#35774;&#22791;&#36861;&#36394;&#31995;&#32479;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21487;&#38752;&#39640;&#25928;&#30340;BLE&#25968;&#25454;&#21253;&#20998;&#32452;&#31639;&#27861;&#21644;&#32467;&#21512;&#20102;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#31574;&#30053;&#30340;LLM&#24494;&#35843;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.19833</link><description>&lt;p&gt;
ChatTracer&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#23454;&#26102;&#34013;&#29273;&#35774;&#22791;&#36861;&#36394;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ChatTracer: Large Language Model Powered Real-time Bluetooth Device Tracking System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19833
&lt;/p&gt;
&lt;p&gt;
ChatTracer&#26159;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#23454;&#26102;&#34013;&#29273;&#35774;&#22791;&#36861;&#36394;&#31995;&#32479;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21487;&#38752;&#39640;&#25928;&#30340;BLE&#25968;&#25454;&#21253;&#20998;&#32452;&#31639;&#27861;&#21644;&#32467;&#21512;&#20102;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#31574;&#30053;&#30340;LLM&#24494;&#35843;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;OpenAI ChatGPT&#21644;Google Bard&#25152;&#23637;&#31034;&#30340;&#65292;&#24050;&#32463;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#32593;&#32476;&#25216;&#26415;&#20114;&#21160;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;LLM&#19982;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#65288;WSN&#65289;&#30456;&#36830;&#25509;&#30340;&#21487;&#33021;&#24615;&#12290;&#19968;&#20010;&#25104;&#21151;&#30340;&#35774;&#35745;&#19981;&#20165;&#20250;&#23558;LLM&#30340;&#30693;&#35782;&#39046;&#22495;&#24310;&#20280;&#21040;&#29289;&#29702;&#19990;&#30028;&#65292;&#32780;&#19988;&#23558;&#24443;&#24213;&#38761;&#26032;&#20154;&#20204;&#19982;WSN&#30340;&#20114;&#21160;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatTracer&#65292;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#23454;&#26102;&#34013;&#29273;&#35774;&#22791;&#36861;&#36394;&#31995;&#32479;&#12290;ChatTracer&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;&#19968;&#31995;&#21015;&#34013;&#29273;&#21957;&#25506;&#33410;&#28857;&#12289;&#19968;&#20010;&#25968;&#25454;&#24211;&#21644;&#19968;&#20010;&#32463;&#36807;&#31934;&#35843;&#30340;LLM&#12290;ChatTracer&#30340;&#35774;&#35745;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#35266;&#23519;&#65292;&#21830;&#29992;&#30340;&#33529;&#26524;/&#23433;&#21331;&#35774;&#22791;&#21363;&#20351;&#22312;&#31354;&#38386;&#29366;&#24577;&#19979;&#20063;&#20250;&#27599;&#20998;&#38047;&#24191;&#25773;&#25968;&#30334;&#20010;BLE&#25968;&#25454;&#21253;&#12290;&#23427;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65306;i&#65289;&#19968;&#20010;&#21487;&#38752;&#39640;&#25928;&#30340;BLE&#25968;&#25454;&#21253;&#20998;&#32452;&#31639;&#27861;&#65307;&#21644;ii&#65289;&#19968;&#20010;&#32467;&#21512;&#20102;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#31574;&#30053;&#30340;LLM&#24494;&#35843;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19833v1 Announce Type: cross  Abstract: Large language models (LLMs), exemplified by OpenAI ChatGPT and Google Bard, have transformed the way we interact with cyber technologies. In this paper, we study the possibility of connecting LLM with wireless sensor networks (WSN). A successful design will not only extend LLM's knowledge landscape to the physical world but also revolutionize human interaction with WSN. To the end, we present ChatTracer, an LLM-powered real-time Bluetooth device tracking system. ChatTracer comprises three key components: an array of Bluetooth sniffing nodes, a database, and a fine-tuned LLM. ChatTracer was designed based on our experimental observation that commercial Apple/Android devices always broadcast hundreds of BLE packets per minute even in their idle status. Its novelties lie in two aspects: i) a reliable and efficient BLE packet grouping algorithm; and ii) an LLM fine-tuning strategy that combines both supervised fine-tuning (SFT) and reinfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Vampire&#23450;&#29702;&#35777;&#26126;&#22120;&#20013;&#20351;&#29992;&#34584;&#34523;&#24335;&#31995;&#32479;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#26500;&#24314;&#24378;&#22823;&#26085;&#31243;&#34920;&#30340;&#38590;&#26131;&#31243;&#24230;&#20197;&#21450;&#26085;&#31243;&#27867;&#21270;&#21040;&#26410;&#30693;&#38382;&#39064;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.12869</link><description>&lt;p&gt;
&#34584;&#34523;&#24335;&#31574;&#30053;&#21457;&#29616;&#21644;&#26085;&#31243;&#26500;&#24314;&#20013;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Regularization in Spider-Style Strategy Discovery and Schedule Construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Vampire&#23450;&#29702;&#35777;&#26126;&#22120;&#20013;&#20351;&#29992;&#34584;&#34523;&#24335;&#31995;&#32479;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#26500;&#24314;&#24378;&#22823;&#26085;&#31243;&#34920;&#30340;&#38590;&#26131;&#31243;&#24230;&#20197;&#21450;&#26085;&#31243;&#27867;&#21270;&#21040;&#26410;&#30693;&#38382;&#39064;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#22810;&#26679;&#30340;&#35777;&#26126;&#31574;&#30053;&#26085;&#31243;&#34920;&#65292;&#22312;&#32473;&#23450;&#38382;&#39064;&#19978;&#23581;&#35797;&#65288;&#39034;&#24207;&#25110;&#24182;&#34892;&#65289;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#19968;&#39033;&#38024;&#23545;Vampire&#23450;&#29702;&#35777;&#26126;&#22120;&#30340;&#31574;&#30053;&#21457;&#29616;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#38024;&#23545;TPTP&#24211;&#30340;FOF&#29255;&#27573;&#24182;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;Andrei Voronkov&#30340;Spider&#31995;&#32479;&#24605;&#24819;&#30340;&#26085;&#31243;&#34920;&#12290;&#25105;&#20204;&#20174;&#21508;&#20010;&#35282;&#24230;&#23457;&#35270;&#35813;&#36807;&#31243;&#65292;&#35752;&#35770;&#20102;&#20026;CASC&#31454;&#36187;&#33719;&#24471;&#24378;&#22823;&#30340;Vampire&#26085;&#31243;&#34920;&#30340;&#38590;&#26131;&#31243;&#24230;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26085;&#31243;&#34920;&#33021;&#22815;&#23545;&#26410;&#30693;&#38382;&#39064;&#27867;&#21270;&#21040;&#20309;&#31181;&#31243;&#24230;&#20197;&#21450;&#24433;&#21709;&#27492;&#23646;&#24615;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12869v1 Announce Type: new  Abstract: To achieve the best performance, automatic theorem provers often rely on schedules of diverse proving strategies to be tried out (either sequentially or in parallel) on a given problem. In this paper, we report on a large-scale experiment with discovering strategies for the Vampire prover, targeting the FOF fragment of the TPTP library and constructing a schedule for it, based on the ideas of Andrei Voronkov's system Spider. We examine the process from various angles, discuss the difficulty (or ease) of obtaining a strong Vampire schedule for the CASC competition, and establish how well a schedule can be expected to generalize to unseen problems and what factors influence this property.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#21644;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#38899;&#39640;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11780</link><description>&lt;p&gt;
Prompt-Singer: &#24102;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#21487;&#25511;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11780
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#21644;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#38899;&#39640;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;(SVS)&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#38899;&#39057;&#36136;&#37327;&#21644;&#33258;&#28982;&#24230;&#65292;&#28982;&#32780;&#23427;&#20204;&#32570;&#20047;&#26174;&#24335;&#25511;&#21046;&#21512;&#25104;&#21809;&#27468;&#39118;&#26684;&#23646;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;SVS&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20165;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#65292;&#20855;&#26377;&#22810;&#23610;&#24230;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#31163;&#38899;&#39640;&#34920;&#31034;&#30340;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#22495;&#25511;&#21046;&#21516;&#26102;&#20445;&#25345;&#20102;&#26059;&#24459;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#24494;&#35843;&#65292;&#20197;&#21450;&#24341;&#20837;&#35821;&#38899;&#25968;&#25454;&#20197;&#20943;&#36731;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#26088;&#22312;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#25511;&#21046;&#33021;&#21147;&#21644;&#38899;&#39057;&#36136;&#37327;&#12290;&#38899;&#39057;&#31034;&#20363;&#21487;&#35775;&#38382; http://prompt-singer.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11780v1 Announce Type: cross  Abstract: Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;</title><link>https://arxiv.org/abs/2403.08551</link><description>&lt;p&gt;
&#39640;&#26031;&#22270;&#20687;&#65306;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;1000&#24103;&#27599;&#31186;&#30340;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08551
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#25552;&#20379;&#20102;&#39640;&#35270;&#35273;&#36136;&#37327;&#21644;&#24555;&#36895;&#28210;&#26579;&#36895;&#24230;&#65292;&#27599;&#31186;10-1000&#24103;&#65292;&#20551;&#35774;&#26377;&#36275;&#22815;&#30340;GPU&#36164;&#28304;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35201;&#27714;&#24120;&#24120;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20869;&#23384;&#26377;&#38480;&#30340;&#20302;&#31471;&#35774;&#22791;&#19978;&#30340;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#30340;&#24320;&#21019;&#24615;&#33539;&#24335;&#65292;&#21517;&#20026;GaussianImage&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;2D&#39640;&#26031;&#26469;&#34920;&#31034;&#22270;&#20687;&#65292;&#20854;&#20013;&#27599;&#20010;&#39640;&#26031;&#20855;&#26377;8&#20010;&#21442;&#25968;&#65292;&#21253;&#25324;&#20301;&#32622;&#12289;&#21327;&#26041;&#24046;&#21644;&#39068;&#33394;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#27714;&#21644;&#30340;&#26032;&#39062;&#28210;&#26579;&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;GPU&#20869;&#23384;&#33267;&#23569;&#38477;&#20302;3&#20493;&#65292;&#25311;&#21512;&#26102;&#38388;&#24555;5&#20493;&#65292;&#19981;&#20165;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#65288;&#20363;&#22914;WIRE&#65292;I-NGP&#65289;&#19981;&#30456;&#19978;&#19979;&#65292;&#32780;&#19988;&#26080;&#35770;&#21442;&#25968;&#22823;&#23567;&#22914;&#20309;&#37117;&#33021;&#25552;&#20379;1500-2000&#24103;&#27599;&#31186;&#30340;&#26356;&#24555;&#28210;&#26579;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08551v1 Announce Type: cross  Abstract: Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.07605</link><description>&lt;p&gt;
&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#20197;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#32654;&#23398;&#21644;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#25551;&#36848;&#19981;&#33391;&#22270;&#20687;&#29305;&#24449;&#30340;&#36127;&#38754;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#33391;&#22909;&#30340;&#36127;&#38754;&#25552;&#31034;&#26159;&#19968;&#39033;&#25163;&#24037;&#32780;&#32321;&#29712;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NegOpt&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#29983;&#25104;&#65292;&#20174;&#32780;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22823;&#24133;&#25552;&#39640;&#20102;25%&#30340;Inception Score&#65292;&#24182;&#36229;&#36234;&#20102;&#26469;&#33258;&#27979;&#35797;&#38598;&#30340;&#26631;&#20934;&#36127;&#38754;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;NegOpt&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20248;&#21270;&#23545;&#25105;&#20204;&#26368;&#37325;&#35201;&#30340;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;Negative Prompts DB&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07605v1 Announce Type: cross  Abstract: In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07362</link><description>&lt;p&gt;
&#25361;&#25112;&#36951;&#24536;&#65306;&#25581;&#31034;&#26426;&#22120;&#36951;&#24536;&#20013;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;
&lt;/p&gt;
&lt;p&gt;
Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38752;&#35889;&#30340;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#31038;&#21306;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21518;&#26377;&#36873;&#25321;&#24615;&#22320;&#8220;&#36951;&#24536;&#8221;&#25968;&#25454;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#24341;&#20986;&#20102;&#26426;&#22120;&#36951;&#24536;(Machine Unlearning, MU)&#38382;&#39064;&#65292;&#26088;&#22312;&#28040;&#38500;&#36873;&#23450;&#25968;&#25454;&#28857;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#27169;&#22411;&#22312;&#36951;&#24536;&#21518;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;MU&#26041;&#27861;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#65292;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#38543;&#26426;&#25968;&#25454;&#36951;&#24536;&#19978;&#65292;&#24573;&#35270;&#20102;&#23545;&#20110;&#30495;&#23454;&#34913;&#37327;&#36951;&#24536;&#24615;&#33021;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30340;&#37325;&#35201;&#25506;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MU&#35780;&#20272;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20986;&#30830;&#23450;&#37027;&#20123;&#23545;&#24433;&#21709;&#25830;&#38500;&#26500;&#25104;&#26368;&#22823;&#25361;&#25112;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#25214;&#20986;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#12290;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#21407;&#21017;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#22312;&#19978;&#23618;&#20248;&#21270;&#20013;&#30340;&#36951;&#24536;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20114;&#21160;&#20013;&#26159;&#21542;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#20204;&#30340;&#34892;&#20026;&#21028;&#26029;&#21644;&#27807;&#36890;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#22312;&#29983;&#25104;&#31038;&#20250;&#21487;&#25509;&#21463;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.05701</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#19982;&#20154;&#20204;&#30340;&#31038;&#20132;&#30452;&#35273;&#30456;&#19968;&#33268;&#65292;&#29992;&#20110;&#20154;&#26426;&#20114;&#21160;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05701
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20114;&#21160;&#20013;&#26159;&#21542;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#20204;&#30340;&#34892;&#20026;&#21028;&#26029;&#21644;&#27807;&#36890;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#22312;&#29983;&#25104;&#31038;&#20250;&#21487;&#25509;&#21463;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#39640;&#23618;&#27425;&#30340;&#34892;&#21160;&#35268;&#21010;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35768;&#22810;&#26426;&#22120;&#20154;&#24212;&#29992;&#28041;&#21450;&#20154;&#31867;&#30417;&#30563;&#21592;&#25110;&#21512;&#20316;&#32773;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#29983;&#25104;&#19982;&#20154;&#20204;&#20559;&#22909;&#21644;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#30340;&#31038;&#20250;&#21487;&#25509;&#21463;&#34892;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;LLMs&#26159;&#21542;&#25429;&#25417;&#21040;&#20154;&#20204;&#22312;&#20154;&#26426;&#20114;&#21160;&#65288;HRI&#65289;&#22330;&#26223;&#20013;&#34892;&#20026;&#21028;&#26029;&#21644;&#27807;&#36890;&#20559;&#22909;&#26041;&#38754;&#30340;&#30452;&#35273;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#37325;&#29616;&#20102;&#19977;&#20010;HRI&#29992;&#25143;&#30740;&#31350;&#65292;&#23558;LLMs&#30340;&#36755;&#20986;&#19982;&#30495;&#23454;&#21442;&#19982;&#32773;&#30340;&#36755;&#20986;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#22312;&#38750;&#24120;&#20986;&#33394;&#22320;&#34920;&#29616;&#65292;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#20004;&#39033;&#30740;&#31350;&#30340;&#29992;&#25143;&#31572;&#26696;&#20855;&#26377;&#24456;&#24378;&#30456;&#20851;&#24615;&#8212;&#8212;&#31532;&#19968;&#39033;&#30740;&#31350;&#28041;&#21450;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#27807;&#36890;&#20030;&#21160;&#32473;&#26426;&#22120;&#20154;&#65288;$r_s$ = 0.82&#65289;&#65292;&#31532;&#20108;&#39033;&#28041;&#21450;&#21028;&#26029;&#34892;&#20026;&#30340;&#21487;&#21462;&#24615;&#12289;&#24847;&#22270;&#24615;&#21644;&#20196;&#20154;&#24778;&#35766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05701v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly used in robotics, especially for high-level action planning. Meanwhile, many robotics applications involve human supervisors or collaborators. Hence, it is crucial for LLMs to generate socially acceptable actions that align with people's preferences and values. In this work, we test whether LLMs capture people's intuitions about behavior judgments and communication preferences in human-robot interaction (HRI) scenarios. For evaluation, we reproduce three HRI user studies, comparing the output of LLMs with that of real participants. We find that GPT-4 strongly outperforms other models, generating answers that correlate strongly with users' answers in two studies $\unicode{x2014}$ the first study dealing with selecting the most appropriate communicative act for a robot in various situations ($r_s$ = 0.82), and the second with judging the desirability, intentionality, and surprisingness of beh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#65288;AdvPO&#65289;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#22870;&#21169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22260;&#32469;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#30340;&#20248;&#21270;&#65292;&#20174;&#32780;&#26377;&#25928;&#32531;&#35299;&#20102;&#35813;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05171</link><description>&lt;p&gt;
&#36890;&#36807;&#36731;&#37327;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#20811;&#26381;&#20102;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#65288;AdvPO&#65289;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#22870;&#21169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22260;&#32469;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#30340;&#20248;&#21270;&#65292;&#20174;&#32780;&#26377;&#25928;&#32531;&#35299;&#20102;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#65288;AdvPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;AdvPO&#22260;&#32469;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;&#35299;&#20915;&#20102;&#19968;&#20010;&#20998;&#24067;&#40065;&#26834;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#25913;&#36827;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;Anthropic HH&#21644;TL;DR&#25688;&#35201;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AdvPO&#22312;&#20943;&#36731;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05171v1 Announce Type: cross  Abstract: We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization occurs when a reward model serves as an imperfect proxy for human preference, and RL-driven policy optimization erroneously exploits reward inaccuracies. In this paper, we begin by introducing a lightweight way to quantify uncertainties in rewards, relying solely on the last layer embeddings of the reward model, without the need for computationally expensive reward ensembles. AdvPO then addresses a distributionally robust optimization problem centred around the confidence interval of the reward model's predictions for policy improvement. Through comprehensive experiments on the Anthropic HH and TL;DR summarization datasets, we illustrate the efficacy of AdvPO in mitigating the overoptimization issue, consequently
&lt;/p&gt;</description></item><item><title>AUFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#30693;&#35782;&#28151;&#21512;&#19987;&#23478;&#21327;&#20316;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#31232;&#32570;&#25968;&#25454;&#38598;&#25110;&#36807;&#24230;&#20381;&#36182;&#39069;&#22806;&#25968;&#25454;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.04697</link><description>&lt;p&gt;
AUFormer: &#35270;&#35273;Transformer&#26159;&#21442;&#25968;&#39640;&#25928;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04697
&lt;/p&gt;
&lt;p&gt;
AUFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#30693;&#35782;&#28151;&#21512;&#19987;&#23478;&#21327;&#20316;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#31232;&#32570;&#25968;&#25454;&#38598;&#25110;&#36807;&#24230;&#20381;&#36182;&#39069;&#22806;&#25968;&#25454;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#22312;&#24773;&#24863;&#35745;&#31639;&#39046;&#22495;&#26159;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;AU&#26816;&#27979;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#22312;&#31232;&#32570;&#30340;AU&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#21033;&#29992;&#22823;&#37327;&#21487;&#23398;&#20064;&#21442;&#25968;&#25110;&#36807;&#24230;&#20381;&#36182;&#22823;&#37327;&#39069;&#22806;&#30456;&#20851;&#25968;&#25454;&#32780;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#21442;&#25968;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#65288;PETL&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#33539;&#24335;&#65292;&#28982;&#32780;&#20854;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#38024;&#23545;AU&#29305;&#24449;&#30340;&#35774;&#35745;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#23558;PETL&#33539;&#24335;&#24212;&#29992;&#20110;AU&#26816;&#27979;&#65292;&#24341;&#20837;AUFormer&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#28151;&#21512;&#19987;&#23478;&#65288;MoKE&#65289;&#21327;&#20316;&#26426;&#21046;&#12290;&#19968;&#20010;&#29305;&#23450;&#20110;&#26576;&#20010;AU&#24182;&#20855;&#26377;&#26368;&#23569;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;MoKE&#39318;&#20808;&#38598;&#25104;&#20010;&#24615;&#21270;&#30340;&#22810;&#23610;&#24230;&#21644;&#30456;&#20851;&#30693;&#35782;&#12290;&#28982;&#21518;MoKE&#19982;&#19987;&#23478;&#32452;&#20013;&#30340;&#20854;&#20182;MoKE&#21512;&#20316;&#65292;&#33719;&#21462;&#32858;&#21512;&#20449;&#24687;&#24182;&#23558;&#20854;&#27880;&#20837;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04697v1 Announce Type: cross  Abstract: Facial Action Units (AU) is a vital concept in the realm of affective computing, and AU detection has always been a hot research topic. Existing methods suffer from overfitting issues due to the utilization of a large number of learnable parameters on scarce AU-annotated datasets or heavy reliance on substantial additional relevant data. Parameter-Efficient Transfer Learning (PETL) provides a promising paradigm to address these challenges, whereas its existing methods lack design for AU characteristics. Therefore, we innovatively investigate PETL paradigm to AU detection, introducing AUFormer and proposing a novel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual MoKE specific to a certain AU with minimal learnable parameters first integrates personalized multi-scale and correlation knowledge. Then the MoKE collaborates with other MoKEs in the expert group to obtain aggregated information and inject it into the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31454;&#20105;&#24615;&#35774;&#26045;&#36873;&#22336;&#30340;&#20004;&#38454;&#27573;&#22810;&#20195;&#29702;&#31995;&#32479;&#65292;&#38024;&#23545;&#19981;&#21487;&#20998;&#21106;&#26435;&#37325;&#30340;&#23458;&#25143;&#25552;&#20986;&#20102;&#28151;&#21512;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#35774;&#26045;&#24067;&#32622;&#26102;&#21487;&#33021;&#20986;&#29616;&#25130;&#28982;&#19981;&#21516;&#30340;&#23458;&#25143;&#22343;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.03114</link><description>&lt;p&gt;
&#20004;&#38454;&#27573;&#35774;&#26045;&#36873;&#22336;&#20013;&#30340;&#22343;&#34913;&#19982;&#21407;&#23376;&#23458;&#25143;
&lt;/p&gt;
&lt;p&gt;
Equilibria in Two-Stage Facility Location with Atomic Clients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31454;&#20105;&#24615;&#35774;&#26045;&#36873;&#22336;&#30340;&#20004;&#38454;&#27573;&#22810;&#20195;&#29702;&#31995;&#32479;&#65292;&#38024;&#23545;&#19981;&#21487;&#20998;&#21106;&#26435;&#37325;&#30340;&#23458;&#25143;&#25552;&#20986;&#20102;&#28151;&#21512;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#35774;&#26045;&#24067;&#32622;&#26102;&#21487;&#33021;&#20986;&#29616;&#25130;&#28982;&#19981;&#21516;&#30340;&#23458;&#25143;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31454;&#20105;&#24615;&#35774;&#26045;&#36873;&#22336;&#35270;&#20026;&#19968;&#20010;&#24102;&#26377;&#20004;&#31181;&#23458;&#25143;&#31867;&#22411;&#30340;&#20004;&#38454;&#27573;&#22810;&#20195;&#29702;&#31995;&#32479;&#12290;&#23545;&#20110;&#20855;&#26377;&#21152;&#26435;&#23458;&#25143;&#30340;&#20027;&#26426;&#22270;&#65292;&#39318;&#20808;&#35774;&#26045;&#20195;&#29702;&#32773;&#25112;&#30053;&#24615;&#22320;&#36873;&#25321;&#24320;&#35774;&#35774;&#26045;&#30340;&#39030;&#28857;&#12290;&#28982;&#21518;&#65292;&#23458;&#25143;&#25112;&#30053;&#24615;&#22320;&#36873;&#25321;&#22312;&#20854;&#37051;&#22495;&#20869;&#30340;&#21738;&#20010;&#24320;&#35774;&#35774;&#26045;&#28040;&#36153;&#12290;&#35774;&#26045;&#24076;&#26395;&#23613;&#21487;&#33021;&#21560;&#24341;&#26356;&#22810;&#23458;&#25143;&#26435;&#37325;&#65292;&#23458;&#25143;&#24076;&#26395;&#26368;&#23567;&#21270;&#25152;&#36873;&#35774;&#26045;&#19978;&#30340;&#25317;&#25380;&#12290;&#25152;&#26377;&#26368;&#36817;&#30740;&#31350;&#30340;&#27492;&#27169;&#22411;&#29256;&#26412;&#37117;&#20551;&#23450;&#23458;&#25143;&#21487;&#20197;&#25112;&#30053;&#24615;&#22320;&#20998;&#25285;&#20182;&#20204;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#19981;&#21487;&#20998;&#21106;&#26435;&#37325;&#30340;&#23458;&#25143;&#65292;&#20294;&#20801;&#35768;&#28151;&#21512;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#23458;&#25143;&#21487;&#20197;&#22312;&#21738;&#20010;&#35774;&#26045;&#28040;&#36153;&#19978;&#38543;&#26426;&#36873;&#25321;&#12290;&#38500;&#20102;&#23545;&#33258;&#28982;&#23458;&#25143;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#22806;&#65292;&#36825;&#31181;&#24494;&#22937;&#30340;&#21464;&#21270;&#20250;&#20135;&#29983; drast &#21095;&#28872;&#30340;&#21464;&#21270;&#65292;&#20363;&#22914;&#65292;&#22312;&#32473;&#23450;&#35774;&#26045;&#24067;&#32622;&#26102;&#65292;&#21487;&#33021;&#20986;&#29616;&#25130;&#28982;&#19981;&#21516;&#30340;&#23458;&#25143;&#22343;&#34913;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#23376;&#21338;&#24328;&#23436;&#32654;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03114v1 Announce Type: cross  Abstract: We consider competitive facility location as a two-stage multi-agent system with two types of clients. For a given host graph with weighted clients on the vertices, first facility agents strategically select vertices for opening their facilities. Then, the clients strategically select which of the opened facilities in their neighborhood to patronize. Facilities want to attract as much client weight as possible, clients want to minimize congestion on the chosen facility.   All recently studied versions of this model assume that clients can split their weight strategically. We consider clients with unsplittable weights, but allow mixed strategies. So clients may randomize over which facility to patronize. Besides modeling a natural client behavior, this subtle change yields drastic changes, e.g., for a given facility placement, qualitatively different client equilibria are possible.   As our main result, we show that pure subgame perfect
&lt;/p&gt;</description></item><item><title>&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.02181</link><description>&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#26159;&#25152;&#26377;LLMs&#30340;&#23618;&#37117;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Not all Layers of LLMs are Necessary during Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02181
&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#38454;&#27573;&#38750;&#24120;&#26114;&#36149;&#12290;&#29702;&#24819;&#30340;LLMs&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#20854;&#33021;&#21147;&#65288;&#20363;&#22914;&#27867;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65289;&#12290;&#26412;&#25991;&#23581;&#35797;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#22312;LLMs&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#31616;&#21333;&#23454;&#20363;&#20351;&#29992;&#27973;&#23618;&#65292;&#24182;&#20026;&#38590;&#20197;&#22788;&#29702;&#30340;&#23454;&#20363;&#20351;&#29992;&#28145;&#23618;&#21527;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#36328;&#20219;&#21153;&#28608;&#27963;&#30340;&#23618;&#26469;&#25351;&#20986;&#24182;&#38750;&#25152;&#26377;&#23618;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#37117;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;AdaInfer&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#25512;&#29702;&#32456;&#27490;&#26102;&#21051;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;AdaInfer&#19981;&#25913;&#21464;LLMs&#21442;&#25968;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#30693;&#21517;LLMs&#65288;&#21363;Llama2&#31995;&#21015;&#21644;OPT&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;AdaInfer&#33410;&#30465;&#20102;&#24179;&#22343;14.8%&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#29978;&#33267;&#22312;&#24773;&#24863;&#26041;&#38754;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02181v1 Announce Type: cross  Abstract: The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, "During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment 
&lt;/p&gt;</description></item><item><title>&#35821;&#20041;&#22522;&#30784;&#32622;&#20110;&#36125;&#21494;&#26031;&#24515;&#28789;&#29702;&#35770;&#20013;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#20204;&#20849;&#21516;&#25512;&#26029;&#20986;&#35299;&#37322;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#19968;&#33268;&#24615;&#30446;&#26631;&#12289;&#20449;&#24565;&#21644;&#35745;&#21010;&#38598;&#21512;&#65292;&#20877;&#36890;&#36807;&#35748;&#35782;&#36923;&#36753;&#35780;&#20272;&#26377;&#20851;&#20195;&#29702;&#20154;&#20449;&#24565;&#30340;&#38472;&#36848;&#65292;&#35299;&#37322;&#20102;&#20154;&#31867;&#20449;&#24565;&#24402;&#22240;&#30340;&#20998;&#32423;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#20197;&#21450;&#20854;&#19982;&#30446;&#26631;&#21644;&#35745;&#21010;&#30340;&#23494;&#20999;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.10416</link><description>&lt;p&gt;
&#23558;&#20851;&#20110;&#20449;&#24565;&#30340;&#35821;&#35328;&#25509;&#22320;&#20110;&#36125;&#21494;&#26031;&#24515;&#28789;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Grounding Language about Belief in a Bayesian Theory-of-Mind
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10416
&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22522;&#30784;&#32622;&#20110;&#36125;&#21494;&#26031;&#24515;&#28789;&#29702;&#35770;&#20013;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#20204;&#20849;&#21516;&#25512;&#26029;&#20986;&#35299;&#37322;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#19968;&#33268;&#24615;&#30446;&#26631;&#12289;&#20449;&#24565;&#21644;&#35745;&#21010;&#38598;&#21512;&#65292;&#20877;&#36890;&#36807;&#35748;&#35782;&#36923;&#36753;&#35780;&#20272;&#26377;&#20851;&#20195;&#29702;&#20154;&#20449;&#24565;&#30340;&#38472;&#36848;&#65292;&#35299;&#37322;&#20102;&#20154;&#31867;&#20449;&#24565;&#24402;&#22240;&#30340;&#20998;&#32423;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#20197;&#21450;&#20854;&#19982;&#30446;&#26631;&#21644;&#35745;&#21010;&#30340;&#23494;&#20999;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20449;&#24565;&#26159;&#26080;&#27861;&#30452;&#25509;&#35266;&#23519;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#20154;&#31867;&#24120;&#24120;&#20351;&#29992;&#20016;&#23500;&#30340;&#32452;&#21512;&#35821;&#35328;&#26469;&#25551;&#36848;&#20182;&#20154;&#30340;&#24819;&#27861;&#21644;&#30693;&#35782;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23558;&#20449;&#24565;&#38472;&#36848;&#30340;&#35821;&#20041;&#22522;&#30784;&#32622;&#20110;&#36125;&#21494;&#26031;&#24515;&#28789;&#29702;&#35770;&#20013;&#65292;&#20026;&#35299;&#37322;&#20154;&#31867;&#22914;&#20309;&#35299;&#37322;&#20182;&#20154;&#38544;&#34255;&#30340;&#35748;&#35782;&#20869;&#23481;&#36808;&#20986;&#20102;&#19968;&#27493;&#65306;&#36890;&#36807;&#24314;&#27169;&#20154;&#31867;&#22914;&#20309;&#20849;&#21516;&#25512;&#26029;&#20986;&#35299;&#37322;&#19968;&#20010;&#20195;&#29702;&#20154;&#34892;&#21160;&#30340;&#19968;&#33268;&#24615;&#30446;&#26631;&#12289;&#20449;&#24565;&#21644;&#35745;&#21010;&#38598;&#21512;&#65292;&#28982;&#21518;&#36890;&#36807;&#35748;&#35782;&#36923;&#36753;&#23545;&#26377;&#20851;&#20195;&#29702;&#20154;&#20449;&#24565;&#30340;&#38472;&#36848;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#20449;&#24565;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#35282;&#33394;&#35821;&#20041;&#65292;&#35299;&#37322;&#20102;&#20154;&#31867;&#20449;&#24565;&#24402;&#22240;&#30340;&#20998;&#32423;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#30446;&#26631;&#21644;&#35745;&#21010;&#30340;&#23494;&#20999;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20154;&#20204;&#22312;&#35266;&#23519;&#19968;&#20010;&#20195;&#29702;&#20154;&#35299;&#20915;&#38382;&#39064;&#26102;&#26159;&#22914;&#20309;&#24402;&#22240;&#30446;&#26631;&#21644;&#20449;&#24565;&#30340;&#26469;&#35780;&#20272;&#36825;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10416v1 Announce Type: new  Abstract: Despite the fact that beliefs are mental states that cannot be directly observed, humans talk about each others' beliefs on a regular basis, often using rich compositional language to describe what others think and know. What explains this capacity to interpret the hidden epistemic content of other minds? In this paper, we take a step towards an answer by grounding the semantics of belief statements in a Bayesian theory-of-mind: By modeling how humans jointly infer coherent sets of goals, beliefs, and plans that explain an agent's actions, then evaluating statements about the agent's beliefs against these inferences via epistemic logic, our framework provides a conceptual role semantics for belief, explaining the gradedness and compositionality of human belief attributions, as well as their intimate connection with goals and plans. We evaluate this framework by studying how humans attribute goals and beliefs while watching an agent solve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.05966</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#21644;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethink Model Re-Basin and the Linear Mode Connectivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#22823;&#37096;&#20998;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#35299;&#21487;&#20197;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#22522;&#24213;&#65292;&#21482;&#26159;&#39034;&#24207;&#21487;&#33021;&#19981;&#21516;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#38454;&#27573;&#65292;&#23545;&#20110;&#27169;&#22411;&#24179;&#22343;&#21270;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#37325;&#26032;&#22522;&#24213;&#31574;&#30053;&#22312;&#25928;&#26524;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23545;&#24213;&#23618;&#26426;&#21046;&#30340;&#29702;&#35299;&#19981;&#22815;&#20840;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26631;&#20934;&#20570;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#39057;&#32321;&#19981;&#36275;&#20043;&#22788;&#65292;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26356;&#30452;&#25509;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21305;&#37197;&#31639;&#27861;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#35266;&#28857;&#19981;&#20165;&#28548;&#28165;&#21644;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#36824;&#20419;&#36827;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;&#20363;&#22914;&#65292;&#23427;&#23558;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;&#19982;&#21098;&#26525;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#28608;&#21457;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#65292;&#21487;&#20197;&#30452;&#25509;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#21098;&#26525;&#25216;&#26415;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies suggest that with sufficiently wide models, most SGD solutions can, up to permutation, converge into the same basin. This phenomenon, known as the model re-basin regime, has significant implications for model averaging. However, current re-basin strategies are limited in effectiveness due to a lack of comprehensive understanding of underlying mechanisms. Addressing this gap, our work revisits standard practices and uncovers the frequent inadequacies of existing matching algorithms, which we show can be mitigated through proper re-normalization. By introducing a more direct analytical approach, we expose the interaction between matching algorithms and re-normalization processes. This perspective not only clarifies and refines previous findings but also facilitates novel insights. For instance, it connects the linear mode connectivity to pruning, motivating a lightweight yet effective post-pruning plug-in that can be directly merged with any existing pruning techniques. Ou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.02352</link><description>&lt;p&gt;
&#36870;&#21521;&#23398;&#20064;&#65306;&#36890;&#36807;&#25441;&#21462;&#23398;&#20064;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Working Backwards: Learning to Place by Picking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02352
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#65288;PvP&#65289;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#20027;&#25910;&#38598;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#25918;&#32622;&#20219;&#21153;&#30340;&#29616;&#23454;&#19990;&#30028;&#28436;&#31034;&#65292;&#20854;&#20013;&#29289;&#20307;&#24517;&#39035;&#34987;&#25805;&#32437;&#21040;&#29305;&#23450;&#30340;&#25509;&#35302;&#38480;&#21046;&#20301;&#32622;&#12290;&#36890;&#36807;PvP&#65292;&#25105;&#20204;&#36890;&#36807;&#39072;&#20498;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#65292;&#25509;&#36817;&#20110;&#26426;&#22120;&#20154;&#29289;&#20307;&#25918;&#32622;&#28436;&#31034;&#30340;&#25910;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#19968;&#32452;&#26368;&#21021;&#20301;&#20110;&#30446;&#26631;&#25918;&#32622;&#20301;&#32622;&#30340;&#29289;&#20307;&#30340;&#25235;&#21462;&#24207;&#21015;&#20013;&#33719;&#24471;&#25918;&#32622;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#30334;&#20010;&#28436;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#65292;&#36825;&#26159;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#27169;&#22359;&#23454;&#29616;&#30340;&#65306;&#35302;&#35273;&#37325;&#26032;&#25235;&#21462;&#21644;&#29992;&#20110;&#25235;&#21462;&#30340;&#39034;&#20174;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#30452;&#25509;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36890;&#36807;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#20013;&#35757;&#32451;&#31574;&#30053;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#31574;&#30053;&#21487;&#20197;&#25512;&#24191;&#21040;&#36229;&#20986;&#35757;&#32451;&#29615;&#22659;&#33539;&#22260;&#30340;&#29289;&#20307;&#25918;&#32622;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02352v2 Announce Type: replace-cross  Abstract: We present placing via picking (PvP), a method to autonomously collect real-world demonstrations for a family of placing tasks in which objects must be manipulated to specific contact-constrained locations. With PvP, we approach the collection of robotic object placement demonstrations by reversing the grasping process and exploiting the inherent symmetry of the pick and place problems. Specifically, we obtain placing demonstrations from a set of grasp sequences of objects initially located at their target placement locations. Our system can collect hundreds of demonstrations in contact-constrained environments without human intervention by combining two modules: tactile regrasping and compliant control for grasps. We train a policy directly from visual observations through behavioral cloning, using the autonomously-collected demonstrations. By doing so, the policy can generalize to object placement scenarios outside of the tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.04929</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#25552;&#21319;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks. (arXiv:2401.04929v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#21069;&#26159;&#21508;&#31181;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#24341;&#21457;&#20102;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#25285;&#24551;&#12290;&#19968;&#31181;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#26159;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#65292;&#23427;&#20801;&#35768;&#23545;&#25163;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#34429;&#28982;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;MIA&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#33021;&#22815;&#22312;&#20302;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;&#21306;&#22495;&#65288;0.01%~1%&#65289;&#23454;&#29616;&#36739;&#39640;&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;MIA&#24517;&#39035;&#32771;&#34385;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MIA&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#30340;TPR&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#65288;LDC-MIA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#23558;&#25968;&#25454;&#35760;&#24405;&#20197;&#20854;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance. However, using sensitive data to train these models raises concerns about privacy and security. One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset. While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA to be practically useful in real-world settings. In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs. Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#19987;&#23478;&#20013;&#24515;&#26412;&#20307;&#35774;&#35745;&#38598;&#25104;&#21040;CRISP-DM&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20844;&#21496;&#22312;&#24212;&#29992;&#25968;&#25454;&#39537;&#21160;&#39033;&#30446;&#26102;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.11637</link><description>&lt;p&gt;
&#23558;&#39046;&#22495;&#19987;&#23478;&#20013;&#24515;&#26412;&#20307;&#35774;&#35745;&#38598;&#25104;&#21040;&#38754;&#21521;&#29289;&#29702;&#29983;&#20135;&#31995;&#32479;&#30340;CRISP-DM&#20013;
&lt;/p&gt;
&lt;p&gt;
Integration of Domain Expert-Centric Ontology Design into the CRISP-DM for Cyber-Physical Production Systems. (arXiv:2307.11637v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#19987;&#23478;&#20013;&#24515;&#26412;&#20307;&#35774;&#35745;&#38598;&#25104;&#21040;CRISP-DM&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20844;&#21496;&#22312;&#24212;&#29992;&#25968;&#25454;&#39537;&#21160;&#39033;&#30446;&#26102;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;4.0&#21644;&#29289;&#29702;&#29983;&#20135;&#31995;&#32479;&#39046;&#22495;&#65292;&#22823;&#37327;&#26377;&#28508;&#22312;&#20215;&#20540;&#30340;&#25968;&#25454;&#34987;&#29983;&#25104;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#22797;&#26434;&#21644;&#38544;&#21547;&#30340;&#27169;&#24335;&#26041;&#38754;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#25152;&#24471;&#21040;&#30340;&#30693;&#35782;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#35832;&#22914;&#35786;&#26029;&#25110;&#32500;&#25252;&#35745;&#21010;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#39537;&#21160;&#39033;&#30446;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#29702;&#35299;&#21644;&#20934;&#22791;&#25968;&#25454;&#65292;&#22240;&#27492;&#24120;&#24120;&#23548;&#33268;&#22833;&#36133;&#12290;&#22312;&#22788;&#29702;&#24037;&#19994;4.0&#29615;&#22659;&#20013;&#30340;&#25361;&#25112;&#26102;&#65292;&#39046;&#22495;&#29305;&#23450;&#26412;&#20307;&#30340;&#24212;&#29992;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#29289;&#29702;&#29983;&#20135;&#31995;&#32479;&#30340;&#26412;&#20307;&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#21644;&#24037;&#20214;&#23578;&#26410;&#31995;&#32479;&#22320;&#38598;&#25104;&#21040;CRISP-DM&#20013;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#20805;&#20998;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#20013;&#24515;&#26412;&#20307;&#35774;&#35745;&#22312;CRISP-DM&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the age of Industry 4.0 and Cyber-Physical Production Systems (CPPSs) vast amounts of potentially valuable data are being generated. Methods from Machine Learning (ML) and Data Mining (DM) have proven to be promising in extracting complex and hidden patterns from the data collected. The knowledge obtained can in turn be used to improve tasks like diagnostics or maintenance planning. However, such data-driven projects, usually performed with the Cross-Industry Standard Process for Data Mining (CRISP-DM), often fail due to the disproportionate amount of time needed for understanding and preparing the data. The application of domain-specific ontologies has demonstrated its advantageousness in a wide variety of Industry 4.0 application scenarios regarding the aforementioned challenges. However, workflows and artifacts from ontology design for CPPSs have not yet been systematically integrated into the CRISP-DM. Accordingly, this contribution intends to present an integrated approach so t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.18256</link><description>&lt;p&gt;
&#29992;Transformer&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#21644;&#25968;&#20540;&#30693;&#35782;&#22270;&#20013;&#30340;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#20102;&#19968;&#20010;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#19977;&#20803;&#32452;&#19982;&#38480;&#23450;&#35789;&#38598;&#21512;&#30456;&#20851;&#32852;; &#19968;&#20010;&#38480;&#23450;&#35789;&#30001;&#20851;&#31995;&#21644;&#23454;&#20307;&#32452;&#25104;&#65292;&#20026;&#19977;&#20803;&#32452;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20551;&#23450;&#23454;&#20307;&#26159;&#31163;&#25955;&#23545;&#35937;&#65292;&#20294;&#26377;&#20123;&#20449;&#24687;&#24212;&#20351;&#29992;&#25968;&#20540;&#34920;&#31034;&#65292;&#20363;&#22914;(J.R.R.&#65292;&#20986;&#29983;&#20110;&#65292;1892)&#12290;&#21516;&#26102;&#65292;&#19977;&#20803;&#32452;(J.R.R.&#65292;&#23601;&#35835;&#20110;&#65292;&#29275;&#27941;&#22823;&#23398;)&#21487;&#20197;&#19982;&#38480;&#23450;&#35789;(&#24320;&#22987;&#26102;&#38388;&#65292;1911)&#30456;&#20851;&#32852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21253;&#21547;&#19977;&#20803;&#32452;&#25110;&#38480;&#23450;&#35789;&#20013;&#25968;&#20540;&#25991;&#23383;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;Transformer&#21644;&#19968;&#20010;&#39044;&#27979;Transformer&#65292;&#26469;&#23398;&#20064;&#34920;&#31034;&#65292;&#19981;&#20165;&#22522;&#20110;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36824;&#22522;&#20110;&#25968;&#20540;&#20449;&#24687;&#12290;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#38480;&#23450;&#35789;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#39304;&#36865;&#32473;Transformer&#26469;&#33719;&#24471;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#20013;&#30340;&#20219;&#21153;&#37096;&#32626;&#21644;&#36793;&#32536;&#36164;&#28304;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09832</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36793;&#32536;&#36164;&#28304;&#20219;&#21153;&#37096;&#32626;&#21644;&#25193;&#23637;&#26041;&#27861;&#29992;&#20110;&#36710;&#36733;&#32593;&#32476;&#26381;&#21153;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
A Deep RL Approach on Task Placement and Scaling of Edge Resources for Cellular Vehicle-to-Network Service Provisioning. (arXiv:2305.09832v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#20013;&#30340;&#20219;&#21153;&#37096;&#32626;&#21644;&#36793;&#32536;&#36164;&#28304;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#36710;&#32852;&#32593;&#8221;&#27491;&#22788;&#20110;&#25105;&#20204;&#31038;&#20250;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#21069;&#27839;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#26041;&#27861;&#29992;&#20110;&#25552;&#20379;&#36710;&#36742;&#36890;&#32852;&#32593;&#65288;C-V2N&#65289;&#26381;&#21153;&#65292;&#35299;&#20915;&#26381;&#21153;&#20219;&#21153;&#37096;&#32626;&#21644;&#36793;&#32536;&#36164;&#28304;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#32852;&#21512;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#20010;&#38382;&#39064;&#30340;&#32852;&#25509;&#26041;&#24335;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#36138;&#24515;&#31639;&#27861;&#30340;&#20851;&#20110;&#20219;&#21153;&#37096;&#32626;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110; Deep Deterministic Policy Gradient (DDPG) &#30340;&#25193;&#23637;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25193;&#23637;&#20195;&#29702;&#19982;&#22810;&#20010;&#29366;&#24577;&#19979;&#26368;&#20808;&#36827;&#30340;&#25193;&#23637;&#26041;&#27861;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular-Vehicle-to-Everything (C-V2X) is currently at the forefront of the digital transformation of our society. By enabling vehicles to communicate with each other and with the traffic environment using cellular networks, we redefine transportation, improving road safety and transportation services, increasing efficiency of traffic flows, and reducing environmental impact. This paper proposes a decentralized approach for provisioning Cellular Vehicular-to-Network (C-V2N) services, addressing the coupled problems of service task placement and scaling of edge resources. We formalize the joint problem and prove its complexity. We propose an approach to tackle it, linking the two problems, employing decentralized decision-making using (i) a greedy approach for task placement and (ii) a Deep Deterministic Policy Gradient (DDPG) based approach for scaling. We benchmark the performance of our approach, focusing on the scaling agent, against several State-of-the-Art (SoA) scaling approaches
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.13988</link><description>&lt;p&gt;
&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#21033;&#29992;&#24515;&#29702;&#23398;&#26041;&#27861;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32039;&#23494;&#32467;&#21512;&#30340;&#20808;&#38155;&#12290;&#30001;&#20110;&#24555;&#36895;&#25216;&#26415;&#36827;&#27493;&#21644;&#20854;&#26497;&#39640;&#30340;&#36890;&#29992;&#24615;&#65292;&#29616;&#20170;LLM&#24050;&#32463;&#25317;&#26377;&#25968;&#30334;&#19975;&#29992;&#25143;&#65292;&#24182;&#27491;&#22788;&#20110;&#25104;&#20026;&#20027;&#35201;&#20449;&#24687;&#26816;&#32034;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#38382;&#39064;&#35299;&#20915;&#31561;&#25216;&#26415;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#23457;&#26597;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#24403;&#21069;LLM&#20013;&#20986;&#29616;&#24840;&#21152;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#23558;&#20854;&#35270;&#20026;&#21442;&#19982;&#20154;&#31867;&#24515;&#29702;&#23454;&#39564;&#30340;&#23545;&#35937;&#65292;&#20197;&#20415;&#26356;&#20026;&#20840;&#38754;&#22320;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;"&#26426;&#22120;&#24515;&#29702;&#23398;"&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#21508;&#31867;&#24515;&#29702;&#23398;&#20998;&#25903;&#22914;&#20309;&#20026;LLM&#30340;&#34892;&#20026;&#27979;&#35797;&#25552;&#20379;&#26377;&#29992;&#21442;&#32771;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25551;&#36848;&#20102;&#34892;&#20026;&#27979;&#35797;&#32467;&#26524;&#22914;&#20309;&#20026;&#26410;&#26469;&#30340;LLM&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;&#65288;BAS&#65289;&#65292;&#26088;&#22312;&#22312;&#26102;&#38388;&#38480;&#21046;&#20869;&#35757;&#32451;&#26368;&#20339;&#30340;OPF&#20195;&#29702;&#12290;BAS&#23558;&#36755;&#20837;&#20998;&#24067;&#20998;&#25104;&#26742;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#20989;&#25968;&#30830;&#23450;&#19979;&#19968;&#27425;&#37319;&#26679;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;BAS&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2208.07497</link><description>&lt;p&gt;
&#23398;&#20064;ACOPF&#30340;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Bucketized Active Sampling for Learning ACOPF. (arXiv:2208.07497v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;&#65288;BAS&#65289;&#65292;&#26088;&#22312;&#22312;&#26102;&#38388;&#38480;&#21046;&#20869;&#35757;&#32451;&#26368;&#20339;&#30340;OPF&#20195;&#29702;&#12290;BAS&#23558;&#36755;&#20837;&#20998;&#24067;&#20998;&#25104;&#26742;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#20989;&#25968;&#30830;&#23450;&#19979;&#19968;&#27425;&#37319;&#26679;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;BAS&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#26368;&#20248;&#28526;&#27969;&#65288;OPF&#65289;&#30340;&#20248;&#21270;&#20195;&#29702;&#65292;&#21363;&#36817;&#20284;OPF&#30340;&#36755;&#20837;/&#36755;&#20986;&#20851;&#31995;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#35777;&#26126;&#36825;&#20123;&#20195;&#29702;&#21487;&#20197;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#38656;&#35201;&#23545;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#36827;&#34892;OPF&#30340;&#65288;&#31163;&#32447;&#65289;&#27714;&#35299;&#12290;&#20026;&#20102;&#28385;&#36275;&#24066;&#22330;&#28165;&#31639;&#24212;&#29992;&#30340;&#35201;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;&#65288;BAS&#65289;&#65292;&#26088;&#22312;&#22312;&#26102;&#38388;&#38480;&#21046;&#20869;&#35757;&#32451;&#26368;&#20339;&#30340;OPF&#20195;&#29702;&#12290;BAS&#23558;&#36755;&#20837;&#20998;&#24067;&#20998;&#25104;&#26742;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#20989;&#25968;&#30830;&#23450;&#19979;&#19968;&#27425;&#37319;&#26679;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#23558;&#30456;&#21516;&#30340;&#20998;&#26742;&#24212;&#29992;&#20110;&#39564;&#35777;&#38598;&#65292;BAS&#21033;&#29992;&#26631;&#35760;&#30340;&#39564;&#35777;&#26679;&#26412;&#26469;&#36873;&#25321;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;BAS&#36824;&#20381;&#36182;&#20110;&#38543;&#26102;&#38388;&#22686;&#21152;&#21644;&#20943;&#23569;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;BAS&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers optimization proxies for Optimal Power Flow (OPF), i.e., machine-learning models that approximate the input/output relationship of OPF. Recent work has focused on showing that such proxies can be of high fidelity. However, their training requires significant data, each instance necessitating the (offline) solving of an OPF for a sample of the input distribution. To meet the requirements of market-clearing applications, this paper proposes Bucketized Active Sampling (BAS), a novel active learning framework that aims at training the best possible OPF proxy within a time limit. BAS partitions the input distribution into buckets and uses an acquisition function to determine where to sample next. By applying the same partitioning to the validation set, BAS leverages labeled validation samples in the selection of unlabeled samples. BAS also relies on an adaptive learning rate that increases and decreases over time. Experimental results demonstrate the benefits of BAS.
&lt;/p&gt;</description></item><item><title>MetaCOG&#26159;&#19968;&#20010;&#23398;&#20064;&#20803;&#35748;&#30693;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#21487;&#38752;&#24615;&#34920;&#31034;&#65292;&#22686;&#21152;&#20102;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#21453;&#39304;&#21644;&#22320;&#38754;&#30495;&#23454;&#30340;&#29289;&#20307;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2110.03105</link><description>&lt;p&gt;
MetaCOG: &#23398;&#20064;&#20803;&#35748;&#30693;&#20197;&#24674;&#22797;&#23454;&#38469;&#23384;&#22312;&#30340;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
MetaCOG: Learning a Metacognition to Recover What Objects Are Actually There. (arXiv:2110.03105v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03105
&lt;/p&gt;
&lt;p&gt;
MetaCOG&#26159;&#19968;&#20010;&#23398;&#20064;&#20803;&#35748;&#30693;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#21487;&#38752;&#24615;&#34920;&#31034;&#65292;&#22686;&#21152;&#20102;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#21453;&#39304;&#21644;&#22320;&#38754;&#30495;&#23454;&#30340;&#29289;&#20307;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19981;&#20165;&#26681;&#25454;&#25105;&#20204;&#25152;&#30475;&#21040;&#30340;&#20869;&#23481;&#24418;&#25104;&#20851;&#20110;&#19990;&#30028;&#30340;&#34920;&#24449;&#65292;&#36824;&#23398;&#20064;&#20851;&#20110;&#25105;&#20204;&#33258;&#24049;&#35270;&#35273;&#22914;&#20309;&#24037;&#20316;&#30340;&#20803;&#35748;&#30693;&#34920;&#24449;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#25105;&#20204;&#30340;&#35270;&#35273;&#19981;&#21487;&#38752;&#65288;&#20363;&#22914;&#65292;&#24403;&#25105;&#20204;&#24847;&#35782;&#21040;&#25105;&#20204;&#27491;&#22312;&#32463;&#21382;&#35270;&#35273;&#38169;&#35273;&#26102;&#65289;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#25105;&#20204;&#25152;&#30475;&#21040;&#30340;&#20869;&#23481;&#25552;&#20986;&#36136;&#30097;&#12290;&#21463;&#21040;&#36825;&#31181;&#20154;&#31867;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaCOG&#65306;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20854;&#21487;&#38752;&#24615;&#34920;&#31034;&#26469;&#22686;&#21152;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaCOG&#26159;&#19968;&#20010;&#23618;&#27425;&#27010;&#29575;&#27169;&#22411;&#65292;&#23545;&#19968;&#20010;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#21644;&#26816;&#27979;&#22120;&#20135;&#29983;&#30340;&#36755;&#20986;&#34920;&#36798;&#20102;&#19968;&#20010;&#32852;&#21512;&#20998;&#24067;&#12290;&#24403;&#19982;&#29616;&#25104;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#37197;&#23545;&#20351;&#29992;&#26102;&#65292;MetaCOG&#23558;&#26816;&#27979;&#32467;&#26524;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25512;&#26029;&#20986;&#26816;&#27979;&#22120;&#38169;&#28431;&#26816;&#26576;&#20123;&#31867;&#21035;&#30340;&#29289;&#20307;&#21644;&#34394;&#26500;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#30340;&#20542;&#21521;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#30340;&#29289;&#20307;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans not only form representations about the world based on what we see, but also learn meta-cognitive representations about how our own vision works. This enables us to recognize when our vision is unreliable (e.g., when we realize that we are experiencing a visual illusion) and enables us to question what we see. Inspired by this human capacity, we present MetaCOG: a model that increases the robustness of object detectors by learning representations of their reliability, and does so without feedback. Specifically, MetaCOG is a hierarchical probabilistic model that expresses a joint distribution over the objects in a 3D scene and the outputs produced by a detector. When paired with an off-the-shelf object detector, MetaCOG takes detections as input and infers the detector's tendencies to miss objects of certain categories and to hallucinate objects that are not actually present, all without access to ground-truth object labels. When paired with three modern neural object detectors, 
&lt;/p&gt;</description></item></channel></rss>