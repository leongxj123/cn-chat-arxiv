<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#23574;&#23792;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25214;&#20986;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#21457;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2312.16903</link><description>&lt;p&gt;
&#21035;&#20877;&#20986;&#29616;&#23574;&#23792;&#20102;&#65306;&#31283;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Spike No More: Stabilizing the Pre-training of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.16903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#23574;&#23792;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25214;&#20986;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#32463;&#24120;&#20986;&#29616;&#25439;&#22833;&#23574;&#23792;&#12290;&#36825;&#20123;&#23574;&#23792;&#20250;&#38477;&#20302;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#20250;&#30772;&#22351;&#39044;&#35757;&#32451;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#24212;&#35813;&#36991;&#20813;&#36825;&#31181;&#23574;&#23792;&#30340;&#20986;&#29616;&#12290;&#20026;&#20102;&#30740;&#31350;&#25439;&#22833;&#23574;&#23792;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#20851;&#27880;&#20869;&#37096;&#23618;&#30340;&#26799;&#24230;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#20004;&#20010;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#38450;&#26799;&#24230;&#29190;&#28856;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#21021;&#22987;&#21270;&#26041;&#27861;&#21644;&#23545;&#23884;&#20837;&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#26469;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training. Since the pre-training needs a vast computational budget, we should avoid such spikes. To investigate the cause of loss spikes, we focus on gradients of internal layers. Through theoretical analyses, we reveal two causes of the exploding gradients, and provide requirements to prevent the explosion. In addition, we propose a method to satisfy the requirements by combining the initialization method and a simple modification to embeddings. We conduct various experiments to verify our theoretical analyses empirically. Experimental results indicate that the combination is effective in preventing spikes during pre-training.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#25163;&#32534;&#25490;&#65288;LABOR&#65289;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#24212;&#23545;&#20102;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#21452;&#25163;&#20219;&#21153;&#21327;&#35843;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.02018</link><description>&lt;p&gt;
&#29992;&#20110;&#32534;&#25490;&#21452;&#25163;&#26426;&#22120;&#20154;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Orchestrating Bimanual Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#25163;&#32534;&#25490;&#65288;LABOR&#65289;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#24212;&#23545;&#20102;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#21452;&#25163;&#20219;&#21153;&#21327;&#35843;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20351;&#26426;&#22120;&#20154;&#20855;&#26377;&#35299;&#20915;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#30340;&#33021;&#21147;&#24050;&#32463;&#21462;&#24471;&#20102;&#36805;&#36895;&#36827;&#23637;&#65292;&#20294;&#20026;&#21452;&#25163;&#26426;&#22120;&#20154;&#29983;&#25104;&#25511;&#21046;&#31574;&#30053;&#20197;&#35299;&#20915;&#28041;&#21450;&#20004;&#21482;&#25163;&#30340;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#22312;&#26377;&#25928;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#21327;&#35843;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#26377;&#36880;&#27493;&#25512;&#29702;&#21644;&#32972;&#26223;&#23398;&#20064;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#25511;&#21046;&#20102;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21333;&#20010;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#36827;&#34892;&#35821;&#35328;&#20132;&#27969;&#30340;&#26412;&#36136;&#20351;&#24471;LLM&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#21452;&#25163;&#20219;&#21153;&#21327;&#35843;&#25104;&#20026;&#19968;&#39033;&#29305;&#27530;&#25361;&#25112;&#12290;&#20026;&#20102;&#39318;&#27425;&#36890;&#36807;LLM&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#25163;&#32534;&#25490;&#65288;LABOR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#20998;&#26512;&#20219;&#21153;&#37197;&#32622;&#24182;&#35774;&#35745;&#21327;&#35843;&#25511;&#21046;&#31574;&#30053;&#20197;&#35299;&#20915;&#38271;&#26399;&#21452;&#25163;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;LABOR&#20195;&#29702;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02018v1 Announce Type: cross  Abstract: Although there has been rapid progress in endowing robots with the ability to solve complex manipulation tasks, generating control policies for bimanual robots to solve tasks involving two hands is still challenging because of the difficulties in effective temporal and spatial coordination. With emergent abilities in terms of step-by-step reasoning and in-context learning, Large Language Models (LLMs) have taken control of a variety of robotic tasks. However, the nature of language communication via a single sequence of discrete symbols makes LLM-based coordination in continuous space a particular challenge for bimanual tasks. To tackle this challenge for the first time by an LLM, we present LAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM to analyze task configurations and devise coordination control policies for addressing long-horizon bimanual tasks. In the simulated environment, the LABOR agent is eval
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#65292;&#22312;&#21508;&#31181;&#32422;&#26463;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.13244</link><description>&lt;p&gt;
&#20351;&#29992;&#24072;&#29983;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13244
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#65292;&#22312;&#21508;&#31181;&#32422;&#26463;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#27169;&#22411;&#21644;&#35745;&#31639;&#24037;&#20855;&#29992;&#20110;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#20998;&#26512;&#65292;&#20294;&#29983;&#25104;&#31526;&#21512;&#25152;&#26377;&#26399;&#26395;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#20998;&#23376;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#31867;&#20284;&#20110;&#23398;&#29983;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#26469;&#33258;&#21508;&#31181;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#65288;&#21363;&#8220;&#32769;&#24072;&#8221;&#65289;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35757;&#32451;TSMMG&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#36825;&#20123;&#8216;&#32769;&#24072;&#8217;&#20013;&#25552;&#21462;&#30340;&#20998;&#23376;&#30693;&#35782;&#26500;&#24314;&#20102;&#22823;&#37327;&#25991;&#26412;-&#20998;&#23376;&#23545;&#65292;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#21508;&#31181;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;TSMMG&#22312;&#29983;&#25104;&#31526;&#21512;&#22797;&#26434;&#12289;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20004;&#12289;&#19977;&#21644;&#22235;&#32422;&#26463;&#20219;&#21153;&#30340;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24179;&#22343;&#20998;&#23376;&#26377;&#25928;&#24615;&#36229;&#36807;99&#65285;&#65292;&#25104;&#21151;&#29575;&#20998;&#21035;&#20026;88.08&#65285;&#12289;65.27&#65285;&#21644;61.44&#65285;&#12290;&#35813;&#27169;&#22411;&#36824;ex
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13244v1 Announce Type: new  Abstract: While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge. Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'. To train TSMMG, we construct a large set of text-molecule pairs by extracting molecular knowledge from these 'teachers', enabling it to generate novel molecules that conform to the descriptions through various text prompts. We experimentally show that TSMMG remarkably performs in generating molecules meeting complex, natural language-described property requirements across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 88.08%, 65.27%, and 61.44%, respectively. The model also ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.12510</link><description>&lt;p&gt;
&#22270;&#20687;&#25805;&#20316;&#30340;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Consistency Trajectory Models for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#21644;&#24674;&#22797;&#31561;&#24212;&#29992;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#22312;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#24615;&#36136;&#65306;&#25193;&#25955;&#23558;&#23558;&#22122;&#22768;&#21040;&#25968;&#25454;&#30340;&#22797;&#26434;&#26144;&#23556;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#21435;&#22122;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#27880;&#20837;&#24341;&#23548;&#39033;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#36807;&#31243;&#20063;&#24120;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#25968;&#21313;&#27425;&#29978;&#33267;&#25968;&#21315;&#27425;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTMs&#65289;&#21487;&#20197;&#22312;&#27010;&#29575;&#27969;ODE&#65288;PFODE&#65289;&#19978;&#20219;&#24847;&#26102;&#38388;&#28857;&#20043;&#38388;&#36827;&#34892;&#36941;&#21382;&#65292;&#24182;&#19988;&#36890;&#36807;&#21333;&#27425;&#20989;&#25968;&#35780;&#20272;&#36827;&#34892;&#24471;&#20998;&#25512;&#23548;&#65292;&#20294;CTMs&#20165;&#20801;&#35768;&#20174;&#39640;&#26031;&#22122;&#22768;&#36716;&#25442;&#20026;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#24191;&#20041;CTMs&#65288;GCTMs&#65289;&#26469;&#21457;&#25381;CTMs&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23454;&#29616;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12242</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;
&lt;/p&gt;
&lt;p&gt;
Reference-based Metrics Disprove Themselves in Question Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12242
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLEU&#21644;BERTScore&#31561;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;(QG)&#12290;&#26412;&#30740;&#31350;&#22312;SQuAD&#21644;HotpotQA&#31561;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#24182;&#19981;&#33021;&#20445;&#35777;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;&#22823;&#22810;&#25968;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#21482;&#26377;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#65307;&#25105;&#20204;&#22797;&#21046;&#20102;&#27880;&#37322;&#36807;&#31243;&#24182;&#25910;&#38598;&#20102;&#21478;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#12290;&#39044;&#26399;&#22909;&#30340;&#25351;&#26631;&#24212;&#35813;&#23545;&#20154;&#24037;&#39564;&#35777;&#30340;&#38382;&#39064;&#30340;&#35780;&#20998;&#19981;&#20250;&#20302;&#20110;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#26032;&#25910;&#38598;&#30340;&#21442;&#32771;&#25991;&#29486;&#19978;&#65292;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#32467;&#26524;&#21364;&#35777;&#26126;&#20102;&#36825;&#20123;&#25351;&#26631;&#26412;&#36523;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#65292;&#30001;&#22810;&#32500;&#26631;&#20934;&#32452;&#25104;&#65292;&#22914;&#33258;&#28982;&#24615;&#12289;&#21487;&#22238;&#31572;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#26631;&#20934;&#19981;&#21463;&#38480;&#20110;&#21333;&#20010;&#21442;&#32771;&#38382;&#39064;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#65292;&#35813;&#25351;&#26631;&#20063;&#19981;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12242v1 Announce Type: cross  Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our met
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BirdSet&#22522;&#20934;&#65292;&#29992;&#20110;&#40479;&#31867;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#25972;&#21512;&#24320;&#28304;&#40479;&#31867;&#24405;&#38899;&#25968;&#25454;&#38598;&#21512;&#65292;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#21644;&#35782;&#21035;&#28508;&#22312;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.10380</link><description>&lt;p&gt;
BirdSet&#65306;&#40479;&#31867;&#29983;&#29289;&#22768;&#23398;&#20998;&#31867;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10380
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BirdSet&#22522;&#20934;&#65292;&#29992;&#20110;&#40479;&#31867;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#25972;&#21512;&#24320;&#28304;&#40479;&#31867;&#24405;&#38899;&#25968;&#25454;&#38598;&#21512;&#65292;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#21644;&#35782;&#21035;&#28508;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#40479;&#31867;&#29983;&#29289;&#22768;&#23398;&#39046;&#22495;&#35786;&#26029;&#29615;&#22659;&#20581;&#24247;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#32473;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BirdSet&#22522;&#20934;&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32508;&#21512;&#30740;&#31350;&#21162;&#21147;&#65292;&#20197;&#20840;&#38754;&#20998;&#31867;&#40479;&#31867;&#40483;&#21483;&#22768;&#12290;BirdSet&#23558;&#24320;&#28304;&#40479;&#31867;&#24405;&#38899;&#25972;&#21512;&#21040;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#21512;&#20013;&#65292;&#25552;&#20379;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#24182;&#35782;&#21035;&#36328;&#19981;&#21516;&#30740;&#31350;&#30340;&#28508;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10380v1 Announce Type: cross  Abstract: Deep learning (DL) models have emerged as a powerful tool in avian bioacoustics to diagnose environmental health and biodiversity. However, inconsistencies in research pose notable challenges hindering progress in this domain. Reliable DL models need to analyze bird calls flexibly across various species and environments to fully harness the potential of bioacoustics in a cost-effective passive acoustic monitoring scenario. Data fragmentation and opacity across studies complicate a comprehensive evaluation of general model performance. To overcome these challenges, we present the BirdSet benchmark, a unified framework consolidating research efforts with a holistic approach for classifying bird vocalizations in avian bioacoustics. BirdSet harmonizes open-source bird recordings into a curated dataset collection. This unified approach provides an in-depth understanding of model performance and identifies potential shortcomings across diffe
&lt;/p&gt;</description></item><item><title>&#23545;&#21307;&#23398;LLMs&#36827;&#34892;&#20102;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23450;&#20041;&#21307;&#23398;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#65292;&#24320;&#21457;&#20102;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.03744</link><description>&lt;p&gt;
&#20026;&#21307;&#33647;&#39046;&#22495;&#25171;&#36896;&#23433;&#20840;&#21644;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Safe and Aligned Large Language Models for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03744
&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#23398;LLMs&#36827;&#34892;&#20102;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23450;&#20041;&#21307;&#23398;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#65292;&#24320;&#21457;&#20102;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#27491;&#22312;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#36827;&#27493;&#65292;&#21363;&#20351;&#26159;&#23427;&#20204;&#30340;&#24320;&#21457;&#32773;&#20063;&#23545;&#23427;&#20204;&#30340;&#28508;&#21147;&#21644;&#39118;&#38505;&#30340;&#28145;&#24230;&#24863;&#21040;&#22256;&#24785;&#12290;&#23613;&#31649;&#24050;&#32463;&#37319;&#21462;&#20102;&#21021;&#27493;&#27493;&#39588;&#35780;&#20272;&#36890;&#29992;&#30693;&#35782;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#24369;&#28857;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23613;&#31649;&#22312;&#20010;&#20154;&#20581;&#24247;&#21644;&#23433;&#20840;&#12289;&#20844;&#20849;&#20581;&#24247;&#21644;&#23433;&#20840;&#20197;&#21450;&#20154;&#26435;&#26041;&#38754;&#23384;&#22312;&#39118;&#38505;&#65292;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#23578;&#26410;&#24471;&#21040;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#21307;&#23398;LLMs&#30340;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21307;&#23398;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#30340;&#23450;&#20041;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;LLM&#30340;&#21307;&#23398;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#36890;&#29992;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26356;&#24191;&#27867;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03744v1 Announce Type: new  Abstract: The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks. While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights. To this end, we carry out the first safety evaluation for medical LLMs. Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale appr
&lt;/p&gt;</description></item><item><title>AutoRD&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#65292;&#23454;&#29616;&#20102;&#25972;&#20307;F1&#24471;&#20998;47.3%&#65292;&#30456;&#23545;&#20110;&#22522;&#30784;LLM&#26377;14.4%&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.00953</link><description>&lt;p&gt;
AutoRD&#65306;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#26500;&#24314;&#30340;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00953
&lt;/p&gt;
&lt;p&gt;
AutoRD&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#65292;&#23454;&#29616;&#20102;&#25972;&#20307;F1&#24471;&#20998;47.3%&#65292;&#30456;&#23545;&#20110;&#22522;&#30784;LLM&#26377;14.4%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#21517;&#20026;AutoRD&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#26377;&#20851;&#32597;&#35265;&#30142;&#30149;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#27979;&#35797;&#26469;&#35780;&#20272;AutoRD&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#24378;&#35843;&#20102;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30340;&#31995;&#32479;AutoRD&#26159;&#19968;&#20010;&#36719;&#20214;&#27969;&#27700;&#32447;&#65292;&#28041;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#23454;&#20307;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#23454;&#20307;&#26657;&#20934;&#21644;&#30693;&#35782;&#22270;&#26500;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30001;&#24320;&#28304;&#21307;&#23398;&#26412;&#20307;&#21457;&#23637;&#32780;&#26469;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20307;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#20197;&#21450;&#30693;&#35782;&#22270;&#26500;&#24314;&#24615;&#33021;&#23545;&#31995;&#32479;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#12290;&#32467;&#26524;&#65306;AutoRD&#21462;&#24471;&#20102;47.3%&#30340;&#25972;&#20307;F1&#20998;&#25968;&#65292;&#36739;&#22522;&#30784;LLM&#25552;&#39640;&#20102;14.4%&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AutoRD&#23454;&#29616;&#20102;56.1%&#30340;&#25972;&#20307;&#23454;&#20307;&#25552;&#21462;F1&#20998;&#25968;&#65288;&#32597;&#35265;&#30142;&#30149;&#65306;83.5%&#65292;&#30142;&#30149;&#65306;35.8%&#65292;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00953v1 Announce Type: cross  Abstract: Objectives: Our objective is to create an end-to-end system called AutoRD, which automates extracting information from clinical text about rare diseases. We have conducted various tests to evaluate the performance of AutoRD and highlighted its strengths and limitations in this paper.   Materials and Methods: Our system, AutoRD, is a software pipeline involving data preprocessing, entity extraction, relation extraction, entity calibration, and knowledge graph construction. We implement this using large language models and medical knowledge graphs developed from open-source medical ontologies. We quantitatively evaluate our system on entity extraction, relation extraction, and the performance of knowledge graph construction.   Results: AutoRD achieves an overall F1 score of 47.3%, a 14.4% improvement compared to the base LLM. In detail, AutoRD achieves an overall entity extraction F1 score of 56.1% (rare_disease: 83.5%, disease: 35.8%, s
&lt;/p&gt;</description></item><item><title>DiaHalu&#26159;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#32423;&#21035;&#19978;&#30340;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.00896</link><description>&lt;p&gt;
DiaHalu&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00896
&lt;/p&gt;
&lt;p&gt;
DiaHalu&#26159;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#32423;&#21035;&#19978;&#30340;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#26368;&#36817;&#20960;&#24180;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#24187;&#35273;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26377;&#35768;&#22810;&#22522;&#20934;&#34987;&#25552;&#20986;&#26469;&#26816;&#27979;&#36825;&#31181;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#22522;&#20934;&#19981;&#26159;&#30001;LLMs&#33258;&#28982;&#29983;&#25104;&#30340;&#65292;&#32780;&#26159;&#26377;&#24847;&#24341;&#21457;&#30340;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22522;&#20934;&#20165;&#20851;&#27880;&#20107;&#23454;&#19978;&#30340;&#24187;&#35273;&#65292;&#32780;&#24573;&#35270;&#20102;&#24544;&#23454;&#24230;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;LLMs&#26102;&#20195;&#23545;&#35805;&#27169;&#24335;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#30446;&#21069;&#30340;&#22522;&#20934;&#20165;&#38598;&#20013;&#22312;&#21477;&#23376;&#32423;&#21644;&#27573;&#33853;&#32423;&#30340;&#24187;&#35273;&#19978;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; DiaHalu&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25910;&#38598;&#30340;&#20027;&#39064;&#38598;&#25104;&#21040;&#31995;&#32479;&#25552;&#31034;&#20013;&#65292;&#20419;&#36827;&#20004;&#20010;ChatGPT3.5&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25163;&#21160;&#20462;&#25913;&#19981;&#31526;&#21512;&#20154;&#31867;&#35821;&#35328;&#32422;&#23450;&#30340;&#20869;&#23481;&#65292;&#28982;&#21518;&#35753;LLMs&#37325;&#26032;&#29983;&#25104;&#65292;&#27169;&#25311;&#30495;&#23454;&#30340;&#20154;&#31867;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00896v1 Announce Type: cross  Abstract: Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, numerous benchmarks are proposed to detect the hallucination. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation benchmark to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-
&lt;/p&gt;</description></item><item><title>TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19467</link><description>&lt;p&gt;
TV-TREES&#65306;&#29992;&#20110;&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;
&lt;/p&gt;
&lt;p&gt;
TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19467
&lt;/p&gt;
&lt;p&gt;
TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#30005;&#35270;&#21098;&#36753;&#31561;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#37096;&#20998;&#26159;&#22240;&#20026;&#24403;&#21069;&#30340;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#25512;&#29702;&#65292;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TV-TREES&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#12290;TV-TREES&#20316;&#20026;&#19968;&#31181;&#20419;&#36827;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#20219;&#21153;&#26469;&#35780;&#20272;&#27492;&#31867;&#26041;&#27861;&#30340;&#25512;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#12289;&#20855;&#26377;&#26368;&#20808;&#36827;&#38646;-shot&#24615;&#33021;&#30340;&#23436;&#25972;&#35270;&#39057;&#21098;&#36753;&#65292;&#23637;&#31034;&#20102;&#19982;&#40657;&#30418;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19467v1 Announce Type: cross  Abstract: It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#36753;&#65288;UKE&#65289;&#65292;&#26088;&#22312;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20316;&#20026;&#30693;&#35782;&#26356;&#26032;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#26500;&#24314;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#21644;&#21709;&#24212;&#24615;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18909</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#20107;&#23454;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#65306;&#36808;&#21521;&#23454;&#29992;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#36753;&#65288;UKE&#65289;&#65292;&#26088;&#22312;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20316;&#20026;&#30693;&#35782;&#26356;&#26032;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#26500;&#24314;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#21644;&#21709;&#24212;&#24615;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#23558;&#30693;&#35782;&#26356;&#26032;&#27880;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#20854;&#20445;&#25345;&#27491;&#30830;&#24615;&#21644;&#26368;&#26032;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#31574;&#30053;&#26126;&#26174;&#19981;&#20999;&#23454;&#38469;&#65306;&#23427;&#20204;&#20165;&#20351;&#29992;&#31934;&#24515;&#31574;&#21010;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#65288;&#20027;&#39064;&#12289;&#20851;&#31995;&#21644;&#23545;&#35937;&#30340;&#19977;&#20803;&#32452;&#65289;&#36827;&#34892;&#26356;&#26032;&#65292;&#32780;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#26356;&#26032;&#36890;&#24120;&#20986;&#29616;&#22312;&#26032;&#38395;&#25991;&#31456;&#31561;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#36753;&#65288;UKE&#65289;&#12290;&#23427;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#30452;&#25509;&#35780;&#20272;&#32534;&#36753;&#24615;&#33021;&#65292;&#31216;&#20026;&#38750;&#32467;&#26500;&#21270;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;UKE&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#32467;&#26500;&#21270;&#20107;&#23454;&#26500;&#24314;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#21709;&#24212;&#36805;&#36895;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#25104;&#20026;&#19968;&#20010;&#26356;&#23454;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#22312;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;UKE&#23545;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#23427;&#20204;&#30340;&#20851;&#38190;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18909v1 Announce Type: cross  Abstract: Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date. However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with subjects, relations, and objects), whereas real-world knowledge updates commonly emerge in unstructured texts like news articles. In this paper, we propose a new benchmark, Unstructured Knowledge Editing (UKE). It evaluates editing performance directly using unstructured texts as knowledge updates, termed unstructured facts. Hence UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, becoming a more practical benchmark. We conduct extensive experiments on newly built datasets and demonstrate that UKE poses a significant challenge to state-of-the-art knowledge editing methods, resulting in their critical performance declines. We further
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.16048</link><description>&lt;p&gt;
LLMs&#24102;&#26377;&#24605;&#32500;&#38142;&#26465;&#26159;&#38750;&#22240;&#26524;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
LLMs with Chain-of-Thought Are Non-Causal Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#26377;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;LLMs&#20013;&#27491;&#30830;&#31572;&#26696;&#36319;&#38543;&#19981;&#27491;&#30830;CoTs&#30340;&#39057;&#29575;&#21450;&#21453;&#20043;&#12290;&#25105;&#20204;&#37319;&#29992;&#22240;&#26524;&#20998;&#26512;&#26469;&#35780;&#20272;CoTs/&#25351;&#20196;&#19982;LLMs&#31572;&#26696;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25581;&#31034;LLMs&#36817;&#20284;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#12290;&#36890;&#36807;&#27604;&#36739;&#26263;&#31034;SCM&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;SCM&#65292;&#25105;&#20204;&#31361;&#26174;&#20102;LLM&#21644;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24433;&#21709;&#26263;&#31034;SCM&#22240;&#26524;&#32467;&#26500;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26174;&#33879;&#24433;&#21709;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;https://github.com/StevenZHB/CoT_Causal_Analysis&#21457;&#24067;&#20102;&#20195;&#30721;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16048v1 Announce Type: cross  Abstract: This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#24335;LLM&#21644;&#31526;&#21512;&#39044;&#27979;&#25216;&#26415;&#30340;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15368</link><description>&lt;p&gt;
&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#30340;&#25216;&#26415;&#23454;&#29616;&#35821;&#35328;&#25351;&#23548;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#23433;&#20840;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#24335;LLM&#21644;&#31526;&#21512;&#39044;&#27979;&#25216;&#26415;&#30340;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#35821;&#35328;&#25351;&#23548;&#26426;&#22120;&#20154;&#22242;&#38431;&#30340;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#12290;&#20219;&#21153;&#29992;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#34920;&#31034;&#65292;&#35201;&#27714;&#26426;&#22120;&#20154;&#22312;&#21508;&#31181;&#20301;&#32622;&#21644;&#35821;&#20041;&#23545;&#35937;&#19978;&#24212;&#29992;&#23427;&#20204;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#31227;&#21160;&#12289;&#25805;&#20316;&#21644;&#24863;&#30693;&#65289;&#12290;&#26368;&#36817;&#20960;&#31687;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35774;&#35745;&#26377;&#25928;&#30340;&#22810;&#26426;&#22120;&#20154;&#35745;&#21010;&#26469;&#35299;&#20915;&#31867;&#20284;&#30340;&#35268;&#21010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#20219;&#21153;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#24335;LLM&#30340;&#35268;&#21010;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#26469;&#23454;&#29616;&#30340;&#65292;CP&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#23545;&#20854;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25512;&#29702;&#12290;CP&#20801;&#35768;&#25152;&#25552;&#20986;&#30340;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#20197;&#20998;&#24067;&#26041;&#24335;&#25512;&#29702;&#20854;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#22312;&#20805;&#20998;&#20449;&#20219;&#26102;&#33021;&#22815;&#20570;&#20986;&#20010;&#21035;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15368v1 Announce Type: cross  Abstract: This paper addresses task planning problems for language-instructed robot teams. Tasks are expressed in natural language (NL), requiring the robots to apply their capabilities (e.g., mobility, manipulation, and sensing) at various locations and semantic objects. Several recent works have addressed similar planning problems by leveraging pre-trained Large Language Models (LLMs) to design effective multi-robot plans. However, these approaches lack mission performance and safety guarantees. To address this challenge, we introduce a new decentralized LLM-based planner that is capable of achieving high mission success rates. This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in black-box models. CP allows the proposed multi-robot planner to reason about its inherent uncertainty in a decentralized fashion, enabling robots to make individual decisions when they are sufficiently ce
&lt;/p&gt;</description></item><item><title>ChatEA&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23454;&#20307;&#23545;&#40784;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;KG-code&#32763;&#35793;&#27169;&#22359;&#21644;&#20004;&#38454;&#27573;EA&#31574;&#30053;&#26469;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15048</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#20307;&#23545;&#40784;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Power of Large Language Models for Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15048
&lt;/p&gt;
&lt;p&gt;
ChatEA&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23454;&#20307;&#23545;&#40784;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;KG-code&#32763;&#35793;&#27169;&#22359;&#21644;&#20004;&#38454;&#27573;EA&#31574;&#30053;&#26469;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#23545;&#20110;&#25972;&#21512;&#19981;&#21516;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;EA&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#27604;&#36739;&#23454;&#20307;&#23884;&#20837;&#65292;&#20294;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#36755;&#20837;KG&#25968;&#25454;&#21644;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;&#33021;&#21147;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#32422;&#26463;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatEA&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34701;&#20837;&#20197;&#25913;&#21892;EA&#12290;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#30340;&#36755;&#20837;KG&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;ChatEA&#24341;&#20837;&#20102;&#19968;&#20010;KG-code&#32763;&#35793;&#27169;&#22359;&#65292;&#23558;KG&#32467;&#26500;&#32763;&#35793;&#25104;LLMs&#21487;&#29702;&#35299;&#30340;&#26684;&#24335;&#65292;&#20174;&#32780;&#20351;LLMs&#33021;&#22815;&#21033;&#29992;&#20854;&#24191;&#27867;&#30340;&#32972;&#26223;&#30693;&#35782;&#25552;&#39640;EA&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#23545;&#23454;&#20307;&#23884;&#20837;&#27604;&#36739;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;ChatEA&#23454;&#29616;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;EA&#31574;&#30053;&#65292;&#21033;&#29992;LLMs&#22312;&#23545;&#35805;&#26684;&#24335;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15048v1 Announce Type: cross  Abstract: Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs' capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy wh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;</title><link>https://arxiv.org/abs/2402.12365</link><description>&lt;p&gt;
&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Universal Physics Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26367;&#20195;&#32773;&#36817;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#23427;&#20204;&#30340;&#25968;&#20540;&#23545;&#24212;&#29289;&#65292;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21363;&#20351;&#31995;&#32479;&#30340;&#22522;&#30784;&#21160;&#24577;&#30456;&#20284;&#12290;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#26159;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#34920;&#36848;&#65292;&#36825;&#20026;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#24314;&#27169;&#22522;&#20110;&#31890;&#23376;&#32780;&#19981;&#26159;&#32593;&#26684;&#30340;&#21160;&#24577;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#27169;&#25311;&#20102;&#19968;&#31995;&#21015;&#26102;&#31354;&#38382;&#39064; - &#23545;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#12290;UPTs&#22312;&#27809;&#26377;&#22522;&#20110;&#32593;&#26684;&#25110;&#22522;&#20110;&#31890;&#23376;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#20174;&#32780;&#22312;&#32593;&#26684;&#21644;&#31890;&#23376;&#20043;&#38388;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;UPTs&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#20256;&#25773;&#21160;&#24577;&#65292;&#24378;&#35843;&#20102;&#36870;&#32534;&#30721;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;UPTs&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12365v1 Announce Type: cross  Abstract: Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space repre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11161</link><description>&lt;p&gt;
PANDA&#65288;Pedantic ANswer-correctness Determination and Adjudication&#65289;&#65306;&#25913;&#36827;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;QA&#65289;&#21482;&#26377;&#22312;&#25105;&#20204;&#30693;&#36947;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#26102;&#25165;&#33021;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#26377;&#36259;&#30340;QA&#31034;&#20363;&#65292;&#24403;&#21069;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#65288;AC&#65289;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20887;&#38271;&#12289;&#33258;&#30001;&#26684;&#24335;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#32570;&#20047;&#25968;&#25454;&#21644;&#27169;&#22411;&#36807;&#22823;&#12290;&#22522;&#20110;LLM&#30340;&#35780;&#20998;&#22120;&#19982;&#20154;&#31867;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20294;&#36825;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#20165;&#22312;&#26377;&#38480;&#30340;QA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#28165;&#26224;&#30340;&#25351;&#21335;&#26469;&#35780;&#20272;&#20174;&#20154;&#31867;QA&#27604;&#36187;&#20013;&#37319;&#32435;&#30340;&#26426;&#22120;QA&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#30830;&#23450;&#21644;&#35009;&#20915;&#65288;Precise ANswer correctness Determination and Adjudication&#65292;PANDA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23567;&#24039;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;AC&#20998;&#31867;&#22120;&#65288;812 KB&#65289;&#65292;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11161v1 Announce Type: cross  Abstract: Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big. LLM based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing clear guidelines for evaluating machine QA adopted from human QA contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.
&lt;/p&gt;</description></item><item><title>SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08653</link><description>&lt;p&gt;
SAGMAN: &#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#24418;&#19978;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08653
&lt;/p&gt;
&lt;p&gt;
SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23545;&#36755;&#20837;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;SAGMAN&#30340;&#35889;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#39564;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;GNN&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#27969;&#24418;&#20043;&#38388;&#24341;&#36215;&#30340;&#36317;&#31163;&#22833;&#30495;: &#24403;&#36755;&#20837;&#27969;&#34892;&#20013;&#20004;&#20010;&#38468;&#36817;&#30340;&#33410;&#28857;&#65288;&#36890;&#36807;GNN&#27169;&#22411;&#65289;&#34987;&#26144;&#23556;&#21040;&#36755;&#20986;&#27969;&#34892;&#19978;&#30340;&#20004;&#20010;&#36828;&#31163;&#30340;&#33410;&#28857;&#26102;&#65292;&#24847;&#21619;&#30528;&#23384;&#22312;&#36739;&#22823;&#30340;&#36317;&#31163;&#22833;&#30495;&#65292;&#20174;&#32780;&#23548;&#33268;GNN&#30340;&#31283;&#23450;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#65288;GDR&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#35889;&#22270;&#23884;&#20837;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#26469;&#21019;&#24314;&#20302;&#32500;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22522;&#20110;&#22270;&#30340;&#27969;&#24418;&#65292;&#20197;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;SAGMAN&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#27599;&#20010;&#33410;&#28857;&#22312;&#38754;&#23545;&#19981;&#21516;&#36793;&#32536;&#25110;&#29305;&#24449;&#25200;&#21160;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
&lt;/p&gt;</description></item><item><title>MOMENT&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03885</link><description>&lt;p&gt;
MOMENT&#65306;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;
&lt;/p&gt;
&lt;p&gt;
MOMENT: A Family of Open Time-series Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03885
&lt;/p&gt;
&lt;p&gt;
MOMENT&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MOMENT&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#32570;&#20047;&#19968;&#20010;&#22823;&#32780;&#26377;&#20957;&#32858;&#21147;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#23384;&#20648;&#24211;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22810;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20351;&#24471;&#22810;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#39564;&#35780;&#20272;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#12289;&#26102;&#38388;&#21644;&#30417;&#30563;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#65288;3&#65289;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;&#26102;&#38388;&#24207;&#21015;&#22534;&#65292;&#20197;&#31995;&#32479;&#22320;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#20197;&#35299;&#38145;&#22823;&#35268;&#27169;&#30340;&#22810;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20511;&#37492;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data 
&lt;/p&gt;</description></item><item><title>Paramanu&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#21333;&#20010;GPU&#19978;&#36827;&#34892;&#20102;&#20174;&#22836;&#39044;&#35757;&#32451;&#12290;&#23427;&#36824;&#21253;&#25324;&#19968;&#20010;&#20808;&#36827;&#30340;&#21360;&#24230;&#20998;&#35789;&#22120;&#20197;&#21450;&#36991;&#20813;&#22810;&#35821;&#35328;&#35781;&#21650;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.18034</link><description>&lt;p&gt;
Paramanu: &#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18034
&lt;/p&gt;
&lt;p&gt;
Paramanu&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#21333;&#20010;GPU&#19978;&#36827;&#34892;&#20102;&#20174;&#22836;&#39044;&#35757;&#32451;&#12290;&#23427;&#36824;&#21253;&#25324;&#19968;&#20010;&#20808;&#36827;&#30340;&#21360;&#24230;&#20998;&#35789;&#22120;&#20197;&#21450;&#36991;&#20813;&#22810;&#35821;&#35328;&#35781;&#21650;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Gyan AI Paramanu&#65288;&#8220;&#21407;&#23376;&#8221;&#65289;&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#23427;&#26159;&#19968;&#20010;&#22312;&#21333;&#20010;GPU&#19978;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#30340;&#21253;&#21547;&#21333;&#35821;&#12289;&#21452;&#35821;&#21644;&#22810;&#35821;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#28085;&#30422;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#65288;&#38463;&#33832;&#22982;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#24247;&#22350;&#23612;&#35821;&#12289;&#36808;&#33922;&#21033;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#22885;&#36842;&#20122;&#35821;&#12289;&#26805;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#20197;&#21450;5&#31181;&#19981;&#21516;&#22823;&#23567;&#30340;&#23383;&#27597;&#34920;&#65288;&#23391;&#21152;&#25289;&#35821;&#12289;&#22825;&#22478;&#20307;&#12289;&#22885;&#36842;&#20122;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;1024&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#22312;&#21333;&#20010;GPU&#19978;&#39044;&#35757;&#32451;&#65292;&#38750;&#24120;&#39640;&#25928;&#12289;&#23567;&#24039;&#12289;&#24555;&#36895;&#19988;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20808;&#36827;&#30340;&#21360;&#24230;&#35821;&#20998;&#35789;&#22120;&#65292;&#29978;&#33267;&#21487;&#20197;&#26631;&#35760;&#26410;&#30693;&#35821;&#35328;&#12290;&#20026;&#20102;&#36991;&#20813;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;mParamanu&#27169;&#22411;&#20013;&#30340;&#8220;&#22810;&#35821;&#35328;&#35781;&#21650;&#8221;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#23383;&#27597;&#34920;&#25353;&#35821;&#35328;&#31867;&#22411;&#36827;&#34892;&#20102;&#21487;&#27604;&#36739;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#35780;&#20272;&#25351;&#26631;&#21253;&#25324;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Gyan AI Paramanu ("atom"), a family of novel language models for Indian languages. It is a collection of auto-regressive monolingual, bilingual, and multilingual Indic language models pretrained from scratch on a single GPU for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are pretrained with a context size of 1024 on a single GPU. The models are very efficient, small, fast, and powerful. We have also developed an efficient most advanced Indic tokenizer that can even tokenize unseen languages. In order to avoid the "curse of multi-linguality" in our multilingual mParamanu model, we pretrained on comparable corpora by typological grouping using the same script. We performed human evaluation of our pretrained models for open end text generation on grammar, coherence, creativity, and factuality metrics fo
&lt;/p&gt;</description></item><item><title>LLaMP&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#20197;&#21450;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2401.17244</link><description>&lt;p&gt;
LLaMP: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#20445;&#30495;&#26448;&#26009;&#30693;&#35782;&#26816;&#32034;&#21644;&#25552;&#28860;&#20013;&#30340;&#24378;&#22823;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17244
&lt;/p&gt;
&lt;p&gt;
LLaMP&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#20197;&#21450;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38169;&#35823;&#20449;&#24687;&#23545;&#20110;&#31185;&#23398;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;LLM&#22825;&#29983;&#32570;&#20047;&#38271;&#26399;&#35760;&#24518;&#65292;&#22240;&#27492;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25991;&#29486;&#21644;&#25968;&#25454;&#19978;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#12289;&#20020;&#26102;&#30340;&#21644;&#19981;&#21487;&#36991;&#20813;&#20855;&#26377;&#20559;&#35265;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LLaMP&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#65292;&#30001;&#22810;&#20010;&#25968;&#25454;&#24863;&#30693;&#30340;&#25512;&#29702;&#19982;&#34892;&#21160;&#65288;ReAct&#65289;&#26234;&#33021;&#20307;&#21160;&#24577;&#19982;Materials Project (MP)&#19978;&#30340;&#35745;&#31639;&#21644;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#12290;&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;LLaMP&#23637;&#31034;&#20102;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26041;&#24335;&#30340;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#21363;&#26102;&#33719;&#21462;&#30456;&#20851;&#25968;&#25454;&#23384;&#20648;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#65288;&#22914;&#26230;&#20307;&#32467;&#26500;&#21644;&#24377;&#24615;&#24352;&#37327;&#65289;&#65292;&#24182;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#30340;&#22810;&#27493;&#39588;&#36807;&#31243;&#12290;&#25105;&#20204;&#35777;&#26126;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#65292;&#23558;&#39057;&#32321;&#35760;&#24405;&#30340;&#33021;&#24102;&#38388;&#38553;MAPE&#38477;&#20302;&#20102;5.21%&#65292;&#23558;&#26174;&#33879;&#30340;&#38169;&#35823;&#38477;&#20302;&#20102;1103.54%
&lt;/p&gt;
&lt;p&gt;
Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences where reproducibility is crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of multiple data-aware reasoning-and-acting (ReAct) agents that dynamically interact with computational and experimental data on Materials Project (MP). Without fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structures and elastic tensors), and summarize multi-step procedures for solid-state synthesis. We show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge, reducing a 5.21% MAPE on frequently-documented bandgaps and a significant 1103.54%
&lt;/p&gt;</description></item><item><title>TAP4LLM&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#34920;&#26684;&#25552;&#31034;&#30340;&#22810;&#21151;&#33021;&#39044;&#22788;&#29702;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#37319;&#26679;&#12289;&#22686;&#34917;&#21644;&#25171;&#21253;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#21644;&#22823;&#22411;&#34920;&#26684;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.09039</link><description>&lt;p&gt;
TAP4LLM&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#34920;&#26684;&#25552;&#20379;&#32773;&#22312;&#23545;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#37319;&#26679;&#12289;&#22686;&#34917;&#21644;&#25171;&#21253;
&lt;/p&gt;
&lt;p&gt;
TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09039
&lt;/p&gt;
&lt;p&gt;
TAP4LLM&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#34920;&#26684;&#25552;&#31034;&#30340;&#22810;&#21151;&#33021;&#39044;&#22788;&#29702;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#37319;&#26679;&#12289;&#22686;&#34917;&#21644;&#25171;&#21253;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#21644;&#22823;&#22411;&#34920;&#26684;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#30340;&#25512;&#29702;&#22312;&#32467;&#21512;&#28145;&#24230;&#27169;&#22411;&#21644;&#31163;&#25955;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#38656;&#35201;&#23545;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#38382;&#39064;&#21644;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#34920;&#26684;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#21482;&#32771;&#34385;&#23567;&#22411;&#34920;&#26684;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#26356;&#22823;&#34920;&#26684;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#25512;&#29702;&#22797;&#26434;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#22522;&#26412;&#20449;&#24687;&#25110;&#20998;&#25955;&#22312;&#19981;&#21516;&#20301;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TAP4LLM&#20316;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#39044;&#22788;&#29702;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#24179;&#34913;&#26631;&#35760;&#20998;&#37197;&#26435;&#34913;&#26469;&#29983;&#25104;&#34920;&#26684;&#25552;&#31034;&#65292;&#23454;&#29616;(1) &#34920;&#26684;&#37319;&#26679;&#65292;(2) &#34920;&#26684;&#22686;&#34917;&#21644;(3) &#34920;&#26684;&#25171;&#21253;&#12290;&#22312;&#27599;&#20010;&#27169;&#22359;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#21644;&#35774;&#35745;&#20102;&#20960;&#31181;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#20351;&#29992;&#30340;&#24120;&#35265;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;&#36895;&#24230;&#19982;&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#65289;&#12290;&#25105;&#20204;&#36824;&#23545;T&#20869;&#37096;&#27599;&#20010;&#32452;&#20214;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09039v2 Announce Type: replace-cross  Abstract: Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and semi-structured tabular data. However, previous table reasoning solutions only consider small-sized tables and exhibit limitations in handling larger tables. In addition, most existing methods struggle to reason over complex questions since they lack essential information or they are scattered in different places. To alleviate these challenges, we propose TAP4LLM as a versatile pre-processing toolbox to generate table prompts through (1) table sampling, (2) table augmentation, and (3) table packing while balancing the token allocation trade-off. In each module, we collect and design several common methods for usage in various scenarios (e.g., speed over accuracy). We also provide a comprehensive evaluation on performance of each components inside T
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#22810;&#27169;&#24577;&#20248;&#21270;Big Bang-Big Crunch&#31639;&#27861;&#30340;&#29256;&#26412;&#65292;&#31216;&#20026;k-BBBC&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#22810;&#27169;&#24577;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06153</link><description>&lt;p&gt;
&#22522;&#20110;k-&#32858;&#31867;Big Bang-Big Crunch&#31639;&#27861;&#30340;&#22810;&#27169;&#24577;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Optimization with k-Cluster Big Bang-Big Crunch Algorithm. (arXiv:2401.06153v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06153
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#22810;&#27169;&#24577;&#20248;&#21270;Big Bang-Big Crunch&#31639;&#27861;&#30340;&#29256;&#26412;&#65292;&#31216;&#20026;k-BBBC&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#22810;&#27169;&#24577;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20248;&#21270;&#32463;&#24120;&#22312;&#24037;&#31243;&#38382;&#39064;&#20013;&#36935;&#21040;&#65292;&#29305;&#21035;&#26159;&#22312;&#23547;&#25214;&#19981;&#21516;&#21644;&#26367;&#20195;&#35299;&#20915;&#26041;&#26696;&#26102;&#12290;&#36827;&#21270;&#31639;&#27861;&#36890;&#36807;&#31181;&#32676;&#30340;&#27010;&#24565;&#12289;&#25506;&#32034;/&#24320;&#21457;&#21151;&#33021;&#21644;&#36866;&#21512;&#24182;&#34892;&#35745;&#31639;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#22810;&#27169;&#24577;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#22810;&#27169;&#24577;&#20248;&#21270;Big Bang-Big Crunch&#31639;&#27861;&#30340;&#29256;&#26412;&#65292;&#31216;&#20026;k-BBBC&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#25972;&#20010;&#31181;&#32676;&#30340;&#23436;&#20840;&#25910;&#25947;&#65292;&#23545;&#20110;&#29305;&#23450;&#38382;&#39064;&#24179;&#22343;&#26816;&#32034;&#21040;99\%&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;(i)&#22312;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#30830;&#23450;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#20197;&#21450;(ii)&#23450;&#37327;&#27979;&#37327;&#27491;&#30830;&#26816;&#32034;&#21040;&#30340;&#26368;&#20248;&#35299;&#25968;&#37327;&#19982;&#39044;&#26399;&#25968;&#37327;&#20043;&#38388;&#30340;&#27604;&#29575;&#65288;&#21363;&#25104;&#21151;&#29575;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;k-BBBC&#22312;&#20855;&#26377;&#22823;&#37327;&#26368;&#20248;&#35299;&#65288;&#27979;&#35797;&#20102;379&#20010;&#26368;&#20248;&#35299;&#65289;&#21644;&#39640;&#32500;&#24230;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal optimization is often encountered in engineering problems, especially when different and alternative solutions are sought. Evolutionary algorithms can efficiently tackle multi-modal optimization thanks to their features such as the concept of population, exploration/exploitation, and being suitable for parallel computation.  This paper introduces a multi-modal optimization version of the Big Bang-Big Crunch algorithm based on clustering, namely, k-BBBC. This algorithm guarantees a complete convergence of the entire population, retrieving on average the 99\% of local optima for a specific problem. Additionally, we introduce two post-processing methods to (i) identify the local optima in a set of retrieved solutions (i.e., a population), and (ii) quantify the number of correctly retrieved optima against the expected ones (i.e., success rate).  Our results show that k-BBBC performs well even with problems having a large number of optima (tested on 379 optima) and high dimensio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#29992;&#20110;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#20013;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#25351;&#23548;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2401.05426</link><description>&lt;p&gt;
CoSS&#65306;&#38024;&#23545;&#25968;&#25454;&#39640;&#25928;AI&#30340;&#20256;&#24863;&#22120;&#21644;&#37319;&#26679;&#29575;&#20248;&#21270;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in Human Activity Recognition. (arXiv:2401.05426v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#29992;&#20110;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#20013;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#25351;&#23548;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#30340;&#36827;&#27493;&#26174;&#33879;&#25552;&#39640;&#20102;&#21033;&#29992;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#20351;&#29992;&#22823;&#37327;&#20256;&#24863;&#22120;&#21644;&#39640;&#37319;&#26679;&#29575;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#25968;&#25454;&#20302;&#25928;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#24517;&#35201;&#25193;&#23637;&#65292;&#32473;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;HAR&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#21033;&#29992;&#65292;&#21516;&#26102;&#32771;&#34385;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#35774;&#35745;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#31216;&#20026;&#8220;&#26435;&#37325;&#20998;&#25968;&#8221;&#65292;&#23427;&#20204;&#35780;&#20272;&#35757;&#32451;&#38454;&#27573;&#20013;&#27599;&#20010;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#20998;&#25968;&#25351;&#23548;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#36873;&#25321;&#12290;&#20462;&#21098;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#22312;&#35745;&#31639;&#39044;&#31639;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#26681;&#25454;&#36873;&#25321;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Artificial Neural Networks have significantly improved human activity recognition using multiple time-series sensors. While employing numerous sensors with high-frequency sampling rates usually improves the results, it often leads to data inefficiency and unnecessary expansion of the ANN, posing a challenge for their practical deployment on edge devices. Addressing these issues, our work introduces a pragmatic framework for data-efficient utilization in HAR tasks, considering the optimization of both sensor modalities and sampling rate simultaneously. Central to our approach are the designed trainable parameters, termed 'Weight Scores,' which assess the significance of each sensor modality and sampling rate during the training phase. These scores guide the sensor modalities and sampling rate selection. The pruning method allows users to make a trade-off between computational budgets and performance by selecting the sensor modalities and sampling rates according t
&lt;/p&gt;</description></item><item><title>&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.17335</link><description>&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Graph Generators. (arXiv:2309.17335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17335
&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;AGG&#23558;&#35266;&#27979;&#20540;&#24314;&#27169;&#20026;&#21160;&#24577;&#22270;&#19978;&#30340;&#33410;&#28857;&#65292;&#24182;&#36890;&#36807;&#36716;&#23548;&#24335;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#12290;AGG&#19981;&#20381;&#36182;&#20110;&#24490;&#29615;&#32452;&#20214;&#25110;&#23545;&#26102;&#38388;&#35268;&#24459;&#30340;&#20551;&#35774;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#23558;&#27979;&#37327;&#20540;&#12289;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#30452;&#25509;&#34920;&#31034;&#22312;&#33410;&#28857;&#20013;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#26679;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#38544;&#24335;&#22320;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#22522;&#20110;&#26410;&#35265;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#23545;&#26032;&#30340;&#27979;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;AGG&#22312;&#27010;&#24565;&#21644;&#23454;&#35777;&#20004;&#26041;&#38754;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#31616;&#35201;&#35752;&#35770;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;AGG&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AGG&#22312;t
&lt;/p&gt;
&lt;p&gt;
We introduce the asynchronous graph generator (AGG), a novel graph neural network architecture for multi-channel time series which models observations as nodes on a dynamic graph and can thus perform data imputation by transductive node generation. Completely free from recurrent components or assumptions about temporal regularity, AGG represents measurements, timestamps and metadata directly in the nodes via learnable embeddings, to then leverage attention to learn expressive relationships across the variables of interest. This way, the proposed architecture implicitly learns a causal graph representation of sensor measurements which can be conditioned on unseen timestamps and metadata to predict new measurements by an expansion of the learnt graph. The proposed AGG is compared both conceptually and empirically to previous work, and the impact of data augmentation on the performance of AGG is also briefly discussed. Our experiments reveal that AGG achieved state-of-the-art results in t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#24191;&#20041;&#31163;&#32676;&#25925;&#38556;&#35786;&#26029;&#65288;GOOFD&#65289;&#26694;&#26550;&#30340;&#38598;&#25104;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#25925;&#38556;&#26816;&#27979;&#12289;&#25925;&#38556;&#20998;&#31867;&#21644;&#26032;&#39062;&#25925;&#38556;&#35786;&#26029;&#31561;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#20869;&#37096;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#26041;&#27861;&#25552;&#21462;&#29305;&#24449;&#24182;&#35782;&#21035;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.15266</link><description>&lt;p&gt;
&#20869;&#37096;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#24191;&#20041;&#31163;&#32676;&#25925;&#38556;&#35786;&#26029;&#65288;GOOFD&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Internal Contrastive Learning for Generalized Out-of-distribution Fault Diagnosis (GOOFD) Framework. (arXiv:2306.15266v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15266
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#24191;&#20041;&#31163;&#32676;&#25925;&#38556;&#35786;&#26029;&#65288;GOOFD&#65289;&#26694;&#26550;&#30340;&#38598;&#25104;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#25925;&#38556;&#26816;&#27979;&#12289;&#25925;&#38556;&#20998;&#31867;&#21644;&#26032;&#39062;&#25925;&#38556;&#35786;&#26029;&#31561;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#20869;&#37096;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#26041;&#27861;&#25552;&#21462;&#29305;&#24449;&#24182;&#35782;&#21035;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#35786;&#26029;&#23545;&#20110;&#24037;&#19994;&#36807;&#31243;&#20013;&#30417;&#27979;&#37325;&#35201;&#26426;&#22120;&#30340;&#29366;&#24577;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#24037;&#20316;&#26465;&#20214;&#30340;&#22797;&#26434;&#21270;&#21644;&#29983;&#20135;&#36816;&#33829;&#36807;&#31243;&#20013;&#23545;&#23433;&#20840;&#30340;&#35201;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#35786;&#26029;&#26041;&#27861;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#24212;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#38598;&#25104;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#35786;&#26029;&#23376;&#20219;&#21153;&#36890;&#24120;&#34987;&#20998;&#24320;&#30740;&#31350;&#65292;&#32780;&#24403;&#21069;&#21487;&#29992;&#30340;&#26041;&#27861;&#22312;&#36825;&#26679;&#19968;&#20010;&#24191;&#20041;&#31995;&#32479;&#19978;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#31163;&#32676;&#25925;&#38556;&#35786;&#26029;&#65288;GOOFD&#65289;&#26694;&#26550;&#65292;&#20197;&#25972;&#21512;&#25925;&#38556;&#26816;&#27979;&#12289;&#25925;&#38556;&#20998;&#31867;&#21644;&#26032;&#39062;&#25925;&#38556;&#35786;&#26029;&#31561;&#35786;&#26029;&#23376;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#23545;&#27604;&#23398;&#20064;&#30340;&#32479;&#19968;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#20316;&#20026;&#35813;&#24191;&#20041;&#26694;&#26550;&#30340;&#22522;&#30784;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20869;&#37096;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#35782;&#21035;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fault diagnosis is essential in industrial processes for monitoring the conditions of important machines. With the ever-increasing complexity of working conditions and demand for safety during production and operation, different diagnosis methods are required, and more importantly, an integrated fault diagnosis system that can cope with multiple tasks is highly desired. However, the diagnosis subtasks are often studied separately, and the currently available methods still need improvement for such a generalized system. To address this issue, we propose the Generalized Out-of-distribution Fault Diagnosis (GOOFD) framework to integrate diagnosis subtasks, such as fault detection, fault classification, and novel fault diagnosis. Additionally, a unified fault diagnosis method based on internal contrastive learning is put forward to underpin the proposed generalized framework. The method extracts features utilizing the internal contrastive learning technique and then recognizes the outliers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.02766</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#32593;&#32476;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Networked Communication for Decentralised Agents in Mean-Field Games. (arXiv:2306.02766v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#32593;&#32476;&#36890;&#20449;&#24341;&#20837;&#22343;&#22330;&#21338;&#24328;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;oracle&#30340;&#24773;&#20917;&#19979;&#65292;N&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#27839;&#30528;&#32463;&#36807;&#30340;&#32463;&#39564;&#31995;&#32479;&#30340;&#21333;&#19968;&#38750;&#21608;&#26399;&#28436;&#21270;&#36335;&#24452;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#21482;&#26377;&#19968;&#20123;&#20851;&#20110;&#32593;&#32476;&#32467;&#26500;&#30340;&#21512;&#29702;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26679;&#26412;&#20445;&#35777;&#65292;&#22312;&#38598;&#20013;&#23398;&#20064;&#21644;&#29420;&#31435;&#23398;&#20064;&#24773;&#20917;&#20043;&#38388;&#26377;&#30028;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#30340;&#26679;&#26412;&#20445;&#35777;&#23454;&#38469;&#19978;&#24182;&#19981;&#20250;&#23548;&#33268;&#23454;&#38469;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#24403;&#29702;&#35770;&#21442;&#25968;&#26410;&#34987;&#35266;&#23519;&#21040;&#65288;&#23548;&#33268;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#19981;&#20934;&#30830;&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#36890;&#20449;&#26041;&#26696;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#19968;&#20010;&#19981;&#21487;&#21462;&#30340;&#38598;&#20013;&#24335;&#25511;&#21046;&#22120;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#36827;&#34892;&#20102;&#20960;&#31181;&#23454;&#38469;&#30340;&#25913;&#36827;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#23427;&#20204;&#30340;&#31532;&#19968;&#20010;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic evolution path of the empirical system. We prove that our architecture, with only a few reasonable assumptions about network structure, has sample guarantees bounded between those of the centralised- and independent-learning cases. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. Accordingly, we show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case, without relying on the undesirable assumption of a centralised controller. We contribute several further practical enhancements to all three theoretical algorithms, allowing us to showcase their first empirical demonstrations. Our expe
&lt;/p&gt;</description></item><item><title>OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.04203</link><description>&lt;p&gt;
OpenDriver: &#19968;&#20221;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenDriver: an open-road driver state detection dataset. (arXiv:2304.04203v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04203
&lt;/p&gt;
&lt;p&gt;
OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#65292;&#36947;&#36335;&#23433;&#20840;&#20005;&#37325;&#20381;&#36182;&#20110;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#21644;&#29983;&#29702;&#29366;&#24577;&#12290;&#30130;&#21171;&#12289;&#26127;&#26127;&#27442;&#30561;&#21644;&#21387;&#21147;&#31561;&#36127;&#38754;&#22240;&#32032;&#20250;&#24433;&#21709;&#39550;&#39542;&#21592;&#30340;&#21453;&#24212;&#26102;&#38388;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#30340;&#21457;&#29983;&#29575;&#22686;&#21152;&#12290;&#22312;&#20247;&#22810;&#30340;&#39550;&#39542;&#21592;&#34892;&#20026;&#30417;&#27979;&#30740;&#31350;&#20013;&#65292;&#21487;&#31359;&#25140;&#29983;&#29702;&#27979;&#37327;&#26159;&#19968;&#31181;&#23454;&#26102;&#30417;&#27979;&#39550;&#39542;&#21592;&#29366;&#24577;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#24320;&#25918;&#36947;&#36335;&#22330;&#26223;&#19979;&#65292;&#32570;&#23569;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#65292;&#24050;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#20449;&#21495;&#36136;&#37327;&#24046;&#12289;&#26679;&#26412;&#37327;&#23567;&#21644;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#30701;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35774;&#35745;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#31181;&#39550;&#39542;&#20449;&#21495;&#27169;&#24577;&#65306;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#26159;&#22312;100&#22810;&#21517;&#39550;&#39542;&#21592;&#36981;&#24490;&#30456;&#21516;&#36335;&#32447;&#34892;&#39542;&#26102;&#35760;&#24405;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern society, road safety relies heavily on the psychological and physiological state of drivers. Negative factors such as fatigue, drowsiness, and stress can impair drivers' reaction time and decision making abilities, leading to an increased incidence of traffic accidents. Among the numerous studies for impaired driving detection, wearable physiological measurement is a real-time approach to monitoring a driver's state. However, currently, there are few driver physiological datasets in open road scenarios and the existing datasets suffer from issues such as poor signal quality, small sample sizes, and short data collection periods. Therefore, in this paper, a large-scale multimodal driving dataset for driver impairment detection and biometric data recognition is designed and described. The dataset contains two modalities of driving signals: six-axis inertial signals and electrocardiogram (ECG) signals, which were recorded while over one hundred drivers were following the same ro
&lt;/p&gt;</description></item><item><title>HiNet&#26159;&#19968;&#31181;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#32593;&#32476;&#65292;&#20855;&#26377;&#23618;&#27425;&#20449;&#24687;&#25552;&#21462;&#21644;&#22330;&#26223;&#24863;&#30693;&#27880;&#24847;&#65292;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06095</link><description>&lt;p&gt;
HiNet: &#19968;&#31181;&#20855;&#26377;&#23618;&#27425;&#20449;&#24687;&#25552;&#21462;&#30340;&#26032;&#22411;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HiNet: Novel Multi-Scenario &amp; Multi-Task Learning with Hierarchical Information Extraction. (arXiv:2303.06095v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06095
&lt;/p&gt;
&lt;p&gt;
HiNet&#26159;&#19968;&#31181;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#32593;&#32476;&#65292;&#20855;&#26377;&#23618;&#27425;&#20449;&#24687;&#25552;&#21462;&#21644;&#22330;&#26223;&#24863;&#30693;&#27880;&#24847;&#65292;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#24212;&#29992;&#30340;&#35768;&#22810;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#20013;&#19968;&#31181;&#26377;&#25928;&#21644;&#23454;&#29992;&#30340;&#26041;&#27861;&#26159;&#22312;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#22810;&#22330;&#26223;&#36801;&#31227;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20197;&#23558;&#25152;&#26377;&#20449;&#24687;&#25237;&#24433;&#21040;&#21516;&#19968;&#29305;&#24449;&#31354;&#38388;&#20026;&#30446;&#26631;&#30340;MoE&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#22330;&#26223;&#21644;&#20219;&#21153;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#22914;&#20154;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#25512;&#33616;&#30340;Hierarchical information extraction Network&#65288;HiNet&#65289;&#65292;&#35813;&#32593;&#32476;&#22522;&#20110;&#20174;&#31895;&#21040;&#32454;&#30340;&#30693;&#35782;&#36716;&#31227;&#26041;&#26696;&#23454;&#29616;&#20998;&#23618;&#25552;&#21462;&#12290;&#20998;&#23618;&#32593;&#32476;&#30340;&#22810;&#20010;&#25552;&#21462;&#23618;&#20351;&#27169;&#22411;&#33021;&#22815;&#22686;&#24378;&#36328;&#22330;&#26223;&#20256;&#36882;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#22330;&#26223;&#21644;&#20219;&#21153;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#22330;&#26223;&#24863;&#30693;&#27880;&#24847;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#20219;&#21153;&#30456;&#20851;&#24615;&#21644;&#22330;&#26223;&#29420;&#29305;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;HiNet&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-scenario &amp; multi-task learning has been widely applied to many recommendation systems in industrial applications, wherein an effective and practical approach is to carry out multi-scenario transfer learning on the basis of the Mixture-of-Expert (MoE) architecture. However, the MoE-based method, which aims to project all information in the same feature space, cannot effectively deal with the complex relationships inherent among various scenarios and tasks, resulting in unsatisfactory performance. To tackle the problem, we propose a Hierarchical information extraction Network (HiNet) for multi-scenario and multi-task recommendation, which achieves hierarchical extraction based on coarse-to-fine knowledge transfer scheme. The multiple extraction layers of the hierarchical network enable the model to enhance the capability of transferring valuable information across scenarios while preserving specific features of scenarios and tasks. Furthermore, a novel scenario-aware attentive netw
&lt;/p&gt;</description></item><item><title>DualStreamFoveaNet&#26159;&#19968;&#31181;&#20855;&#26377;&#35299;&#21078;&#24847;&#35782;&#30340;&#21452;&#27969;&#34701;&#21512;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#32593;&#33180;&#21644;&#34880;&#31649;&#20998;&#24067;&#36827;&#34892;&#22810;&#32447;&#32034;&#34701;&#21512;&#65292;&#23454;&#29616;&#23545;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26550;&#26500;&#22312;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06961</link><description>&lt;p&gt;
DualStreamFoveaNet: &#19968;&#31181;&#20855;&#26377;&#35299;&#21078;&#24847;&#35782;&#30340;&#21452;&#27969;&#34701;&#21512;&#26550;&#26500;&#29992;&#20110;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization. (arXiv:2302.06961v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06961
&lt;/p&gt;
&lt;p&gt;
DualStreamFoveaNet&#26159;&#19968;&#31181;&#20855;&#26377;&#35299;&#21078;&#24847;&#35782;&#30340;&#21452;&#27969;&#34701;&#21512;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#32593;&#33180;&#21644;&#34880;&#31649;&#20998;&#24067;&#36827;&#34892;&#22810;&#32447;&#32034;&#34701;&#21512;&#65292;&#23454;&#29616;&#23545;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26550;&#26500;&#22312;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#23545;&#20110;&#20998;&#26512;&#35270;&#32593;&#33180;&#30142;&#30149;&#20197;&#39044;&#38450;&#19981;&#21487;&#36870;&#35270;&#21147;&#25439;&#22833;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#34429;&#28982;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20294;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#20013;&#22830;&#20985;&#28857;&#21608;&#22260;&#23616;&#37096;&#35299;&#21078;&#26631;&#35760;&#30340;&#32570;&#22833;&#12289;&#19981;&#33021;&#40065;&#26834;&#22320;&#22788;&#29702;&#30149;&#21464;&#35270;&#32593;&#33180;&#22270;&#20687;&#21644;&#22270;&#20687;&#26465;&#20214;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#31216;&#20026;DualStreamFoveaNet (DSFN)&#29992;&#20110;&#22810;&#32447;&#32034;&#34701;&#21512;&#12290;&#35813;&#26550;&#26500;&#26126;&#30830;&#22320;&#21033;&#29992;&#35270;&#32593;&#33180;&#21644;&#34880;&#31649;&#20998;&#24067;&#26469;&#23454;&#29616;&#38271;&#31243;&#36830;&#25509;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#34701;&#21512;&#65292;&#23454;&#29616;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#12290;&#25105;&#20204;&#22312;&#21452;&#27969;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#34701;&#21512;&#33258;&#23398;&#20064;&#30340;&#35299;&#21078;&#20449;&#24687;&#65292;&#26356;&#27880;&#37325;&#20998;&#24067;&#22312;&#34880;&#31649;&#27839;&#32447;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#20196;&#29260;&#25968;&#37327;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate fovea localization is essential for analyzing retinal diseases to prevent irreversible vision loss. While current deep learning-based methods outperform traditional ones, they still face challenges such as the lack of local anatomical landmarks around the fovea, the inability to robustly handle diseased retinal images, and the variations in image conditions. In this paper, we propose a novel transformer-based architecture called DualStreamFoveaNet (DSFN) for multi-cue fusion. This architecture explicitly incorporates long-range connections and global features using retina and vessel distributions for robust fovea localization. We introduce a spatial attention mechanism in the dual-stream encoder to extract and fuse self-learned anatomical information, focusing more on features distributed along blood vessels and significantly reducing computational costs by decreasing token numbers. Our extensive experiments show that the proposed architecture achieves state-of-the-art perform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#29305;&#24449;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#31070;&#35861;&#24182;&#24471;&#21040;&#20102;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/1703.01347</link><description>&lt;p&gt;
&#24102;&#22122;&#22768;&#29305;&#24449;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#65306;&#26397;&#21521;&#36125;&#21494;&#26031;&#31070;&#35861;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles. (arXiv:1703.01347v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1703.01347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#29305;&#24449;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#31070;&#35861;&#24182;&#24471;&#21040;&#20102;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#22122;&#22768;&#21644;&#32570;&#22833;&#39033;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#22122;&#22768;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#35266;&#27979;&#22122;&#22768;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#32473;&#20986;&#30340;&#36125;&#21494;&#26031;&#31070;&#35861;&#12290;&#25105;&#20204;&#30340;&#36125;&#21494;&#26031;&#20998;&#26512;&#21457;&#29616;&#65292;&#26368;&#20248;&#20551;&#35774;&#21487;&#33021;&#20250;&#36828;&#31163;&#28508;&#22312;&#30340;&#21487;&#23454;&#29616;&#20989;&#25968;&#65292;&#36825;&#21462;&#20915;&#20110;&#22122;&#22768;&#29305;&#24449;&#65292;&#36825;&#26159;&#39640;&#24230;&#38750;&#30452;&#35266;&#30340;&#65292;&#24182;&#19988;&#22312;&#32463;&#20856;&#30340;&#26080;&#22122;&#22768;&#35774;&#32622;&#19979;&#19981;&#20250;&#21457;&#29983;&#12290;&#36825;&#24847;&#21619;&#30528;&#32463;&#20856;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#38750;&#24179;&#20961;&#30340;&#36951;&#25022;&#30028;&#65288;regret bound&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#26088;&#22312;&#20174;&#36825;&#20010;&#27169;&#22411;&#19979;&#30340;&#35266;&#23519;&#20449;&#24687;&#20013;&#23454;&#29616;&#36125;&#21494;&#26031;&#31070;&#35861;&#65292;&#24403;&#26377;&#22823;&#37327;&#25163;&#33218;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;$\tilde{O}(d\sqrt{T})$&#36951;&#25022;&#30028;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study contextual linear bandit problems under feature uncertainty; they are noisy with missing entries. To address the challenges of the noise, we analyze Bayesian oracles given observed noisy features. Our Bayesian analysis finds that the optimal hypothesis can be far from the underlying realizability function, depending on the noise characteristics, which are highly non-intuitive and do not occur for classical noiseless setups. This implies that classical approaches cannot guarantee a non-trivial regret bound. Therefore, we propose an algorithm that aims at the Bayesian oracle from observed information under this model, achieving $\tilde{O}(d\sqrt{T})$ regret bound when there is a large number of arms. We demonstrate the proposed algorithm using synthetic and real-world datasets.
&lt;/p&gt;</description></item></channel></rss>