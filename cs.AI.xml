<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#36890;&#36807;&#32508;&#36848;&#36229;&#36234;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#26356;&#28145;&#20837;&#20102;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;LLMs&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#38754;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01869</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;--&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32508;&#36848;&#36229;&#36234;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#26356;&#28145;&#20837;&#20102;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;LLMs&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#38754;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#28041;&#21450;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#28608;&#28872;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#28145;&#24230;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#37096;&#20998;&#28304;&#33258;&#23545;&#27169;&#22411;&#25512;&#29702;&#34892;&#20026;&#30340;&#28145;&#20837;&#35843;&#26597;&#32780;&#38750;&#20165;&#20165;&#36890;&#36807;&#34920;&#38754;&#20934;&#30830;&#24615;&#25351;&#26631;&#26469;&#34913;&#37327;&#20219;&#21153;&#34920;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#32508;&#36848;&#36229;&#36234;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35780;&#20272;LLMs&#25512;&#29702;&#34892;&#20026;&#30340;&#20027;&#35201;&#26041;&#27861;&#35770;&#65292;&#24378;&#35843;&#20102;&#24403;&#21069;&#23545;&#26356;&#32454;&#33268;&#25512;&#29702;&#20998;&#26512;&#30340;&#36235;&#21183;&#21644;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#34920;&#26126;&#65292;LLMs&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#38754;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01869v1 Announce Type: cross  Abstract: Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rat
&lt;/p&gt;</description></item><item><title>CR3DT&#26159;&#19968;&#20010;&#30456;&#26426;&#19982;&#38647;&#36798;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#38647;&#36798;&#22312;3D&#26816;&#27979;&#21644;&#36319;&#36394;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#22312;State-of-the-Art&#30456;&#26426;&#26550;&#26500;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.15313</link><description>&lt;p&gt;
CR3DT&#65306;&#30456;&#26426;&#19982;&#38647;&#36798;&#34701;&#21512;&#29992;&#20110;3D&#26816;&#27979;&#21644;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15313
&lt;/p&gt;
&lt;p&gt;
CR3DT&#26159;&#19968;&#20010;&#30456;&#26426;&#19982;&#38647;&#36798;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#38647;&#36798;&#22312;3D&#26816;&#27979;&#21644;&#36319;&#36394;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#22312;State-of-the-Art&#30456;&#26426;&#26550;&#26500;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#26816;&#27979;&#21644;&#36319;&#36394;&#21608;&#22260;&#29289;&#20307;&#23545;&#20110;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20809;&#25506;&#27979;&#19982;&#27979;&#36317;&#65288;LiDAR&#65289;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#39640;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#20294;&#20165;&#20351;&#29992;&#30456;&#26426;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#20854;&#25104;&#26412;&#25928;&#30410;&#12290;&#23613;&#31649;&#26080;&#32447;&#30005;&#25506;&#27979;&#19982;&#27979;&#36317;&#65288;RADAR&#65289;&#20256;&#24863;&#22120;&#22312;&#27773;&#36710;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#21644;&#27979;&#37327;&#22122;&#22768;&#30340;&#21407;&#22240;&#65292;&#23427;&#20204;&#22312;3D&#26816;&#27979;&#21644;&#36319;&#36394;&#20013;&#30340;&#28508;&#21147;&#38271;&#26399;&#34987;&#24573;&#35270;&#12290;&#20316;&#20026;&#19968;&#20010;&#26368;&#26032;&#30340;&#21457;&#23637;&#65292;&#30456;&#26426;&#19982;&#38647;&#36798;&#30340;&#32467;&#21512;&#27491;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Camera-RADAR 3D Detection and Tracking (CR3DT)&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;3D&#29289;&#20307;&#26816;&#27979;&#21644;&#22810;&#29289;&#20307;&#36319;&#36394;&#30340;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#27169;&#22411;&#12290;&#22312;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#21482;&#26377;&#30456;&#26426;&#30340;BEVDet&#26550;&#26500;&#30340;&#22522;&#30784;&#19978;&#65292;CR3DT&#22312;&#26816;&#27979;&#21644;&#36319;&#36394;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#25972;&#21512;&#38647;&#36798;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15313v1 Announce Type: cross  Abstract: Accurate detection and tracking of surrounding objects is essential to enable self-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high performance, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the sp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#32462;&#24335;&#30340;&#22270;&#35889;&#36777;&#35770;&#26041;&#27861;&#65288;BDoG&#65289;&#65292;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#38450;&#27490;&#24847;&#35265;&#38472;&#33104;&#21270;&#21644;&#20943;&#23569;&#30001;&#22270;&#20687;&#24341;&#20837;&#30340;&#20998;&#24515;&#27010;&#24565;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#31185;&#23398;&#38382;&#31572;&#21644;MMBench&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14972</link><description>&lt;p&gt;
&#19968;&#22270;&#32988;&#21315;&#35328;&#65306;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#22270;&#35889;&#36777;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#32462;&#24335;&#30340;&#22270;&#35889;&#36777;&#35770;&#26041;&#27861;&#65288;BDoG&#65289;&#65292;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#38450;&#27490;&#24847;&#35265;&#38472;&#33104;&#21270;&#21644;&#20943;&#23569;&#30001;&#22270;&#20687;&#24341;&#20837;&#30340;&#20998;&#24515;&#27010;&#24565;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#31185;&#23398;&#38382;&#31572;&#21644;MMBench&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26088;&#22312;&#23558;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#24341;&#20837;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#35797;&#28857;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#30001;&#20110;&#36807;&#24230;&#24635;&#32467;&#32780;&#23548;&#33268;&#24847;&#35265;&#38472;&#33104;&#21270;&#65292;&#20197;&#21450;&#30001;&#20110;&#22270;&#20687;&#24341;&#20837;&#36716;&#31227;&#24615;&#27010;&#24565;&#32780;&#23548;&#33268;&#27880;&#24847;&#21147;&#20998;&#25955;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#33258;&#29616;&#26377;&#36777;&#35770;&#26041;&#26696;&#30340;&#24402;&#32435;&#65288;&#33258;&#19979;&#32780;&#19978;&#65289;&#24615;&#36136;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#32462;&#65288;&#33258;&#19978;&#32780;&#19979;&#65289;&#30340;&#36777;&#35770;&#26041;&#27861;&#65292;&#31216;&#20026;&#22270;&#35889;&#36777;&#35770;&#65288;BDoG&#65289;&#12290;&#22312;BDoG&#20013;&#65292;&#36777;&#35770;&#20165;&#38480;&#20110;&#34013;&#22270;&#22270;&#20013;&#65292;&#20197;&#38450;&#27490;&#36890;&#36807;&#19990;&#30028;&#32423;&#25688;&#35201;&#32780;&#23548;&#33268;&#24847;&#35265;&#38472;&#33104;&#21270;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#22270;&#20013;&#30340;&#20998;&#25903;&#20013;&#23384;&#20648;&#35777;&#25454;&#65292;BDoG&#32531;&#35299;&#20102;&#39057;&#32321;&#20294;&#26080;&#20851;&#30340;&#27010;&#24565;&#24102;&#26469;&#30340;&#20998;&#25955;&#27880;&#24847;&#21147;&#29616;&#35937;&#12290;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;BDoG&#65292;&#22312;&#31185;&#23398;&#38382;&#31572;&#21644;MMBench&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#65292;&#24182;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14972v1 Announce Type: new  Abstract: This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivialization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate BDoG, achieving state-of-the-art results in Science QA and MMBench with significant improvements over previous methods.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#27454;&#21517;&#20026;ClarifAI&#30340;&#33258;&#21160;&#21270;&#23459;&#20256;&#26816;&#27979;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#23459;&#20256;&#24182;&#25552;&#20379;&#20016;&#23500;&#35299;&#37322;&#65292;&#20197;&#28608;&#21457;&#26356;&#22810;&#25209;&#21028;&#24615;&#38405;&#35835;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#24378;&#35843;&#20102;&#35299;&#37322;&#23545;&#20110;&#22521;&#20859;&#25209;&#21028;&#24615;&#24605;&#32500;&#30340;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2402.19135</link><description>&lt;p&gt;
&#24555;&#36895;&#24605;&#32771;&#65292;&#24930;&#36895;&#24605;&#32771;&#65292;&#25209;&#21028;&#24615;&#24605;&#32771;&#65306;&#35774;&#35745;&#19968;&#27454;&#33258;&#21160;&#21270;&#30123;&#24773;&#26816;&#27979;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19135
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#27454;&#21517;&#20026;ClarifAI&#30340;&#33258;&#21160;&#21270;&#23459;&#20256;&#26816;&#27979;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#23459;&#20256;&#24182;&#25552;&#20379;&#20016;&#23500;&#35299;&#37322;&#65292;&#20197;&#28608;&#21457;&#26356;&#22810;&#25209;&#21028;&#24615;&#38405;&#35835;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#24378;&#35843;&#20102;&#35299;&#37322;&#23545;&#20110;&#22521;&#20859;&#25209;&#21028;&#24615;&#24605;&#32500;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#65292;&#24555;&#36895;&#30340;&#26032;&#38395;&#28040;&#36153;&#21644;&#26085;&#30410;&#23545;&#23459;&#20256;&#30340;&#33030;&#24369;&#24615;&#25104;&#20026;&#29305;&#28857;&#65292;&#22521;&#20859;&#20844;&#27665;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#23545;&#20110;&#31283;&#23450;&#30340;&#27665;&#20027;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ClarifAI&#30340;&#35774;&#35745;&#65292;&#36825;&#26159;&#19968;&#27454;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#23459;&#20256;&#26816;&#27979;&#24037;&#20855;&#65292;&#26088;&#22312;&#36890;&#36807;&#28608;&#27963;&#20998;&#26512;&#24615;&#24605;&#32500;&#27169;&#24335;&#65292;&#36981;&#24490;&#24247;&#26364;&#30340;&#35748;&#30693;&#21452;&#31995;&#32479;&#29702;&#35770;&#65292;&#25512;&#21160;&#35835;&#32773;&#26356;&#21152;&#25209;&#21028;&#24615;&#22320;&#28040;&#36153;&#26032;&#38395;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;ClarifAI&#21487;&#20197;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#23459;&#20256;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#32972;&#26223;&#35299;&#37322;&#65292;&#22686;&#24378;&#29992;&#25143;&#30340;&#29702;&#35299;&#21644;&#25209;&#21028;&#24615;&#24605;&#32500;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ClarifAI&#30340;&#35774;&#35745;&#65307;&#20854;&#27425;&#65292;&#22312;&#19968;&#39033;&#22312;&#32447;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#35774;&#35745;&#26377;&#25928;&#22320;&#40723;&#21169;&#26032;&#38395;&#35835;&#32773;&#26356;&#22810;&#22320;&#36827;&#34892;&#25209;&#21028;&#24615;&#38405;&#35835;&#65307;&#31532;&#19977;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#35299;&#37322;&#23545;&#20110;&#22521;&#20859;&#25209;&#21028;&#24615;&#24605;&#32500;&#30340;&#20215;&#20540;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26082;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#24037;&#20855;&#65292;&#21448;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#25903;&#25745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19135v1 Announce Type: cross  Abstract: In today's digital age, characterized by rapid news consumption and increasing vulnerability to propaganda, fostering citizens' critical thinking is crucial for stable democracies. This paper introduces the design of ClarifAI, a novel automated propaganda detection tool designed to nudge readers towards more critical news consumption by activating the analytical mode of thinking, following Kahneman's dual-system theory of cognition. Using Large Language Models, ClarifAI detects propaganda in news articles and provides context-rich explanations, enhancing users' understanding and critical thinking. Our contribution is threefold: first, we propose the design of ClarifAI; second, in an online experiment, we demonstrate that this design effectively encourages news readers to engage in more critical reading; and third, we emphasize the value of explanations for fostering critical thinking. The study thus offers both a practical tool and use
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22996;&#25176;&#28216;&#25103;&#20013;&#25506;&#35752;&#20102;&#25511;&#21046;&#38382;&#39064;&#65288;&#20195;&#29702;&#20154;&#26410;&#33021;&#25353;&#29031;&#20854;&#22996;&#25176;&#20154;&#30340;&#20559;&#22909;&#34892;&#20107;&#65289;&#21644;&#21512;&#20316;&#38382;&#39064;&#65288;&#20195;&#29702;&#20154;&#26410;&#33021;&#33391;&#22909;&#22320;&#21327;&#20316;&#65289;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#40784;&#21644;&#33021;&#21147;&#23545;&#22996;&#25176;&#20154;&#31119;&#21033;&#30340;&#24433;&#21709;&#12290;en_tdlr: This paper explores the issues of control (agents failing to act in line with their principals' preferences) and cooperation (agents failing to work well together) in delegation games, analyzing how alignment and capabilities impact principals' welfare.</title><link>https://arxiv.org/abs/2402.15821</link><description>&lt;p&gt;
&#22996;&#25176;&#28216;&#25103;&#20013;&#30340;&#21512;&#20316;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Cooperation and Control in Delegation Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22996;&#25176;&#28216;&#25103;&#20013;&#25506;&#35752;&#20102;&#25511;&#21046;&#38382;&#39064;&#65288;&#20195;&#29702;&#20154;&#26410;&#33021;&#25353;&#29031;&#20854;&#22996;&#25176;&#20154;&#30340;&#20559;&#22909;&#34892;&#20107;&#65289;&#21644;&#21512;&#20316;&#38382;&#39064;&#65288;&#20195;&#29702;&#20154;&#26410;&#33021;&#33391;&#22909;&#22320;&#21327;&#20316;&#65289;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#40784;&#21644;&#33021;&#21147;&#23545;&#22996;&#25176;&#20154;&#31119;&#21033;&#30340;&#24433;&#21709;&#12290;en_tdlr: This paper explores the issues of control (agents failing to act in line with their principals' preferences) and cooperation (agents failing to work well together) in delegation games, analyzing how alignment and capabilities impact principals' welfare.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28041;&#21450;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#24863;&#20852;&#36259;&#30340;&#22330;&#26223; - &#20174;&#34394;&#25311;&#20010;&#20154;&#21161;&#29702;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742; - &#21487;&#20197;&#33258;&#28982;&#22320;&#24314;&#27169;&#20026;&#22996;&#25176;&#20154;&#65288;&#20154;&#31867;&#65289;&#22996;&#25176;&#32473;&#20195;&#29702;&#20154;&#65288;&#26426;&#22120;&#65289;&#65292;&#36825;&#20123;&#20195;&#29702;&#20154;&#20043;&#21518;&#20195;&#34920;&#20182;&#20204;&#30340;&#22996;&#25176;&#20154;&#30456;&#20114;&#20132;&#20114;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#22810;&#22996;&#25176;&#20154;&#65292;&#22810;&#20195;&#29702;&#20154;&#30340;&#24773;&#20917;&#31216;&#20026;&#22996;&#25176;&#28216;&#25103;&#12290;&#22312;&#36825;&#31867;&#28216;&#25103;&#20013;&#65292;&#23384;&#22312;&#20004;&#31181;&#37325;&#35201;&#30340;&#22833;&#36133;&#27169;&#24335;&#65306;&#25511;&#21046;&#38382;&#39064;&#65288;&#20195;&#29702;&#20154;&#26410;&#33021;&#25353;&#29031;&#20854;&#22996;&#25176;&#20154;&#30340;&#20559;&#22909;&#34892;&#20107;&#65289;&#21644;&#21512;&#20316;&#38382;&#39064;&#65288;&#20195;&#29702;&#20154;&#26410;&#33021;&#33391;&#22909;&#22320;&#21327;&#20316;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#21644;&#20998;&#26512;&#36825;&#20123;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23558;&#20854;&#35299;&#37322;&#20026;&#23545;&#40784;&#65288;&#21442;&#19982;&#32773;&#26159;&#21542;&#20855;&#26377;&#30456;&#20284;&#30340;&#20559;&#22909;&#65311;&#65289;&#21644;&#33021;&#21147;&#65288;&#21442;&#19982;&#32773;&#22312;&#28385;&#36275;&#36825;&#20123;&#20559;&#22909;&#26041;&#38754;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#36825;&#20123;&#25514;&#26045;&#22914;&#20309;&#30830;&#23450;&#22996;&#25176;&#20154;&#30340;&#31119;&#21033;&#65292;&#22914;&#20309;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#30340;&#35266;&#23519;&#26469;&#20272;&#35745;&#36825;&#20123;&#25514;&#26045;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15821v1 Announce Type: cross  Abstract: Many settings of interest involving humans and machines -- from virtual personal assistants to autonomous vehicles -- can naturally be modelled as principals (humans) delegating to agents (machines), which then interact with each other on their principals' behalf. We refer to these multi-principal, multi-agent scenarios as delegation games. In such games, there are two important failure modes: problems of control (where an agent fails to act in line their principal's preferences) and problems of cooperation (where the agents fail to work well together). In this paper we formalise and analyse these problems, further breaking them down into issues of alignment (do the players have similar preferences?) and capabilities (how competent are the players at satisfying those preferences?). We show -- theoretically and empirically -- how these measures determine the principals' welfare, how they can be estimated using limited observations, and 
&lt;/p&gt;</description></item><item><title>GAOKAO-MM &#26159;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20154;&#31867;&#27700;&#24179;&#35201;&#27714;&#65292;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#30340;LVLMs&#30340;&#20934;&#30830;&#29575;&#26222;&#36941;&#19981;&#36275;50%&#12290;</title><link>https://arxiv.org/abs/2402.15745</link><description>&lt;p&gt;
GAOKAO-MM: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#30340;&#20013;&#22269;&#20154;&#31867;&#27700;&#24179;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15745
&lt;/p&gt;
&lt;p&gt;
GAOKAO-MM &#26159;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20154;&#31867;&#27700;&#24179;&#35201;&#27714;&#65292;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#30340;LVLMs&#30340;&#20934;&#30830;&#29575;&#26222;&#36941;&#19981;&#36275;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#24050;&#32463;&#22312;&#22270;&#20687;&#24863;&#30693;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26497;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#22522;&#26412;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#36825;&#20123;&#26080;&#27861;&#20805;&#20998;&#21453;&#26144;&#20986;LVLMs&#30340;&#20840;&#38754;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GAOKAO-MM&#65292;&#19968;&#20010;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#21253;&#25324;8&#20010;&#31185;&#30446;&#21644;12&#31181;&#31867;&#22411;&#30340;&#22270;&#29255;&#65292;&#22914;&#22270;&#34920;&#12289;&#20989;&#25968;&#22270;&#12289;&#22320;&#22270;&#21644;&#29031;&#29255;&#12290;GAOKAO-MM&#26469;&#28304;&#20110;&#20013;&#22269;&#26412;&#22303;&#32972;&#26223;&#65292;&#24182;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#35201;&#27714;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#29702;&#35299;&#12289;&#30693;&#35782;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;10&#20010;LVLMs&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#20934;&#30830;&#29575;&#37117;&#20302;&#20110;50%&#65292;&#20854;&#20013;GPT-4-Vision&#65288;48.1%&#65289;&#12289;Qwen-VL-Plus&#65288;41.2%&#65289;&#21644;Gemini-Pro-Vision&#65288;35.1%&#65289;&#20301;&#21015;&#21069;&#19977;&#21517;&#12290;&#25105;&#20204;&#30340;&#22810;&#32500;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;LVLMs&#20855;&#26377;&#36866;&#24230;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15745v1 Announce Type: cross  Abstract: The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs have moder
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#20851;&#27880;&#20102;&#22312;&#31579;&#36873;&#21644;&#25552;&#21462;&#38454;&#27573;&#30340;&#21322;&#33258;&#21160;&#21270;&#36807;&#31243;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#21253;&#25324;&#20256;&#32479;&#29305;&#24449;&#21644;&#20154;&#24037;&#26234;&#33021;&#29305;&#24449;&#30340;&#26694;&#26550;&#26469;&#32771;&#23519;21&#20010;&#39046;&#20808;&#30340;&#25991;&#29486;&#32508;&#36848;&#24037;&#20855;&#65292;&#24182;&#20998;&#26512;&#20102;11&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#29486;&#25628;&#32034;&#21644;&#23398;&#26415;&#20889;&#20316;&#36741;&#21161;&#30340;&#26368;&#26032;&#24037;&#20855;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#36235;&#21183;&#12289;&#20027;&#35201;&#30740;&#31350;&#25361;&#25112;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.08565</link><description>&lt;p&gt;
&#25991;&#29486;&#32508;&#36848;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Literature Reviews: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08565
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#20851;&#27880;&#20102;&#22312;&#31579;&#36873;&#21644;&#25552;&#21462;&#38454;&#27573;&#30340;&#21322;&#33258;&#21160;&#21270;&#36807;&#31243;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#21253;&#25324;&#20256;&#32479;&#29305;&#24449;&#21644;&#20154;&#24037;&#26234;&#33021;&#29305;&#24449;&#30340;&#26694;&#26550;&#26469;&#32771;&#23519;21&#20010;&#39046;&#20808;&#30340;&#25991;&#29486;&#32508;&#36848;&#24037;&#20855;&#65292;&#24182;&#20998;&#26512;&#20102;11&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#29486;&#25628;&#32034;&#21644;&#23398;&#26415;&#20889;&#20316;&#36741;&#21161;&#30340;&#26368;&#26032;&#24037;&#20855;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#36235;&#21183;&#12289;&#20027;&#35201;&#30740;&#31350;&#25361;&#25112;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20154;&#24037;&#26234;&#33021;&#22312;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65288;SLR&#65289;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;SLR&#26159;&#19968;&#31181;&#20005;&#35880;&#26377;&#24207;&#30340;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#25972;&#21512;&#20851;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#20808;&#21069;&#30740;&#31350;&#12290;&#35768;&#22810;&#24037;&#20855;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#36741;&#21161;&#21644;&#37096;&#20998;&#33258;&#21160;&#21270;SLR&#36807;&#31243;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#26085;&#30410;&#37325;&#35201;&#35282;&#33394;&#26174;&#31034;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26356;&#26377;&#25928;&#25903;&#25345;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#26397;&#30528;&#25991;&#29486;&#32508;&#36848;&#30340;&#21322;&#33258;&#21160;&#21270;&#21019;&#24314;&#26041;&#21521;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;SLR&#30340;&#21322;&#33258;&#21160;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#31579;&#36873;&#21644;&#25552;&#21462;&#38454;&#27573;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#23558;23&#20010;&#20256;&#32479;&#29305;&#24449;&#19982;11&#20010;&#20154;&#24037;&#26234;&#33021;&#29305;&#24449;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23545;21&#20010;&#39046;&#20808;&#30340;SLR&#24037;&#20855;&#36827;&#34892;&#20102;&#32771;&#23519;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#29486;&#25628;&#32034;&#21644;&#36741;&#21161;&#23398;&#26415;&#20889;&#20316;&#30340;11&#20010;&#26368;&#26032;&#24037;&#20855;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#36235;&#21183;&#65292;&#27010;&#36848;&#20102;&#20027;&#35201;&#30340;&#30740;&#31350;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates previous research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#23569;&#37327;&#26597;&#35810;&#23545;&#25552;&#21462;&#8220;&#23433;&#20840;&#27169;&#24335;&#8221;&#65292;&#25104;&#21151;&#35268;&#36991;&#30446;&#26631;&#27169;&#22411;&#30340;&#38450;&#24481;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36234;&#29425;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.06824</link><description>&lt;p&gt;
&#25171;&#24320;LLMs&#30340;&#28504;&#22810;&#25289;&#39764;&#30418;&#65306;&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06824
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#23569;&#37327;&#26597;&#35810;&#23545;&#25552;&#21462;&#8220;&#23433;&#20840;&#27169;&#24335;&#8221;&#65292;&#25104;&#21151;&#35268;&#36991;&#30446;&#26631;&#27169;&#22411;&#30340;&#38450;&#24481;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36234;&#29425;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#29425;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#35825;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#23545;&#24694;&#24847;&#26597;&#35810;&#20135;&#29983;&#26377;&#27602;&#21709;&#24212;&#65292;&#26469;&#25506;&#32034;LLMs&#23433;&#20840;&#24615;&#36793;&#30028;&#65292;&#36825;&#22312;LLMs&#31038;&#21306;&#20869;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#65288;Jailbreaking LLMs through Representation Engineering&#65292;JRE&#65289;&#30340;&#26032;&#39062;&#36234;&#29425;&#26041;&#27861;&#65292;&#20854;&#20165;&#38656;&#35201;&#23569;&#37327;&#26597;&#35810;&#23545;&#20197;&#25552;&#21462;&#21487;&#29992;&#20110;&#35268;&#36991;&#30446;&#26631;&#27169;&#22411;&#38450;&#24481;&#30340;&#8220;&#23433;&#20840;&#27169;&#24335;&#8221;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36234;&#29425;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06824v2 Announce Type: replace-cross  Abstract: Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community. While existing jailbreaking methods primarily rely on prompt engineering, altering inputs to evade LLM safety mechanisms, they suffer from low attack success rates and significant time overheads, rendering them inflexible. To overcome these limitations, we propose a novel jailbreaking approach, named Jailbreaking LLMs through Representation Engineering (JRE). Our method requires only a small number of query pairs to extract ``safety patterns'' that can be used to circumvent the target model's defenses, achieving unprecedented jailbreaking performance. Building upon these findings, we also introduce a novel defense framework inspired by JRE principles, which demonstrates notable effectiveness. Extensive experimentation conf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#20013;&#25968;&#25454;&#25277;&#35937;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;Small and Incomplete Dataset Analyser (SaNDA)&#37319;&#29992;ROC&#26354;&#32447;&#26041;&#27861;&#24320;&#21457;&#30340;&#25968;&#25454;&#25277;&#35937;&#21327;&#35758;&#65292;&#35813;&#26041;&#27861;&#22312;&#32570;&#23569;&#20540;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25104;&#20026;&#38543;&#26426;&#26862;&#26519;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#65292;&#22987;&#32456;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11044</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#20013;&#25968;&#25454;&#25277;&#35937;&#26041;&#27861;&#22312;&#20851;&#38190;&#20915;&#31574;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Significance of Data Abstraction Methods in Machine Learning Classification Processes for Critical Decision-Making. (arXiv:2401.11044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#20013;&#25968;&#25454;&#25277;&#35937;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;Small and Incomplete Dataset Analyser (SaNDA)&#37319;&#29992;ROC&#26354;&#32447;&#26041;&#27861;&#24320;&#21457;&#30340;&#25968;&#25454;&#25277;&#35937;&#21327;&#35758;&#65292;&#35813;&#26041;&#27861;&#22312;&#32570;&#23569;&#20540;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25104;&#20026;&#38543;&#26426;&#26862;&#26519;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#65292;&#22987;&#32456;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#37319;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#26041;&#27861;&#22312;&#20998;&#31867;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#20102;&#35299;&#37322;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#34892;&#20026;&#31185;&#23398;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#65292;&#20854;&#20013;&#36131;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;Small and Incomplete Dataset Analyser (SaNDA)&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;ROC&#26354;&#32447;&#30340;&#26041;&#27861;&#24320;&#21457;&#25968;&#25454;&#25277;&#35937;&#21327;&#35758;&#65292;&#20197;&#22686;&#24378;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#25191;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#21015;&#38388;&#25968;&#25454;&#36716;&#25442;&#65292;&#21363;&#25277;&#35937;&#65292;&#36825;&#23545;SaNDA&#30340;&#20998;&#31867;&#36807;&#31243;&#38750;&#24120;&#20851;&#38190;&#65292;&#24182;&#25506;&#35752;&#20102;&#26367;&#20195;&#30340;&#25277;&#35937;&#21327;&#35758;&#65292;&#22914;&#24120;&#37327;&#20998;&#31665;&#21644;&#20998;&#20301;&#25968;&#12290;&#23558;&#26368;&#20339;&#30340;&#26041;&#27861;&#19982;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#22522;&#20934;&#27169;&#22411;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#25968;&#25454;&#19981;&#23436;&#25972;&#65292;SaNDA&#22312;&#32570;&#23569;&#20540;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#21487;&#20197;&#25104;&#20026;&#38543;&#26426;&#26862;&#26519;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#65292;&#24182;&#19988;&#22987;&#32456;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The applicability of widely adopted machine learning (ML) methods to classification is circumscribed by the imperatives of explicability and uncertainty, particularly evident in domains such as healthcare, behavioural sciences, and finances, wherein accountability assumes priority. Recently, Small and Incomplete Dataset Analyser (SaNDA) has been proposed to enhance the ability to perform classification in such domains, by developing a data abstraction protocol using a ROC curve-based method. This paper focuses on column-wise data transformations called abstractions, which are crucial for SaNDA's classification process and explores alternative abstractions protocols, such as constant binning and quantiles. The best-performing methods have been compared against Random Forest as a baseline for explainable methods. The results suggests that SaNDA can be a viable substitute for Random Forest when data is incomplete, even with minimal missing values. It consistently maintains high accuracy e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#39044;&#27979;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#24182;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#35843;&#25972;&#65292;&#35813;&#26694;&#26550;&#38477;&#20302;&#20102;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#24182;&#23558;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;1.9&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.10660</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21333;&#35821;&#25991;&#26412;&#29983;&#25104;&#30340;&#21152;&#36895;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simple Framework to Accelerate Multilingual Language Model for Monolingual Text Generation. (arXiv:2401.10660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10660
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#39044;&#27979;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#24182;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#35843;&#25972;&#65292;&#35813;&#26694;&#26550;&#38477;&#20302;&#20102;&#35299;&#30721;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#24182;&#23558;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;1.9&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#19981;&#20165;&#22312;&#33521;&#35821;&#32780;&#19988;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#37117;&#20419;&#36827;&#20102;&#22797;&#26434;&#30340;&#35821;&#35328;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#35760;&#22120;&#65288;&#22914;Llama&#65289;&#22312;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#65292;&#20542;&#21521;&#20110;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#36807;&#20998;&#20998;&#21106;&#26631;&#35760;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38750;&#32599;&#39532;&#23383;&#27597;&#35821;&#35328;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#36825;&#20123;&#35821;&#35328;&#36890;&#24120;&#22312;&#23383;&#31526;&#25110;Unicode&#32423;&#21035;&#19978;&#34987;&#21010;&#20998;&#65292;&#23548;&#33268;&#25991;&#26412;&#29983;&#25104;&#36895;&#24230;&#36739;&#24930;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#36825;&#20123;&#35821;&#35328;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#39044;&#27979;&#27604;&#20256;&#32479;&#30340;&#22810;&#35821;&#35328;&#26631;&#35760;&#22120;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#65292;&#24182;&#19988;&#19987;&#38376;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35299;&#30721;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#35299;&#30721;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;1.9&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models have facilitated the execution of complex language tasks, not only in English but also in non-English languages. However, the tokenizers of most language models, such as Llama, trained on English-centric corpora, tend to excessively fragment tokens in non-English languages. This issue is especially pronounced in non-roman alphabetic languages, which are often divided at a character or even Unicode level, leading to slower text generation. To address this, our study introduces a novel framework designed to expedite text generation in these languages. This framework predicts larger linguistic units than those of conventional multilingual tokenizers and is specifically tailored to the target language, thereby reducing the number of decoding steps required. Our empirical results demonstrate that the proposed framework increases the generation speed by a factor of 1.9 compared to standard decoding while maintaining the performance of a pre-traine
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12428</link><description>&lt;p&gt;
&#23454;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#65306;&#22522;&#20110;&#37051;&#36817;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach. (arXiv:2310.12428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12428
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#65292;&#21033;&#29992;&#20102;&#20219;&#20309;RF&#37117;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#33258;&#36866;&#24212;&#21152;&#26435;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#27169;&#22411;&#30340;&#20107;&#23454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;RF&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#21040;&#30340;&#28857;&#20043;&#38388;&#30340;&#37051;&#36817;&#24615;&#65292;&#23558;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#37325;&#20889;&#20026;&#35757;&#32451;&#25968;&#25454;&#28857;&#30446;&#26631;&#26631;&#31614;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;&#36825;&#31181;&#32447;&#24615;&#24615;&#36136;&#26377;&#21161;&#20110;&#22312;&#35757;&#32451;&#38598;&#35266;&#27979;&#20013;&#20026;&#20219;&#20309;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#65292;&#20174;&#32780;&#20026;RF&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#34917;&#20805;&#20102;SHAP&#31561;&#24050;&#26377;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21017;&#20026;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#19978;&#30340;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#20110;&#32654;&#22269;&#20844;&#21496;&#20538;&#21048;&#20132;&#26131;&#25968;&#25454;&#30340;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate a novel approach to explain the out of sample performance of random forest (RF) models by exploiting the fact that any RF can be formulated as an adaptive weighted K nearest-neighbors model. Specifically, we use the proximity between points in the feature space learned by the RF to re-write random forest predictions exactly as a weighted average of the target labels of training data points. This linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established methods like SHAP, which instead generates attributions for a model prediction across dimensions of the feature space. We demonstrate this approach in the context of a bond pricing model trained on US corporate bond trades, and compare our approach to various existing approaches to model explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#32534;&#31243;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;GPT-4&#20316;&#20026;&#8220;&#23548;&#24072;&#8221;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#65292;&#21033;&#29992;&#22833;&#36133;&#30340;&#27979;&#35797;&#29992;&#20363;&#30340;&#20449;&#24687;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36739;&#24369;&#30340;GPT-3.5&#27169;&#22411;&#20316;&#20026;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#19968;&#27493;&#39564;&#35777;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03780</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20154;&#24037;&#23548;&#24072;&#24335;&#32534;&#31243;&#21453;&#39304;: &#21033;&#29992;GPT-4&#23548;&#24072;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#21644;GPT-3.5&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation. (arXiv:2310.03780v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#32534;&#31243;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;GPT-4&#20316;&#20026;&#8220;&#23548;&#24072;&#8221;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#65292;&#21033;&#29992;&#22833;&#36133;&#30340;&#27979;&#35797;&#29992;&#20363;&#30340;&#20449;&#24687;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36739;&#24369;&#30340;GPT-3.5&#27169;&#22411;&#20316;&#20026;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#19968;&#27493;&#39564;&#35777;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#32534;&#31243;&#21453;&#39304;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#25552;&#20379;&#20154;&#24037;&#23548;&#24072;&#24335;&#32534;&#31243;&#25552;&#31034;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#20197;&#24110;&#21161;&#23398;&#29983;&#35299;&#20915;&#31243;&#24207;&#20013;&#30340;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#24037;&#20316;&#34429;&#28982;&#23545;&#21508;&#31181;&#21453;&#39304;&#29983;&#25104;&#22330;&#26223;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#20854;&#25972;&#20307;&#36136;&#37327;&#20173;&#36828;&#19981;&#21450;&#20154;&#24037;&#23548;&#24072;&#65292;&#24182;&#19988;&#36824;&#27809;&#26377;&#20934;&#22791;&#22909;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25237;&#20837;&#20351;&#29992;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25552;&#20379;&#39640;&#36136;&#37327;&#32534;&#31243;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21517;&#20026;GPT4Hints-GPT3.5Val&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;GPT-4&#20316;&#20026;&#8220;&#23548;&#24072;&#8221;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#22833;&#36133;&#30340;&#27979;&#35797;&#29992;&#20363;&#30340;&#31526;&#21495;&#20449;&#24687;&#21644;&#25552;&#31034;&#20013;&#30340;&#20462;&#22797;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;&#36739;&#24369;&#30340;GPT-3.5&#27169;&#22411;&#20316;&#20026;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#19968;&#27493;&#39564;&#35777;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a first step, our technique leverages GPT-4 as a ``tutor'' model to generate hints -- it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a ``student'' model to further validate the hint 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09797</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33268;&#32858;&#21512;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting with Coherent Aggregation. (arXiv:2307.09797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#33719;&#24471;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#36816;&#33829;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33021;&#28304;&#31649;&#29702;&#12289;&#20379;&#24212;&#38142;&#35268;&#21010;&#21644;&#36164;&#28304;&#37197;&#32622;&#31561;&#39046;&#22495;&#12290;&#23545;&#20110;&#22810;&#21464;&#37327;&#39044;&#27979;&#65292;&#22522;&#26412;&#25361;&#25112;&#22312;&#20110;&#39044;&#27979;&#36890;&#24120;&#38656;&#35201;&#19982;&#23618;&#27425;&#32467;&#26500;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#36890;&#36807;&#26500;&#24314;&#26469;&#20135;&#29983;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35266;&#23519;&#32467;&#26524;&#65288;&#21487;&#20132;&#25442;&#24615;&#65289;&#65306;&#32622;&#25442;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#22522;&#26412;&#32423;&#21035;&#24207;&#21015;&#19981;&#20250;&#25913;&#21464;&#23427;&#20204;&#30340;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#22240;&#23376;&#12289;&#23427;&#20204;&#30340;&#21152;&#36733;&#21644;&#22522;&#26412;&#32423;&#21035;&#20998;&#24067;&#30340;&#21442;&#25968;&#65307;&#23427;&#20135;&#29983;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#30340;&#26679;&#26412;&#65307;&#22240;&#27492;&#23427;&#21487;&#20197;&#23545;&#20219;&#20309;&#22522;&#20110;&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#21253;&#25324;&#36830;&#32493;&#25490;&#21517;&#27010;&#29575;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting \textit{}base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probabili
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.08354</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tool Learning with Foundation Models. (arXiv:2304.08354v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08354
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25317;&#26377;&#38750;&#20961;&#30340;&#21019;&#36896;&#21644;&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#20182;&#20204;&#33021;&#22815;&#20811;&#26381;&#29289;&#29702;&#38480;&#21046;&#24182;&#25506;&#32034;&#26032;&#30340;&#39046;&#22495;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#26377;&#26395;&#20687;&#20154;&#31867;&#19968;&#26679;&#29087;&#32451;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;&#36825;&#31181;&#33539;&#24335;&#21363;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#23613;&#31649;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#35813;&#39046;&#22495;&#20173;&#32570;&#20047;&#23545;&#20851;&#38190;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#24037;&#20855;&#23398;&#20064;&#30340;&#32972;&#26223;&#65292;&#21253;&#25324;&#20854;&#35748;&#30693;&#36215;&#28304;&#12289;&#22522;&#30784;&#27169;&#22411;&#30340;&#33539;&#24335;&#36716;&#25442;&#21644;&#24037;&#20855;&#21644;&#27169;&#22411;&#30340;&#20114;&#34917;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#30740;&#31350;&#65292;&#21253;&#25324;&#22522;&#20110;&#24037;&#20855;&#21644;&#38754;&#21521;&#24037;&#20855;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#31181;&#31995;&#32479;&#30340;&#25506;&#32034;&#23558;&#20026;&#26410;&#26469;&#24320;&#21457;&#20855;&#26377;&#22797;&#26434;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#36339;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool l
&lt;/p&gt;</description></item></channel></rss>