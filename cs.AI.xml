<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24443;&#24213;&#37325;&#26657;&#20934;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#30340;&#20215;&#20540;&#35266;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#30340;&#38887;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02745</link><description>&lt;p&gt;
CURATRON&#65306;&#23436;&#25972;&#20581;&#22766;&#20559;&#22909;&#25968;&#25454;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20581;&#22766;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24443;&#24213;&#37325;&#26657;&#20934;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#30340;&#20215;&#20540;&#35266;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#65288;PL&#65289;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#19981;&#23436;&#25972;&#21644;&#25439;&#22351;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24443;&#24213;&#21644;&#23436;&#20840;&#22320;&#37325;&#26032;&#26657;&#20934;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#20215;&#20540;&#35266;&#65292;&#20197;&#22686;&#24378;LLMs&#23545;&#38382;&#39064;&#30340;&#38887;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#20445;&#35777;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#25490;&#21517;&#31639;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#20960;&#31181;&#29616;&#26377;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#27604;&#22914;&#32463;&#20856;&#30340;Bradley&#8211;Terry&#8211;Luce&#65288;BTL&#65289;&#65288;Bradley&#21644;Terry&#65292;1952&#65289;&#27169;&#22411;&#20197;&#21450;&#23545;&#20854;&#26576;&#20123;&#25512;&#24191;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#19968;&#31181;&#21487;&#35777;&#26126;&#22312;&#39640;&#27010;&#29575;&#19979;&#24674;&#22797;{\epsilon}-&#26368;&#20248;&#25490;&#24207;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#20801;&#35768;&#27599;&#20010;&#27169;&#22411;&#21709;&#24212;&#22810;&#36798;O(n)&#25200;&#21160;&#30340;&#25104;&#23545;&#27604;&#36739;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37096;&#20998;&#35266;&#23519;&#35774;&#32622;&#19979;&#30340;&#20581;&#22766;&#24674;&#22797;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02745v1 Announce Type: new  Abstract: This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), with a focus on the issues of incomplete and corrupted data in preference datasets. We propose a novel method for robustly and completely recalibrating values within these datasets to enhance LLMs resilience against the issues. In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley--Terry--Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it. To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an {\epsilon}-optimal ranking with high probability while allowing as large as O(n) perturbed pairwise comparison results per model response. Furthermore, we show robust recovery results in the partially observed setting. Our experiments confirm that our algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23481;&#38169;&#24615;&#37327;&#23376;&#35745;&#31639;&#30340;&#35270;&#35282;&#19979;Transformer&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#26500;&#24314;Transformer&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.16714</link><description>&lt;p&gt;
&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#26159;Transformer&#26550;&#26500;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Quantum linear algebra is all you need for Transformer architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23481;&#38169;&#24615;&#37327;&#23376;&#35745;&#31639;&#30340;&#35270;&#35282;&#19979;Transformer&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#26500;&#24314;Transformer&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#21019;&#20316;&#12290;&#26412;&#25991;&#36890;&#36807;&#23481;&#38169;&#24615;&#37327;&#23376;&#35745;&#31639;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;Transformer&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20934;&#22791;self-attention&#30697;&#38453;&#30340;&#22359;&#32534;&#30721;&#65292;&#24182;&#32467;&#21512;&#37327;&#23376;&#23376;&#31243;&#24207;&#26500;&#24314;&#20102;Transformer&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16714v1 Announce Type: cross  Abstract: Generative machine learning methods such as large-language models are revolutionizing the creation of text and images. While these models are powerful they also harness a large amount of computational resources. The transformer is a key component in large language models that aims to generate a suitable completion of a given partial sequence. In this work, we investigate transformer architectures under the lens of fault-tolerant quantum computing. The input model is one where pre-trained weight matrices are given as block encodings to construct the query, key, and value matrices for the transformer. As a first step, we show how to prepare a block encoding of the self-attention matrix, with a row-wise application of the softmax function using the Hadamard product. In addition, we combine quantum subroutines to construct important building blocks in the transformer, the residual connection, layer normalization, and the feed-forward neura
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#30340;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#26234;&#33021;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#65292;&#24182;&#20855;&#26377;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07870</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#23454;&#29616;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Computational Intelligence via Predictive Coding. (arXiv:2308.07870v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#30340;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#26234;&#33021;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#65292;&#24182;&#20855;&#26377;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#36805;&#36895;&#25104;&#20026;&#26412;&#19990;&#32426;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22312;AI&#39046;&#22495;&#21462;&#24471;&#30340;&#22823;&#37096;&#20998;&#25104;&#26524;&#37117;&#26159;&#20351;&#29992;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#21450;&#24212;&#29992;&#24050;&#32463;&#20984;&#26174;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#38590;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12289;&#32570;&#20047;&#40065;&#26834;&#24615;&#12289;&#19981;&#21487;&#38752;&#24615;&#21644;&#29983;&#29289;&#23398;&#19978;&#30340;&#19981;&#21512;&#29702;&#24615;&#12290;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#38656;&#35201;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#30340;&#21551;&#21457;&#21644;&#25351;&#23548;&#30340;&#26041;&#26696;&#12290;&#20854;&#20013;&#19968;&#31181;&#29702;&#35770;&#31216;&#20026;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#65292;&#22312;&#26426;&#22120;&#26234;&#33021;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#20196;&#20154;&#20852;&#22859;&#30340;&#29305;&#24615;&#65292;&#20351;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#65306;PC&#21487;&#20197;&#27169;&#25311;&#19981;&#21516;&#33041;&#21306;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#21487;&#20197;&#29992;&#20110;&#35748;&#30693;&#25511;&#21046;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#24182;&#22312;&#21464;&#20998;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm. However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility. It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories. One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a pow
&lt;/p&gt;</description></item></channel></rss>