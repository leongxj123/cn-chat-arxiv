<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01204</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Learning for Non-Sequential Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#20013;SSL&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#19978;&#19979;&#25991;&#21270;&#21644;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;SSL&#24050;&#25104;&#20026;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#26032;&#36235;&#21183;&#65292;&#36825;&#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#26126;&#30830;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#25551;&#36848;&#24615;&#30340;&#34920;&#31034;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#22238;&#39038;&#21644;&#24635;&#32467;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#65288;SSL4NS-TD&#65289;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;NS-TD&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#26041;&#27861;&#34987;&#20998;&#20026;&#19977;&#32452;&#8212;&#8212;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#24182;&#20171;&#32461;&#20102;&#27599;&#20010;&#26041;&#21521;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#21160;&#26426;&#21644;&#20248;&#28857;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#20171;&#32461;&#20102;SSL4NS-TD&#30340;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.02476</link><description>&lt;p&gt;
&#29992;&#20110;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Traveling Purchaser Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#65288;TPP&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#29992;&#20110;&#25429;&#25417;&#24066;&#22330;-&#20135;&#21697;&#20851;&#31995;&#30340;TPP&#30340;&#20108;&#37096;&#22270;&#34920;&#31034;&#65292;&#20197;&#21450;&#20174;&#20108;&#37096;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#29992;&#20110;&#39034;&#24207;&#26500;&#24314;&#36335;&#30001;&#30340;&#31574;&#30053;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02476v1 Announce Type: cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently const
&lt;/p&gt;</description></item><item><title>LeTac-MPC&#26159;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;GelSight&#21644;&#19981;&#21516;iable MPC&#23618;&#65292;&#23454;&#29616;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#21644;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#29289;&#20307;&#19978;&#36827;&#34892;&#31283;&#20581;&#25235;&#21462;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04934</link><description>&lt;p&gt;
LeTac-MPC&#65306;&#29992;&#20110;&#35302;&#35273;&#21453;&#24212;&#25235;&#21462;&#30340;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LeTac-MPC: Learning Model Predictive Control for Tactile-reactive Grasping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04934
&lt;/p&gt;
&lt;p&gt;
LeTac-MPC&#26159;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;GelSight&#21644;&#19981;&#21516;iable MPC&#23618;&#65292;&#23454;&#29616;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#21644;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#29289;&#20307;&#19978;&#36827;&#34892;&#31283;&#20581;&#25235;&#21462;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25235;&#21462;&#26159;&#26426;&#22120;&#20154;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#38656;&#35201;&#35302;&#35273;&#21453;&#39304;&#21644;&#21453;&#24212;&#24615;&#25235;&#21462;&#35843;&#25972;&#65292;&#20197;&#23454;&#29616;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#21644;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#23545;&#35937;&#30340;&#31283;&#20581;&#25235;&#21462;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LeTac-MPC&#65292;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#29992;&#20110;&#35302;&#35273;&#21453;&#24212;&#24335;&#25235;&#21462;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#22841;&#29226;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#21147;&#20132;&#20114;&#20219;&#21153;&#20013;&#25235;&#21462;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;GelSight&#65292;&#35813;&#20256;&#24863;&#22120;&#33021;&#22815;&#24863;&#30693;&#21253;&#21547;&#25235;&#21462;&#23545;&#35937;&#30340;&#29289;&#29702;&#23646;&#24615;&#21644;&#29366;&#24577;&#20449;&#24687;&#30340;&#39640;&#20998;&#36776;&#29575;&#35302;&#35273;&#21453;&#39304;&#12290;LeTac-MPC&#21253;&#21547;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;MPC&#23618;&#65292;&#35774;&#35745;&#29992;&#20110;&#23545;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20174;&#35302;&#35273;&#21453;&#39304;&#20013;&#25552;&#21462;&#30340;&#23884;&#20837;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#31181;&#35774;&#35745;&#26377;&#21161;&#20110;&#22312;25 Hz&#30340;&#39057;&#29575;&#19979;&#23454;&#29616;&#25910;&#25947;&#21644;&#31283;&#20581;&#30340;&#25235;&#21462;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#32452;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04934v1 Announce Type: cross  Abstract: Grasping is a crucial task in robotics, necessitating tactile feedback and reactive grasping adjustments for robust grasping of objects under various conditions and with differing physical properties. In this paper, we introduce LeTac-MPC, a learning-based model predictive control (MPC) for tactile-reactive grasping. Our approach enables the gripper grasp objects with different physical properties on dynamic and force-interactive tasks. We utilize a vision-based tactile sensor, GelSight, which is capable of perceiving high-resolution tactile feedback that contains the information of physical properties and states of the grasped object. LeTac-MPC incorporates a differentiable MPC layer designed to model the embeddings extracted by a neural network (NN) from tactile feedback. This design facilitates convergent and robust grasping control at a frequency of 25 Hz. We propose a fully automated data collection pipeline and collect a dataset 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#65292;</title><link>https://arxiv.org/abs/2403.04650</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Context-Based Multimodal Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20294;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#23616;&#38480;&#24615;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#31216;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65288;CBMF&#65289;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04650v1 Announce Type: cross  Abstract: The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks. However, they have significant limitations related to aligning data distributions across different modalities. This challenge can lead to inconsistencies and difficulties in learning robust representations. Alignment models, while specifically addressing this issue, often require training "from scratch" with large datasets to achieve optimal results, which can be costly in terms of resources and time. To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment. In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality. This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements. A
&lt;/p&gt;</description></item><item><title>MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10093</link><description>&lt;p&gt;
MIM-Refiner&#65306;&#19968;&#31181;&#20174;&#20013;&#38388;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#33719;&#24471;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10093
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MIM-Refiner&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#35757;&#32451;MIM&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#12290;MIM-Refiner&#30340;&#21160;&#26426;&#22312;&#20110;MIM&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#34920;&#31034;&#36890;&#24120;&#20301;&#20110;&#20013;&#38388;&#23618;&#12290;&#22240;&#27492;&#65292;MIM-Refiner&#21033;&#29992;&#36830;&#25509;&#21040;&#19981;&#21516;&#20013;&#38388;&#23618;&#30340;&#22810;&#20010;&#23545;&#27604;&#22836;&#12290;&#22312;&#27599;&#20010;&#22836;&#20013;&#65292;&#20462;&#25913;&#21518;&#30340;&#26368;&#36817;&#37051;&#30446;&#26631;&#24110;&#21161;&#26500;&#24314;&#30456;&#24212;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;&#27492;&#36807;&#31243;&#30701;&#32780;&#26377;&#25928;&#65292;&#22312;&#20960;&#20010;epochs&#20869;&#65292;&#25105;&#20204;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#12290;&#20351;&#29992;data2vec 2.0&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;ViT-H&#32463;&#36807;&#25913;&#36827;&#21518;&#65292;&#22312;&#32447;&#24615;&#25506;&#27979;&#21644;&#20302;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65288;&#20998;&#21035;&#20026;84.7%&#21644;64.2%&#65289;&#65292;&#36229;&#36807;&#20102;&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10093v1 Announce Type: cross  Abstract: We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger mo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04856</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Explaining Learned Reward Functions with Counterfactual Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04856
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#34892;&#20026;&#25110;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#26159;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#26080;&#27861;&#22987;&#32456;&#25552;&#21462;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#25509;&#25910;&#30340;&#22870;&#21169;&#26469;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;CTEs&#21046;&#23450;&#20102;&#20845;&#20010;&#36136;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Monte-Carlo&#30340;&#26032;&#31639;&#27861;&#26469;&#29983;&#25104;&#20248;&#21270;&#36825;&#20123;&#36136;&#37327;&#26631;&#20934;&#30340;CTEs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#20154;&#27169;&#22411;&#26469;&#34913;&#37327;&#29983;&#25104;&#30340;&#35299;&#37322;&#23545;&#20854;&#30340;&#20449;&#24687;&#24615;&#12290;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#22686;&#21152;&#20102;&#20854;&#39044;&#27979;&#19982;&#26410;&#35265;&#36712;&#36857;&#19978;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#23398;&#20250;&#20102;&#20934;&#30830;&#21028;&#26029;&#36712;&#36857;&#20043;&#38388;&#30340;&#22870;&#21169;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajec
&lt;/p&gt;</description></item><item><title>MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.02263</link><description>&lt;p&gt;
MixedNUTS: &#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02263
&lt;/p&gt;
&lt;p&gt;
MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24448;&#24448;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#65292;&#38459;&#30861;&#20102;&#40065;&#26834;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#24050;&#35757;&#32451;&#30340;&#22823;&#22411;&#39640;&#24615;&#33021;&#27169;&#22411;&#20860;&#23481;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#26080;&#38656;&#35757;&#32451;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#40065;&#26834;&#27169;&#22411;&#22312;&#24178;&#20928;&#25968;&#25454;&#21644;&#23545;&#25239;&#25968;&#25454;&#19978;&#30340;&#27491;&#30830;&#39044;&#27979;&#27604;&#38169;&#35823;&#39044;&#27979;&#26356;&#33258;&#20449;&#65292;&#25105;&#20204;&#25512;&#27979;&#36890;&#36807;&#22686;&#24378;&#36825;&#31181;&#8220;&#33391;&#24615;&#32622;&#20449;&#24230;&#29305;&#24615;&#8221;&#21487;&#20197;&#22312;&#38598;&#25104;&#29615;&#22659;&#20013;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MixedNUTS&#8221;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#26377;&#19977;&#20010;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#26469;&#22788;&#29702;&#40065;&#26834;&#20998;&#31867;&#22120;&#21644;&#26631;&#20934;&#38750;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;Logits&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;MixedNUTS&#23558;&#36716;&#25442;&#21518;&#30340;Logits&#36716;&#25442;&#20026;&#27010;&#29575;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20316;&#20026;&#26368;&#32456;&#30340;&#36755;&#20986;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
&lt;/p&gt;</description></item><item><title>CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2401.11944</link><description>&lt;p&gt;
CMMMU&#65306;&#19968;&#20010;&#20013;&#22269;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11944
&lt;/p&gt;
&lt;p&gt;
CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#21319;&#65292;&#35780;&#20272;LMMs&#30340;&#34920;&#29616;&#26085;&#30410;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#22312;&#35780;&#20272;LMMs&#22312;&#20013;&#25991;&#31561;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26356;&#22823;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CMMMU&#65292;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LMMs&#22312;&#38656;&#35201;&#22823;&#23398;&#27700;&#24179;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;CMMMU&#21463;&#21040;&#20102;MMMUs&#30340;&#26631;&#27880;&#21644;&#20998;&#26512;&#27169;&#24335;&#30340;&#21551;&#21457;&#24182;&#20005;&#26684;&#36981;&#24490;&#12290;CMMMU&#21253;&#25324;&#26469;&#33258;&#22823;&#23398;&#32771;&#35797;&#12289;&#27979;&#39564;&#21644;&#25945;&#31185;&#20070;&#30340;1.2&#19975;&#20010;&#25163;&#21160;&#25910;&#38598;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#28085;&#30422;&#20845;&#20010;&#26680;&#24515;&#23398;&#31185;&#65306;&#33402;&#26415;&#19982;&#35774;&#35745;&#12289;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#20581;&#24247;&#19982;&#21307;&#23398;&#12289;&#20154;&#25991;&#31038;&#31185;&#20197;&#21450;&#25216;&#26415;&#19982;&#24037;&#31243;&#65292;&#23601;&#20687;&#20854;&#20249;&#20276;MMMMU&#19968;&#26679;&#12290;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;30&#20010;&#23398;&#31185;&#65292;&#21253;&#25324;39&#20010;&#39640;&#24230;&#24322;&#36136;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11944v2 Announce Type: replace-cross  Abstract: As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp; Social Science, and Tech &amp; Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#26435;&#34913;&#22788;&#29702;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20013;&#30340;&#32039;&#24352;&#20851;&#31995;&#30340;&#20116;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23454;&#26045;&#20840;&#38754;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2401.08103</link><description>&lt;p&gt;
&#35299;&#20915;&#22312;&#23454;&#26045;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20262;&#29702;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Resolving Ethics Trade-offs in Implementing Responsible AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#26435;&#34913;&#22788;&#29702;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20013;&#30340;&#32039;&#24352;&#20851;&#31995;&#30340;&#20116;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23454;&#26045;&#20840;&#38754;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25226;&#39640;&#32423;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#24212;&#29992;&#21040;&#23454;&#38469;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22788;&#29702;&#24213;&#23618;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26041;&#38754;&#30340;&#32039;&#24352;&#20851;&#31995;&#26041;&#38754;&#20173;&#23384;&#22312;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20116;&#31181;&#22788;&#29702;&#36825;&#20123;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#19981;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#32771;&#34385;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#12289;&#33539;&#22260;&#12289;&#34913;&#37327;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#21644;&#35777;&#26126;&#31243;&#24230;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#20123;&#26041;&#27861;&#20013;&#27809;&#26377;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#32452;&#32455;&#12289;&#31995;&#32479;&#25110;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#31215;&#26497;&#35782;&#21035;&#32039;&#24352;&#20851;&#31995;&#65292;&#65288;ii&#65289;&#20248;&#20808;&#22788;&#29702;&#21644;&#26435;&#34913;&#20262;&#29702;&#26041;&#38754;&#65292;&#65288;iii&#65289;&#35777;&#26126;&#21644;&#35760;&#24405;&#26435;&#34913;&#20915;&#31574;&#12290;&#35813;&#25552;&#35758;&#30340;&#26694;&#26550;&#26088;&#22312;&#20419;&#36827;&#23454;&#26045;&#31526;&#21512;&#28508;&#22312;&#30417;&#31649;&#35201;&#27714;&#30340;&#20840;&#38754;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the operationalisation of high-level AI ethics principles into practical AI/ML systems has made progress, there is still a theory-practice gap in managing tensions between the underlying AI ethics aspects. We cover five approaches for addressing the tensions via trade-offs, ranging from rudimentary to complex. The approaches differ in the types of considered context, scope, methods for measuring contexts, and degree of justification. None of the approaches is likely to be appropriate for all organisations, systems, or applications. To address this, we propose a framework which consists of: (i) proactive identification of tensions, (ii) prioritisation and weighting of ethics aspects, (iii) justification and documentation of trade-off decisions. The proposed framework aims to facilitate the implementation of well-rounded AI/ML systems that are appropriate for potential regulatory requirements.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimelyGPT&#30340;&#21487;&#25512;&#24191;&#30340;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#21487;&#25512;&#24191;&#30340;&#20301;&#32622;&#23884;&#20837;&#21644;&#24490;&#29615;&#27880;&#24847;&#21147;&#20197;&#21450;&#26102;&#38388;&#21367;&#31215;&#27169;&#22359;&#26377;&#25928;&#22320;&#25429;&#25417;&#36229;&#38271;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2312.00817</link><description>&lt;p&gt;
&#21487;&#25512;&#24191;&#30340;Transformer&#39044;&#35757;&#32451;&#29992;&#20110;&#36229;&#38271;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00817
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimelyGPT&#30340;&#21487;&#25512;&#24191;&#30340;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#21487;&#25512;&#24191;&#30340;&#20301;&#32622;&#23884;&#20837;&#21644;&#24490;&#29615;&#27880;&#24847;&#21147;&#20197;&#21450;&#26102;&#38388;&#21367;&#31215;&#27169;&#22359;&#26377;&#25928;&#22320;&#25429;&#25417;&#36229;&#38271;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#65292;&#22914;BERT&#21644;GPT&#65292;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;PTMs&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#21457;&#23637;&#28382;&#21518;&#12290;&#36825;&#20984;&#26174;&#20102;&#29616;&#26377;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#25429;&#25417;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21363;&#26102;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65288;TimelyGPT&#65289;&#12290;TimelyGPT&#37319;&#29992;&#21487;&#25512;&#24191;&#20301;&#32622;&#65288;xPos&#65289;&#23884;&#20837;&#23558;&#36235;&#21183;&#21644;&#21608;&#26399;&#27169;&#24335;&#32534;&#30721;&#21040;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20013;&#12290;&#23427;&#36824;&#38598;&#25104;&#20102;&#24490;&#29615;&#27880;&#24847;&#21147;&#21644;&#26102;&#38388;&#21367;&#31215;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TimelyGPT&#22312;&#24314;&#27169;&#36830;&#32493;&#30417;&#27979;&#30340;&#29983;&#29289;&#20449;&#21495;&#21644;&#32463;&#24120;&#20986;&#29616;&#22312;&#32437;&#21521;&#30005;&#30913;&#27874;&#39046;&#22495;&#20013;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00817v2 Announce Type: replace-cross  Abstract: Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success in Natural Language Processing and Computer Vision domains. However, the development of PTMs on time-series data is lagging behind. This underscores the limitations of the existing transformer-based architectures, particularly their scalability to handle large-scale data and ability to capture long-term temporal dependencies. In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT). TimelyGPT employs an extrapolatable position (xPos) embedding to encode trend and periodic patterns into time-series representations. It also integrates recurrent attention and temporal convolution modules to effectively capture global-local temporal dependencies. Our experiments show that TimelyGPT excels in modeling continuously monitored biosignals and irregularly-sampled time series data commonly observed in longitudinal electro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#24191;&#21578;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;LLM&#30340;&#21160;&#24577;&#21019;&#24847;&#20248;&#21270;&#30340;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2311.07601</link><description>&lt;p&gt;
&#22312;&#32447;&#24191;&#21578;&#19982;LLMs&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Online Advertisements with LLMs: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#24191;&#21578;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;LLM&#30340;&#21160;&#24577;&#21019;&#24847;&#20248;&#21270;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#36825;&#20010;&#31995;&#32479;&#24517;&#39035;&#28385;&#36275;&#30340;&#38544;&#31169;&#12289;&#24310;&#36831;&#12289;&#21487;&#38752;&#24615;&#20197;&#21450;&#29992;&#25143;&#21644;&#24191;&#21578;&#21830;&#30340;&#28385;&#24847;&#24230;&#31561;&#20851;&#38190;&#35201;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#20010;LLM&#24191;&#21578;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21253;&#25324;&#20462;&#25913;&#12289;&#31454;&#26631;&#12289;&#39044;&#27979;&#21644;&#25293;&#21334;&#27169;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27599;&#20010;&#27169;&#22359;&#30340;&#19981;&#21516;&#35774;&#35745;&#32771;&#34385;&#65292;&#24182;&#23545;&#20854;&#23454;&#29992;&#24615;&#21644;&#23454;&#26045;&#20013;&#30340;&#25216;&#26415;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;LLM&#30340;&#21160;&#24577;&#21019;&#24847;&#20248;&#21270;&#30340;&#21069;&#26223;&#65292;&#20197;&#26174;&#33879;&#25552;&#21319;&#24191;&#21578;&#23545;&#29992;&#25143;&#30340;&#21560;&#24341;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#25152;&#38754;&#20020;&#30340;&#39069;&#22806;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07601v2 Announce Type: replace-cross Abstract: This paper explores the potential for leveraging Large Language Models (LLM) in the realm of online advertising systems. We delve into essential requirements including privacy, latency, reliability as well as the satisfaction of users and advertisers which such a system must fulfill. We further introduce a general framework for LLM advertisement, consisting of modification, bidding, prediction, and auction modules. Different design considerations for each module is presented, with an in-depth examination of their practicality and the technical challenges inherent to their implementation. Finally, we explore the prospect of LLM-based dynamic creative optimization as a means to significantly enhance the appeal of advertisements to users and discuss its additional challenges.
&lt;/p&gt;</description></item><item><title>LUX&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;&#65292;&#36890;&#36807;&#36873;&#25321;&#39640;&#23494;&#24230;&#31751;&#24418;&#24335;&#30340;&#23616;&#37096;&#27010;&#24565;&#26469;&#24418;&#25104;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2310.14894</link><description>&lt;p&gt;
&#26412;&#22320;&#36890;&#29992;&#35299;&#37322;&#22120;&#65288;LUX&#65289;-- &#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#20855;&#26377;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Local Universal Explainer (LUX) -- a rule-based explainer with factual, counterfactual and visual explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14894
&lt;/p&gt;
&lt;p&gt;
LUX&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;&#65292;&#36890;&#36807;&#36873;&#25321;&#39640;&#23494;&#24230;&#31751;&#24418;&#24335;&#30340;&#23616;&#37096;&#27010;&#24565;&#26469;&#24418;&#25104;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#36817;&#24180;&#26469;&#26368;&#34987;&#24191;&#27867;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20043;&#19968;&#12290;&#23427;&#20063;&#26159;&#26368;&#20998;&#25955;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#26377;&#22810;&#31181;&#26041;&#27861;&#19987;&#27880;&#20110;&#35299;&#37322;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#36825;&#20351;&#24471;&#19968;&#27425;&#24615;&#20197;&#32039;&#20945;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#33719;&#24471;&#23436;&#25972;&#30340;&#35299;&#37322;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26412;&#22320;&#36890;&#29992;&#35299;&#37322;&#22120;&#65288;LUX&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;&#12290;&#23427;&#22522;&#20110;&#20462;&#25913;&#21518;&#30340;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#20801;&#35768;&#26012;&#20132;&#21644;&#38598;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;XAI&#26041;&#27861;&#65292;&#22914;SHAP&#25110;LIME&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#21453;&#65292;&#23427;&#19981;&#20351;&#29992;&#25968;&#25454;&#29983;&#25104;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#36873;&#25321;&#20197;&#39640;&#23494;&#24230;&#31751;&#24418;&#24335;&#20986;&#29616;&#30340;&#30495;&#23454;&#25968;&#25454;&#30340;&#23616;&#37096;&#27010;&#24565;&#65292;&#36825;&#20123;&#23616;&#37096;&#27010;&#24565;&#23545;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#24418;&#25104;&#26368;&#22823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) is one of the most intensively developed area of AI in recent years. It is also one of the most fragmented with multiple methods that focus on different aspects of explanations. This makes difficult to obtain the full spectrum of explanation at once in a compact and consistent way. To address this issue, we present Local Universal Explainer (LUX), which is a rule-based explainer that can generate factual, counterfactual and visual explanations. It is based on a modified version of decision tree algorithms that allows for oblique splits and integration with feature importance XAI methods such as SHAP or LIME. It does not use data generation in opposite to other algorithms, but is focused on selecting local concepts in a form of high-density clusters of real data that have the highest impact on forming the decision boundary of the explained model. We tested our method on real and synthetic datasets and compared it with state-of-the-art rule-based
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2310.07644</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rethinking the BERT-like Pretraining for DNA Sequences. (arXiv:2310.07644v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07644
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25104;&#21151;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;DNA&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22240;&#20854;&#25429;&#25417;&#22522;&#22240;&#30340;&#36890;&#29992;&#20449;&#24687;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DNA&#24207;&#21015;&#39044;&#35757;&#32451;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30452;&#25509;&#24341;&#20837;&#30340;BERT&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#19987;&#38376;&#23450;&#21046;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#25506;&#32034;&#24615;&#23454;&#39564;&#65292;&#24182;&#33719;&#24471;&#20102;&#20960;&#20010;&#26377;&#21551;&#21457;&#24615;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;1&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#65292;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#32780;&#19981;&#26159;K-mer&#38750;&#37325;&#21472;&#26631;&#35760;&#21270;&#26102;&#65292;&#37325;&#21472;&#21644;&#38750;&#37325;&#21472;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#22343;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;2&#65289;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#20250;&#36805;&#36895;&#20135;&#29983;&#28165;&#26224;&#30340;K-mer&#23884;&#20837;&#65292;&#24182;&#23558;&#25439;&#22833;&#38477;&#20302;&#21040;&#38750;&#24120;&#20302;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the success of large-scale pretraining in NLP, there is an increasing trend of applying it to the domain of life sciences. In particular, pretraining methods based on DNA sequences have garnered growing attention due to their potential to capture generic information about genes. However, existing pretraining methods for DNA sequences largely rely on direct adoptions of BERT pretraining from NLP, lacking a comprehensive understanding and a specifically tailored approach. To address this research gap, we first conducted a series of exploratory experiments and gained several insightful observations: 1) In the fine-tuning phase of downstream tasks, when using K-mer overlapping tokenization instead of K-mer non-overlapping tokenization, both overlapping and non-overlapping pretraining weights show consistent performance improvement.2) During the pre-training process, using K-mer overlapping tokenization quickly produces clear K-mer embeddings and reduces the loss to a very low level, w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#24418;&#29366;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.00156</link><description>&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#23398;&#20064;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning Generalizable Tool-use Skills through Trajectory Generation. (arXiv:2310.00156v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00156
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#24418;&#29366;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21033;&#29992;&#24037;&#20855;&#30340;&#33258;&#20027;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#35768;&#22810;&#24120;&#35265;&#20219;&#21153;&#65292;&#22914;&#28921;&#39274;&#21644;&#28165;&#27905;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#31995;&#32479;&#22312;&#36866;&#24212;&#26032;&#24037;&#20855;&#26041;&#38754;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#30340;&#26234;&#33021;&#27700;&#24179;&#12290;&#22522;&#20110;&#21487;&#21450;&#24615;&#30340;&#20808;&#21069;&#24037;&#20316;&#36890;&#24120;&#23545;&#29615;&#22659;&#20570;&#20986;&#20102;&#24456;&#24378;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#26080;&#27861;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#20219;&#21153;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#20351;&#29992;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#24037;&#20855;&#26469;&#25805;&#32437;&#21487;&#21464;&#24418;&#29289;&#20307;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#20316;&#20026;&#19968;&#31995;&#21015;&#28857;&#20113;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#24037;&#20855;&#24418;&#29366;&#12290;&#23545;&#20110;&#20219;&#20309;&#26032;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#20808;&#29983;&#25104;&#19968;&#20010;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#65292;&#28982;&#21518;&#20248;&#21270;&#24037;&#20855;&#23039;&#21183;&#24207;&#21015;&#20197;&#19982;&#29983;&#25104;&#30340;&#36712;&#36857;&#23545;&#40784;&#12290;&#25105;&#20204;&#20026;&#22235;&#31181;&#19981;&#21516;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;&#20219;&#21153;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#21333;&#20010;&#24037;&#20855;&#30340;&#31034;&#33539;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model for four different challenging deformable object manipulation tasks. Our model is trained with demonstration data from just a single tool for each task and is able to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#65292;&#21487;&#20197;&#22312;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#32454;&#33410;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#20316;&#20026;&#26032;&#30340;&#32447;&#32034;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;DEFormer&#36824;&#21487;&#20197;&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06941</link><description>&lt;p&gt;
DEFormer: &#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#21644;&#26263;&#35270;&#35273;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
DEFormer: DCT-driven Enhancement Transformer for Low-light Image and Dark Vision. (arXiv:2309.06941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#65292;&#21487;&#20197;&#22312;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#32454;&#33410;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#20316;&#20026;&#26032;&#30340;&#32447;&#32034;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;DEFormer&#36824;&#21487;&#20197;&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#30446;&#26631;&#26159;&#24674;&#22797;&#22270;&#20687;&#30340;&#39068;&#33394;&#21644;&#32454;&#33410;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#39640;&#32423;&#35270;&#35273;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;RGB&#39046;&#22495;&#24456;&#38590;&#24674;&#22797;&#26263;&#21306;&#22495;&#30340;&#20002;&#22833;&#32454;&#33410;&#12290;&#26412;&#25991;&#23558;&#39057;&#29575;&#20316;&#20026;&#32593;&#32476;&#30340;&#26032;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#29992;&#20110;&#39057;&#29575;&#22686;&#24378;&#65292;&#21253;&#25324;DCT&#22788;&#29702;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#12290;CFE&#35745;&#31639;&#27599;&#20010;&#36890;&#36947;&#30340;&#26354;&#29575;&#20197;&#34920;&#31034;&#19981;&#21516;&#39057;&#29575;&#24102;&#30340;&#32454;&#33410;&#20016;&#23500;&#24230;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#39057;&#29575;&#29305;&#24449;&#21010;&#20998;&#20026;&#26356;&#20016;&#23500;&#32441;&#29702;&#30340;&#39057;&#29575;&#24102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;RGB&#39046;&#22495;&#21644;&#39057;&#29575;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;DEFormer&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;DEFormer&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of low-light image enhancement is to restore the color and details of the image and is of great significance for high-level visual tasks in autonomous driving. However, it is difficult to restore the lost details in the dark area by relying only on the RGB domain. In this paper we introduce frequency as a new clue into the network and propose a novel DCT-driven enhancement transformer (DEFormer). First, we propose a learnable frequency branch (LFB) for frequency enhancement contains DCT processing and curvature-based frequency enhancement (CFE). CFE calculates the curvature of each channel to represent the detail richness of different frequency bands, then we divides the frequency features, which focuses on frequency bands with richer textures. In addition, we propose a cross domain fusion (CDF) for reducing the differences between the RGB domain and the frequency domain. We also adopt DEFormer as a preprocessing in dark detection, DEFormer effectively improves the performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#22914;&#20309;&#36890;&#36807;&#24494;&#23567;&#20462;&#25913;&#24178;&#25200;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#21457;&#29616;&#36825;&#21487;&#33021;&#26159;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#19979;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#23454;&#38469;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#34892;&#20026;&#65292;&#21253;&#25324;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#38543;&#26426;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#39564;&#35777;&#23454;&#39564;&#36824;&#34920;&#26126;&#20102;&#30456;&#21516;&#29616;&#35937;&#22312;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2309.03665</link><description>&lt;p&gt;
&#22914;&#20309;&#25915;&#20987;&#21487;&#20197;&#24178;&#25200;&#30475;&#20284;&#31283;&#23450;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
How adversarial attacks can disrupt seemingly stable accurate classifiers. (arXiv:2309.03665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#22914;&#20309;&#36890;&#36807;&#24494;&#23567;&#20462;&#25913;&#24178;&#25200;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#21457;&#29616;&#36825;&#21487;&#33021;&#26159;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#19979;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#23454;&#38469;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#34892;&#20026;&#65292;&#21253;&#25324;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#38543;&#26426;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#39564;&#35777;&#23454;&#39564;&#36824;&#34920;&#26126;&#20102;&#30456;&#21516;&#29616;&#35937;&#22312;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#36890;&#36807;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#65292;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#21407;&#26412;&#20934;&#30830;&#30340;&#23398;&#20064;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#20855;&#26377;&#35773;&#21050;&#24847;&#21619;&#30340;&#26159;&#65292;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#21363;&#20351;&#31995;&#32479;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#22823;&#24133;&#24230;&#38543;&#26426;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36755;&#20837;&#25968;&#25454;&#30340;&#23567;&#20247;&#12289;&#26131;&#20110;&#26500;&#36896;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#21487;&#33021;&#26159;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#19979;&#20998;&#31867;&#22120;&#30340;&#19968;&#20010;&#22522;&#26412;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36890;&#29992;&#24615;&#21644;&#26222;&#36866;&#24615;&#26694;&#26550;&#65292;&#20854;&#20013;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#34892;&#20026;&#20855;&#26377;&#39640;&#27010;&#29575;&#20986;&#29616;&#65292;&#23588;&#20854;&#26159;&#65288;&#21407;&#26412;&#20934;&#30830;&#30340;&#65289;&#27169;&#22411;&#23545;&#26131;&#20110;&#26500;&#36896;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#21516;&#26102;&#23481;&#26131;&#21463;&#21040;&#36755;&#20837;&#25968;&#25454;&#30340;&#38543;&#26426;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#19978;&#39564;&#35777;&#20102;&#30456;&#21516;&#29616;&#35937;&#22312;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30452;&#25509;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#20351;&#26159;&#22823;&#24133;&#24230;&#30340;&#21152;&#24615;&#38543;&#26426;&#22122;&#22768;&#20063;&#26080;&#27861;&#24178;&#25200;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks dramatically change the output of an otherwise accurate learning system using a seemingly inconsequential modification to a piece of input data. Paradoxically, empirical evidence indicates that even systems which are robust to large random perturbations of the input data remain susceptible to small, easily constructed, adversarial perturbations of their inputs. Here, we show that this may be seen as a fundamental feature of classifiers working with high dimensional input data. We introduce a simple generic and generalisable framework for which key behaviours observed in practical systems arise with high probability -- notably the simultaneous susceptibility of the (otherwise accurate) model to easily constructed adversarial attacks, and robustness to random perturbations of the input data. We confirm that the same phenomena are directly observed in practical neural networks trained on standard image classification problems, where even large additive random noise fai
&lt;/p&gt;</description></item><item><title>PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16911</link><description>&lt;p&gt;
PointLLM&#65306;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16911
&lt;/p&gt;
&lt;p&gt;
PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#65292;&#20294;&#22312;3D&#29702;&#35299;&#39046;&#22495;&#20173;&#26377;&#24453;&#23436;&#20840;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PointLLM&#65292;&#36825;&#26159;&#19968;&#39033;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#30340;&#21021;&#27493;&#24037;&#20316;&#65292;&#20351;LLM&#33021;&#22815;&#29702;&#35299;&#28857;&#20113;&#65292;&#24182;&#25552;&#20379;&#20102;&#36229;&#36234;2D&#35270;&#35273;&#25968;&#25454;&#30340;&#26032;&#36884;&#24452;&#12290;PointLLM&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#22788;&#29702;&#24102;&#26377;&#39068;&#33394;&#30340;&#29289;&#20307;&#28857;&#20113;&#65292;&#24182;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#28857;&#20113;&#21644;&#24120;&#35782;&#30340;&#25484;&#25569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#24378;&#22823;&#30340;LLM&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#20102;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;66&#19975;&#20010;&#31616;&#21333;&#21644;7&#19975;&#20010;&#22797;&#26434;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#65292;&#20197;&#23454;&#29616;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#23545;&#40784;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#23545;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unprecedented advancements in Large Language Models (LLMs) have created a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, thereby enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM processes colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: initially aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#26469;&#25429;&#33719;&#21608;&#22260;&#37051;&#23621;&#26234;&#33021;&#20307;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#20998;&#21487;&#35266;&#23519;MARL&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12653</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention. (arXiv:2304.12653v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#26469;&#25429;&#33719;&#21608;&#22260;&#37051;&#23621;&#26234;&#33021;&#20307;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#20998;&#21487;&#35266;&#23519;MARL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38590;&#20197;&#22312;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#24212;&#29992;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#22343;&#22330;&#29702;&#35770;&#25552;&#39640;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#35266;&#23519;&#21040;&#22266;&#23450;&#33539;&#22260;&#20869;&#30340;&#20854;&#20182;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#24433;&#21709;&#20102;&#26234;&#33021;&#20307;&#35780;&#20272;&#21608;&#22260;&#26234;&#33021;&#20307;&#34892;&#21160;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#24320;&#21457;&#19968;&#31181;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#33719;&#21462;&#26356;&#26377;&#25928;&#20449;&#24687;&#20197;&#36873;&#25321;&#26356;&#26377;&#25928;&#34892;&#21160;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#20197;&#21069;&#24037;&#20316;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#25110;&#21152;&#26435;&#22343;&#22330;&#26469;&#26356;&#26032;&#37051;&#23621;&#26234;&#33021;&#20307;&#24179;&#22343;&#34892;&#21160;&#65292;&#20294;&#23427;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#21608;&#22260;&#37051;&#23621;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#23548;&#33268;&#20102;&#23616;&#37096;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#23427;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#26469;&#25429;&#33719;&#21608;&#22260;&#37051;&#23621;&#26234;&#33021;&#20307;&#30340;&#29305;&#24449;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#20998;&#21487;&#35266;&#23519;MARL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional multi-agent reinforcement learning algorithms are difficultly applied in a large-scale multi-agent environment. The introduction of mean field theory has enhanced the scalability of multi-agent reinforcement learning in recent years. This paper considers partially observable multi-agent reinforcement learning (MARL), where each agent can only observe other agents within a fixed range. This partial observability affects the agent's ability to assess the quality of the actions of surrounding agents. This paper focuses on developing a method to capture more effective information from local observations in order to select more effective actions. Previous work in this field employs probability distributions or weighted mean field to update the average actions of neighborhood agents, but it does not fully consider the feature information of surrounding neighbors and leads to a local optimum. In this paper, we propose a novel multi-agent reinforcement learning algorithm, Partially
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#38376;&#25915;&#20987;&#19968;&#30452;&#26159;&#19968;&#20010;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#25913;&#36827;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#36824;&#27809;&#26377;&#25214;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#20173;&#28982;&#20540;&#24471;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2304.10985</link><description>&lt;p&gt;
&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#21551;&#21160;&#24378;&#38887;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Launching a Robust Backdoor Attack under Capability Constrained Scenarios. (arXiv:2304.10985v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10985
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#38376;&#25915;&#20987;&#19968;&#30452;&#26159;&#19968;&#20010;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#25913;&#36827;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#36824;&#27809;&#26377;&#25214;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#20173;&#28982;&#20540;&#24471;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20851;&#38190;&#39046;&#22495;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#21152;&#65292;&#20154;&#20204;&#24320;&#22987;&#25285;&#24515;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#12290;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#27745;&#26579;&#30340;&#21518;&#38376;&#27169;&#22411;&#22312;&#26222;&#36890;&#29615;&#22659;&#19979;&#21487;&#33021;&#34920;&#29616;&#27491;&#24120;&#65292;&#20294;&#24403;&#36755;&#20837;&#21253;&#21547;&#35302;&#21457;&#22120;&#26102;&#65292;&#20250;&#26174;&#31034;&#20986;&#24694;&#24847;&#34892;&#20026;&#12290;&#30446;&#21069;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#25913;&#21892;&#35302;&#21457;&#22120;&#30340;&#31192;&#23494;&#24615;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#20363;&#22914;&#23545;&#27169;&#22411;&#32467;&#26500;&#30340;&#20102;&#35299;&#25110;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;&#30001;&#20110;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#25915;&#20987;&#32773;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#33976;&#39311;&#24120;&#29992;&#20110;&#31616;&#21270;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20197;&#21069;&#30340;&#35768;&#22810;&#21518;&#38376;&#25915;&#20987;&#22312;&#27169;&#22411;&#33976;&#39311;&#21518;&#22343;&#22833;&#36133;;&#22270;&#20687;&#22686;&#24378;&#25805;&#20316;&#21487;&#20197;&#30772;&#22351;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#20351;&#21518;&#38376;&#25915;&#20987;&#22833;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks continue to be used in critical domains, concerns over their security have emerged. Deep learning models are vulnerable to backdoor attacks due to the lack of transparency. A poisoned backdoor model may perform normally in routine environments, but exhibit malicious behavior when the input contains a trigger. Current research on backdoor attacks focuses on improving the stealthiness of triggers, and most approaches require strong attacker capabilities, such as knowledge of the model structure or control over the training process. These attacks are impractical since in most cases the attacker's capabilities are limited. Additionally, the issue of model robustness has not received adequate attention. For instance, model distillation is commonly used to streamline model size as the number of parameters grows exponentially, and most of previous backdoor attacks failed after model distillation; the image augmentation operations can destroy the trigger and thus disabl
&lt;/p&gt;</description></item></channel></rss>