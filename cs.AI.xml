<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#23545;&#20316;&#29289;&#20998;&#31867;&#20855;&#26377;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.16582</link><description>&lt;p&gt;
&#22312;&#21033;&#29992;&#20840;&#29699;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#20316;&#29289;&#20998;&#31867;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20339;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16582
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#23545;&#20316;&#29289;&#20998;&#31867;&#20855;&#26377;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#29289;&#20998;&#31867;&#22312;&#30740;&#31350;&#20316;&#29289;&#27169;&#24335;&#21464;&#21270;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#30899;&#22266;&#23384;&#20013;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#36827;&#34892;&#39044;&#27979;&#26102;&#65292;&#21033;&#29992;&#21508;&#31181;&#26102;&#38388;&#25968;&#25454;&#28304;&#26159;&#24517;&#35201;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26144;&#23556;&#21040;&#39640;&#32423;&#34920;&#31034;&#20197;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#36755;&#20837;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#25991;&#29486;&#23545;&#22810;&#35270;&#22270;&#23398;&#20064;&#65288;MVL&#65289;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#25351;&#23548;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#25506;&#32034;&#20855;&#26377;&#29305;&#23450;&#32534;&#30721;&#22120;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#24182;&#22312;&#23616;&#37096;&#22320;&#21306;&#23545;&#20854;&#36827;&#34892;&#39564;&#35777;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#23545;&#20892;&#30000;&#22303;&#22320;&#21644;&#20316;&#29289;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#26102;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16582v1 Announce Type: cross  Abstract: Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration. When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary. Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction. However, they face substantial challenges when dealing with multiple input patterns. The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions. In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications. We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15412</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#27979;&#37327;&#21644;&#24314;&#27169;&#8220;&#25991;&#21270;&#8221;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring and Modeling "Culture" in LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15412
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21576;&#29616;&#20102;&#23545;39&#31687;&#26368;&#26032;&#35770;&#25991;&#30340;&#35843;&#26597;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27809;&#26377;&#19968;&#31687;&#30740;&#31350;&#23450;&#20041;&#8220;&#25991;&#21270;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#12289;&#22810;&#23618;&#38754;&#30340;&#27010;&#24565;&#65307;&#30456;&#21453;&#65292;&#23427;&#20204;&#22312;&#19968;&#20123;&#29305;&#21035;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20195;&#34920;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#38754;&#31216;&#20026;&#25991;&#21270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#22312;&#20154;&#21475;&#32479;&#35745;&#12289;&#35821;&#20041;&#21644;&#35821;&#35328;&#25991;&#21270;&#20132;&#20114;&#20195;&#29702;&#30340;&#19977;&#20010;&#32500;&#24230;&#19978;&#12290;&#25105;&#20204;&#36824;&#23545;&#37319;&#29992;&#30340;&#25506;&#26597;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21482;&#26377;&#8220;&#25991;&#21270;&#8221;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#22914;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#65292;&#34987;&#30740;&#31350;&#20102;&#65292;&#30041;&#19979;&#20102;&#20960;&#20010;&#20854;&#20182;&#26377;&#36259;&#19988;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22823;&#37327;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#65288;Hershcovich&#31561;&#20154;&#65292;2022&#65289;&#30340;&#26410;&#34987;&#25506;&#31350;&#12290;&#21478;&#22806;&#20004;&#20010;&#20851;&#38190;&#30340;&#31354;&#30333;&#26159;&#30446;&#21069;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#24773;&#22659;&#24615;&#30340;&#32570;&#20047;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15412v1 Announce Type: cross  Abstract: We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define "culture," which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture." We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of "culture," such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08763</link><description>&lt;p&gt;
&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simple and Scalable Strategies to Continually Pre-train Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08763
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25968;&#21313;&#20159;&#30340;&#26631;&#35760;&#19978;&#36827;&#34892;&#24120;&#35268;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#23601;&#37325;&#26032;&#24320;&#22987;&#35813;&#36807;&#31243;&#12290;&#19968;&#20010;&#26356;&#26377;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25345;&#32493;&#39044;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#33021;&#33410;&#30465;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#20197;&#21069;&#25968;&#25454;&#19978;&#38477;&#20302;&#24615;&#33021;&#25110;&#26080;&#27861;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#65288;LR&#65289;&#37325;&#26032;&#21319;&#28201;&#12289;LR&#37325;&#26032;&#34928;&#20943;&#21644;&#37325;&#25918;&#19978;&#19968;&#25968;&#25454;&#30340;&#32452;&#21512;&#36275;&#20197;&#19982;&#23436;&#20840;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20174;&#26368;&#32456;&#25439;&#22833;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22522;&#20934;&#30340;&#35282;&#24230;&#34913;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#8594;&#33521;&#35821;&#65289;&#20043;&#38388;&#30340;&#24369;&#20294;&#29616;&#23454;&#30340;&#20998;&#24067;&#36716;&#31227;&#20197;&#21450;&#26356;&#24378;&#28872;&#30340;&#20998;&#24067;&#36716;&#31227;&#65288;&#33521;&#35821;&#8594;&#24503;&#35821;&#65289;&#19979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#23618;&#20869;&#23384;&#28040;&#32791;&#22686;&#38271;&#24555;&#12289;&#38480;&#21046;&#36739;&#22823;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07711</link><description>&lt;p&gt;
SSM&#36935;&#19978;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;: &#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#19979;&#30340;&#39640;&#25928;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07711
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#23618;&#20869;&#23384;&#28040;&#32791;&#22686;&#38271;&#24555;&#12289;&#38480;&#21046;&#36739;&#22823;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22270;&#20687;&#29983;&#25104;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#30340;&#26174;&#33879;&#25104;&#23601;&#65292;&#30740;&#31350;&#30028;&#23545;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#35270;&#39057;&#29983;&#25104;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#29992;&#20110;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#26469;&#25552;&#21462;&#26102;&#38388;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#23618;&#30340;&#20869;&#23384;&#28040;&#32791;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#36825;&#31181;&#38480;&#21046;&#22312;&#23581;&#35797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26356;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#20250;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#12290;&#30001;&#20110;&#30456;&#23545;&#20110;&#24207;&#21015;&#38271;&#24230;&#65292;SSMs&#20855;&#26377;&#32447;&#24615;&#20869;&#23384;&#28040;&#32791;&#65292;&#26368;&#36817;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;UCF101&#36825;&#19968;&#35270;&#39057;&#29983;&#25104;&#30340;&#26631;&#20934;&#22522;&#20934;&#26469;&#35780;&#20272;&#25105;&#20204;&#22522;&#20110;SSM&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#25506;&#35752;SSMs&#22312;&#26356;&#38271;&#35270;&#39057;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07711v1 Announce Type: cross  Abstract: Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, 
&lt;/p&gt;</description></item><item><title>MOKA&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#24320;&#25918;&#35789;&#27719;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.03174</link><description>&lt;p&gt;
MOKA&#65306;&#22522;&#20110;&#26631;&#35760;&#30340;&#35270;&#35273;&#25552;&#31034;&#23454;&#29616;&#24320;&#25918;&#35789;&#27719;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03174
&lt;/p&gt;
&lt;p&gt;
MOKA&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#24320;&#25918;&#35789;&#27719;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#30340;&#27867;&#21270;&#35201;&#27714;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#28041;&#21450;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#29615;&#22659;&#20197;&#21450;&#20219;&#21153;&#30446;&#26631;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MOKA&#65288;Marking Open-vocabulary Keypoint Affordances&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#26469;&#35299;&#20915;&#30001;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#25551;&#36848;&#25351;&#23450;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03174v1 Announce Type: cross  Abstract: Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question. In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions. At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world. By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from br
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#37327;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#36827;&#34892;&#20943;&#36731;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22238;&#28335;&#27861;&#32416;&#27491;&#65292;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Correction with Backtracking Reduces Hallucination in Summarization. (arXiv:2310.16176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#37327;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#36827;&#34892;&#20943;&#36731;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#28304;&#25991;&#20214;&#30340;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#65292;&#26082;&#31616;&#27905;&#21448;&#20445;&#30041;&#37325;&#35201;&#20803;&#32032;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#31070;&#32463;&#25991;&#26412;&#25688;&#35201;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65288;&#25110;&#26356;&#20934;&#30830;&#22320;&#35828;&#26159;&#28151;&#28102;&#65289;&#65292;&#21363;&#29983;&#25104;&#30340;&#25688;&#35201;&#21253;&#21547;&#28304;&#25991;&#20214;&#20013;&#27809;&#26377;&#26681;&#25454;&#30340;&#32454;&#33410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#27493;&#39588;&#65306;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#36731;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#27979;&#37327;&#26377;&#20851;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#31616;&#21333;&#32479;&#35745;&#20449;&#24687;&#21487;&#20197;&#23454;&#29616;&#21069;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#22312;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#24778;&#20154;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstractive summarization aims at generating natural language summaries of a source document that are succinct while preserving the important elements. Despite recent advances, neural text summarization models are known to be susceptible to hallucinating (or more correctly confabulating), that is to produce summaries with details that are not grounded in the source document. In this paper, we introduce a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization. The approach is based on two steps: hallucination detection and mitigation. We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words. Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation. We thoroughly evaluate the proposed method with prior art on three benchmark datasets for text summarization. The results show that CoBa is effective and efficient in reducing hall
&lt;/p&gt;</description></item><item><title>Sentinel&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#24182;&#23450;&#20041;&#19968;&#20010;&#19977;&#27493;&#32858;&#21512;&#21327;&#35758;&#26469;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;Sentinel&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.08097</link><description>&lt;p&gt;
Sentinel: &#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#32858;&#21512;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Sentinel: An Aggregation Function to Secure Decentralized Federated Learning. (arXiv:2310.08097v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08097
&lt;/p&gt;
&lt;p&gt;
Sentinel&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#24182;&#23450;&#20041;&#19968;&#20010;&#19977;&#27493;&#32858;&#21512;&#21327;&#35758;&#26469;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;Sentinel&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24555;&#36895;&#25972;&#21512;&#21040;&#32593;&#32476;&#20013;&#28085;&#30422;&#20102;&#32593;&#32476;&#31649;&#29702;&#12289;&#26381;&#21153;&#36136;&#37327;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#21508;&#20010;&#26041;&#38754;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#21327;&#20316;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#28857;&#22833;&#25928;&#30340;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;FL&#21644;DFL&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24615;&#21463;&#21040;&#27745;&#26579;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23545;&#20854;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#38024;&#23545;&#38598;&#20013;&#24335;FL&#36827;&#34892;&#35774;&#35745;&#65292;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;DFL&#30340;&#29305;&#28857;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;Sentinel&#65292;&#19968;&#31181;&#22312;DFL&#20013;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;Sentinel&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#19977;&#27493;&#32858;&#21512;&#21327;&#35758;&#65292;&#21253;&#25324;&#30456;&#20284;&#24615;&#36807;&#28388;&#12289;&#24341;&#23548;&#39564;&#35777;&#21644;&#26631;&#20934;&#21270;&#65292;&#20197;&#38450;&#27490;&#24694;&#24847;&#27169;&#22411;&#26356;&#26032;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#23545;Sentinel&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid integration of Federated Learning (FL) into networking encompasses various aspects such as network management, quality of service, and cybersecurity while preserving data privacy. In this context, Decentralized Federated Learning (DFL) emerges as an innovative paradigm to train collaborative models, addressing the single point of failure limitation. However, the security and trustworthiness of FL and DFL are compromised by poisoning attacks, negatively impacting its performance. Existing defense mechanisms have been designed for centralized FL and they do not adequately exploit the particularities of DFL. Thus, this work introduces Sentinel, a defense strategy to counteract poisoning attacks in DFL. Sentinel leverages the accessibility of local data and defines a three-step aggregation protocol consisting of similarity filtering, bootstrap validation, and normalization to safeguard against malicious model updates. Sentinel has been evaluated with diverse datasets and various 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20223;&#21046;&#35895;&#27468;&#30524;&#21160;&#35770;&#25991;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#23454;&#29616;&#19982;&#35895;&#27468;&#35770;&#25991;&#30456;&#24403;&#30340;&#20934;&#30830;&#30524;&#21160;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.13495</link><description>&lt;p&gt;
&#24320;&#25918;&#27880;&#35270;&#65306;&#19968;&#20010;&#20223;&#21046;&#35895;&#27468;&#30524;&#21160;&#35770;&#25991;&#30340;&#24320;&#28304;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Open Gaze: An Open-Source Implementation Replicating Google's Eye Tracking Paper. (arXiv:2308.13495v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20223;&#21046;&#35895;&#27468;&#30524;&#21160;&#35770;&#25991;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#23454;&#29616;&#19982;&#35895;&#27468;&#35770;&#25991;&#30456;&#24403;&#30340;&#20934;&#30830;&#30524;&#21160;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30524;&#21160;&#24050;&#32463;&#25104;&#20026;&#35270;&#35273;&#30740;&#31350;&#12289;&#35821;&#35328;&#20998;&#26512;&#21644;&#21487;&#29992;&#24615;&#35780;&#20272;&#31561;&#19981;&#21516;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#19987;&#38376;&#30340;&#12289;&#26114;&#36149;&#30340;&#30524;&#21160;&#36861;&#36394;&#30828;&#20214;&#30340;&#25193;&#23637;&#24335;&#26700;&#38754;&#26174;&#31034;&#22120;&#19978;&#12290;&#23613;&#31649;&#26234;&#33021;&#25163;&#26426;&#30340;&#26222;&#21450;&#29575;&#21644;&#20351;&#29992;&#39057;&#29575;&#24456;&#39640;&#65292;&#20294;&#23545;&#20110;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#30524;&#29699;&#31227;&#21160;&#27169;&#24335;&#21364;&#40092;&#26377;&#35265;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#24320;&#28304;&#27880;&#35270;&#36861;&#36394;&#23454;&#29616;&#65292;&#27169;&#25311;&#20102;&#35895;&#27468;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#65288;&#20854;&#28304;&#20195;&#30721;&#20173;&#28982;&#26159;&#19987;&#26377;&#30340;&#65289;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#30828;&#20214;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#19982;&#35895;&#27468;&#35770;&#25991;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#26412;&#22320;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#20934;&#30830;&#30524;&#21160;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#31227;&#21160;&#30524;&#21160;&#36861;&#36394;&#22120;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye tracking has been a pivotal tool in diverse fields such as vision research, language analysis, and usability assessment. The majority of prior investigations, however, have concentrated on expansive desktop displays employing specialized, costly eye tracking hardware that lacks scalability. Remarkably little insight exists into ocular movement patterns on smartphones, despite their widespread adoption and significant usage. In this manuscript, we present an open-source implementation of a smartphone-based gaze tracker that emulates the methodology proposed by a GooglePaper (whose source code remains proprietary). Our focus is on attaining accuracy comparable to that attained through the GooglePaper's methodology, without the necessity for supplementary hardware. Through the integration of machine learning techniques, we unveil an accurate eye tracking solution that is native to smartphones. Our approach demonstrates precision akin to the state-of-the-art mobile eye trackers, which 
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.13565</link><description>&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65306;&#22522;&#30784;&#12289;&#29616;&#29366;&#12289;&#22522;&#20934;&#21644;&#26410;&#26469;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities. (arXiv:2307.13565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13565
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#35757;&#32451;&#27169;&#22411;&#20197;&#20248;&#21270;&#20915;&#31574;&#65292;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#20013;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#36825;&#20010;&#33539;&#24335;&#26377;&#26395;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20915;&#31574;&#21046;&#23450;&#65292;&#36825;&#20123;&#24212;&#29992;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36816;&#20316;&#65292;&#22312;&#36825;&#20123;&#20915;&#31574;&#27169;&#22411;&#20013;&#20272;&#35745;&#26410;&#30693;&#21442;&#25968;&#32463;&#24120;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#23545;DFL&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#23427;&#23545;&#21508;&#31181;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26681;&#25454;&#20854;&#29420;&#29305;&#29305;&#24449;&#26469;&#21306;&#20998;DFL&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;DFL&#30340;&#21512;&#36866;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;DFL&#30740;&#31350;&#20013;&#24403;&#21069;&#21644;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#35299;&#37322;&#26041;&#27861;&#8212;&#8212;BODEM&#65292;&#23427;&#37319;&#29992;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#29983;&#25104;&#22810;&#20010;&#29256;&#26412;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#27604;&#30446;&#21069;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.17249</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#35937;&#26816;&#27979;&#30340;&#27169;&#22411;&#26080;&#20851;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic explainable artificial intelligence for object detection in image data. (arXiv:2303.17249v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#35299;&#37322;&#26041;&#27861;&#8212;&#8212;BODEM&#65292;&#23427;&#37319;&#29992;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#29983;&#25104;&#22810;&#20010;&#29256;&#26412;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#27604;&#30446;&#21069;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#36890;&#36807;&#24320;&#21457;&#22823;&#22411;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#36879;&#26126;&#24230;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#22952;&#30861;&#36825;&#20123;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#20013;&#24320;&#21457;&#26041;&#27861;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#30340;&#34892;&#20026;&#12289;&#20915;&#31574;&#36923;&#36753;&#21644;&#28431;&#27934;&#12290;&#26412;&#25991;&#20026;&#20102;&#35299;&#37322;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23545;&#35937;&#26816;&#27979;&#31995;&#32479;&#35774;&#35745;&#21644;&#23454;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;Black-box Object Detection Explanation by Masking&#65288;BODEM&#65289;&#30340;&#40657;&#30418;&#35828;&#26126;&#26041;&#27861;&#65292;&#37319;&#29992;&#26032;&#30340;&#25513;&#34109;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#26469;&#29983;&#25104;&#36755;&#20837;&#22270;&#20687;&#30340;&#22810;&#20010;&#29256;&#26412;&#12290;&#23616;&#37096;&#25513;&#34109;&#29992;&#20110;&#24178;&#25200;&#30446;&#26631;&#23545;&#35937;&#20869;&#30340;&#20687;&#32032;&#65292;&#20197;&#20102;&#35299;&#23545;&#35937;&#26816;&#27979;&#22120;&#23545;&#36825;&#20123;&#21464;&#21270;&#30340;&#21453;&#24212;&#65292;&#32780;&#36828;&#31243;&#25513;&#34109;&#21017;&#29992;&#20110;&#30740;&#31350;&#23545;&#35937;&#26816;&#27979;&#22120;&#22312;&#22270;&#20687;&#32972;&#26223;&#19978;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;BODEM&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection is a fundamental task in computer vision, which has been greatly progressed through developing large and intricate deep learning models. However, the lack of transparency is a big challenge that may not allow the widespread adoption of these models. Explainable artificial intelligence is a field of research where methods are developed to help users understand the behavior, decision logics, and vulnerabilities of AI-based systems. Black-box explanation refers to explaining decisions of an AI system without having access to its internals. In this paper, we design and implement a black-box explanation method named Black-box Object Detection Explanation by Masking (BODEM) through adopting a new masking approach for AI-based object detection systems. We propose local and distant masking to generate multiple versions of an input image. Local masks are used to disturb pixels within a target object to figure out how the object detector reacts to these changes, while distant ma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#32467;&#26500;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#30772;&#22351;&#30495;&#23454;&#30340;&#31354;&#38388;&#32467;&#26500;&#34920;&#31034;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.11783</link><description>&lt;p&gt;
&#36731;&#37327;&#32423;&#23545;&#27604;&#34507;&#30333;&#36136;&#32467;&#26500;-&#24207;&#21015;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Lightweight Contrastive Protein Structure-Sequence Transformation. (arXiv:2303.11783v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#32467;&#26500;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#30772;&#22351;&#30495;&#23454;&#30340;&#31354;&#38388;&#32467;&#26500;&#34920;&#31034;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#34507;&#30333;&#36136;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#26080;&#26631;&#31614;&#30340;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#32467;&#26500;&#27169;&#22411;&#26159;&#20851;&#38190;&#22522;&#30784;&#12290;&#20256;&#32479;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#26041;&#27861;&#36981;&#24490;&#25104;&#29087;&#30340;&#33258;&#28982;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20363;&#22914;&#21435;&#22122;&#37325;&#26500;&#21644;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65292;&#20294;&#36890;&#24120;&#20250;&#30772;&#22351;&#30495;&#23454;&#30340;&#31354;&#38388;&#32467;&#26500;&#34920;&#31034;&#12290;&#20854;&#20182;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#21487;&#33021;&#20250;&#39044;&#27979;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#23450;&#23545;&#35937;&#31867;&#21035;&#65292;&#20854;&#20013;&#21463;&#38480;&#30340;&#30417;&#30563;&#26041;&#24335;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#25351;&#23450;&#20219;&#20309;&#20854;&#20182;&#30340;&#34507;&#30333;&#36136;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#24378;&#22823;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#35758;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#23545;&#40784;&#26469;&#25351;&#23548;&#32467;&#26500;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#32467;&#26500;&#32422;&#26463;&#65292;&#20197;&#36827;&#19968;&#27493;&#23398;&#20064;&#20869;&#22312;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained protein structure models without labels are crucial foundations for the majority of protein downstream applications. The conventional structure pretraining methods follow the mature natural language pretraining methods such as denoised reconstruction and masked language modeling but usually destroy the real representation of spatial structures. The other common pretraining methods might predict a fixed set of predetermined object categories, where a restricted supervised manner limits their generality and usability as additional labeled data is required to specify any other protein concepts. In this work, we introduce a novel unsupervised protein structure representation pretraining with a robust protein language model. In particular, we first propose to leverage an existing pretrained language model to guide structure model learning through an unsupervised contrastive alignment. In addition, a self-supervised structure constraint is proposed to further learn the intrinsic i
&lt;/p&gt;</description></item></channel></rss>