<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#29983;&#25104;&#27169;&#22411;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#21382;&#21490;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#22797;&#26434;&#25968;&#25454;&#65292;&#20026;&#25512;&#33616;&#31995;&#32479;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00579</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#65288;Gen-RecSys&#65289;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00579
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#21382;&#21490;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#22797;&#26434;&#25968;&#25454;&#65292;&#20026;&#25512;&#33616;&#31995;&#32479;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#36890;&#24120;&#20351;&#29992;&#29992;&#25143;-&#39033;&#30446;&#35780;&#20998;&#21382;&#21490;&#20316;&#20026;&#20027;&#35201;&#25968;&#25454;&#26469;&#28304;&#65292;&#21327;&#21516;&#36807;&#28388;&#26159;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#27169;&#22411;&#26368;&#36817;&#24050;&#32463;&#20855;&#22791;&#20102;&#24314;&#27169;&#21644;&#37319;&#26679;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#21487;&#20197;&#28085;&#30422;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#21382;&#21490;&#65292;&#36824;&#21487;&#20197;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#20026;&#26032;&#39062;&#30340;&#25512;&#33616;&#20219;&#21153;&#35299;&#38145;&#20102;&#20016;&#23500;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#36825;&#31687;&#20840;&#38754;&#30340;&#12289;&#22810;&#23398;&#31185;&#30340;&#35843;&#30740;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65288;Gen-RecSys&#65289;&#22312;RS&#20013;&#30340;&#20851;&#38190;&#36827;&#23637;&#65292;&#21253;&#25324;&#65306;&#22522;&#20110;&#20132;&#20114;&#39537;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;&#27010;&#36848;&#65307;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#25512;&#33616;&#12289;&#26816;&#32034;&#21644;&#23545;&#35805;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;&#65307;&#20197;&#21450;&#29992;&#20110;&#22788;&#29702;&#21644;&#29983;&#25104;RS&#20013;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#30340;&#25972;&#20307;&#35270;&#35282;&#20351;&#25105;&#20204;&#33021;&#22815;&#24378;&#35843;&#35780;&#20272;&#25152;&#38656;&#33539;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00579v1 Announce Type: cross  Abstract: Traditional recommender systems (RS) have used user-item rating histories as their primary data source, with collaborative filtering being one of the principal methods. However, generative models have recently developed abilities to model and sample from complex data distributions, including not only user-item interaction histories but also text, images, and videos - unlocking this rich data for novel recommendation tasks. Through this comprehensive and multi-disciplinary survey, we aim to connect the key advancements in RS using Generative Models (Gen-RecSys), encompassing: a foundational overview of interaction-driven generative models; the application of large language models (LLM) for generative recommendation, retrieval, and conversational recommendation; and the integration of multimodal models for processing and generating image and video content in RS. Our holistic perspective allows us to highlight necessary paradigms for eval
&lt;/p&gt;</description></item><item><title>DexCap&#26159;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#32467;&#21512;DexIL&#31639;&#27861;&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20855;&#26377;&#31934;&#30830;&#36861;&#36394;&#21644;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07788</link><description>&lt;p&gt;
DexCap&#65306;&#29992;&#20110;&#28789;&#24039;&#25805;&#20316;&#30340;&#21487;&#25193;&#23637;&#21644;&#21487;&#31227;&#26893;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07788
&lt;/p&gt;
&lt;p&gt;
DexCap&#26159;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#32467;&#21512;DexIL&#31639;&#27861;&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20855;&#26377;&#31934;&#30830;&#36861;&#36394;&#21644;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#26159;&#20026;&#26426;&#22120;&#20154;&#36171;&#20104;&#31867;&#20154;&#28789;&#24039;&#22312;&#29616;&#23454;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#36884;&#24452;&#65292;&#28982;&#32780;&#65292;&#29616;&#23384;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#30340;&#21487;&#31227;&#26893;&#24615;&#20197;&#21450;&#23558;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#36716;&#21270;&#20026;&#26377;&#25928;&#25511;&#21046;&#31574;&#30053;&#30340;&#22256;&#38590;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DexCap&#65292;&#19968;&#20010;&#20415;&#25658;&#24335;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#20197;&#21450;DexIL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#20223;&#31639;&#27861;&#65292;&#21487;&#30452;&#25509;&#20174;&#20154;&#31867;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#35757;&#32451;&#28789;&#24039;&#26426;&#22120;&#20154;&#25216;&#33021;&#12290;DexCap&#22522;&#20110;SLAM&#21644;&#30005;&#30913;&#22330;&#20197;&#21450;&#29615;&#22659;&#30340;3D&#35266;&#23519;&#65292;&#25552;&#20379;&#20102;&#23545;&#25163;&#33109;&#21644;&#25163;&#25351;&#36816;&#21160;&#30340;&#31934;&#30830;&#12289;&#25239;&#36974;&#25377;&#30340;&#36319;&#36394;&#12290;&#21033;&#29992;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;DexIL&#37319;&#29992;&#36870;&#36816;&#21160;&#23398;&#21644;&#22522;&#20110;&#28857;&#20113;&#30340;&#27169;&#20223;&#23398;&#20064;&#26469;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#19982;&#26426;&#22120;&#20154;&#25163;&#12290;&#38500;&#20102;&#20174;&#20154;&#31867;&#36816;&#21160;&#20013;&#23398;&#20064;&#22806;&#65292;DexCap&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07788v1 Announce Type: cross  Abstract: Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an op
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#31227;&#21160;&#35745;&#31639;&#20013;&#30340;&#25506;&#32034;&#21644;&#20250;&#21512;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#21035;&#33021;&#22312;&#22270;&#20013;$m$&#20010;&#21516;&#27493;&#26102;&#38388;&#27493;&#23454;&#29616;&#38598;&#20307;&#25506;&#32034;&#21644;$\frac{3}{2}m$&#26102;&#38388;&#27493;&#20869;&#23454;&#29616;&#20250;&#21512;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07748</link><description>&lt;p&gt;
&#38463;&#29790;&#38463;&#24503;&#28037;&#21644;&#24530;&#20462;&#26031;&#65306;&#22312;&#26410;&#30693;&#22270;&#20013;&#25506;&#32034;&#21644;&#20250;&#21512;&#30340;&#20004;&#20010;&#31227;&#21160;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents in an Unknown Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07748
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#31227;&#21160;&#35745;&#31639;&#20013;&#30340;&#25506;&#32034;&#21644;&#20250;&#21512;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#21035;&#33021;&#22312;&#22270;&#20013;$m$&#20010;&#21516;&#27493;&#26102;&#38388;&#27493;&#23454;&#29616;&#38598;&#20307;&#25506;&#32034;&#21644;$\frac{3}{2}m$&#26102;&#38388;&#27493;&#20869;&#23454;&#29616;&#20250;&#21512;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#31227;&#21160;&#35745;&#31639;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#25506;&#32034;&#21644;&#20250;&#21512;&#65292;&#28041;&#21450;&#21040;&#19968;&#20010;&#26410;&#30693;&#22270;&#20013;&#30340;&#20004;&#20010;&#19981;&#21516;&#31227;&#21160;&#20195;&#29702;&#12290;&#36825;&#20004;&#20010;&#20195;&#29702;&#21487;&#20197;&#22312;&#25152;&#26377;&#33410;&#28857;&#19978;&#30340;&#30333;&#26495;&#19978;&#35835;&#20889;&#20449;&#24687;&#12290;&#23427;&#20204;&#27599;&#19968;&#27493;&#37117;&#27839;&#30528;&#19968;&#20010;&#30456;&#37051;&#30340;&#36793;&#31227;&#21160;&#12290;&#22312;&#25506;&#32034;&#38382;&#39064;&#20013;&#65292;&#20004;&#20010;&#20195;&#29702;&#20174;&#22270;&#20013;&#30456;&#21516;&#30340;&#33410;&#28857;&#20986;&#21457;&#65292;&#24517;&#39035;&#36941;&#21382;&#25152;&#26377;&#30340;&#36793;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#19968;&#20010;&#31616;&#21333;&#21464;&#20307;&#21487;&#20197;&#22312;$m$&#20010;&#21516;&#27493;&#26102;&#38388;&#27493;&#20013;&#23454;&#29616;&#38598;&#20307;&#25506;&#32034;&#65292;&#20854;&#20013;$m$&#26159;&#22270;&#30340;&#36793;&#25968;&#12290;&#36825;&#25552;&#39640;&#20102;&#38598;&#20307;&#22270;&#25506;&#32034;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;&#22312;&#20250;&#21512;&#38382;&#39064;&#20013;&#65292;&#20195;&#29702;&#20174;&#22270;&#20013;&#19981;&#21516;&#30340;&#33410;&#28857;&#20986;&#21457;&#65292;&#24517;&#39035;&#23613;&#24555;&#30456;&#36935;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#20445;&#35777;&#22312;&#33267;&#22810;$\frac{3}{2}m$&#20010;&#26102;&#38388;&#27493;&#20869;&#20250;&#21512;&#12290;&#36825;&#27604;&#25152;&#35859;&#30340;&#8220;&#31561;&#22920;&#22920;&#8221;&#31639;&#27861;&#38656;&#27714;&#30340;$2m$&#26102;&#38388;&#27493;&#26356;&#22909;&#12290;&#25105;&#20204;&#25152;&#26377;&#30340;&#20445;&#35777;&#37117;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07748v1 Announce Type: cross  Abstract: We investigate two fundamental problems in mobile computing: exploration and rendezvous, with two distinct mobile agents in an unknown graph. The agents can read and write information on whiteboards that are located at all nodes. They both move along one adjacent edge at every time-step. In the exploration problem, both agents start from the same node of the graph and must traverse all of its edges. We show that a simple variant of depth-first search achieves collective exploration in $m$ synchronous time-steps, where $m$ is the number of edges of the graph. This improves the competitive ratio of collective graph exploration. In the rendezvous problem, the agents start from different nodes of the graph and must meet as fast as possible. We introduce an algorithm guaranteeing rendezvous in at most $\frac{3}{2}m$ time-steps. This improves over the so-called `wait for Mommy' algorithm which requires $2m$ time-steps. All our guarantees are
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.07322</link><description>&lt;p&gt;
&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07322
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#22312;&#36890;&#36807;&#20998;&#26512;&#23398;&#29983;&#21382;&#21490;&#23398;&#20064;&#36807;&#31243;&#26469;&#39044;&#27979;&#20854;&#26410;&#26469;&#34920;&#29616;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#30693;&#35782;&#36861;&#36394;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#27169;&#25311;&#30693;&#35782;&#36861;&#36394;&#36807;&#31243;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#23558;&#38382;&#39064;&#30340;&#20010;&#20307;&#20449;&#24687;&#34701;&#20837;&#24314;&#27169;&#20013;&#12290;&#36825;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#23613;&#31649;&#38382;&#39064;&#20849;&#20139;&#30456;&#21516;&#30340;&#30693;&#35782;&#32452;&#20214;&#65288;KC&#65289;&#65292;&#20294;&#23398;&#29983;&#23545;&#21516;&#36136;&#38382;&#39064;&#30340;&#30693;&#35782;&#20064;&#24471;&#21487;&#20197;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#22312;&#20110;&#35299;&#37322;&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#34429;&#28982;&#21487;&#33021;&#24182;&#19981;&#38656;&#35201;&#23436;&#20840;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#20851;&#38190;&#26159;&#20197;&#32769;&#24072;&#33021;&#29702;&#35299;&#30340;&#26041;&#24335;&#21576;&#29616;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07322v1 Announce Type: cross  Abstract: Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.06725</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#37325;&#35201;&#24615;&#26426;&#21046;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#22522;&#20110;&#23398;&#29983;&#30340;&#21382;&#21490;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#30693;&#35782;&#25484;&#25569;&#31243;&#24230;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;KT&#65288;DLKT&#65289;&#26041;&#27861;&#22312;KT&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#22914;&#39044;&#31639;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35266;&#23519;&#21040;&#30340;&#20114;&#21160;&#38750;&#24120;&#26377;&#38480;&#65292;&#21363;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#12290;&#30452;&#25509;&#22312;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DLKT&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#24456;&#38590;&#36873;&#25321;&#36866;&#24403;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;KT&#26694;&#26550;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;&#21463;&#30427;&#34892;&#30340;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-4&#29983;&#25104;&#32534;&#31243;&#32451;&#20064;&#21453;&#39304;&#30340;&#36136;&#37327;&#65292;&#23545;&#21253;&#21547;&#32534;&#31243;&#20219;&#21153;&#35268;&#33539;&#21644;&#23398;&#29983;&#25552;&#20132;&#20869;&#23481;&#30340;&#25552;&#31034;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.04449</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#29983;&#25104;&#32534;&#31243;&#32451;&#20064;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Feedback-Generation for Programming Exercises With GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-4&#29983;&#25104;&#32534;&#31243;&#32451;&#20064;&#21453;&#39304;&#30340;&#36136;&#37327;&#65292;&#23545;&#21253;&#21547;&#32534;&#31243;&#20219;&#21153;&#35268;&#33539;&#21644;&#23398;&#29983;&#25552;&#20132;&#20869;&#23481;&#30340;&#25552;&#31034;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21450;&#30456;&#20851;&#24212;&#29992;&#24191;&#27867;&#21487;&#29992;&#20197;&#26469;&#65292;&#26377;&#20960;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#23427;&#20204;&#22312;&#21327;&#21161;&#25945;&#32946;&#24037;&#20316;&#32773;&#21644;&#25903;&#25345;&#39640;&#31561;&#25945;&#32946;&#23398;&#29983;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20687;Codex&#12289;GPT-3.5&#21644;GPT 4&#36825;&#26679;&#30340;LLMs&#22312;&#22823;&#22411;&#32534;&#31243;&#35838;&#31243;&#30340;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23398;&#29983;&#21487;&#20197;&#20174;&#21450;&#26102;&#19988;&#22823;&#35268;&#27169;&#25552;&#20379;&#30340;&#21453;&#39304;&#21644;&#25552;&#31034;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;GPT-4 Turbo&#20026;&#21253;&#21547;&#32534;&#31243;&#20219;&#21153;&#35268;&#33539;&#21644;&#23398;&#29983;&#25552;&#20132;&#20869;&#23481;&#30340;&#25552;&#31034;&#29983;&#25104;&#36755;&#20986;&#30340;&#36136;&#37327;&#12290;&#20174;&#19968;&#38376;&#20837;&#38376;&#32423;&#32534;&#31243;&#35838;&#31243;&#20013;&#36873;&#25321;&#20102;&#20004;&#39033;&#20316;&#19994;&#65292;&#24182;&#35201;&#27714;GPT-4&#20026;&#38543;&#26426;&#36873;&#25321;&#30340;55&#20221;&#30495;&#23454;&#23398;&#29983;&#32534;&#31243;&#25552;&#20132;&#29983;&#25104;&#21453;&#39304;&#12290;&#23545;&#36755;&#20986;&#36827;&#34892;&#20102;&#20851;&#20110;&#27491;&#30830;&#24615;&#12289;&#20010;&#24615;&#21270;&#12289;&#25925;&#38556;&#23450;&#20301;&#21644;&#26448;&#26009;&#20013;&#35782;&#21035;&#30340;&#20854;&#20182;&#29305;&#24449;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#20197;&#21450;GPT-3.5&#30340;&#20998;&#26512;&#30456;&#27604;&#65292;GPT-4
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04449v1 Announce Type: new  Abstract: Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input. Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of GPT-3.5, GPT-4 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00791</link><description>&lt;p&gt;
$\textit{L+M-24}$&#65306;&#22312;ACL 2024&#24180;&#20026;&#35821;&#35328;+&#20998;&#23376;&#26500;&#24314;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
$\textit{L+M-24}$: Building a Dataset for Language + Molecules @ ACL 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00791
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;-&#20998;&#23376;&#27169;&#22411;&#24050;&#25104;&#20026;&#20998;&#23376;&#21457;&#29616;&#21644;&#29702;&#35299;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#23376;-&#35821;&#35328;&#23545;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#24050;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26377;&#20197;&#19979;&#20960;&#31181;&#31867;&#22411;&#65306;1) &#23567;&#35268;&#27169;&#19988;&#20174;&#29616;&#26377;&#25968;&#25454;&#24211;&#20013;&#25235;&#21462;&#65292;2) &#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#19988;&#36890;&#36807;&#22312;&#31185;&#23398;&#25991;&#29486;&#19978;&#25191;&#34892;&#23454;&#20307;&#38142;&#25509;&#26469;&#26500;&#24314;&#65292;3) &#36890;&#36807;&#23558;&#23646;&#24615;&#39044;&#27979;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20351;&#29992;&#27169;&#26495;&#32780;&#26500;&#24314;&#12290;&#22312;&#26412;&#25991;&#26723;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#21019;&#24314;&#30340;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#12290;&#29305;&#21035;&#22320;&#65292;$\textit{L+M-24}$&#26088;&#22312;&#38598;&#20013;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#39033;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00791v1 Announce Type: cross  Abstract: Language-molecule models have emerged as an exciting direction for molecular discovery and understanding. However, training these models is challenging due to the scarcity of molecule-language pair datasets. At this point, datasets have been released which are 1) small and scraped from existing databases, 2) large but noisy and constructed by performing entity linking on the scientific literature, and 3) built by converting property prediction datasets to natural language using templates. In this document, we detail the $\textit{L+M-24}$ dataset, which has been created for the Language + Molecules Workshop shared task at ACL 2024. In particular, $\textit{L+M-24}$ is designed to focus on three key benefits of natural language in molecule design: compositionality, functionality, and abstraction.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#24378;&#21270;&#23398;&#20064;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21644;&#21518;&#24724;-&#21069;&#36827;&#20196;&#29260;&#26469;&#33719;&#21462;&#20219;&#21153;&#20449;&#24687;&#24182;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.17423</link><description>&lt;p&gt;
&#21152;&#24378;&#19978;&#19979;&#25991;&#40657;&#30418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reinforced In-Context Black-Box Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17423
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#24378;&#21270;&#23398;&#20064;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21644;&#21518;&#24724;-&#21069;&#36827;&#20196;&#29260;&#26469;&#33719;&#21462;&#20219;&#21153;&#20449;&#24687;&#24182;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#24050;&#32463;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#20803;&#23398;&#20064;BBO&#31639;&#27861;&#30340;&#29305;&#23450;&#32452;&#20214;&#65292;&#20197;&#21152;&#24555;&#20248;&#21270;&#36895;&#24230;&#24182;&#25670;&#33073;&#32321;&#29712;&#30340;&#25163;&#24037;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#20316;&#20026;&#25193;&#23637;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25972;&#20010;&#31639;&#27861;&#38656;&#35201;&#19987;&#23478;&#26368;&#23569;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20379;&#26368;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RIBBO&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#24378;&#21270;&#23398;&#20064;BBO&#31639;&#27861;&#12290;RIBBO&#21033;&#29992;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#26469;&#23398;&#20064;&#22810;&#20010;&#34892;&#20026;&#31639;&#27861;&#21644;&#20219;&#21153;&#20135;&#29983;&#30340;&#20248;&#21270;&#21382;&#21490;&#65292;&#21033;&#29992;&#22823;&#22411;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#25552;&#21462;&#20219;&#21153;&#20449;&#24687;&#24182;&#30456;&#24212;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#22686;&#21152;&#21518;&#24724;-&#21069;&#36827;&#20196;&#29260;&#26469;&#22686;&#24378;&#20248;&#21270;&#21382;&#21490;&#65292;&#36825;&#20123;&#20196;&#29260;&#26088;&#22312;&#22522;&#20110;&#32047;&#31215;&#34920;&#29616;&#26469;&#34920;&#31034;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17423v1 Announce Type: cross  Abstract: Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering. Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics. As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility. In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion. RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly. Central to our method is to augment the optimization histories with regret-to-go tokens, which are designed to represent the performance of an algorithm based on cumul
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.17398</link><description>&lt;p&gt;
&#37327;&#23376;&#26041;&#27861;&#30740;&#31350;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;
&lt;/p&gt;
&lt;p&gt;
A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17398
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;Quantum-SMOTE&#21463;&#21040;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#37327;&#23376;&#36807;&#31243;&#22914;&#20132;&#25442;&#27979;&#35797;&#21644;&#37327;&#23376;&#26059;&#36716;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#28857;&#12290;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;SMOTE&#31639;&#27861;&#20351;&#29992;K-&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#21644;&#27431;&#27663;&#36317;&#31163;&#30340;&#26041;&#24335;&#26377;&#25152;&#19981;&#21516;&#65292;&#33021;&#22815;&#20174;&#23569;&#25968;&#31867;&#25968;&#25454;&#28857;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#37051;&#36817;&#24615;&#12290;&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#38656;&#27714;&#30340;&#23450;&#21046;&#12290;&#35813;&#26041;&#27861;&#22312;TelecomChurn&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#20004;&#31181;&#20027;&#35201;&#30340;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17398v1 Announce Type: cross  Abstract: The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation. The process varies from the conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity. The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements. The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification
&lt;/p&gt;</description></item><item><title>LoRA+&#36890;&#36807;&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#25913;&#36827;&#21407;&#22987;LoRA&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#24494;&#35843;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12354</link><description>&lt;p&gt;
LoRA+: &#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#39640;&#25928;&#20302;&#31209;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoRA+: Efficient Low Rank Adaptation of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12354
&lt;/p&gt;
&lt;p&gt;
LoRA+&#36890;&#36807;&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#25913;&#36827;&#21407;&#22987;LoRA&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#24494;&#35843;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26368;&#21021;&#30001;&#32993;&#31561;&#20154;&#65288;2021&#24180;&#65289;&#24341;&#20837;&#65292;&#23548;&#33268;&#23545;&#20855;&#26377;&#22823;&#23485;&#24230;&#65288;&#23884;&#20837;&#32500;&#24230;&#65289;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#34920;&#29616;&#20122;&#20248;&#12290;&#36825;&#26159;&#22240;&#20026;LoRA&#20013;&#30340;&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#29575;&#36827;&#34892;&#26356;&#26032;&#12290;&#36890;&#36807;&#23545;&#22823;&#23485;&#24230;&#32593;&#32476;&#36827;&#34892;&#32553;&#25918;&#21442;&#25968;&#30340;&#35770;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#29575;&#19981;&#21033;&#20110;&#26377;&#25928;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;LoRA&#30340;&#36825;&#31181;&#27425;&#20248;&#24615;&#21487;&#20197;&#31616;&#21333;&#22320;&#36890;&#36807;&#20026;LoRA&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#20197;&#21450;&#19968;&#20010;&#31934;&#24515;&#36873;&#25321;&#30340;&#27604;&#29575;&#26469;&#36827;&#34892;&#26657;&#27491;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25552;&#20986;&#30340;&#31639;&#27861;&#31216;&#20026;LoRA$+$&#12290;&#22312;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;LoRA$+$&#22312;&#30456;&#21516;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#39640;&#20102;&#24615;&#33021;&#65288;1-2&#65285;&#30340;&#25913;&#36827;&#65289;&#21644;&#24494;&#35843;&#36895;&#24230;&#65288;&#26368;&#22810;&#25552;&#36895;&#32422;2&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12354v1 Announce Type: cross  Abstract: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#65292;&#38381;&#28304;&#27169;&#22411;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#19982;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.11997</link><description>&lt;p&gt;
&#22238;&#24518;&#37027;&#19968;&#24180;&#21457;&#29983;&#30340;&#20107;&#20214;&#65311;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11997
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#65292;&#38381;&#28304;&#27169;&#22411;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#19982;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#25512;&#29702;&#21644;&#20445;&#30041;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#20107;&#20214;&#30340;&#39034;&#24207;&#24615;&#23545;&#20851;&#38190;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#25968;&#25454;&#38598;\textbf{TempUN}&#19978;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26102;&#38388;&#20445;&#30041;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#33879;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38381;&#28304;&#27169;&#22411;&#26356;&#39057;&#32321;&#22320;&#26174;&#31034;&#20986;&#30693;&#35782;&#24046;&#36317;&#65292;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#21644;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;&#24182;&#27809;&#26377;&#24102;&#26469;&#20027;&#35201;&#24615;&#33021;&#25913;&#36827;&#12290;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#33719;&#24471;&#65288;https://github.com/lingoiitgn/TempUN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11997v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various fine-tuning approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN).
&lt;/p&gt;</description></item><item><title>ScreenAI&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#20462;&#34917;&#31574;&#30053;&#21644;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#20197;&#21450;&#38024;&#23545;UI&#20803;&#32032;&#30340;&#23631;&#24149;&#27880;&#35299;&#20219;&#21153;&#30340;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04615</link><description>&lt;p&gt;
ScreenAI: &#29992;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ScreenAI: A Vision-Language Model for UI and Infographics Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04615
&lt;/p&gt;
&lt;p&gt;
ScreenAI&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#20462;&#34917;&#31574;&#30053;&#21644;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#20197;&#21450;&#38024;&#23545;UI&#20803;&#32032;&#30340;&#23631;&#24149;&#27880;&#35299;&#20219;&#21153;&#30340;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23631;&#24149;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#21644;&#20449;&#24687;&#22270;&#34920;&#22312;&#20154;&#31867;&#27807;&#36890;&#21644;&#20154;&#26426;&#20132;&#20114;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#20849;&#20139;&#30456;&#20284;&#30340;&#35270;&#35273;&#35821;&#35328;&#21644;&#35774;&#35745;&#21407;&#21017;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ScreenAI&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25913;&#36827;&#20102;PaLI&#26550;&#26500;&#65292;&#37319;&#29992;&#20102;pix2struct&#30340;&#28789;&#27963;&#20462;&#34917;&#31574;&#30053;&#65292;&#24182;&#32463;&#36807;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#12290;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#26680;&#24515;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#23631;&#24149;&#27880;&#35299;&#20219;&#21153;&#65292;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;UI&#20803;&#32032;&#30340;&#31867;&#22411;&#21644;&#20301;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25991;&#26412;&#27880;&#35299;&#26469;&#25551;&#36848;&#23631;&#24149;&#65292;&#24182;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#38382;&#31572;&#65288;QA&#65289;&#65292;UI&#23548;&#33322;&#21644;&#25688;&#35201;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#20197;&#23637;&#31034;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#22312;&#20165;&#26377;5B&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;ScreenAI&#22312;&#22522;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#30340;&#20219;&#21153;&#65288;&#22810;&#39029;&#25991;&#26723;VQA&#65292;WebSRC&#65292;MoTIF&#21644;Widget&#23383;&#24149;&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.01695</link><description>&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language-Guided World Models: A Model-Based Approach to AI Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01695
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#23433;&#35013;&#21040;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20013;&#65292;&#20026;&#20154;&#31867;&#19982;&#36825;&#20123;&#20195;&#29702;&#27807;&#36890;&#21644;&#25511;&#21046;&#25171;&#24320;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#28192;&#36947;&#12290;&#38500;&#20102;&#26356;&#26032;&#20195;&#29702;&#31574;&#30053;&#65292;&#20154;&#31867;&#36824;&#21487;&#20197;&#20462;&#25913;&#20182;&#20204;&#30340;&#20869;&#37096;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#24433;&#21709;&#20195;&#29702;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#38590;&#20197;&#36866;&#24212;&#20154;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#33258;&#28982;&#30340;&#36890;&#20449;&#30028;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;&#23427;&#20204;&#36824;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#26368;&#21021;&#29992;&#20110;&#25351;&#23548;&#20154;&#31867;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#33258;&#25105;&#23398;&#20064;&#12290;&#20026;&#20102;&#20419;&#36827;LWMs&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;MESSENGER&#28216;&#25103;&#65288;Hanjie&#31561;&#20154;&#65292;2021&#65289;&#30340;&#25361;&#25112;&#22522;&#20934;&#65292;&#38656;&#35201;&#23545;&#26032;&#22330;&#26223;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;GPT-4&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#23384;&#22312;&#22522;&#26412;&#38169;&#35823;&#65292;&#24182;&#19988;&#21363;&#20351;&#26159;GPT-4&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2401.17169</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Conditional and Modal Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;GPT-4&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#23384;&#22312;&#22522;&#26412;&#38169;&#35823;&#65292;&#24182;&#19988;&#21363;&#20351;&#26159;GPT-4&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#30740;&#31350;&#27491;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#35748;&#30693;&#31185;&#23398;&#39046;&#22495;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21313;&#20960;&#20010;LLM&#33021;&#21542;&#21306;&#20998;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#25512;&#35770;&#21644;&#36923;&#36753;&#19978;&#33618;&#35884;&#30340;&#25512;&#35770;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#28041;&#21450;&#26465;&#20214;&#21477;&#65288;&#20363;&#22914;&#65292;&#8220;&#22914;&#26524;&#23433;&#26377;&#19968;&#20010;&#30343;&#21518;&#65292;&#37027;&#20040;&#40077;&#21187;&#26377;&#19968;&#20010;J&#29260;&#8221;&#65289;&#21644;&#35748;&#35782;&#24773;&#24577;&#65288;&#20363;&#22914;&#65292;&#8220;&#23433;&#21487;&#33021;&#26377;&#19968;&#20010;A&#29260;&#8221;&#65292;&#8220;&#40077;&#21187;&#24517;&#39035;&#26377;&#19968;&#20010;K&#29260;&#8221;&#65289;&#30340;&#25512;&#29702;&#27169;&#24335;&#12290;&#36825;&#20123;&#25512;&#29702;&#27169;&#24335;&#23545;&#20110;&#36923;&#36753;&#23398;&#23478;&#12289;&#21746;&#23398;&#23478;&#21644;&#35821;&#35328;&#23398;&#23478;&#26469;&#35828;&#20855;&#26377;&#29305;&#27530;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#22312;&#20154;&#31867;&#25512;&#29702;&#20013;&#25198;&#28436;&#19968;&#20010;&#26680;&#24515;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;LLM&#22312;&#36825;&#20123;&#25512;&#29702;&#27169;&#24335;&#19978;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#30456;&#21305;&#37197;&#26159;&#38750;&#24120;&#30456;&#20851;&#30340;&#12290;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;LLM&#20013;&#65292;&#38500;&#20102;GPT-4&#65292;&#20854;&#20182;&#37117;&#24120;&#24120;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#29359;&#22522;&#26412;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;GPT-4&#65292;&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science. In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning. Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.
&lt;/p&gt;</description></item><item><title>PythonSaga&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;Python&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;,&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#23384;&#22312;&#30340;&#32534;&#31243;&#27010;&#24565;&#20559;&#35265;&#21644;&#31616;&#21333;&#20219;&#21153;&#26222;&#36941;&#24615;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2401.03855</link><description>&lt;p&gt;
PythonSaga&#65306;&#37325;&#26032;&#23450;&#20041;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;LLM&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03855
&lt;/p&gt;
&lt;p&gt;
PythonSaga&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;Python&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;,&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#23384;&#22312;&#30340;&#32534;&#31243;&#27010;&#24565;&#20559;&#35265;&#21644;&#31616;&#21333;&#20219;&#21153;&#26222;&#36941;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#20195;&#30721;&#28608;&#22686;&#30340;&#25512;&#21160;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;LLMs&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#23545;HumanEval&#21644;MBPP&#20004;&#20010;&#27969;&#34892;&#30340;Python&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#22810;&#26679;&#24615;&#21644;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#32534;&#31243;&#27010;&#24565;&#23384;&#22312;&#20005;&#37325;&#20559;&#35265;&#65292;&#23436;&#20840;&#24573;&#35270;&#20102;&#22823;&#22810;&#25968;&#20854;&#20182;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22823;&#37327;&#31616;&#21333;&#20219;&#21153;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#21487;&#33021;&#22840;&#22823;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;PythonSaga&#65292;&#21253;&#21547;&#20102;185&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#65292;&#28085;&#30422;&#20102;38&#20010;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#32534;&#31243;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03855v2 Announce Type: replace-cross  Abstract: Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of HumanEval and MBPP, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks, potentially inflating model performance estimations. To address these limitations, we propose a novel benchmark, PythonSaga, featuring 185 hand-crafted prompts on a balanced representation of 38 programming concepts across diverse difficulty levels.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Contrastive Activation Addition&#65288;CAA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#65292;&#26174;&#33879;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#24182;&#22312;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#39069;&#22806;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.06681</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#28608;&#27963;&#21152;&#27861;&#25351;&#23548;Llama 2
&lt;/p&gt;
&lt;p&gt;
Steering Llama 2 via Contrastive Activation Addition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06681
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Contrastive Activation Addition&#65288;CAA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#65292;&#26174;&#33879;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#24182;&#22312;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#39069;&#22806;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;Contrastive Activation Addition&#65288;CAA&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#20462;&#25913;&#20854;&#28608;&#27963;&#26469;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#12290;CAA&#36890;&#36807;&#23545;&#26576;&#31181;&#34892;&#20026;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#31034;&#20363;&#20043;&#38388;&#27531;&#24046;&#27969;&#28608;&#27963;&#30340;&#24046;&#24322;&#27714;&#24179;&#22343;&#65292;&#35745;&#31639;&#20986;&#8220;&#25351;&#23548;&#21521;&#37327;&#8221;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22312;&#29992;&#25143;&#25552;&#31034;&#21518;&#30340;&#25152;&#26377;token&#20301;&#32622;&#19978;&#20197;&#27491;&#36127;&#31995;&#25968;&#28155;&#21152;&#36825;&#20123;&#25351;&#23548;&#21521;&#37327;&#65292;&#20174;&#32780;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#36873;&#25321;&#34892;&#20026;&#38382;&#39064;&#25968;&#25454;&#38598;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#22312;Llama 2 Chat&#19978;&#35780;&#20272;&#20102;CAA&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;CAA&#26174;&#30528;&#25913;&#21464;&#20102;&#27169;&#22411;&#34892;&#20026;&#65292;&#19981;&#20165;&#22312;&#20256;&#32479;&#26041;&#27861;&#22914;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#26377;&#25928;&#65292;&#32780;&#19988;&#26368;&#23567;&#31243;&#24230;&#22320;&#38477;&#20302;&#20102;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#34892;&#20026;&#20570;&#20986;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06681v3 Announce Type: replace-cross  Abstract: We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25552;&#31034;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;GPT4 APO&#22312;&#26631;&#20934;&#21270;&#25552;&#21319;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#19987;&#23478;&#23450;&#21046;&#21270;&#23545;&#20110;&#20869;&#23481;&#36136;&#37327;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.09684</link><description>&lt;p&gt;
&#21307;&#29983;&#20204;&#30693;&#36947;&#22914;&#20309;&#25552;&#37266;&#21527;&#65311;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#24110;&#21161;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25552;&#31034;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;GPT4 APO&#22312;&#26631;&#20934;&#21270;&#25552;&#21319;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#19987;&#23478;&#23450;&#21046;&#21270;&#23545;&#20110;&#20869;&#23481;&#36136;&#37327;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#25552;&#31034;&#24037;&#31243;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#26469;&#25913;&#36827;&#21021;&#22987;&#25552;&#31034;&#65292;&#24182;&#27604;&#36739;&#20102;&#21307;&#23398;&#19987;&#23478;&#12289;&#38750;&#21307;&#23398;&#19987;&#23478;&#20197;&#21450;&#32463;&#36807;APO&#22686;&#24378;&#30340;GPT3.5&#21644;GPT4&#30340;&#36755;&#20986;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;GPT4 APO&#22312;&#26631;&#20934;&#21270;&#20020;&#24202;&#31508;&#35760;&#21508;&#33410;&#25552;&#31034;&#36136;&#37327;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#20154;&#22312;&#29615;&#20013;&#26041;&#27861;&#26174;&#31034;&#65292;&#19987;&#23478;&#22312;APO&#21518;&#20445;&#25345;&#20869;&#23481;&#36136;&#37327;&#65292;&#20294;&#26356;&#20559;&#22909;&#33258;&#24049;&#30340;&#20462;&#25913;&#65292;&#34920;&#26126;&#20102;&#19987;&#23478;&#23450;&#21046;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#20004;&#38454;&#27573;&#20248;&#21270;&#36807;&#31243;&#65292;&#21033;&#29992;APO-GPT4&#30830;&#20445;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;&#19987;&#23478;&#36755;&#20837;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09684v2 Announce Type: replace-cross  Abstract: This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the outputs of medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in standardizing prompt quality across clinical note sections. A human-in-the-loop approach shows that experts maintain content quality post-APO, with a preference for their own modifications, suggesting the value of expert customization. We recommend a two-phase optimization process, leveraging APO-GPT4 for consistency and expert input for personalization.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#20808;&#36827;&#30340;&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#19981;&#22914;&#19968;&#20010;&#31616;&#21333;&#30340;&#26816;&#27979;&#22120;&#12290;&#30740;&#31350;&#35748;&#20026;&#38656;&#35201;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#29305;&#23450;&#26816;&#27979;&#22120;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#25215;&#35748;&#23454;&#36341;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.16807</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#20132;&#27969;&#20013;&#26816;&#27979;LLM&#36741;&#21161;&#20889;&#20316;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Detecting LLM-Assisted Writing in Scientific Communication: Are We There Yet?. (arXiv:2401.16807v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#20808;&#36827;&#30340;&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#19981;&#22914;&#19968;&#20010;&#31616;&#21333;&#30340;&#26816;&#27979;&#22120;&#12290;&#30740;&#31350;&#35748;&#20026;&#38656;&#35201;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#29305;&#23450;&#26816;&#27979;&#22120;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#25215;&#35748;&#23454;&#36341;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#20889;&#20316;&#36741;&#21161;&#39046;&#22495;&#12290;&#23613;&#31649;&#20262;&#29702;&#32771;&#34385;&#24378;&#35843;&#20102;&#22312;&#31185;&#23398;&#20132;&#27969;&#20013;&#36879;&#26126;&#22320;&#25215;&#35748;LLM&#30340;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#30495;&#23454;&#30340;&#25215;&#35748;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#40723;&#21169;&#20934;&#30830;&#25215;&#35748;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#19968;&#20010;&#28508;&#22312;&#36884;&#24452;&#28041;&#21450;&#20351;&#29992;&#33258;&#21160;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#21069;&#27839;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#19981;&#22914;&#19968;&#20010;&#31616;&#21333;&#30340;&#20020;&#26102;&#26816;&#27979;&#22120;&#65292;&#35813;&#26816;&#27979;&#22120;&#35774;&#35745;&#29992;&#20110;&#35782;&#21035;&#22312;LLM&#22823;&#37327;&#20986;&#29616;&#26102;&#30340;&#31361;&#28982;&#20889;&#20316;&#39118;&#26684;&#21464;&#21270;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;LLM&#36741;&#21161;&#20889;&#20316;&#26816;&#27979;&#30340;&#19987;&#29992;&#26816;&#27979;&#22120;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#22312;&#20419;&#36827;&#23545;LLM&#21442;&#19982;&#31185;&#23398;&#20132;&#27969;&#30340;&#26356;&#30495;&#23454;&#35748;&#21487;&#12289;&#35299;&#20915;&#24403;&#21069;&#25215;&#35748;&#23454;&#36341;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), exemplified by ChatGPT, have significantly reshaped text generation, particularly in the realm of writing assistance. While ethical considerations underscore the importance of transparently acknowledging LLM use, especially in scientific communication, genuine acknowledgment remains infrequent. A potential avenue to encourage accurate acknowledging of LLM-assisted writing involves employing automated detectors. Our evaluation of four cutting-edge LLM-generated text detectors reveals their suboptimal performance compared to a simple ad-hoc detector designed to identify abrupt writing style changes around the time of LLM proliferation. We contend that the development of specialized detectors exclusively dedicated to LLM-assisted writing detection is necessary. Such detectors could play a crucial role in fostering more authentic recognition of LLM involvement in scientific communication, addressing the current challenges in acknowledgment practices.
&lt;/p&gt;</description></item><item><title>AI&#24605;&#24819;&#23545;&#20010;&#20307;&#21019;&#36896;&#21147;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13481</link><description>&lt;p&gt;
AI&#24605;&#24819;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24605;&#24819;&#30340;&#21019;&#36896;&#21147;&#12289;&#22810;&#26679;&#24615;&#21644;&#36827;&#21270;&#65306;&#26469;&#33258;&#19968;&#20010;&#22823;&#35268;&#27169;&#21160;&#24577;&#23454;&#39564;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment. (arXiv:2401.13481v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13481
&lt;/p&gt;
&lt;p&gt;
AI&#24605;&#24819;&#23545;&#20010;&#20307;&#21019;&#36896;&#21147;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#25509;&#35302;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#12290;&#35266;&#30475;&#21040;AI&#29983;&#25104;&#30340;&#24605;&#24819;&#23558;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24605;&#24819;&#65311;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23454;&#39564;&#65288;800+&#21442;&#19982;&#32773;&#65292;40+&#20010;&#22269;&#23478;&#65289;&#65292;&#21442;&#19982;&#32773;&#35266;&#30475;&#20102;&#26469;&#33258;ChatGPT&#25110;&#20043;&#21069;&#23454;&#39564;&#21442;&#19982;&#32773;&#30340;&#21019;&#24847;&#24605;&#24819;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#33258;&#24049;&#30340;&#21019;&#24847;&#24605;&#32771;&#12290;&#25105;&#20204;&#21464;&#21270;&#20102;AI&#29983;&#25104;&#31034;&#20363;&#30340;&#25968;&#37327;&#65288;&#26080;&#12289;&#20302;&#12289;&#39640;&#26333;&#20809;&#65289;&#20197;&#21450;&#31034;&#20363;&#26159;&#21542;&#26631;&#35760;&#20026;&#8220;AI&#8221;&#65288;&#25259;&#38706;&#65289;&#12290;&#25105;&#20204;&#30340;&#21160;&#24577;&#23454;&#39564;&#35774;&#35745; - &#22312;&#21516;&#19968;&#23454;&#39564;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;&#20043;&#21069;&#21442;&#19982;&#32773;&#30340;&#24605;&#24819;&#20316;&#20026;&#26410;&#26469;&#21442;&#19982;&#32773;&#30340;&#21050;&#28608; - &#27169;&#25311;&#20102;&#25991;&#21270;&#21019;&#36896;&#30340;&#30456;&#20114;&#20381;&#36182;&#36807;&#31243;&#65306;&#21019;&#36896;&#24615;&#24605;&#24819;&#24314;&#31435;&#22312;&#20043;&#21069;&#30340;&#24605;&#24819;&#22522;&#30784;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25429;&#25417;&#21040;&#20102;LLM&#8220;&#22312;&#25991;&#21270;&#24490;&#29615;&#20013;&#8221;&#30340;&#22797;&#21512;&#25928;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;AI&#26333;&#20809;&#65288;&#20294;&#19981;&#26159;&#20302;AI&#26333;&#20809;&#65289;&#24182;&#27809;&#26377;&#24433;&#21709;&#20010;&#20154;&#24605;&#24819;&#30340;&#21019;&#36896;&#21147;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#24179;&#22343;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;AI&#20351;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#32047;&#31215;&#25928;&#24212;&#22686;&#24378;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- mimics the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made 
&lt;/p&gt;</description></item><item><title>PhotoBot&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#30456;&#20114;&#20316;&#29992;&#30340;&#33258;&#21160;&#21270;&#29031;&#29255;&#33719;&#21462;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#26469;&#25552;&#20379;&#25668;&#24433;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#35745;&#31639;&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2401.11061</link><description>&lt;p&gt;
PhotoBot&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#30340;&#21442;&#32771;&#20114;&#21160;&#25668;&#24433;
&lt;/p&gt;
&lt;p&gt;
PhotoBot: Reference-Guided Interactive Photography via Natural Language. (arXiv:2401.11061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11061
&lt;/p&gt;
&lt;p&gt;
PhotoBot&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#30456;&#20114;&#20316;&#29992;&#30340;&#33258;&#21160;&#21270;&#29031;&#29255;&#33719;&#21462;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#26469;&#25552;&#20379;&#25668;&#24433;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#35745;&#31639;&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PhotoBot&#30340;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#39640;&#32423;&#20154;&#31867;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#20174;&#31574;&#23637;&#30011;&#24266;&#20013;&#26816;&#32034;&#21040;&#30340;&#21442;&#32771;&#22270;&#29255;&#21521;&#29992;&#25143;&#20256;&#36798;&#25668;&#24433;&#24314;&#35758;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#23545;&#21442;&#32771;&#22270;&#29255;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#22522;&#20110;&#29992;&#25143;&#35821;&#35328;&#26597;&#35810;&#30340;&#25991;&#26412;&#25512;&#29702;&#26816;&#32034;&#30456;&#20851;&#30340;&#21442;&#32771;&#22270;&#29255;&#12290;&#20026;&#20102;&#23545;&#24212;&#21442;&#32771;&#22270;&#29255;&#21644;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#26174;&#33879;&#19981;&#21516;&#30340;&#22270;&#20687;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#36879;&#35270;n-&#28857;&#65288;PnP&#65289;&#38382;&#39064;&#26469;&#35745;&#31639;RGB-D&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#37197;&#22791;&#26377;&#25163;&#33109;&#30456;&#26426;&#30340;&#30495;&#23454;&#26426;&#26800;&#25163;&#33218;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;PhotoBot&#25293;&#25668;&#30340;&#29031;&#29255;&#20855;&#26377;&#33391;&#22909;&#30340;&#36136;&#37327;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PhotoBot, a framework for automated photo acquisition based on an interplay between high-level human language guidance and a robot photographer. We propose to communicate photography suggestions to the user via a reference picture that is retrieved from a curated gallery. We exploit a visual language model (VLM) and an object detector to characterize reference pictures via textual descriptions and use a large language model (LLM) to retrieve relevant reference pictures based on a user's language query through text-based reasoning. To correspond the reference picture and the observed scene, we exploit pre-trained features from a vision transformer capable of capturing semantic similarity across significantly varying images. Using these features, we compute pose adjustments for an RGB-D camera by solving a Perspective-n-Point (PnP) problem. We demonstrate our approach on a real-world manipulator equipped with a wrist camera. Our user studies show that photos taken by PhotoBo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10690</link><description>&lt;p&gt;
&#36229;&#36234;RMSE&#21644;MAE&#65306;&#24341;&#20837;EAUC&#26469;&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#30340;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#19968;&#23545;&#23454;&#20307;&#30340;&#23454;&#20540;&#32467;&#26524;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#22522;&#30784;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39044;&#27979;&#29992;&#25143;&#23545;&#20135;&#21697;&#30340;&#35780;&#20998;&#65289;&#65292;&#22312;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#20013;&#20063;&#26377;&#35768;&#22810;&#28508;&#21147;&#20294;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#65288;&#20363;&#22914;&#65292;&#22312;&#20010;&#24615;&#21270;&#33647;&#29702;&#23398;&#20013;&#36817;&#20284;&#30830;&#23450;&#24739;&#32773;&#30340;&#36866;&#24403;&#21058;&#37327;&#65289;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20010;&#20307;&#23454;&#20307;&#35266;&#23519;&#20540;&#20998;&#24067;&#30340;&#38750;&#22343;&#21248;&#24615;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#20013;&#30340;&#20005;&#37325;&#20559;&#35265;&#39044;&#27979;&#65292;&#20559;&#21521;&#20110;&#23454;&#20307;&#30340;&#35266;&#23519;&#36807;&#21435;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#24182;&#22312;&#21478;&#31867;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#27604;&#38543;&#26426;&#39044;&#27979;&#26356;&#24046;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#19981;&#36275;&#20197;&#25429;&#25417;&#21040;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#21478;&#31867;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#21478;&#31867;-&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;EAUC&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#34917;&#20805;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#30740;&#31350;&#30340;&#27169;&#22411;&#20013;&#37327;&#21270;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#24863;&#30693;&#22810;&#22336;&#35775;&#38382;&#26041;&#26696;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#21033;&#29992;&#19982;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#28385;&#36275;&#26410;&#26469;6G&#24212;&#29992;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06308</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#24335;&#12289;&#21160;&#24577;6G&#24212;&#29992;&#30340;&#35821;&#20041;&#24863;&#30693;&#22810;&#22336;&#35775;&#38382;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Semantic-Aware Multiple Access Scheme for Distributed, Dynamic 6G-Based Applications. (arXiv:2401.06308v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#24863;&#30693;&#22810;&#22336;&#35775;&#38382;&#26041;&#26696;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#21033;&#29992;&#19982;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#28385;&#36275;&#26410;&#26469;6G&#24212;&#29992;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#24863;&#30693;&#33539;&#24335;&#30340;&#20986;&#29616;&#20026;&#21019;&#26032;&#30340;&#26381;&#21153;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#23588;&#20854;&#26159;&#22312;&#22522;&#20110;6G&#30340;&#24212;&#29992;&#29615;&#22659;&#20013;&#12290;&#23613;&#31649;&#22312;&#35821;&#20041;&#25552;&#21462;&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23558;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#36164;&#28304;&#20998;&#37197;&#20915;&#31574;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#32570;&#20047;&#23545;&#26410;&#26469;&#31995;&#32479;&#38656;&#27714;&#21644;&#29305;&#24615;&#30340;&#32771;&#34385;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#32447;&#39057;&#35889;&#22810;&#22336;&#35775;&#38382;&#38382;&#39064;&#30340;&#24314;&#27169;&#12290;&#23427;&#26088;&#22312;&#20248;&#21270;&#21033;&#29992;&#29575;&#19982;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#65292;&#20351;&#29992;&#945;-&#20844;&#24179;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#33258;&#21161;&#21534;&#21520;&#37327;&#21644;&#21327;&#21161;&#21534;&#21520;&#37327;&#30340;&#27010;&#24565;&#26469;&#32771;&#34385;&#29992;&#25143;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#12290;&#39318;&#20808;&#65292;&#20998;&#26512;&#20102;&#35813;&#38382;&#39064;&#65292;&#25214;&#20986;&#20102;&#26368;&#20248;&#35299;&#12290;&#25509;&#19979;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#22810;&#20027;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#35821;&#20041;&#24863;&#30693;&#22810;&#26234;&#33021;&#20307;&#21452;&#37325;&#21644;&#20915;&#26007;&#28145;&#24230;Q&#23398;&#20064; (SAMA-D3QL) &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of the semantic-aware paradigm presents opportunities for innovative services, especially in the context of 6G-based applications. Although significant progress has been made in semantic extraction techniques, the incorporation of semantic information into resource allocation decision-making is still in its early stages, lacking consideration of the requirements and characteristics of future systems. In response, this paper introduces a novel formulation for the problem of multiple access to the wireless spectrum. It aims to optimize the utilization-fairness trade-off, using the $\alpha$-fairness metric, while accounting for user data correlation by introducing the concepts of self- and assisted throughputs. Initially, the problem is analyzed to identify its optimal solution. Subsequently, a Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL) technique is proposed. This method is grounded in Model-free Multi-Agent Deep Reinforcement Learning (MADRL),
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;212&#20010;&#30495;&#23454;&#30340;&#24694;&#24847;&#26381;&#21153;&#65288;Malla&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22320;&#19979;&#24066;&#22330;&#30340;&#25193;&#25955;&#21644;&#23545;&#20844;&#20849;LLM&#26381;&#21153;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#30340;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2401.03315</link><description>&lt;p&gt;
Malla: &#25581;&#31192;&#29616;&#23454;&#19990;&#30028;&#20013;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#24694;&#24847;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Malla: Demystifying Real-world Large Language Model Integrated Malicious Services. (arXiv:2401.03315v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;212&#20010;&#30495;&#23454;&#30340;&#24694;&#24847;&#26381;&#21153;&#65288;Malla&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22320;&#19979;&#24066;&#22330;&#30340;&#25193;&#25955;&#21644;&#23545;&#20844;&#20849;LLM&#26381;&#21153;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#30340;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22320;&#19979;&#21033;&#29992;&#65292;&#20063;&#31216;&#20026;Malla&#65292;&#27491;&#22312;&#22686;&#21152;&#65292;&#21152;&#21095;&#20102;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#65292;&#24182;&#23545;LLMs&#25216;&#26415;&#30340;&#21487;&#20449;&#24230;&#25552;&#20986;&#20102;&#30097;&#38382;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#21162;&#21147;&#21435;&#20102;&#35299;&#36825;&#31181;&#26032;&#22411;&#32593;&#32476;&#29359;&#32618;&#30340;&#35268;&#27169;&#12289;&#24433;&#21709;&#21644;&#25216;&#26415;&#12290;&#26412;&#25991;&#26159;&#31532;&#19968;&#27425;&#23545;212&#20010;&#30495;&#23454;&#30340;Malla&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22320;&#19979;&#24066;&#22330;&#30340;&#25193;&#25955;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#24320;&#20102;Malla&#29983;&#24577;&#31995;&#32479;&#65292;&#25581;&#31034;&#20102;&#20854;&#26174;&#33879;&#30340;&#22686;&#38271;&#23545;&#24403;&#20170;&#20844;&#20849;LLM&#26381;&#21153;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;212&#20010;Mallas&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;8&#20010;&#21518;&#31471;LLMs&#65292;&#20197;&#21450;182&#20010;&#32469;&#36807;&#20844;&#20849;LLM API&#20445;&#25252;&#25514;&#26045;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;Mallas&#20351;&#29992;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#28389;&#29992;&#26410;&#32463;&#23457;&#26597;&#30340;LLMs&#21644;&#36890;&#36807;&#36234;&#29425;&#25552;&#31034;&#21033;&#29992;&#20844;&#20849;LLM API&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;Malla&#29359;&#32618;&#34892;&#20026;&#30340;&#23454;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The underground exploitation of large language models (LLMs) for malicious services (i.e., Malla) is witnessing an uptick, amplifying the cyber threat landscape and posing questions about the trustworthiness of LLM technologies. However, there has been little effort to understand this new cybercrime, in terms of its magnitude, impact, and techniques. In this paper, we conduct the first systematic study on 212 real-world Mallas, uncovering their proliferation in underground marketplaces and exposing their operational modalities. Our study discloses the Malla ecosystem, revealing its significant growth and impact on today's public LLM services. Through examining 212 Mallas, we uncovered eight backend LLMs used by Mallas, along with 182 prompts that circumvent the protective measures of public LLM APIs. We further demystify the tactics employed by Mallas, including the abuse of uncensored LLMs and the exploitation of public LLM APIs through jailbreak prompts. Our findings enable a better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#20154;&#33080;&#20316;&#20026;&#22836;&#20687;&#30340;&#20266;&#36896;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#65292;&#21457;&#29616;&#23427;&#20204;&#29992;&#20110;&#20256;&#25773;&#35784;&#39575;&#12289;&#22403;&#22334;&#20449;&#24687;&#20197;&#21450;&#25918;&#22823;&#21327;&#21516;&#20449;&#24687;&#31561;&#19981;&#30495;&#23454;&#27963;&#21160;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20316;&#32773;&#20272;&#35745;&#20351;&#29992;GAN&#29983;&#25104;&#30340;&#38754;&#23380;&#30340;&#36134;&#25143;&#26222;&#36941;&#24615;&#19979;&#38480;&#22312;0.021%&#21040;0.044%&#20043;&#38388;&#65292;&#32422;&#20026;&#27599;&#26085;&#27963;&#36291;&#36134;&#25143;10K&#20010;&#12290;</title><link>http://arxiv.org/abs/2401.02627</link><description>&lt;p&gt;
&#20266;&#36896;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#30340;&#29305;&#28857;&#21644;&#26222;&#36941;&#24615;&#65292;&#20351;&#29992;&#30340;&#26159;AI&#29983;&#25104;&#30340;&#38754;&#23380;
&lt;/p&gt;
&lt;p&gt;
Characteristics and prevalence of fake social media profiles with AI-generated faces. (arXiv:2401.02627v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#20154;&#33080;&#20316;&#20026;&#22836;&#20687;&#30340;&#20266;&#36896;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#65292;&#21457;&#29616;&#23427;&#20204;&#29992;&#20110;&#20256;&#25773;&#35784;&#39575;&#12289;&#22403;&#22334;&#20449;&#24687;&#20197;&#21450;&#25918;&#22823;&#21327;&#21516;&#20449;&#24687;&#31561;&#19981;&#30495;&#23454;&#27963;&#21160;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20316;&#32773;&#20272;&#35745;&#20351;&#29992;GAN&#29983;&#25104;&#30340;&#38754;&#23380;&#30340;&#36134;&#25143;&#26222;&#36941;&#24615;&#19979;&#38480;&#22312;0.021%&#21040;0.044%&#20043;&#38388;&#65292;&#32422;&#20026;&#27599;&#26085;&#27963;&#36291;&#36134;&#25143;10K&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#24341;&#21457;&#20102;&#23545;&#20854;&#21487;&#33021;&#21019;&#24314;&#20986;&#36924;&#30495;&#20266;&#36896;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#30340;&#25285;&#24551;&#65292;&#20294;&#32570;&#20047;&#23454;&#35777;&#35777;&#25454;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#30340;&#20154;&#33080;&#20316;&#20026;&#22836;&#20687;&#30340;Twitter(X)&#36134;&#25143;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;1,353&#20010;&#27492;&#31867;&#36134;&#25143;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#29992;&#20110;&#20256;&#25773;&#35784;&#39575;&#12289;&#22403;&#22334;&#20449;&#24687;&#20197;&#21450;&#25918;&#22823;&#21327;&#21516;&#20449;&#24687;&#31561;&#19981;&#30495;&#23454;&#27963;&#21160;&#12290;&#36890;&#36807;&#21033;&#29992;GAN&#29983;&#25104;&#30340;&#38754;&#23380;&#30340;&#19968;&#20010;&#29305;&#24449;&#8212;&#8212;&#30524;&#30555;&#30340;&#19968;&#33268;&#20301;&#32622;&#65292;&#24182;&#19982;&#20154;&#24037;&#27880;&#37322;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#37326;&#22806;&#20013;&#20351;&#29992;GAN&#29983;&#25104;&#30340;&#36134;&#25143;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#38543;&#26426;&#26679;&#26412;&#30340;&#27963;&#36291;Twitter&#29992;&#25143;&#20013;&#65292;&#25105;&#20204;&#20272;&#35745;&#20351;&#29992;GAN&#29983;&#25104;&#30340;&#38754;&#23380;&#30340;&#36134;&#25143;&#26222;&#36941;&#24615;&#19979;&#38480;&#22312;0.021%&#21040;0.044%&#20043;&#38388;&#65292;&#32422;&#20026;&#27599;&#26085;&#27963;&#36291;&#36134;&#25143;10K&#20010;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#22810;&#27169;&#24335;&#36134;&#25143;&#23545;&#20110;&#26032;&#20852;&#23041;&#32961;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in generative artificial intelligence (AI) have raised concerns about their potential to create convincing fake social media accounts, but empirical evidence is lacking. In this paper, we present a systematic analysis of Twitter(X) accounts using human faces generated by Generative Adversarial Networks (GANs) for their profile pictures. We present a dataset of 1,353 such accounts and show that they are used to spread scams, spam, and amplify coordinated messages, among other inauthentic activities. Leveraging a feature of GAN-generated faces -- consistent eye placement -- and supplementing it with human annotation, we devise an effective method for identifying GAN-generated profiles in the wild. Applying this method to a random sample of active Twitter users, we estimate a lower bound for the prevalence of profiles using GAN-generated faces between 0.021% and 0.044% -- around 10K daily active accounts. These findings underscore the emerging threats posed by multimod
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#26159;&#29616;&#26377;&#31639;&#27861;&#30340;&#25968;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.08419</link><description>&lt;p&gt;
&#22312;&#20108;&#21313;&#20010;&#26597;&#35810;&#20013;&#30772;&#35299;&#40657;&#30418;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Black Box Large Language Models in Twenty Queries. (arXiv:2310.08419v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08419
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#26159;&#29616;&#26377;&#31639;&#27861;&#30340;&#25968;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#30772;&#35299;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#36843;&#20351;LLMs&#36229;&#36234;&#20854;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#36825;&#20123;&#28431;&#27934;&#23545;&#20110;&#29702;&#35299;&#22266;&#26377;&#24369;&#28857;&#24182;&#38450;&#27490;&#26410;&#26469;&#30340;&#19981;&#24403;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Automatic Iterative Refinement&#65288;PAIR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20165;&#36890;&#36807;&#23545;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#31639;&#27861;&#29983;&#25104;&#35821;&#20041;&#30772;&#35299;&#12290;PAIR&#21463;&#21040;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#30340;&#21551;&#21457;&#65292;&#20351;&#29992;&#25915;&#20987;&#32773;LLM&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#30446;&#26631;LLM&#30340;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#25915;&#20987;&#32773;LLM&#36890;&#36807;&#36845;&#20195;&#26597;&#35810;&#30446;&#26631;LLM&#26469;&#26356;&#26032;&#21644;&#25913;&#36827;&#20505;&#36873;&#30772;&#35299;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#36825;&#27604;&#29616;&#26377;&#31639;&#27861;&#39640;&#25928;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;PAIR&#36824;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#30772;&#35299;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbrea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04444</link><description>&lt;p&gt;
&#39764;&#27861;&#35789;&#26159;&#20160;&#20040;&#65311;LLM&#25552;&#31034;&#30340;&#25511;&#21046;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;LLM&#30340;&#37096;&#32626;&#20013;&#26159;&#26377;&#25928;&#21644;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#25968;&#23398;&#19978;&#29702;&#35299;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35748;&#20026;&#26159;&#35843;&#33410;LLM&#36755;&#20986;&#20998;&#24067;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;token&#24207;&#21015;&#65292;&#26159;&#21542;&#24635;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#21487;&#20197;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65311;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26368;&#20248;&#25552;&#31034;&#31216;&#20026;&#39764;&#27861;&#35789;&#65292;&#22240;&#20026;&#28155;&#21152;&#25552;&#31034;&#20250;&#23548;&#33268;LLM&#36755;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#23384;&#22312;&#39764;&#27861;&#35789;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#23427;&#20204;&#65311;&#22914;&#26524;&#21487;&#20197;&#65292;&#23427;&#20204;&#30340;&#29305;&#24615;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#25511;&#21046;&#29702;&#35770;&#24212;&#29992;&#20110;&#33258;&#27880;&#24847;&#21147;&#22836;&#30340;&#20998;&#26512;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20989;&#25968;&#20026;&#21487;&#25511;&#21046;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20511;&#37492;&#25511;&#21046;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;$k-\epsilon$&#21487;&#25511;&#21046;&#24615;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
&lt;/p&gt;</description></item><item><title>FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.12325</link><description>&lt;p&gt;
FUTURE-AI&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;&#21644;&#21487;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#30340;&#22269;&#38469;&#20849;&#35782;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare. (arXiv:2309.12325v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12325
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;AI&#25216;&#26415;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#37096;&#32626;&#21644;&#37319;&#29992;&#20173;&#21463;&#38480;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#21307;&#30103;AI&#30340;&#25216;&#26415;&#12289;&#20020;&#24202;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#39118;&#38505;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;&#20026;&#20102;&#22686;&#21152;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37319;&#29992;&#65292;&#21307;&#30103;AI&#24037;&#20855;&#24517;&#39035;&#24471;&#21040;&#24739;&#32773;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20581;&#24247;&#32452;&#32455;&#21644;&#24403;&#23616;&#30340;&#20449;&#20219;&#21644;&#25509;&#21463;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;FUTURE-AI&#25351;&#21335;&#20316;&#20026;&#31532;&#19968;&#20010;&#29992;&#20110;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#12290;FUTURE-AI&#32852;&#30431;&#25104;&#31435;&#20110;2021&#24180;&#65292;&#30446;&#21069;&#21253;&#25324;&#26469;&#33258;51&#20010;&#22269;&#23478;&#30340;118&#20301;&#36328;&#23398;&#31185;&#19987;&#23478;&#65292;&#20195;&#34920;&#20102;&#25152;&#26377;&#22823;&#27954;&#65292;&#21253;&#25324;AI&#31185;&#23398;&#23478;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20262;&#29702;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#23478;&#12290;&#22312;&#20026;&#26399;&#20004;&#24180;&#30340;&#26102;&#38388;&#37324;&#65292;&#32852;&#30431;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#23450;&#20041;&#20102;&#21487;&#20449;AI&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20854;&#20013;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Despite major advances in artificial intelligence (AI) for medicine and healthcare, the deployment and adoption of AI technologies remain limited in real-world clinical practice. In recent years, concerns have been raised about the technical, clinical, ethical and legal risks associated with medical AI. To increase real world adoption, it is essential that medical AI tools are trusted and accepted by patients, clinicians, health organisations and authorities. This work describes the FUTURE-AI guideline as the first international consensus framework for guiding the development and deployment of trustworthy AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and currently comprises 118 inter-disciplinary experts from 51 countries representing all continents, including AI scientists, clinicians, ethicists, and social scientists. Over a two-year period, the consortium defined guiding principles and best practices for trustworthy AI through an iterative process comprising a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#20363;&#31354;&#38388;&#20998;&#26512;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#38382;&#39064;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#36873;&#25321;&#31995;&#32479;&#65292;&#39044;&#27979;&#20102;&#22312;&#32473;&#23450;&#29305;&#24449;&#19979;&#26368;&#36866;&#21512;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#28145;&#20837;&#20102;&#35299;&#20102;&#31639;&#27861;&#24615;&#33021;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.03229</link><description>&lt;p&gt;
&#22914;&#20309;&#36873;&#25321;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#20013;&#30340;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Which algorithm to select in sports timetabling?. (arXiv:2309.03229v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#20363;&#31354;&#38388;&#20998;&#26512;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#38382;&#39064;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#36873;&#25321;&#31995;&#32479;&#65292;&#39044;&#27979;&#20102;&#22312;&#32473;&#23450;&#29305;&#24449;&#19979;&#26368;&#36866;&#21512;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#28145;&#20837;&#20102;&#35299;&#20102;&#31639;&#27861;&#24615;&#33021;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#20307;&#32946;&#31454;&#36187;&#37117;&#38656;&#35201;&#19968;&#20010;&#36187;&#31243;&#23433;&#25490;&#65292;&#30830;&#23450;&#27604;&#36187;&#38431;&#20237;&#20309;&#26102;&#20309;&#22320;&#30456;&#36935;&#12290;&#26368;&#36817;&#30340;&#22269;&#38469;&#36187;&#31243;&#23433;&#25490;&#31454;&#36187;(ITC2021)&#25581;&#31034;&#20102;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#34429;&#28982;&#21487;&#33021;&#24320;&#21457;&#20986;&#36890;&#29992;&#31639;&#27861;&#65292;&#20294;&#27599;&#20010;&#31639;&#27861;&#22312;&#38382;&#39064;&#23454;&#20363;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#12290;&#26412;&#25991;&#22312;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#26041;&#38754;&#25552;&#20379;&#20102;&#23454;&#20363;&#31354;&#38388;&#20998;&#26512;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20102;&#20843;&#31181;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#36873;&#25321;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#38382;&#39064;&#23454;&#20363;&#30340;&#29305;&#24449;&#39044;&#27979;&#21738;&#31181;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#21487;&#33021;&#34920;&#29616;&#26368;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#21738;&#20123;&#29305;&#24449;&#22312;&#20570;&#20986;&#39044;&#27979;&#26102;&#24456;&#37325;&#35201;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#24314;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#23454;&#20363;&#30340;&#32463;&#39564;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#22823;&#35268;&#27169;&#35745;&#31639;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any sports competition needs a timetable, specifying when and where teams meet each other. The recent International Timetabling Competition (ITC2021) on sports timetabling showed that, although it is possible to develop general algorithms, the performance of each algorithm varies considerably over the problem instances. This paper provides an instance space analysis for sports timetabling, resulting in powerful insights into the strengths and weaknesses of eight state-of-the-art algorithms. Based on machine learning techniques, we propose an algorithm selection system that predicts which algorithm is likely to perform best when given the characteristics of a sports timetabling problem instance. Furthermore, we identify which characteristics are important in making that prediction, providing insights in the performance of the algorithms, and suggestions to further improve them. Finally, we assess the empirical hardness of the instances. Our results are based on large computational exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;Logical-GLM&#65292;&#29992;&#20110;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#27491;&#30830;&#36923;&#36753;&#30340;&#25991;&#26412;&#65292;&#24182;&#20197;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Logical-GLM&#22312;&#20351;&#29992;&#36739;&#23569;&#25968;&#25454;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.13782</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#36923;&#36753;&#22270;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#29983;&#25104;&#30340;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning with Logical Graph-based Language Model for Instruction Generation. (arXiv:2308.13782v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;Logical-GLM&#65292;&#29992;&#20110;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#27491;&#30830;&#36923;&#36753;&#30340;&#25991;&#26412;&#65292;&#24182;&#20197;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Logical-GLM&#22312;&#20351;&#29992;&#36739;&#23569;&#25968;&#25454;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#31070;&#32463;&#27169;&#22411;&#38590;&#20197;&#20174;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#20013;&#25429;&#25417;&#21040;&#38544;&#21547;&#30340;&#35268;&#21017;&#65292;&#22240;&#27492;&#24456;&#38590;&#29983;&#25104;&#20855;&#26377;&#27491;&#30830;&#36923;&#36753;&#30340;&#25991;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;Logical-GLM&#65292;&#23558;&#36923;&#36753;&#27880;&#20837;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#26356;&#26377;&#25928;&#30340;&#25991;&#26412;&#29983;&#25104;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#26500;&#24314;&#36890;&#24120;&#25551;&#36848;&#39046;&#22495;&#30340;&#36923;&#36753;&#36125;&#21494;&#26031;&#22270;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#29983;&#25104;&#36923;&#36753;&#39592;&#26550;&#20197;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20132;&#26367;&#20248;&#21270;&#22270;&#30340;&#25628;&#32034;&#31574;&#30053;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#30452;&#33267;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Logical-GLM&#19982;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#23613;&#31649;&#20351;&#29992;&#35268;&#27169;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#65292;&#20173;&#28982;&#20855;&#26377;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26377;&#25928;&#30340;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the superior performance of large language models to generate natural language texts, it is hard to generate texts with correct logic according to a given task, due to the difficulties for neural models to capture implied rules from free-form texts. In this paper, we propose a novel graph-based language model, Logical-GLM, to infuse logic into language models for more valid text generation and interpretability. Specifically, we first capture information from natural language instructions and construct logical bayes graphs that generally describe domains. Next, we generate logical skeletons to guide language model training, infusing domain knowledge into language models. Finally, we alternately optimize the searching policy of graphs and language models until convergence. The experimental results show that Logical-GLM is both effective and efficient compared with traditional language models, despite using smaller-scale training data and fewer parameters. Our approach can generat
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2308.13191</link><description>&lt;p&gt;
Chunk, Align, Select: &#19968;&#31181;&#31616;&#21333;&#30340;&#29992;&#20110;transformer&#30340;&#38271;&#24207;&#21015;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers. (arXiv:2308.13191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13191
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#30528;&#38271;&#24207;&#21015;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;transformer&#20013;&#33258;&#27880;&#24847;&#25805;&#20316;&#30340;&#35745;&#31639;&#25104;&#26412;&#38543;&#30528;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#38271;&#24207;&#21015;&#22788;&#29702;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;transformer&#33021;&#22815;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#38271;&#24207;&#21015;&#36755;&#20837;&#21010;&#20998;&#20026;&#19968;&#25209;chunk&#65292;&#28982;&#21518;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23545;chunk&#20043;&#38388;&#30340;&#20449;&#24687;&#36827;&#34892;&#23545;&#40784;&#65292;&#26368;&#21518;&#20174;&#32534;&#30721;&#22120;&#20013;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#35299;&#30721;&#12290;&#20026;&#20102;&#25552;&#21462;chunk&#20043;&#38388;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#32534;&#30721;transformer&#22359;&#20013;&#23545;chunk&#20043;&#38388;&#30340;&#36215;&#22987;&#21644;&#32467;&#26463;token&#36827;&#34892;&#23545;&#40784;&#12290;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#26377;&#25928;&#30340;&#38544;&#34255;&#29366;&#24577;&#36873;&#25321;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#37325;&#26356;&#26032;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although dominant in natural language processing, transformer-based models remain challenged by the task of long-sequence processing, because the computational cost of self-attention operations in transformers swells quadratically with the input sequence length. To alleviate the complexity of long-sequence processing, we propose a simple framework to enable the offthe-shelf pre-trained transformers to process much longer sequences, while the computation and memory costs remain growing linearly with the input sequence lengths. More specifically, our method divides each long-sequence input into a batch of chunks, then aligns the interchunk information during the encoding steps, and finally selects the most representative hidden states from the encoder for the decoding process. To extract inter-chunk semantic information, we align the start and end token embeddings among chunks in each encoding transformer block. To learn an effective hidden selection policy, we design a dual updating sch
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#26469;&#20248;&#21270;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#24067;&#32622;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#36317;&#31163;&#21644;&#30899;&#36275;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.11038</link><description>&lt;p&gt;
&#29289;&#27969;&#38598;&#25955;&#22320;&#20301;&#32622;&#20248;&#21270;&#65306;&#19968;&#31181;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances. (arXiv:2308.11038v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#26469;&#20248;&#21270;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#24067;&#32622;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#36317;&#31163;&#21644;&#30899;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#27969;&#38598;&#25955;&#22320;&#22312;&#26368;&#21518;&#19968;&#20844;&#37324;&#37197;&#36865;&#36317;&#31163;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65307;&#21363;&#20351;&#36317;&#31163;&#24494;&#23567;&#22686;&#21152;&#20063;&#20250;&#23545;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#30340;&#19994;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#20250;&#22686;&#21152;&#20854;&#30899;&#36275;&#36857;&#12290;&#29305;&#21035;&#26159;&#22312;Covid-19&#20043;&#21518;&#65292;&#35813;&#34892;&#19994;&#30340;&#22686;&#38271;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#20248;&#21270;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#24067;&#32622;&#12290;&#35813;&#26041;&#27861;&#20381;&#27425;&#37319;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#31354;&#38388;&#20301;&#32622;&#65292;&#20351;&#29992;K-Means&#23545;&#20132;&#20184;&#28857;&#36827;&#34892;&#32858;&#31867;&#12290;&#32858;&#31867;&#26041;&#27861;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#65292;&#32780;&#19981;&#26159;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#12290;&#36991;&#20813;&#20351;&#29992;&#38750;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#23548;&#33268;&#38169;&#35823;&#21644;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;P-Median&#26041;&#27861;&#30830;&#23450;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#12290;P-Median&#26041;&#27861;&#36824;&#23558;&#20132;&#20184;&#25968;&#37327;&#21644;&#20154;&#21475;&#20316;&#20026;&#26435;&#37325;&#32771;&#34385;&#22312;&#20869;&#12290;&#20351;&#29992;Muller&#21644;Phipps&#65288;M&#65286;P&#65289;&#30340;&#23454;&#38469;&#20132;&#20184;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Logistic hubs play a pivotal role in the last-mile delivery distance; even a slight increment in distance negatively impacts the business of the e-commerce industry while also increasing its carbon footprint. The growth of this industry, particularly after Covid-19, has further intensified the need for optimized allocation of resources in an urban environment. In this study, we use a hybrid approach to optimize the placement of logistic hubs. The approach sequentially employs different techniques. Initially, delivery points are clustered using K-Means in relation to their spatial locations. The clustering method utilizes road network distances as opposed to Euclidean distances. Non-road network-based approaches have been avoided since they lead to erroneous and misleading results. Finally, hubs are located using the P-Median method. The P-Median method also incorporates the number of deliveries and population as weights. Real-world delivery data from Muller and Phipps (M&amp;P) is used to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;LEGO&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#20351;&#29992;LiDAR&#21333;&#29420;&#36827;&#34892;&#36319;&#36394;&#30340;LEGO&#26041;&#27861;&#22312;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09908</link><description>&lt;p&gt;
LEGO: &#23545;&#20110;&#22522;&#20110;&#28857;&#20113;&#30340;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds. (arXiv:2308.09908v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;LEGO&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#20351;&#29992;LiDAR&#21333;&#29420;&#36827;&#34892;&#36319;&#36394;&#30340;LEGO&#26041;&#27861;&#22312;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#36319;&#36394;-&#26816;&#27979;&#26041;&#27861;&#65292;&#25968;&#25454;&#20851;&#32852;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#65288;LEGO&#65289;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;LEGO&#36319;&#36394;&#22120;&#38598;&#25104;&#20102;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21046;&#23450;&#20851;&#32852;&#35780;&#20998;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#39640;&#25928;&#30340;&#30446;&#26631;&#21305;&#37197;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#29366;&#24577;&#26356;&#26032;&#36807;&#31243;&#65292;&#26412;&#25991;&#36824;&#28155;&#21152;&#20102;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#23558;&#23545;&#35937;&#29366;&#24577;&#30340;&#26102;&#38388;&#36830;&#36143;&#24615;&#32435;&#20837;&#36319;&#36394;&#20013;&#65292;&#30830;&#20445;&#19968;&#33268;&#30340;&#36319;&#36394;&#12290;&#19982;&#20854;&#20182;&#22312;&#32447;&#36319;&#36394;&#26041;&#27861;&#65288;&#21253;&#25324;&#22522;&#20110;LiDAR&#21644;&#22522;&#20110;LiDAR-&#30456;&#26426;&#34701;&#21512;&#30340;&#26041;&#27861;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20165;&#21033;&#29992;LiDAR&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#25552;&#20132;&#32467;&#26524;&#33267;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#25490;&#34892;&#27036;&#26102;&#65292;LEGO&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online multi-object tracking (MOT) plays a pivotal role in autonomous systems. The state-of-the-art approaches usually employ a tracking-by-detection method, and data association plays a critical role. This paper proposes a learning and graph-optimized (LEGO) modular tracker to improve data association performance in the existing literature. The proposed LEGO tracker integrates graph optimization and self-attention mechanisms, which efficiently formulate the association score map, facilitating the accurate and efficient matching of objects across time frames. To further enhance the state update process, the Kalman filter is added to ensure consistent tracking by incorporating temporal coherence in the object states. Our proposed method utilizing LiDAR alone has shown exceptional performance compared to other online tracking approaches, including LiDAR-based and LiDAR-camera fusion-based methods. LEGO ranked 1st at the time of submitting results to KITTI object tracking evaluation ranki
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#22312;OOD&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#31070;&#31192;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27867;&#21270;&#25968;&#23383;&#36816;&#31639;&#26102;&#30340;&#34892;&#20026;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#26410;&#35299;&#20915;&#26412;&#36136;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.08268</link><description>&lt;p&gt;
&#23427;&#20854;&#23454;&#19981;&#37027;&#20040;&#31967;&#31957;&#65306;&#29702;&#35299;&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#23545;OOD&#27867;&#21270;&#30340;&#31070;&#31192;&#24615;&#33021;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models. (arXiv:2308.08268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08268
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#22312;OOD&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#31070;&#31192;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27867;&#21270;&#25968;&#23383;&#36816;&#31639;&#26102;&#30340;&#34892;&#20026;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#26410;&#35299;&#20915;&#26412;&#36136;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#27809;&#26377;&#23436;&#20840;&#34987;&#29702;&#35299;&#65292;&#24182;&#19988;&#24182;&#19981;&#24635;&#26159;&#20196;&#20154;&#28385;&#24847;&#12290;&#30740;&#31350;&#20154;&#21592;&#20174;&#22522;&#26412;&#30340;&#25968;&#23398;&#20219;&#21153;&#65288;&#22914;n&#20301;&#25968;&#30340;&#21152;&#27861;&#25110;&#20056;&#27861;&#65289;&#24320;&#22987;&#65292;&#20316;&#20026;&#37325;&#35201;&#35270;&#35282;&#26469;&#30740;&#31350;&#27169;&#22411;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#35266;&#23519;&#21040;&#24403;&#27169;&#22411;&#22312;n&#20301;&#25968;&#36816;&#31639;&#65288;&#20363;&#22914;&#21152;&#27861;&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#38271;&#24230;&#20026;n&#20301;&#30340;&#36755;&#20837;&#19978;&#21487;&#20197;&#25104;&#21151;&#27867;&#21270;&#65288;&#21363;&#20869;&#20998;&#24067;&#27867;&#21270;&#65289;&#65292;&#20294;&#22312;&#38271;&#24230;&#26356;&#38271;&#12289;&#26410;&#35265;&#36807;&#30340;&#24773;&#20917;&#19979;&#65288;&#21363;&#22806;&#20998;&#24067;&#27867;&#21270;&#65289;&#20250;&#22833;&#36133;&#24182;&#19988;&#34920;&#29616;&#31070;&#31192;&#12290;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#20462;&#25913;&#20301;&#32622;&#23884;&#20837;&#12289;&#24494;&#35843;&#21644;&#24341;&#20837;&#26356;&#24191;&#27867;&#25110;&#26356;&#26377;&#25351;&#23548;&#24615;&#30340;&#25968;&#25454;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#35299;&#20915;&#26412;&#36136;&#26426;&#21046;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30340;&#31283;&#20581;&#24615;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Transformer-based models have achieved remarkable proficiency on solving diverse problems. However, their generalization ability is not fully understood and not always satisfying. Researchers take basic mathematical tasks like n-digit addition or multiplication as important perspectives for investigating their generalization behaviors. Curiously, it is observed that when training on n-digit operations (e.g., additions) in which both input operands are n-digit in length, models generalize successfully on unseen n-digit inputs (in-distribution (ID) generalization), but fail miserably and mysteriously on longer, unseen cases (out-of-distribution (OOD) generalization). Studies try to bridge this gap with workarounds such as modifying position embedding, fine-tuning, and priming with more extensive or instructive data. However, without addressing the essential mechanism, there is hardly any guarantee regarding the robustness of these solutions. We bring this unexplained performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#21033;&#29992;&#38271;&#31243;&#20381;&#36182;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;&#21644;&#28176;&#36827;&#25277;&#26679;&#65288;LMSPS&#65289;&#30340;&#33258;&#21160;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#21644;&#19987;&#38376;&#30340;&#20803;&#36335;&#24452;&#36873;&#25321;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;LMSPS&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08430</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#36890;&#36807;&#28176;&#36827;&#25277;&#26679;&#36827;&#34892;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Long-range Meta-path Search through Progressive Sampling on Large-scale Heterogeneous Information Networks. (arXiv:2307.08430v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#21033;&#29992;&#38271;&#31243;&#20381;&#36182;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;&#21644;&#28176;&#36827;&#25277;&#26679;&#65288;LMSPS&#65289;&#30340;&#33258;&#21160;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#21644;&#19987;&#38376;&#30340;&#20803;&#36335;&#24452;&#36873;&#25321;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;LMSPS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31243;&#20381;&#36182;&#30340;&#21033;&#29992;&#22312;&#21516;&#36136;&#22270;&#20013;&#26377;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#24456;&#23569;&#30740;&#31350;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#39640;&#25104;&#26412;&#21644;&#26377;&#25928;&#20449;&#24687;&#21033;&#29992;&#30340;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#20803;&#36335;&#24452;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#21033;&#29992;&#38271;&#31243;&#20381;&#36182;&#30340;&#33258;&#21160;&#26694;&#26550;&#65292;&#31216;&#20026;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;&#21644;&#28176;&#36827;&#25277;&#26679;&#65288;LMSPS&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#21508;&#31181;&#25968;&#25454;&#38598;&#25110;&#20219;&#21153;&#30340;&#20803;&#36335;&#24452;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#25152;&#26377;&#30446;&#26631;&#33410;&#28857;&#30456;&#20851;&#20803;&#36335;&#24452;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#36890;&#36807;&#28176;&#36827;&#25277;&#26679;&#31639;&#27861;&#65292;&#25105;&#20204;&#21160;&#24577;&#22320;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#20197;&#36339;&#25968;&#26080;&#20851;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#39537;&#21160;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#30001;&#24403;&#21069;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#21644;&#20219;&#21153;&#39537;&#21160;&#30340;&#32039;&#20945;&#25628;&#32034;&#31354;&#38388;&#12290;&#21033;&#29992;&#25277;&#26679;&#35780;&#20272;&#31574;&#30053;&#20316;&#20026;&#25351;&#23548;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19987;&#38376;&#21644;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#20803;&#36335;&#24452;&#36873;&#25321;&#12290;&#23545;&#20843;&#20010;&#24322;&#26500;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LMSPS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing long-range dependency, though extensively studied in homogeneous graphs, is rarely studied in large-scale heterogeneous information networks (HINs), whose main challenge is the high costs and the difficulty in utilizing effective information. To this end, we investigate the importance of different meta-paths and propose an automatic framework for utilizing long-range dependency in HINs, called Long-range Meta-path Search through Progressive Sampling (LMSPS). Specifically, to discover meta-paths for various datasets or tasks without prior, we develop a search space with all target-node-related meta-paths. With a progressive sampling algorithm, we dynamically shrink the search space with hop-independent time complexity, leading to a compact search space driven by the current HIN and task. Utilizing a sampling evaluation strategy as the guidance, we conduct a specialized and expressive meta-path selection. Extensive experiments on eight heterogeneous datasets demonstrate that LM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102; ChatGPT &#21644; GPT-4 &#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#36816;&#29992;&#21644;&#24615;&#33021;&#34920;&#29616;&#65292;&#36890;&#36807;&#20197;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#30740;&#31350;&#25361;&#25112;&#20026;&#20363;&#65292;&#32467;&#35770;&#26159; ChatGPT &#21644; GPT-4 &#30340;&#32452;&#21512;&#26159;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#19968;&#31181;&#38750;&#24120;&#39640;&#25928;&#19988;&#33410;&#30465;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05036</link><description>&lt;p&gt;
HCI&#25361;&#25112;&#30340;&#26144;&#23556;&#65306;ChatGPT&#21644;GPT-4&#22312;&#25104;&#26412;&#25928;&#30410;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Cost-Efficient Question Answering. (arXiv:2306.05036v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102; ChatGPT &#21644; GPT-4 &#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#36816;&#29992;&#21644;&#24615;&#33021;&#34920;&#29616;&#65292;&#36890;&#36807;&#20197;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#30740;&#31350;&#25361;&#25112;&#20026;&#20363;&#65292;&#32467;&#35770;&#26159; ChatGPT &#21644; GPT-4 &#30340;&#32452;&#21512;&#26159;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#19968;&#31181;&#38750;&#24120;&#39640;&#25928;&#19988;&#33410;&#30465;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT-4&#27491;&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#24773;&#20917;&#12290;&#20294;&#26159;&#65292;&#36825;&#20004;&#31181;LLM&#26159;&#38381;&#28304;&#30340;&#65292;&#24182;&#19988;&#24456;&#23569;&#26377;&#20851;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#24615;&#33021;&#30340;&#20102;&#35299;&#12290;&#22312;&#23398;&#26415;&#30028;&#20013;&#65292;LLM&#30340;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#37327;&#30340;&#65292;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#21487;&#33021;&#24050;&#27844;&#28431;&#21040;ChatGPT&#21644;GPT-4&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ChatGPT&#21644;GPT-4&#24212;&#29992;&#20110;&#25104;&#26412;&#25928;&#30410;&#38382;&#31572;&#30340;&#23454;&#38469;&#20219;&#21153;&#65292;&#20197;&#20174;2023&#24180;&#20154;&#26426;&#20132;&#20114;&#20250;&#35758;&#65288;CHI&#65289;&#30340;&#35770;&#25991;&#38598;&#20013;&#25552;&#21462;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;LLM&#22312;&#36825;&#20010;&#23454;&#38469;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;ChatGPT&#21644;GPT-4&#30340;&#32452;&#21512;&#26159;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#26497;&#20339;&#25104;&#26412;&#25928;&#30410;&#25163;&#27573;&#12290;&#25104;&#26412;&#25928;&#29575;&#23545;&#20110;&#21407;&#22411;&#30740;&#31350;&#24819;&#27861;&#21644;&#20174;&#19981;&#21516;&#35282;&#24230;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT and GPT-4, are gaining wide-spread real world use. Yet, the two LLMs are closed source, and little is known about the LLMs' performance in real-world use cases. In academia, LLM performance is often measured on benchmarks which may have leaked into ChatGPT's and GPT-4's training data. In this paper, we apply and evaluate ChatGPT and GPT-4 for the real-world task of cost-efficient extractive question answering over a text corpus that was published after the two LLMs completed training. More specifically, we extract research challenges for researchers in the field of HCI from the proceedings of the 2023 Conference on Human Factors in Computing Systems (CHI). We critically evaluate the LLMs on this practical task and conclude that the combination of ChatGPT and GPT-4 makes an excellent cost-efficient means for analyzing a text corpus at scale. Cost-efficiency is key for prototyping research ideas and analyzing text corpora from different persp
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#8212;&#8212;&#36234;&#30028;&#36951;&#24536;&#65292;&#24403;&#32473;&#23450;&#31867;&#21035;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#23427;&#20250;&#26174;&#30528;&#21066;&#24369;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.00427</link><description>&lt;p&gt;
&#38024;&#23545;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#30340;&#36807;&#24230;&#36951;&#24536;&#65306;&#36830;&#32493;&#23398;&#20064;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift. (arXiv:2306.00427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00427
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#8212;&#8212;&#36234;&#30028;&#36951;&#24536;&#65292;&#24403;&#32473;&#23450;&#31867;&#21035;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#23427;&#20250;&#26174;&#30528;&#21066;&#24369;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#35753;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#24037;&#20316;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#30693;&#36947;&#30001;&#24847;&#22270;&#25915;&#20987;&#25110;&#29615;&#22659;&#25200;&#21160;&#24341;&#36215;&#30340;&#36234;&#30028;&#38382;&#39064;&#20005;&#37325;&#24433;&#21709;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#30001;&#36234;&#30028;&#38382;&#39064;&#24341;&#36215;&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#36234;&#30028;&#36951;&#24536;&#65288;OODF&#65289;&#12290;&#22312;&#36830;&#32493;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#38024;&#23545;&#32473;&#23450;&#31867;&#21035;&#65292;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26174;&#30528;&#21066;&#24369;&#20102;&#21518;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#31181;&#29616;&#35937;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#32780;&#35328;&#26159;&#29305;&#27530;&#30340;&#65292;&#22240;&#20026;&#21516;&#26679;&#32423;&#21035;&#30340;&#20998;&#24067;&#36716;&#31227;&#21482;&#26377;&#24494;&#19981;&#36275;&#36947;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) is an important technique to allow artificial neural networks to work in open environments. CL enables a system to learn new tasks without severe interference to its performance on old tasks, i.e., overcome the problems of catastrophic forgetting. In joint learning, it is well known that the out-of-distribution (OOD) problem caused by intentional attacks or environmental perturbations will severely impair the ability of networks to generalize. In this work, we reported a special form of catastrophic forgetting raised by the OOD problem in continual learning settings, and we named it out-of-distribution forgetting (OODF). In continual image classification tasks, we found that for a given category, introducing an intra-class distribution shift significantly impaired the recognition accuracy of CL methods for that category during subsequent learning. Interestingly, this phenomenon is special for CL as the same level of distribution shift had only negligible effects
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13338</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#22240;&#38598;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
Gene Set Summarization using Large Language Models. (arXiv:2305.13338v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#29983;&#29289;&#23398;&#23478;&#32463;&#24120;&#35299;&#37322;&#20174;&#39640;&#36890;&#37327;&#23454;&#39564;&#21644;&#35745;&#31639;&#20998;&#26512;&#20013;&#33719;&#24471;&#30340;&#22522;&#22240;&#21015;&#34920;&#12290;&#36825;&#36890;&#24120;&#26159;&#36890;&#36807;&#32479;&#35745;&#23500;&#38598;&#20998;&#26512;&#26469;&#23436;&#25104;&#30340;&#65292;&#35813;&#20998;&#26512;&#27979;&#37327;&#19982;&#22522;&#22240;&#25110;&#20854;&#23646;&#24615;&#30456;&#20851;&#30340;&#29983;&#29289;&#21151;&#33021;&#26415;&#35821;&#30340;&#36807;&#24230;&#25110;&#27424;&#34920;&#31034;&#31243;&#24230;&#65292;&#22522;&#20110;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#65288;&#20363;&#22914;Gene Ontology&#65288;GO&#65289;&#65289;&#20013;&#30340;&#32534;&#35793;&#26029;&#35328;&#12290;&#35299;&#37322;&#22522;&#22240;&#21015;&#34920;&#20063;&#21487;&#20197;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#25991;&#26412;&#27010;&#25324;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#65292;&#21487;&#33021;&#30452;&#25509;&#21033;&#29992;&#31185;&#23398;&#25991;&#26412;&#24182;&#36991;&#20813;&#20381;&#36182;KB&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;SPINDOCTOR&#65288;&#31283;&#23450;&#30340;&#25552;&#31034;&#25554;&#20540;&#30340;&#21463;&#25511;&#26415;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#27169;&#26495;&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;GPT&#27169;&#22411;&#25191;&#34892;&#22522;&#22240;&#38598;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#26631;&#20934;&#23500;&#38598;&#20998;&#26512;&#30340;&#34917;&#20805;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#22240;&#21151;&#33021;&#20449;&#24687;&#26469;&#28304;&#65306;&#65288;1&#65289;&#20174;&#37492;&#23450;&#30340;&#26412;&#20307;KB&#27880;&#37322;&#20013;&#33719;&#24471;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#65288;2&#65289;&#20174;&#25991;&#26412;&#25366;&#25496;&#20013;&#25512;&#26029;&#30340;&#26412;&#20307;&#26415;&#35821;&#65292;&#20197;&#21450;&#65288;3&#65289;&#30452;&#25509;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#24471;&#30340;&#26415;&#35821;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;1813&#20010;&#22522;&#22240;&#38598;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SPINDOCTOR&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;GPT&#27169;&#22411;&#26174;&#33879;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#30340;&#22522;&#22240;&#21151;&#33021;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular biologists frequently interpret gene lists derived from high-throughput experiments and computational analysis. This is typically done as a statistical enrichment analysis that measures the over- or under-representation of biological function terms associated with genes or their properties, based on curated assertions from a knowledge base (KB) such as the Gene Ontology (GO). Interpreting gene lists can also be framed as a textual summarization task, enabling the use of Large Language Models (LLMs), potentially utilizing scientific texts directly and avoiding reliance on a KB.  We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language Descriptions of Controlled Terms for Ontology Reporting), a method that uses GPT models to perform gene set function summarization as a complement to standard enrichment analysis. This method can use different sources of gene functional information: (1) structured text derived from curated ontological KB annotations, (2) ontol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#30340;&#31526;&#21495;&#12289;&#20122;&#31526;&#21495;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65288;SDP&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#26080;&#35770;&#26159;&#22522;&#20110;&#33258;&#21160;&#21270;&#35268;&#21010;&#65288;AP&#65289;&#36824;&#26159;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#37117;&#28085;&#30422;&#20102;&#35299;&#20915;SDP&#30340;&#26041;&#27861;&#21644;&#23398;&#20064;&#20854;&#32467;&#26500;&#30340;&#26041;&#38754;&#12290;&#23545;&#20110;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#20063;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2304.10590</link><description>&lt;p&gt;
&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#30340;&#31526;&#21495;&#12289;&#20122;&#31526;&#21495;&#21644;&#28151;&#21512;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Symbolic, Subsymbolic and Hybrid Methods for Sequential Decision Making. (arXiv:2304.10590v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#30340;&#31526;&#21495;&#12289;&#20122;&#31526;&#21495;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65288;SDP&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#26080;&#35770;&#26159;&#22522;&#20110;&#33258;&#21160;&#21270;&#35268;&#21010;&#65288;AP&#65289;&#36824;&#26159;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#37117;&#28085;&#30422;&#20102;&#35299;&#20915;SDP&#30340;&#26041;&#27861;&#21644;&#23398;&#20064;&#20854;&#32467;&#26500;&#30340;&#26041;&#38754;&#12290;&#23545;&#20110;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#20063;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#65288;SDM&#65289;&#39046;&#22495;&#25552;&#20379;&#20102;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65288;SDP&#65289;&#30340;&#24037;&#20855;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#24517;&#39035;&#20570;&#20986;&#19968;&#31995;&#21015;&#20915;&#31574;&#20197;&#23436;&#25104;&#20219;&#21153;&#25110;&#23454;&#29616;&#30446;&#26631;&#12290;&#21382;&#21490;&#19978;&#65292;&#20004;&#31181;&#31454;&#20105;&#30340;SDM&#33539;&#20363;&#20027;&#23548;&#20102;&#35813;&#39046;&#22495;&#12290;&#33258;&#21160;&#21270;&#35268;&#21010;&#65288;AP&#65289;&#25552;&#20986;&#36890;&#36807;&#23545;&#19990;&#30028;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#20915;SDP&#65292;&#36890;&#24120;&#20351;&#29992;&#31526;&#21495;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21017;&#25552;&#20986;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;SDP&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#38656;&#35201;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#20197;&#20122;&#31526;&#21495;&#24418;&#24335;&#34920;&#31034;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#12290;&#26412;&#32508;&#36848;&#22312;&#21327;&#35843;&#20004;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;SDM&#30340;&#31526;&#21495;&#12289;&#20122;&#31526;&#21495;&#21644;&#28151;&#21512;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#35299;&#20915;SDP&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;AP&#12289;RL&#21644;&#23398;&#20064;&#35268;&#21010;&#30340;&#25216;&#26415;&#65289;&#20197;&#21450;&#23398;&#20064;&#20854;&#32467;&#26500;&#30340;&#26041;&#38754;&#65288;&#20363;&#22914;&#19990;&#30028;&#27169;&#22411;&#12289;&#29366;&#24577;&#19981;&#21464;&#37327;&#21644;&#22320;&#26631;&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#35813;&#39046;&#22495;&#20013;&#27809;&#26377;&#20854;&#20182;&#32508;&#36848;&#25552;&#20379;&#30456;&#21516;&#30340;&#33539;&#22260;&#12290;&#20316;&#20026;&#39069;&#22806;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21508;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#22914;&#20309;&#23558;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#22823;&#12289;&#26356;&#22797;&#26434;&#30340;&#39046;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Sequential Decision Making (SDM) provides tools for solving Sequential Decision Processes (SDPs), where an agent must make a series of decisions in order to complete a task or achieve a goal. Historically, two competing SDM paradigms have view for supremacy. Automated Planning (AP) proposes to solve SDPs by performing a reasoning process over a model of the world, often represented symbolically. Conversely, Reinforcement Learning (RL) proposes to learn the solution of the SDP from data, without a world model, and represent the learned knowledge subsymbolically. In the spirit of reconciliation, we provide a review of symbolic, subsymbolic and hybrid methods for SDM. We cover both methods for solving SDPs (e.g., AP, RL and techniques that learn to plan) and for learning aspects of their structure (e.g., world models, state invariants and landmarks). To the best of our knowledge, no other review in the field provides the same scope. As an additional contribution, we discuss w
&lt;/p&gt;</description></item><item><title>&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16166</link><description>&lt;p&gt;
&#27809;&#26377;&#27491;&#30830;&#24615;&#30340;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#37325;&#35201;&#65306;&#22312;NLP&#39046;&#22495;&#20013;&#27979;&#35797;&#20195;&#30721;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16166
&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20854;&#22312;&#30740;&#31350;&#23454;&#39564;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20195;&#30721;&#27491;&#30830;&#24615;&#24448;&#24448;&#20165;&#22522;&#20110;&#32467;&#26524;&#30340;&#24863;&#30693;&#36136;&#37327;&#32780;&#34987;&#20551;&#23450;&#12290;&#36825;&#24102;&#26469;&#20102;&#38169;&#35823;&#32467;&#26524;&#21644;&#28508;&#22312;&#35823;&#23548;&#24615;&#21457;&#29616;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#20851;&#27880;&#32467;&#26524;&#37325;&#29616;&#24212;&#35813;&#19982;&#24378;&#35843;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#25903;&#25345;&#25105;&#20204;&#21521;NLP&#31038;&#21306;&#21457;&#20986;&#30340;&#21495;&#21484;&#65292;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#24182;&#32416;&#27491;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;Conformer&#26550;&#26500;&#30340;&#24320;&#28304;&#23454;&#29616;&#20013;&#30340;&#19977;&#20010;Bug&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Bug&#30340;&#23384;&#22312;&#24182;&#19981;&#20250;&#22952;&#30861;&#33719;&#24471;&#33391;&#22909;&#30340;&#21644;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#65292;&#21453;&#32780;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#33021;&#25552;&#20379;&#38169;&#35823;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#39033;&#30740;&#31350;&#21628;&#21505;&#37319;&#29992;&#26088;&#22312;&#20419;&#36827;NLP&#30740;&#31350;&#20013;&#27491;&#30830;&#24615;&#30340;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#25509;&#35302;&#38544;&#24335;&#21452;&#23618;&#20248;&#21270;&#26469;&#35268;&#21010;&#25903;&#28857;&#25805;&#32437;&#24182;&#22686;&#21152;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#25705;&#25830;&#21147;&#26469;&#24357;&#34917;&#29289;&#20307;&#21644;&#29615;&#22659;&#29289;&#29702;&#23646;&#24615;&#20272;&#35745;&#20013;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#20197;&#24212;&#23545;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.08965</link><description>&lt;p&gt;
&#20351;&#29992;&#25509;&#35302;&#38544;&#24335;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#40065;&#26834;&#30340;&#25903;&#28857;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Robust Pivoting Manipulation using Contact Implicit Bilevel Optimization. (arXiv:2303.08965v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25509;&#35302;&#38544;&#24335;&#21452;&#23618;&#20248;&#21270;&#26469;&#35268;&#21010;&#25903;&#28857;&#25805;&#32437;&#24182;&#22686;&#21152;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#25705;&#25830;&#21147;&#26469;&#24357;&#34917;&#29289;&#20307;&#21644;&#29615;&#22659;&#29289;&#29702;&#23646;&#24615;&#20272;&#35745;&#20013;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#20197;&#24212;&#23545;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#25805;&#32437;&#35201;&#27714;&#26426;&#22120;&#20154;&#33021;&#22815;&#19982;&#26032;&#29289;&#20307;&#21644;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#20010;&#35201;&#27714;&#20351;&#24471;&#25805;&#32437;&#21464;&#24471;&#24322;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#24517;&#39035;&#32771;&#34385;&#21040;&#19981;&#30830;&#23450;&#22240;&#32032;&#19979;&#30340;&#22797;&#26434;&#25705;&#25830;&#30456;&#20114;&#20316;&#29992;&#21450;&#29289;&#20307;&#21644;&#29615;&#22659;&#30340;&#29289;&#29702;&#23646;&#24615;&#20272;&#35745;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25903;&#28857;&#25805;&#20316;&#35268;&#21010;&#30340;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#22914;&#20309;&#21033;&#29992;&#25705;&#25830;&#21147;&#26469;&#24357;&#34917;&#29289;&#29702;&#29305;&#24615;&#20272;&#35745;&#20013;&#30340;&#19981;&#20934;&#30830;&#24615;&#30340;&#35265;&#35299;&#12290;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#23548;&#20986;&#20102;&#25705;&#25830;&#21147;&#25552;&#20379;&#30340;&#25903;&#28857;&#25805;&#20316;&#31283;&#23450;&#35029;&#24230;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;&#28982;&#21518;&#65292;&#22312;&#25509;&#35302;&#38544;&#24335;&#21452;&#23618;&#20248;&#21270;(CIBO)&#26694;&#26550;&#20013;&#20351;&#29992;&#35813;&#35029;&#24230;&#26469;&#20248;&#21270;&#36712;&#36857; &#65292;&#20197;&#22686;&#24378;&#23545;&#29289;&#20307;&#22810;&#20010;&#29289;&#29702;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#23545;&#20110;&#20005;&#37325;&#24178;&#25200;&#30340;&#21442;&#25968;&#65292;&#20998;&#26512;&#20102;&#31283;&#23450;&#35029;&#24230;&#65292;&#24182;&#26174;&#31034;&#20102;&#20248;&#21270;&#36712;&#36857;&#30340;&#25913;&#21892;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalizable manipulation requires that robots be able to interact with novel objects and environment. This requirement makes manipulation extremely challenging as a robot has to reason about complex frictional interactions with uncertainty in physical properties of the object and the environment. In this paper, we study robust optimization for planning of pivoting manipulation in the presence of uncertainties. We present insights about how friction can be exploited to compensate for inaccuracies in the estimates of the physical properties during manipulation. Under certain assumptions, we derive analytical expressions for stability margin provided by friction during pivoting manipulation. This margin is then used in a Contact Implicit Bilevel Optimization (CIBO) framework to optimize a trajectory that maximizes this stability margin to provide robustness against uncertainty in several physical parameters of the object. We present analysis of the stability margin with respect to sever
&lt;/p&gt;</description></item></channel></rss>