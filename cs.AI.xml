<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#35774;&#35745;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22312;&#36328;&#27983;&#35272;&#22120;&#29615;&#22659;&#19979;&#26377;&#25928;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#65292;&#32467;&#26524;&#22312;Chrome&#21644;Firefox&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01240</link><description>&lt;p&gt;
&#36229;&#36234;&#35831;&#27714;&#65306;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#22312;&#19981;&#24179;&#34913;&#29615;&#22659;&#20013;&#36827;&#34892;&#36328;&#27983;&#35272;&#22120;Web&#36861;&#36394;&#22120;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser Web Tracker Classification in an Imbalanced Setting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#35774;&#35745;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22312;&#36328;&#27983;&#35272;&#22120;&#29615;&#22659;&#19979;&#26377;&#25928;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#65292;&#32467;&#26524;&#22312;Chrome&#21644;Firefox&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19975;&#32500;&#32593;&#30340;&#36830;&#36890;&#24615;&#20027;&#35201;&#24402;&#22240;&#20110;HTTP&#21327;&#35758;&#65292;&#20854;&#20013;&#30340;HTTP&#28040;&#24687;&#25552;&#20379;&#20102;&#26377;&#20851;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#20449;&#24687;&#22836;&#23383;&#27573;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;Web&#36861;&#36394;&#12290;&#23613;&#31649;&#24050;&#26377;&#30740;&#31350;&#21033;&#29992;HTTP/S&#35831;&#27714;&#28040;&#24687;&#26469;&#35782;&#21035;Web&#36861;&#36394;&#22120;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;HTTP/S&#21709;&#24212;&#22836;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#20351;&#29992;HTTP/S&#21709;&#24212;&#22836;&#36827;&#34892;Web&#36861;&#36394;&#22120;&#26816;&#27979;&#30340;&#26377;&#25928;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#27983;&#35272;&#22120;&#25193;&#23637;&#31243;&#24207;T.EX&#33719;&#21462;&#30340;Chrome&#12289;Firefox&#21644;Brave&#27983;&#35272;&#22120;&#30340;&#25968;&#25454;&#20316;&#20026;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;Chrome&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;11&#20010;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#22312;&#25152;&#26377;&#27983;&#35272;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;Chrome&#21644;Firefox&#19978;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12289;F1&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#26368;&#23567;&#23545;&#25968;&#25439;&#22833;&#35823;&#24046;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;Brave&#27983;&#35272;&#22120;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20854;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#24449;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The World Wide Web's connectivity is greatly attributed to the HTTP protocol, with HTTP messages offering informative header fields that appeal to disciplines like web security and privacy, especially concerning web tracking. Despite existing research employing HTTP/S request messages to identify web trackers, HTTP/S response headers are often overlooked. This study endeavors to design effective machine learning classifiers for web tracker detection using HTTP/S response headers. Data from the Chrome, Firefox, and Brave browsers, obtained through the traffic monitoring browser extension T.EX, serves as our data set. Eleven supervised models were trained on Chrome data and tested across all browsers. The results demonstrated high accuracy, F1-score, precision, recall, and minimal log-loss error for Chrome and Firefox, but subpar performance on Brave, potentially due to its distinct data distribution and feature set. The research suggests that these classifiers are viable for detecting w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;(DGA)&#21644;&#35299;&#32544;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;(DVGA)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01143</link><description>&lt;p&gt;
&#29992;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#32593;&#32476;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Network Representations with Disentangled Graph Auto-Encoder
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;(DGA)&#21644;&#35299;&#32544;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;(DVGA)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
(&#21464;&#20998;)&#22270;&#33258;&#32534;&#30721;&#22120;&#24191;&#27867;&#29992;&#20110;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#22270;&#30340;&#24418;&#25104;&#26159;&#19968;&#20010;&#30001;&#28508;&#22312;&#22240;&#32032;&#24433;&#21709;&#30340;&#22797;&#26434;&#21644;&#24322;&#36136;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;&#22522;&#26412;&#19978;&#26159;&#25972;&#20307;&#30340;&#65292;&#24573;&#35270;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#32416;&#32544;&#12290;&#36825;&#19981;&#20165;&#20351;&#24471;&#22270;&#20998;&#26512;&#20219;&#21153;&#19981;&#22826;&#26377;&#25928;&#65292;&#32780;&#19988;&#20351;&#24471;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#34920;&#31034;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#29992;(&#21464;&#20998;)&#22270;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#35299;&#32544;&#30340;&#22270;&#34920;&#31034;&#38754;&#20020;&#30528;&#37325;&#35201;&#25361;&#25112;&#65292;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;(DGA)&#21644;&#35299;&#32544;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;(DVGA)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#35299;&#32544;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#20351;&#29992;&#22810;&#36890;&#36947;&#28040;&#24687;&#20256;&#36882;&#23618;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#32858;&#21512;&#19982;&#27599;&#20010;&#33410;&#28857;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The (variational) graph auto-encoder is extensively employed for learning representations of graph-structured data. However, the formation of real-world graphs is a complex and heterogeneous process influenced by latent factors. Existing encoders are fundamentally holistic, neglecting the entanglement of latent factors. This not only makes graph analysis tasks less effective but also makes it harder to understand and explain the representations. Learning disentangled graph representations with (variational) graph auto-encoder poses significant challenges, and remains largely unexplored in the existing literature. In this article, we introduce the Disentangled Graph Auto-Encoder (DGA) and Disentangled Variational Graph Auto-Encoder (DVGA), approaches that leverage generative models to learn disentangled representations. Specifically, we first design a disentangled graph convolutional network with multi-channel message-passing layers, as the encoder aggregating information related to eac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Generalized Latent Equilibrium (GLE)&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#29289;&#29702;&#21160;&#24577;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.16933</link><description>&lt;p&gt;
&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#22823;&#33041;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Backpropagation through space, time, and the brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Generalized Latent Equilibrium (GLE)&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#29289;&#29702;&#21160;&#24577;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38656;&#35201;&#26681;&#25454;&#23427;&#20204;&#23545;&#35299;&#20915;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#26469;&#35843;&#25972;&#21333;&#20010;&#31361;&#35302;&#12290;&#28982;&#32780;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#36824;&#26159;&#20154;&#24037;&#30340;&#29289;&#29702;&#31070;&#32463;&#31995;&#32479;&#37117;&#21463;&#21040;&#26102;&#31354;&#23616;&#38480;&#12290;&#36825;&#26679;&#30340;&#32593;&#32476;&#22914;&#20309;&#25191;&#34892;&#39640;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#38169;&#35823;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#20960;&#20046;&#26222;&#36941;&#34987;&#31354;&#38388;&#65288;BP&#65289;&#21644;&#26102;&#38388;&#65288;BPTT&#65289;&#20004;&#31181;&#26041;&#24335;&#32473;&#20986;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;BP(TT)&#34987;&#24191;&#27867;&#35748;&#20026;&#20381;&#36182;&#20110;&#19981;&#20855;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#20551;&#35774;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#26102;&#31354;&#23616;&#38480;&#24615;&#65292;&#32780;&#27491;&#21521;&#20256;&#25773;&#27169;&#22411;&#65292;&#22914;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#65292;&#21017;&#21463;&#21040;&#20869;&#23384;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24191;&#20041;&#28508;&#22312;&#24179;&#34913;&#65288;GLE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#31070;&#32463;&#20803;&#29289;&#29702;&#21160;&#24577;&#32593;&#32476;&#23436;&#20840;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;&#25105;&#20204;&#20174;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16933v1 Announce Type: cross  Abstract: Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#25239;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#20013;&#30340;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#31561;&#35828;&#26381;&#26041;&#24335;&#65292;&#23545;&#27604;&#23553;&#38381;&#21644;&#24320;&#25918;&#20132;&#20114;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#21644;&#35805;&#39064;&#23618;&#38754;&#65292;&#21457;&#29616;&#20102;&#22312;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.15449</link><description>&lt;p&gt;
&#24974;&#24680;&#28304;&#20110;&#26080;&#30693;&#65281;&#23545;&#25239;&#20250;&#35805;&#24615;&#20167;&#24680;&#35328;&#35770;&#20013;&#35828;&#26381;&#26041;&#24335;&#30340;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Hatred Stems from Ignorance! Distillation of the Persuasion Modes in Countering Conversational Hate Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15449
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#25239;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#20013;&#30340;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#31561;&#35828;&#26381;&#26041;&#24335;&#65292;&#23545;&#27604;&#23553;&#38381;&#21644;&#24320;&#25918;&#20132;&#20114;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#21644;&#35805;&#39064;&#23618;&#38754;&#65292;&#21457;&#29616;&#20102;&#22312;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#25239;&#35328;&#35770;&#20351;&#29992;&#30340;&#22240;&#32032;&#26159;&#29702;&#35299;&#22312;&#32447;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#30340;&#26680;&#24515;&#12290;&#21508;&#31181;&#30740;&#31350;&#35780;&#20272;&#23545;&#25239;&#35328;&#35770;&#20013;&#20351;&#29992;&#30340;&#24773;&#24863;&#22522;&#30784;&#22240;&#32032;&#65292;&#22914;&#24773;&#24863;&#20849;&#40483;&#12289;&#20882;&#29359;&#31243;&#24230;&#21644;&#25932;&#24847;&#31243;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20250;&#35805;&#20132;&#20114;&#20013;&#20351;&#29992;&#30340;&#23545;&#25239;&#35328;&#35770;&#65292;&#26412;&#30740;&#31350;&#23558;&#35828;&#26381;&#26041;&#24335;&#20998;&#35299;&#20026;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#65292;&#28982;&#21518;&#35780;&#20272;&#23427;&#20204;&#22312;&#28041;&#21450;&#31181;&#26063;&#20027;&#20041;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#23447;&#25945;&#38382;&#39064;&#30340;&#20004;&#31181;&#23545;&#35805;&#20132;&#20114;&#31867;&#22411;&#20013;&#30340;&#20351;&#29992;&#12290;&#35780;&#20272;&#28085;&#30422;&#20102;&#20154;&#31867;&#19982;&#29983;&#25104;&#23545;&#25239;&#35328;&#35770;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22238;&#22797;&#30340;&#31435;&#22330;&#19982;&#27599;&#31181;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#35828;&#26381;&#26041;&#24335;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#22312;&#24320;&#25918;&#21644;&#23553;&#38381;&#20132;&#20114;&#30340;&#23545;&#25239;&#35328;&#35770;&#35828;&#26381;&#26041;&#24335;&#19978;&#30340;&#24494;&#22937;&#24046;&#24322; -- &#23588;&#20854;&#26159;&#22312;&#35805;&#39064;&#23618;&#38754;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15449v1 Announce Type: cross  Abstract: Examining the factors that the counter-speech uses is at the core of understanding the optimal methods for confronting hate speech online. Various studies assess the emotional base factor used in counter speech, such as emotion-empathy, offensiveness, and level of hostility. To better understand the counter-speech used in conversational interactions, this study distills persuasion modes into reason, emotion, and credibility and then evaluates their use in two types of conversation interactions: closed (multi-turn) and open (single-turn) conversation interactions concerning racism, sexism, and religion. The evaluation covers the distinct behaviors of human versus generated counter-speech. We also assess the interplay between the replies' stance and each mode of persuasion in the counter-speech. Notably, we observe nuanced differences in the counter-speech persuasion modes for open and closed interactions -- especially on the topic level
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.11021</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Video Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#35270;&#39057;&#25968;&#25454;&#29983;&#20135;&#30340;&#31354;&#21069;&#28608;&#22686;&#38656;&#27714;&#39640;&#25928;&#30340;&#24037;&#20855;&#65292;&#20197;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#24103;&#20379;&#19979;&#28216;&#20219;&#21153;&#20351;&#29992;&#12290; &#38271;&#26399;&#26102;&#38388;&#25512;&#29702;&#26159;&#24103;&#26816;&#32034;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#35201;&#27714;&#12290; &#34429;&#28982; VideoLLaMA &#21644; ViCLIP &#31561;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#30701;&#26399;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23427;&#20204;&#22312;&#36328;&#24103;&#30340;&#38271;&#26399;&#25512;&#29702;&#26041;&#38754;&#21364;&#20196;&#20154;&#24778;&#35766;&#22320;&#22833;&#36133;&#12290; &#36825;&#31181;&#22833;&#36133;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#23427;&#20204;&#23558;&#36880;&#24103;&#24863;&#30693;&#21644;&#26102;&#38388;&#25512;&#29702;&#20132;&#32455;&#25104;&#21333;&#20010;&#28145;&#24230;&#32593;&#32476;&#12290; &#22240;&#27492;&#65292;&#35299;&#32806;&#20294;&#20849;&#21516;&#35774;&#35745;&#35821;&#20041;&#29702;&#35299;&#21644;&#26102;&#38388;&#25512;&#29702;&#23545;&#20110;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#21333;&#20010;&#24103;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#20294;&#26377;&#25928;&#22320;&#36890;&#36807;&#20351;&#29992;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#65288;TL&#65289;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#20123;&#20844;&#24335;&#22312;&#26412;&#36136;&#19978;&#25429;&#25417;&#20102;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11021v1 Announce Type: cross  Abstract: The unprecedented surge in video data production in recent years necessitates efficient tools to extract meaningful frames from videos for downstream tasks. Long-term temporal reasoning is a key desideratum for frame retrieval systems. While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are proficient in short-term semantic understanding, they surprisingly fail at long-term reasoning across frames. A key reason for this failure is that they intertwine per-frame perception and temporal reasoning into a single deep network. Hence, decoupling but co-designing semantic understanding and temporal reasoning is essential for efficient scene identification. We propose a system that leverages vision-language models for semantic understanding of individual frames but effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory. Our TL-based reas
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#25506;&#35752;&#20102;Q-learning&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#21463;&#21040;&#31574;&#30053;&#24615;&#23545;&#25163;&#30340;&#25805;&#32437;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08906</link><description>&lt;p&gt;
&#38024;&#23545;Q-&#23398;&#20064;&#32773;&#30340;&#31574;&#30053;&#21270;&#23545;&#25239;&#65306;&#19968;&#31181;&#25511;&#21046;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Strategizing against Q-learners: A Control-theoretical Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08906
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#25506;&#35752;&#20102;Q-learning&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#21463;&#21040;&#31574;&#30053;&#24615;&#23545;&#25163;&#30340;&#25805;&#32437;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;Q-learning&#31639;&#27861;(&#19968;&#31181;&#32463;&#20856;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;)&#22312;&#28216;&#25103;&#20013;&#23545;&#31574;&#30053;&#24615;&#23545;&#25163;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#22914;&#26524;&#31574;&#30053;&#24615;&#23545;&#25163;&#20102;&#35299;&#23545;&#25163;&#30340;Q-learning&#31639;&#27861;&#65292;&#22905;&#21487;&#20197;&#21033;&#29992;&#19968;&#20010;&#22825;&#30495;&#30340;Q-&#23398;&#20064;&#32773;&#22810;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#31574;&#30053;&#34892;&#20026;&#32773;&#30340;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(&#20855;&#26377;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;Q&#20540;&#30340;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;)&#65292;&#23601;&#22909;&#20687;Q-&#23398;&#20064;&#31639;&#27861;&#26159;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#19968;&#26679;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37327;&#21270;&#30340;&#36817;&#20284;&#26041;&#26696;&#26469;&#22788;&#29702;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#20998;&#26512;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08906v1 Announce Type: cross  Abstract: In this paper, we explore the susceptibility of the Q-learning algorithm (a classical and widely used reinforcement learning method) to strategic manipulation of sophisticated opponents in games. We quantify how much a strategically sophisticated agent can exploit a naive Q-learner if she knows the opponent's Q-learning algorithm. To this end, we formulate the strategic actor's problem as a Markov decision process (with a continuum state space encompassing all possible Q-values) as if the Q-learning algorithm is the underlying dynamical system. We also present a quantization-based approximation scheme to tackle the continuum state space and analyze its performance both analytically and numerically.
&lt;/p&gt;</description></item><item><title>MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02694</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24863;&#30693;&#35821;&#20041;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Semantic Cache for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02694
&lt;/p&gt;
&lt;p&gt;
MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12289;Google Bard&#12289;Claude&#21644;Llama 2&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25628;&#32034;&#24341;&#25806;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36896;&#25104;&#20102;&#24322;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MeanCache&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#20197;&#30830;&#23450;&#32531;&#23384;&#21629;&#20013;&#25110;&#26410;&#21629;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#39034;&#24207;&#21270;&#30340;MCMC&#32467;&#26500;&#23398;&#20064;&#25216;&#26415;&#21644;&#26799;&#24230;&#22270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#23558;&#22240;&#26524;&#32467;&#26500;&#25512;&#26029;&#38382;&#39064;&#20998;&#35299;&#20026;&#21464;&#37327;&#25299;&#25169;&#39034;&#24207;&#25512;&#26029;&#21644;&#21464;&#37327;&#29238;&#33410;&#28857;&#38598;&#21512;&#25512;&#26029;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22240;&#26524;&#26426;&#21046;&#24314;&#27169;&#23454;&#29616;&#31934;&#30830;&#36793;&#32536;&#21270;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;Rao-Blackwell&#21270;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.14781</link><description>&lt;p&gt;
Rao-Blackwellising Bayesian Causal Inference
&lt;/p&gt;
&lt;p&gt;
Rao-Blackwellising Bayesian Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#39034;&#24207;&#21270;&#30340;MCMC&#32467;&#26500;&#23398;&#20064;&#25216;&#26415;&#21644;&#26799;&#24230;&#22270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#23558;&#22240;&#26524;&#32467;&#26500;&#25512;&#26029;&#38382;&#39064;&#20998;&#35299;&#20026;&#21464;&#37327;&#25299;&#25169;&#39034;&#24207;&#25512;&#26029;&#21644;&#21464;&#37327;&#29238;&#33410;&#28857;&#38598;&#21512;&#25512;&#26029;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22240;&#26524;&#26426;&#21046;&#24314;&#27169;&#23454;&#29616;&#31934;&#30830;&#36793;&#32536;&#21270;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;Rao-Blackwell&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#65292;&#21363;&#25512;&#26029;&#29992;&#20110;&#19979;&#28216;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#22240;&#26524;&#27169;&#22411;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#26500;&#25104;&#20102;&#19968;&#20010;&#22312;&#25991;&#29486;&#20013;&#40092;&#26377;&#25506;&#35752;&#30340;&#38590;&#35299;&#30340;&#35745;&#31639;&#25512;&#26029;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#22522;&#20110;&#39034;&#24207;&#30340;MCMC&#32467;&#26500;&#23398;&#20064;&#25216;&#26415;&#19982;&#26368;&#36817;&#26799;&#24230;&#22270;&#23398;&#20064;&#30340;&#36827;&#23637;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#30340;&#38382;&#39064;&#20998;&#35299;&#20026;(i)&#25512;&#26029;&#21464;&#37327;&#20043;&#38388;&#30340;&#25299;&#25169;&#39034;&#24207;&#20197;&#21450;(ii)&#25512;&#26029;&#27599;&#20010;&#21464;&#37327;&#30340;&#29238;&#33410;&#28857;&#38598;&#21512;&#12290;&#24403;&#38480;&#21046;&#27599;&#20010;&#21464;&#37327;&#30340;&#29238;&#33410;&#28857;&#25968;&#37327;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23436;&#20840;&#36793;&#32536;&#21270;&#29238;&#33410;&#28857;&#38598;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#24314;&#27169;&#26410;&#30693;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#20174;&#32780;&#20801;&#35768;&#20854;&#31934;&#30830;&#36793;&#32536;&#21270;&#12290;&#36825;&#24341;&#20837;&#20102;&#19968;&#20010;Rao-Blackwell&#21270;&#26041;&#26696;&#65292;&#20854;&#20013;&#38500;&#20102;&#22240;&#26524;&#39034;&#24207;&#20043;&#22806;&#65292;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#37117;&#34987;&#28040;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14781v1 Announce Type: cross  Abstract: Bayesian causal inference, i.e., inferring a posterior over causal models for the use in downstream causal reasoning tasks, poses a hard computational inference problem that is little explored in literature. In this work, we combine techniques from order-based MCMC structure learning with recent advances in gradient-based graph learning into an effective Bayesian causal inference framework. Specifically, we decompose the problem of inferring the causal structure into (i) inferring a topological order over variables and (ii) inferring the parent sets for each variable. When limiting the number of parents per variable, we can exactly marginalise over the parent sets in polynomial time. We further use Gaussian processes to model the unknown causal mechanisms, which also allows their exact marginalisation. This introduces a Rao-Blackwellization scheme, where all components are eliminated from the model, except for the causal order, for whi
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22240;&#26524;&#22270;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#24515;&#29702;&#23398;&#20551;&#35774;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#32852;&#21512;&#26041;&#27861;&#22312;&#26032;&#39062;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2402.14424</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#21270;&#24515;&#29702;&#23398;&#20551;&#35774;&#29983;&#25104;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14424
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22240;&#26524;&#22270;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#24515;&#29702;&#23398;&#20551;&#35774;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#32852;&#21512;&#26041;&#27861;&#22312;&#26032;&#39062;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#29983;&#25104;&#24515;&#29702;&#23398;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;LLM&#20998;&#26512;&#20102;43,312&#31687;&#24515;&#29702;&#23398;&#25991;&#31456;&#65292;&#25552;&#21462;&#20102;&#22240;&#26524;&#20851;&#31995;&#23545;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#24515;&#29702;&#23398;&#30340;&#22240;&#26524;&#22270;&#12290;&#24212;&#29992;&#38142;&#25509;&#39044;&#27979;&#31639;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;130&#20010;&#20851;&#27880;&#8220;&#24184;&#31119;&#8221;&#30340;&#28508;&#22312;&#24515;&#29702;&#23398;&#20551;&#35774;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#21338;&#22763;&#23398;&#32773;&#26500;&#24605;&#30340;&#30740;&#31350;&#24819;&#27861;&#21644;&#20165;&#30001;LLM&#20135;&#29983;&#30340;&#24819;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;LLM&#21644;&#22240;&#26524;&#22270;&#30340;&#32852;&#21512;&#26041;&#27861;&#22312;&#26032;&#39062;&#24615;&#26041;&#38754;&#19982;&#19987;&#23478;&#27700;&#24179;&#30340;&#27934;&#23519;&#21147;&#20445;&#25345;&#19968;&#33268;&#65292;&#26126;&#26174;&#20248;&#20110;&#20165;LLM&#30340;&#20551;&#35774;&#65288;&#20998;&#21035;&#20026;t(59)=3.34&#65292;p=0.007&#21644;t(59)=4.32&#65292;p&lt;0.001&#65289;&#12290;&#36825;&#31181;&#19968;&#33268;&#24615;&#36827;&#19968;&#27493;&#36890;&#36807;&#28145;&#24230;&#35821;&#20041;&#20998;&#26512;&#24471;&#21040;&#35777;&#23454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;LLM&#19982;&#22240;&#26524;&#22270;&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#24515;&#29702;&#23398;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14424v1 Announce Type: new  Abstract: Leveraging the synergy between causal knowledge graphs and a large language model (LLM), our study introduces a groundbreaking approach for computational hypothesis generation in psychology. We analyzed 43,312 psychology articles using a LLM to extract causal relation pairs. This analysis produced a specialized causal graph for psychology. Applying link prediction algorithms, we generated 130 potential psychological hypotheses focusing on `well-being', then compared them against research ideas conceived by doctoral scholars and those produced solely by the LLM. Interestingly, our combined approach of a LLM and causal graphs mirrored the expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) = 4.32, p&lt;0.001, respectively). This alignment was further corroborated using deep semantic analysis. Our results show that combining LLM with machine learning techniques such as causal k
&lt;/p&gt;</description></item><item><title>FairProof&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#26469;&#20844;&#24320;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#25345;&#26426;&#23494;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;ZKPs&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12572</link><description>&lt;p&gt;
FairProof&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#23494;&#21644;&#21487;&#35748;&#35777;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
FairProof : Confidential and Certifiable Fairness for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12572
&lt;/p&gt;
&lt;p&gt;
FairProof&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#26469;&#20844;&#24320;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#25345;&#26426;&#23494;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;ZKPs&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#31038;&#20250;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#28982;&#32780;&#27861;&#24459;&#21644;&#38544;&#31169;&#38382;&#39064;&#35201;&#27714;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#38656;&#35201;&#20445;&#23494;&#12290;&#22240;&#27492;&#65292;&#28040;&#36153;&#32773;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#23646;&#24615;&#36234;&#26469;&#36234;&#19981;&#20449;&#20219;&#65292;&#28040;&#36153;&#32773;&#36890;&#24120;&#26159;&#27169;&#22411;&#39044;&#27979;&#30340;&#25509;&#25910;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FairProof - &#19968;&#31181;&#31995;&#32479;&#65292;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#65288;&#19968;&#31181;&#23494;&#30721;&#21407;&#35821;&#65289;&#26469;&#20844;&#24320;&#39564;&#35777;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#26426;&#23494;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#21512;&#20110;ZKPs&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#31639;&#27861;&#65292;&#24182;&#22312;&#35813;&#31995;&#32479;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;Gnark&#20013;&#23454;&#29616;&#20102;FairProof&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#23454;&#38469;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12572v1 Announce Type: cross  Abstract: Machine learning models are increasingly used in societal applications, yet legal and privacy concerns demand that they very often be kept confidential. Consequently, there is a growing distrust about the fairness properties of these models in the minds of consumers, who are often at the receiving end of model predictions. To this end, we propose FairProof - a system that uses Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the fairness of a model, while maintaining confidentiality. We also propose a fairness certification algorithm for fully-connected neural networks which is befitting to ZKPs and is used in this system. We implement FairProof in Gnark and demonstrate empirically that our system is practically feasible.
&lt;/p&gt;</description></item><item><title>LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02544</link><description>&lt;p&gt;
LHRS-Bot&#65306;&#21033;&#29992;VGI&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36965;&#24863;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02544
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38761;&#21629;&#24615;&#33021;&#21147;&#24320;&#21019;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24182;&#20419;&#36827;&#20102;&#22312;&#21508;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36965;&#24863;&#65288;RS&#65289;&#39046;&#22495;&#20013;&#65292;&#36817;&#26399;&#30340;MLLM&#21162;&#21147;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#36965;&#24863;&#22270;&#20687;&#20013;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;LHRS-Align&#65292;&#20197;&#21450;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;RS&#29305;&#23450;&#25351;&#23548;&#25968;&#25454;&#38598;LHRS-Instruct&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;&#65288;VGI&#65289;&#21644;&#20840;&#29699;&#21487;&#29992;&#30340;RS&#22270;&#20687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LHRS-Bot&#65292;&#19968;&#31181;&#38024;&#23545;RS&#22270;&#20687;&#29702;&#35299;&#30340;MLLM&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#24212;&#29992;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#26377;&#25928;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2401.16650</link><description>&lt;p&gt;
&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22238;&#25918;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Augmenting Replay in World Models for Continual Reinforcement Learning. (arXiv:2401.16650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#24212;&#29992;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#26377;&#25928;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#29615;&#22659;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#25104;&#21151;&#30340;&#31995;&#32479;&#24212;&#35813;&#36866;&#24403;&#24179;&#34913;&#20445;&#25345;&#24050;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#20195;&#29702;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#35201;&#27714;&#12290;&#39318;&#36827;&#20808;&#20986;&#32531;&#20914;&#21306;&#36890;&#24120;&#29992;&#20110;&#22686;&#24378;&#27492;&#31867;&#35774;&#32622;&#20013;&#30340;&#23398;&#20064;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22686;&#24378;&#26041;&#27861;&#24212;&#29992;&#20110;&#27492;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#32531;&#35299;&#20869;&#23384;&#38480;&#21046;&#65292;&#24182;&#19982;&#22522;&#20110;&#19990;&#30028;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#35780;&#20272;&#20854;&#22312;&#20419;&#36827;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;Procgen&#21644;Atari&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28508;&#22312;&#19990;&#30028;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#20998;&#24067;&#21305;&#37197;&#22686;&#24378;&#21487;&#20197;&#25104;&#21151;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24182;&#38750;&#23436;&#20840;&#26080;&#25032;&#21487;&#20987;&#65292;
&lt;/p&gt;
&lt;p&gt;
In continual RL, the environment of a reinforcement learning (RL) agent undergoes change. A successful system should appropriately balance the conflicting requirements of retaining agent performance on already learned tasks, stability, whilst learning new tasks, plasticity. The first-in-first-out buffer is commonly used to enhance learning in such settings but requires significant memory. We explore the application of an augmentation to this buffer which alleviates the memory constraints, and use it with a world model model-based reinforcement learning algorithm, to evaluate its effectiveness in facilitating continual learning. We evaluate the effectiveness of our method in Procgen and Atari RL benchmarks and show that the distribution matching augmentation to the replay-buffer used in the context of latent world models can successfully prevent catastrophic forgetting with significantly reduced computational overhead. Yet, we also find such a solution to not be entirely infallible, and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28014;&#28857;&#36816;&#31639;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20108;&#36827;&#21046;&#38408;&#20540;&#21333;&#20803;&#25110;ReLU&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#35760;&#24518;&#20219;&#20309;&#23454;&#25968;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23567;&#35823;&#24046;&#20869;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.15121</link><description>&lt;p&gt;
ReLU&#21644;Step&#32593;&#32476;&#22312;&#28014;&#28857;&#36816;&#31639;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Expressive Power of ReLU and Step Networks under Floating-Point Operations. (arXiv:2401.15121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28014;&#28857;&#36816;&#31639;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20108;&#36827;&#21046;&#38408;&#20540;&#21333;&#20803;&#25110;ReLU&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#35760;&#24518;&#20219;&#20309;&#23454;&#25968;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23567;&#35823;&#24046;&#20869;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32467;&#26524;&#20551;&#35774;&#23454;&#25968;&#36755;&#20837;&#21644;&#21442;&#25968;&#20197;&#21450;&#22312;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#36807;&#31243;&#20013;&#36827;&#34892;&#31934;&#30830;&#36816;&#31639;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#22312;&#21482;&#33021;&#34920;&#31034;&#23454;&#25968;&#30340;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#65292;&#24182;&#19988;&#36827;&#34892;&#19981;&#31934;&#30830;&#30340;&#36816;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#26356;&#23454;&#38469;&#30340;&#35774;&#32622;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65306;&#20351;&#29992;&#28014;&#28857;&#25968;&#21644;&#28014;&#28857;&#36816;&#31639;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#32452;&#32467;&#26524;&#20551;&#35774;&#28014;&#28857;&#36816;&#31639;&#20013;&#65292;&#28014;&#28857;&#25968;&#30340;&#26377;&#25928;&#20301;&#25968;&#30001;&#26377;&#38480;&#20301;&#34920;&#31034;&#65292;&#20294;&#20854;&#25351;&#25968;&#21487;&#20197;&#21462;&#20219;&#20309;&#25972;&#25968;&#20540;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#20108;&#36827;&#21046;&#38408;&#20540;&#21333;&#20803;&#25110;ReLU&#21487;&#20197;&#35760;&#24518;&#20219;&#20309;&#26377;&#38480;&#30340;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#24182;&#21487;&#20197;&#22312;&#23567;&#35823;&#24046;&#20869;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#28014;&#28857;&#36816;&#31639;&#19979;&#20851;&#20110;&#35760;&#24518;&#21644;&#36890;&#29992;&#36924;&#36817;&#30340;&#31867;&#20284;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of the expressive power of neural networks has investigated the fundamental limits of neural networks. Most existing results assume real-valued inputs and parameters as well as exact operations during the evaluation of neural networks. However, neural networks are typically executed on computers that can only represent a tiny subset of the reals and apply inexact operations. In this work, we analyze the expressive power of neural networks under a more realistic setup: when we use floating-point numbers and operations. Our first set of results assumes floating-point operations where the significand of a float is represented by finite bits but its exponent can take any integer value. Under this setup, we show that neural networks using a binary threshold unit or ReLU can memorize any finite input/output pairs and can approximate any continuous function within a small error. We also show similar results on memorization and universal approximation when floating-point operations u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.13311</link><description>&lt;p&gt;
ConTextual: &#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35780;&#20272;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models. (arXiv:2401.13311v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#20844;&#20849;&#22330;&#25152;&#23548;&#33322;&#22320;&#22270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ConTextual&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#19987;&#38376;&#35774;&#35745;&#30340;&#25351;&#20196;&#65292;&#29992;&#20110;&#35780;&#20272;LMMs&#22312;&#25191;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ConTextual&#24378;&#35843;&#20102;&#22810;&#26679;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#65288;&#20363;&#22914;&#26102;&#38388;&#38405;&#35835;&#12289;&#23548;&#33322;&#12289;&#36141;&#29289;&#31561;&#65289;&#65292;&#35201;&#27714;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;LMM&#65292;GPT-4V(ision)&#65292;&#19982;&#20154;&#31867;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;30.8%&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#20351;&#29992;&#20154;&#31867;&#35780;&#20272;&#25351;&#20986;&#22312;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34429;&#28982;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#65288;&#22914;&#27169;&#22240;&#21644;&#24341;&#25991;&#35299;&#37322;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In add
&lt;/p&gt;</description></item><item><title>TeacherLM-7.1B&#26159;&#19968;&#20010;&#23567;&#22411;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#65292;&#25945;&#20250;&#20854;&#20182;&#27169;&#22411;&#8220;&#20026;&#20160;&#20040;&#8221;&#32780;&#19981;&#20165;&#20165;&#26159;&#8220;&#20160;&#20040;&#8221;&#12290;&#23427;&#22312;MMLU&#19978;&#21462;&#24471;&#20102;52.3&#30340;&#38646;&#26679;&#26412;&#24471;&#20998;&#65292;&#21516;&#26102;&#20855;&#26377;&#20986;&#33394;&#30340;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#12290;&#21457;&#24067;TeacherLM&#31995;&#21015;&#27169;&#22411;&#21644;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#20316;&#20026;&#24320;&#28304;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2310.19019</link><description>&lt;p&gt;
TeacherLM: &#25945;&#20154;&#25171;&#40060;&#32780;&#19981;&#26159;&#32473;&#40060;&#65292;&#35821;&#35328;&#24314;&#27169;&#21516;&#29702;
&lt;/p&gt;
&lt;p&gt;
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise. (arXiv:2310.19019v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19019
&lt;/p&gt;
&lt;p&gt;
TeacherLM-7.1B&#26159;&#19968;&#20010;&#23567;&#22411;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#65292;&#25945;&#20250;&#20854;&#20182;&#27169;&#22411;&#8220;&#20026;&#20160;&#20040;&#8221;&#32780;&#19981;&#20165;&#20165;&#26159;&#8220;&#20160;&#20040;&#8221;&#12290;&#23427;&#22312;MMLU&#19978;&#21462;&#24471;&#20102;52.3&#30340;&#38646;&#26679;&#26412;&#24471;&#20998;&#65292;&#21516;&#26102;&#20855;&#26377;&#20986;&#33394;&#30340;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#12290;&#21457;&#24067;TeacherLM&#31995;&#21015;&#27169;&#22411;&#21644;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#20316;&#20026;&#24320;&#28304;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24778;&#20154;&#30340;&#25512;&#29702;&#21644;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23567;&#22411;&#27169;&#22411;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TeacherLM-7.1B&#65292;&#33021;&#22815;&#32473;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26679;&#26412;&#36827;&#34892;&#30456;&#20851;&#22522;&#30784;&#30693;&#35782;&#12289;&#24605;&#32500;&#38142;&#21644;&#24120;&#35265;&#38169;&#35823;&#30340;&#27880;&#37322;&#65292;&#20351;&#27880;&#37322;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#31572;&#26696;&#65292;&#32780;&#19988;&#20351;&#20854;&#20182;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#8220;&#20026;&#20160;&#20040;&#8221;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#8220;&#20160;&#20040;&#8221;&#12290;TeacherLM-7.1B&#27169;&#22411;&#22312;MMLU&#19978;&#23454;&#29616;&#20102;52.3&#30340;&#38646;&#26679;&#26412;&#24471;&#20998;&#65292;&#36229;&#36807;&#20102;&#25317;&#26377;100B&#21442;&#25968;&#30340;&#22823;&#22810;&#25968;&#27169;&#22411;&#12290;&#26356;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#20854;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#12290;&#22522;&#20110;TeacherLM-7.1B&#65292;&#25105;&#20204;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#20351;&#29992;&#20102;&#26469;&#33258;OPT&#21644;BLOOM&#31995;&#21015;&#30340;&#19981;&#21516;&#21442;&#25968;&#30340;&#22810;&#20010;&#23398;&#29983;&#27169;&#22411;&#23545;58&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TeacherLM&#25552;&#20379;&#30340;&#25968;&#25454;&#22686;&#24378;&#24102;&#26469;&#20102;&#26174;&#30528;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#23558;&#20316;&#20026;&#24320;&#28304;&#21457;&#24067;TeacherLM&#31995;&#21015;&#27169;&#22411;&#21644;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn "why" instead of just "what". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#36870;&#21521;&#25628;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.14402</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;3.2&#37096;&#20998;&#65292;&#30693;&#35782;&#25805;&#25511;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 3.2, Knowledge Manipulation. (arXiv:2309.14402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#36870;&#21521;&#25628;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23384;&#20648;&#22823;&#37327;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#22312;&#20351;&#29992;&#36825;&#20123;&#30693;&#35782;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#20854;&#23384;&#20648;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#22235;&#31181;&#25805;&#25511;&#31867;&#22411;&#65306;&#26816;&#32034;&#65288;&#20363;&#22914;&#65292;&#8220;A&#30340;&#23646;&#24615;X&#26159;&#20160;&#20040;&#8221;&#65289;&#12289;&#20998;&#31867;&#65288;&#20363;&#22914;&#65292;&#8220;A&#30340;&#23646;&#24615;X&#26159;&#22855;&#25968;&#36824;&#26159;&#20598;&#25968;&#8221;&#65289;&#12289;&#27604;&#36739;&#65288;&#20363;&#22914;&#65292;&#8220;&#22312;&#23646;&#24615;X&#20013;A&#26159;&#21542;&#22823;&#20110;B&#8221;&#65289;&#21644;&#36870;&#21521;&#25628;&#32034;&#65288;&#20363;&#22914;&#65292;&#8220;&#21738;&#20010;&#20154;&#30340;&#23646;&#24615;X&#31561;&#20110;T&#8221;&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20687;GPT2/3/4&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#25110;&#27604;&#36739;&#20219;&#21153;&#20013;&#24456;&#38590;&#32988;&#20219;&#65292;&#38500;&#38750;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;Chain of Thoughts&#65288;CoTs&#65289;&#12290;&#26080;&#35770;&#25552;&#31034;&#26159;&#20160;&#20040;&#65292;&#23427;&#20204;&#22312;&#36870;&#21521;&#30693;&#35782;&#25628;&#32034;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#20026;&#25511;&#21046;&#23454;&#39564;&#32780;&#35774;&#35745;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35777;&#23454;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models can store vast amounts of factual knowledge, but their ability to use this knowledge for logical reasoning remains questionable. This paper explores a language model's ability to manipulate its stored knowledge during inference. We focus on four manipulation types: retrieval (e.g., "What is person A's attribute X"), classification (e.g., "Is A's attribute X even or odd?"), comparison (e.g., "Is A greater than B in attribute X?") and inverse search (e.g., "Which person's attribute X equals T?")  We observe that pre-trained language models like GPT2/3/4 excel in knowledge retrieval but struggle with simple classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. They also perform poorly in inverse knowledge search, irrespective of the prompts. Our primary contribution is a synthetic dataset for a controlled experiment that confirms these inherent weaknesses: a language model cannot efficiently manipulate knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23558;&#39046;&#22495;&#30693;&#35782;&#24211;&#19982;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#21644;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#29983;&#21644;&#39046;&#22495;&#19987;&#23478;&#23545;&#20110;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2309.12367</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#30740;&#31350;&#39046;&#22495;&#30693;&#35782;&#24211;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Examining the Influence of Varied Levels of Domain Knowledge Base Inclusion in GPT-based Intelligent Tutors. (arXiv:2309.12367v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23558;&#39046;&#22495;&#30693;&#35782;&#24211;&#19982;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#21644;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#29983;&#21644;&#39046;&#22495;&#19987;&#23478;&#23545;&#20110;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20419;&#36827;&#20102;&#20855;&#26377;&#22797;&#26434;&#23545;&#35805;&#33021;&#21147;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;LLM&#23545;&#26597;&#35810;&#30340;&#22238;&#31572;&#32463;&#24120;&#19981;&#20934;&#30830;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#19982;LLM&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#38598;&#25104;&#20197;&#22686;&#21152;&#22238;&#31572;&#21487;&#38752;&#24615;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#65292;&#25945;&#32946;&#30417;&#30563;&#21592;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#35838;&#31243;&#65292;&#35813;&#35838;&#31243;&#20250;&#34987;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#33258;&#21160;&#22788;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#35780;&#20272;&#23454;&#39564;&#65292;&#23398;&#29983;&#21442;&#19982;&#32773;&#38656;&#35201;&#22238;&#31572;&#26377;&#20851;&#20154;&#24037;&#26234;&#33021;&#35838;&#31243;&#30340;&#38382;&#39064;&#12290; GPT-4&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#23618;&#27425;&#30340;KB&#35775;&#38382;&#26435;&#38480;&#65292;&#24182;&#30001;&#20154;&#31867;&#39046;&#22495;&#19987;&#23478;&#35780;&#20272;&#36825;&#20123;&#22238;&#31572;&#12290;&#26368;&#21518;&#65292;&#23398;&#29983;&#23545;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#20132;&#21449;&#39564;&#35777;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#21508;&#31181;&#25945;&#23398;&#33021;&#21147;&#36827;&#34892;&#20102;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have facilitated the development of chatbots with sophisticated conversational capabilities. However, LLMs exhibit frequent inaccurate responses to queries, hindering applications in educational settings. In this paper, we investigate the effectiveness of integrating a knowledge base (KB) with LLM intelligent tutors to increase response reliability. To achieve this, we design a scaleable KB that affords educational supervisors seamless integration of lesson curricula, which is automatically processed by the intelligent tutoring system. We then detail an evaluation, where student participants were presented with questions about the artificial intelligence curriculum to respond to. GPT-4 intelligent tutors with varying hierarchies of KB access and human domain experts then assessed these responses. Lastly, students cross-examined the intelligent tutors' responses to the domain experts' and ranked their various pedagogical abilities. Res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15193</link><description>&lt;p&gt;
&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning in Repeated Multi-Unit Pay-As-Bid Auctions. (arXiv:2307.15193v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#30899;&#25490;&#25918;&#20132;&#26131;&#26041;&#26696;&#12289;&#22269;&#20538;&#25293;&#21334;&#21644;&#37319;&#36141;&#25293;&#21334;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#37117;&#28041;&#21450;&#25293;&#21334;&#21516;&#36136;&#30340;&#22810;&#20010;&#21333;&#20301;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22914;&#20309;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#22312;&#27599;&#20010;&#25293;&#21334;&#20013;&#65292;&#22823;&#37327;&#65288;&#30456;&#21516;&#30340;&#65289;&#29289;&#21697;&#23558;&#34987;&#20998;&#37197;&#32473;&#26368;&#39640;&#30340;&#20986;&#20215;&#65292;&#27599;&#20010;&#20013;&#26631;&#20215;&#31561;&#20110;&#20986;&#20215;&#26412;&#36523;&#12290;&#30001;&#20110;&#34892;&#21160;&#31354;&#38388;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#23398;&#20064;&#22914;&#20309;&#22312;&#20184;&#36153;&#25293;&#21334;&#20013;&#20986;&#20215;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20851;&#27880;&#31163;&#32447;&#35774;&#32622;&#65292;&#20854;&#20013;&#25237;&#26631;&#20154;&#36890;&#36807;&#21482;&#33021;&#35775;&#38382;&#20854;&#20182;&#25237;&#26631;&#20154;&#36807;&#21435;&#25552;&#20132;&#30340;&#20986;&#20215;&#26469;&#20248;&#21270;&#20182;&#20204;&#30340;&#20986;&#20215;&#21521;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#26041;&#26696;&#26469;&#33719;&#24471;&#12290;&#25105;&#20204;&#21033;&#29992;DP&#26041;&#26696;&#30340;&#32467;&#26500;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under fu
&lt;/p&gt;</description></item><item><title>MOCA&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#26469;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#21644;&#23545;&#22270;&#20687;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#21644;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#35757;&#32451;&#36895;&#24230;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.09361</link><description>&lt;p&gt;
MOCA: &#33258;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09361
&lt;/p&gt;
&lt;p&gt;
MOCA&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#26469;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#21644;&#23545;&#22270;&#20687;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#21644;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#35757;&#32451;&#36895;&#24230;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#32531;&#35299;Vision Transformer&#32593;&#32476;&#23545;&#22823;&#22411;&#20840;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#36138;&#23146;&#38656;&#27714;&#12290;&#19981;&#21516;&#31867;&#21035;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#20855;&#26377;&#33391;&#22909;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#20351;&#29992;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#31574;&#30053;&#65292;&#25110;&#32773;&#23545;&#22270;&#20687;&#25200;&#21160;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#38454;&#27573;&#12289;&#29420;&#31435;&#30340;&#26041;&#27861;MOCA&#65292;&#20351;&#29992;&#22522;&#20110;&#39640;&#32423;&#29305;&#24449;&#65288;&#32780;&#19981;&#26159;&#20687;&#32032;&#32423;&#32454;&#33410;&#65289;&#23450;&#20041;&#30340;&#26032;&#22411;&#25513;&#30721;&#21644;&#39044;&#27979;&#30446;&#26631;&#26469;&#32479;&#19968;&#36825;&#20004;&#31181;&#26399;&#26395;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#21327;&#21516;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#24212;&#29992;&#36825;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20854;&#35757;&#32451;&#36895;&#24230;&#33267;&#23569;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DP-SGD&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.00310</link><description>&lt;p&gt;
&#26799;&#24230;&#30456;&#20284;&#65306;&#25935;&#24863;&#24230;&#32463;&#24120;&#34987;&#36807;&#39640;&#20272;&#35745;&#22312;DP-SGD&#20013;
&lt;/p&gt;
&lt;p&gt;
Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD. (arXiv:2307.00310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DP-SGD&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#26159;&#31169;&#26377;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#31639;&#27861;&#12290;&#34429;&#28982;&#24050;&#30693;&#20854;&#38544;&#31169;&#20998;&#26512;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#26159;&#32039;&#23494;&#30340;&#65292;&#20294;&#26159;&#19968;&#20123;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#26174;&#33879;&#20943;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;DP-SGD&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25429;&#25417;&#21040;&#22312;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#30456;&#20284;&#37051;&#23621;&#30340;&#28857;&#20139;&#21463;&#26356;&#22909;&#38544;&#31169;&#24615;&#30340;&#30452;&#35273;&#12290;&#24418;&#24335;&#19978;&#26469;&#35828;&#65292;&#36825;&#26159;&#36890;&#36807;&#20462;&#25913;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#35745;&#31639;&#24471;&#21040;&#30340;&#27169;&#22411;&#26356;&#26032;&#30340;&#27599;&#27493;&#38544;&#31169;&#24615;&#20998;&#26512;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#23450;&#29702;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#27599;&#27493;&#20998;&#26512;&#26469;&#25512;&#29702;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;DP-SGD&#20998;&#26512;&#20351;&#25105;&#20204;&#33021;&#22815;&#27491;&#24335;&#22320;&#26174;&#31034;DP-SGD&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private stochastic gradient descent (DP-SGD) is the canonical algorithm for private deep learning. While it is known that its privacy analysis is tight in the worst-case, several empirical results suggest that when training on common benchmark datasets, the models obtained leak significantly less privacy for many datapoints. In this paper, we develop a new analysis for DP-SGD that captures the intuition that points with similar neighbors in the dataset enjoy better privacy than outliers. Formally, this is done by modifying the per-step privacy analysis of DP-SGD to introduce a dependence on the distribution of model updates computed from a training dataset. We further develop a new composition theorem to effectively use this new per-step analysis to reason about an entire training run. Put all together, our evaluation shows that this novel DP-SGD analysis allows us to now formally show that DP-SGD leaks significantly less privacy for many datapoints. In particular, we ob
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#30340;&#21487;&#23398;&#20064;&#33293;&#20837;&#26426;&#21046;FlexRound&#65292;&#20351;&#24471;&#21518;&#35757;&#32451;&#37327;&#21270;&#26102;&#26356;&#22909;&#22320;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#20844;&#20849;&#37327;&#21270;&#32593;&#26684;&#22823;&#23567;&#20197;&#21450;&#27599;&#20010;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#19981;&#21516;&#27604;&#20363;&#23610;&#12290;</title><link>http://arxiv.org/abs/2306.00317</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#30340;&#21487;&#23398;&#20064;&#33293;&#20837;&#29992;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization. (arXiv:2306.00317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#30340;&#21487;&#23398;&#20064;&#33293;&#20837;&#26426;&#21046;FlexRound&#65292;&#20351;&#24471;&#21518;&#35757;&#32451;&#37327;&#21270;&#26102;&#26356;&#22909;&#22320;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#20844;&#20849;&#37327;&#21270;&#32593;&#26684;&#22823;&#23567;&#20197;&#21450;&#27599;&#20010;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#19981;&#21516;&#27604;&#20363;&#23610;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#32463;&#22312;&#36164;&#28304;&#26377;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#19982;&#37327;&#21270;&#24863;&#30693;&#22521;&#35757;&#19981;&#21516;&#65292;&#23436;&#20840;&#19981;&#38656;&#35201;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#31471;&#21040;&#31471;&#22521;&#35757;&#12290;&#22240;&#20026;&#22522;&#20110;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#36755;&#20986;&#30340;PTQ&#26041;&#26696;&#25928;&#26524;&#26174;&#30528;&#20197;&#22686;&#24378;&#37327;&#21270;&#27169;&#22411;&#24615;&#33021;&#65292;&#25152;&#20197;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20102;&#31639;&#27861;&#26469;&#35774;&#35745;&#21644;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#26435;&#37325;&#33293;&#20837;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26032;&#30340;PTQ&#26435;&#37325;&#33293;&#20837;&#26426;&#21046;&#65292;&#21517;&#20026;FlexRound&#65292;&#20854;&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#32780;&#19981;&#26159;&#20856;&#22411;&#30340;&#20803;&#32032;&#21152;&#27861;&#65292;&#20174;&#32780;&#20351;FlexRound&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#20844;&#20849;&#37327;&#21270;&#32593;&#26684;&#22823;&#23567;&#20197;&#21450;&#27599;&#20010;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#19981;&#21516;&#27604;&#20363;&#23610;&#12290;&#30001;&#20110;&#20803;&#32032;&#38500;&#27861;&#20135;&#29983;&#30340;&#23548;&#25968;&#30340;&#20114;&#34917;&#35268;&#21017;&#65292;FlexRound&#22312;&#26356;&#26032;&#20854;&#30456;&#20851;&#39044;&#35757;&#32451;&#26435;&#37325;&#26102;&#22825;&#29983;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) has been gaining popularity for the deployment of deep neural networks on resource-limited devices since unlike quantization-aware training, neither a full training dataset nor end-to-end training is required at all. As PTQ schemes based on reconstructing each layer or block output turn out to be effective to enhance quantized model performance, recent works have developed algorithms to devise and learn a new weight-rounding scheme so as to better reconstruct each layer or block output. In this work, we propose a simple yet effective new weight-rounding mechanism for PTQ, coined FlexRound, based on element-wise division instead of typical element-wise addition such that FlexRound enables jointly learning a common quantization grid size as well as a different scale for each pre-trained weight. Thanks to the reciprocal rule of derivatives induced by element-wise division, FlexRound is inherently able to exploit pre-trained weights when updating their corr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.02997</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#32988;&#36807;&#22686;&#24378;&#26641;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02997
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#20043;&#19968;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#20154;&#20204;&#20173;&#22312;&#31215;&#26497;&#35752;&#35770;NN&#26159;&#21542;&#36890;&#24120;&#20248;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#35201;&#20040;&#35748;&#20026;GBDT&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19968;&#36143;&#20248;&#20110;NN&#65292;&#35201;&#20040;&#35748;&#20026;NN&#20248;&#20110;GBDT&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#38382;&#65306;'&#36825;&#37325;&#35201;&#21527;&#65311;'&#25105;&#20204;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#27604;&#36739;19&#31181;&#31639;&#27861;&#65292;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;'NN vs. GBDT'&#20105;&#35770;&#34987;&#36807;&#20998;&#24378;&#35843;&#65306;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#30456;&#24403;&#22810;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#35201;&#20040;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#35201;&#20040;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;965&#20010;&#20803;&#29305;&#24449;&#65292;&#20197;&#30830;&#23450;&#25968;&#25454;&#38598;&#30340;&#21738;&#20123;&#29305;&#24615;&#20351;NN&#25110;GBDT&#26356;&#36866;&#21512;&#34920;&#29616;&#33391;&#22909;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;GBDT&#35201;&#27604;NN&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.11090</link><description>&lt;p&gt;
&#22312;ChatGPT&#26102;&#20195;&#36808;&#21521;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#30340;&#21442;&#32771;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems. (arXiv:2304.11090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25512;&#20986;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24341;&#36215;&#20102;&#24040;&#22823;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#23558;&#25104;&#20026;&#26410;&#26469;&#22823;&#22810;&#25968;AI&#31995;&#32479;&#30340;&#22522;&#30784;&#26500;&#24314;&#22359;&#30340;&#36235;&#21183;&#27491;&#22312;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#32435;&#20837;AI&#31995;&#32479;&#24341;&#21457;&#20102;&#23545;&#36127;&#36131;&#20219;AI&#30340;&#37325;&#22823;&#20851;&#27880;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#40657;&#21283;&#23376;&#24615;&#36136;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#36229;&#32423;&#26234;&#33021;&#24341;&#36215;&#30340;&#12290;&#27492;&#22806;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#22686;&#38271;&#33021;&#21147;&#26368;&#32456;&#21487;&#33021;&#20250;&#21534;&#22124;AI&#31995;&#32479;&#30340;&#20854;&#20182;&#32452;&#20214;&#65292;&#24341;&#20837;&#26550;&#26500;&#35774;&#35745;&#20013;&#30340;&#36816;&#21160;&#36793;&#30028;&#21644;&#25509;&#21475;&#28436;&#21464;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#12290;&#29305;&#21035;&#22320;&#65292;&#26412;&#25991;&#39318;&#20808;&#21576;&#29616;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#22312;&#26550;&#26500;&#28436;&#36827;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#20174;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#36830;&#25509;&#22120;"&#21040;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21333;&#29255;&#26426;&#26680;"&#12290;&#28982;&#21518;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26550;&#26500;&#65292;&#21253;&#25324;&#20116;&#20010;&#31867;&#21035;&#30340;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#20026;&#35774;&#35745;&#36127;&#36131;&#20219;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#21644;&#36879;&#26126;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT, Bard, and other large language model (LLM)-based chatbots has drawn huge attention on foundations models worldwide. There is a growing trend that foundation models will serve as the fundamental building blocks for most of the future AI systems. However, incorporating foundation models in AI systems raises significant concerns about responsible AI due to their black box nature and rapidly advancing super-intelligence. Additionally, the foundation model's growing capabilities can eventually absorb the other components of AI systems, introducing the moving boundary and interface evolution challenges in architecture design. To address these challenges, this paper proposes a pattern-oriented responsible-AI-by-design reference architecture for designing foundation model-based AI systems. Specially, the paper first presents an architecture evolution of AI systems in the era of foundation models, from "foundation-model-as-a-connector" to "foundation-model-as-a-monolithi
&lt;/p&gt;</description></item></channel></rss>