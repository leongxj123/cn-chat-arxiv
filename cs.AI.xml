<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#25214;&#34920;&#36827;&#34892;&#39640;&#25928;&#36793;&#32536;&#22270;&#20687;&#25512;&#26029;&#30340;ICELUT&#31639;&#27861;&#65292;&#26080;&#38656;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#38477;&#20302;&#30828;&#20214;&#25512;&#26029;&#26102;&#38388;&#21644;&#21151;&#32791;&#30340;&#21516;&#26102;&#23454;&#29616;&#36817;&#20046;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19238</link><description>&lt;p&gt;
&#39640;&#25928;&#22270;&#20687;&#20462;&#39280;&#30340;&#26597;&#25214;&#34920;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Taming Lookup Tables for Efficient Image Retouching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19238
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#25214;&#34920;&#36827;&#34892;&#39640;&#25928;&#36793;&#32536;&#22270;&#20687;&#25512;&#26029;&#30340;ICELUT&#31639;&#27861;&#65292;&#26080;&#38656;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#38477;&#20302;&#30828;&#20214;&#25512;&#26029;&#26102;&#38388;&#21644;&#21151;&#32791;&#30340;&#21516;&#26102;&#23454;&#29616;&#36817;&#20046;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#28165;&#23631;&#24149;&#22312;&#31471;&#35774;&#22791;(&#22914;&#32456;&#31471;&#29992;&#25143;&#30456;&#26426;&#12289;&#26234;&#33021;&#25163;&#26426;&#21644;&#30005;&#35270;)&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#25512;&#21160;&#20102;&#22270;&#20687;&#22686;&#24378;&#38656;&#27714;&#30340;&#26174;&#30528;&#22686;&#38271;&#12290;&#29616;&#26377;&#30340;&#22686;&#24378;&#27169;&#22411;&#36890;&#24120;&#22312;&#20248;&#21270;&#39640;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20943;&#23569;&#30828;&#20214;&#25512;&#26029;&#26102;&#38388;&#21644;&#21151;&#32791;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#21463;&#38480;&#30340;&#31471;&#35774;&#22791;&#32780;&#35328;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#39068;&#33394;&#22686;&#24378;&#26597;&#25214;&#34920;(ICELUT)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#26597;&#25214;&#34920;&#36827;&#34892;&#26497;&#20854;&#39640;&#25928;&#30340;&#36793;&#32536;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36880;&#28857;(1x1)&#21367;&#31215;&#26469;&#25552;&#21462;&#39068;&#33394;&#20449;&#24687;&#65292;&#21516;&#26102;&#20351;&#29992;&#20998;&#21106;&#20840;&#36830;&#25509;&#23618;&#26469;&#34701;&#20837;&#20840;&#23616;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#36825;&#20004;&#20010;&#32452;&#20214;&#37117;&#26080;&#32541;&#36716;&#25442;&#20026;&#26597;&#25214;&#34920;&#65292;&#20197;&#20415;&#36827;&#34892;&#30828;&#20214;&#26080;&#20851;&#30340;&#37096;&#32626;&#12290;ICELUT&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21151;&#32791;&#26497;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19238v1 Announce Type: cross  Abstract: The widespread use of high-definition screens in edge devices, such as end-user cameras, smartphones, and televisions, is spurring a significant demand for image enhancement. Existing enhancement models often optimize for high performance while falling short of reducing hardware inference time and power consumption, especially on edge devices with constrained computing and storage resources. To this end, we propose Image Color Enhancement Lookup Table (ICELUT) that adopts LUTs for extremely efficient edge inference, without any convolutional neural network (CNN). During training, we leverage pointwise (1x1) convolution to extract color information, alongside a split fully connected layer to incorporate global information. Both components are then seamlessly converted into LUTs for hardware-agnostic deployment. ICELUT achieves near-state-of-the-art performance and remarkably low power consumption. We observe that the pointwise network s
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#23545;&#36890;&#36807;&#21019;&#26032;&#24615;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#28237;&#27969;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20026;AI&#21644;&#28237;&#27969;&#30740;&#31350;&#20043;&#38388;&#32039;&#23494;&#20132;&#32455;&#30340;&#26410;&#26469;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.17993</link><description>&lt;p&gt;
&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#33258;&#28982;&#26234;&#33021;&#30456;&#34701;&#21512;&#65306;&#20174;&#32479;&#35745;&#21147;&#23398;&#21040;&#20154;&#24037;&#26234;&#33021;&#20877;&#21040;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17993
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23545;&#36890;&#36807;&#21019;&#26032;&#24615;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#28237;&#27969;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20026;AI&#21644;&#28237;&#27969;&#30740;&#31350;&#20043;&#38388;&#32039;&#23494;&#20132;&#32455;&#30340;&#26410;&#26469;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#21453;&#24605;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#26410;&#26469;&#35282;&#33394;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#28237;&#27969;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#26681;&#26893;&#20110;&#38750;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#26816;&#39564;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;&#27169;&#22411;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#23457;&#26597;&#20102;&#28237;&#27969;&#30740;&#31350;&#20013;&#30340;&#21508;&#31181;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#32479;&#35745;&#27969;&#20307;&#21147;&#23398;&#30340;&#21516;&#26102;&#21457;&#23637;&#20013;&#30340;&#28508;&#22312;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17993v1 Announce Type: cross  Abstract: The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16149</link><description>&lt;p&gt;
&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#27969;&#37327;&#30340;&#35843;&#26597;&#65306;&#23433;&#20840;&#19982;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
A Survey on Consumer IoT Traffic: Security and Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#24050;&#32463;&#36827;&#20837;&#20102;&#20844;&#20247;&#29983;&#27963;&#12290;&#23613;&#31649;CIoT&#25552;&#39640;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20415;&#21033;&#24615;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#36825;&#19968;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#25214;&#20986;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20174;&#27969;&#37327;&#20998;&#26512;&#20013;&#20102;&#35299;CIoT&#23433;&#20840;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#26412;&#35843;&#26597;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#25506;&#35752;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#20013;&#30340;&#26032;&#29305;&#24449;&#12289;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;2018&#24180;1&#26376;&#33267;2023&#24180;12&#26376;&#25910;&#38598;&#20102;310&#31687;&#19982;CIoT&#27969;&#37327;&#20998;&#26512;&#26377;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#30340;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#35782;&#21035;&#20102;CIoT&#26032;&#29305;&#24449;&#30340;CIoT&#27969;&#37327;&#20998;&#26512;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20116;&#20010;&#24212;&#29992;&#30446;&#26631;&#35814;&#32454;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#12289;&#29992;&#25143;&#27963;&#21160;&#25512;&#26029;&#12289;&#24694;&#24847;&#34892;&#20026;&#26816;&#27979;&#12289;&#38544;&#31169;&#27844;&#38706;&#20197;&#21450;&#36890;&#20449;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16149v1 Announce Type: cross  Abstract: For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#39057;&#32534;&#36753;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#21407;&#22987;&#35270;&#39057;&#21644;&#32534;&#36753;&#35270;&#39057;&#30340;&#26102;&#31354;&#33258;&#30456;&#20284;&#24615;&#65292;&#22312;&#20998;&#25968;&#33976;&#39311;&#36807;&#31243;&#20013;&#35299;&#20915;&#20102;&#26032;&#20869;&#23481;&#24341;&#20837;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#32467;&#26500;&#21644;&#36816;&#21160;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12002</link><description>&lt;p&gt;
DreamMotion&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#39057;&#32534;&#36753;&#30340;&#26102;&#31354;&#33258;&#30456;&#20284;&#24615;&#20998;&#25968;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12002
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#39057;&#32534;&#36753;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#21407;&#22987;&#35270;&#39057;&#21644;&#32534;&#36753;&#35270;&#39057;&#30340;&#26102;&#31354;&#33258;&#30456;&#20284;&#24615;&#65292;&#22312;&#20998;&#25968;&#33976;&#39311;&#36807;&#31243;&#20013;&#35299;&#20915;&#20102;&#26032;&#20869;&#23481;&#24341;&#20837;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#32467;&#26500;&#21644;&#36816;&#21160;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12002v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#25193;&#25955;&#24335;&#35270;&#39057;&#32534;&#36753;&#22312;&#22270;&#20687;&#32534;&#36753;&#25991;&#29486;&#20013;&#26174;&#29616;&#20102;&#19968;&#39033;&#29420;&#29305;&#25361;&#25112;&#65306;&#24314;&#31435;&#30495;&#23454;&#19990;&#30028;&#36816;&#21160;&#12290;&#19982;&#29616;&#26377;&#30340;&#35270;&#39057;&#32534;&#36753;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#19987;&#27880;&#20110;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65292;&#20197;&#35268;&#36991;&#26631;&#20934;&#30340;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#20174;&#24050;&#23637;&#29616;&#33258;&#28982;&#36816;&#21160;&#30340;&#35270;&#39057;&#20013;&#21551;&#21160;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#34429;&#28982;&#35270;&#39057;&#20998;&#25968;&#33976;&#39311;&#21487;&#20197;&#26377;&#25928;&#22320;&#24341;&#20837;&#30446;&#26631;&#25991;&#26412;&#25351;&#31034;&#30340;&#26032;&#20869;&#23481;&#65292;&#20294;&#20063;&#21487;&#33021;&#23548;&#33268;&#26174;&#33879;&#30340;&#32467;&#26500;&#21644;&#36816;&#21160;&#20559;&#24046;&#12290;&#20026;&#20102;&#25269;&#28040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#20998;&#25968;&#33976;&#39311;&#36807;&#31243;&#20013;&#21305;&#37197;&#21407;&#22987;&#35270;&#39057;&#21644;&#32534;&#36753;&#35270;&#39057;&#30340;&#26102;&#31354;&#33258;&#30456;&#20284;&#24615;&#12290;&#30001;&#20110;&#20351;&#29992;&#20102;&#20998;&#25968;&#33976;&#39311;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#24212;&#29992;&#20110;&#32423;&#32852;&#21644;&#38750;&#32423;&#32852;&#35270;&#39057;&#25193;&#25955;&#26694;&#26550;&#12290;&#36890;&#36807;&#19982;&#39046;&#20808;&#26041;&#27861;&#30340;&#24191;&#27867;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#35270;&#39057;&#32534;&#36753;&#20013;&#30340;&#21331;&#36234;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12002v1 Announce Type: cross  Abstract: Text-driven diffusion-based video editing presents a unique challenge not encountered in image editing literature: establishing real-world motion. Unlike existing video editing approaches, here we focus on score distillation sampling to circumvent the standard reverse diffusion process and initiate optimization from videos that already exhibit natural motion. Our analysis reveals that while video score distillation can effectively introduce new content indicated by target text, it can also cause significant structure and motion deviation. To counteract this, we propose to match space-time self-similarities of the original video and the edited video during the score distillation. Thanks to the use of score distillation, our approach is model-agnostic, which can be applied for both cascaded and non-cascaded video diffusion frameworks. Through extensive comparisons with leading methods, our approach demonstrates its superiority in alterin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11299</link><description>&lt;p&gt;
SQ-LLaVA&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#32463;&#36807;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#21518;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#30528;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#40511;&#27807;&#25104;&#20026;&#25972;&#20010;&#32593;&#32476;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#25913;&#21892;&#36328;&#27169;&#24577;&#23545;&#40784;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#32771;&#34385;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#35270;&#35273;&#20219;&#21153;&#33539;&#22260;&#30340;&#26356;&#22810;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#29992;&#20110;&#38382;&#31572;&#65292;&#20294;&#36825;&#31181;&#25805;&#20316;&#25104;&#26412;&#36739;&#39640;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#21253;&#21547;&#22823;&#37327;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20294;&#36825;&#19968;&#26041;&#38754;&#19968;&#30452;&#40092;&#26377;&#20154;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20869;&#37096;&#34987;&#24573;&#35270;&#30340;&#19978;&#19979;&#25991;&#65292;&#35757;&#32451;&#27169;&#22411;&#33258;&#25105;&#35757;&#32451;'&#23398;&#20064;'&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;&#12290;SQ-LLaVA&#22312;&#29983;&#25104;&#28789;&#27963;&#19988;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11299v1 Announce Type: cross  Abstract: Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20197;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#65292;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10707</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#38598;&#25104;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#30340;&#28508;&#22312;&#20027;&#39064;&#65306;&#27668;&#20505;&#36816;&#21160;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20197;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#65292;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25581;&#31034;&#21644;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#37492;&#20110;&#20256;&#32479;&#20027;&#39064;&#32423;&#20998;&#26512;&#30340;&#23616;&#38480;&#24615;&#65292;&#24448;&#24448;&#21482;&#25429;&#25417;&#21040;&#25972;&#20307;&#27169;&#24335;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#23545;&#26356;&#31934;&#32454;&#12289;&#20027;&#39064;&#32858;&#28966;&#30340;&#25506;&#32034;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#20027;&#39064;&#21457;&#29616;&#26041;&#27861;&#65292;&#28041;&#21450;&#25163;&#21160;&#27969;&#31243;&#21644;&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#22312;&#20280;&#32553;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#36164;&#28304;&#24378;&#24230;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#28041;&#21450;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20808;&#36827;&#21151;&#33021;&#30340;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#26356;&#28145;&#20837;&#22320;&#35843;&#26597;&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;&#30340;&#20027;&#39064;&#26041;&#38754;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#22810;&#26679;&#30340;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#26356;&#24191;&#27867;&#20027;&#39064;&#20869;&#26377;&#30340;&#24494;&#22937;&#32454;&#33410;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10707v1 Announce Type: cross  Abstract: This paper introduces a novel approach to uncovering and analyzing themes in social media messaging. Recognizing the limitations of traditional topic-level analysis, which tends to capture only the overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Conventional methods of theme discovery, involving manual processes and a human-in-the-loop approach, are valuable but face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). This approach allows for a deeper investigation into the thematic aspects of social media discourse, enabling us to uncover a diverse array of themes, each with unique characteristics and relevance, thereby offering a comprehensive understanding of the nuances present within broader topics.
&lt;/p&gt;</description></item><item><title>SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.08370</link><description>&lt;p&gt;
SMART: &#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#23376;&#27169;&#22359;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SMART: Submodular Data Mixture Strategy for Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08370
&lt;/p&gt;
&lt;p&gt;
SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#28041;&#21450;&#22312;&#19968;&#32452;&#20197;&#25351;&#20196;&#26684;&#24335;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#27604;&#20363;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#25214;&#21040;&#21512;&#36866;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#25110;&#20381;&#36182;&#20174;&#19994;&#32773;&#30340;&#30452;&#35273;&#22806;&#65292;&#23578;&#26080;&#31995;&#32479;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SMART&#65288;Submodular data Mixture strAtegy for instRuction Tuning&#65289;- &#19968;&#31181;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#26032;&#39062;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#20998;&#25968;&#26469;&#30830;&#23450;&#28151;&#21512;&#26435;&#37325;&#12290;&#32473;&#23450;&#24494;&#35843;&#39044;&#31639;&#65292;SMART&#37325;&#26032;&#20998;&#37197;&#20219;&#21153;&#38388;&#30340;&#39044;&#31639;&#65292;&#24182;&#20174;&#27599;&#20010;&#20219;&#21153;&#20013;&#36873;&#25321;&#38750;&#20887;&#20313;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SMART&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#20363;&#23376;&#27604;&#20363;&#28151;&#21512;&#21644;&#22343;&#31561;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08370v1 Announce Type: cross  Abstract: Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39318;&#35201;&#20559;&#24046;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#22823;&#37327;&#26356;&#26032;&#27604;&#20363;&#19979;&#65292;&#20215;&#20540;&#39640;&#20272;&#26159;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05996</link><description>&lt;p&gt;
&#29992;&#39640;&#26356;&#26032;&#27604;&#20363;&#21078;&#26512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#24212;&#23545;&#20215;&#20540;&#39640;&#20272;&#21644;&#21457;&#25955;
&lt;/p&gt;
&lt;p&gt;
Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39318;&#35201;&#20559;&#24046;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#22823;&#37327;&#26356;&#26032;&#27604;&#20363;&#19979;&#65292;&#20215;&#20540;&#39640;&#20272;&#26159;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35774;&#32622;&#20013;&#21487;&#20197;&#22312;&#26799;&#24230;&#26356;&#26032;&#27425;&#25968;&#22823;&#22823;&#36229;&#36807;&#29615;&#22659;&#26679;&#26412;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23398;&#20064;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#32622;&#32593;&#32476;&#21442;&#25968;&#12290;&#22312;&#36825;&#31181;&#22823;&#37327;&#26356;&#26032;&#19982;&#25968;&#25454;&#27604;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23612;&#22522;&#36763;&#31561;&#20154; (2022) &#30340;&#26368;&#36817;&#19968;&#39033;&#30740;&#31350;&#25351;&#20986;&#20102;&#19968;&#20010;&#39318;&#35201;&#20559;&#24046;&#30340;&#20986;&#29616;&#65292;&#21363;&#20195;&#29702;&#22312;&#26089;&#26399;&#20132;&#20114;&#20013;&#36807;&#25311;&#21512;&#24182;&#28129;&#21270;&#21518;&#32493;&#32463;&#39564;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#20854;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#35299;&#26512;&#20102;&#23548;&#33268;&#39318;&#35201;&#20559;&#24046;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#24212;&#35813;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#26159;&#38271;&#26399;&#20197;&#26469;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#20215;&#20540;&#39640;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;Q&#20540;&#19981;&#20165;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#34987;&#39640;&#20272;&#65292;&#32780;&#19988;&#22312;&#20998;&#24067;&#20869;&#25968;&#25454;&#19978;&#20063;&#26159;&#22914;&#27492;&#65292;&#21487;&#20197;&#36861;&#28335;&#21040;&#30001;&#20248;&#21270;&#22120;&#21160;&#37327;&#25512;&#21160;&#30340;&#26410;&#35265;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21333;&#20301;&#29699;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#26356;&#26032;&#27604;&#20363;&#19979;&#23454;&#29616;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05996v1 Announce Type: cross  Abstract: We show that deep reinforcement learning can maintain its ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples. Under such large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we dissect the phenomena underlying the primacy bias. We inspect the early stages of training that ought to cause the failure to learn and find that a fundamental challenge is a long-standing acquaintance: value overestimation. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be traced to unseen action prediction propelled by optimizer momentum. We employ a simple unit-ball normalization that enables learning under large update ratios
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18659</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#28216;&#25103;&#65306;&#35843;&#30740;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Games: A Survey and Roadmap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#24613;&#21095;&#22686;&#21152;&#65292;&#24182;&#20276;&#38543;&#30528;&#20844;&#20247;&#23545;&#35813;&#20027;&#39064;&#30340;&#21442;&#19982;&#12290;&#23613;&#31649;&#36215;&#21021;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;LLMs&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#21253;&#25324;&#28216;&#25103;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21450;&#20026;&#28216;&#25103;&#25552;&#20379;&#25903;&#25345;&#30340;&#21508;&#31181;&#24212;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#26126;&#30830;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21487;&#20197;&#25198;&#28436;&#30340;&#19981;&#21516;&#35282;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23578;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#21644;LLMs&#22312;&#28216;&#25103;&#20013;&#26410;&#26469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;LLMs&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#20316;&#20026;LLMs&#21644;&#28216;&#25103;&#20132;&#21449;&#39046;&#22495;&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#21644;&#36335;&#32447;&#22270;&#65292;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#25104;&#20026;&#36825;&#19968;&#28608;&#21160;&#20154;&#24515;&#30340;&#26032;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#21644;&#21019;&#26032;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18659v1 Announce Type: cross  Abstract: Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17012</link><description>&lt;p&gt;
Pandora's White-Box&#65306;&#24320;&#25918;LLMs&#20013;&#35757;&#32451;&#25968;&#25454;&#27844;&#28431;&#30340;&#22686;&#21152;
&lt;/p&gt;
&lt;p&gt;
Pandora's White-Box: Increased Training Data Leakage in Open LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36973;&#21463;&#30340;&#38544;&#31169;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#20854;&#20013;&#23545;&#25163;&#21487;&#20197;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#12289;&#26799;&#24230;&#25110;&#25439;&#22833;&#65292;&#35797;&#22270;&#21033;&#29992;&#23427;&#20204;&#26469;&#20102;&#35299;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;&#31532;&#19968;&#20010;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#39640;TPR&#21644;&#20302;FPR&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#24213;&#23618;&#27169;&#22411;&#30340;&#19981;&#21516;&#35775;&#38382;&#31243;&#24230;&#12289;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#21046;&#21270;&#20197;&#21450;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;&#22312;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#30333;&#30418;MIAs&#65306;&#22522;&#20110;&#26799;&#24230;&#33539;&#25968;&#30340;&#25915;&#20987;&#12289;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#21644;&#21333;&#27493;&#25439;&#22833;&#27604;&#25915;&#20987;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#40657;&#30418;&#22522;&#32447;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;.....
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17012v1 Announce Type: cross  Abstract: In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack. All outperform existing black-box baselines, and our supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#27010;&#29575;&#27010;&#24565;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20854;&#32479;&#35745;&#20445;&#35777;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#21028;&#26029;&#22256;&#38590;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15429</link><description>&lt;p&gt;
ProTIP&#65306;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#25239;&#38543;&#26426;&#25200;&#21160;&#30340;&#27010;&#29575;&#40065;&#26834;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#27010;&#29575;&#27010;&#24565;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20854;&#32479;&#35745;&#20445;&#35777;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#21028;&#26029;&#22256;&#38590;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#23637;&#29616;&#20102;&#22312;&#31616;&#21333;&#25991;&#26412;&#25551;&#36848;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#19968;&#26679;&#65292;DMs&#23384;&#22312;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;T2I DMs&#30340;&#40065;&#26834;&#24615;&#26102;&#65292;&#23384;&#22312;&#20197;&#20108;&#20803;&#25110;&#26368;&#22351;&#24773;&#20917;&#38382;&#39064;&#35299;&#26041;&#38754;&#30340;&#23581;&#35797;&#65292;&#20294;&#26080;&#27861;&#22238;&#31572;&#27169;&#22411;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AE&#65289;&#26102;&#30340;&#24635;&#20307;&#40065;&#26834;&#24615;&#22914;&#20309;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#24341;&#20837;&#20102;T2I DMs&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65307;&#28982;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#35780;&#20272;&#12290;&#20027;&#35201;&#25361;&#25112;&#28304;&#33258;&#65306;i&#65289;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65307;&#21644;ii&#65289;&#30830;&#23450;&#25200;&#21160;&#36755;&#20837;&#26159;&#21542;&#20026;AE&#28041;&#21450;&#27604;&#36739;&#20004;&#20010;&#36755;&#20986;&#20998;&#24067;&#65292;&#36825;&#19982;&#20854;&#20182;DL&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#65289;&#19981;&#21516;&#65292;&#20854;&#20013;AE&#26159;&#22312;&#26631;&#31614;&#38169;&#35823;&#39044;&#27979;&#26102;&#34987;&#35782;&#21035;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15429v1 Announce Type: cross  Abstract: Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions. However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness. While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found. In this study, we first introduce a probabilistic notion of T2I DMs' robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees. The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels. To tackle
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#23545;NeSy&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.14047</link><description>&lt;p&gt;
&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple and Effective Transfer Learning for Neuro-Symbolic Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#23545;NeSy&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27867;&#21270;&#21644;&#25191;&#34892;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#23545;NeSy&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#24863;&#30693;&#26144;&#23556;&#21040;&#31526;&#21495;&#65292;&#24182;&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#32773;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14047v1 Announce Type: cross  Abstract: Deep Learning (DL) techniques have achieved remarkable successes in recent years. However, their ability to generalize and execute reasoning tasks remains a challenge. A potential solution to this issue is Neuro-Symbolic Integration (NeSy), where neural approaches are combined with symbolic reasoning. Most of these methods exploit a neural network to map perceptions to symbols and a logical reasoner to predict the output of the downstream task. These methods exhibit superior generalization capacity compared to fully neural architectures. However, they suffer from several issues, including slow convergence, learning difficulties with complex perception tasks, and convergence to local minima. This paper proposes a simple yet effective method to ameliorate these problems. The key idea involves pretraining a neural model on the downstream task. Then, a NeSy model is trained on the same task via transfer learning, where the weights of the p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25506;&#32034;&#39537;&#21160;&#31574;&#30053;&#20248;&#21270;&#30340;RLHF&#31639;&#27861;&#65292;&#36890;&#36807;&#36712;&#36857;&#27604;&#36739;&#21453;&#39304;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#65292;&#20026;&#35299;&#37322;&#23569;&#37327;&#20154;&#31867;&#21453;&#39304;&#36275;&#20197;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;</title><link>https://arxiv.org/abs/2402.10342</link><description>&lt;p&gt;
&#22312;RLHF&#20013;&#22522;&#20110;&#25506;&#32034;&#39537;&#21160;&#30340;&#31574;&#30053;&#20248;&#21270;&#65306;&#20851;&#20110;&#26377;&#25928;&#25968;&#25454;&#21033;&#29992;&#30340;&#29702;&#35770;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25506;&#32034;&#39537;&#21160;&#31574;&#30053;&#20248;&#21270;&#30340;RLHF&#31639;&#27861;&#65292;&#36890;&#36807;&#36712;&#36857;&#27604;&#36739;&#21453;&#39304;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#65292;&#20026;&#35299;&#37322;&#23569;&#37327;&#20154;&#31867;&#21453;&#39304;&#36275;&#20197;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#22312;&#20381;&#36182;&#23569;&#37327;&#20154;&#31867;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#29616;&#35937;&#23384;&#22312;&#30528;&#26377;&#38480;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#26368;&#36817;&#30340;&#32463;&#39564;&#25104;&#21151;&#37319;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#30740;&#31350;&#20173;&#20391;&#37325;&#20110;&#22522;&#20110;&#20215;&#20540;&#30340;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#65288;PO-RLHF&#65289;&#30340;RLHF&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#27969;&#34892;&#30340;&#31574;&#30053;&#35206;&#30422;-&#31574;&#30053;&#26799;&#24230;&#65288;PC-PG&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20551;&#35774;&#23545;&#22870;&#21169;&#20989;&#25968;&#26377;&#30693;&#35782;&#12290;&#22312;PO-RLHF&#20013;&#65292;&#19981;&#20551;&#35774;&#30693;&#36947;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#36712;&#36857;&#30340;&#27604;&#36739;&#21453;&#39304;&#26469;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;PO-RLHF&#25552;&#20379;&#20102;&#20302;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#24615;&#33021;&#30028;&#38480;&#65292;&#36825;&#20026;&#35299;&#37322;&#20026;&#20160;&#20040;&#23569;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#21487;&#33021;&#36275;&#20197;&#22312;RLHF&#20013;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#21019;&#26032;&#26159;&#25105;&#20204;&#30340;&#36712;&#36857;&#32423;el
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10342v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with RLHF. A key novelty is our trajectory-level el
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#27491;&#21017;&#21270;&#65288;DReg&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08384</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#23398;&#20064;&#65306;&#23454;&#29616;&#21160;&#24577;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Selective Learning: Towards Robust Calibration with Dynamic Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#27491;&#21017;&#21270;&#65288;DReg&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35823;&#26657;&#20934;&#25351;&#30340;&#26159;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#19982;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#26159;&#30001;&#36807;&#25311;&#21512;&#38382;&#39064;&#24341;&#36215;&#30340;&#65292;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#29305;&#28857;&#26159;&#23398;&#20064;&#35757;&#32451;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#25152;&#26377;&#20869;&#23481;&#65292;&#23548;&#33268;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#36827;&#34892;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#28155;&#21152;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#24182;&#32531;&#35299;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;&#36825;&#20010;&#30446;&#26631;&#21487;&#20197;&#29702;&#35299;&#20026;&#23547;&#25214;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#21152;&#21487;&#20449;&#24230;&#26469;&#36866;&#24212;&#23454;&#38469;&#26631;&#31614;&#65292;&#21516;&#26102;&#36890;&#36807;&#38477;&#20302;&#21487;&#20449;&#24230;&#26469;&#26368;&#22823;&#21270;&#39044;&#27979;&#27010;&#29575;&#30340;&#29109;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#21487;&#20449;&#24230;&#35843;&#25972;&#30340;&#26126;&#30830;&#25351;&#23548;&#65292;&#23548;&#33268;&#30446;&#26631;&#20914;&#31361;&#65288;&#22686;&#21152;&#20294;&#20063;&#38477;&#20302;&#21487;&#20449;&#24230;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#21160;&#24577;&#27491;&#21017;&#21270;&#65288;DReg&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#65292;&#20174;&#32780;&#36991;&#20813;&#21487;&#20449;&#24230;&#35843;&#25972;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Miscalibration in deep learning refers to there is a discrepancy between the predicted confidence and performance. This problem usually arises due to the overfitting problem, which is characterized by learning everything presented in the training set, resulting in overconfident predictions during testing. Existing methods typically address overfitting and mitigate the miscalibration by adding a maximum-entropy regularizer to the objective function. The objective can be understood as seeking a model that fits the ground-truth labels by increasing the confidence while also maximizing the entropy of predicted probabilities by decreasing the confidence. However, previous methods lack clear guidance on confidence adjustment, leading to conflicting objectives (increasing but also decreasing confidence). Therefore, we introduce a method called Dynamic Regularization (DReg), which aims to learn what should be learned during training thereby circumventing the confidence adjusting trade-off. At 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07754</link><description>&lt;p&gt;
&#24605;&#24819;&#20256;&#25773;&#65306;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#22788;&#29702;&#20013;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#30456;&#23545;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;CoT&#26159;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#32500;&#25193;&#25955;&#65288;DoT&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#25512;&#29702;&#27493;&#39588;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#22312;&#26102;&#38388;&#19978;&#20256;&#25773;&#12290;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36880;&#20010;token&#20174;&#24038;&#21040;&#21491;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;DoT&#22312;&#35745;&#31639;&#21644;&#25512;&#29702;&#24615;&#33021;&#20043;&#38388;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DoT&#22312;&#22810;&#20301;&#25968;&#20056;&#27861;&#21644;&#23567;&#23398;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;DoT&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#65292;&#24182;&#20174;&#29616;&#26377;&#30340;&#22686;&#24378;&#25512;&#29702;&#25216;&#26415;&#65288;&#22914;&#33258;&#19968;&#33268;&#35299;&#30721;&#65289;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#21457;&#23637;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.05119</link><description>&lt;p&gt;
&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Limitations of Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;&#26159;&#20351;&#29992;&#25351;&#20196;-&#22238;&#24212;&#23545;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36807;&#31243;&#65292;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#39044;&#35757;&#32451;LLM&#36716;&#21270;&#20026;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;IT&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#24182;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21644;&#19981;&#36275;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#23545;LLM&#36890;&#36807;IT&#21457;&#29983;&#30340;&#21464;&#21270;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;IT&#30340;&#22810;&#31181;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;IT&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#25110;&#25216;&#33021;&#12290;LoRA&#24494;&#35843;&#20165;&#38480;&#20110;&#23398;&#20064;&#22238;&#24212;&#30340;&#21551;&#21160;&#21644;&#26679;&#24335;&#20196;&#29260;&#65292;&#32780;&#20840;&#21442;&#25968;&#24494;&#35843;&#20250;&#23548;&#33268;&#30693;&#35782;&#36864;&#21270;&#12290;&#65288;2&#65289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;IT&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#20250;&#23548;&#33268;&#22238;&#24212;&#36136;&#37327;&#19979;&#38477;&#12290;&#65288;3&#65289;&#20840;&#21442;&#25968;&#24494;&#35843;&#36890;&#36807;&#19981;&#20934;&#30830;&#22320;&#20174;IT&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#27010;&#24565;&#19978;&#30456;&#20284;&#23454;&#20363;&#30340;&#26631;&#35760;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;AI&#31995;&#32479;&#33021;&#22815;&#26381;&#21153;&#20110;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#38656;&#27714;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#40784;&#23450;&#20041;&#21644;&#23454;&#29616;&#22810;&#20803;&#20027;&#20041;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#26469;&#35780;&#20272;&#21644;&#27979;&#35797;&#22810;&#20803;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05070</link><description>&lt;p&gt;
&#36890;&#24448;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
A Roadmap to Pluralistic Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05070
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;AI&#31995;&#32479;&#33021;&#22815;&#26381;&#21153;&#20110;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#38656;&#27714;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#40784;&#23450;&#20041;&#21644;&#23454;&#29616;&#22810;&#20803;&#20027;&#20041;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#26469;&#35780;&#20272;&#21644;&#27979;&#35797;&#22810;&#20803;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26435;&#21147;&#21644;&#26222;&#21450;&#31243;&#24230;&#30340;&#22686;&#21152;&#65292;&#35774;&#35745;&#33021;&#22815;&#20026;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#20154;&#26381;&#21153;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#20197;&#26381;&#21153;&#22810;&#20803;&#20154;&#31867;&#20215;&#20540;&#35266;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20855;&#20307;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#30830;&#23450;&#21644;&#24418;&#24335;&#21270;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#26041;&#24335;&#26469;&#23450;&#20041;&#21644;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#22810;&#20803;&#20027;&#20041;&#65306;1&#65289;Overton&#22810;&#20803;&#27169;&#22411;&#65292;&#23637;&#31034;&#21512;&#29702;&#21453;&#24212;&#30340;&#20809;&#35889;&#65307;2&#65289;&#21487;&#25805;&#25511;&#30340;&#22810;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#35843;&#25972;&#20197;&#21453;&#26144;&#29305;&#23450;&#30340;&#35266;&#28857;&#65307;3&#65289;&#20998;&#24067;&#22810;&#20803;&#27169;&#22411;&#65292;&#22312;&#20998;&#24067;&#20013;&#24456;&#22909;&#22320;&#26657;&#20934;&#32473;&#23450;&#20154;&#32676;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#21644;&#24418;&#24335;&#21270;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#65306;1&#65289;&#22810;&#30446;&#26631;&#22522;&#20934;&#65307;2&#65289;&#26435;&#34913;&#21487;&#25805;&#25511;&#22522;&#20934;&#65292;&#40723;&#21169;&#27169;&#22411;&#23545;&#20219;&#24847;&#26435;&#34913;&#36827;&#34892;&#35843;&#25972;&#65307;3&#65289;&#38506;&#23457;&#22242;&#22810;&#20803;&#22522;&#20934;&#65292;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#19981;&#21516;&#38506;&#23457;&#22242;&#30340;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02085</link><description>&lt;p&gt;
DeCoF:&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#36827;&#34892;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeCoF: Generated Video Detection via Frame Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02085
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#30340;&#35270;&#39057;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#65292;&#36825;&#23548;&#33268;&#31038;&#20250;&#38754;&#20020;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#20351;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#25104;&#20026;&#32039;&#36843;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#20026;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#26126;&#30830;&#29992;&#20110;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#20026;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#20197;&#35780;&#20272;&#21644;&#25913;&#36827;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#27979;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#20266;&#24433;&#22312;&#24320;&#21457;&#29983;&#25104;&#35270;&#39057;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#26816;&#27979;&#22120;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#35270;&#39057;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#27979;&#27169;&#22411;&#65288;DeCoF&#65289;&#65292;&#23427;&#28040;&#38500;&#20102;&#31354;&#38388;&#20266;&#24433;&#22312;&#36890;&#29992;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;DeCoF&#22312;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#35270;&#39057;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The escalating quality of video generated by advanced video generation methods leads to new security challenges in society, which makes generated video detection an urgent research priority.To foster collaborative research in this area, we construct the first open-source dataset explicitly for generated video detection, providing a valuable resource for the community to benchmark and improve detection methodologies. Through a series of carefully designed probe experiments, our study explores the significance of temporal and spatial artifacts in developing general and robust detectors for generated video. Based on the principle of video frame consistency, we introduce a simple yet effective detection model (DeCoF) that eliminates the impact of spatial artifacts during generalizing feature learning. Our extensive experiments demonstrate the efficacy of DeCoF in detecting videos produced by unseen video generation models and confirm its powerful generalization capabilities across several 
&lt;/p&gt;</description></item><item><title>LatentEditor &#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#23454;&#29616;&#23545;&#31070;&#32463;&#22330;&#36827;&#34892;&#31934;&#30830;&#21644;&#23616;&#37096;&#21463;&#25511;&#32534;&#36753;&#65292;&#23558;&#30495;&#23454;&#22330;&#26223;&#23884;&#20837;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#26356;&#20855;&#36866;&#24212;&#24615;&#30340;&#32534;&#36753;NeRF&#39592;&#24178;&#12290;&#24341;&#20837;&#20102;&#22686;&#37327;&#20998;&#25968;&#21644;&#20687;&#32032;&#32423;&#35780;&#20998;&#26041;&#27861;&#20197;&#25552;&#39640;&#32534;&#36753;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.09313</link><description>&lt;p&gt;
LatentEditor: &#25991;&#26412;&#39537;&#21160;&#30340;&#19977;&#32500;&#22330;&#26223;&#23616;&#37096;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
LatentEditor: Text Driven Local Editing of 3D Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09313
&lt;/p&gt;
&lt;p&gt;
LatentEditor &#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#23454;&#29616;&#23545;&#31070;&#32463;&#22330;&#36827;&#34892;&#31934;&#30830;&#21644;&#23616;&#37096;&#21463;&#25511;&#32534;&#36753;&#65292;&#23558;&#30495;&#23454;&#22330;&#26223;&#23884;&#20837;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#26356;&#20855;&#36866;&#24212;&#24615;&#30340;&#32534;&#36753;NeRF&#39592;&#24178;&#12290;&#24341;&#20837;&#20102;&#22686;&#37327;&#20998;&#25968;&#21644;&#20687;&#32032;&#32423;&#35780;&#20998;&#26041;&#27861;&#20197;&#25552;&#39640;&#32534;&#36753;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#22330;&#22312;&#35270;&#22270;&#21512;&#25104;&#21644;&#22330;&#26223;&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#38544;&#21547;&#22320;&#20174;&#22810;&#35270;&#22270;&#36755;&#20837;&#32534;&#30721;&#20960;&#20309;&#21644;&#32441;&#29702;&#20449;&#24687;&#65292;&#32534;&#36753;&#23427;&#20204;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;\textsc{LatentEditor}&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#36171;&#20104;&#29992;&#25143;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#25191;&#34892;&#31070;&#32463;&#22330;&#30340;&#31934;&#30830;&#21644;&#23616;&#37096;&#21463;&#25511;&#32534;&#36753;&#30340;&#33021;&#21147;&#12290;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#23884;&#20837;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#23545;NeRF&#39592;&#24178;&#36827;&#34892;&#26356;&#24555;&#26356;&#20855;&#36866;&#24212;&#24615;&#30340;&#32534;&#36753;&#12290;&#20026;&#20102;&#22686;&#24378;&#32534;&#36753;&#31934;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22686;&#37327;&#20998;&#25968;&#26469;&#35745;&#31639;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;2D&#25513;&#30721;&#65292;&#20316;&#20026;&#23616;&#37096;&#20462;&#25913;&#30340;&#25351;&#21335;&#65292;&#21516;&#26102;&#20445;&#30041;&#19981;&#30456;&#20851;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#20687;&#32032;&#32423;&#35780;&#20998;&#26041;&#27861;&#21033;&#29992;&#20102;InstructPix2Pix (IP2P)&#30340;&#33021;&#21147;&#65292;&#20197;&#36776;&#21035; IP2 &#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09313v3 Announce Type: replace-cross  Abstract: While neural fields have made significant strides in view synthesis and scene reconstruction, editing them poses a formidable challenge due to their implicit encoding of geometry and texture information from multi-view inputs. In this paper, we introduce \textsc{LatentEditor}, an innovative framework designed to empower users with the ability to perform precise and locally controlled editing of neural fields using text prompts. Leveraging denoising diffusion models, we successfully embed real-world scenes into the latent space, resulting in a faster and more adaptable NeRF backbone for editing compared to traditional methods. To enhance editing precision, we introduce a delta score to calculate the 2D mask in the latent space that serves as a guide for local modifications while preserving irrelevant regions. Our novel pixel-level scoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the disparity between IP2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#38899;&#20048;&#29983;&#25104;AI&#39046;&#22495;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;AI&#38899;&#20048;&#29983;&#25104;&#24179;&#21488;&#30340;&#29256;&#31246;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;AI&#29983;&#25104;&#38899;&#20048;&#36827;&#34892;&#29256;&#26435;&#24402;&#22240;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.06646</link><description>&lt;p&gt;
&#35745;&#31639;&#29256;&#26435;: &#38754;&#21521;&#38899;&#20048;&#29983;&#25104;AI&#30340;&#29256;&#31246;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Computational Copyright: Towards A Royalty Model for Music Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#38899;&#20048;&#29983;&#25104;AI&#39046;&#22495;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;AI&#38899;&#20048;&#29983;&#25104;&#24179;&#21488;&#30340;&#29256;&#31246;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;AI&#29983;&#25104;&#38899;&#20048;&#36827;&#34892;&#29256;&#26435;&#24402;&#22240;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#30340;&#36827;&#27493;&#24341;&#21457;&#20102;&#29256;&#26435;&#25361;&#25112;&#65292;&#22312;&#38899;&#20048;&#34892;&#19994;&#23588;&#20026;&#31361;&#20986;&#12290;&#26412;&#25991;&#20851;&#27880;&#36825;&#20123;&#25361;&#25112;&#30340;&#32463;&#27982;&#26041;&#38754;&#65292;&#24378;&#35843;&#32463;&#27982;&#24433;&#21709;&#22312;&#29256;&#26435;&#39046;&#22495;&#20013;&#26500;&#25104;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#40657;&#30418;&#29983;&#25104;AI&#25216;&#26415;&#30340;&#22797;&#26434;&#24615;&#19981;&#20165;&#34920;&#26126;&#65292;&#32780;&#19988;&#38656;&#35201;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32570;&#22833;&#65292;&#23548;&#33268;&#30417;&#31649;&#25361;&#25112;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20026;AI&#38899;&#20048;&#29983;&#25104;&#24179;&#21488;&#25552;&#20986;&#28508;&#22312;&#30340;&#29256;&#31246;&#27169;&#22411;&#26469;&#24357;&#34917;&#24403;&#21069;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;Spotify&#21644;YouTube&#31561;&#24179;&#21488;&#29616;&#26377;&#29256;&#31246;&#27169;&#22411;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#23558;&#20854;&#35843;&#25972;&#21040;AI&#29983;&#25104;&#38899;&#20048;&#30340;&#29420;&#29305;&#32972;&#26223;&#20013;&#12290;&#25105;&#20204;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#23558;AI&#29983;&#25104;&#30340;&#38899;&#20048;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#26377;&#24433;&#21709;&#21147;&#30340;&#29256;&#26435;&#20869;&#23481;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#25968;&#25454;&#24402;&#22240;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of generative AI has given rise to pressing copyright challenges, particularly in music industry. This paper focuses on the economic aspects of these challenges, emphasizing that the economic impact constitutes a central issue in the copyright arena. The complexity of the black-box generative AI technologies not only suggests but necessitates algorithmic solutions. However, such solutions have been largely missing, leading to regulatory challenges in this landscape. We aim to bridge the gap in current approaches by proposing potential royalty models for revenue sharing on AI music generation platforms. Our methodology involves a detailed analysis of existing royalty models in platforms like Spotify and YouTube, and adapting these to the unique context of AI-generated music. A significant challenge we address is the attribution of AI-generated music to influential copyrighted content in the training data. To this end, we present algorithmic solutions employing data attri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#25552;&#20986;&#20102;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#23450;&#20041;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20998;&#31867;&#20307;&#31995;&#65292;&#20197;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#36827;&#34892;&#20844;&#24179;&#24615;&#20998;&#26512;&#21644;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2309.00770</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#19982;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bias and Fairness in Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#25552;&#20986;&#20102;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#23450;&#20041;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20998;&#31867;&#20307;&#31995;&#65292;&#20197;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#36827;&#34892;&#20844;&#24179;&#24615;&#20998;&#26512;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#20154;&#20204;&#33021;&#22815;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#65292;&#36880;&#28176;&#34701;&#20837;&#35302;&#21450;&#25105;&#20204;&#31038;&#20132;&#39046;&#22495;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23398;&#20064;&#12289;&#24310;&#32493;&#21644;&#25918;&#22823;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#25972;&#21512;&#12289;&#24418;&#24335;&#21270;&#21644;&#25193;&#23637;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#65292;&#23450;&#20041;&#20102;&#20260;&#23475;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#20960;&#20010;&#23454;&#29616;LLMs&#20844;&#24179;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19977;&#20010;&#30452;&#35266;&#30340;&#20998;&#31867;&#20307;&#31995;&#32479;&#19968;&#20102;&#25991;&#29486;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#20559;&#35265;&#35780;&#20272;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#21363;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#32531;&#35299;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00770v2 Announce Type: replace-cross  Abstract: Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10712</link><description>&lt;p&gt;
Q&amp;A&#25552;&#31034;&#65306;&#36890;&#36807;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#25552;&#31034;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#28385;&#36275;&#23545;&#22810;&#26679;&#19990;&#30028;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#22238;&#31572;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;AI&#27169;&#22411;&#37197;&#22791;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#30340;&#35748;&#30693;&#26041;&#26696;&#23578;&#26410;&#31995;&#32479;&#22320;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30456;&#20449;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#23613;&#21487;&#33021;&#25910;&#38598;&#32473;&#23450;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#25105;&#20204;&#23558;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#65292;&#26356;&#23481;&#26131;&#22238;&#24518;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#26368;&#32456;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#36825;&#20123;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#25552;&#31034;&#21457;&#36865;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22270;&#20687;-&#31572;&#26696;&#23545;&#21644;&#30456;&#24212;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35757;&#32451;&#19968;&#20010;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
&lt;/p&gt;</description></item><item><title>CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.05544</link><description>&lt;p&gt;
CodePrompt&#65306;&#36890;&#36807;Prompt&#23398;&#20064;&#30340;&#30693;&#35782;&#29305;&#24449;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning. (arXiv:2401.05544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05544
&lt;/p&gt;
&lt;p&gt;
CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CodeBERT&#65289;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;CodeBERT&#30340;&#25991;&#26412;&#23884;&#20837;&#33021;&#21147;&#21644;"[CLS]"&#21477;&#23376;&#23884;&#20837;&#20449;&#24687;&#20316;&#20026;&#19979;&#28216;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#35821;&#20041;&#34920;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#26469;&#25552;&#21462;&#26377;&#25928;&#29305;&#24449;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#33021;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CodePrompt&#65292;&#36890;&#36807;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have explored the potential of utilizing pre-trained language models, such as CodeBERT, to improve source code-related tasks. Previous studies have mainly relied on CodeBERT's text embedding capability and the `[CLS]' sentence embedding information as semantic representations for fine-tuning downstream source code-related tasks. However, these methods require additional neural network layers to extract effective features, resulting in higher computational costs. Furthermore, existing approaches have not leveraged the rich knowledge contained in both source code and related text, which can lead to lower accuracy. This paper presents a novel approach, CodePrompt, which utilizes rich knowledge recalled from a pre-trained model by prompt learning and an attention mechanism to improve source code-related classification tasks. Our approach initially motivates the language model with prompt information to retrieve abundant knowledge associated with the input as representative feat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.13538</link><description>&lt;p&gt;
&#23398;&#20250;&#35828;&#27597;&#35821;&#65306;&#20197;&#27597;&#35821;&#39118;&#26684;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#29616;&#20195;&#24037;&#20855;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#25991;&#26412;&#39118;&#26684;&#22914;&#20309;&#24433;&#21709;LLMs&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;LLMs&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290; "&#27597;&#35821;"&#26159;&#25351;LLMs&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#21487;&#20197;&#36890;&#36807;&#38646;-shot&#22330;&#26223;&#25506;&#27979;&#12290; AlignedCoT&#24191;&#27867;&#36866;&#29992;&#20110;ICL&#26041;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#38382;&#31572;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#25991;&#26412;&#29702;&#35299;&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#32780;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;AlignedCoT&#30456;&#27604;&#31934;&#24515;&#25163;&#24037;&#21046;&#20316;&#30340;&#28436;&#31034;&#25991;&#31295;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specificall
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;&#36825;&#31181;&#23398;&#20064;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21560;&#25910;&#24191;&#27867;&#36866;&#29992;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20135;&#29983;&#30340;&#20004;&#31181;&#20551;&#35774;&#65292;&#24182;&#23601;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.15047</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20803;-&#65288;&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#65289;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta- (out-of-context) learning in neural networks. (arXiv:2310.15047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;&#36825;&#31181;&#23398;&#20064;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21560;&#25910;&#24191;&#27867;&#36866;&#29992;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20135;&#29983;&#30340;&#20004;&#31181;&#20551;&#35774;&#65292;&#24182;&#23601;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Brown&#31561;&#20154;&#65288;2020&#65289;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#23454;&#39564;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20351;LLMs&#26356;&#23481;&#26131;&#8220;&#20869;&#21270;&#8221;&#25991;&#26412;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#35813;&#25991;&#26412;&#24191;&#27867;&#36866;&#29992;&#65288;&#20363;&#22914;&#30495;&#23454;&#38472;&#36848;&#25110;&#26435;&#23041;&#26469;&#28304;&#30340;&#25991;&#26412;&#65289;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23427;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#21512;&#25104;&#35745;&#31639;&#26426;&#35270;&#35273;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20551;&#35774;&#65292;&#35299;&#37322;&#20102;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#30340;&#20986;&#29616;&#65306;&#19968;&#31181;&#26159;&#20381;&#36182;&#20110;&#27169;&#22411;&#22312;&#20854;&#21442;&#25968;&#20013;&#23384;&#20648;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#21478;&#19968;&#31181;&#26159;&#26263;&#31034;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#30340;&#38544;&#21547;&#26799;&#24230;&#23545;&#40784;&#20559;&#24046;&#21487;&#33021;&#36127;&#36131;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#33021;&#24847;&#21619;&#30528;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#24605;&#32771;&#65292;&#24182;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/krasheni&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brown et al. (2020) famously introduced the phenomenon of in-context learning in large language models (LLMs). We establish the existence of a phenomenon we call meta-out-of-context learning (meta-OCL) via carefully designed synthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMs to more readily "internalize" the semantic content of text that is, or appears to be, broadly useful (such as true statements, or text from authoritative sources) and use it in appropriate circumstances. We further demonstrate meta-OCL in a synthetic computer vision setting, and propose two hypotheses for the emergence of meta-OCL: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based optimizers may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks. Our code can be found at https://github.com/krasheni
&lt;/p&gt;</description></item><item><title>DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.12128</link><description>&lt;p&gt;
DiagrammerGPT: &#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12128
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#20351;&#29992;T2I&#27169;&#22411;&#29983;&#25104;&#22270;&#34920;&#26041;&#38754;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22270;&#34920;&#26159;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#20016;&#23500;&#21644;&#31354;&#38388;&#22797;&#26434;&#30340;&#21487;&#35270;&#21270;&#26469;&#35299;&#37322;&#20449;&#24687;&#30340;&#31526;&#21495;/&#31034;&#24847;&#24615;&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#19968;&#31181;&#23494;&#38598;&#30340;&#30456;&#20851;&#23545;&#35937;&#12289;&#25991;&#26412;&#26631;&#31614;&#12289;&#26041;&#21521;&#31661;&#22836;&#12289;&#36830;&#25509;&#32447;&#31561;&#32452;&#21512;&#65289;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#34920;&#26102;&#32463;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#23545;&#35937;&#36890;&#36807;&#22797;&#26434;&#30340;&#20851;&#31995;&#65288;&#22914;&#31661;&#22836;/&#32447;&#65289;&#23494;&#38598;&#36830;&#25509;&#26102;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#23545;&#35937;&#24067;&#23616;&#25511;&#21046;&#65292;&#24182;&#19988;&#32463;&#24120;&#19981;&#33021;&#28210;&#26579;&#20986;&#21487;&#29702;&#35299;&#30340;&#25991;&#26412;&#26631;&#31614;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiagrammerGPT&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;&#22270;&#34920;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLM&#65288;&#22914;GPT-4&#65289;&#30340;&#24067;&#23616;&#24341;&#23548;&#33021;&#21147;&#26469;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#8220;&#22270;&#34920;&#35268;&#21010;&#8221;&#65288;&#22312;&#19968;&#20010;&#35268;&#21010;&#26041;&#26696;&#20013;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23433;&#20840;&#23884;&#20837;&#24335;MDP&#20013;&#32467;&#21512;&#36712;&#36857;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23433;&#20840;&#32422;&#26463;&#23884;&#20837;&#21160;&#20316;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06903</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#36712;&#36857;&#20248;&#21270;&#30340;&#23433;&#20840;&#23884;&#20837;&#24335;MDP&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning in a Safety-Embedded MDP with Trajectory Optimization. (arXiv:2310.06903v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23433;&#20840;&#23884;&#20837;&#24335;MDP&#20013;&#32467;&#21512;&#36712;&#36857;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23433;&#20840;&#32422;&#26463;&#23884;&#20837;&#21160;&#20316;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22312;&#23558;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#36712;&#36857;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#31649;&#29702;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#23433;&#20840;&#32422;&#26463;&#23884;&#20837;&#21040;&#20462;&#25913;&#21518;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#21160;&#20316;&#31354;&#38388;&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#22120;&#20135;&#29983;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#36825;&#20123;&#34892;&#21160;&#36716;&#21270;&#20026;&#23433;&#20840;&#36712;&#36857;&#65292;&#20174;&#32780;&#26377;&#25928;&#30830;&#20445;&#23433;&#20840;&#24182;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;Safety Gym&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#20960;&#20046;&#38646;&#30340;&#23433;&#20840;&#36829;&#35268;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24615;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#25512;&#21160;&#31665;&#23376;&#31359;&#36807;&#38556;&#30861;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning (RL) plays an important role in applying RL algorithms to safety-critical real-world applications, addressing the trade-off between maximizing rewards and adhering to safety constraints. This work introduces a novel approach that combines RL with trajectory optimization to manage this trade-off effectively. Our approach embeds safety constraints within the action space of a modified Markov Decision Process (MDP). The RL agent produces a sequence of actions that are transformed into safe trajectories by a trajectory optimizer, thereby effectively ensuring safety and increasing training stability. This novel approach excels in its performance on challenging Safety Gym tasks, achieving significantly higher rewards and near-zero safety violations during inference. The method's real-world applicability is demonstrated through a safe and effective deployment in a real robot task of box-pushing around obstacles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;</title><link>http://arxiv.org/abs/2309.15091</link><description>&lt;p&gt;
VideoDirectorGPT: &#36890;&#36807;LLM&#24341;&#23548;&#30340;&#35268;&#21010;&#23454;&#29616;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning. (arXiv:2309.15091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#29983;&#25104;&#21333;&#20010;&#20107;&#20214;&#21644;&#21333;&#19968;&#32972;&#26223;&#30340;&#30701;&#35270;&#39057;&#29255;&#27573;&#65288;&#21363;&#21333;&#22330;&#26223;&#35270;&#39057;&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#24067;&#23616;&#21644;&#25511;&#21046;&#19979;&#28216;&#35270;&#35273;&#27169;&#22359;&#65288;&#22914;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65289;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;&#36825;&#20123;LLMs&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#36827;&#34892;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#21333;&#20010;&#25991;&#26412;&#25552;&#31034;&#36755;&#20837;&#25105;&#20204;&#30340;&#35270;&#39057;&#35268;&#21010;&#22120;LLM&#65288;GPT-4&#65289;&#20013;&#65292;&#23558;&#20854;&#25193;&#23637;&#20026;&#8220;&#35270;&#39057;&#35745;&#21010;&#8221;&#65292;&#20854;&#20013;&#21253;&#25324;&#29983;&#25104;&#22330;&#26223;&#25551;&#36848;&#12289;&#23454;&#20307;&#21450;&#20854;&#24067;&#23616;&#12289;&#27599;&#20010;&#22330;&#26223;&#30340;&#32972;&#26223;&#20197;&#21450;&#20445;&#25345;&#19968;&#33268;&#24615;&#31561;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10864</link><description>&lt;p&gt;
&#23558;&#27880;&#24847;&#21147;&#20998;&#21106;&#19982;&#32465;&#23450;&#29992;&#20110;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#36924;&#30495;&#30340;&#21387;&#20498;&#24615;&#32467;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23436;&#20840;&#20381;&#29031;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#8212;&#8212;&#20851;&#27880;&#19982;&#28608;&#21457;&#65292;&#24341;&#20837;&#20102;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#65288;GSN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#26029;&#26102;&#20248;&#21270;&#36328;&#27880;&#24847;&#21147;&#20197;&#26356;&#22909;&#22320;&#34701;&#20837;&#35821;&#20041;&#12290;&#23427;&#22312;&#29983;&#25104;&#31616;&#21333;&#25552;&#31034;&#65292;&#22914;&#8220;&#19968;&#21482;&#29483;&#21644;&#19968;&#21482;&#29399;&#8221;&#65292;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;&#20197;&#21450;&#35299;&#20915;&#19981;&#36866;&#24403;&#30340;&#23646;&#24615;&#32465;&#23450;&#38382;&#39064;&#26041;&#38754;&#30340;&#21151;&#25928;&#26377;&#25152;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#25552;&#31034;&#25110;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#25913;&#36827;&#30340;&#23646;&#24615;&#32465;&#23450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#21106;&#19982;&#32465;&#23450;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;GSN&#25439;&#22833;&#30446;&#26631;&#65306;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20002;&#22833;&#21644;&#19968;&#31181;&#32465;&#23450;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#32435;&#20837;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#29305;&#28857;&#19978;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend &amp; Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide &amp; Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#32452;&#21512;&#24335;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#20102;&#22235;&#20010;&#21253;&#21547;256&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#30001;&#24615;&#33021;&#19981;&#21516;&#30340;&#20195;&#29702;&#37319;&#38598;&#65292;&#21253;&#21547;2.56&#20159;&#26465;&#36716;&#25442;&#35760;&#24405;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;</title><link>http://arxiv.org/abs/2307.07091</link><description>&lt;p&gt;
&#31163;&#32447;&#32452;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning. (arXiv:2307.07091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#32452;&#21512;&#24335;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#20102;&#22235;&#20010;&#21253;&#21547;256&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#30001;&#24615;&#33021;&#19981;&#21516;&#30340;&#20195;&#29702;&#37319;&#38598;&#65292;&#21253;&#21547;2.56&#20159;&#26465;&#36716;&#25442;&#35760;&#24405;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21487;&#20197;&#35753;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#26114;&#36149;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#30340;&#37325;&#22797;&#12290;&#20026;&#20102;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#32452;&#21512;&#24335;&#24378;&#21270;&#23398;&#20064;&#23545;&#20110;&#29983;&#25104;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#23588;&#20026;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;1&#65289;&#23427;&#20801;&#35768;&#20174;&#23569;&#37327;&#32452;&#20214;&#20013;&#21019;&#24314;&#22810;&#20010;&#20219;&#21153;&#65292;2&#65289;&#20219;&#21153;&#32467;&#26500;&#21487;&#20197;&#35753;&#35757;&#32451;&#22909;&#30340;&#20195;&#29702;&#36890;&#36807;&#32452;&#21512;&#30456;&#20851;&#30340;&#23398;&#20064;&#32452;&#20214;&#26469;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#24182;&#19988;3&#65289;&#32452;&#21512;&#32500;&#24230;&#25552;&#20379;&#20102;&#20219;&#21153;&#20851;&#32852;&#24615;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#22235;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20351;&#29992;&#20102;&#26469;&#33258;CompoSuite [Mendez et al., 2022a]&#30340;256&#20010;&#20219;&#21153;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#24615;&#33021;&#31561;&#32423;&#30340;&#20195;&#29702;&#25910;&#38598;&#30340;&#65292;&#21253;&#21547;&#20102;2.56&#20159;&#26465;&#36716;&#25442;&#35760;&#24405;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#35774;&#32622;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#23398;&#20064;&#32452;&#21512;&#20219;&#21153;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22522;&#20934;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is a promising direction that allows RL agents to pre-train on large datasets, avoiding the recurrence of expensive data collection. To advance the field, it is crucial to generate large-scale datasets. Compositional RL is particularly appealing for generating such large datasets, since 1) it permits creating many tasks from few components, 2) the task structure may enable trained agents to solve new tasks by combining relevant learned components, and 3) the compositional dimensions provide a notion of task relatedness. This paper provides four offline RL datasets for simulated robotic manipulation created using the 256 tasks from CompoSuite [Mendez et al., 2022a]. Each dataset is collected from an agent with a different degree of performance, and consists of 256 million transitions. We provide training and evaluation settings for assessing an agent's ability to learn compositional task policies. Our benchmarking experiments on each setting show that
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.00416</link><description>&lt;p&gt;
&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controllable Motion Diffusion Model. (arXiv:2306.00416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#21160;&#30011;&#20013;&#65292;&#20026;&#34394;&#25311;&#35282;&#33394;&#29983;&#25104;&#36924;&#30495;&#19988;&#21487;&#25511;&#30340;&#36816;&#21160;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20174;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#38480;&#20110;&#31163;&#32447;&#24212;&#29992;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21516;&#26102;&#29983;&#25104;&#25152;&#26377;&#27493;&#39588;&#30340;&#24207;&#21015;&#32423;&#29983;&#25104;&#12290;&#20026;&#20102;&#33021;&#22815;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20197;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#20026;&#22522;&#30784;&#65292;&#36880;&#27493;&#29983;&#25104;&#36816;&#21160;&#24207;&#21015;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#20351;&#29992;&#26631;&#20934;DDPM&#31639;&#27861;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#20135;&#29983;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#36816;&#21160;&#25511;&#21046;&#19979;&#38271;&#26102;&#38388;&#20869;&#30340;&#39640;&#20445;&#30495;&#24230;&#36816;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating realistic and controllable motions for virtual characters is a challenging task in computer animation, and its implications extend to games, simulations, and virtual reality. Recent studies have drawn inspiration from the success of diffusion models in image generation, demonstrating the potential for addressing this task. However, the majority of these studies have been limited to offline applications that target at sequence-level generation that generates all steps simultaneously. To enable real-time motion synthesis with diffusion models in response to time-varying control signals, we propose the framework of the Controllable Motion Diffusion Model (COMODO). Our framework begins with an auto-regressive motion diffusion model (A-MDM), which generates motion sequences step by step. In this way, simply using the standard DDPM algorithm without any additional complexity, our framework is able to generate high-fidelity motion sequences over extended periods with different type
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03942</link><description>&lt;p&gt;
&#23398;&#20064;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#28151;&#21512;&#28436;&#21592;-&#35780;&#35770;&#21592;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03942
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#28789;&#24039;&#24615;&#20013;&#65292;&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#26159;&#25805;&#20316;&#29289;&#20307;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38750;&#25235;&#21462;&#24335;&#25805;&#32437;&#21487;&#20197;&#20351;&#19982;&#29289;&#20307;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#65292;&#20294;&#20063;&#22312;&#25512;&#29702;&#20132;&#20114;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;HACMan&#30340;&#28151;&#21512;&#28436;&#21592;&#35780;&#35770;&#21592;&#22320;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;6D&#38750;&#25235;&#21462;&#24335;&#29289;&#20307;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;HACMan&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#25277;&#35937;&#21644;&#31354;&#38388;&#22522;&#30784;&#30340;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;RL&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#28151;&#21512;&#30340;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;HACMan&#36827;&#34892;&#20102;6D&#29289;&#20307;&#23039;&#24577;&#23545;&#40784;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#29256;&#26412;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#29289;&#20307;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;HACMan&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25216;&#26415;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#20542;&#21521;&#24615;&#20272;&#35745;&#30340;&#25928;&#26524;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#26657;&#20934;&#21518;&#30340;IPS&#20272;&#35745;&#22120;&#22312;Coat&#21644;yahoo&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.12973</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#21453;&#20107;&#23454;&#20542;&#21521;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Calibration for Counterfactual Propensity Estimation in Recommendation. (arXiv:2303.12973v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25216;&#26415;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#20542;&#21521;&#24615;&#20272;&#35745;&#30340;&#25928;&#26524;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#26657;&#20934;&#21518;&#30340;IPS&#20272;&#35745;&#22120;&#22312;Coat&#21644;yahoo&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#30001;&#20110;&#36873;&#25321;&#20559;&#24046;&#65292;&#35768;&#22810;&#35780;&#20998;&#20449;&#24687;&#37117;&#20002;&#22833;&#20102;&#65292;&#36825;&#34987;&#31216;&#20026;&#38750;&#38543;&#26426;&#32570;&#22833;&#12290;&#21453;&#20107;&#23454;&#36870;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#34987;&#29992;&#20110;&#34913;&#37327;&#27599;&#20010;&#35266;&#23519;&#21040;&#30340;&#35780;&#20998;&#30340;&#22635;&#20805;&#38169;&#35823;&#12290;&#34429;&#28982;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#26377;&#25928;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;IPS&#20272;&#35745;&#30340;&#24615;&#33021;&#21463;&#21040;&#20542;&#21521;&#24615;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#20195;&#34920;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25216;&#26415;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#20542;&#21521;&#24615;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;&#36890;&#36807;&#23545;&#20559;&#35823;&#21644;&#25512;&#24191;&#30028;&#38480;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#26657;&#20934;&#30340;IPS&#20272;&#35745;&#22120;&#20248;&#20110;&#26410;&#26657;&#20934;&#30340;IPS&#20272;&#35745;&#22120;&#12290; Coat&#21644;yahoo&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#24471;&#21040;&#25913;&#36827;&#65292;&#20174;&#32780;&#20351;&#25512;&#33616;&#32467;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recommendation systems, a large portion of the ratings are missing due to the selection biases, which is known as Missing Not At Random. The counterfactual inverse propensity scoring (IPS) was used to weight the imputation error of every observed rating. Although effective in multiple scenarios, we argue that the performance of IPS estimation is limited due to the uncertainty miscalibration of propensity estimation. In this paper, we propose the uncertainty calibration for the propensity estimation in recommendation systems with multiple representative uncertainty calibration techniques. Theoretical analysis on the bias and generalization bound shows the superiority of the calibrated IPS estimator over the uncalibrated one. Experimental results on the coat and yahoo datasets shows that the uncertainty calibration is improved and hence brings the better recommendation results.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#38598;&#25104;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#21644;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#27668;&#21160;&#25928;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.11694</link><description>&lt;p&gt;
&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#22312;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#36319;&#36394;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Constrained Reinforcement Learning using Distributional Representation for Trustworthy Quadrotor UAV Tracking Control. (arXiv:2302.11694v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11694
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#38598;&#25104;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#21644;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#27668;&#21160;&#25928;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#21516;&#26102;&#23454;&#29616;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#36319;&#36394;&#25511;&#21046;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#26469;&#33258;&#27668;&#21160;&#21147;&#30340;&#38459;&#21147;&#21644;&#21147;&#30697;&#21464;&#21270;&#26159;&#28151;&#27788;&#30340;&#65292;&#24182;&#19988;&#38590;&#20197;&#31934;&#30830;&#35782;&#21035;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22235;&#26059;&#32764;&#36319;&#36394;&#31995;&#32479;&#23558;&#20854;&#35270;&#20026;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#20013;&#30340;&#31616;&#21333;&#8220;&#24178;&#25200;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#36712;&#36857;&#36319;&#36394;&#22120;&#65292;&#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#19982;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65288;SMPC&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26410;&#30693;&#30340;&#27668;&#21160;&#25928;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#8220;&#21463;&#38480;&#20998;&#24067;&#24335;&#24378;&#21270;&#24178;&#25200;&#20272;&#35745;&#22120;&#8221;&#65288;ConsDRED&#65289;&#20934;&#30830;&#22320;&#35782;&#21035;&#30495;&#23454;&#27668;&#21160;&#25928;&#24212;&#19982;&#20272;&#35745;&#20540;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#37319;&#29992;&#31616;&#21270;&#20223;&#23556;&#24178;&#25200;&#21453;&#39304;&#36827;&#34892;&#25511;&#21046;&#21442;&#25968;&#21270;&#65292;&#20197;&#20445;&#35777;&#20984;&#24615;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;SMPC&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;ConsDRED&#33267;&#23569;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneously accurate and reliable tracking control for quadrotors in complex dynamic environments is challenging. As aerodynamics derived from drag forces and moment variations are chaotic and difficult to precisely identify, most current quadrotor tracking systems treat them as simple `disturbances' in conventional control approaches. We propose a novel, interpretable trajectory tracker integrating a Distributional Reinforcement Learning disturbance estimator for unknown aerodynamic effects with a Stochastic Model Predictive Controller (SMPC). The proposed estimator `Constrained Distributional Reinforced disturbance estimator' (ConsDRED) accurately identifies uncertainties between true and estimated values of aerodynamic effects. Simplified Affine Disturbance Feedback is used for control parameterization to guarantee convexity, which we then integrate with a SMPC. We theoretically guarantee that ConsDRED achieves at least an optimal global convergence rate and a certain sublinear r
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#20351;&#29992;MCFS&#31639;&#27861;&#21512;&#25104;UNSAT&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#21253;&#25324;SAT&#20844;&#24335;&#19981;&#21487;&#28385;&#36275;&#24615;&#35777;&#26126;&#12289;&#21487;&#28385;&#36275;SAT&#20844;&#24335;&#35299;&#30340;&#25968;&#37327;&#35745;&#25968;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21512;&#25104;&#26862;&#26519;&#26500;&#24314;&#31639;&#27861;&#21644;&#21512;&#25104;MDP&#31867;&#26469;&#36991;&#20813;&#26500;&#24314;&#20505;&#36873;&#26641;&#26862;&#26519;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.12581</link><description>&lt;p&gt;
&#36890;&#36807;Monte Carlo Forest Search&#23454;&#29616;UNSAT&#27714;&#35299;&#22120;&#30340;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
UNSAT Solver Synthesis via Monte Carlo Forest Search. (arXiv:2211.12581v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12581
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#20351;&#29992;MCFS&#31639;&#27861;&#21512;&#25104;UNSAT&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#21253;&#25324;SAT&#20844;&#24335;&#19981;&#21487;&#28385;&#36275;&#24615;&#35777;&#26126;&#12289;&#21487;&#28385;&#36275;SAT&#20844;&#24335;&#35299;&#30340;&#25968;&#37327;&#35745;&#25968;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21512;&#25104;&#26862;&#26519;&#26500;&#24314;&#31639;&#27861;&#21644;&#21512;&#25104;MDP&#31867;&#26469;&#36991;&#20813;&#26500;&#24314;&#20505;&#36873;&#26641;&#26862;&#26519;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Monte Carlo Forest Search&#65288;MCFS&#65289;&#65292;&#19968;&#31867;&#29992;&#20110;&#23398;&#20064;&#20915;&#31574;&#26641;MDP&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#12290;&#36825;&#20123;&#38382;&#39064;&#30340;&#31034;&#20363;&#21253;&#25324;&#35777;&#26126;SAT&#20844;&#24335;&#30340;&#19981;&#21487;&#28385;&#36275;&#24615;&#65307;&#35745;&#31639;&#21487;&#28385;&#36275;&#30340;SAT&#20844;&#24335;&#30340;&#35299;&#30340;&#25968;&#37327;&#65307;&#20197;&#21450;&#25214;&#21040;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#12290;MCFS&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;Monte Carlo Tree Search&#65288;MCTS&#65289;&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#22312;&#20505;&#36873;&#26641;&#30340;&#26862;&#26519;&#20013;&#23547;&#25214;&#19968;&#20010;&#23567;&#26641;&#65292;&#32780;&#19981;&#26159;&#22312;&#26641;&#20013;&#25214;&#21040;&#19968;&#20010;&#22909;&#36335;&#24452;&#65288;&#35299;&#20915;&#26041;&#26696;&#65289;&#12290;&#25105;&#20204;&#22312;&#31639;&#27861;&#20013;&#23454;&#20363;&#21270;&#21644;&#35780;&#20272;&#20102;&#33258;&#24049;&#30340;&#24819;&#27861;&#65292;&#31216;&#20043;&#20026;Knuth Synthesis&#65292;&#36825;&#26159;&#19968;&#20010;MCFS&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;DPLL&#20998;&#25903;&#31574;&#30053;&#26469;&#35299;&#20915;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#38382;&#39064;&#12290;&#36825;&#21033;&#29992;&#20102;&#20004;&#20010;&#20851;&#38190;&#24605;&#24819;&#65292;&#20197;&#36991;&#20813;&#26500;&#24314;&#20505;&#36873;&#26641;&#26862;&#26519;&#30340;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#31181;&#21512;&#25104;&#26862;&#26519;&#26500;&#24314;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#27744;&#20013;&#38543;&#26426;&#36873;&#25321;&#8220;&#22909;&#8221;&#30340;&#26641;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#26356;&#22823;&#30340;&#26862;&#26519;&#26469;&#36880;&#27493;&#26500;&#24314;&#26862;&#26519;&#65307;&#65288;2&#65289;&#19968;&#31181;&#21512;&#25104;MDP&#31867;&#65292;&#29992;&#20316;&#30495;&#23454;&#26641;MDP&#30340;&#20195;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#35745;&#31639;&#33410;&#28857;&#38388;&#36716;&#25442;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Monte Carlo Forest Search (MCFS), a class of reinforcement learning (RL) algorithms for learning policies in {tree MDPs}, for which policy execution involves traversing an exponential-sized tree. Examples of such problems include proving unsatisfiability of a SAT formula; counting the number of solutions of a satisfiable SAT formula; and finding the optimal solution to a mixed-integer program. MCFS algorithms can be seen as extensions of Monte Carlo Tree Search (MCTS) to cases where, rather than finding a good path (solution) within a tree, the problem is to find a small tree within a forest of candidate trees. We instantiate and evaluate our ideas in an algorithm that we dub Knuth Synthesis, an MCFS algorithm that learns DPLL branching policies for solving the Boolean satisfiability (SAT) problem, with the objective of achieving good average-case performance on a given distribution of unsatisfiable problem instances. Knuth Synthesis leverages two key ideas to avoid the pr
&lt;/p&gt;</description></item></channel></rss>