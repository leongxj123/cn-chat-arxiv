<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DIVERSE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;173,000&#26465;YouTube&#35270;&#39057;&#35780;&#35770;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#21477;&#23376;&#20013;&#30340;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.03334</link><description>&lt;p&gt;
DIVERSE&#65306;&#36890;&#36807;&#35270;&#39057;&#35780;&#35770;&#24577;&#24230;&#20998;&#26512;&#35299;&#35835;&#20114;&#32852;&#32593;&#23545;&#32654;&#22269;&#20891;&#20107;&#30340;&#30475;&#27861;&#65292;&#19968;&#20010;&#29992;&#20110;&#31435;&#22330;&#20998;&#31867;&#30340;&#26032;&#39062;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DIVERSE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;173,000&#26465;YouTube&#35270;&#39057;&#35780;&#35770;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#21477;&#23376;&#20013;&#30340;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#31435;&#22330;&#26816;&#27979;&#26159;&#28041;&#21450;&#35782;&#21035;&#22312;&#26377;&#20105;&#35758;&#20027;&#39064;&#19978;&#25317;&#26377;&#30456;&#21453;&#35266;&#28857;&#30340;&#29992;&#25143;&#32676;&#32452;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#30123;&#33495;&#25509;&#31181;&#21644;&#20105;&#35770;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#31435;&#22330;&#25552;&#20379;&#20102;&#23545;&#23454;&#20307;&#31435;&#22330;&#30340;&#25351;&#31034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DIVERSE&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#23545;&#36229;&#36807;173,000&#20010;YouTube&#35270;&#39057;&#35780;&#35770;&#36827;&#34892;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#20110;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#12290;&#36825;&#20123;&#31435;&#22330;&#36890;&#36807;&#19968;&#31181;&#30001;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#36827;&#34892;&#26631;&#27880;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#21477;&#23376;&#20013;&#34164;&#21547;&#30340;&#35821;&#27668;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#65292;&#32780;&#38750;&#20351;&#29992;&#20154;&#31867;&#25163;&#21160;&#27880;&#37322;&#12290;&#36825;&#20123;&#24369;&#20449;&#21495;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#21644;&#35773;&#21050;&#30340;&#23384;&#22312;&#65292;&#29305;&#23450;&#20851;&#38190;&#35789;&#30340;&#23384;&#22312;&#65292;&#25991;&#26412;&#30340;&#24773;&#24863;&#20197;&#21450;&#20174;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#30340;&#31435;&#22330;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#35780;&#35770;&#34987;&#27880;&#37322;&#20043;&#21069;&#65292;&#36825;&#20123;&#24369;&#20449;&#21495;&#20351;&#29992;&#25968;&#25454;&#32534;&#31243;&#27169;&#22411;&#36827;&#34892; consol
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03334v1 Announce Type: cross  Abstract: Stance detection of social media text is a key component of downstream tasks involving the identification of groups of users with opposing opinions on contested topics such as vaccination and within arguments. In particular, stance provides an indication of an opinion towards an entity. This paper introduces DIVERSE, a dataset of over 173,000 YouTube video comments annotated for their stance towards videos of the U.S. military. The stance is annotated through a human-guided, machine-assisted labeling methodology that makes use of weak signals of tone within the sentence as supporting indicators, as opposed to using manual annotations by humans. These weak signals consist of the presence of hate speech and sarcasm, the presence of specific keywords, the sentiment of the text, and the stance inference from two Large Language Models. The weak signals are then consolidated using a data programming model before each comment is annotated wit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;&#22810;&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#26469;&#30740;&#31350;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#22635;&#34917;&#20102;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#25512;&#36827;&#20102;&#35780;&#20272;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2402.09766</link><description>&lt;p&gt;
&#20174;&#21464;&#21160;&#24615;&#21040;&#31283;&#23450;&#24615;&#65306;&#25512;&#33616;&#31995;&#32479;&#22522;&#20934;&#21270;&#23454;&#36341;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
From Variability to Stability: Advancing RecSys Benchmarking Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;&#22810;&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#26469;&#30740;&#31350;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#22635;&#34917;&#20102;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#25512;&#36827;&#20102;&#35780;&#20272;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#26032;&#30340;&#31639;&#27861;&#32463;&#24120;&#36890;&#36807;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#20219;&#24847;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#26469;&#22768;&#31216;&#33258;&#24049;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20840;&#38754;&#21453;&#26144;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#20844;&#24179;&#21644;&#31283;&#20581;&#30340;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#65292;&#20174;&#32780;&#25512;&#36827;&#35780;&#20272;&#23454;&#36341;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#25324;&#26412;&#25991;&#20171;&#32461;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;30&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;9&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;11&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23558;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#32858;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#25490;&#21517;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09766v1 Announce Type: cross  Abstract: In the rapidly evolving domain of Recommender Systems (RecSys), new algorithms frequently claim state-of-the-art performance based on evaluations over a limited set of arbitrarily selected datasets. However, this approach may fail to holistically reflect their effectiveness due to the significant impact of dataset characteristics on algorithm performance. Addressing this deficiency, this paper introduces a novel benchmarking methodology to facilitate a fair and robust comparison of RecSys algorithms, thereby advancing evaluation practices. By utilizing a diverse set of $30$ open datasets, including two introduced in this work, and evaluating $11$ collaborative filtering algorithms across $9$ metrics, we critically examine the influence of dataset characteristics on algorithm performance. We further investigate the feasibility of aggregating outcomes from multiple datasets into a unified ranking. Through rigorous experimental analysis, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#21644;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#65292;&#20197;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#26469;&#23454;&#29616;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23454;&#35777;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09281</link><description>&lt;p&gt;
&#25552;&#21319;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#21327;&#26041;&#24046;&#21644;Hessian&#30697;&#38453;&#30340;&#21327;&#21516;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#21644;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#65292;&#20197;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#26469;&#23454;&#29616;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23454;&#35777;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#21327;&#26041;&#24046;&#21644;Hessian&#30697;&#38453;&#20998;&#21035;&#34987;&#21333;&#29420;&#20998;&#26512;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#30697;&#38453;&#38598;&#25104;&#36215;&#26469;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#35777;&#26126;&#20102;&#23427;&#21487;&#20197;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#24182;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#25353;&#29031;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#30340;&#26631;&#20934;&#23454;&#29616;&#20102;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#21644;&#20581;&#24247;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#39564;&#35777;&#22987;&#32456;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09281v1 Announce Type: cross Abstract: Covariance and Hessian matrices have been analyzed separately in the literature for classification problems. However, integrating these matrices has the potential to enhance their combined power in improving classification performance. We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks. Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances. By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria. Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#12290;&#36890;&#36807;&#32508;&#36848;&#25991;&#29486;&#21644;&#19987;&#23478;&#21672;&#35810;&#65292;&#26500;&#24314;&#20102;&#25968;&#25454;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#29983;&#25104;&#20102;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;MLLM&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#20027;&#27969;&#25945;&#32946;&#20013;&#30340;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#20559;&#37325;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.06264</link><description>&lt;p&gt;
LLaVA-Docent&#65306;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#30340;&#25945;&#23398;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#12290;&#36890;&#36807;&#32508;&#36848;&#25991;&#29486;&#21644;&#19987;&#23478;&#21672;&#35810;&#65292;&#26500;&#24314;&#20102;&#25968;&#25454;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#29983;&#25104;&#20102;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;MLLM&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#20027;&#27969;&#25945;&#32946;&#20013;&#30340;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#20559;&#37325;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#37492;&#36175;&#23545;&#20110;&#22521;&#20859;&#23398;&#20064;&#32773;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#24773;&#24863;&#26234;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#24120;&#38754;&#20020;&#33402;&#26415;&#36164;&#28304;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24369;&#21183;&#23398;&#29983;&#65292;&#24182;&#19988;&#22312;&#20027;&#27969;&#25945;&#32946;&#20013;&#36807;&#24230;&#24378;&#35843;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#31185;&#30446;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#25216;&#26415;&#36827;&#27493;&#20026;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#26469;&#21033;&#29992;&#36825;&#20123;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#21672;&#35810;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#25968;&#25454;&#26694;&#26550;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;GPT-4&#21033;&#29992;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;MLLM&#65288;&#21363;LLaVA-Docent&#65289;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#20845;&#21517;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Art appreciation is vital in nurturing critical thinking and emotional intelligence among learners. However, traditional art appreciation education has often been hindered by limited access to art resources, especially for disadvantaged students, and an imbalanced emphasis on STEM subjects in mainstream education. In response to these challenges, recent technological advancements have paved the way for innovative solutions. This study explores the application of multi-modal large language models (MLLMs) in art appreciation education, focusing on developing LLaVA-Docent, a model that leverages these advancements. Our approach involved a comprehensive literature review and consultations with experts in the field, leading to developing a robust data framework. Utilizing this framework, we generated a virtual dialogue dataset that was leveraged by GPT-4. This dataset was instrumental in training the MLLM, named LLaVA-Docent. Six researchers conducted quantitative and qualitative evaluation
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#20154;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#20174;&#30446;&#26631;&#39046;&#22495;&#37319;&#38598;&#26080;&#20559;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#20174;&#28304;&#39046;&#22495;&#33719;&#21462;&#25968;&#25454;&#30340;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#21516;&#26102;&#65292;&#24635;&#32467;&#20102;&#19981;&#21516;&#38382;&#39064;&#35774;&#32622;&#19979;&#30340;&#35774;&#35745;&#32771;&#34385;&#21644;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04580</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04580
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#20154;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#20174;&#30446;&#26631;&#39046;&#22495;&#37319;&#38598;&#26080;&#20559;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#20174;&#28304;&#39046;&#22495;&#33719;&#21462;&#25968;&#25454;&#30340;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#21516;&#26102;&#65292;&#24635;&#32467;&#20102;&#19981;&#21516;&#38382;&#39064;&#35774;&#32622;&#19979;&#30340;&#35774;&#35745;&#32771;&#34385;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#34028;&#21187;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#22823;&#37327;&#25968;&#25454;&#30340;&#38656;&#27714;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#21644;&#20005;&#26684;&#30340;&#23433;&#20840;&#35201;&#27714;&#65292;&#20174;&#30446;&#26631;&#39046;&#22495;&#25910;&#38598;&#36275;&#22815;&#30340;&#26080;&#20559;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#37319;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#28304;&#39046;&#22495;&#25968;&#25454;&#65288;&#20363;&#22914;&#27169;&#25311;&#21644;&#23454;&#39564;&#23460;&#29615;&#22659;&#65289;&#65292;&#20197;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#30340;&#25968;&#25454;&#33719;&#21462;&#21644;&#24555;&#36895;&#27169;&#22411;&#36845;&#20195;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28304;&#39046;&#22495;&#30340;&#29615;&#22659;&#21644;&#20855;&#36523;&#26041;&#24335;&#21487;&#33021;&#19982;&#30446;&#26631;&#39046;&#22495;&#30340;&#29305;&#24449;&#30456;&#24046;&#24456;&#22823;&#65292;&#24378;&#35843;&#20102;&#26377;&#25928;&#30340;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#36890;&#36807;&#23545;&#39046;&#22495;&#24046;&#36317;&#30340;&#31934;&#32454;&#20998;&#31867;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#27599;&#20010;&#38382;&#39064;&#35774;&#32622;&#30340;&#24635;&#20307;&#35265;&#35299;&#21644;&#35774;&#35745;&#32771;&#34385;&#12290;&#25105;&#20204;&#36824;&#23601;&#20351;&#29992;&#30340;&#20851;&#38190;&#26041;&#27861;&#36827;&#34892;&#20102;&#39640;&#23618;&#27425;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
The burgeoning fields of robot learning and embodied AI have triggered an increasing demand for large quantities of data. However, collecting sufficient unbiased data from the target domain remains a challenge due to costly data collection processes and stringent safety requirements. Consequently, researchers often resort to data from easily accessible source domains, such as simulation and laboratory environments, for cost-effective data acquisition and rapid model iteration. Nevertheless, the environments and embodiments of these source domains can be quite different from their target domain counterparts, underscoring the need for effective cross-domain policy transfer approaches. In this paper, we conduct a systematic review of existing cross-domain policy transfer methods. Through a nuanced categorization of domain gaps, we encapsulate the overarching insights and design considerations of each problem setting. We also provide a high-level discussion about the key methodologies used
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#30340;AI&#39537;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#23548;&#31995;&#32479;&#65292;&#22312;&#35299;&#20915;&#39640;&#20013;&#23398;&#29983;&#21327;&#21516;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#26102;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#20449;&#24687;&#27844;&#38706;&#12289;&#28389;&#29992;&#35821;&#35328;&#21644;&#20844;&#24179;&#24615;&#31561;&#20262;&#29702;&#21644;&#20449;&#20219;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01760</link><description>&lt;p&gt;
&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#30340;AI&#39537;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#23548;&#31995;&#32479;&#20013;&#30340;&#20449;&#20219;&#21644;&#20262;&#29702;&#32771;&#34385;&#65306;&#20197;&#21327;&#21516;&#35299;&#20915;&#39764;&#26041;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Trust and ethical considerations in a multi-modal, explainable AI-driven chatbot tutoring system: The case of collaboratively solving Rubik's Cube
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01760
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#30340;AI&#39537;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#23548;&#31995;&#32479;&#65292;&#22312;&#35299;&#20915;&#39640;&#20013;&#23398;&#29983;&#21327;&#21516;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#26102;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#20449;&#24687;&#27844;&#38706;&#12289;&#28389;&#29992;&#35821;&#35328;&#21644;&#20844;&#24179;&#24615;&#31561;&#20262;&#29702;&#21644;&#20449;&#20219;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#20174;&#22823;&#37327;&#20851;&#20110;&#23398;&#29983;&#23398;&#20064;&#27169;&#24335;&#30340;&#25968;&#25454;&#20013;&#21457;&#29616;&#27934;&#23519;&#21147;&#30340;&#28508;&#21147;&#65292;&#26377;&#26395;&#25913;&#21464;&#25945;&#32946;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;&#21644;&#20449;&#20219;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#20294;&#23578;&#26410;&#35299;&#20915;&#12290;&#22312;&#39640;&#20013;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#20013;&#65292;&#31361;&#20986;&#30340;&#20262;&#29702;&#38382;&#39064;&#21253;&#25324;&#25968;&#25454;&#38544;&#31169;&#12289;&#20449;&#24687;&#27844;&#38706;&#12289;&#28389;&#29992;&#35821;&#35328;&#21644;&#20844;&#24179;&#24615;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#20026;&#35299;&#20915;&#39640;&#20013;&#23398;&#29983;&#19982;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#35299;&#20915;&#39764;&#26041;&#30340;&#20262;&#29702;&#21644;&#20449;&#20219;&#38382;&#39064;&#32780;&#26500;&#24314;&#30340;&#25216;&#26415;&#32452;&#20214;&#65288;&#31216;&#20026;ALLURE chatbot&#65289;&#12290;&#22312;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#65292;&#25105;&#20204;&#24076;&#26395;&#30830;&#20445;&#20799;&#31461;&#12289;&#29238;&#27597;&#21644;&#25945;&#24072;&#30340;&#30693;&#24773;&#21516;&#24847;&#22788;&#20110;&#20219;&#20309;&#31649;&#29702;&#30340;&#25968;&#25454;&#30340;&#20013;&#24515;&#20301;&#32622;&#12290;&#30001;&#20110;&#28041;&#21450;&#20799;&#31461;&#65292;&#31995;&#32479;&#33021;&#22815;&#25509;&#21463;&#29992;&#25143;&#21644;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#30340;&#25991;&#26412;&#12289;&#38899;&#39057;&#25110;&#35270;&#35273;&#35821;&#35328;&#65292;&#24182;&#23558;&#20114;&#21160;&#24341;&#23548;&#36828;&#31163;&#21361;&#38505;&#24773;&#20917;&#12290;&#22312;&#20449;&#24687;&#31649;&#29702;&#26041;&#38754;&#65292;&#25105;&#20204;&#36824;&#24076;&#26395;&#30830;&#20445;&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#26426;&#21046;&#38450;&#27490;&#20449;&#24687;&#27844;&#38706;&#30340;&#21361;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has the potential to transform education with its power of uncovering insights from massive data about student learning patterns. However, ethical and trustworthy concerns of AI have been raised but are unsolved. Prominent ethical issues in high school AI education include data privacy, information leakage, abusive language, and fairness. This paper describes technological components that were built to address ethical and trustworthy concerns in a multi-modal collaborative platform (called ALLURE chatbot) for high school students to collaborate with AI to solve the Rubik's cube. In data privacy, we want to ensure that the informed consent of children, parents, and teachers, is at the center of any data that is managed. Since children are involved, language, whether textual, audio, or visual, is acceptable both from users and AI and the system can steer interaction away from dangerous situations. In information management, we also want to ensure that the sys
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#22238;&#24518;&#26041;&#27861;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#21151;&#29575;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.10393</link><description>&lt;p&gt;
&#33258;&#28982;&#30340;&#21151;&#29575;&#27861;&#21017;&#23398;&#20064;&#29615;&#22659;&#20013;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments. (arXiv:2401.10393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#22238;&#24518;&#26041;&#27861;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#21151;&#29575;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#36973;&#21463;&#28798;&#38590;&#24615;&#24178;&#25200;&#65288;CI&#65289;&#65306;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#12290;&#36825;&#19982;&#20154;&#31867;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#20154;&#31867;&#21487;&#20197;&#36830;&#32493;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#20250;&#26126;&#26174;&#24536;&#35760;&#20808;&#21069;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#20943;&#36731;CI&#30340;&#25216;&#26415;&#65292;&#20363;&#22914;&#27491;&#21017;&#21270;&#12289;&#22238;&#24518;&#12289;&#29983;&#25104;&#24615;&#22238;&#25918;&#21644;&#27987;&#32553;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#30340;&#25351;&#23548;&#65292;&#35813;&#30740;&#31350;&#34920;&#26126;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#36935;&#21040;&#20219;&#21153;&#30340;&#27010;&#29575;&#19982;&#26368;&#21518;&#19968;&#27425;&#25191;&#34892;&#20219;&#21153;&#30340;&#26102;&#38388;&#25104;&#21151;&#29575;&#27861;&#21017;&#36882;&#20943;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27169;&#25311;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#20943;&#36731;CI&#25216;&#26415;&#30340;&#30495;&#23454;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#31867;&#20284;&#20154;&#31867;&#38754;&#20020;&#30340;&#21151;&#29575;&#27861;&#21017;&#29615;&#22659;&#20013;&#35757;&#32451;&#31616;&#21333;&#30340;&#22238;&#24518;&#26041;&#27861;&#26102;&#65292;CI&#30340;&#20943;&#36731;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#36825;&#31181;&#22522;&#20110;&#22238;&#24518;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often suffer from catastrophic interference (CI): performance on previously learned tasks drops off significantly when learning a new task. This contrasts strongly with humans, who can sequentially learn new tasks without appreciably forgetting previous tasks. Prior work has explored various techniques for mitigating CI such as regularization, rehearsal, generative replay, and distillation methods. The current work takes a different approach, one guided by cognitive science research showing that in naturalistic environments, the probability of encountering a task decreases as a power-law of the time since it was last performed. We argue that a realistic evaluation of techniques for the mitigation of CI should be performed in simulated naturalistic learning environments. Thus, we evaluate the extent of mitigation of CI when training simple rehearsal-based methods in power-law environments similar to the ones humans face. Our work explores this novel rehearsal-based appro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.08669</link><description>&lt;p&gt;
&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes. (arXiv:2401.08669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#19968;&#20123;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#29983;&#25104;&#30340;&#31574;&#30053;&#26102;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20123;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#23454;&#20363;&#65292;&#36825;&#20123;&#25216;&#26415;&#24050;&#32463;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#19968;&#20123;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#21644;&#38750;&#24120;&#22797;&#26434;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21464;&#20307;&#65292;&#23578;&#26410;&#35777;&#26126;&#26377;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#29992;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#19968;&#31181;&#36825;&#26679;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#36742;&#21345;&#36710;&#21644;&#22810;&#20998;&#27573;&#36335;&#24452;&#35201;&#27714;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#38656;&#27714;&#38656;&#35201;&#27839;&#30528;&#33410;&#28857;&#24207;&#21015;&#31227;&#21160;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20174;&#36215;&#28857;&#21040;&#32456;&#28857;&#12290;&#20026;&#20102;&#20351;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25104;&#20026;&#36866;&#29992;&#20110;&#23454;&#38469;&#24037;&#19994;&#35268;&#27169;&#30340;&#20379;&#24212;&#38142;&#29289;&#27969;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#20102;&#26032;&#25193;&#23637;&#65292;&#20351;&#20854;&#33021;&#22788;&#29702;&#22810;&#21345;&#36710;&#21644;&#22810;&#20998;&#27573;&#36335;&#24452;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#36825;&#26679;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#19979;&#36827;&#34892;&#65292;&#24182;&#33021;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) has been shown to be effective in producing approximate solutions to some vehicle routing problems (VRPs), especially when using policies generated by encoder-decoder attention mechanisms. While these techniques have been quite successful for relatively simple problem instances, there are still under-researched and highly complex VRP variants for which no effective RL method has been demonstrated. In this work we focus on one such VRP variant, which contains multiple trucks and multi-leg routing requirements. In these problems, demand is required to move along sequences of nodes, instead of just from a start node to an end node. With the goal of making deep RL a viable strategy for real-world industrial-scale supply chain logistics, we develop new extensions to existing encoder-decoder attention models which allow them to handle multiple trucks and multi-leg routing requirements. Our models have the advantage that they can be trained for a small number 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20844;&#24179;&#20108;&#20998;&#31867;&#22120;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#25581;&#31034;&#20102;&#38024;&#23545;&#20844;&#24179;&#22686;&#24378;&#27169;&#22411;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26080;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20844;&#24179;&#24615;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#20013;&#22823;&#22810;&#25968;&#23376;&#32676;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2311.03865</link><description>&lt;p&gt;
&#24403;&#20844;&#24179;&#24615;&#36935;&#35265;&#38544;&#31169;&#65306;&#36890;&#36807;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#25506;&#32034;&#20844;&#24179;&#20108;&#20998;&#31867;&#22120;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers through Membership Inference Attacks. (arXiv:2311.03865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20844;&#24179;&#20108;&#20998;&#31867;&#22120;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#25581;&#31034;&#20102;&#38024;&#23545;&#20844;&#24179;&#22686;&#24378;&#27169;&#22411;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26080;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20844;&#24179;&#24615;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#20013;&#22823;&#22810;&#25968;&#23376;&#32676;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#20855;&#26377;&#27495;&#35270;&#34892;&#20026;&#30340;&#26377;&#20559;&#27169;&#22411;&#30340;&#20844;&#24179;&#26041;&#27861;&#65292;&#20197;&#36798;&#21040;&#20844;&#24179;&#39044;&#27979;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;&#22312;&#36825;&#20123;&#25915;&#20987;&#20013;&#65292;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#39044;&#27979;&#20998;&#25968;&#25512;&#26029;&#20986;&#29305;&#23450;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#38024;&#23545;&#20844;&#24179;&#22686;&#24378;&#27169;&#22411;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26159;&#26080;&#25928;&#30340;&#12290;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#35757;&#32451;&#30340;&#27169;&#22411;&#36864;&#21270;&#20026;&#31616;&#21333;&#30340;&#38408;&#20540;&#27169;&#22411;&#65292;&#20174;&#32780;&#23548;&#33268;&#25915;&#20987;&#24615;&#33021;&#38477;&#20302;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20844;&#24179;&#24615;&#26041;&#27861;&#24448;&#24448;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22823;&#22810;&#25968;&#23376;&#32676;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#25552;&#39640;&#20102;&#25104;&#21151;&#25915;&#20987;&#30340;&#38590;&#24230;&#65292;&#21516;&#26102;&#25193;&#22823;&#20102;&#25104;&#21592;&#21644;&#38750;&#25104;&#21592;&#25968;&#25454;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have developed fairness methods for biased models that exhibit discriminatory behaviors towards specific subgroups. While these models have shown promise in achieving fair predictions, recent research has identified their potential vulnerability to score-based membership inference attacks (MIAs). In these attacks, adversaries can infer whether a particular data sample was used during training by analyzing the model's prediction scores. However, our investigations reveal that these score-based MIAs are ineffective when targeting fairness-enhanced models in binary classifications. The attack models trained to launch the MIAs degrade into simplistic threshold models, resulting in lower attack performance. Meanwhile, we observe that fairness methods often lead to prediction performance degradation for the majority subgroups of the training data. This raises the barrier to successful attacks and widens the prediction gaps between member and non-member data. Building upon th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SnD&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#38454;&#27573;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#20196;&#29260;&#23884;&#20837;&#23618;&#21644;&#24341;&#20837;&#22122;&#22768;&#26469;&#20248;&#21270;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.09130</link><description>&lt;p&gt;
&#20351;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#65306;&#25286;&#20998;&#19982;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Split-and-Denoise: Protect large language model inference with local differential privacy. (arXiv:2310.09130v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SnD&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#38454;&#27573;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#20196;&#29260;&#23884;&#20837;&#23618;&#21644;&#24341;&#20837;&#22122;&#22768;&#26469;&#20248;&#21270;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#25429;&#25417;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#38544;&#34255;&#35821;&#20041;&#65292;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#36825;&#19968;&#36807;&#31243;&#20016;&#23500;&#20102;&#25991;&#26412;&#23884;&#20837;&#30340;&#20215;&#20540;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20316;&#20026;&#26381;&#21153;&#65288;EaaS&#65289;&#30340;&#23884;&#20837;&#27169;&#22411;&#21830;&#19994;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#25991;&#26412;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#38754;&#20020;&#30528;&#36739;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#65292;&#36825;&#26159;&#19968;&#20010;&#23578;&#26410;&#24471;&#21040;&#26377;&#25928;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Split-N-Denoise&#65288;SnD&#65289;&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#25191;&#34892;&#20196;&#29260;&#23884;&#20837;&#23618;&#26469;&#25286;&#20998;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#23558;&#23884;&#20837;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#20043;&#21069;&#24341;&#20837;&#22122;&#22768;&#65292;&#24182;&#38543;&#21518;&#25509;&#25910;&#21644;&#21435;&#22122;&#21518;&#30340;&#25200;&#21160;&#36755;&#20986;&#23884;&#20837;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#20026;LLMs&#30340;&#25512;&#26029;&#38454;&#27573;&#35774;&#35745;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;SnD&#22312;&#21508;&#31181;LLM&#20013;&#20248;&#21270;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) shows powerful capability in natural language understanding by capturing hidden semantics in vector space. This process enriches the value of the text embeddings for various downstream tasks, thereby fostering the Embedding-as-a-Service (EaaS) business model. However, the direct transmission of text to servers poses a largely unaddressed risk of privacy leakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an innovative framework that split the model to execute the token embedding layer on the client side at minimal computational cost. This allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters. Extensive experiments demonstrate SnD's effectiveness in optimizing the privacy-utility tradeoff across various LLM a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21487;&#36870;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#31185;&#22827;&#38142;&#65292;&#21363;&#8220;&#22240;&#26524;Zig-Zag&#37319;&#26679;&#22120;&#8221;&#65292;&#29992;&#20110;&#25512;&#26029;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#37327;&#21464;&#37327;&#65292;&#35813;&#37319;&#26679;&#22120;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#28151;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05655</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#37327;&#36827;&#34892;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#65306;&#22312;DAG&#30340;Markov&#31561;&#20215;&#31867;&#19978;&#37319;&#26679;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Causal structure learning with momentum: Sampling distributions over Markov Equivalence Classes of DAGs. (arXiv:2310.05655v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21487;&#36870;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#31185;&#22827;&#38142;&#65292;&#21363;&#8220;&#22240;&#26524;Zig-Zag&#37319;&#26679;&#22120;&#8221;&#65292;&#29992;&#20110;&#25512;&#26029;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#37327;&#21464;&#37327;&#65292;&#35813;&#37319;&#26679;&#22120;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#28151;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#26029;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#65288;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;DAG&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#21487;&#36870;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#31185;&#22827;&#38142;&#65292;&#21363;&#8220;&#22240;&#26524;Zig-Zag&#37319;&#26679;&#22120;&#8221;&#65292;&#35813;&#37319;&#26679;&#22120;&#38024;&#23545;&#19968;&#31867;&#35266;&#27979;&#31561;&#20215;&#65288;Markov&#31561;&#20215;&#65289;DAG&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#20123;&#31867;&#21035;&#20197;&#23436;&#25104;&#30340;&#37096;&#20998;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;CPDAG&#65289;&#34920;&#31034;&#12290;&#38750;&#21487;&#36870;&#39532;&#23572;&#31185;&#22827;&#38142;&#20381;&#36182;&#20110;Chickering&#30340;&#36138;&#23146;&#31561;&#20215;&#25628;&#32034;&#65288;GES&#65289;&#20013;&#20351;&#29992;&#30340;&#25805;&#20316;&#31526;&#65292;&#24182;&#19988;&#20855;&#26377;&#19968;&#20010;&#21160;&#37327;&#21464;&#37327;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#28151;&#21512;&#24615;&#33021;&#12290;&#21487;&#33021;&#30340;&#30446;&#26631;&#20998;&#24067;&#21253;&#25324;&#22522;&#20110;DAG&#20808;&#39564;&#21644;Markov&#31561;&#20215;&#20284;&#28982;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23454;&#29616;&#65292;&#20854;&#20013;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#21015;&#20030;&#12289;&#35745;&#25968;&#12289;&#22343;&#21248;&#37319;&#26679;&#21644;&#24212;&#29992;GES&#25805;&#20316;&#31526;&#30340;&#21487;&#33021;&#31227;&#21160;&#65292;&#25152;&#26377;&#36825;&#20123;&#31639;&#27861;&#37117;&#26174;&#33879;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of inferring a Bayesian network structure (directed acyclic graph, DAG for short), we devise a non-reversible continuous time Markov chain, the "Causal Zig-Zag sampler", that targets a probability distribution over classes of observationally equivalent (Markov equivalent) DAGs. The classes are represented as completed partially directed acyclic graphs (CPDAGs). The non-reversible Markov chain relies on the operators used in Chickering's Greedy Equivalence Search (GES) and is endowed with a momentum variable, which improves mixing significantly as we show empirically. The possible target distributions include posterior distributions based on a prior over DAGs and a Markov equivalent likelihood. We offer an efficient implementation wherein we develop new algorithms for listing, counting, uniformly sampling, and applying possible moves of the GES operators, all of which significantly improve upon the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;TVLARS&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21487;&#37197;&#32622;&#30340;&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#27604;LARS&#21644;LAMB&#37117;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.14053</link><description>&lt;p&gt;
&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#25209;&#37327;&#35757;&#32451;&#27867;&#21270;&#24615;&#33021;&#30340;LARS&#20877;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;TVLARS&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21487;&#37197;&#32622;&#30340;&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#27604;LARS&#21644;LAMB&#37117;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20351;&#29992;&#36880;&#23618;&#33258;&#36866;&#24212;&#32553;&#25918;&#27604;(LARS)&#26469;&#25506;&#32034;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#20855;&#26377;&#28909;&#36523;&#38454;&#27573;&#30340;LARS&#31639;&#27861;&#30001;&#20110;&#20887;&#20313;&#30340;&#27604;&#20363;&#32553;&#25918;&#23548;&#33268;&#22312;&#26089;&#26399;&#38519;&#20837;&#23574;&#38160;&#30340;&#26497;&#23567;&#21270;&#22120;&#12290;&#27492;&#22806;&#65292;&#21518;&#26399;&#22266;&#23450;&#30340;&#38497;&#23789;&#19979;&#38477;&#38480;&#21046;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#36941;&#21382;&#26089;&#26399;&#23574;&#38160;&#30340;&#26497;&#23567;&#21270;&#22120;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;Time Varying LARS (TVLARS)&#65292;&#23427;&#29992;&#21487;&#37197;&#32622;&#30340;&#31867;&#20284;sigmoid&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#22312;&#21021;&#22987;&#38454;&#27573;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;TVLARS&#22312;&#26089;&#26399;&#20419;&#36827;&#20102;&#26799;&#24230;&#25506;&#32034;&#65292;&#36229;&#36234;&#20102;&#23574;&#38160;&#30340;&#20248;&#21270;&#22120;&#65292;&#24182;&#36880;&#28176;&#36807;&#28193;&#21040;LARS&#20197;&#23454;&#29616;&#21518;&#26399;&#30340;&#31283;&#20581;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#22987;&#32456;&#20248;&#20110;LARS&#21644;LAMB&#65292;&#20998;&#31867;&#22330;&#26223;&#20013;&#30340;&#25913;&#36827;&#36798;&#21040;2\%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25152;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26696;&#20363;&#20013;&#65292;TVLARS&#37117;&#32988;&#36807;&#20102;LARS&#21644;LAMB&#65292;&#24182;&#19988;&#24615;&#33021;&#25552;&#21319;&#20102;
&lt;/p&gt;
&lt;p&gt;
This paper explores Large Batch Training techniques using layer-wise adaptive scaling ratio (LARS) across diverse settings, uncovering insights. LARS algorithms with warm-up tend to be trapped in sharp minimizers early on due to redundant ratio scaling. Additionally, a fixed steep decline in the latter phase restricts deep neural networks from effectively navigating early-phase sharp minimizers. Building on these findings, we propose Time Varying LARS (TVLARS), a novel algorithm that replaces warm-up with a configurable sigmoid-like function for robust training in the initial phase. TVLARS promotes gradient exploration early on, surpassing sharp optimizers and gradually transitioning to LARS for robustness in later phases. Extensive experiments demonstrate that TVLARS consistently outperforms LARS and LAMB in most cases, with up to 2\% improvement in classification scenarios. Notably, in all self-supervised learning cases, TVLARS dominates LARS and LAMB with performance improvements of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#21644;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#39044;&#27979;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16818</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network. (arXiv:2308.16818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#21644;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#39044;&#27979;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#20013;&#21463;&#26234;&#33021;&#20132;&#21449;&#21475;&#25511;&#21046;&#30340;&#20132;&#21449;&#21475;&#30340;&#20132;&#36890;&#27969;&#37327;&#23545;&#20110;&#25552;&#21319;&#20132;&#36890;&#20986;&#34892;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#19981;&#35268;&#21017;&#65292;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#24182;&#19988;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#24322;&#27493;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;2&#65289;&#20132;&#36890;&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;3) &#38656;&#35201;&#39044;&#27979;&#30340;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#24403;&#21069;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;(ASeer)&#26469;&#39044;&#27979;&#26234;&#33021;&#20132;&#21449;&#21475;&#36827;&#20837;&#36710;&#36947;&#30340;&#20132;&#36890;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#20132;&#36890;&#25193;&#25955;&#22270;&#19978;&#36830;&#25509;&#36710;&#36947;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#22270;&#25193;&#25955;&#32593;&#32476;&#26469;&#27169;&#25311;&#36710;&#36947;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate traffic forecasting at intersections governed by intelligent traffic signals is critical for the advancement of an effective intelligent traffic signal control system. However, due to the irregular traffic time series produced by intelligent intersections, the traffic forecasting task becomes much more intractable and imposes three major new challenges: 1) asynchronous spatial dependency, 2) irregular temporal dependency among traffic data, and 3) variable-length sequence to be predicted, which severely impede the performance of current traffic forecasting methods. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic states of the lanes entering intelligent intersections in a future time window. Specifically, by linking lanes via a traffic diffusion graph, we first propose an Asynchronous Graph Diffusion Network to model the asynchronous spatial dependency between the time-misaligned traffic state measurements of la
&lt;/p&gt;</description></item><item><title>Seq2Seq&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#28508;&#21147;&#22312;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#33021;&#26377;&#25928;&#25552;&#21319;Seq2Seq&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.14856</link><description>&lt;p&gt;
&#21457;&#25381;Seq2Seq&#27169;&#22411;&#20316;&#20026;&#31283;&#20581;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners. (arXiv:2307.14856v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14856
&lt;/p&gt;
&lt;p&gt;
Seq2Seq&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#28508;&#21147;&#22312;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#33021;&#26377;&#25928;&#25552;&#21319;Seq2Seq&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#21482;&#26377;&#35299;&#30721;&#22120;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#32780;&#32534;&#30721;-&#35299;&#30721;&#65288;&#21363;Seq2Seq&#65289;&#27169;&#22411;&#22312;&#20381;&#36182;&#20110;&#26435;&#37325;&#26356;&#26032;&#30340;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;Seq2Seq&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#20294;&#36825;&#20165;&#38480;&#20110;&#19982;Seq2Seq&#20307;&#31995;&#32467;&#26500;&#30456;&#21305;&#37197;&#30340;&#20219;&#21153;&#65292;&#22914;&#25688;&#35201;&#21644;&#32763;&#35793;&#12290;&#21463;&#21040;&#36825;&#20123;&#21021;&#22987;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#27425;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#33021;&#26356;&#26377;&#25928;&#22320;&#24341;&#21457;Seq2Seq&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#26041;&#27861;&#65306;&#30446;&#26631;&#23545;&#40784;&#25552;&#31034;&#21644;&#22522;&#20110;&#34701;&#21512;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#19968;&#20010;&#20307;&#31215;&#26159;&#20854;&#20845;&#20493;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#24120;&#35268;Seq2Seq&#27169;&#22411;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates. Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation. Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks. Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a var
&lt;/p&gt;</description></item></channel></rss>