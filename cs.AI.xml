<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;AVicuna&#65292;&#29983;&#25104;&#20102;PU-VALOR&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#38899;&#39057;-&#35270;&#35273;&#26102;&#38388;&#25351;&#20195;&#23545;&#35805;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#20934;&#30830;&#26102;&#38388;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#22797;&#26434;&#26102;&#38388;&#32447;&#32034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16276</link><description>&lt;p&gt;
AVicuna&#65306;&#20855;&#26377;&#20132;&#38169;&#22120;&#21644;&#19978;&#19979;&#25991;&#36793;&#30028;&#23545;&#40784;&#30340;&#38899;&#39057;-&#35270;&#35273;LLM&#29992;&#20110;&#26102;&#38388;&#25351;&#20195;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16276
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;AVicuna&#65292;&#29983;&#25104;&#20102;PU-VALOR&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#38899;&#39057;-&#35270;&#35273;&#26102;&#38388;&#25351;&#20195;&#23545;&#35805;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#20934;&#30830;&#26102;&#38388;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#22797;&#26434;&#26102;&#38388;&#32447;&#32034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#65292;&#20154;&#31867;&#32463;&#24120;&#20351;&#29992;&#35821;&#38899;&#21644;&#25163;&#21183;&#26469;&#25351;&#20195;&#29305;&#23450;&#21306;&#22495;&#25110;&#23545;&#35937;&#65292;&#36825;&#20010;&#36807;&#31243;&#31216;&#20026;&#25351;&#20195;&#23545;&#35805;&#65288;RD&#65289;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25110;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#35843;&#26597;&#20102;RD&#65292;&#20294;&#22312;&#38899;&#39057;-&#35270;&#35273;&#23186;&#20307;&#20013;&#25506;&#32034;&#26102;&#38388;&#25351;&#20195;&#23545;&#35805;&#65288;TRD&#65289;&#20173;&#28982;&#26377;&#38480;&#12290;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#38459;&#30861;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65306;&#65288;1&#65289;&#32570;&#20047;&#20855;&#26377;&#31934;&#30830;&#26102;&#38388;&#27880;&#37322;&#30340;&#20840;&#38754;&#26410;&#20462;&#21098;&#38899;&#39057;-&#35270;&#35273;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#65288;2&#65289;&#38656;&#35201;&#26377;&#25928;&#25972;&#21512;&#22797;&#26434;&#30340;&#26102;&#38388;&#21548;&#35273;&#21644;&#35270;&#35273;&#32447;&#32034;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29983;&#25104;PU-VALOR&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;114,000&#20010;&#26410;&#20462;&#21098;&#35270;&#39057;&#30340;&#24191;&#27867;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;AVicuna&#65292;&#20855;&#26377;&#38899;&#39057;-&#35270;&#35273;&#20196;&#29260;&#20132;&#38169;&#22120;&#65288;AVTI&#65289;&#65292;&#30830;&#20445;&#20102;&#26102;&#38388;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16276v1 Announce Type: cross  Abstract: In everyday communication, humans frequently use speech and gestures to refer to specific areas or objects, a process known as Referential Dialogue (RD). While prior studies have investigated RD through Large Language Models (LLMs) or Large Multimodal Models (LMMs) in static contexts, the exploration of Temporal Referential Dialogue (TRD) within audio-visual media remains limited. Two primary challenges hinder progress in this field: (1) the absence of comprehensive, untrimmed audio-visual video datasets with precise temporal annotations, and (2) the need for methods to integrate complex temporal auditory and visual cues effectively. To address these challenges, we introduce a novel framework to generate PU-VALOR, an extensive audio-visual dataset comprising over 114,000 untrimmed videos with accurate temporal demarcations. We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI) that ensures the temporal alignment 
&lt;/p&gt;</description></item><item><title>LLM^3&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#20855;&#22791;&#24378;&#22823;&#30340;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#36890;&#36807;&#25509;&#21475;&#25552;&#20986;&#31526;&#21495;&#21160;&#20316;&#24207;&#21015;&#21644;&#36873;&#25321;&#36830;&#32493;&#21160;&#20316;&#21442;&#25968;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#36816;&#21160;&#35268;&#21010;&#30340;&#21453;&#39304;&#26469;&#36845;&#20195;&#20248;&#21270;&#25552;&#35758;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#22788;&#29702;&#39046;&#22495;&#29305;&#23450;&#28040;&#24687;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11552</link><description>&lt;p&gt;
LLM^3:&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#20197;&#21450;&#36816;&#21160;&#22833;&#36133;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLM^3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11552
&lt;/p&gt;
&lt;p&gt;
LLM^3&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#20855;&#22791;&#24378;&#22823;&#30340;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#36890;&#36807;&#25509;&#21475;&#25552;&#20986;&#31526;&#21495;&#21160;&#20316;&#24207;&#21015;&#21644;&#36873;&#25321;&#36830;&#32493;&#21160;&#20316;&#21442;&#25968;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#36816;&#21160;&#35268;&#21010;&#30340;&#21453;&#39304;&#26469;&#36845;&#20195;&#20248;&#21270;&#25552;&#35758;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#22788;&#29702;&#39046;&#22495;&#29305;&#23450;&#28040;&#24687;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#30028;&#38754;&#65292;&#23558;&#31526;&#21495;&#20219;&#21153;&#35268;&#21010;&#19982;&#36830;&#32493;&#36816;&#21160;&#29983;&#25104;&#36830;&#25509;&#36215;&#26469;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#12289;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#27169;&#22359;&#22312;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#26032;&#20219;&#21153;&#26041;&#38754;&#26377;&#38480;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM^3&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;TAMP&#26694;&#26550;&#65292;&#20855;&#26377;&#39046;&#22495;&#26080;&#20851;&#30340;&#25509;&#21475;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#30340;&#24378;&#22823;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#26469;&#25552;&#20986;&#31526;&#21495;&#21160;&#20316;&#24207;&#21015;&#65292;&#24182;&#36873;&#25321;&#36830;&#32493;&#21160;&#20316;&#21442;&#25968;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#12290;&#20851;&#38190;&#26159;&#65292;LLM^3&#36890;&#36807;&#25552;&#31034;&#23558;&#36816;&#21160;&#35268;&#21010;&#21453;&#39304;&#21040;&#20854;&#20013;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#36890;&#36807;&#23545;&#36816;&#21160;&#22833;&#36133;&#36827;&#34892;&#25512;&#29702;&#26469;&#36845;&#20195;&#22320;&#20248;&#21270;&#20854;&#25552;&#35758;&#12290;&#22240;&#27492;&#65292;LLM^3&#22312;&#20219;&#21153;&#35268;&#21010;&#21644;&#36816;&#21160;&#35268;&#21010;&#20043;&#38388;&#24314;&#31435;&#25509;&#21475;&#65292;&#20943;&#36731;&#20102;&#22788;&#29702;&#23427;&#20204;&#20043;&#38388;&#29305;&#23450;&#39046;&#22495;&#28040;&#24687;&#30340;&#22797;&#26434;&#35774;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11552v1 Announce Type: cross  Abstract: Conventional Task and Motion Planning (TAMP) approaches rely on manually crafted interfaces connecting symbolic task planning with continuous motion generation. These domain-specific and labor-intensive modules are limited in addressing emerging tasks in real-world settings. Here, we present LLM^3, a novel Large Language Model (LLM)-based TAMP framework featuring a domain-independent interface. Specifically, we leverage the powerful reasoning and planning capabilities of pre-trained LLMs to propose symbolic action sequences and select continuous action parameters for motion planning. Crucially, LLM^3 incorporates motion planning feed- back through prompting, allowing the LLM to iteratively refine its proposals by reasoning about motion failure. Consequently, LLM^3 interfaces between task planning and motion planning, alleviating the intricate design process of handling domain- specific messages between them. Through a series of simulat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;</title><link>https://arxiv.org/abs/2403.11116</link><description>&lt;p&gt;
&#21338;&#22763;&#35770;&#25991;&#65306;&#19968;&#20010;&#25552;&#31034;&#30340;&#35270;&#35273;&#24187;&#35273;&#35780;&#20272;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PhD: A Prompted Visual Hallucination Evaluation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#25512;&#21160;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#21457;&#23637;&#12290;&#22312;LLMs&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#24187;&#35273;&#25361;&#25112;&#20063;&#20986;&#29616;&#22312;LVLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;LVLM&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;&#19978;&#65292;&#24573;&#30053;&#20102;LVLM&#24187;&#35273;&#30340;&#22810;&#26679;&#21270;&#31867;&#22411;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22266;&#26377;&#35270;&#35273;&#35821;&#35328;&#24187;&#35273;&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#65292;&#23545;&#23548;&#33268;&#24187;&#35273;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;IVL-Hallu&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#65288;a&#65289;&#23545;&#35937;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#35937;&#30340;&#35823;&#35782;&#21035;&#32780;&#20135;&#29983;&#65292;&#65288;b&#65289;&#23646;&#24615;&#24187;&#35273;&#65292;&#30001;&#20110;&#23646;&#24615;&#30340;&#35823;&#35782;&#21035;&#32780;&#24341;&#36215;&#65292;&#65288;c&#65289;&#22810;&#27169;&#24577;&#20914;&#31361;&#24187;&#35273;&#65292;&#28304;&#33258;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#20197;&#21450;&#65288;d&#65289;&#21453;&#24120;&#35782;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#31435;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11116v1 Announce Type: cross  Abstract: The rapid growth of Large Language Models (LLMs) has driven the development of Large Vision-Language Models (LVLMs). The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs. However, most existing efforts mainly focus on object hallucination in LVLM, ignoring diverse types of LVLM hallucinations. In this study, we delve into the Intrinsic Vision-Language Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of IVL-Hallu on their causes and reflections. Specifically, we propose several novel IVL-Hallu tasks and categorize them into four types: (a) object hallucination, which arises from the misidentification of objects, (b) attribute hallucination, which is caused by the misidentification of attributes, (c) multi-modal conflicting hallucination, which derives from the contradictions between textual and visual information, and (d) counter-common-sense hallucination, which owes to the contradictions betwee
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DiffuMatting&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#25216;&#26415;&#23454;&#29616;&#20102;&#8220;&#25248;&#22270;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#27880;&#37322;&#65292;&#21516;&#26102;&#20860;&#23481;&#31038;&#21306;LoRAs&#25110;&#21508;&#31181;&#26465;&#20214;&#25511;&#21046;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06168</link><description>&lt;p&gt;
DiffuMatting&#65306;&#20351;&#29992;Matting&#32423;&#21035;&#26631;&#27880;&#21512;&#25104;&#20219;&#24847;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06168
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DiffuMatting&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#25216;&#26415;&#23454;&#29616;&#20102;&#8220;&#25248;&#22270;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#27880;&#37322;&#65292;&#21516;&#26102;&#20860;&#23481;&#31038;&#21306;LoRAs&#25110;&#21508;&#31181;&#26465;&#20214;&#25511;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33719;&#21462;&#39640;&#24230;&#20934;&#30830;&#25110;&#25248;&#22270;&#27880;&#37322;&#30340;&#22256;&#38590;&#21644;&#21171;&#21160;&#23494;&#38598;&#24615;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#39640;&#24230;&#20934;&#30830;&#26631;&#31614;&#25968;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;DiffuMatting&#65292;&#23427;&#32487;&#25215;&#20102;&#25193;&#25955;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#36171;&#20104;&#20102;&#8220;&#25248;&#22270;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;DiffuMatting&#21487;&#20197;&#65306;1&#65289;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#27880;&#37322;&#30340;&#20219;&#24847;&#25248;&#22270;&#24037;&#21378;&#65307;2&#65289;&#19982;&#31038;&#21306;LoRAs&#25110;&#21508;&#31181;&#26465;&#20214;&#25511;&#21046;&#26041;&#27861;&#20860;&#23481;&#65292;&#20197;&#23454;&#29616;&#31038;&#21306;&#21451;&#22909;&#30340;&#33402;&#26415;&#35774;&#35745;&#21644;&#21487;&#25511;&#29983;&#25104;&#12290;&#20855;&#20307;&#22320;&#65292;&#21463;&#32511;&#24149;&#25248;&#20687;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#25945;&#25480;&#25193;&#25955;&#27169;&#22411;&#22312;&#22266;&#23450;&#30340;&#32511;&#24149;&#30011;&#24067;&#19978;&#32472;&#30011;&#12290;&#20026;&#27492;&#65292;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#32511;&#24149;&#25968;&#25454;&#38598;&#65288;Green100K&#65289;&#20316;&#20026;DiffuMatting&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32511;&#33394;&#32972;&#26223;&#25511;&#21046;&#25439;&#22833;&#65292;&#20197;&#20445;&#25345;&#30011;&#24067;&#20026;&#32431;&#32511;&#33394;&#20197;&#36827;&#34892;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06168v1 Announce Type: cross  Abstract: Due to the difficulty and labor-consuming nature of getting highly accurate or matting annotations, there only exists a limited amount of highly accurate labels available to the public. To tackle this challenge, we propose a DiffuMatting which inherits the strong Everything generation ability of diffusion and endows the power of "matting anything". Our DiffuMatting can 1). act as an anything matting factory with high accurate annotations 2). be well-compatible with community LoRAs or various conditional control approaches to achieve the community-friendly art design and controllable generation. Specifically, inspired by green-screen-matting, we aim to teach the diffusion model to paint on a fixed green screen canvas. To this end, a large-scale greenscreen dataset (Green100K) is collected as a training dataset for DiffuMatting. Secondly, a green background control loss is proposed to keep the drawing board as a pure green color to disti
&lt;/p&gt;</description></item><item><title>ComTraQ-MPC&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;DQN&#21644;MPC&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#22312;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;</title><link>https://arxiv.org/abs/2403.01564</link><description>&lt;p&gt;
ComTraQ-MPC&#65306;&#20803;&#35757;&#32451;&#30340;DQN-MPC&#38598;&#25104;&#29992;&#20110;&#20855;&#26377;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#30340;&#36712;&#36857;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
ComTraQ-MPC: Meta-Trained DQN-MPC Integration for Trajectory Tracking with Limited Active Localization Updates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01564
&lt;/p&gt;
&lt;p&gt;
ComTraQ-MPC&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;DQN&#21644;MPC&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#22312;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23616;&#37096;&#21487;&#35266;&#23519;&#12289;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36712;&#36857;&#36319;&#36394;&#30340;&#26368;&#20339;&#20915;&#31574;&#24448;&#24448;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#26159;&#25351;&#20195;&#29702;&#20174;&#20256;&#24863;&#22120;&#33719;&#21462;&#30495;&#23454;&#29366;&#24577;&#20449;&#24687;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#24179;&#34913;&#36164;&#28304;&#20445;&#23384;&#12289;&#20934;&#30830;&#29366;&#24577;&#20272;&#35745;&#21644;&#31934;&#30830;&#36319;&#36394;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ComTraQ-MPC&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;Deep Q-Networks (DQN)&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;&#20803;&#35757;&#32451;&#30340;DQN&#30830;&#20445;&#20102;&#33258;&#36866;&#24212;&#20027;&#21160;&#23450;&#20301;&#35843;&#24230;&#65292;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01564v1 Announce Type: cross  Abstract: Optimal decision-making for trajectory tracking in partially observable, stochastic environments where the number of active localization updates -- the process by which the agent obtains its true state information from the sensors -- are limited, presents a significant challenge. Traditional methods often struggle to balance resource conservation, accurate state estimation and precise tracking, resulting in suboptimal performance. This problem is particularly pronounced in environments with large action spaces, where the need for frequent, accurate state data is paramount, yet the capacity for active localization updates is restricted by external limitations. This paper introduces ComTraQ-MPC, a novel framework that combines Deep Q-Networks (DQN) and Model Predictive Control (MPC) to optimize trajectory tracking with constrained active localization updates. The meta-trained DQN ensures adaptive active localization scheduling, while the
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102; Persona-DB&#65292;&#19968;&#20010;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#32423;&#26500;&#24314;&#36807;&#31243;&#21644;&#21327;&#21516;&#20248;&#21270;&#65292;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#20013;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26816;&#32034;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11060</link><description>&lt;p&gt;
Persona-DB&#65306;&#29992;&#20110;&#21709;&#24212;&#39044;&#27979;&#30340;&#39640;&#25928;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#19982;&#21327;&#21516;&#25968;&#25454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11060
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102; Persona-DB&#65292;&#19968;&#20010;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#32423;&#26500;&#24314;&#36807;&#31243;&#21644;&#21327;&#21516;&#20248;&#21270;&#65292;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#20013;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26816;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20010;&#24615;&#21270;&#20132;&#20114;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#20934;&#30830;&#24555;&#36895;&#35782;&#21035;&#29992;&#25143;&#24847;&#35265;&#21644;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#26816;&#32034;&#22686;&#24378;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#31574;&#30053;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#29992;&#25143;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26816;&#32034;&#38454;&#27573;&#65292;&#24182;&#23545;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#20248;&#21270;&#36827;&#34892;&#20102;&#26377;&#38480;&#30340;&#25506;&#32034;&#65292;&#36825;&#26159;&#20010;&#24615;&#21270;&#31561;&#20219;&#21153;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#30528;&#37325;&#20110;&#22914;&#20309;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#25968;&#25454;&#65292;&#20197;&#20415;&#22312;LLM&#23450;&#21046;&#30340;&#24773;&#22659;&#19979;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#26816;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Persona-DB&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#20998;&#23618;&#26500;&#24314;&#36807;&#31243;&#65292;&#20197;&#25913;&#21892;&#36328;&#20219;&#21153;&#32972;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#34892;&#21327;&#21516;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11060v1 Announce Type: cross  Abstract: The increasing demand for personalized interactions with large language models (LLMs) calls for the development of methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;AI&#31995;&#32479;&#33021;&#22815;&#26381;&#21153;&#20110;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#38656;&#27714;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#40784;&#23450;&#20041;&#21644;&#23454;&#29616;&#22810;&#20803;&#20027;&#20041;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#26469;&#35780;&#20272;&#21644;&#27979;&#35797;&#22810;&#20803;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05070</link><description>&lt;p&gt;
&#36890;&#24448;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
A Roadmap to Pluralistic Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05070
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;AI&#31995;&#32479;&#33021;&#22815;&#26381;&#21153;&#20110;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#38656;&#27714;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#40784;&#23450;&#20041;&#21644;&#23454;&#29616;&#22810;&#20803;&#20027;&#20041;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#26469;&#35780;&#20272;&#21644;&#27979;&#35797;&#22810;&#20803;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26435;&#21147;&#21644;&#26222;&#21450;&#31243;&#24230;&#30340;&#22686;&#21152;&#65292;&#35774;&#35745;&#33021;&#22815;&#20026;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#20154;&#26381;&#21153;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#20197;&#26381;&#21153;&#22810;&#20803;&#20154;&#31867;&#20215;&#20540;&#35266;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20855;&#20307;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#30830;&#23450;&#21644;&#24418;&#24335;&#21270;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#26041;&#24335;&#26469;&#23450;&#20041;&#21644;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#22810;&#20803;&#20027;&#20041;&#65306;1&#65289;Overton&#22810;&#20803;&#27169;&#22411;&#65292;&#23637;&#31034;&#21512;&#29702;&#21453;&#24212;&#30340;&#20809;&#35889;&#65307;2&#65289;&#21487;&#25805;&#25511;&#30340;&#22810;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#35843;&#25972;&#20197;&#21453;&#26144;&#29305;&#23450;&#30340;&#35266;&#28857;&#65307;3&#65289;&#20998;&#24067;&#22810;&#20803;&#27169;&#22411;&#65292;&#22312;&#20998;&#24067;&#20013;&#24456;&#22909;&#22320;&#26657;&#20934;&#32473;&#23450;&#20154;&#32676;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#21644;&#24418;&#24335;&#21270;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#65306;1&#65289;&#22810;&#30446;&#26631;&#22522;&#20934;&#65307;2&#65289;&#26435;&#34913;&#21487;&#25805;&#25511;&#22522;&#20934;&#65292;&#40723;&#21169;&#27169;&#22411;&#23545;&#20219;&#24847;&#26435;&#34913;&#36827;&#34892;&#35843;&#25972;&#65307;3&#65289;&#38506;&#23457;&#22242;&#22810;&#20803;&#22522;&#20934;&#65292;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#19981;&#21516;&#38506;&#23457;&#22242;&#30340;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly m
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;Clarify&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#19968;&#33268;&#22833;&#36133;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03715</link><description>&lt;p&gt;
&#28548;&#28165;&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Clarify: Improving Model Robustness With Natural Language Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03715
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;Clarify&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#19968;&#33268;&#22833;&#36133;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30456;&#20851;&#24615;&#12290;&#36825;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#20381;&#36182;&#20110;&#39640;&#32423;&#38169;&#35823;&#27010;&#24565;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#38169;&#35823;&#27010;&#24565;&#65292;&#25105;&#20204;&#24517;&#39035;&#25552;&#20379;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20123;&#39069;&#22806;&#30340;&#23454;&#20363;&#32423;&#30417;&#30563;&#24418;&#24335;&#65292;&#20363;&#22914;&#26631;&#35760;&#34394;&#20551;&#29305;&#24449;&#25110;&#26469;&#33258;&#24179;&#34913;&#20998;&#24067;&#30340;&#39069;&#22806;&#26631;&#35760;&#25968;&#25454;&#12290;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#20250;&#21464;&#24471;&#26114;&#36149;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20197;&#25509;&#36817;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#36827;&#34892;&#39069;&#22806;&#27880;&#37322;&#12290;&#25105;&#20204;&#20551;&#35774;&#26377;&#38024;&#23545;&#24615;&#30340;&#20851;&#20110;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#39069;&#22806;&#30417;&#30563;&#24418;&#24335;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Clarify&#65292;&#19968;&#31181;&#26032;&#22411;&#30028;&#38754;&#21644;&#26041;&#27861;&#26469;&#20132;&#20114;&#24335;&#22320;&#32416;&#27491;&#27169;&#22411;&#30340;&#38169;&#35823;&#27010;&#24565;&#12290;&#36890;&#36807;Clarify&#65292;&#29992;&#25143;&#21482;&#38656;&#35201;&#25552;&#20379;&#19968;&#20010;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#25551;&#36848;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#22833;&#36133;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23436;&#20840;&#33258;&#21160;&#21270;&#22320;&#20351;&#29992;s
&lt;/p&gt;
&lt;p&gt;
In supervised learning, models are trained to extract correlations from a static dataset. This often leads to models that rely on high-level misconceptions. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision. We introduce Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns. Then, in an entirely automated way, we use s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;Copilot&#29983;&#25104;&#30340;Python&#20195;&#30721;&#20013;&#30340;&#20195;&#30721;&#24322;&#21619;&#65292;&#35780;&#20272;Copilot&#20462;&#22797;&#36825;&#20123;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;8&#31181;Python&#20195;&#30721;&#24322;&#21619;&#21487;&#20197;&#22312;Copilot&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#26816;&#27979;&#21040;&#12290;</title><link>http://arxiv.org/abs/2401.14176</link><description>&lt;p&gt;
Copilot&#32454;&#21270;&#65306;&#35299;&#20915;Copilot&#29983;&#25104;&#30340;Python&#20195;&#30721;&#20013;&#30340;&#20195;&#30721;&#24322;&#21619;
&lt;/p&gt;
&lt;p&gt;
Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code. (arXiv:2401.14176v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;Copilot&#29983;&#25104;&#30340;Python&#20195;&#30721;&#20013;&#30340;&#20195;&#30721;&#24322;&#21619;&#65292;&#35780;&#20272;Copilot&#20462;&#22797;&#36825;&#20123;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;8&#31181;Python&#20195;&#30721;&#24322;&#21619;&#21487;&#20197;&#22312;Copilot&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26368;&#27969;&#34892;&#30340;&#21160;&#24577;&#35821;&#35328;&#20043;&#19968;&#65292;Python&#22312;&#23384;&#22312;&#20195;&#30721;&#24322;&#21619;&#26102;&#21487;&#35835;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#20250;&#19979;&#38477;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;AI&#25903;&#25345;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#37325;&#26500;&#24037;&#20855;&#30340;&#26085;&#30410;&#20851;&#27880;&#12290;GitHub Copilot&#26159;&#20854;&#20013;&#19968;&#31181;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#12290;Copilot Chat&#26159;&#22312;2023&#24180;9&#26376;&#21457;&#24067;&#30340;&#19968;&#31181;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#39537;&#21160;&#30340;&#32534;&#30721;&#25552;&#20379;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29702;&#35299;Copilot&#29983;&#25104;&#30340;Python&#20195;&#30721;&#20013;&#30340;&#20195;&#30721;&#24322;&#21619;&#20197;&#21450;Copilot&#20462;&#22797;&#20854;&#29983;&#25104;&#30340;&#20195;&#30721;&#24322;&#21619;&#30340;&#33021;&#21147;&#65292;&#20154;&#20204;&#24182;&#27809;&#26377;&#32473;&#20104;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;102&#20010;Copilot&#29983;&#25104;&#30340;Python&#20195;&#30721;&#20013;&#30340;&#20195;&#30721;&#24322;&#21619;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39318;&#20808;&#25506;&#32034;Copilot&#29983;&#25104;&#30340;Python&#20195;&#30721;&#20013;&#20195;&#30721;&#24322;&#21619;&#30340;&#21457;&#29983;&#24773;&#20917;&#65292;&#28982;&#21518;&#35780;&#20272;Copilot&#22312;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#20462;&#22797;&#36825;&#20123;&#20195;&#30721;&#24322;&#21619;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;10&#31181;Python&#20195;&#30721;&#24322;&#21619;&#20013;&#26377;8&#31181;&#21487;&#20197;&#22312;Copilot&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released on September 2023, functions as an interactive tool aims at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot's ability to fix the code smells it generates. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot in fixing these code smells employing different prompts. The results show that 8 out of 10 types of Python smells can be detected in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;GPS&#23450;&#20301;&#26694;&#26550;E2E-PrNet&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;PrNet&#26469;&#36827;&#34892;&#20266;&#36317;&#20462;&#27491;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#31471;&#21040;&#31471;GPS&#23450;&#20301;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10685</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#20551;&#20266;&#36317;&#20462;&#27491;&#23454;&#29616;&#31471;&#21040;&#31471;GPS&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-End GPS Localization with Neural Pseudorange Correction. (arXiv:2401.10685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;GPS&#23450;&#20301;&#26694;&#26550;E2E-PrNet&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;PrNet&#26469;&#36827;&#34892;&#20266;&#36317;&#20462;&#27491;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#31471;&#21040;&#31471;GPS&#23450;&#20301;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#36317;&#35823;&#24046;&#26159;GPS&#23450;&#20301;&#19981;&#20934;&#30830;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#20197;&#24448;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#20013;&#38388;&#26631;&#31614;&#36827;&#34892;&#20266;&#36317;&#35823;&#24046;&#22238;&#24402;&#21644;&#28040;&#38500;&#12290;&#19982;&#20043;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;GPS&#23450;&#20301;&#26694;&#26550;E2E-PrNet&#65292;&#36890;&#36807;&#20351;&#29992;GPS&#25509;&#25910;&#26426;&#29366;&#24577;&#30340;&#30495;&#23454;&#20540;&#35745;&#31639;&#26368;&#32456;&#20219;&#21153;&#25439;&#22833;&#65292;&#30452;&#25509;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#20266;&#36317;&#20462;&#27491;&#30340;&#31070;&#32463;&#32593;&#32476;PrNet&#12290;&#25439;&#22833;&#23545;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#26799;&#24230;&#36890;&#36807;&#21487;&#24494;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#22120;&#21453;&#21521;&#20256;&#25773;&#21040;PrNet&#12290;&#36890;&#36807;&#20351;&#29992;Android&#25163;&#26426;&#25910;&#38598;&#30340;GPS&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#26174;&#31034;E2E-PrNet&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31471;&#21040;&#31471;GPS&#23450;&#20301;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudorange errors are the root cause of localization inaccuracy in GPS. Previous data-driven methods regress and eliminate pseudorange errors using handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS localization framework, E2E-PrNet, to train a neural network for pseudorange correction (PrNet) directly using the final task loss calculated with the ground truth of GPS receiver states. The gradients of the loss with respect to learnable parameters are backpropagated through a differentiable nonlinear least squares optimizer to PrNet. The feasibility is verified with GPS data collected by Android phones, showing that E2E-PrNet outperforms the state-of-the-art end-to-end GPS localization methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#24212;&#29992;&#21644;&#32467;&#26524;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#24739;&#32773;&#21442;&#19982;&#22686;&#24378;&#31561;&#26041;&#38754;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#35782;&#21035;&#21644;&#35752;&#35770;&#20102;&#22312;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.02984</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Mental Health Care: a Scoping Review. (arXiv:2401.02984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#24212;&#29992;&#21644;&#32467;&#26524;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#24739;&#32773;&#21442;&#19982;&#22686;&#24378;&#31561;&#26041;&#38754;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#35782;&#21035;&#21644;&#35752;&#35770;&#20102;&#22312;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#38656;&#35201;&#23545;&#23427;&#20204;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;&#21644;&#32467;&#26524;&#36827;&#34892;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#26412;&#32508;&#36848;&#30740;&#31350;&#26088;&#22312;&#23545;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#29616;&#26377;&#21457;&#23637;&#21644;&#24212;&#29992;&#36827;&#34892;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#31361;&#20986;&#23427;&#20204;&#30340;&#25104;&#21151;&#65292;&#24182;&#35782;&#21035;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;2023&#24180;11&#26376;&#65292;&#22312;PubMed&#12289;Web of Science&#12289;Google Scholar&#12289;arXiv&#12289;medRxiv&#21644;PsyArXiv&#20845;&#20010;&#25968;&#25454;&#24211;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#25628;&#32034;&#65292;&#36981;&#24490;2020&#24180;&#29256;&#30340;&#8220;&#31995;&#32479;&#35780;&#20215;&#21644;Meta&#20998;&#26512;&#30340;&#39318;&#36873;&#25253;&#21578;&#39033;&#30446;&#8221;&#65288;PRISMA&#65289;&#25351;&#21335;&#12290;&#26368;&#21021;&#35782;&#21035;&#20102;313&#31687;&#20986;&#29256;&#29289;&#65292;&#25353;&#29031;&#30740;&#31350;&#32435;&#20837;&#26631;&#20934;&#65292;&#26368;&#32456;&#36873;&#25321;&#20102;34&#31687;&#20986;&#29256;&#29289;&#36827;&#34892;&#32508;&#36848;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#20102;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#35786;&#26029;&#12289;&#27835;&#30103;&#12289;&#24739;&#32773;&#21442;&#19982;&#22686;&#24378;&#31561;&#12290;&#20851;&#38190;&#25361;&#25112;&#21644;&#38480;&#21046;&#26041;&#38754;&#30340;&#21457;&#29616;&#23558;&#34987;&#24635;&#32467;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The growing use of large language models (LLMs) stimulates a need for a comprehensive review of their applications and outcomes in mental health care contexts. This scoping review aims to critically analyze the existing development and applications of LLMs in mental health care, highlighting their successes and identifying their challenges and limitations in these specialized fields. Materials and Methods: A broad literature search was conducted in November 2023 using six databases (PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and PsyArXiv) following the 2020 version of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A total of 313 publications were initially identified, and after applying the study inclusion criteria, 34 publications were selected for the final review. Results: We identified diverse applications of LLMs in mental health care, including diagnosis, therapy, patient engagement enhancement, etc. Key challen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;TACO&#65292;&#29992;&#20110;&#24314;&#27169;&#39640;&#24230;&#19982;&#25299;&#25169;&#32467;&#26500;&#30456;&#20851;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#25299;&#25169;&#24863;&#30693;&#30456;&#20851;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23454;&#20307;&#26080;&#20851;&#30340;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.11528</link><description>&lt;p&gt;
&#23398;&#20064;&#20851;&#31995;&#20043;&#38388;&#30340;&#23436;&#25972;&#25299;&#25169;&#24863;&#30693;&#30456;&#20851;&#24615;&#20197;&#36827;&#34892;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Complete Topology-Aware Correlations Between Relations for Inductive Link Prediction. (arXiv:2309.11528v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;TACO&#65292;&#29992;&#20110;&#24314;&#27169;&#39640;&#24230;&#19982;&#25299;&#25169;&#32467;&#26500;&#30456;&#20851;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#25299;&#25169;&#24863;&#30693;&#30456;&#20851;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23454;&#20307;&#26080;&#20851;&#30340;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#8212;&#8212;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#23454;&#20307;&#21487;&#33021;&#19981;&#21516;&#8212;&#8212;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#20197;&#23454;&#20307;&#26080;&#20851;&#30340;&#26041;&#24335;&#23436;&#25104;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#35768;&#22810;&#27969;&#34892;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#24314;&#27169;&#22270;&#32423;&#29305;&#24449;&#65292;&#32780;&#36793;&#32423;&#20132;&#20114;&#8212;&#8212;&#23588;&#20854;&#26159;&#20851;&#31995;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#8212;&#8212;&#21017;&#34987;&#36739;&#23569;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#35821;&#20041;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#19968;&#20010;&#29702;&#24819;&#29305;&#24615;&#26159;&#23427;&#20204;&#22312;&#26412;&#36136;&#19978;&#26159;&#36793;&#32423;&#21644;&#23454;&#20307;&#26080;&#20851;&#30340;&#12290;&#36825;&#24847;&#21619;&#30528;&#35821;&#20041;&#30456;&#20851;&#24615;&#23545;&#20110;&#23454;&#20307;&#26080;&#20851;&#30340;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#21363;TACO&#65292;&#26469;&#24314;&#27169;&#19982;&#20854;&#23376;&#22270;&#20869;&#30340;&#25299;&#25169;&#32467;&#26500;&#39640;&#24230;&#30456;&#20851;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#25299;&#25169;&#24863;&#30693;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inductive link prediction -- where entities during training and inference stages can be different -- has shown great potential for completing evolving knowledge graphs in an entity-independent manner. Many popular methods mainly focus on modeling graph-level features, while the edge-level interactions -especially the semantic correlations between relations -- have been less explored. However, we notice a desirable property of semantic correlations between relations is that they are inherently edge-level and entity-independent. This implies the great potential of the semantic correlations for the entity-independent inductive link prediction task. Inspired by this observation, we propose a novel subgraph-based method, namely TACO, to model Topology-Aware COrrelations between relations that are highly correlated to their topological structures within subgraphs. Specifically, we prove that semantic correlations between any two relations can be categorized into seven topological patterns,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#21270;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#36827;&#34892;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.09237</link><description>&lt;p&gt;
SCALE: &#25552;&#21319;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
SCALE: Scaling up the Complexity for Advanced Language Model Evaluation. (arXiv:2306.09237v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#21270;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#36827;&#34892;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#24050;&#32463;&#39281;&#21644;&#20102;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65288;&#21253;&#25324;&#19987;&#19994;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#65289;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#26032;&#39062;&#12289;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26469;&#27491;&#30830;&#35780;&#20272;LLM&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#23545;&#24403;&#21069;LLM&#30340;&#22235;&#20010;&#20851;&#38190;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#65306;&#22788;&#29702;&#38271;&#25991;&#26723;&#65288;&#22810;&#36798;50K&#20010;&#26631;&#35760;&#65289;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65288;&#20307;&#29616;&#22312;&#27861;&#24459;&#25991;&#26412;&#20013;&#65289;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#65288;&#28085;&#30422;&#20116;&#31181;&#35821;&#35328;&#65289;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#65288;&#21253;&#25324;&#27861;&#24459;&#25991;&#20214;&#21040;&#25991;&#20214;&#20449;&#24687;&#26816;&#32034;&#12289;&#27861;&#24237;&#35270;&#22270;&#29983;&#25104;&#12289;&#37325;&#35201;&#20915;&#31574;&#25688;&#35201;&#12289;&#24341;&#29992;&#25552;&#21462;&#21644;&#20843;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65289;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#30340;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#32852;&#37030;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#24378;&#28872;&#30340;&#23457;&#26597;/&#20998;&#26512;&#20219;&#21153;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26723;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent strides in Large Language Models (LLMs) have saturated many NLP benchmarks (even professional domain-specific ones), emphasizing the need for novel, more challenging novel ones to properly assess LLM capabilities. In this paper, we introduce a novel NLP benchmark that poses challenges to current LLMs across four key dimensions: processing long documents (up to 50K tokens), utilizing domain specific knowledge (embodied in legal texts), multilingual understanding (covering five languages), and multitasking (comprising legal document to document Information Retrieval, Court View Generation, Leading Decision Summarization, Citation Extraction, and eight challenging Text Classification tasks). Our benchmark comprises diverse legal NLP datasets from the Swiss legal system, allowing for a comprehensive study of the underlying Non-English, inherently multilingual, federal legal system. Despite recent advances, efficiently processing long documents for intense review/analysis tasks remai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#30340;&#31038;&#20132;&#32593;&#32476;&#20027;&#39064;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#35821;&#35328;&#32467;&#26500;&#24182;&#35774;&#35745;&#19987;&#38376;&#30340;&#25277;&#21462;&#31639;&#27861;&#65292;&#22312;FA-CUP&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13066</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#30340;&#31038;&#20132;&#32593;&#32476;&#20027;&#39064;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Human Word Association based model for topic detection in social networks. (arXiv:2301.13066v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#30340;&#31038;&#20132;&#32593;&#32476;&#20027;&#39064;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#35821;&#35328;&#32467;&#26500;&#24182;&#35774;&#35745;&#19987;&#38376;&#30340;&#25277;&#21462;&#31639;&#27861;&#65292;&#22312;FA-CUP&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#32593;&#32476;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#26816;&#27979;&#36825;&#20123;&#32593;&#32476;&#20013;&#35752;&#35770;&#30340;&#20027;&#39064;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#22522;&#20110;&#39057;&#32321;&#27169;&#24335;&#25366;&#25496;&#25110;&#35821;&#20041;&#20851;&#31995;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#35821;&#35328;&#32467;&#26500;&#12290;&#35821;&#35328;&#32467;&#26500;&#26041;&#27861;&#30340;&#24847;&#20041;&#22312;&#20110;&#21457;&#29616;&#35789;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#20154;&#31867;&#22914;&#20309;&#29702;&#35299;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#21033;&#29992;&#35789;&#27719;&#32852;&#24819;&#30340;&#24515;&#29702;&#33021;&#21147;&#27169;&#25311;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#30340;&#31038;&#20132;&#32593;&#32476;&#20027;&#39064;&#26816;&#27979;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19987;&#38376;&#30340;&#25277;&#21462;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;FA-CUP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20027;&#39064;&#26816;&#27979;&#39046;&#22495;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20027;&#39064;&#21484;&#22238;&#29575;&#21644;&#20851;&#38190;&#35789;F1&#20540;&#19978;&#26377;&#36739;&#22909;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#20027;&#39064;&#26816;&#27979;&#39046;&#22495;&#20013;&#30340;&#22823;&#22810;&#25968;&#20808;&#21069;&#24037;&#20316;&#20027;&#35201;&#22522;&#20110;&#27169;&#24335;&#25366;&#25496;&#25110;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of social networks, detecting the topics discussed in these networks has become a significant challenge. The current works are mainly based on frequent pattern mining or semantic relations, and the language structure is not considered. The meaning of language structural methods is to discover the relationship between words and how humans understand them. Therefore, this paper uses the Concept of the Imitation of the Mental Ability of Word Association to propose a topic detection framework in social networks. This framework is based on the Human Word Association method. A special extraction algorithm has also been designed for this purpose. The performance of this method is evaluated on the FA-CUP dataset. It is a benchmark dataset in the field of topic detection. The results show that the proposed method is a good improvement compared to other methods, based on the Topic-recall and the keyword F1 measure. Also, most of the previous works in the field of topic de
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#27169;&#22411;&#21270;&#25277;&#35937;&#30446;&#26631;&#65292;&#20197;&#38477;&#20302;&#34892;&#21160;&#39044;&#27979;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;&#35270;&#35273;&#34920;&#24449;&#26469;&#25551;&#36848;&#21160;&#20316;&#21644;&#30446;&#26631;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#25277;&#35937;&#30446;&#26631;&#20026;&#19968;&#20010;&#20998;&#24067;&#12290;&#35813;&#27169;&#22411;&#21487;&#22312;Epic-Kitchen&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05044</link><description>&lt;p&gt;
&#24314;&#27169;&#25277;&#35937;&#30446;&#26631;&#39044;&#27979;&#19979;&#19968;&#27493;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Predicting the Next Action by Modeling the Abstract Goal. (arXiv:2209.05044v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#27169;&#22411;&#21270;&#25277;&#35937;&#30446;&#26631;&#65292;&#20197;&#38477;&#20302;&#34892;&#21160;&#39044;&#27979;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;&#35270;&#35273;&#34920;&#24449;&#26469;&#25551;&#36848;&#21160;&#20316;&#21644;&#30446;&#26631;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#25277;&#35937;&#30446;&#26631;&#20026;&#19968;&#20010;&#20998;&#24067;&#12290;&#35813;&#27169;&#22411;&#21487;&#22312;Epic-Kitchen&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20154;&#31867;&#21160;&#20316;&#30340;&#38382;&#39064;&#20855;&#26377;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#26159;&#65292;&#22914;&#26524;&#25105;&#20204;&#26377;&#20851;&#20110;&#21160;&#20316;&#23454;&#29616;&#30446;&#26631;&#30340;&#24863;&#30693;&#65292;&#21487;&#20197;&#38477;&#20302;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#21160;&#39044;&#27979;&#27169;&#22411;&#65292;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#26469;&#20943;&#23569;&#26410;&#26469;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#35270;&#35273;&#34920;&#24449;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21160;&#20316;&#21644;&#30446;&#26631;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#25277;&#35937;&#30446;&#26631;&#30340;&#26032;&#27010;&#24565;&#65292;&#20854;&#21462;&#20915;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#29305;&#24449;&#24207;&#21015;&#65292;&#29992;&#20110;&#34892;&#21160;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25277;&#35937;&#30446;&#26631;&#35774;&#35745;&#20026;&#19968;&#20010;&#20998;&#24067;&#65292;&#20854;&#21442;&#25968;&#26159;&#20351;&#29992;&#21464;&#20998;&#36882;&#24402;&#32593;&#32476;&#20272;&#35745;&#30340;&#12290;&#25105;&#20204;&#23545;&#19979;&#19968;&#20010;&#21160;&#20316;&#36827;&#34892;&#22810;&#27425;&#37319;&#26679;&#65292;&#24182;&#24341;&#20837;&#30446;&#26631;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#30830;&#23450;&#20174;&#25277;&#35937;&#30446;&#26631;&#24471;&#20986;&#30340;&#26368;&#20339;&#20505;&#36873;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;Epic-Kitchen&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of anticipating human actions is an inherently uncertain one. However, we can reduce this uncertainty if we have a sense of the goal that the actor is trying to achieve. Here, we present an action anticipation model that leverages goal information for the purpose of reducing the uncertainty in future predictions. Since we do not possess goal information or the observed actions during inference, we resort to visual representation to encapsulate information about both actions and goals. Through this, we derive a novel concept called abstract goal which is conditioned on observed sequences of visual features for action anticipation. We design the abstract goal as a distribution whose parameters are estimated using a variational recurrent network. We sample multiple candidates for the next action and introduce a goal consistency measure to determine the best candidate that follows from the abstract goal. Our method obtains impressive results on the very challenging Epic-Kitchen
&lt;/p&gt;</description></item></channel></rss>