<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#26356;&#24378;&#30340;&#24179;&#22343;&#24773;&#20917;&#35745;&#31639;&#20998;&#31163;&#65292;&#23545;&#20110;&#8220;&#20856;&#22411;&#8221;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#20219;&#21153;&#23454;&#20363;&#65292;&#21333;&#27169;&#24577;&#23398;&#20064;&#22312;&#35745;&#31639;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#22810;&#27169;&#24577;&#23398;&#20064;&#21364;&#24456;&#23481;&#26131;&#12290;</title><link>https://arxiv.org/abs/2404.02254</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#27169;&#24577;&#19982;&#21333;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#26356;&#24378;&#30340;&#35745;&#31639;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02254
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26356;&#24378;&#30340;&#24179;&#22343;&#24773;&#20917;&#35745;&#31639;&#20998;&#31163;&#65292;&#23545;&#20110;&#8220;&#20856;&#22411;&#8221;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#20219;&#21153;&#23454;&#20363;&#65292;&#21333;&#27169;&#24577;&#23398;&#20064;&#22312;&#35745;&#31639;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#22810;&#27169;&#24577;&#23398;&#20064;&#21364;&#24456;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23558;&#22810;&#31181;&#25968;&#25454;&#27169;&#24577;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#32467;&#21512;&#36215;&#26469;&#20197;&#20419;&#36827;&#26356;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#36825;&#20173;&#28982;&#36866;&#29992;&#20110;&#30456;&#24212;&#30340;&#21333;&#27169;&#24577;&#20219;&#21153;&#65288;&#20363;&#22914;&#25991;&#26412;&#29983;&#25104;&#65289;&#12290;&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#32463;&#39564;&#25104;&#21151;&#65288;&#20363;&#22914;GPT-4&#65289;&#12290;&#21463;&#21040;&#20026;&#36825;&#31181;&#32463;&#39564;&#25104;&#21151;&#24320;&#21457;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#65292;Lu&#65288;NeurIPS '23&#65292;ALT '24&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#23398;&#20064;&#29702;&#35770;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#23398;&#20064;&#30340;&#29702;&#35770;&#27169;&#22411;&#20043;&#38388;&#21487;&#33021;&#30340;&#20998;&#31163;&#12290;&#29305;&#21035;&#26159;Lu&#65288;ALT '24&#65289;&#23637;&#31034;&#20102;&#19968;&#31181;&#35745;&#31639;&#20998;&#31163;&#65292;&#36825;&#23545;&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#22351;&#24773;&#20917;&#23454;&#20363;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02254v1 Announce Type: cross  Abstract: In multimodal machine learning, multiple modalities of data (e.g., text and images) are combined to facilitate the learning of a better machine learning model, which remains applicable to a corresponding unimodal task (e.g., text generation). Recently, multimodal machine learning has enjoyed huge empirical success (e.g. GPT-4). Motivated to develop theoretical justification for this empirical success, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning, and considers possible separations between theoretical models of multimodal and unimodal learning. In particular, Lu (ALT '24) shows a computational separation, which is relevant to worst-case instances of the learning task.   In this paper, we give a stronger average-case computational separation, where for "typical" instances of the learning task, unimodal learning is computationally hard, but multimodal learning is easy. We then question how "organic" the average-cas
&lt;/p&gt;</description></item><item><title>CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01343</link><description>&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01343
&lt;/p&gt;
&lt;p&gt;
CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#21644;&#36719;&#20214;&#24179;&#21488;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#20351;&#29992;&#20687;GPT-3.5&#12289;GPT-4&#12289;GLM-3&#21644;LLaMa-2&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23458;&#25143;&#26381;&#21153;&#30340;&#32842;&#22825;&#36741;&#21161;&#25110;&#25512;&#29702;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#23458;&#25143;&#26381;&#21153;&#27169;&#22411;&#22312;&#19982;&#23458;&#25143;&#37197;&#32622;&#25991;&#20214;&#30340;&#38598;&#25104;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#24182;&#19988;&#32570;&#20047;&#26377;&#25928;&#26381;&#21153;&#25152;&#38656;&#30340;&#25805;&#20316;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#65288;CHat with custOmer Profile in existing System&#65289;&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#65306;&#65288;1&#65289;&#39640;&#25928;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#20197;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#25110;&#25353;&#29031;&#29616;&#26377;&#25351;&#21335;&#19982;&#36825;&#20123;&#31995;&#32479;&#20132;&#20114;&#65307;&#65288;2&#65289;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#22312;&#31995;&#32479;&#20013;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#65307;&#65288;3&#65289;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01343v1 Announce Type: cross  Abstract: Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverag
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#24320;&#21457;&#25351;&#20196;&#39537;&#21160;&#28216;&#25103;&#24341;&#25806;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21019;&#24314;&#28216;&#25103;&#65292;&#20174;&#32780;&#38477;&#20302;&#28216;&#25103;&#24320;&#21457;&#30340;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00276</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25351;&#20196;&#39537;&#21160;&#28216;&#25103;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Instruction-Driven Game Engines on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00276
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#24320;&#21457;&#25351;&#20196;&#39537;&#21160;&#28216;&#25103;&#24341;&#25806;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21019;&#24314;&#28216;&#25103;&#65292;&#20174;&#32780;&#38477;&#20302;&#28216;&#25103;&#24320;&#21457;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Instruction-Driven Game Engine (IDGE) &#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36981;&#24490;&#33258;&#30001;&#24418;&#24335;&#30340;&#28216;&#25103;&#35268;&#21017;&#24182;&#33258;&#21160;&#29983;&#25104;&#28216;&#25103;&#36807;&#31243;&#26469;&#20351;&#28216;&#25103;&#24320;&#21457;&#27665;&#20027;&#21270;&#12290;IDGE&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#21457;&#20986;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#21019;&#24314;&#28216;&#25103;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#28216;&#25103;&#24320;&#21457;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#23558;IDGE&#30340;&#23398;&#20064;&#36807;&#31243;&#35270;&#20026;&#19979;&#19968;&#20010;&#29366;&#24577;&#39044;&#27979;&#20219;&#21153;&#65292;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#39044;&#27979;&#29609;&#23478;&#34892;&#21160;&#32473;&#20986;&#30340;&#28216;&#25103;&#29366;&#24577;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28216;&#25103;&#29366;&#24577;&#30340;&#35745;&#31639;&#24517;&#39035;&#20934;&#30830;&#65307;&#21542;&#21017;&#65292;&#36731;&#24494;&#30340;&#38169;&#35823;&#21487;&#33021;&#20250;&#30772;&#22351;&#28216;&#25103;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20197;&#35838;&#31243;&#26041;&#24335;&#35757;&#32451;IDGE&#65292;&#36880;&#28176;&#22686;&#21152;&#27169;&#22411;&#23545;&#22797;&#26434;&#22330;&#26223;&#30340;&#25509;&#35302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00276v1 Announce Type: new  Abstract: The Instruction-Driven Game Engine (IDGE) project aims to democratize game development by enabling a large language model (LLM) to follow free-form game rules and autonomously generate game-play processes. The IDGE allows users to create games by issuing simple natural language instructions, which significantly lowers the barrier for game development. We approach the learning process for IDGEs as a Next State Prediction task, wherein the model autoregressively predicts in-game states given player actions. It is a challenging task because the computation of in-game states must be precise; otherwise, slight errors could disrupt the game-play. To address this, we train the IDGE in a curriculum manner that progressively increases the model's exposure to complex scenarios.   Our initial progress lies in developing an IDGE for Poker, a universally cherished card game. The engine we've designed not only supports a wide range of poker variants b
&lt;/p&gt;</description></item><item><title>TinySaver&#26159;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#26469;&#33258;&#36866;&#24212;&#22320;&#26367;&#25442;&#22823;&#22411;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17726</link><description>&lt;p&gt;
&#23567;&#22411;&#27169;&#22411;&#26159;&#22823;&#22411;&#27169;&#22411;&#30340;&#35745;&#31639;&#33410;&#30465;&#32773;
&lt;/p&gt;
&lt;p&gt;
Tiny Models are the Computational Saver for Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17726
&lt;/p&gt;
&lt;p&gt;
TinySaver&#26159;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#26469;&#33258;&#36866;&#24212;&#22320;&#26367;&#25442;&#22823;&#22411;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TinySaver&#65292;&#19968;&#31181;&#31867;&#20284;&#20110;&#26089;&#26399;&#36864;&#20986;&#30340;&#21160;&#24577;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#26469;&#33258;&#36866;&#24212;&#22320;&#26367;&#25442;&#22823;&#22411;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#21387;&#32553;&#25216;&#26415;&#19981;&#21516;&#65292;&#20687;TinySaver&#36825;&#26679;&#30340;&#21160;&#24577;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#38590;&#24230;&#24046;&#24322;&#65292;&#20351;&#24471;&#26576;&#20123;&#36755;&#20837;&#33021;&#22815;&#25552;&#21069;&#23436;&#25104;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26089;&#26399;&#36864;&#20986;&#35774;&#35745;&#26159;&#36890;&#36807;&#21521;&#27169;&#22411;&#30340;&#39592;&#24178;&#32467;&#26500;&#38468;&#21152;&#39069;&#22806;&#30340;&#32593;&#32476;&#20998;&#25903;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23436;&#20840;&#29420;&#31435;&#30340;&#23567;&#22411;&#27169;&#22411;&#21487;&#20197;&#22312;&#23545;&#24615;&#33021;&#24433;&#21709;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#26367;&#20195;&#36739;&#22823;&#27169;&#22411;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#12290;&#23558;&#23427;&#20204;&#20316;&#20026;&#31532;&#19968;&#20010;&#36864;&#20986;&#28857;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#25628;&#32034;&#24182;&#20351;&#29992;&#26368;&#21512;&#36866;&#30340;&#23567;&#22411;&#27169;&#22411;&#20316;&#20026;&#32473;&#23450;&#22823;&#22411;&#27169;&#22411;&#30340;&#35745;&#31639;&#33410;&#30465;&#32773;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17726v1 Announce Type: new  Abstract: This paper introduces TinySaver, an early-exit-like dynamic model compression approach which employs tiny models to substitute large models adaptively. Distinct from traditional compression techniques, dynamic methods like TinySaver can leverage the difficulty differences to allow certain inputs to complete their inference processes early, thereby conserving computational resources. Most existing early exit designs are implemented by attaching additional network branches to the model's backbone. Our study, however, reveals that completely independent tiny models can replace a substantial portion of the larger models' job with minimal impact on performance. Employing them as the first exit can remarkably enhance computational efficiency. By searching and employing the most appropriate tiny model as the computational saver for a given large model, the proposed approaches work as a novel and generic method to model compression. This finding
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13890</link><description>&lt;p&gt;
&#20197;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#23545;&#27604;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#23545;&#27604;&#21058;&#21487;&#20197;&#23450;&#20301;&#32959;&#30244;&#24182;&#35266;&#23519;&#20854;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#36825;&#23545;&#20110;&#30284;&#30151;&#34920;&#24449;&#21644;&#27835;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#21058;&#30340;&#20351;&#29992;&#19981;&#20165;&#19982;&#19981;&#33391;&#20581;&#24247;&#39118;&#38505;&#30456;&#20851;&#65292;&#32780;&#19988;&#23545;&#20110;&#24576;&#23381;&#24739;&#32773;&#12289;&#32958;&#21151;&#33021;&#38556;&#30861;&#24739;&#32773;&#25110;&#20854;&#20182;&#19981;&#33391;&#21453;&#24212;&#24739;&#32773;&#23384;&#22312;&#38480;&#21046;&#12290;&#30001;&#20110;&#23545;&#27604;&#21058;&#25668;&#21462;&#26159;&#30149;&#28790;&#24694;&#24615;&#12289;&#30284;&#30151;&#22797;&#21457;&#39118;&#38505;&#21644;&#27835;&#30103;&#21453;&#24212;&#30340;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22240;&#27492;&#20943;&#23569;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#36827;&#34892;DCE-MRI&#26102;&#38388;&#24207;&#21015;&#30340;&#33719;&#21462;&#26102;&#38388;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#30340;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#21464;&#24322;&#24615;&#30340;Fr\'echet&#25918;&#23556;&#32452;&#23398;&#36317;&#31163;&#20316;&#20026;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13890v1 Announce Type: cross  Abstract: Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow to localize tumors and observe their contrast kinetics, which is essential for cancer characterization and respective treatment decision-making. However, contrast agent administration is not only associated with adverse health risks, but also restricted for patients during pregnancy, and for those with kidney malfunction, or other adverse reactions. With contrast uptake as key biomarker for lesion malignancy, cancer recurrence risk, and treatment response, it becomes pivotal to reduce the dependency on intravenous contrast agent administration. To this end, we propose a multi-conditional latent diffusion model capable of acquisition time-conditioned image synthesis of DCE-MRI temporal sequences. To evaluate medical image synthesis, we additionally propose and validate the Fr\'echet radiomics distance as an image quality measure based on biomarker variability 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.09857</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09857
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#27169;&#22411;&#26088;&#22312;&#22312;&#20445;&#30041;&#26087;&#31867;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#36880;&#27493;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#31232;&#32570;&#26679;&#26412;&#12290;&#29616;&#26377;&#30340;FSCIL&#26041;&#27861;&#36890;&#24120;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#24494;&#35843;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38459;&#30861;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#28508;&#21147;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26368;&#36817;&#22522;&#20110;&#25552;&#31034;&#30340;CIL&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#29992;&#36275;&#22815;&#30340;&#25968;&#25454;&#35757;&#32451;&#25552;&#31034;&#26469;&#20943;&#36731;&#36951;&#24536;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#65288;ASP&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;ASP&#36890;&#36807;&#20174;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;ASP&#20013;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#25552;&#20379;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;&#24635;&#20043;&#65292;ASP&#38450;&#27490;&#20102;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#30340;&#36807;&#25311;&#21512;&#65292;&#24182;&#19981;&#38656;&#35201;&#22312;&#23569;&#26679;&#26412;&#22686;&#37327;&#20219;&#21153;&#20013;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09857v1 Announce Type: cross  Abstract: Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#36827;&#34892;&#21487;&#20449;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65292;&#22312;&#32500;&#25345;&#31454;&#20105;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#21487;&#38752;&#25512;&#26029;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.03281</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#36827;&#34892;&#21487;&#20449;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03281
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#36827;&#34892;&#21487;&#20449;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65292;&#22312;&#32500;&#25345;&#31454;&#20105;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#21487;&#38752;&#25512;&#26029;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#38024;&#23545;&#36776;&#21035;&#23398;&#20064;&#30340;&#36831;&#21040;&#22810;&#27169;&#24577;&#34701;&#21512;&#38382;&#39064;&#12290;&#21463;&#21040;&#38656;&#35201;&#29702;&#35299;&#27599;&#20010;&#25968;&#25454;&#28304;&#21487;&#38752;&#24615;&#30340;&#22024;&#26434;&#30340;&#22810;&#28304;&#39046;&#22495;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#21487;&#20449;&#24230;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#26469;&#32467;&#21512;&#20010;&#20307;&#27169;&#24577;&#19978;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#32452;&#21512;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#23450;&#20041;&#20102;&#19968;&#31181;&#27010;&#29575;&#24230;&#37327;&#26469;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#30340;&#21487;&#20449;&#24230;&#65292;&#36890;&#36807;PC&#19978;&#30340;&#25512;&#29702;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34701;&#21512;&#26041;&#27861;&#33021;&#22815;&#21487;&#38752;&#22320;&#25512;&#26029;&#21487;&#20449;&#24230;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03281v1 Announce Type: cross  Abstract: We consider the problem of late multi-modal fusion for discriminative learning. Motivated by noisy, multi-source domains that require understanding the reliability of each data source, we explore the notion of credibility in the context of multi-modal fusion. We propose a combination function that uses probabilistic circuits (PCs) to combine predictive distributions over individual modalities. We also define a probabilistic measure to evaluate the credibility of each modality via inference queries over the PC. Our experimental evaluation demonstrates that our fusion method can reliably infer credibility while maintaining competitive performance with the state-of-the-art.
&lt;/p&gt;</description></item><item><title>TaylorShift&#36890;&#36807;&#24341;&#20837;TaylorSoftmax&#37325;&#26032;&#35745;&#31639;&#20840;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#30001;&#24179;&#26041;&#32423;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02920</link><description>&lt;p&gt;
TaylorShift&#65306;&#21033;&#29992;TaylorSoftmax&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#20174;&#24179;&#26041;&#32423;&#36716;&#21464;&#20026;&#32447;&#24615;&#32423;&#65288;&#20877;&#36716;&#22238;&#21435;&#65289;
&lt;/p&gt;
&lt;p&gt;
TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02920
&lt;/p&gt;
&lt;p&gt;
TaylorShift&#36890;&#36807;&#24341;&#20837;TaylorSoftmax&#37325;&#26032;&#35745;&#31639;&#20840;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#30001;&#24179;&#26041;&#32423;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#26159;&#20351;&#29992;Transformer&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#38754;&#20020;&#30340;&#26368;&#22823;&#38556;&#30861;&#20043;&#19968;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#31232;&#30095;&#34920;&#31034;&#25110;&#26377;&#29366;&#24577;&#30340;&#24490;&#29615;&#65292;&#29306;&#29298;&#20102;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#26368;&#32456;&#23548;&#33268;&#24615;&#33021;&#19978;&#30340;&#22949;&#21327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TaylorShift&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;Taylor softmax &#37325;&#26500;&#65292;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#21644;&#31354;&#38388;&#20869;&#35745;&#31639;&#20840;&#20307;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#30830;&#23450;&#20102;&#20351;&#29992;TaylorShift&#27604;&#20256;&#32479;&#27880;&#24847;&#21147;&#26356;&#21152;&#39640;&#25928;&#30340;&#20132;&#21449;&#28857;&#65292;&#36825;&#19982;&#23454;&#35777;&#27979;&#37327;&#32467;&#26524;&#23494;&#20999;&#21305;&#37197;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;TaylorShift&#25552;&#39640;&#20102;&#23545;&#30701;&#33267;800&#20010;&#35760;&#21495;&#30340;&#24207;&#21015;&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#24182;&#21152;&#36895;&#20102;&#23545;&#38271;&#36798;&#32422;1700&#20010;&#35760;&#21495;&#21450;&#20197;&#19978;&#36755;&#20837;&#30340;&#25512;&#26029;&#12290;&#23545;&#20110;&#36739;&#30701;&#30340;&#24207;&#21015;&#65292;TaylorShift&#19982;&#21407;&#22987;&#27880;&#24847;&#21147;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#20998;&#31867;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02920v1 Announce Type: cross  Abstract: The quadratic complexity of the attention mechanism represents one of the biggest hurdles for processing long sequences using Transformers. Current methods, relying on sparse representations or stateful recurrence, sacrifice token-to-token interactions, which ultimately leads to compromises in performance. This paper introduces TaylorShift, a novel reformulation of the Taylor softmax that enables computing full token-to-token interactions in linear time and space. We analytically determine the crossover points where employing TaylorShift becomes more efficient than traditional attention, aligning closely with empirical measurements. Specifically, our findings demonstrate that TaylorShift enhances memory efficiency for sequences as short as 800 tokens and accelerates inference for inputs of approximately 1700 tokens and beyond. For shorter sequences, TaylorShift scales comparably with the vanilla attention. Furthermore, a classification
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#31639;&#27861;&#36766;&#32844;&#20316;&#20026;&#31649;&#29702;&#32452;&#32455;&#20869;&#20351;&#29992;AI&#31995;&#32479;&#30340;&#25112;&#30053;&#26041;&#27861;&#65292;&#25552;&#20986;&#36890;&#36807;&#22312;AI&#31995;&#32479;&#20013;&#23884;&#20837;&#27835;&#29702;&#26426;&#21046;&#65292;&#26377;&#24847;&#35782;&#21644;&#26126;&#26234;&#22320;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33073;&#31163;AI&#36741;&#21161;&#65292;&#20197;&#25351;&#23548;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#25110;&#36991;&#20813;&#36825;&#20123;&#31995;&#32479;&#65292;&#20174;&#32780;&#33719;&#24471;&#22810;&#26041;&#38754;&#30340;&#21033;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.18326</link><description>&lt;p&gt;
&#31639;&#27861;&#20309;&#26102;&#35813;&#36766;&#32844;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Should Algorithms Resign?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18326
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#31639;&#27861;&#36766;&#32844;&#20316;&#20026;&#31649;&#29702;&#32452;&#32455;&#20869;&#20351;&#29992;AI&#31995;&#32479;&#30340;&#25112;&#30053;&#26041;&#27861;&#65292;&#25552;&#20986;&#36890;&#36807;&#22312;AI&#31995;&#32479;&#20013;&#23884;&#20837;&#27835;&#29702;&#26426;&#21046;&#65292;&#26377;&#24847;&#35782;&#21644;&#26126;&#26234;&#22320;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33073;&#31163;AI&#36741;&#21161;&#65292;&#20197;&#25351;&#23548;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#25110;&#36991;&#20813;&#36825;&#20123;&#31995;&#32479;&#65292;&#20174;&#32780;&#33719;&#24471;&#22810;&#26041;&#38754;&#30340;&#21033;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#31639;&#27861;&#36766;&#32844;&#65292;&#36825;&#26159;&#19968;&#31181;&#31649;&#29702;&#32452;&#32455;&#20869;&#20351;&#29992;AI&#31995;&#32479;&#30340;&#25112;&#30053;&#26041;&#27861;&#12290;&#31639;&#27861;&#36766;&#32844;&#28041;&#21450;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26377;&#24847;&#35782;&#21644;&#26126;&#26234;&#22320;&#33073;&#31163;AI&#36741;&#21161;&#65292;&#36890;&#36807;&#30452;&#25509;&#23558;&#27835;&#29702;&#26426;&#21046;&#23884;&#20837;&#21040;AI&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#19981;&#20165;&#20165;&#26159;&#20851;&#20110;&#19981;&#20351;&#29992;AI&#65292;&#36824;&#21253;&#25324;&#25351;&#23548;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#25110;&#36991;&#20813;&#36825;&#20123;&#31995;&#32479;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#31639;&#27861;&#36766;&#32844;&#30340;&#22810;&#26041;&#38754;&#21033;&#30410;&#65292;&#28085;&#30422;&#32463;&#27982;&#25928;&#29575;&#12289;&#22768;&#35465;&#22686;&#30410;&#21644;&#27861;&#24459;&#21512;&#35268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#36890;&#36807;&#31215;&#26497;&#21644;&#28040;&#26497;&#21161;&#25512;&#12289;&#21033;&#30410;&#30456;&#20851;&#32773;&#28608;&#21169;&#23545;&#40784;&#20197;&#21450;&#23457;&#24910;&#32771;&#34385;AI&#21442;&#19982;&#31243;&#24230;&#31561;&#21508;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36766;&#32844;&#12290;&#36890;&#36807;&#35832;&#22914;&#26377;&#36873;&#25321;&#22320;&#38459;&#27490;&#35775;&#38382;AI&#36755;&#20986;&#25110;&#23545;&#31995;&#32479;&#24615;&#33021;&#26126;&#30830;&#22768;&#26126;&#20813;&#36131;&#31561;&#25216;&#26415;&#25163;&#27573;&#65292;&#31639;&#27861;&#36766;&#32844;&#19981;&#20165;&#33021;&#38477;&#20302;&#19982;&#20043;&#30456;&#20851;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18326v1 Announce Type: cross  Abstract: This paper discusses algorithmic resignation, a strategic approach for managing the use of AI systems within organizations. Algorithmic resignation involves the deliberate and informed disengagement from AI assistance in certain scenarios, by embedding governance mechanisms directly into AI systems. Our proposal is not merely about disuse of AI but includes guiding when and how these systems should be used or avoided. We discuss the multifaceted benefits of algorithmic resignation, spanning economic efficiency, reputational gains, and legal compliance. Further, we outline the operationalization of resignation through various methods such as positive and negative nudges, stakeholder incentive alignment, and careful consideration of the level of AI engagement. Using techniques like barring access to AI outputs selectively or providing explicit disclaimers on system performance, algorithmic resignation not only mitigates risks associated 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MerRec&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;C2C&#25512;&#33616;&#32780;&#25552;&#20986;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;C2C&#25512;&#33616;&#25968;&#25454;&#38598;&#20013;&#29289;&#21697;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#35268;&#27169;&#31561;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;</title><link>https://arxiv.org/abs/2402.14230</link><description>&lt;p&gt;
MerRec&#65306;&#29992;&#20110;&#28040;&#36153;&#32773;&#23545;&#28040;&#36153;&#32773;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#22810;&#21151;&#33021;Mercari&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MerRec: A Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer Recommendation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MerRec&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;C2C&#25512;&#33616;&#32780;&#25552;&#20986;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;C2C&#25512;&#33616;&#25968;&#25454;&#38598;&#20013;&#29289;&#21697;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#35268;&#27169;&#31561;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#22320;&#22609;&#36896;&#20102;&#29992;&#25143;&#20307;&#39564;&#21644;&#21442;&#19982;&#24230;&#12290;&#28040;&#36153;&#32773;&#23545;&#28040;&#36153;&#32773;&#65288;C2C&#65289;&#25512;&#33616;&#31995;&#32479;&#30340;&#23835;&#36215;&#65292;&#20197;&#20854;&#28789;&#27963;&#24615;&#21644;&#20026;&#23458;&#25143;&#20379;&#24212;&#21830;&#25552;&#20379;&#26131;&#20110;&#35775;&#38382;&#30340;&#29305;&#28857;&#65292;&#26631;&#24535;&#30528;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#23398;&#26415;&#20851;&#27880;&#20027;&#35201;&#38598;&#20013;&#22312;&#21830;&#23478;&#23545;&#28040;&#36153;&#32773;&#65288;B2C&#65289;&#27169;&#22411;&#19978;&#65292;&#30041;&#19979;&#20102;&#19968;&#20010;&#31354;&#30333;&#65292;&#21363;&#32570;&#20047;&#29289;&#21697;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#35268;&#27169;&#30340;C2C&#25512;&#33616;&#25968;&#25454;&#38598;&#12290;C2C&#25512;&#33616;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#36827;&#19968;&#27493;&#31361;&#20986;&#20102;&#29992;&#25143;&#25198;&#28436;&#21334;&#23478;&#21644;&#20080;&#23478;&#20004;&#31181;&#35282;&#33394;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#19981;&#37027;&#20040;&#32479;&#19968;&#21644;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MerRec&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;C2C&#25512;&#33616;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28304;&#33258;Mercari&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#65292;&#35206;&#30422;&#20102;2023&#24180;6&#20010;&#26376;&#20869;&#25968;&#30334;&#19975;&#29992;&#25143;&#21644;&#20135;&#21697;&#12290;MerRec&#19981;&#20165;&#21253;&#25324;&#26631;&#20934;&#29305;&#24449;&#65292;&#22914;user_id&#12289;item_id&#21644;session_id
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14230v1 Announce Type: cross  Abstract: In the evolving e-commerce field, recommendation systems crucially shape user experience and engagement. The rise of Consumer-to-Consumer (C2C) recommendation systems, noted for their flexibility and ease of access for customer vendors, marks a significant trend. However, the academic focus remains largely on Business-to-Consumer (B2C) models, leaving a gap filled by the limited C2C recommendation datasets that lack in item attributes, user diversity, and scale. The intricacy of C2C recommendation systems is further accentuated by the dual roles users assume as both sellers and buyers, introducing a spectrum of less uniform and varied inputs. Addressing this, we introduce MerRec, the first large-scale dataset specifically for C2C recommendations, sourced from the Mercari e-commerce platform, covering millions of users and products over 6 months in 2023. MerRec not only includes standard features such as user_id, item_id, and session_id
&lt;/p&gt;</description></item><item><title>EmoBench&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12071</link><description>&lt;p&gt;
EmoBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
EmoBench: Evaluating the Emotional Intelligence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12071
&lt;/p&gt;
&lt;p&gt;
EmoBench&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20984;&#26174;&#20102;&#38656;&#35201;&#31283;&#20581;&#12289;&#20840;&#38754;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#24773;&#24863;&#26234;&#33021;&#65288;EI&#65289;&#36827;&#34892;&#35780;&#20272;&#30340;&#30740;&#31350;&#30456;&#24403;&#26377;&#38480;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#24773;&#24863;&#35782;&#21035;&#65292;&#24573;&#35270;&#20102;&#24773;&#24863;&#35843;&#33410;&#31561;&#37325;&#35201;&#30340;&#24773;&#24863;&#26234;&#33021;&#33021;&#21147;&#65292;&#32780;&#24773;&#24863;&#29702;&#35299;&#21017;&#20419;&#36827;&#24773;&#24863;; &#20854;&#27425;&#65292;&#23427;&#20204;&#20027;&#35201;&#22522;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#39057;&#32321;&#27169;&#24335;&#12289;&#26126;&#30830;&#20449;&#24687;&#21644;&#27880;&#37322;&#38169;&#35823;&#65292;&#23548;&#33268;&#35780;&#20272;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EmoBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20511;&#37492;&#20102;&#24050;&#24314;&#31435;&#30340;&#24515;&#29702;&#29702;&#35770;&#65292;&#24182;&#20026;&#26426;&#22120;EI&#25552;&#20986;&#20102;&#32508;&#21512;&#23450;&#20041;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;EmoBench&#21253;&#25324;&#19968;&#32452;400&#20010;&#29992;&#33521;&#35821;&#21644;&#20013;&#25991;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#65292;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#38656;&#35201;&#28145;&#20837;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12071v1 Announce Type: cross  Abstract: Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning
&lt;/p&gt;</description></item><item><title>BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.10373</link><description>&lt;p&gt;
BioMistral&#65306;&#38754;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10373
&lt;/p&gt;
&lt;p&gt;
BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#21644;&#21307;&#23398;&#31561;&#19987;&#19994;&#39046;&#22495;&#25552;&#20379;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#38024;&#23545;&#20581;&#24247;&#39046;&#22495;&#23450;&#21046;&#30340;&#24320;&#28304;LLMs&#21487;&#29992;&#65292;&#20294;&#23558;&#36890;&#29992;LLMs&#35843;&#25972;&#21040;&#21307;&#23398;&#39046;&#22495;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioMistral&#65292;&#19968;&#31181;&#19987;&#20026;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#28304;LLM&#65292;&#37319;&#29992;Mistral&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;PubMed Central&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;10&#20010;&#24050;&#24314;&#31435;&#30340;&#33521;&#25991;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#19978;&#23545;BioMistral&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#36890;&#36807;&#37327;&#21270;&#21644;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#33719;&#24471;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BioMistral&#30456;&#36739;&#20110;&#29616;&#26377;&#24320;&#28304;&#21307;&#23398;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#19982;&#19987;&#26377;&#23545;&#25163;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10373v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Diffusion-ES&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06559</link><description>&lt;p&gt;
Diffusion-ES:&#22522;&#20110;&#25193;&#25955;&#30340;&#38646;&#26799;&#24230;&#35268;&#21010;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#38646;&#38454;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Diffusion-ES&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#20915;&#31574;&#21644;&#25511;&#21046;&#20013;&#23545;&#22797;&#26434;&#21644;&#22810;&#27169;&#24577;&#36712;&#36857;&#20998;&#24067;&#24314;&#27169;&#26377;&#24456;&#24378;&#20248;&#21183;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22870;&#21169;&#26799;&#24230;&#24341;&#23548;&#21435;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#20135;&#29983;&#22312;&#25193;&#25955;&#27169;&#22411;&#25152;&#25429;&#33719;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#21487;&#24494;&#20998;&#22870;&#21169;&#20989;&#25968;&#21644;&#20284;&#28982;&#24615;&#30340;&#36712;&#36857;&#12290;&#22870;&#21169;&#26799;&#24230;&#24341;&#23548;&#21435;&#22122;&#38656;&#35201;&#19968;&#20010;&#36866;&#21512;&#20110;&#28165;&#27905;&#21644;&#22122;&#22768;&#26679;&#26412;&#30340;&#21487;&#24494;&#20998;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#36712;&#36857;&#20248;&#21270;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffusionES&#65292;&#19968;&#31181;&#23558;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#27969;&#24418;&#20013;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;Diffusion-ES&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#12290;&#23427;&#36890;&#36807;&#25130;&#26029;&#25193;&#25955;&#36807;&#31243;&#23545;&#24471;&#20998;&#39640;&#30340;&#36712;&#36857;&#36827;&#34892;&#21464;&#24322;&#65292;&#35813;&#36807;&#31243;&#24212;&#29992;&#23569;&#37327;&#30340;&#22122;&#22768;&#21644;&#21435;&#22122;&#27493;&#39588;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models excel at modeling complex and multimodal trajectory distributions for decision-making and control. Reward-gradient guided denoising has been recently proposed to generate trajectories that maximize both a differentiable reward function and the likelihood under the data distribution captured by a diffusion model. Reward-gradient guided denoising requires a differentiable reward function fitted to both clean and noised samples, limiting its applicability as a general trajectory optimizer. In this paper, we propose DiffusionES, a method that combines gradient-free optimization with trajectory denoising to optimize black-box non-differentiable objectives while staying in the data manifold. Diffusion-ES samples trajectories during evolutionary search from a diffusion model and scores them using a black-box reward function. It mutates high-scoring trajectories using a truncated diffusion process that applies a small number of noising and denoising steps, allowing for much mo
&lt;/p&gt;</description></item><item><title>TimeDRL&#26159;&#19968;&#20010;&#20855;&#26377;&#35299;&#32544;&#21452;&#23618;&#23884;&#20837;&#30340;&#36890;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#38388;&#25139;&#32423;&#21035;&#21644;&#23454;&#20363;&#32423;&#21035;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#35299;&#32544;&#27966;&#29983;&#20197;&#21450;&#26102;&#38388;&#25139;-&#39044;&#27979;&#21644;&#23454;&#20363;-&#23545;&#27604;&#20219;&#21153;&#30340;&#21033;&#29992;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#20016;&#23500;&#34920;&#31034;&#24182;&#35299;&#20915;&#24402;&#32435;&#20559;&#24046;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.04142</link><description>&lt;p&gt;
TimeDRL&#65306;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TimeDRL: Disentangled Representation Learning for Multivariate Time-Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04142
&lt;/p&gt;
&lt;p&gt;
TimeDRL&#26159;&#19968;&#20010;&#20855;&#26377;&#35299;&#32544;&#21452;&#23618;&#23884;&#20837;&#30340;&#36890;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#38388;&#25139;&#32423;&#21035;&#21644;&#23454;&#20363;&#32423;&#21035;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#35299;&#32544;&#27966;&#29983;&#20197;&#21450;&#26102;&#38388;&#25139;-&#39044;&#27979;&#21644;&#23454;&#20363;-&#23545;&#27604;&#20219;&#21153;&#30340;&#21033;&#29992;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#20016;&#23500;&#34920;&#31034;&#24182;&#35299;&#20915;&#24402;&#32435;&#20559;&#24046;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65288;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#24037;&#19994;&#65289;&#38750;&#24120;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26631;&#31614;&#21644;&#39640;&#32500;&#24230;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30740;&#31350;&#26174;&#31034;&#20102;&#22312;&#23398;&#20064;&#20016;&#23500;&#34920;&#31034;&#32780;&#19981;&#20381;&#36182;&#20110;&#26631;&#31614;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#23398;&#20064;&#35299;&#32544;&#23884;&#20837;&#21644;&#35299;&#20915;&#24402;&#32435;&#20559;&#24046;&#65288;&#20363;&#22914;&#21464;&#25442;&#19981;&#21464;&#24615;&#65289;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TimeDRL&#65292;&#19968;&#20010;&#20855;&#26377;&#35299;&#32544;&#21452;&#23618;&#23884;&#20837;&#30340;&#36890;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;TimeDRL&#30340;&#19977;&#20010;&#26032;&#39062;&#29305;&#24449;&#20026;&#65306;&#65288;i&#65289;&#20351;&#29992;[CLS]&#20196;&#29260;&#31574;&#30053;&#20174;&#25171;&#34917;&#19969;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#35299;&#32544;&#26102;&#38388;&#25139;&#32423;&#21644;&#23454;&#20363;&#32423;&#23884;&#20837;&#65307;&#65288;ii&#65289;&#21033;&#29992;&#26102;&#38388;&#25139;&#39044;&#27979;&#21644;&#23454;&#20363;&#23545;&#27604;&#20219;&#21153;&#36827;&#34892;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#21069;&#32773;&#20248;&#21270;&#26102;&#38388;&#25139;&#32423;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04142v2 Announce Type: replace-cross  Abstract: Multivariate time-series data in numerous real-world applications (e.g., healthcare and industry) are informative but challenging due to the lack of labels and high dimensionality. Recent studies in self-supervised learning have shown their potential in learning rich representations without relying on labels, yet they fall short in learning disentangled embeddings and addressing issues of inductive bias (e.g., transformation-invariance). To tackle these challenges, we propose TimeDRL, a generic multivariate time-series representation learning framework with disentangled dual-level embeddings. TimeDRL is characterized by three novel features: (i) disentangled derivation of timestamp-level and instance-level embeddings from patched time-series data using a [CLS] token strategy; (ii) utilization of timestamp-predictive and instance-contrastive tasks for disentangled representation learning, with the former optimizing timestamp-lev
&lt;/p&gt;</description></item><item><title>NOD-TAMP&#26159;&#19968;&#20010;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#26469;&#35299;&#20915;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#36712;&#36857;&#24182;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01530</link><description>&lt;p&gt;
NOD-TAMP:&#22810;&#27493;&#39588;&#25805;&#32437;&#35268;&#21010;&#20013;&#30340;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;
&lt;/p&gt;
&lt;p&gt;
NOD-TAMP: Multi-Step Manipulation Planning with Neural Object Descriptors. (arXiv:2311.01530v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01530
&lt;/p&gt;
&lt;p&gt;
NOD-TAMP&#26159;&#19968;&#20010;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#26469;&#35299;&#20915;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#36712;&#36857;&#24182;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23478;&#23621;&#21644;&#24037;&#21378;&#29615;&#22659;&#20013;&#24320;&#21457;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38271;&#26102;&#31243;&#20219;&#21153;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#32437;&#20197;&#21450;&#38656;&#35201;&#22312;&#21508;&#31181;&#29289;&#20307;&#24418;&#29366;&#21644;&#22330;&#26223;&#24067;&#23616;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#34429;&#28982;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#30340;&#20551;&#35774;&#65292;&#22914;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#26032;&#39062;&#32972;&#26223;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#65288;NODs&#65289;&#22312;&#29289;&#20307;&#21644;&#22330;&#26223;&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#24191;&#27867;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;NOD-TAMP&#20174;&#23569;&#25968;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#30701;&#30340;&#25805;&#32437;&#36712;&#36857;&#65292;&#20351;&#29992;NOD&#29305;&#24449;&#26469;&#35843;&#25972;&#36825;&#20123;&#36712;&#36857;&#65292;&#24182;&#32452;&#21512;&#23427;&#20204;&#26469;&#35299;&#20915;&#24191;&#27867;&#30340;&#38271;&#26102;&#31243;&#20219;&#21153;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#39564;&#35777;&#21518;&#65292;NOD-TAMP&#26377;&#25928;&#24212;&#23545;&#21508;&#31181;&#25361;&#25112;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#25805;&#32437;&#35268;&#21010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing intelligent robots for complex manipulation tasks in household and factory settings remains challenging due to long-horizon tasks, contact-rich manipulation, and the need to generalize across a wide variety of object shapes and scene layouts. While Task and Motion Planning (TAMP) offers a promising solution, its assumptions such as kinodynamic models limit applicability in novel contexts. Neural object descriptors (NODs) have shown promise in object and scene generalization but face limitations in addressing broader tasks. Our proposed TAMP-based framework, NOD-TAMP, extracts short manipulation trajectories from a handful of human demonstrations, adapts these trajectories using NOD features, and composes them to solve broad long-horizon tasks. Validated in a simulation environment, NOD-TAMP effectively tackles varied challenges and outperforms existing methods, establishing a cohesive framework for manipulation planning. For videos and other supplemental material, see the pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#20248;&#21270;&#30340;&#36923;&#36753;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.19806</link><description>&lt;p&gt;
&#39640;&#32423;&#36923;&#36753;&#31243;&#24207;&#31561;&#20215;&#24615;&#23646;&#24615;&#30340;&#33258;&#21160;&#39564;&#35777;-&#23398;&#22763;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Automated Verification of Equivalence Properties in Advanced Logic Programs -- Bachelor Thesis. (arXiv:2310.19806v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19806
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#20248;&#21270;&#30340;&#36923;&#36753;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20351;&#29992;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#24037;&#19994;&#24212;&#29992;&#22686;&#21152;&#65292;&#23545;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#23545;&#20851;&#38190;&#24212;&#29992;&#30340;&#38656;&#27714;&#20063;&#22686;&#21152;&#20102;&#12290;&#22312;&#31243;&#24207;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#24076;&#26395;&#26377;&#19968;&#31181;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#20248;&#21270;&#30340;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#12290;&#20174;&#24418;&#24335;&#19978;&#35762;&#65292;&#36825;&#23545;&#24212;&#20110;&#39564;&#35777;&#20004;&#20010;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#24320;&#21457;&#20102;&#32763;&#35793;&#24037;&#20855;anthem&#12290;&#23427;&#21487;&#20197;&#19982;&#29992;&#20110;&#32463;&#20856;&#36923;&#36753;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#19968;&#36215;&#20351;&#29992;&#65292;&#20197;&#39564;&#35777;&#20004;&#20010;&#31243;&#24207;&#26159;&#21542;&#24378;&#31561;&#20215;&#12290;&#22312;&#24403;&#21069;&#29256;&#26412;&#30340;anthem&#20013;&#65292;&#21482;&#33021;&#39564;&#35777;&#20855;&#26377;&#21463;&#38480;&#36755;&#20837;&#35821;&#35328;&#30340;&#27491;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#12290;&#36825;&#26159;anthem&#20013;&#23454;&#29616;&#30340;&#32763;&#35793;&#964;*&#30340;&#32467;&#26524;&#65292;&#23427;&#29983;&#25104;&#20102;here-and-there&#36923;&#36753;&#20013;&#30340;&#20844;&#24335;&#65292;&#35813;&#36923;&#36753;&#21482;&#23545;&#27491;&#31243;&#24207;&#19982;&#32463;&#20856;&#36923;&#36753;&#30456;&#19968;&#33268;&#12290;&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;anthem&#65292;&#20197;&#20415;&#21487;&#20197;&#39564;&#35777;&#26356;&#24191;&#27867;&#30340;&#39640;&#32423;&#36923;&#36753;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increase in industrial applications using Answer Set Programming, the need for formal verification tools, particularly for critical applications, has also increased. During the program optimisation process, it would be desirable to have a tool which can automatically verify whether an optimised subprogram can replace the original subprogram. Formally this corresponds to the problem of verifying the strong equivalence of two programs. In order to do so, the translation tool anthem was developed. It can be used in conjunction with an automated theorem prover for classical logic to verify that two programs are strongly equivalent. With the current version of anthem, only the strong equivalence of positive programs with a restricted input language can be verified. This is a result of the translation $\tau^*$ implemented in anthem that produces formulas in the logic of here-and-there, which coincides with classical logic only for positive programs. This thesis extends anthem in ord
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#38598;&#19982;&#35757;&#32451;&#38598;&#20851;&#20110;&#23646;&#24615;&#24378;&#24230;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17261</link><description>&lt;p&gt;
&#22522;&#20110;&#23646;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Attribute Based Interpretable Evaluation Metrics for Generative Models. (arXiv:2310.17261v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#38598;&#19982;&#35757;&#32451;&#38598;&#20851;&#20110;&#23646;&#24615;&#24378;&#24230;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#29399;&#21644;&#29483;&#30340;&#27604;&#20363;&#20026;1:1&#26102;&#65292;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#29399;&#21644;&#29483;&#20063;&#24212;&#26356;&#22909;&#22320;&#31526;&#21512;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#21482;&#25552;&#20379;&#20102;&#8220;&#22810;&#26679;&#24615;&#8221;&#36825;&#20010;&#35299;&#37322;&#24615;&#20043;&#22806;&#30340;&#32500;&#24230;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#38598;&#19982;&#35757;&#32451;&#38598;&#20851;&#20110;&#23646;&#24615;&#24378;&#24230;&#20998;&#24067;&#30340;&#24046;&#24322;&#26469;&#25429;&#25417;&#36825;&#31181;&#29616;&#35937;&#12290;&#21333;&#23646;&#24615;&#24046;&#24322;&#65288;SaD&#65289;&#34913;&#37327;&#20102;&#20851;&#20110;&#21333;&#20010;&#23646;&#24615;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#24046;&#24322;&#12290;&#21452;&#23646;&#24615;&#24046;&#24322;&#65288;PaD&#65289;&#34913;&#37327;&#20102;&#20851;&#20110;&#19968;&#23545;&#23646;&#24615;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#24046;&#24322;&#12290;&#23427;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#23646;&#24615;&#12290;&#20026;&#20102;&#34913;&#37327;&#22270;&#20687;&#30340;&#23646;&#24615;&#24378;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24322;&#26500;CLIP&#35780;&#20998;&#65288;HCS&#65289;&#65292;&#23427;&#36890;&#36807;&#27979;&#37327;&#22270;&#20687;&#21644;&#25991;&#26412;&#21521;&#37327;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond "diversity". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the distribution of attribute strengths as follows. Single-attribute Divergence (SaD) measures the divergence regarding PDFs of a single attribute. Paired-attribute Divergence (PaD) measures the divergence regarding joint PDFs of a pair of attributes. They provide which attributes the models struggle. For measuring the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures the cosine similarity between image and text vectors with heterogeneous 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#65292;&#24182;&#21033;&#29992;WGAN-GP&#21644;VAE&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16867</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation. (arXiv:2310.16867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#65292;&#24182;&#21033;&#29992;WGAN-GP&#21644;VAE&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#33041;&#37096;&#35760;&#24405;&#65292;&#36827;&#34892;&#31934;&#31070;&#20998;&#35010;&#30151;&#30340;&#33258;&#21160;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#33021;&#22815;&#21033;&#29992;&#26102;&#39057;&#29305;&#24449;&#65292;&#20174;&#21407;&#22987;&#20449;&#21495;&#20013;&#25552;&#21462;&#20102;&#39057;&#35889;&#22270;&#12290;&#22312;&#25506;&#32034;&#20102;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#35774;&#32622;&#21518;&#65292;&#36873;&#25321;&#20102;&#36866;&#21512;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;Wasserstein GAN with Gradient Penalty(WGAN-GP)&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#29983;&#25104;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#22686;&#24378;&#21021;&#22987;&#25968;&#25454;&#38598;&#24182;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20351;&#29992;VAE&#29983;&#25104;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#22312;&#20934;&#30830;&#24230;&#19978;&#25552;&#39640;&#20102;3.0&#65285;&#65292;&#36798;&#21040;&#20102;99.0&#65285;&#65292;&#21516;&#26102;&#24471;&#21040;&#20102;&#26356;&#20302;&#30340;&#25439;&#22833;&#20540;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;(LIME)&#26041;&#27861;&#35299;&#20915;&#20102;&#23545;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#20449;&#20219;&#32570;&#20047;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we leverage a deep learning-based method for the automatic diagnosis of schizophrenia using EEG brain recordings. This approach utilizes generative data augmentation, a powerful technique that enhances the accuracy of the diagnosis. To enable the utilization of time-frequency features, spectrograms were extracted from the raw signals. After exploring several neural network architectural setups, a proper convolutional neural network (CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two different synthetic datasets were generated in order to augment the initial dataset and address the over-fitting issue. The augmented dataset using VAE achieved a 3.0\% improvement in accuracy reaching up to 99.0\% and yielded a lower loss value as well as a faster convergence. Finally, we addressed the lack of trust in black-box models using the Local Interpretable Model-agnostic Explanations (LI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.05506</link><description>&lt;p&gt;
&#26597;&#35810;&#21644;&#24212;&#31572;&#22686;&#24378;&#19981;&#33021;&#24110;&#21161;&#39046;&#22495;&#22806;&#25968;&#23398;&#25512;&#29702;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization. (arXiv:2310.05506v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#26102;&#65292;&#36890;&#36807;&#26597;&#35810;&#28436;&#21270;&#21644;&#22810;&#26679;&#21270;&#25512;&#29702;&#36335;&#24452;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#32463;&#39564;&#19978;&#34987;&#39564;&#35777;&#20026;&#26377;&#25928;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#24320;&#28304;LLMs&#21644;&#39030;&#23574;&#19987;&#26377;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#26088;&#22312;&#22238;&#31572;&#65306;&#65288;1&#65289;&#21738;&#20123;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26356;&#26377;&#25928;&#65307;&#65288;2&#65289;&#22686;&#24378;&#25968;&#25454;&#37327;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32553;&#25918;&#20851;&#31995;&#22914;&#20309;&#65307;&#65288;3&#65289;&#25968;&#25454;&#22686;&#24378;&#33021;&#21542;&#28608;&#21169;&#39046;&#22495;&#22806;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#30340;&#27867;&#21270;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;GSM8K&#26597;&#35810;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#20197;&#21450;&#37319;&#26679;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;AugGSM8K&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;AugGSM8K&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;LLMs&#65292;&#31216;&#20026;MuggleMath&#12290;MuggleMath&#22312;GSM8K&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65288;&#22312;7B&#35268;&#27169;&#19978;&#20174;54%&#25552;&#39640;&#21040;68.4%&#65292;&#22312;&#25193;&#25918;&#21040;63.9%&#21040;74.0%&#20043;&#38388;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K, by complicating and diversifying the queries from GSM8K and sampling multiple reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the scal
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23616;&#23454;&#38469;&#25490;&#21517;&#26041;&#27861;&#26469;&#20998;&#25674;&#23454;&#38469;&#31243;&#24207;&#21512;&#25104;&#20013;&#35745;&#31639;&#36127;&#25285;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20351;&#29992;&#21333;&#20010;&#28436;&#31034;&#30340;&#23454;&#38469;&#21512;&#25104;&#22120;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#20840;&#23616;&#25490;&#21517;&#26377;&#25928;&#36817;&#20284;&#20102;&#20840;&#20307;&#21512;&#29702;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.03225</link><description>&lt;p&gt;
&#20351;&#29992;&#25490;&#21517;&#26469;&#25674;&#38144;&#23454;&#38469;&#31243;&#24207;&#21512;&#25104;&#30340;&#25104;&#26412;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Amortizing Pragmatic Program Synthesis with Rankings. (arXiv:2309.03225v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03225
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23616;&#23454;&#38469;&#25490;&#21517;&#26041;&#27861;&#26469;&#20998;&#25674;&#23454;&#38469;&#31243;&#24207;&#21512;&#25104;&#20013;&#35745;&#31639;&#36127;&#25285;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20351;&#29992;&#21333;&#20010;&#28436;&#31034;&#30340;&#23454;&#38469;&#21512;&#25104;&#22120;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#20840;&#23616;&#25490;&#21517;&#26377;&#25928;&#36817;&#20284;&#20102;&#20840;&#20307;&#21512;&#29702;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31243;&#24207;&#21512;&#25104;&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#31995;&#32479;&#25509;&#25910;&#19968;&#32452;&#29992;&#25143;&#29983;&#25104;&#30340;&#31034;&#20363;&#65292;&#24182;&#36820;&#22238;&#19968;&#20010;&#36923;&#36753;&#19968;&#33268;&#30340;&#31243;&#24207;&#12290;&#20351;&#29992;&#21512;&#29702;&#28436;&#35828;&#34892;&#20026;&#65288;RSA&#65289;&#26694;&#26550;&#22312;&#26500;&#24314;&#8220;&#23454;&#38469;&#8221;&#31243;&#24207;&#21512;&#25104;&#22120;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36825;&#20123;&#21512;&#25104;&#22120;&#36820;&#22238;&#30340;&#31243;&#24207;&#19981;&#20165;&#22312;&#36923;&#36753;&#19978;&#19968;&#33268;&#65292;&#32780;&#19988;&#36824;&#32771;&#34385;&#20102;&#29992;&#25143;&#22914;&#20309;&#36873;&#25321;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#36816;&#34892;RSA&#31639;&#27861;&#30340;&#35745;&#31639;&#36127;&#25285;&#38480;&#21046;&#20102;&#23454;&#38469;&#31243;&#24207;&#21512;&#25104;&#22312;&#21487;&#33021;&#31243;&#24207;&#20010;&#25968;&#36739;&#23569;&#30340;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#8220;&#20840;&#23616;&#23454;&#38469;&#25490;&#21517;&#8221; - &#19968;&#20010;&#21333;&#19968;&#30340;&#12289;&#24635;&#30340;&#25490;&#21015;&#25152;&#26377;&#20551;&#35774;&#30340;&#26041;&#27861;&#26469;&#20998;&#25674;RSA&#31639;&#27861;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#21333;&#20010;&#28436;&#31034;&#30340;&#23454;&#38469;&#21512;&#25104;&#22120;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#23616;&#25490;&#21517;&#26041;&#27861;&#23436;&#20840;&#22797;&#21046;&#20102;RSA&#30340;&#25490;&#24207;&#21709;&#24212;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#26174;&#31034;&#65292;&#20840;&#23616;&#25490;&#21517;&#26377;&#25928;&#22320;&#36817;&#20284;&#20102;&#20840;&#20307;&#21512;&#29702;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In program synthesis, an intelligent system takes in a set of user-generated examples and returns a program that is logically consistent with these examples. The usage of Rational Speech Acts (RSA) framework has been successful in building \emph{pragmatic} program synthesizers that return programs which -in addition to being logically consistent -- account for the fact that a user chooses their examples informatively. However, the computational burden of running the RSA algorithm has restricted the application of pragmatic program synthesis to domains with a small number of possible programs. This work presents a novel method of amortizing the RSA algorithm by leveraging a \emph{global pragmatic ranking} -- a single, total ordering of all the hypotheses. We prove that for a pragmatic synthesizer that uses a single demonstration, our global ranking method exactly replicates RSA's ranked responses. We further empirically show that global rankings effectively approximate the full pragma
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05731</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review. (arXiv:2308.05731v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05731
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#20010;&#20154;&#12289;&#20844;&#20849;&#21644;&#36135;&#36816;&#20132;&#36890;&#30340;&#26041;&#24335;&#12290;&#38500;&#20102;&#24863;&#30693;&#29615;&#22659;&#30340;&#24040;&#22823;&#25361;&#25112;&#22806;&#65292;&#21363;&#20934;&#30830;&#22320;&#20351;&#29992;&#21487;&#29992;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#24863;&#30693;&#29615;&#22659;&#65292;&#33258;&#21160;&#39550;&#39542;&#36824;&#21253;&#25324;&#35268;&#21010;&#19968;&#20010;&#23433;&#20840;&#12289;&#33298;&#36866;&#21644;&#39640;&#25928;&#30340;&#36816;&#21160;&#36712;&#36857;&#12290;&#20026;&#20102;&#20419;&#36827;&#23433;&#20840;&#21644;&#36827;&#27493;&#65292;&#35768;&#22810;&#24037;&#20316;&#20381;&#36182;&#20110;&#27169;&#22359;&#21270;&#30340;&#20132;&#36890;&#26410;&#26469;&#36816;&#21160;&#30340;&#39044;&#27979;&#12290;&#27169;&#22359;&#21270;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#36890;&#24120;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#20316;&#20026;&#39034;&#24207;&#30340;&#29420;&#31435;&#20219;&#21153;&#22788;&#29702;&#12290;&#34429;&#28982;&#36825;&#32771;&#34385;&#20102;&#21608;&#22260;&#20132;&#36890;&#23545;&#33258;&#36710;&#30340;&#24433;&#21709;&#65292;&#20294;&#23427;&#26410;&#33021;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#23545;&#33258;&#36710;&#34892;&#20026;&#30340;&#21453;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#25972;&#21512;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26159;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#33298;&#36866;&#39550;&#39542;&#25152;&#24517;&#38656;&#30340;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#27169;&#22411;&#23454;&#29616;&#20102;&#36825;&#31181;&#38598;&#25104;&#31995;&#32479;&#65292;&#20294;&#23545;&#19981;&#21516;&#21407;&#29702;&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated driving has the potential to revolutionize personal, public, and freight mobility. Besides the enormous challenge of perception, i.e. accurately perceiving the environment using available sensor data, automated driving comprises planning a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential separate tasks. While this accounts for the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior. Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. While various models implement such integrated systems, a comprehensive overview and theoretical understanding of different principles are lacking.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.03415</link><description>&lt;p&gt;
&#20302;&#24310;&#36831;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#31471;&#21040;&#31471;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
End-to-End Evaluation for Low-Latency Simultaneous Speech Translation. (arXiv:2308.03415v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#25361;&#25112;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#35768;&#22810;&#20986;&#29256;&#29289;&#21644;&#20849;&#20139;&#20219;&#21153;&#20063;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#35780;&#20272;&#36825;&#20123;&#19981;&#21516;&#30340;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#31995;&#32479;&#30340;&#29305;&#23450;&#26041;&#38754;&#34987;&#35780;&#20272;&#65292;&#24182;&#19988;&#24448;&#24448;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#25191;&#34892;&#21644;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#21508;&#20010;&#26041;&#38754;&#30340;&#26694;&#26550;&#12290;&#35780;&#20272;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#30340;&#65292;&#21253;&#25324;&#38899;&#39057;&#30340;&#20998;&#27573;&#20197;&#21450;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26694;&#26550;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20855;&#26377;&#20462;&#35746;&#36755;&#20986;&#36873;&#39033;&#30340;&#27169;&#22411;&#20197;&#21450;&#20855;&#26377;&#22266;&#23450;&#36755;&#20986;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30452;&#25509;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#32423;&#32852;&#31995;&#32479;&#21644;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#20010;&#32479;&#19968;&#30340;&#24230;&#37327;&#26469;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches.  In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components.  Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework all
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#30340;&#26032;&#27010;&#24565;&#27169;&#22411;DNA&#65292;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#20316;&#20026;&#32039;&#20945;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#26469;&#30830;&#23450;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2308.02121</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;DNA&#30340;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Model Provenance via Model DNA. (arXiv:2308.02121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#30340;&#26032;&#27010;&#24565;&#27169;&#22411;DNA&#65292;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#20316;&#20026;&#32039;&#20945;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#26469;&#30830;&#23450;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#29983;&#21629;&#21608;&#26399;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#65288;&#20363;&#22914;&#65292;&#20102;&#35299;&#27169;&#22411;&#30340;&#26469;&#28304;&#65292;&#35757;&#32451;&#26041;&#24335;&#20197;&#21450;&#20351;&#29992;&#26041;&#24335;&#65289;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#36825;&#19968;&#39046;&#22495;&#20869;&#30340;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#65288;MP&#65289;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#30446;&#26631;&#27169;&#22411;&#19982;&#20854;&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#19968;&#20010;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#30693;&#35782;&#20135;&#26435;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22810;&#20851;&#27880;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#21363;&#27169;&#22411;DNA&#65292;&#23427;&#20195;&#34920;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#32534;&#30721;&#20026;&#27169;&#22411;&#30340;&#32039;&#20945;&#19988;&#20840;&#38754;&#30340;&#34920;&#31034;&#65288;&#21363;DNA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the life cycle of the machine learning (ML) model is an intriguing area of research (e.g., understanding where the model comes from, how it is trained, and how it is used). This paper focuses on a novel problem within this field, namely Model Provenance (MP), which concerns the relationship between a target model and its pre-training model and aims to determine whether a source model serves as the provenance for a target model. This is an important problem that has significant implications for ensuring the security and intellectual property of machine learning models but has not received much attention in the literature. To fill in this gap, we introduce a novel concept of Model DNA which represents the unique characteristics of a machine learning model. We utilize a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this 
&lt;/p&gt;</description></item><item><title>DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09124</link><description>&lt;p&gt;
DIFFender&#65306;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#29992;&#20110;&#25269;&#24481;Patch&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks. (arXiv:2306.09124v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09124
&lt;/p&gt;
&lt;p&gt;
DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;Patch&#25915;&#20987;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#24320;&#21457;&#21487;&#38752;&#30340;&#38450;&#24481;&#26041;&#27861;&#20197;&#25269;&#24481;Patch&#25915;&#20987;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24403;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIFFender&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#12290;DIFFender&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;Patch&#23450;&#20301;&#21644;Patch&#24674;&#22797;&#12290;&#22312;&#23450;&#20301;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#36259;&#29305;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#23545;&#25239;&#24615;Patch&#30340;&#20301;&#32622;&#12290;&#22312;&#24674;&#22797;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#22270;&#20687;&#20013;&#30340;&#23545;&#25239;&#24615;&#21306;&#22495;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20004;&#20010;&#38454;&#27573;&#37117;&#21463;&#21040;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#31934;&#24515;&#24341;&#23548;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#32039;&#23494;&#30456;&#20114;&#20316;&#29992;&#26469;&#25552;&#39640;&#25972;&#20010;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks, particularly patch attacks, pose significant threats to the robustness and reliability of deep learning models. Developing reliable defenses against patch attacks is crucial for real-world applications, yet current research in this area is not satisfactory. In this paper, we propose DIFFender, a novel defense method that leverages a text-guided diffusion model to defend against adversarial patches. DIFFender includes two main stages: patch localization and patch restoration. In the localization stage, we find and exploit an intriguing property of the diffusion model to effectively identify the locations of adversarial patches. In the restoration stage, we employ the diffusion model to reconstruct the adversarial regions in the images while preserving the integrity of the visual content. Importantly, these two stages are carefully guided by a unified diffusion model, thus we can utilize the close interaction between them to improve the whole defense performance. Mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PLAYBEST&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20174;&#31726;&#29699;&#27604;&#36187;&#21382;&#21490;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#65292;&#29983;&#25104;&#26356;&#21152;&#30495;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#29699;&#21592;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#29699;&#21592;&#30340;&#20915;&#31574;&#21046;&#23450;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.04090</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#35268;&#21010;&#36827;&#34892;&#32844;&#19994;&#31726;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Professional Basketball Player Behavior Synthesis via Planning with Diffusion. (arXiv:2306.04090v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PLAYBEST&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20174;&#31726;&#29699;&#27604;&#36187;&#21382;&#21490;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#65292;&#29983;&#25104;&#26356;&#21152;&#30495;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#29699;&#21592;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#29699;&#21592;&#30340;&#20915;&#31574;&#21046;&#23450;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#21160;&#24577;&#35268;&#21010;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#25913;&#21892;&#21508;&#31181;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#32844;&#19994;&#31726;&#29699;&#20316;&#20026;&#19968;&#20010;&#21253;&#21547;&#38544;&#34109;&#24615;&#25112;&#30053;&#31574;&#30053;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#21160;&#24577;&#26102;&#31354;&#21338;&#24328;&#30340;&#24341;&#20154;&#27880;&#30446;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#22810;&#26679;&#30340;&#22330;&#19978;&#20449;&#21495;&#21644;&#23548;&#33322;&#28508;&#22312;&#21160;&#20316;&#21644;&#32467;&#26524;&#30340;&#24191;&#38420;&#31354;&#38388;&#20351;&#24471;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#36805;&#36895;&#35782;&#21035;&#21709;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#23450;&#20041;&#20026;&#26465;&#20214;&#36712;&#36857;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;PLAYBEST&#65288;PLAYER BEhavior SynThesis&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#39640;&#29699;&#21592;&#20915;&#31574;&#21046;&#23450;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#20197;&#20174;&#21382;&#21490;&#30340;&#32654;&#22269;&#32844;&#19994;&#31726;&#29699;&#32852;&#36187;(NBA)&#29699;&#21592;&#36816;&#21160;&#36319;&#36394;&#25968;&#25454;&#20013;&#23398;&#20064;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#21160;&#24577;&#12290;&#20026;&#20102;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#21464;&#37327;&#21040;PLAYBEST&#20013;&#65292;&#20197;&#36866;&#24212;&#22806;&#37096;&#36755;&#20837;&#24182;&#29983;&#25104;&#30495;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#29699;&#21592;&#36712;&#36857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PLAYBEST&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29699;&#21592;&#34892;&#20026;&#65292;&#24182;&#22312;&#21508;&#31181;&#35780;&#20272;&#22330;&#26223;&#20013;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamically planning in multi-agent systems has been explored to improve decision-making in various domains. Professional basketball serves as a compelling example of a dynamic spatio-temporal game, encompassing both concealed strategic policies and decision-making. However, processing the diverse on-court signals and navigating the vast space of potential actions and outcomes makes it difficult for existing approaches to swiftly identify optimal strategies in response to evolving circumstances. In this study, we first formulate the sequential decision-making process as a conditional trajectory generation process. We further introduce PLAYBEST (PLAYer BEhavior SynThesis), a method for enhancing player decision-making. We extend the state-of-the-art generative model, diffusion probabilistic model, to learn challenging multi-agent environmental dynamics from historical National Basketball Association (NBA) player motion tracking data. To incorporate data-driven strategies, an auxiliary v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24615;&#20559;&#24046;&#65292;&#30528;&#30524;&#20110;&#20854;&#20013;&#28041;&#21450;&#30340;&#20613;&#37324;&#21494;&#39057;&#29575;&#19982;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20123;&#39057;&#29575;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15203</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#22312;&#32500;&#24230;&#23558;&#38544;&#24615;&#20559;&#35265;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30456;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Relating Implicit Bias and Adversarial Attacks through Intrinsic Dimension. (arXiv:2305.15203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24615;&#20559;&#24046;&#65292;&#30528;&#30524;&#20110;&#20854;&#20013;&#28041;&#21450;&#30340;&#20613;&#37324;&#21494;&#39057;&#29575;&#19982;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20123;&#39057;&#29575;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#23427;&#20204;&#26131;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#26159;&#38024;&#23545;&#27169;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#30340;&#23567;&#24178;&#25200;&#65292;&#26088;&#22312;&#27450;&#39575;&#27169;&#22411;&#12290;&#33258;&#28982;&#32780;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#35774;&#32622;&#25110;&#23646;&#24615;&#19982;&#25915;&#20987;&#30340;&#24615;&#36136;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#32852;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24615;&#20559;&#24046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#20854;&#22266;&#26377;&#20542;&#21521;&#20110;&#25903;&#25345;&#29305;&#23450;&#27169;&#24335;&#25110;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38544;&#24615;&#20559;&#24046;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#20854;&#20013;&#21253;&#25324;&#36827;&#34892;&#20934;&#30830;&#22270;&#20687;&#20998;&#31867;&#25152;&#38656;&#30340;&#22522;&#26412;&#20613;&#37324;&#21494;&#39057;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#27979;&#35797;&#20197;&#35780;&#20272;&#36825;&#20123;&#39057;&#29575;&#19982;&#25104;&#21151;&#25915;&#20987;&#25152;&#38656;&#30340;&#39057;&#29575;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#12290;&#20026;&#20102;&#28145;&#20837;&#25506;&#35752;&#36825;&#31181;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25581;&#31034;&#22352;&#26631;&#38598;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#22352;&#26631;&#38598;&#23601;&#26159;&#21069;&#36848;&#30340;&#20613;&#37324;&#21494;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance in classification, neural networks are known to be vulnerable to adversarial attacks. These attacks are small perturbations of the input data designed to fool the model. Naturally, a question arises regarding the potential connection between the architecture, settings, or properties of the model and the nature of the attack. In this work, we aim to shed light on this problem by focusing on the implicit bias of the neural network, which refers to its inherent inclination to favor specific patterns or outcomes. Specifically, we investigate one aspect of the implicit bias, which involves the essential Fourier frequencies required for accurate image classification. We conduct tests to assess the statistical relationship between these frequencies and those necessary for a successful attack. To delve into this relationship, we propose a new method that can uncover non-linear correlations between sets of coordinates, which, in our case, are the aforementio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13062</link><description>&lt;p&gt;
GPT4Table&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#21527;&#65311;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#25512;&#29702;&#22120;&#26469;&#35299;&#20915;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#36234;&#26469;&#36234;&#20855;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#30340;&#29702;&#35299;&#31243;&#24230;&#36824;&#26377;&#24456;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#22320;&#26041;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#34920;&#26684;&#24207;&#21015;&#21270;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;LLMs&#26159;&#21542;&#30495;&#27491;&#33021;&#22815;&#29702;&#35299;&#36825;&#31867;&#25968;&#25454;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;LLMs&#30340;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65288;SUC&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19971;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#21333;&#20803;&#26684;&#26597;&#25214;&#12289;&#34892;&#26816;&#32034;&#21644;&#22823;&#23567;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#24615;&#33021;&#22240;&#22810;&#31181;&#36755;&#20837;&#36873;&#25321;&#32780;&#24322;&#65292;&#21253;&#25324;&#34920;&#26684;&#36755;&#20837;&#26684;&#24335;&#12289;&#20869;&#23481;&#39034;&#24207;&#12289;&#35282;&#33394;&#25552;&#31034;&#21644;&#20998;&#21306;&#26631;&#35760;&#31561;&#12290;&#26681;&#25454;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#25152;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#26412;&#36136;&#65292;&#21363;&#26469;&#33258;&#20110;&#25945;&#24072;&#27169;&#22411;&#30340;top-1&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;&#22522;&#20110;&#35789;&#32423;&#21035;&#30340;&#30693;&#35782;&#33976;&#39311;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;Top-1 Information&#12290;</title><link>http://arxiv.org/abs/2305.08096</link><description>&lt;p&gt;
&#25506;&#31350;&#21644;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation. (arXiv:2305.08096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#26412;&#36136;&#65292;&#21363;&#26469;&#33258;&#20110;&#25945;&#24072;&#27169;&#22411;&#30340;top-1&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;&#22522;&#20110;&#35789;&#32423;&#21035;&#30340;&#30693;&#35782;&#33976;&#39311;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;Top-1 Information&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#20013;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22312;&#21738;&#37324;&#38544;&#34255;&#30340;&#38382;&#39064;&#20173;&#19981;&#28165;&#26970;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#30693;&#35782;&#33976;&#39311;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#23454;&#35777;&#35282;&#24230;&#25581;&#24320;&#20102;&#36825;&#20010;&#35868;&#22242;&#65292;&#24182;&#23637;&#31034;&#20102;&#30693;&#35782;&#26469;&#33258;&#25945;&#24072;&#30340;top-1&#39044;&#27979;&#65292;&#36825;&#20063;&#24110;&#21161;&#25105;&#20204;&#24314;&#31435;&#20102;&#35789;&#32423;&#21644;&#24207;&#21015;&#32423;&#33976;&#39311;&#20043;&#38388;&#30340;&#28508;&#22312;&#36830;&#25509;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#25351;&#20986;&#20102;&#22522;&#30784;&#35789;&#32423;&#33976;&#39311;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#30693;&#35782;&#30340;&#24403;&#21069;&#30446;&#26631;&#26159;&#23558;&#27880;&#24847;&#21147;&#25193;&#25955;&#21040;&#25972;&#20010;&#20998;&#24067;&#19978;&#23398;&#20064;&#30693;&#35782;&#65292;&#20294;&#32570;&#20047;&#23545;&#26368;&#20851;&#38190;&#30340;top-1&#20449;&#24687;&#30340;&#29305;&#27530;&#22788;&#29702;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#25945;&#24072;&#30340;top-1&#39044;&#27979;&#19982;&#22320;&#38754;&#23454;&#20917;&#26631;&#35760;&#37325;&#21472;&#65292;&#22240;&#27492;&#30693;&#35782;&#34987;&#40644;&#37329;&#20449;&#24687;&#25152;&#21344;&#25454;&#65292;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#28508;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{T}op-1 \textbf{I}nformation&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is a promising technique for model compression in neural machine translation. However, where the knowledge hides in KD is still not clear, which may hinder the development of KD. In this work, we first unravel this mystery from an empirical perspective and show that the knowledge comes from the top-1 predictions of teachers, which also helps us build a potential connection between word- and sequence-level KD. Further, we point out two inherent issues in vanilla word-level KD based on this finding. Firstly, the current objective of KD spreads its focus to whole distributions to learn the knowledge, yet lacks special treatment on the most crucial top-1 information. Secondly, the knowledge is largely covered by the golden information due to the fact that most top-1 predictions of teachers overlap with ground-truth tokens, which further restricts the potential of KD. To address these issues, we propose a novel method named \textbf{T}op-1 \textbf{I}nformation \te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;RoConE&#65292;&#23427;&#20801;&#35768;&#23398;&#20064;&#20851;&#31995;&#27169;&#24335;&#24182;&#25552;&#39640;&#20102;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11858</link><description>&lt;p&gt;
&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#24212;&#31572;&#30340;&#20851;&#31995;&#27169;&#24335;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Relational Patterns for Logical Query Answering over Knowledge Graphs. (arXiv:2303.11858v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;RoConE&#65292;&#23427;&#20801;&#35768;&#23398;&#20064;&#20851;&#31995;&#27169;&#24335;&#24182;&#25552;&#39640;&#20102;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36827;&#34892;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#26597;&#35810;&#30340;&#22238;&#31572;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;KG&#19981;&#23436;&#25972;&#24615;&#32780;&#23548;&#33268;&#30340;&#12290;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#26597;&#35810;&#30340;&#20302;&#32500;&#24230;&#21521;&#37327;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;KG&#34920;&#29616;&#20986;&#23545;&#31216;&#24615;&#21644;&#32452;&#21512;&#24615;&#31561;&#20851;&#31995;&#27169;&#24335;&#65292;&#24314;&#27169;&#36825;&#20123;&#27169;&#24335;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26597;&#35810;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#24335;&#22312;&#26597;&#35810;&#23884;&#20837;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20801;&#35768;&#23398;&#20064;&#20851;&#31995;&#27169;&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#21152;&#24378;FOL&#26597;&#35810;&#25512;&#29702;&#30340;&#27169;&#24335;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;RoConE&#65292;&#23427;&#23558;&#26597;&#35810;&#21306;&#22495;&#23450;&#20041;&#20026;&#20960;&#20309;&#38181;&#20307;&#65292;&#24182;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#26059;&#36716;&#20195;&#25968;&#26597;&#35810;&#31639;&#23376;&#12290;RoConE&#32467;&#21512;&#20102;&#20960;&#20309;&#38181;&#20307;&#20316;&#20026;&#21487;&#20197;&#26126;&#30830;&#23450;&#20041;&#26597;&#35810;&#34920;&#31034;&#30340;&#20960;&#20309;&#34920;&#31034;&#21644;&#26059;&#36716;&#20195;&#25968;&#26597;&#35810;&#31639;&#23376;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering first-order logical (FOL) queries over knowledge graphs (KG) remains a challenging task mainly due to KG incompleteness. Query embedding approaches this problem by computing the low-dimensional vector representations of entities, relations, and logical queries. KGs exhibit relational patterns such as symmetry and composition and modeling the patterns can further enhance the performance of query embedding models. However, the role of such patterns in answering FOL queries by query embedding models has not been yet studied in the literature. In this paper, we fill in this research gap and empower FOL queries reasoning with pattern inference by introducing an inductive bias that allows for learning relation patterns. To this end, we develop a novel query embedding method, RoConE, that defines query regions as geometric cones and algebraic query operators by rotations in complex space. RoConE combines the advantages of Cone as a well-specified geometric representation for query e
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;</title><link>http://arxiv.org/abs/2302.13268</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#38761;&#26032;&#22522;&#22240;&#32452;&#23398;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Genomics with Reinforcement Learning Techniques. (arXiv:2302.13268v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13268
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#20986;&#29616;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20915;&#31574;&#21644;&#22522;&#22240;&#32452;&#23398;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#30340;&#21407;&#22987;&#22522;&#22240;&#32452;&#25968;&#25454;&#25351;&#25968;&#22686;&#38271;&#24050;&#32463;&#36229;&#20986;&#20102;&#25163;&#21160;&#20998;&#26512;&#30340;&#33021;&#21147;&#65292;&#36825;&#23548;&#33268;&#23545;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;RL&#31639;&#27861;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#20154;&#24037;&#30417;&#30563;&#19979;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#20351;&#29992;RL&#30340;&#19968;&#20010;&#20851;&#38190;&#22909;&#22788;&#26159;&#38477;&#20302;&#20102;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36825;&#26159;&#30417;&#30563;&#23398;&#20064;&#25152;&#38656;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#26412;&#35843;&#26597;&#20165;&#19987;&#27880;&#20110;&#22312;&#21508;&#31181;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#65288;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65292;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#65289;&#20013;&#20351;&#29992;RL&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30740;&#31350;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Reinforcement Learning (RL) has emerged as a powerful tool for solving a wide range of problems, including decision-making and genomics. The exponential growth of raw genomic data over the past two decades has exceeded the capacity of manual analysis, leading to a growing interest in automatic data analysis and processing. RL algorithms are capable of learning from experience with minimal human supervision, making them well-suited for genomic data analysis and interpretation. One of the key benefits of using RL is the reduced cost associated with collecting labeled training data, which is required for supervised learning. While there have been numerous studies examining the applications of Machine Learning (ML) in genomics, this survey focuses exclusively on the use of RL in various genomics research fields, including gene regulatory networks (GRNs), genome assembly, and sequence alignment. We present a comprehensive technical overview of existing studies on the applic
&lt;/p&gt;</description></item></channel></rss>