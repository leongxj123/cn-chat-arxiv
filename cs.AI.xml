<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22522;&#20110;Vision Transformer&#30340;DRCT&#26041;&#27861;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#31354;&#38388;&#20449;&#24687;&#34928;&#20943;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00722</link><description>&lt;p&gt;
DRCT&#65306;&#23558;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20445;&#23384;&#22312;&#20449;&#24687;&#29942;&#39048;&#20043;&#22806;
&lt;/p&gt;
&lt;p&gt;
DRCT: Saving Image Super-resolution away from Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00722
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Vision Transformer&#30340;DRCT&#26041;&#27861;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#31354;&#38388;&#20449;&#24687;&#34928;&#20943;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Vision Transformer&#30340;&#20302;&#23618;&#35270;&#35273;&#20219;&#21153;&#24212;&#29992;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#19982;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;Transformer&#26356;&#25797;&#38271;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#21033;&#29992;&#38750;&#23616;&#37096;&#21306;&#22495;&#30340;&#20449;&#24687;&#37325;&#24314;&#22270;&#20687;&#12290;&#22312;&#36229;&#20998;&#36776;&#29575;&#39046;&#22495;&#65292;&#22522;&#20110;Swin Transformer&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#19988;&#20855;&#26377;&#26059;&#36716;&#31383;&#21475;&#27880;&#24847;&#26426;&#21046;&#65292;&#26377;&#21161;&#20110;&#22312;&#19981;&#21516;&#31383;&#21475;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25193;&#22823;&#24863;&#30693;&#37326;&#25110;&#35774;&#35745;&#22797;&#26434;&#32593;&#32476;&#26469;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#21644;&#32593;&#32476;&#25928;&#29575;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#28145;&#24230;&#22686;&#21152;&#65292;&#31354;&#38388;&#20449;&#24687;&#24448;&#24448;&#20250;&#20943;&#23569;&#65292;&#20174;&#32780;&#23548;&#33268;&#31354;&#38388;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#24182;&#26368;&#32456;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00722v1 Announce Type: cross  Abstract: In recent years, Vision Transformer-based applications to low-level vision tasks have achieved widespread success. Unlike CNN-based models, Transformers are more adept at capturing long-range dependencies, enabling the reconstruction of images utilizing information from non-local areas. In the domain of super-resolution, Swin-transformer-based approaches have become mainstream due to their capacity to capture global spatial information and their shifting-window attention mechanism that facilitates the interchange of information between different windows. Many researchers have enhanced image quality and network efficiency by expanding the receptive field or designing complex networks, yielding commendable results. However, we observed that spatial information tends to diminish during the forward propagation process due to increased depth, leading to a loss of spatial information and, consequently, limiting the model's potential. To addr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13802</link><description>&lt;p&gt;
ZigMa&#65306;&#34623;&#34578;&#26364;&#24052;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZigMa: Zigzag Mamba Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#21487;&#20280;&#32553;&#24615;&#21644;&#20108;&#27425;&#22797;&#26434;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32467;&#26500;&#20869;&#37096;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;&#26364;&#24052;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#65292;&#20197;&#25193;&#23637;&#20854;&#22312;&#35270;&#35273;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22823;&#22810;&#25968;&#24403;&#21069;&#22522;&#20110;&#26364;&#24052;&#30340;&#35270;&#35273;&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30095;&#24573;&#65292;&#21363;&#26364;&#24052;&#30340;&#25195;&#25551;&#26041;&#26696;&#20013;&#32570;&#20047;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#32771;&#34385;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#31616;&#21333;&#12289;&#21363;&#25554;&#21363;&#29992;&#12289;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#23427;&#20248;&#20110;&#22522;&#20110;&#26364;&#24052;&#30340;&#22522;&#32447;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22522;&#32447;&#26356;&#24555;&#36895;&#21644;&#26356;&#22909;&#30340;&#20869;&#23384;&#21033;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;Zigzag Mamba&#38598;&#25104;&#21040;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#20013;&#65292;&#20197;&#30740;&#31350;&#27169;&#22411;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;FacesHQ $1024\times 1024$&#21644;UCF101&#65292;MultiModal-CelebA-HQ&#65289;&#19978;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13802v1 Announce Type: cross  Abstract: The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#24369;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36923;&#36753;&#36830;&#36143;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#65292;&#23454;&#39564;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#20986;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.11793</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65306;&#23545;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11793
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#24369;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36923;&#36753;&#36830;&#36143;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#65292;&#23454;&#39564;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#20986;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#33021;&#21147;&#30340;&#29616;&#26377;&#26041;&#27861;&#20197;&#32467;&#26524;&#20026;&#20013;&#24515;&#65292;&#20351;&#24471;&#35780;&#20272;&#25512;&#29702;&#36807;&#31243;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25968;&#25454;&#38598;&#20197;&#36807;&#31243;&#20026;&#20013;&#24515;&#30340;&#26041;&#24335;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;ARC&#35201;&#27714;&#35299;&#20915;&#38382;&#39064;&#26102;&#20855;&#26377;&#20005;&#35880;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#19968;&#20010;&#33021;&#22815;&#20419;&#36827;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#30340;&#22522;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#24369;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36923;&#36753;&#36830;&#36143;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31361;&#26174;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11793v1 Announce Type: cross  Abstract: The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been results-centric, making it difficult to assess the inference process. We introduce a new approach using the Abstract and Reasoning Corpus (ARC) dataset to evaluate the inference and contextual understanding abilities of large language models in a process-centric manner. ARC demands rigorous logical structures for problem-solving, making it a benchmark that facilitates the comparison of model inference abilities with humans. Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity. Our experiments highlight the reasoning capabilities of LLMs, proposing development paths for achieving human-level reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.10842</link><description>&lt;p&gt;
&#20351;&#29992;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#30340;&#21452;Transformer&#22312;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#65288;FDD&#65289;&#23545;&#20110;&#30830;&#20445;&#24037;&#19994;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FDD&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#65288;TEP&#65289;&#65292;&#36825;&#26159;&#21270;&#24037;&#36807;&#31243;&#25511;&#21046;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;Transformer&#20998;&#25903;&#65292;&#33021;&#22815;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#21462;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#65288;GDLAttention&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#38376;&#25511;&#26426;&#21046;&#21644;&#21160;&#24577;&#23398;&#20064;&#33021;&#21147;&#12290;&#38376;&#25511;&#26426;&#21046;&#35843;&#33410;&#27880;&#24847;&#26435;&#37325;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20851;&#27880;&#36755;&#20837;&#30340;&#26368;&#30456;&#20851;&#37096;&#20998;&#12290;&#21160;&#24577;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;&#27880;&#24847;&#26426;&#21046;&#20351;&#29992;&#21452;&#32447;&#24615;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#26469;&#25429;&#25417;&#26597;&#35810;&#21644;&#36755;&#20837;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10842v1 Announce Type: cross  Abstract: Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety and efficiency of industrial processes. We propose a novel FDD methodology for the Tennessee Eastman Process (TEP), a widely used benchmark for chemical process control. The model employs two separate Transformer branches, enabling independent processing of input data and potential extraction of diverse information. A novel attention mechanism, Gated Dynamic Learnable Attention (GDLAttention), is introduced which integrates a gating mechanism and dynamic learning capabilities. The gating mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input. The dynamic learning approach adapts the attention strategy during training, potentially leading to improved performance. The attention mechanism uses a bilinear similarity function, providing greater flexibility in capturing complex relationships between query and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#31934;&#24230;&#19979;&#36827;&#34892;&#35757;&#32451;&#12289;&#22312;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#36827;&#34892;&#22235;&#33293;&#20116;&#20837;&#65292;&#24182;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#65292;&#20197;&#24212;&#23545;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.09603</link><description>&lt;p&gt;
&#25511;&#21046;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#36827;&#34892;&#20048;&#35266;&#21487;&#39564;&#35777;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Optimistic Verifiable Training by Controlling Hardware Nondeterminism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#31934;&#24230;&#19979;&#36827;&#34892;&#35757;&#32451;&#12289;&#22312;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#36827;&#34892;&#22235;&#33293;&#20116;&#20837;&#65292;&#24182;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#65292;&#20197;&#24212;&#23545;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#26085;&#30410;&#22686;&#21152;&#30340;&#35745;&#31639;&#38656;&#27714;&#23548;&#33268;&#20102;&#20026;&#32570;&#20047;&#24517;&#35201;&#36164;&#28304;&#30340;&#23458;&#25143;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#26381;&#21153;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#35757;&#32451;&#30340;&#27491;&#30830;&#24615;&#24182;&#38450;&#33539;&#28508;&#22312;&#30340;&#35757;&#32451;&#26102;&#25915;&#20987;&#65292;&#20363;&#22914;&#25968;&#25454;&#27602;&#21270;&#65292;&#37117;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#21487;&#39564;&#35777;&#35757;&#32451;&#30340;&#24037;&#20316;&#20027;&#35201;&#20998;&#20026;&#20004;&#31867;&#65306;&#22522;&#20110;&#35777;&#26126;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#38656;&#35201;&#21152;&#23494;&#25216;&#26415;&#32780;&#38590;&#20197;&#25193;&#23637;&#65292;&#20197;&#21450;&#32771;&#34385;&#21040;&#19968;&#20010;&#21487;&#20449;&#31532;&#19977;&#26041;&#23457;&#35745;&#21592;&#22797;&#21046;&#35757;&#32451;&#36807;&#31243;&#30340;&#8220;&#20048;&#35266;&#8221;&#26041;&#27861;&#12290; &#21518;&#32773;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;GPU&#31867;&#22411;&#20043;&#38388;&#30340;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#38459;&#27490;&#23457;&#35745;&#21592;&#31934;&#30830;&#22797;&#21046;&#35757;&#32451;&#36807;&#31243;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#26041;&#26696;&#19981;&#22815;&#20581;&#22766;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#30340;&#31934;&#24230;&#19979;&#36827;&#34892;&#65292;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#22235;&#33293;&#20116;&#20837;&#65292;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09603v1 Announce Type: cross  Abstract: The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and "optimistic" methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RialTo&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.03949</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#35843;&#21644;&#29616;&#23454;&#65306;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#25805;&#20316;&#30340;&#23454;-&#27169;-&#23454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RialTo&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#30417;&#30563;&#26469;&#23398;&#20064;&#23545;&#29289;&#20307;&#23039;&#21183;&#21464;&#21270;&#12289;&#29289;&#29702;&#24178;&#25200;&#21644;&#35270;&#35273;&#25200;&#21160;&#40065;&#26834;&#30340;&#31574;&#30053;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#20197;&#23398;&#20064;&#31283;&#20581;&#34892;&#20026;&#65292;&#20294;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#36127;&#25285;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RialTo&#65292;&#19968;&#20010;&#36890;&#36807;&#22312;&#21363;&#23558;&#20174;&#23569;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#26500;&#24314;&#30340;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#30340;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#23454;-&#27169;-&#23454;&#27969;&#27700;&#32447;&#65292;RialTo&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#25509;&#21475;&#65292;&#29992;&#20110;&#24555;&#36895;&#25195;&#25551;&#21644;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#21453;&#21521;&#25552;&#28860;&#8221;&#36807;&#31243;&#65292;&#29992;&#20110;&#32473;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#24102;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03949v1 Announce Type: cross  Abstract: Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in "digital twin" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel "inverse distillation" procedure for bringing real-world demonstrations
&lt;/p&gt;</description></item><item><title>HanDiffuser&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#25163;&#37096;&#23884;&#20837;&#26469;&#23454;&#29616;&#36924;&#30495;&#30340;&#25163;&#37096;&#22806;&#35266;&#65292;&#21253;&#25324;Text-to-Hand-Params&#25193;&#25955;&#27169;&#22411;&#21644;Text-Guided Hand-Params-to-Image&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.01693</link><description>&lt;p&gt;
HanDiffuser: &#20855;&#26377;&#36924;&#30495;&#25163;&#37096;&#22806;&#35266;&#30340;&#25991;&#26412;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01693
&lt;/p&gt;
&lt;p&gt;
HanDiffuser&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#25163;&#37096;&#23884;&#20837;&#26469;&#23454;&#29616;&#36924;&#30495;&#30340;&#25163;&#37096;&#22806;&#35266;&#65292;&#21253;&#25324;Text-to-Hand-Params&#25193;&#25955;&#27169;&#22411;&#21644;Text-Guided Hand-Params-to-Image&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01693v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25991;&#25688;: &#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#24418;&#35937;&#65292;&#20294;&#22312;&#29983;&#25104;&#25163;&#37096;&#26102;&#20250;&#22833;&#21435;&#36924;&#30495;&#24230;&#12290;&#24120;&#35265;&#30340;&#32570;&#38519;&#21253;&#25324;&#19981;&#35268;&#21017;&#30340;&#25163;&#37096;&#23039;&#21183;&#12289;&#24418;&#29366;&#12289;&#38169;&#35823;&#30340;&#25163;&#25351;&#25968;&#37327;&#20197;&#21450;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#25163;&#25351;&#26041;&#21521;&#12290;&#20026;&#20102;&#29983;&#25104;&#20855;&#26377;&#36924;&#30495;&#25163;&#37096;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26032;&#39062;&#26550;&#26500;&#65292;&#31216;&#20026;HanDiffuser&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#25163;&#37096;&#23884;&#20837;&#26469;&#23454;&#29616;&#36924;&#30495;&#24230;&#12290;HanDiffuser&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;:Text-to-Hand-Params&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#36755;&#20837;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;SMPL-&#36523;&#20307;&#21644;MANO-&#25163;&#37096;&#21442;&#25968;&#65292;&#20197;&#21450;Text-Guided Hand-Params-to-Image&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19978;&#19968;&#37096;&#20214;&#29983;&#25104;&#30340;&#25552;&#31034;&#21644;&#25163;&#37096;&#21442;&#25968;&#19978;&#36827;&#34892;&#35843;&#33410;&#26469;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#21512;&#24182;&#20102;&#25163;&#37096;&#34920;&#31034;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;3D&#24418;&#29366;&#21644;&#20851;&#33410;&#32423;&#25163;&#25351;&#20301;&#32622;&#12289;&#26041;&#21521;&#21644;&#20851;&#33410;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#23398;&#20064;&#21644;&#21487;&#38752;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01693v1 Announce Type: cross  Abstract: Text-to-image generative models can generate high-quality humans, but realism is lost when generating hands. Common artifacts include irregular hand poses, shapes, incorrect numbers of fingers, and physically implausible finger orientations. To generate images with realistic hands, we propose a novel diffusion-based architecture called HanDiffuser that achieves realism by injecting hand embeddings in the generative process. HanDiffuser consists of two components: a Text-to-Hand-Params diffusion model to generate SMPL-Body and MANO-Hand parameters from input text prompts, and a Text-Guided Hand-Params-to-Image diffusion model to synthesize images by conditioning on the prompts and hand parameters generated by the previous component. We incorporate multiple aspects of hand representation, including 3D shapes and joint-level finger positions, orientations and articulations, for robust learning and reliable performance during inference. We
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#65292;&#38024;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14081</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#36816;&#21160;&#20195;&#30721;&#30340;&#38543;&#26426;&#36807;&#31243;&#27169;&#22411;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning of Noisy Time Series Collections Using Stochastic Process Models with Motion Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14081
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#65292;&#38024;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#38382;&#39064;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20855;&#26377;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#38271;&#24230;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24773;&#20917;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#23454;&#20363;&#21487;&#20197;&#30475;&#20316;&#26159;&#22024;&#26434;&#21160;&#24577;&#27169;&#22411;&#30340;&#19968;&#20010;&#26679;&#26412;&#23454;&#29616;&#65292;&#20854;&#29305;&#28857;&#26159;&#36830;&#32493;&#38543;&#26426;&#36807;&#31243;&#12290;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#65292;&#25968;&#25454;&#26159;&#28151;&#21512;&#30340;&#65292;&#30001;&#22810;&#20010;&#38543;&#26426;&#36807;&#31243;&#24314;&#27169;&#30340;&#20960;&#31181;&#31867;&#22411;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#24207;&#21015;&#32452;&#25104;&#65292;&#20351;&#24471;&#39044;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#19981;&#26159;&#31616;&#21333;&#22320;&#23558;&#25968;&#25454;&#22238;&#24402;&#21040;&#27599;&#31181;&#26102;&#38388;&#24207;&#21015;&#31867;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#20026;&#27599;&#31181;&#31867;&#22411;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#33258;&#21160;&#20998;&#37197;&#19968;&#20010;&#31216;&#20026;&#20854;&#36816;&#21160;&#20195;&#30721;&#30340;&#31614;&#21517;&#21521;&#37327;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#20998;&#37197;&#30340;&#36816;&#21160;&#20195;&#30721;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#30456;&#20851;&#24615;&#30340;&#31232;&#30095;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14081v1 Announce Type: cross  Abstract: While time series classification and forecasting problems have been extensively studied, the cases of noisy time series data with arbitrary time sequence lengths have remained challenging. Each time series instance can be thought of as a sample realization of a noisy dynamical model, which is characterized by a continuous stochastic process. For many applications, the data are mixed and consist of several types of noisy time series sequences modeled by multiple stochastic processes, making the forecasting and classification tasks even more challenging. Instead of regressing data naively and individually to each time series type, we take a latent variable model approach using a mixtured Gaussian processes with learned spectral kernels. More specifically, we auto-assign each type of noisy time series data a signature vector called its motion code. Then, conditioned on each assigned motion code, we infer a sparse approximation of the corr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#24182;&#32467;&#21512;&#29305;&#27530;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#65292;&#21487;&#20197;&#29983;&#25104;&#29289;&#29702;&#19990;&#30028;&#20013;&#20855;&#26377;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#30340;&#20266;&#35013;&#12290;</title><link>https://arxiv.org/abs/2402.13575</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#24322;&#26041;&#27861;&#30340;&#28789;&#27963;&#29289;&#29702;&#20266;&#35013;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Flexible Physical Camouflage Generation Based on a Differential Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13575
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#24182;&#32467;&#21512;&#29305;&#27530;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#65292;&#21487;&#20197;&#29983;&#25104;&#29289;&#29702;&#19990;&#30028;&#20013;&#20855;&#26377;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#30340;&#20266;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;&#23545;&#25239;&#20266;&#35013;&#65292;&#22312;&#24191;&#27867;&#30340;&#19977;&#32500;&#28210;&#26579;&#26694;&#26550;&#20869;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#24544;&#23454;&#22320;&#27169;&#25311;&#20809;&#29031;&#26465;&#20214;&#21644;&#26448;&#26009;&#21464;&#21270;&#65292;&#30830;&#20445;&#22312;&#19977;&#32500;&#30446;&#26631;&#19978;&#23545;&#32441;&#29702;&#36827;&#34892;&#24494;&#22937;&#32780;&#36924;&#30495;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#12290;&#36825;&#28041;&#21450;&#23558;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#30830;&#20445;&#20266;&#35013;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20266;&#35013;&#22312;&#36148;&#32440;&#27169;&#24335;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#35206;&#30422;&#30446;&#26631;&#32780;&#19981;&#24433;&#21709;&#23545;&#25239;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29289;&#29702;&#23454;&#39564;&#65292;FPA&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13575v1 Announce Type: cross  Abstract: This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework. Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target. To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model. This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world. Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information. Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferabili
&lt;/p&gt;</description></item><item><title>&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09721</link><description>&lt;p&gt;
&#35828;&#26381;&#19968;&#20301;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Persuading a Learning Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09721
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20219;&#20309;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65289;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#20195;&#29702;&#20154;&#20351;&#29992;&#31639;&#27861;&#26469;&#23398;&#20064;&#22914;&#20309;&#23545;&#22996;&#25176;&#20154;&#30340;&#20449;&#21495;&#20570;&#20986;&#21709;&#24212;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#19968;&#27425;&#24615;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#20195;&#29702;&#20154;&#36817;&#20284;&#22320;&#26368;&#20339;&#21709;&#24212;&#12290;&#36890;&#36807;&#36825;&#20010;&#31616;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#65306;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#21487;&#20197;&#20445;&#35777;&#20854;&#25928;&#29992;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#20043;&#38388;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#65307;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#22996;&#25176;&#20154;&#22312;&#23398;&#20064;&#27169;&#22411;&#19982;&#38750;&#23398;&#20064;&#27169;&#22411;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#25928;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#26159;&#26377;&#30028;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09721v1 Announce Type: cross  Abstract: We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bound
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TASER&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05396</link><description>&lt;p&gt;
TASER: &#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#24555;&#36895;&#20934;&#30830;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05396
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TASER&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNN&#65289;&#22312;&#21253;&#25324;&#27450;&#35784;&#26816;&#27979;&#21644;&#20869;&#23481;&#25512;&#33616;&#22312;&#20869;&#30340;&#21508;&#31181;&#37325;&#35201;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;TGNN&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#26102;&#38388;&#36807;&#26102;&#30340;&#38142;&#25509;&#21644;&#20559;&#26012;&#30340;&#20132;&#20114;&#20998;&#24067;&#12290;&#36825;&#20123;&#22122;&#22768;&#23548;&#33268;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20005;&#37325;&#25439;&#23475;&#20102;TGNN&#30340;&#20934;&#30830;&#24615;&#65306;&#65288;1&#65289;&#27169;&#22411;&#21463;&#21040;&#36739;&#24046;&#20132;&#20114;&#30340;&#30417;&#30563;&#65292;&#65288;2&#65289;&#22122;&#22768;&#36755;&#20837;&#23548;&#33268;&#32858;&#21512;&#28040;&#24687;&#30340;&#39640;&#26041;&#24046;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;TGNN&#21435;&#22122;&#25216;&#26415;&#24182;&#26410;&#32771;&#34385;&#27599;&#20010;&#33410;&#28857;&#30340;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#22122;&#22768;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#38754;&#20020;&#30528;&#36941;&#21382;&#26356;&#22810;&#37051;&#23621;&#23548;&#33268;&#20135;&#29983;&#36807;&#22810;&#23567;&#25209;&#37327;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#30456;&#20449;&#24555;&#36895;&#20934;&#30830;&#30340;TGNN&#30340;&#35299;&#20915;&#26041;&#27861;&#22312;&#20110;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TASER&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#36827;&#34892;&#20248;&#21270;&#30340;TGNN&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and sc
&lt;/p&gt;</description></item><item><title>TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04616</link><description>&lt;p&gt;
TinyLLM: &#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TinyLLM: Learning a Small Student from Multiple Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04616
&lt;/p&gt;
&lt;p&gt;
TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26356;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#36739;&#23567;&#30340;LLMs&#26356;&#28789;&#27963;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;&#22312;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#30693;&#35782;&#33976;&#39311;&#22240;&#20854;&#20986;&#33394;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#20419;&#36827;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TinyLLM&#65292;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#25945;&#24072;LLMs&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;LLM&#30340;&#26032;&#22411;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#40723;&#21169;&#23398;&#29983;LLM&#19981;&#20165;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#32780;&#19988;&#29702;&#35299;&#36825;&#20123;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;&#37492;&#20110;&#19981;&#21516;&#30340;LLMs&#20855;&#26377;&#19981;&#21516;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#21560;&#25910;&#26469;&#33258;&#22810;&#20010;&#25945;&#24072;LLMs&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#32769;&#24072;&#24378;&#21046;&#27169;&#22359;...
&lt;/p&gt;
&lt;p&gt;
Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#32771;&#34385;&#20102;&#25439;&#22833;&#21644;&#19981;&#30830;&#23450;&#24615;&#22522;&#30784;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.13927</link><description>&lt;p&gt;
&#20851;&#20110;&#25439;&#22833;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the convergence of loss and uncertainty-based active learning algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13927
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32771;&#34385;&#20102;&#25439;&#22833;&#21644;&#19981;&#30830;&#23450;&#24615;&#22522;&#30784;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#25439;&#22833;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#32452;&#26465;&#20214;&#65292;&#30830;&#20445;&#22312;&#24212;&#29992;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#26102;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#21253;&#25324;&#35777;&#26126;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#22522;&#20110;&#25439;&#22833;&#30340;&#37319;&#26679;&#30340;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24050;&#30693;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23548;&#20986;&#25439;&#22833;&#37319;&#26679;&#30340;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#28857;&#37319;&#26679;&#21644;&#38543;&#26426;Polyak&#27493;&#38271;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20851;&#20110;&#37319;&#26679;&#36807;&#31243;&#30340;&#26465;&#20214;&#65292;&#30830;&#20445;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#65292;&#29305;&#21035;&#26159;&#22312;&#20809;&#28369;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13927v2 Announce Type: replace-cross  Abstract: We consider the convergence rates of loss and uncertainty-based active learning algorithms under various assumptions. Firstly, we establish a set of conditions that ensure convergence rates when applied to linear classifiers and linearly separable datasets. This includes demonstrating convergence rate guarantees for loss-based sampling with various loss functions. Secondly, we introduce a framework that allows us to derive convergence rate bounds for loss-based sampling by leveraging known convergence rate bounds for stochastic gradient descent algorithms. Lastly, we propose a new algorithm that combines point sampling and stochastic Polyak's step size. We establish a condition on the sampling process, ensuring a convergence rate guarantee for this algorithm, particularly in the case of smooth convex loss functions. Our numerical results showcase the efficiency of the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39640;&#25928;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;&#65288;EMDM&#65289;&#65292;&#33021;&#22815;&#22312;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#20013;&#23454;&#29616;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#21160;&#20316;&#29983;&#25104;</title><link>https://arxiv.org/abs/2312.02256</link><description>&lt;p&gt;
&#39640;&#25928;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;&#65288;EMDM&#65289;&#29992;&#20110;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#21160;&#20316;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02256
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39640;&#25928;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;&#65288;EMDM&#65289;&#65292;&#33021;&#22815;&#22312;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#20013;&#23454;&#29616;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#21160;&#20316;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;&#65288;EMDM&#65289;&#65292;&#29992;&#20110;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#21160;&#20316;&#29983;&#25104;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#22312;&#36861;&#27714;&#24555;&#36895;&#29983;&#25104;&#30340;&#21516;&#26102;&#29306;&#29298;&#20102;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EMDM&#65292;&#23427;&#36890;&#36807;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#22810;&#27425;&#37319;&#26679;&#27493;&#39588;&#20013;&#25429;&#25417;&#22797;&#26434;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#21644;&#29983;&#25104;&#36807;&#31243;&#30340;&#26174;&#30528;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02256v2 Announce Type: replace-cross  Abstract: We introduce Efficient Motion Diffusion Model (EMDM) for fast and high-quality human motion generation. Current state-of-the-art generative diffusion models have produced impressive results but struggle to achieve fast generation without sacrificing quality. On the one hand, previous works, like motion latent diffusion, conduct diffusion within a latent space for efficiency, but learning such a latent space can be a non-trivial effort. On the other hand, accelerating generation by naively increasing the sampling step size, e.g., DDIM, often leads to quality degradation as it fails to approximate the complex denoising distribution. To address these issues, we propose EMDM, which captures the complex distribution during multiple sampling steps in the diffusion model, allowing for much fewer sampling steps and significant acceleration in generation. This is achieved by a conditional denoising diffusion GAN to capture multimodal da
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#65292;&#24341;&#20837;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Word4Per&#12290;</title><link>https://arxiv.org/abs/2311.16515</link><description>&lt;p&gt;
Word4Per: Zero-shot&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Word4Per: Zero-shot Composed Person Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16515
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#65292;&#24341;&#20837;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Word4Per&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#29305;&#23450;&#20154;&#21592;&#20855;&#26377;&#26497;&#22823;&#30340;&#31038;&#20250;&#25928;&#30410;&#21644;&#23433;&#20840;&#20215;&#20540;&#65292;&#36890;&#24120;&#28041;&#21450;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;CPR&#38656;&#35201;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#21487;&#29992;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#39046;&#22495;&#30456;&#20851;&#25968;&#25454;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#32780;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#23398;&#20064;ZS-CPR&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;Word4Per&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25991;&#26412;&#21453;&#36716;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16515v2 Announce Type: replace-cross  Abstract: Searching for specific person has great social benefits and security value, and it often involves a combination of visual and textual information. Conventional person retrieval methods, whether image-based or text-based, usually fall short in effectively harnessing both types of information, leading to the loss of accuracy. In this paper, a whole new task called Composed Person Retrieval (CPR) is proposed to jointly utilize both image and text information for target person retrieval. However, the supervised CPR requires very costly manual annotation dataset, while there are currently no available resources. To mitigate this issue, we firstly introduce the Zero-shot Composed Person Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we propose a two-stage learning framework, Word4Per, where a lightweight Textual Inversion Netw
&lt;/p&gt;</description></item><item><title>AccoMontage-3&#26159;&#19968;&#31181;&#36890;&#36807;&#39034;&#24207;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#36712;&#36947;&#21151;&#33021;&#20808;&#39564;&#23454;&#29616;&#20840;&#38899;&#20048;&#20276;&#22863;&#32534;&#25490;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20027;&#26059;&#24459;&#19982;&#21644;&#24358;&#30340;&#36755;&#20837;&#29983;&#25104;&#22810;&#38899;&#36712;&#30340;&#20276;&#22863;&#12290;</title><link>http://arxiv.org/abs/2310.16334</link><description>&lt;p&gt;
AccoMontage-3: &#36890;&#36807;&#39034;&#24207;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#36712;&#36947;&#21151;&#33021;&#20808;&#39564;&#23454;&#29616;&#20840;&#38899;&#20048;&#20276;&#22863;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
AccoMontage-3: Full-Band Accompaniment Arrangement via Sequential Style Transfer and Multi-Track Function Prior. (arXiv:2310.16334v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16334
&lt;/p&gt;
&lt;p&gt;
AccoMontage-3&#26159;&#19968;&#31181;&#36890;&#36807;&#39034;&#24207;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#36712;&#36947;&#21151;&#33021;&#20808;&#39564;&#23454;&#29616;&#20840;&#38899;&#20048;&#20276;&#22863;&#32534;&#25490;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20027;&#26059;&#24459;&#19982;&#21644;&#24358;&#30340;&#36755;&#20837;&#29983;&#25104;&#22810;&#38899;&#36712;&#30340;&#20276;&#22863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;AccoMontage-3&#65292;&#36825;&#26159;&#19968;&#20010;&#31526;&#21495;&#38899;&#20048;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20027;&#26059;&#24459;&#19982;&#21644;&#24358;&#30340;&#36755;&#20837;&#65288;&#21363;&#24341;&#23548;&#20048;&#35889;&#65289;&#65292;&#29983;&#25104;&#22810;&#38899;&#36712;&#30340;&#20840;&#38899;&#20048;&#20276;&#22863;&#12290;&#35813;&#31995;&#32479;&#21253;&#21547;&#19977;&#20010;&#27169;&#22359;&#21270;&#32452;&#20214;&#65292;&#27599;&#20010;&#32452;&#20214;&#27169;&#25311;&#20840;&#38899;&#20048;&#20316;&#26354;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#31532;&#19968;&#20010;&#32452;&#20214;&#26159;&#38050;&#29748;&#32534;&#26354;&#24072;&#65292;&#36890;&#36807;&#23558;&#32441;&#29702;&#39118;&#26684;&#36716;&#25442;&#20026;&#21644;&#24358;&#65292;&#20351;&#29992;&#28508;&#22312;&#30340;&#21644;&#24358;-&#32441;&#29702;&#20998;&#31163;&#21644;&#21551;&#21457;&#24335;&#32441;&#29702;&#20379;&#24212;&#32773;&#26816;&#32034;&#65292;&#29983;&#25104;&#38050;&#29748;&#20276;&#22863;&#12290;&#31532;&#20108;&#20010;&#32452;&#20214;&#26681;&#25454;&#20010;&#21035;&#38899;&#36712;&#21151;&#33021;&#32534;&#30721;&#30340;&#31649;&#24358;&#20048;&#39118;&#26684;&#65292;&#23558;&#38050;&#29748;&#20276;&#22863;&#20048;&#35889;&#32534;&#25490;&#25104;&#20840;&#38899;&#20048;&#20276;&#22863;&#12290;&#23558;&#21069;&#20004;&#20010;&#32452;&#20214;&#36830;&#25509;&#36215;&#26469;&#30340;&#31532;&#19977;&#20010;&#32452;&#20214;&#26159;&#19968;&#20010;&#20808;&#39564;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#25972;&#39318;&#38899;&#20048;&#20316;&#21697;&#19978;&#30340;&#32534;&#26354;&#39118;&#26684;&#30340;&#20840;&#23616;&#32467;&#26500;&#12290;&#25972;&#20010;&#31995;&#32479;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#33258;&#25105;&#30417;&#30563;&#22320;&#23398;&#20064;&#29983;&#25104;&#20840;&#38899;&#20048;&#20276;&#22863;&#65292;&#23558;&#39118;&#26684;&#36716;&#25442;&#24212;&#29992;&#20110;&#20004;&#20010;&#23618;&#38754;&#30340;&#22810;&#22768;&#37096;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose AccoMontage-3, a symbolic music automation system capable of generating multi-track, full-band accompaniment based on the input of a lead melody with chords (i.e., a lead sheet). The system contains three modular components, each modelling a vital aspect of full-band composition. The first component is a piano arranger that generates piano accompaniment for the lead sheet by transferring texture styles to the chords using latent chord-texture disentanglement and heuristic retrieval of texture donors. The second component orchestrates the piano accompaniment score into full-band arrangement according to the orchestration style encoded by individual track functions. The third component, which connects the previous two, is a prior model characterizing the global structure of orchestration style over the whole piece of music. From end to end, the system learns to generate full-band accompaniment in a self-supervised fashion, applying style transfer at two levels of polyphonic co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item><item><title>FocDepthFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;LSTM&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#12290;&#36890;&#36807;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;LSTM&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11178</link><description>&lt;p&gt;
FocDepthFormer: &#20351;&#29992;LSTM&#30340;Transformer&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus. (arXiv:2310.11178v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11178
&lt;/p&gt;
&lt;p&gt;
FocDepthFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;LSTM&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#12290;&#36890;&#36807;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;LSTM&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28966;&#28857;&#22534;&#26632;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#22534;&#26632;&#20013;&#30340;&#28966;&#28857;/&#31163;&#28966;&#32447;&#32034;&#25512;&#26029;&#28145;&#24230;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#22270;&#20687;&#22534;&#26632;&#19978;&#24212;&#29992;&#20108;&#32500;&#25110;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#26469;&#22788;&#29702;&#27492;&#38382;&#39064;&#65292;&#20197;&#22312;&#22270;&#20687;&#21644;&#22534;&#26632;&#20043;&#38388;&#23398;&#20064;&#29305;&#24449;&#12290;&#30001;&#20110;CNN&#30340;&#23616;&#37096;&#24615;&#36136;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#19988;&#23427;&#20204;&#34987;&#38480;&#21046;&#22312;&#22788;&#29702;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#19968;&#33268;&#30340;&#22266;&#23450;&#25968;&#37327;&#30340;&#22534;&#26632;&#19978;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65292;FocDepthFormer&#65292;&#20027;&#35201;&#30001;&#24102;&#26377;LSTM&#27169;&#22359;&#21644;CNN&#35299;&#30721;&#22120;&#30340;Transformer&#32452;&#25104;&#12290;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#36890;&#36807;&#38544;&#21547;&#38750;&#23616;&#37096;&#20132;&#21449;&#21442;&#32771;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;LSTM&#27169;&#22359;&#34987;&#23398;&#20064;&#29992;&#20110;&#23558;&#34920;&#31034;&#38598;&#25104;&#21040;&#20855;&#26377;&#20219;&#24847;&#22270;&#20687;&#30340;&#22534;&#26632;&#20013;&#12290;&#20026;&#20102;&#30452;&#25509;&#25429;&#33719;&#20302;&#32423;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Depth estimation from focal stacks is a fundamental computer vision problem that aims to infer depth from focus/defocus cues in the image stacks. Most existing methods tackle this problem by applying convolutional neural networks (CNNs) with 2D or 3D convolutions over a set of fixed stack images to learn features across images and stacks. Their performance is restricted due to the local properties of the CNNs, and they are constrained to process a fixed number of stacks consistent in train and inference, limiting the generalization to the arbitrary length of stacks. To handle the above limitations, we develop a novel Transformer-based network, FocDepthFormer, composed mainly of a Transformer with an LSTM module and a CNN decoder. The self-attention in Transformer enables learning more informative features via an implicit non-local cross reference. The LSTM module is learned to integrate the representations across the stack with arbitrary images. To directly capture the low-level featur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;Neural PG-RANK&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26816;&#32034;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#12289;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.04407</link><description>&lt;p&gt;
&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Policy-Gradient Training of Language Models for Ranking. (arXiv:2310.04407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;Neural PG-RANK&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26816;&#32034;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#12289;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#26816;&#32034;&#22312;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#21040;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32842;&#22825;&#24335;&#32593;&#39029;&#25628;&#32034;&#21040;&#38382;&#31572;&#31995;&#32479;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#36807;&#20856;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#22522;&#20110;LLM&#30340;&#26816;&#32034;&#22120;&#38656;&#35201;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21253;&#25324;&#36873;&#25321;&#22256;&#38590;&#30340;&#36127;&#26679;&#26412;&#21644;&#20351;&#29992;&#39069;&#22806;&#30340;&#30417;&#30563;&#20316;&#20026;&#23398;&#20064;&#20449;&#21495;&#12290;&#36825;&#31181;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#21407;&#22240;&#26159;&#23545;&#27604;&#25439;&#22833;&#26412;&#36523;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#19981;&#33021;&#30452;&#25509;&#20248;&#21270;&#22788;&#29702;&#27969;&#31243;&#26411;&#31471;&#20915;&#31574;&#36136;&#37327;&#30340;&#19979;&#28216;&#25351;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;PG-RANK&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;LLM&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23398;&#20064;&#25490;&#24207;&#12290;&#31070;&#32463;PG-RANK&#20026;&#26816;&#32034;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#20316;&#20026;&#26356;&#22823;&#30340;&#20915;&#31574;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text retrieval plays a crucial role in incorporating factual knowledge for decision making into language processing pipelines, ranging from chat-based web search to question answering systems. Current state-of-the-art text retrieval models leverage pre-trained large language models (LLMs) to achieve competitive performance, but training LLM-based retrievers via typical contrastive losses requires intricate heuristics, including selecting hard negatives and using additional supervision as learning signals. This reliance on heuristics stems from the fact that the contrastive loss itself is heuristic and does not directly optimize the downstream metrics of decision quality at the end of the processing pipeline. To address this issue, we introduce Neural PG-RANK, a novel training algorithm that learns to rank by instantiating a LLM as a Plackett-Luce ranking policy. Neural PG-RANK provides a principled method for end-to-end training of retrieval models as part of larger decision systems vi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoSAM Adapter&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.14936</link><description>&lt;p&gt;
&#20026;&#31227;&#21160;&#21451;&#22909;&#30340;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#33258;&#21160;&#25552;&#31034;SAM
&lt;/p&gt;
&lt;p&gt;
Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation. (arXiv:2308.14936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14936
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoSAM Adapter&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#24050;&#32463;&#34987;&#36805;&#36895;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#38500;&#20102;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#22806;&#65292;2D&#21644;3D&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#24067;&#23616;&#24046;&#24322;&#65292;&#24378;&#22823;&#30340;GPU&#26381;&#21153;&#22120;&#25152;&#24102;&#26469;&#30340;&#22823;&#37327;&#35745;&#31639;&#36127;&#25285;&#65292;&#20197;&#21450;&#32791;&#26102;&#30340;&#25163;&#21160;&#25552;&#31034;&#29983;&#25104;&#20351;&#24471;SAM&#26080;&#27861;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;AutoSAM Adapter&#65292;&#19987;&#20026;3D&#22810;&#22120;&#23448;CT&#20998;&#21106;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#20419;&#36827;&#23558;SAM&#27169;&#22411;&#30340;&#33021;&#21147;&#36716;&#21270;&#20026;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has rapidly been adopted for segmenting a wide range of natural images. However, recent studies have indicated that SAM exhibits subpar performance on 3D medical image segmentation tasks. In addition to the domain gaps between natural and medical images, disparities in the spatial arrangement between 2D and 3D images, the substantial computational burden imposed by powerful GPU servers, and the time-consuming manual prompt generation impede the extension of SAM to a broader spectrum of medical image segmentation applications. To address these challenges, in this work, we introduce a novel method, AutoSAM Adapter, designed specifically for 3D multi-organ CT-based segmentation. We employ parameter-efficient adaptation techniques in developing an automatic prompt learning paradigm to facilitate the transformation of the SAM model's capabilities to 3D medical image segmentation, eliminating the need for manually generated prompts. Furthermore, we effectivel
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35270;&#35282;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#25200;&#21160;&#24402;&#22240;&#29305;&#24449;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#25581;&#31034;&#20102;&#24402;&#22240;&#29305;&#24449;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.08949</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#37325;&#35270;&#35282;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Dual-Perspective Approach to Evaluating Feature Attribution Methods. (arXiv:2308.08949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08949
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35270;&#35282;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#25200;&#21160;&#24402;&#22240;&#29305;&#24449;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#25581;&#31034;&#20102;&#24402;&#22240;&#29305;&#24449;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24314;&#31435;&#19968;&#20010;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#32479;&#19968;&#26694;&#26550;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20960;&#20010;&#35270;&#35282;&#26469;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#35270;&#35282;&#26159;&#35266;&#23519;&#25200;&#21160;&#24402;&#22240;&#29305;&#24449;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65288;&#21363;&#24544;&#23454;&#24230;&#65289;&#12290;&#23613;&#31649;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#27934;&#35265;&#65292;&#20294;&#29616;&#26377;&#30340;&#24544;&#23454;&#24230;&#35780;&#20272;&#23384;&#22312;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25581;&#31034;&#30340;&#32570;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24544;&#23454;&#24230;&#33539;&#24335;&#20869;&#30340;&#20004;&#20010;&#26032;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;&#30452;&#35266;&#30340;&#23646;&#24615;&#65306;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#27491;&#30830;&#24615;&#35780;&#20272;&#24402;&#22240;&#29305;&#24449;&#30495;&#27491;&#26159;&#39044;&#27979;&#24615;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#32780;&#23436;&#25972;&#24615;&#26816;&#26597;&#25152;&#24471;&#24402;&#22240;&#22914;&#20309;&#24456;&#22909;&#22320;&#25581;&#31034;&#25152;&#26377;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;&#36825;&#20004;&#20010;&#35270;&#35282;&#22522;&#20110;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;&#30340;&#23450;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods attempt to explain neural network predictions by identifying relevant features. However, establishing a cohesive framework for assessing feature attribution remains a challenge. There are several views through which we can evaluate attributions. One principal lens is to observe the effect of perturbing attributed features on the model's behavior (i.e., faithfulness). While providing useful insights, existing faithfulness evaluations suffer from shortcomings that we reveal in this paper. In this work, we propose two new perspectives within the faithfulness paradigm that reveal intuitive properties: soundness and completeness. Soundness assesses the degree to which attributed features are truly predictive features, while completeness examines how well the resulting attribution reveals all the predictive features. The two perspectives are based on a firm mathematical foundation and provide quantitative metrics that are computable through efficient algorithms. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.09059</link><description>&lt;p&gt;
&#25991;&#23383;&#24819;&#35937;&#30340;&#37322;&#25918;&#65306;&#36890;&#36807;&#25506;&#32034;&#25991;&#23383;&#30340;&#21147;&#37327;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#30446;&#26631;&#26159;&#20174;&#22823;&#22411;&#22270;&#24211;&#20013;&#26816;&#32034;&#19982;&#32473;&#23450;&#25991;&#26412;&#25551;&#36848;&#30456;&#21305;&#37197;&#30340;&#20154;&#29289;&#22270;&#20687;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#20449;&#24687;&#34920;&#31034;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25991;&#26412;&#27169;&#24577;&#36890;&#36807;&#35789;&#27719;&#21644;&#35821;&#27861;&#32467;&#26500;&#20256;&#36882;&#25277;&#35937;&#21644;&#31934;&#30830;&#30340;&#20449;&#24687;&#65292;&#32780;&#35270;&#35273;&#27169;&#24577;&#36890;&#36807;&#22270;&#20687;&#20256;&#36882;&#20855;&#20307;&#21644;&#30452;&#35266;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25991;&#23383;&#34920;&#31034;&#30340;&#34920;&#36798;&#21147;&#65292;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#22270;&#20687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#21477;&#23376;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#37322;&#25918;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#29289;&#26816;&#32034;&#20013;&#30340;&#25991;&#23383;&#24819;&#35937;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#20840;&#38754;CLIP&#27169;&#22411;&#20316;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21452;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.  To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Imag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23384;&#22312;&#26816;&#27979;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#26102;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10802</link><description>&lt;p&gt;
BTS&#65306;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#23460;&#20869;&#20004;&#25151;&#38388;&#23384;&#22312;&#26816;&#27979;&#20013;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
BTS: Bifold Teacher-Student in Semi-Supervised Learning for Indoor Two-Room Presence Detection Under Time-Varying CSI. (arXiv:2212.10802v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23384;&#22312;&#26816;&#27979;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#26102;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#23460;&#20869;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;CSI&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#22914;&#29289;&#20307;&#31227;&#21160;&#12289;&#22823;&#27668;&#22240;&#32032;&#21644;&#26426;&#22120;&#37325;&#21551;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#32791;&#26102;&#30340;&#26631;&#27880;&#26469;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#35774;&#35745;&#19968;&#20010;&#36830;&#32493;&#30417;&#25511;&#30340;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24605;&#20102;&#19968;&#31181;&#21452;&#25240;&#21472;&#24072;&#29983;&#65288;BTS&#65289;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#23384;&#22312;&#20110;&#31995;&#32479;&#20013;&#30340;&#23384;&#22312;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#21407;&#22987;&#23545;&#20598;&#24072;&#29983;&#32593;&#32476;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;CSI&#20013;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#22686;&#24378;&#30340;&#24809;&#32602;&#25439;&#22833;&#20989;&#25968;&#21033;&#29992;&#29109;&#21644;&#36317;&#31163;&#27979;&#37327;&#26469;&#21306;&#20998;&#28145;&#23618;&#29305;&#24449;&#65292;&#38477;&#20302;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, indoor human presence detection based on supervised learning (SL) and channel state information (CSI) has attracted much attention. However, the existing studies that rely on spatial information of CSI are susceptible to environmental changes, such as object movement, atmospheric factors, and machine rebooting, which degrade prediction accuracy. Moreover, SL-based methods require time-consuming labeling for retraining models. Therefore, it is imperative to design a continuously monitored model life-cycle using a semi-supervised learning (SSL) based scheme. In this paper, we conceive a bifold teacher-student (BTS) learning approach for presence detection systems that combines SSL by utilizing partially labeled and unlabeled datasets. The proposed primal-dual teacher-student network intelligently learns spatial and temporal features from labeled and unlabeled CSI. Additionally, the enhanced penalized loss function leverages entropy and distance measures to distinguish dr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11483</link><description>&lt;p&gt;
Deanthropomorphising NLP&#65306;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24847;&#35782;&#21040;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#26368;&#36817;&#26377;&#20851;&#20351;&#29992;Transformer&#27169;&#22411;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#36827;&#34892;&#35752;&#35770;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#24182;&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#32508;&#21512;&#20449;&#24687;&#29702;&#35770;&#23545;Transformer&#26550;&#26500;&#36827;&#34892;&#20998;&#26512;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#26159;NLP&#25253;&#36947;&#20013;&#20351;&#29992;&#25311;&#20154;&#21270;&#35821;&#35328;&#30340;&#26356;&#24191;&#27867;&#20542;&#21521;&#30340;&#19968;&#37096;&#20998;&#12290;&#26080;&#35770;&#36825;&#20123;&#35828;&#27861;&#30340;&#30495;&#23454;&#24615;&#22914;&#20309;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#22312;&#26159;&#35780;&#20272;&#35821;&#35328;&#24314;&#27169;&#36827;&#23637;&#24182;&#32771;&#34385;&#35813;&#20219;&#21153;&#30340;&#20262;&#29702;&#24433;&#21709;&#30340;&#36866;&#24403;&#26102;&#26426;&#12290;&#20026;&#20102;&#20351;&#26412;&#25991;&#26377;&#21161;&#20110;NLP&#31038;&#21306;&#20197;&#22806;&#30340;&#35835;&#32773;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;NLP&#22522;&#30784;&#30693;&#35782;&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is intended as a voice in the discussion over the recent claims that LaMDA, a pretrained language model based on the Transformer model architecture, is sentient. This claim, if confirmed, would have serious ramifications in the Natural Language Processing (NLP) community due to wide-spread use of similar models. However, here we take the position that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it. We justify this by analysing the Transformer architecture through Integrated Information Theory. We see the claims of consciousness as part of a wider tendency to use anthropomorphic language in NLP reporting. Regardless of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task. In order to make this work helpful for readers outside the NLP community, we also present the
&lt;/p&gt;</description></item></channel></rss>