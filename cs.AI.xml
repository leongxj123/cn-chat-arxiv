<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;GraphLM&#21644;&#25552;&#20986;GraphLM+&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2403.04483</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#29702;&#35299;&#21644;&#25512;&#29702;&#21151;&#33021;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;GraphInstruct
&lt;/p&gt;
&lt;p&gt;
GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;GraphLM&#21644;&#25552;&#20986;GraphLM+&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36890;&#29992;&#33021;&#21147;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22270;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#29702;&#35299;&#22270;&#25968;&#25454;&#23545;&#20110;&#25512;&#36827;&#36890;&#29992;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;&#21644;&#22686;&#24378;LLMs&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#20840;&#38754;&#21253;&#25324;21&#20010;&#32463;&#20856;&#22270;&#25512;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#22810;&#26679;&#30340;&#22270;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#35814;&#32454;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;&#22522;&#20110;GraphInstruct&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#39640;&#25928;&#30340;&#25351;&#23548;&#35843;&#25972;&#26500;&#24314;&#20102;GraphLM&#65292;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;LLM&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27493;&#39588;&#25513;&#30721;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;GraphLM+&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#22686;&#24378;LLMs&#22270;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20808;&#39537;&#24615;&#21162;&#21147;&#20043;&#19968;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04483v1 Announce Type: new  Abstract: Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#25193;&#25955;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10028</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#19982;&#22823;&#21160;&#20316;&#31354;&#38388;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models Meet Contextual Bandits with Large Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#25193;&#25955;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21160;&#20316;&#31354;&#38388;&#36739;&#22823;&#65292;&#26377;&#25928;&#30340;&#25506;&#32034;&#26159;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#35774;&#35745;&#20102;&#25193;&#25955;&#27748;&#26222;&#26862;&#37319;&#26679;&#65288;dTS&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#20026;dTS&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#31639;&#27861;&#22522;&#30784;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10028v1 Announce Type: cross  Abstract: Efficient exploration is a key challenge in contextual bandits due to the large size of their action space, where uninformed exploration can result in computational and statistical inefficiencies. Fortunately, the rewards of actions are often correlated and this can be leveraged to explore them efficiently. In this work, we capture such correlations using pre-trained diffusion models; upon which we design diffusion Thompson sampling (dTS). Both theoretical and algorithmic foundations are developed for dTS, and empirical evaluation also shows its favorable performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04678</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#20449;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As Faithful Explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#37096;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24050;&#32463;&#33021;&#22815;&#29087;&#32451;&#35299;&#20915;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#20256;&#32479;&#30340;&#20197;&#36755;&#20837;&#20026;&#37325;&#28857;&#30340;&#35299;&#37322;&#31639;&#27861;&#26469;&#35299;&#37322;LLMs&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#26426;&#21046;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#36827;&#34892;&#21333;&#21521;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;LLMs&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#32463;&#24120;&#22240;&#20026;&#32570;&#20047;&#21487;&#20449;&#24230;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#22240;&#20026;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#19981;&#20934;&#30830;&#22320;&#21453;&#26144;LLMs&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;xLLM&#65292;&#20197;&#25552;&#39640;LLMs&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;xLLM&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#25991;&#26723;&#36130;&#21153;&#38382;&#31572;&#20219;&#21153;&#65292;&#23558;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;700&#20010;&#35789;&#25193;&#23637;&#21040;123k&#20010;&#35789;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.06915</link><description>&lt;p&gt;
DocFinQA&#65306;&#19968;&#20010;&#38271;&#25991;&#26412;&#36130;&#21153;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DocFinQA: A Long-Context Financial Reasoning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06915
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#25991;&#26723;&#36130;&#21153;&#38382;&#31572;&#20219;&#21153;&#65292;&#23558;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;700&#20010;&#35789;&#25193;&#23637;&#21040;123k&#20010;&#35789;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#39046;&#22495;&#21457;&#25381;&#20316;&#29992;&#65292;&#38656;&#35201;&#30740;&#31350;&#29616;&#23454;&#20219;&#21153;&#21644;&#25968;&#25454;&#12290;&#37329;&#34701;&#19987;&#19994;&#20154;&#22763;&#32463;&#24120;&#19982;&#38271;&#36798;&#25968;&#30334;&#39029;&#30340;&#25991;&#26723;&#36827;&#34892;&#20132;&#20114;&#65292;&#20294;&#22823;&#22810;&#25968;&#37329;&#34701;&#30740;&#31350;&#25968;&#25454;&#38598;&#20165;&#22788;&#29702;&#36825;&#20123;&#25991;&#26723;&#30340;&#31616;&#30701;&#25688;&#24405;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#25991;&#26723;&#36130;&#21153;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29616;&#26377;FinQA&#25968;&#25454;&#38598;&#20013;&#30340;7,437&#20010;&#38382;&#39064;&#20013;&#22686;&#21152;&#23436;&#25972;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#23558;FinQA&#20013;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;&#19981;&#21040;700&#20010;&#35789;&#25193;&#23637;&#21040;DocFinQA&#20013;&#30340;123k&#20010;&#35789;&#12290;&#25105;&#20204;&#22312;&#26816;&#32034;&#24335;QA&#31649;&#36947;&#21644;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#65292;DocFinQA&#20063;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#23545;DocFinQA&#20013;&#26368;&#38271;&#25991;&#26723;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#36825;&#20123;&#25991;&#26723;&#19978;&#29305;&#21035;&#22256;&#38590;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06915v2 Announce Type: replace-cross  Abstract: For large language models (LLMs) to be effective in the financial domain -- where each decision can have a significant impact -- it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents that are hundreds of pages long, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with the full-document context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents. Addressing these challen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32447;POMDP&#35268;&#21010;&#20013;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#36817;&#20284;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.01791</link><description>&lt;p&gt;
&#20855;&#26377;&#20219;&#24847;&#30830;&#23450;&#24615;&#20445;&#35777;&#30340;&#22312;&#32447;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Online POMDP Planning with Anytime Deterministic Guarantees. (arXiv:2310.01791v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32447;POMDP&#35268;&#21010;&#20013;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#36817;&#20284;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#33258;&#20027;&#26234;&#33021;&#20307;&#32463;&#24120;&#36935;&#21040;&#19981;&#30830;&#23450;&#24615;&#24182;&#22522;&#20110;&#19981;&#23436;&#25972;&#20449;&#24687;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#35268;&#21010;&#21487;&#20197;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;POMDP&#30340;&#26368;&#20248;&#35268;&#21010;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#21482;&#26377;&#22312;&#23567;&#35268;&#27169;&#20219;&#21153;&#20013;&#21487;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#36817;&#20284;&#31639;&#27861;&#65288;&#22914;&#26641;&#25628;&#32034;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36739;&#22823;&#38382;&#39064;&#30340;&#20808;&#36827;POMDP&#27714;&#35299;&#22120;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20165;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#37319;&#26679;&#30340;&#32536;&#25925;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#36873;&#25321;&#19968;&#32452;&#35266;&#27979;&#20197;&#22312;&#35745;&#31639;&#27599;&#20010;&#21518;&#39564;&#33410;&#28857;&#26102;&#20998;&#25903;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents operating in real-world scenarios frequently encounter uncertainty and make decisions based on incomplete information. Planning under uncertainty can be mathematically formalized using partially observable Markov decision processes (POMDPs). However, finding an optimal plan for POMDPs can be computationally expensive and is feasible only for small tasks. In recent years, approximate algorithms, such as tree search and sample-based methodologies, have emerged as state-of-the-art POMDP solvers for larger problems. Despite their effectiveness, these algorithms offer only probabilistic and often asymptotic guarantees toward the optimal solution due to their dependence on sampling. To address these limitations, we derive a deterministic relationship between a simplified solution that is easier to obtain and the theoretically optimal one. First, we derive bounds for selecting a subset of the observations to branch from while computing a complete belief at each posterior nod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GPT-4&#22522;&#20110;&#30340;GNAS&#26041;&#27861;&#65288;GPT4GNAS&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;GPT-4&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01436</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Architecture Search with GPT-4. (arXiv:2310.01436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GPT-4&#22522;&#20110;&#30340;GNAS&#26041;&#27861;&#65288;GPT4GNAS&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;GPT-4&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#22312;&#33258;&#21160;&#35774;&#35745;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;GNAS&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#21171;&#21160;&#21644;&#20016;&#23500;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#35774;&#35745;&#25628;&#32034;&#31354;&#38388;&#21644;&#25628;&#32034;&#31574;&#30053;&#12290;&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;GNAS&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-4&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65288;&#31616;&#31216;&#20026;GPT4GNAS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20026;GPT-4&#35774;&#35745;&#19968;&#31867;&#26032;&#30340;&#25552;&#31034;&#65292;&#20197;&#25351;&#23548;GPT-4&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#36825;&#20123;&#25552;&#31034;&#21253;&#25324;GNAS&#30340;&#25628;&#32034;&#31354;&#38388;&#12289;&#25628;&#32034;&#31574;&#30053;&#21644;&#25628;&#32034;&#21453;&#39304;&#30340;&#25551;&#36848;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#36816;&#34892;&#20855;&#26377;&#25552;&#31034;&#30340;GPT-4&#65292;GPT4GNAS&#33021;&#22815;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;GNAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Architecture Search (GNAS) has shown promising results in automatically designing graph neural networks. However, GNAS still requires intensive human labor with rich domain knowledge to design the search space and search strategy. In this paper, we integrate GPT-4 into GNAS and propose a new GPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The basic idea of our method is to design a new class of prompts for GPT-4 to guide GPT-4 toward the generative task of graph neural architectures. The prompts consist of descriptions of the search space, search strategy, and search feedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS generates more accurate graph neural networks with fast convergence. Experimental results show that embedding GPT-4 into GNAS outperforms the state-of-the-art GNAS methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;Learnable Behavioral Control (LBC)&#26694;&#26550;&#65292;&#20351;&#24471;&#34892;&#20026;&#36873;&#25321;&#31354;&#38388;&#24471;&#21040;&#25193;&#22823;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36172;&#21338;&#26426;&#30340;&#20803;&#25511;&#21046;&#22120;&#23454;&#29616;&#34892;&#20026;&#25511;&#21046;&#12290;&#22312;Atari&#28216;&#25103;&#19978;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#24050;&#32463;&#36798;&#21040;10&#20010;&#28216;&#25103;&#30340;&#20154;&#31867;&#27700;&#24179;&#65292;&#24182;&#22312;7&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#20102;&#30446;&#21069;&#30340;&#26368;&#39640;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.05239</link><description>&lt;p&gt;
&#21487;&#23398;&#20064;&#30340;&#34892;&#20026;&#25511;&#21046;&#65306;&#36890;&#36807;&#39640;&#25928;&#34892;&#20026;&#36873;&#25321;&#25171;&#30772;Atari&#20154;&#31867;&#19990;&#30028;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection. (arXiv:2305.05239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;Learnable Behavioral Control (LBC)&#26694;&#26550;&#65292;&#20351;&#24471;&#34892;&#20026;&#36873;&#25321;&#31354;&#38388;&#24471;&#21040;&#25193;&#22823;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36172;&#21338;&#26426;&#30340;&#20803;&#25511;&#21046;&#22120;&#23454;&#29616;&#34892;&#20026;&#25511;&#21046;&#12290;&#22312;Atari&#28216;&#25103;&#19978;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#24050;&#32463;&#36798;&#21040;10&#20010;&#28216;&#25103;&#30340;&#20154;&#31867;&#27700;&#24179;&#65292;&#24182;&#22312;7&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#20102;&#30446;&#21069;&#30340;&#26368;&#39640;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#38382;&#39064;&#26159;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#24037;&#20316;&#23581;&#35797;&#20351;&#29992;&#22522;&#20110;&#32676;&#20307;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#19981;&#21516;&#25506;&#32034;&#31574;&#30053;&#30340;&#20154;&#32676;&#20013;&#25910;&#38598;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#30340;&#26679;&#26412;&#12290;&#33258;&#36866;&#24212;&#31574;&#30053;&#36873;&#25321;&#24050;&#34987;&#29992;&#20110;&#34892;&#20026;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#34892;&#20026;&#36873;&#25321;&#31354;&#38388;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#39044;&#23450;&#20041;&#31574;&#30053;&#31181;&#32676;&#30340;&#38480;&#21046;&#65292;&#36825;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#34892;&#20026;&#22810;&#26679;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#31216;&#20026;&#21487;&#23398;&#20064;&#30340;&#34892;&#20026;&#25511;&#21046;&#65288;LBC&#65289;&#26469;&#35299;&#20915;&#36825;&#31181;&#38480;&#21046;&#12290;&#35813;&#26694;&#26550;a)&#36890;&#36807;&#20174;&#25152;&#26377;&#31574;&#30053;&#20013;&#21046;&#23450;&#28151;&#21512;&#34892;&#20026;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#25193;&#22823;&#30340;&#34892;&#20026;&#36873;&#25321;&#31354;&#38388;&#65307;b)&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21487;&#23398;&#20064;&#30340;&#34892;&#20026;&#36873;&#25321;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;LBC&#24341;&#20837;&#20998;&#24067;&#24335;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#20013;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36172;&#21338;&#26426;&#30340;&#20803;&#25511;&#21046;&#22120;&#20248;&#21270;&#34892;&#20026;&#26144;&#23556;&#30340;&#36873;&#25321;&#26469;&#23454;&#29616;&#34892;&#20026;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#24050;&#32463;&#22312;10&#20010;Atari&#28216;&#25103;&#20013;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#65292;&#24182;&#22312;7&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#20102;&#30446;&#21069;&#30340;&#26368;&#39640;&#20998;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LBC&#26694;&#26550;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity. In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#30340;&#26032;&#30340;&#34920;&#29616;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#20197;&#19977;&#20803;&#32452;&#27604;&#36739;&#20026;&#22522;&#30784;&#30340;&#34920;&#29616;&#23450;&#29702;&#38382;&#39064;&#12290;&#36825;&#31181;&#34920;&#29616;&#23450;&#29702;&#21487;&#20197;&#29992;&#20869;&#31215;&#35825;&#23548;&#30340;&#33539;&#25968;&#26469;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.03720</link><description>&lt;p&gt;
&#24230;&#37327;&#23398;&#20064;&#19982;&#20559;&#22909;&#23398;&#20064;&#30340;&#34920;&#29616;&#23450;&#29702;&#65306;&#22522;&#20110;&#20960;&#20309;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Representer Theorems for Metric and Preference Learning: A Geometric Perspective. (arXiv:2304.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03720
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#30340;&#26032;&#30340;&#34920;&#29616;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#20197;&#19977;&#20803;&#32452;&#27604;&#36739;&#20026;&#22522;&#30784;&#30340;&#34920;&#29616;&#23450;&#29702;&#38382;&#39064;&#12290;&#36825;&#31181;&#34920;&#29616;&#23450;&#29702;&#21487;&#20197;&#29992;&#20869;&#31215;&#35825;&#23548;&#30340;&#33539;&#25968;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#34920;&#29616;&#23450;&#29702;&#21487;&#20197;&#26681;&#25454;&#38382;&#39064;&#32467;&#26500;&#20869;&#22312;&#30340;&#20869;&#31215;&#25152;&#35825;&#23548;&#30340;&#33539;&#25968;&#26469;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#19977;&#20803;&#32452;&#27604;&#36739;&#30340;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#23427;&#23548;&#33268;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#33258;&#21253;&#21547;&#30340;&#35813;&#20219;&#21153;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#21487;&#20197;&#20351;&#29992;&#31867;&#20284;&#20110;&#32463;&#20856;&#34920;&#29616;&#23450;&#29702;&#30340;&#26680;&#26415;&#35821;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the metric and preference learning problem in Hilbert spaces. We obtain a novel representer theorem for the simultaneous task of metric and preference learning. Our key observation is that the representer theorem can be formulated with respect to the norm induced by the inner product inherent in the problem structure. Additionally, we demonstrate how our framework can be applied to the task of metric learning from triplet comparisons and show that it leads to a simple and self-contained representer theorem for this task. In the case of Reproducing Kernel Hilbert Spaces (RKHS), we demonstrate that the solution to the learning problem can be expressed using kernel terms, akin to classical representer theorems.
&lt;/p&gt;</description></item></channel></rss>