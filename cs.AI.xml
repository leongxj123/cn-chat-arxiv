<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>LLMs&#20511;&#21161;Reasoning-Path-Editing (Readi)&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#24544;&#23454;&#22320;&#25512;&#29702;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#20010;KGQA&#21644;TableQA&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.08593</link><description>&lt;p&gt;
&#24403;&#38656;&#35201;&#26102;&#32473;&#25105;&#25171;&#30005;&#35805;&#65306;LLM&#21487;&#20197;&#39640;&#25928;&#32780;&#24544;&#23454;&#22320;&#25512;&#29702;&#32467;&#26500;&#21270;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08593
&lt;/p&gt;
&lt;p&gt;
LLMs&#20511;&#21161;Reasoning-Path-Editing (Readi)&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#24544;&#23454;&#22320;&#25512;&#29702;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#20010;KGQA&#21644;TableQA&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#25512;&#29702;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#21644;&#34920;&#26684;&#12290;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#65292;&#21363;&#23558;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#19982;&#29615;&#22659;&#20013;&#30340;&#23454;&#20363;&#21305;&#37197;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#36880;&#27493;&#26500;&#24314;&#25512;&#29702;&#36335;&#24452;&#65292;&#20854;&#20013;LLMs&#36890;&#36807;&#19982;&#29615;&#22659;&#36880;&#27493;&#20132;&#20114;&#26469;&#35843;&#29992;&#24037;&#20855;&#25110;&#36873;&#25321;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;Reasoning-Path-Editing&#65288;Readi&#65289;&#65292;&#22312;&#20854;&#20013;LLMs&#21487;&#20197;&#39640;&#25928;&#19988;&#24544;&#23454;&#22320;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;Readi&#20013;&#65292;LLMs&#22312;&#32473;&#23450;&#26597;&#35810;&#26102;&#26368;&#21021;&#29983;&#25104;&#19968;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#21482;&#26377;&#22312;&#24517;&#35201;&#26102;&#25165;&#32534;&#36753;&#36335;&#24452;&#12290;&#25105;&#20204;&#23558;&#36335;&#24452;&#23454;&#20363;&#21270;&#21040;&#32467;&#26500;&#21270;&#29615;&#22659;&#19978;&#65292;&#24182;&#22312;&#20986;&#29616;&#38382;&#39064;&#26102;&#25552;&#20379;&#21453;&#39304;&#20197;&#32534;&#36753;&#36335;&#24452;&#12290;&#23545;&#19977;&#20010;KGQA&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;TableQA&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Readi&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#25152;&#26377;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#65288;&#22312;WebQ&#19978;&#25552;&#39640;&#20102;9.1&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08593v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graph and table. Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment. Previous methods leverage LLMs to incrementally build a reasoning path, where the LLMs either invoke tools or pick up schemas by step-by-step interacting with the environment. We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments. In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA datasets and two TableQA datasets show the effectiveness of Readi, significantly surpassing all LLM-based methods (by 9.1% on WebQ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.03777</link><description>&lt;p&gt;
ENOT&#65306;&#26399;&#26395;&#22238;&#24402;&#29992;&#20110;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#30340;&#24555;&#36895;&#21644;&#20934;&#30830;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03777
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#20849;&#36717;&#21183;&#27491;&#21017;&#21270;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#12290;&#29616;&#26377;NOT&#27714;&#35299;&#22120;&#30340;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#25214;&#21040;&#20849;&#36717;&#31639;&#23376;&#65288;&#21363;c-transform&#65289;&#30340;&#25509;&#36817;&#31934;&#30830;&#36817;&#20284;&#30340;&#36807;&#31243;&#65292;&#36825;&#35201;&#20040;&#36890;&#36807;&#20248;&#21270;&#26368;&#23567;-&#26368;&#22823;&#30446;&#26631;&#65292;&#35201;&#20040;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#23545;&#21021;&#22987;&#36817;&#20284;&#39044;&#27979;&#30340;&#31934;&#32454;&#35843;&#25972;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#12289;&#22312;&#26399;&#26395;&#22238;&#24402;&#24418;&#24335;&#19978;&#24378;&#21046;&#36866;&#24212;&#24615;&#26465;&#20214;&#20110;&#23398;&#20064;&#23545;&#20598;&#21183;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#21270;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#21487;&#33021;&#20849;&#36717;&#21183;&#20998;&#24067;&#30340;&#19978;&#38480;&#20272;&#35745;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#65292;&#28040;&#38500;&#20102;&#23545;&#39069;&#22806;&#24191;&#27867;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03777v1 Announce Type: cross  Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our me
&lt;/p&gt;</description></item><item><title>&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.03322</link><description>&lt;p&gt;
&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Configuration Performance Learning: A Systematic Survey and Taxonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03322
&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#21487;&#20197;&#35828;&#26159;&#21453;&#26144;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#26368;&#20851;&#38190;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29616;&#20195;&#36719;&#20214;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;&#21508;&#31181;&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#24615;&#33021;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#25104;&#20026;&#36719;&#20214;&#32500;&#25252;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#27809;&#26377;&#23545;&#36719;&#20214;&#31995;&#32479;&#26377;&#36879;&#24443;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#24314;&#27169;&#30340;&#65292;&#20027;&#35201;&#20381;&#36182;&#25968;&#25454;&#65292;&#36825;&#27491;&#22909;&#31526;&#21512;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;948&#31687;&#26469;&#33258;&#20845;&#20010;&#32034;&#24341;&#26381;&#21153;&#30340;&#35770;&#25991;&#65292;&#22522;&#20110;&#27492;&#25552;&#21462;&#24182;&#20998;&#26512;&#20102;85&#31687;&#20027;&#35201;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24635;&#32467;&#20102;&#37197;&#32622;&#25968;&#25454;&#22914;&#20309;&#20934;&#22791;&#65292;&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#26500;&#24314;&#65292;&#20197;&#21450;&#35813;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#35780;&#20272;&#31561;&#20851;&#38190;&#20027;&#39064;&#21644;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03322v1 Announce Type: cross  Abstract: Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.   In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evalu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.17376</link><description>&lt;p&gt;
&#20248;&#21270;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerating Diffusion Sampling with Optimized Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17376
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#37319;&#26679;&#27493;&#39588;&#65292;&#20854;&#37319;&#26679;&#25928;&#29575;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#36817;&#26399;&#39640;&#38454;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#22312;DPMs&#20013;&#30340;&#24212;&#29992;&#20351;&#24471;&#29992;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#39033;&#37325;&#22823;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#20173;&#28982;&#37319;&#29992;&#22343;&#21248;&#26102;&#38388;&#27493;&#38271;&#65292;&#32780;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#26102;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#20026;DPMs&#30340;&#29305;&#23450;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#27492;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#26368;&#23567;&#21270;&#22320;&#23454;&#29616;&#22320;&#30495;&#23454;&#35299;&#19982;&#19982;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#24212;&#30340;&#36817;&#20284;&#35299;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#21463;&#38480;&#20449;&#36182;&#22495;&#26041;&#27861;&#36827;&#34892;&#39640;&#25928;&#27714;&#35299;&#65292;&#26102;&#38388;&#23569;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17376v1 Announce Type: cross  Abstract: Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;DPO-Positive&#65288;DPOP&#65289;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#20013;&#28508;&#22312;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;</title><link>https://arxiv.org/abs/2402.13228</link><description>&lt;p&gt;
Smaug&#65306;&#20351;&#29992;DPO-Positive&#20462;&#22797;&#20559;&#22909;&#20248;&#21270;&#30340;&#22833;&#36133;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13228
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;DPO-Positive&#65288;DPOP&#65289;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#20013;&#28508;&#22312;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#22312;&#26174;&#33879;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#24635;&#32467;&#21644;&#23545;&#40784;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290; DPO&#20351;&#29992;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#25968;&#25454;&#23545;&#27169;&#22411;&#36873;&#25321;&#19968;&#20010;&#21709;&#24212;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#30340;&#8220;&#30456;&#23545;&#8221;&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#21482;&#35201;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#23545;&#27010;&#29575;&#22686;&#21152;&#65292;&#26631;&#20934;DPO&#25439;&#22833;&#23601;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23545;&#39318;&#36873;&#31034;&#20363;&#30340;&#21487;&#33021;&#24615;&#38477;&#20302;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#24403;&#22312;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;LLMs&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#23436;&#25104;&#20043;&#38388;&#30340;&#32534;&#36753;&#36317;&#31163;&#36739;&#30701;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#20250;&#20986;&#29616;&#36825;&#31181;&#29616;&#35937;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DPO-Positive&#65288;DPOP&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#36825;&#31181;&#22833;&#36133;&#27169;&#24335;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13228v1 Announce Type: cross  Abstract: Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO a
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#23545;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#20449;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#22312;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#26041;&#38754;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#12289;&#36741;&#21161;&#25216;&#26415;&#21644;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#31561;&#20116;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10086</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10086
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#23545;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#20449;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#22312;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#26041;&#38754;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#12289;&#36741;&#21161;&#25216;&#26415;&#21644;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#31561;&#20116;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20854;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#20219;&#21153;&#20013;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#30340;&#24212;&#29992;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38590;&#20197;&#29702;&#35299;&#30340;AI&#31995;&#32479;&#21152;&#21095;&#20102;&#23545;AD&#23433;&#20840;&#20445;&#35777;&#30340;&#29616;&#26377;&#25361;&#25112;&#12290;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#26041;&#27861;&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;AD&#20013;&#30340;&#20840;&#38754;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#22312;AD&#32972;&#26223;&#19979;AI&#30340;&#35201;&#27714;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#26426;&#26500;&#36825;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;XAI&#23545;&#20110;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;AI&#20013;&#35299;&#37322;&#30340;&#26469;&#28304;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;XAI&#30340;&#20998;&#31867;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;XAI&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;AD&#20013;&#30340;&#20116;&#20010;&#20027;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#65292;&#36741;&#21161;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10086v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) shows promising applications for the perception and planning tasks in autonomous driving (AD) due to its superior performance compared to conventional methods. However, inscrutable AI systems exacerbate the existing challenge of safety assurance of AD. One way to mitigate this challenge is to utilize explainable AI (XAI) techniques. To this end, we present the first comprehensive systematic literature review of explainable methods for safe and trustworthy AD. We begin by analyzing the requirements for AI in the context of AD, focusing on three key aspects: data, model, and agency. We find that XAI is fundamental to meeting these requirements. Based on this, we explain the sources of explanations in AI and describe a taxonomy of XAI. We then identify five key contributions of XAI for safe and trustworthy AI in AD, which are interpretable design, interpretable surrogate models, interpretable monitoring, auxil
&lt;/p&gt;</description></item><item><title>DistiLLM&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03898</link><description>&lt;p&gt;
DistiLLM: &#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DistiLLM: Towards Streamlined Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03898
&lt;/p&gt;
&lt;p&gt;
DistiLLM&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#23558;&#25945;&#24072;&#27169;&#22411;&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#65288;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;KD&#26041;&#27861;&#23384;&#22312;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#20351;&#29992;&#23398;&#29983;&#29983;&#25104;&#30340;&#36755;&#20986;&#26469;&#35299;&#20915;&#35757;&#32451;-&#25512;&#29702;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#20570;&#27861;&#26174;&#33879;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DistiLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#12290;DistiLLM&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#65292;&#25105;&#20204;&#25581;&#31034;&#24182;&#21033;&#29992;&#20102;&#23427;&#30340;&#29702;&#35770;&#23646;&#24615;&#65307;&#65288;2&#65289;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#21033;&#29992;&#23398;&#29983;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#25928;&#29575;&#12290;&#21253;&#25324;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#22312;&#20869;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;DistiLLM&#22312;&#26500;&#24314;&#39640;&#24615;&#33021;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03214</link><description>&lt;p&gt;
&#26377;&#26426;&#25110;&#25193;&#25955;&#65306;&#25105;&#20204;&#33021;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22270;&#20687;&#30340;&#20986;&#29616;&#23436;&#20840;&#39072;&#35206;&#20102;&#33402;&#26415;&#30028;&#12290;&#20174;&#20154;&#31867;&#33402;&#26415;&#20013;&#35782;&#21035;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20854;&#24433;&#21709;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19981;&#26029;&#22686;&#21152;&#12290;&#26410;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#32773;&#27450;&#35784;&#37027;&#20123;&#25903;&#20184;&#39640;&#20215;&#36141;&#20080;&#20154;&#31867;&#33402;&#26415;&#21697;&#30340;&#20010;&#20154;&#21644;&#31105;&#27490;&#20351;&#29992;AI&#22270;&#20687;&#30340;&#20844;&#21496;&#12290;&#36825;&#23545;&#20110;&#38656;&#35201;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#20197;&#36991;&#20813;&#28508;&#22312;&#27169;&#22411;&#23849;&#28291;&#30340;AI&#27169;&#22411;&#35757;&#32451;&#32773;&#26469;&#35828;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#22270;&#20687;&#30340;&#26041;&#27861;&#26377;&#22810;&#31181;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#65292;&#20197;&#21450;&#36890;&#36807;&#19987;&#19994;&#33402;&#26415;&#23478;&#21033;&#29992;&#20182;&#20204;&#23545;&#33402;&#26415;&#25216;&#24039;&#30340;&#30693;&#35782;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#33391;&#24615;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;7&#31181;&#39118;&#26684;&#30340;&#30495;&#23454;&#20154;&#31867;&#33402;&#26415;&#65292;&#20174;5&#20010;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20102;&#19982;&#20043;&#21305;&#37197;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20102;8&#20010;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors 
&lt;/p&gt;</description></item><item><title>&#20381;&#36182;&#22522;&#20934;&#25490;&#34892;&#27036;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23384;&#22312;&#36739;&#39640;&#25935;&#24863;&#24615;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#25490;&#21517;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#31572;&#26696;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01781</link><description>&lt;p&gt;
&#24403;&#22522;&#20934;&#25104;&#20026;&#30446;&#26631;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25490;&#34892;&#27036;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01781
&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#22522;&#20934;&#25490;&#34892;&#27036;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23384;&#22312;&#36739;&#39640;&#25935;&#24863;&#24615;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#25490;&#21517;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#31572;&#26696;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#20934;&#25490;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#25490;&#34892;&#27036;&#32463;&#24120;&#34987;&#29992;&#26469;&#25351;&#23548;&#23454;&#36341;&#32773;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#12290;&#36890;&#24120;&#65292;&#21457;&#24067;&#30340;&#25490;&#34892;&#27036;&#25490;&#21517;&#34987;&#30452;&#25509;&#25509;&#21463; - &#25105;&#20204;&#34920;&#26126;&#36825;&#26159;&#19968;&#20010;&#65288;&#28508;&#22312;&#26114;&#36149;&#30340;&#65289;&#38169;&#35823;&#12290;&#22312;&#29616;&#26377;&#30340;&#25490;&#34892;&#27036;&#19979;&#65292;LLM&#30340;&#30456;&#23545;&#24615;&#33021;&#23545;&#65288;&#36890;&#24120;&#24494;&#23567;&#30340;&#65289;&#32454;&#33410;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#27969;&#34892;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22522;&#20934;&#65288;&#20363;&#22914;MMLU&#65289;&#65292;&#23545;&#22522;&#20934;&#30340;&#24494;&#23567;&#25200;&#21160;&#65292;&#22914;&#25913;&#21464;&#36873;&#39033;&#39034;&#24207;&#25110;&#31572;&#26696;&#36873;&#25321;&#26041;&#27861;&#65292;&#20250;&#23548;&#33268;&#25490;&#21517;&#21464;&#21270;&#36798;&#21040;8&#20010;&#20301;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#25200;&#21160;&#31867;&#21035;&#36827;&#34892;&#31995;&#32479;&#23454;&#39564;&#24182;&#30830;&#23450;&#36825;&#19968;&#34892;&#20026;&#30340;&#26469;&#28304;&#26469;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#20248;&#21270;&#30340;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#36827;&#34892;&#31572;&#26696;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20381;&#36182;&#31616;&#21333;&#22522;&#20934;&#35780;&#20272;&#30340;&#39118;&#38505;&#65292;&#24182;&#20026;&#26356;&#20581;&#22766;&#30340;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#25351;&#23548;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#19982;&#23545;&#35805;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;DST&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2311.15623</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Injecting linguistic knowledge into BERT for Dialogue State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#19982;&#23545;&#35805;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;DST&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#25512;&#29702;&#36807;&#31243;&#32570;&#20047;&#36879;&#26126;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26694;&#26550;&#25552;&#21462;&#35821;&#35328;&#30693;&#35782;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#22686;&#24378;BERT&#22312;DST&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#30693;&#35782;&#25552;&#21462;&#36807;&#31243;&#35745;&#31639;&#32463;&#27982;&#39640;&#25928;&#65292;&#19981;&#38656;&#35201;&#27880;&#37322;&#25110;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27880;&#20837;&#25552;&#21462;&#30340;&#30693;&#35782;&#21482;&#38656;&#35201;&#28155;&#21152;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#12290;&#25105;&#20204;&#20351;&#29992;&#20984;&#22810;&#38754;&#20307;&#27169;&#22411;(CPM)&#20316;&#20026;DST&#20219;&#21153;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#65292;&#24182;&#34920;&#26126;&#25152;&#33719;&#21462;&#30340;&#29305;&#24449;&#19982;&#23545;&#35805;&#20013;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#12290;&#36825;&#31181;&#30456;&#20851;&#24615;&#26377;&#21161;&#20110;&#20840;&#38754;&#29702;&#35299;&#24433;&#21709;DST&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;DST&#20219;&#21153;&#19978;&#23545;&#36825;&#20010;&#26694;&#26550;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and ob
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#24037;&#31243;&#20219;&#21153;&#23545;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;PE2&#26041;&#27861;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#30340;&#27880;&#20837;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.05661</link><description>&lt;p&gt;
Prompt Engineering a Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering a Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#20219;&#21153;&#23545;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;PE2&#26041;&#27861;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#30340;&#27880;&#20837;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#26159;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#26816;&#26597;&#27169;&#22411;&#30340;&#38169;&#35823;&#65292;&#20551;&#35774;&#24403;&#21069;&#25552;&#31034;&#20013;&#32570;&#23569;&#25110;&#35823;&#23548;&#20102;&#20160;&#20040;&#65292;&#24182;&#28165;&#26224;&#22320;&#20256;&#36798;&#20219;&#21153;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#20803;&#25552;&#31034;&#26469;&#25191;&#34892;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#30001;&#20110;&#20803;&#25552;&#31034;&#20013;&#32570;&#20047;&#22797;&#26434;&#25512;&#29702;&#30340;&#20805;&#20998;&#25351;&#23548;&#65292;&#23427;&#20204;&#30340;&#28508;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#27880;&#20837;&#21040;&#20803;&#25552;&#31034;&#20013;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25152;&#24471;&#21040;&#30340;&#26041;&#27861;&#31216;&#20026;PE2&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20219;&#21153;&#20013;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#12290;&#23427;&#25214;&#21040;&#30340;&#25552;&#31034;&#22312;MultiArith&#19978;&#27604;&#8220;&#25353;&#27493;&#39588;&#24605;&#32771;&#8221;&#39640;&#20986;6.3%&#65292;&#22312;GSM8K&#19978;&#39640;&#20986;3.1%&#65292;&#24182;&#22312;&#23545;&#31435;&#20219;&#21153;&#19978;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05661v2 Announce Type: replace-cross  Abstract: Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, showcases remarkable versatility across diverse language tasks. It finds prompts that outperform "let's think step by step" by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#20026;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2401.15935</link><description>&lt;p&gt;
&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65306;&#29983;&#25104;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning. (arXiv:2401.15935v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#20026;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#33719;&#21462;&#20107;&#20214;&#24207;&#21015;&#34920;&#31034;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#36825;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#27169;&#24577;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#38134;&#34892;&#12289;&#30005;&#23376;&#21830;&#21153;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#25105;&#20204;&#23545;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20998;&#21035;&#24212;&#29992;&#20102;&#23427;&#20204;&#12290;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#32477;&#23545;&#20248;&#36234;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32467;&#21512;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#20316;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#65292;&#20174;&#24403;&#20195;&#22810;&#27169;&#24577;&#30740;&#31350;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#26041;&#27861;&#36890;&#24120;&#34987;&#35270;&#20026;&#20114;&#26021;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#23427;&#20204;&#30340;&#32852;&#21512;&#25506;&#32034;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#33267;&#23569;&#19982;&#29616;&#26377;&#26041;&#27861;&#25345;&#24179;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#26356;&#21152;&#26222;&#36866;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates self-supervised learning techniques to obtain representations of Event Sequences. It is a key modality in various applications, including but not limited to banking, e-commerce, and healthcare.  We perform a comprehensive study of generative and contrastive approaches in self-supervised learning, applying them both independently. We find that there is no single supreme method. Consequently, we explore the potential benefits of combining these approaches. To achieve this goal, we introduce a novel method that aligns generative and contrastive embeddings as distinct modalities, drawing inspiration from contemporary multimodal research.  Generative and contrastive approaches are often treated as mutually exclusive, leaving a gap for their combined exploration. Our results demonstrate that this aligned model performs at least on par with, and mostly surpasses, existing methods and is more universal across a variety of tasks. Furthermore, we demonstrate that self-sup
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;transformer&#32593;&#32476;&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#30382;&#23618;&#27874;&#22312;&#25552;&#21462;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14267</link><description>&lt;p&gt;
Transformers&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#65306;&#22312;&#26102;&#38388;&#19978;&#20256;&#36882;&#19978;&#19979;&#25991;&#30340;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers and Cortical Waves: Encoders for Pulling In Context Across Time. (arXiv:2401.14267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;transformer&#32593;&#32476;&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#30382;&#23618;&#27874;&#22312;&#25552;&#21462;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;ChatGPT&#21644;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;transformer&#32593;&#32476;&#30340;&#33021;&#21147;&#24050;&#32463;&#24341;&#36215;&#20102;&#19990;&#30028;&#30340;&#20851;&#27880;&#12290;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#23558;&#23436;&#25972;&#30340;&#36755;&#20837;&#24207;&#21015;&#65288;&#20363;&#22914;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#21333;&#35789;&#65289;&#36716;&#21270;&#20026;&#19968;&#20010;&#38271;&#30340;&#8220;&#32534;&#30721;&#21521;&#37327;&#8221;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#23398;&#20064;&#33258;&#28982;&#24207;&#21015;&#20013;&#30340;&#38271;&#31243;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#8220;&#33258;&#27880;&#24847;&#21147;&#8221;&#24212;&#29992;&#20110;&#36825;&#20010;&#32534;&#30721;&#21521;&#37327;&#65292;&#36890;&#36807;&#35745;&#31639;&#36755;&#20837;&#24207;&#21015;&#20013;&#21333;&#35789;&#23545;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#22686;&#24378;&#20102;transformer&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#35748;&#20026;&#31070;&#32463;&#27963;&#21160;&#22312;&#21333;&#20010;&#30382;&#23618;&#21306;&#22495;&#20869;&#25110;&#25972;&#20010;&#22823;&#33041;&#33539;&#22260;&#20869;&#20256;&#25773;&#30340;&#27874;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#32534;&#30721;&#21407;&#29702;&#12290;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#21051;&#23558;&#26368;&#36817;&#30340;&#36755;&#20837;&#21382;&#21490;&#23553;&#35013;&#20026;&#21333;&#20010;&#31354;&#38388;&#27169;&#24335;&#65292;&#30382;&#23618;&#27874;&#21487;&#20197;&#20174;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#25552;&#21462;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#36825;&#19982;&#35745;&#31639;&#21407;&#29702;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention. The crucial computational mechanism underlying their performance relies on transforming a complete input sequence - for example, all the words in a sentence into a long "encoding vector" - that allows transformers to learn long-range temporal dependencies in naturalistic sequences. Specifically, "self-attention" applied to this encoding vector enhances temporal context in transformers by computing associations between pairs of words in the input sequence. We suggest that waves of neural activity, traveling across single cortical regions or across multiple regions at the whole-brain scale, could implement a similar encoding principle. By encapsulating recent input history into a single spatial pattern at each moment in time, cortical waves may enable temporal context to be extracted from sequences of sensory inputs, the same computational principle used in
&lt;/p&gt;</description></item><item><title>LauraGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.04673</link><description>&lt;p&gt;
LauraGPT&#65306;&#20351;&#29992;GPT&#36827;&#34892;&#21548;&#12289;&#20851;&#27880;&#12289;&#29702;&#35299;&#21644;&#20877;&#29983;&#38899;&#39057;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT. (arXiv:2310.04673v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04673
&lt;/p&gt;
&lt;p&gt;
LauraGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#31867;&#20284;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#38899;&#39057;&#20219;&#21153;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#20197;&#21069;&#25552;&#20986;&#30340;&#29992;&#20110;&#38899;&#39057;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#32570;&#20047;&#20805;&#20998;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#35201;&#20040;&#23616;&#38480;&#20110;&#35782;&#21035;&#21644;&#29702;&#35299;&#38899;&#39057;&#20869;&#23481;&#30340;&#20219;&#21153;&#65292;&#35201;&#20040;&#26126;&#26174;&#19981;&#21450;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65288;SOTA&#65289;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LauraGPT&#65292;&#19968;&#20010;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#32479;&#19968;GPT&#27169;&#22411;&#12290;LauraGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#38899;&#39057;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#22312;&#20219;&#24847;&#27169;&#24335;&#19979;&#29983;&#25104;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#36827;&#34892;&#19982;&#20869;&#23481;&#12289;&#35821;&#20041;&#12289;&#35821;&#38899;&#23398;&#21644;&#38899;&#39057;&#20449;&#21495;&#20998;&#26512;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20123;&#20540;&#24471;&#27880;&#24847;&#30340;&#20219;&#21153;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#38899;&#22686;&#24378;&#12289;&#33258;&#21160;&#38899;&#39057;&#25429;&#33719;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks. However, there has been limited research on applying similar frameworks to audio tasks. Previously proposed large language models for audio tasks either lack sufficient quantitative evaluations, or are limited to tasks for recognizing and understanding audio content, or significantly underperform existing state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified GPT model for audio recognition, understanding, and generation. LauraGPT is a versatile language model that can process both audio and text inputs and generate outputs in either modalities. It can perform a wide range of tasks related to content, semantics, paralinguistics, and audio-signal analysis. Some of its noteworthy tasks include automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio capt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.04218</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;&#20063;&#31216;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#65289;&#26159;&#32534;&#30721;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#22312;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#65292;&#38543;&#26426;&#21464;&#37327;&#34987;&#24314;&#27169;&#20026;&#26377;&#21521;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#19988;&#35268;&#23450;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#22312;&#32473;&#23450;&#20854;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#19982;&#20854;&#31062;&#20808;&#33410;&#28857;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21516;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#21487;&#20197;&#20934;&#30830;&#32534;&#30721;&#30456;&#21516;&#30340;&#19968;&#32452;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#65292;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31561;&#20215;&#31867;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#65288;MEC&#65289;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#23545;&#20110;MEC&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20123;&#32654;&#20029;&#30340;&#32452;&#21512;&#29305;&#24449;&#65292;&#24182;&#19988;&#24050;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#21516;&#19968;MEC&#20013;&#30340;&#25152;&#26377;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#24517;&#39035;&#20855;&#26377;&#30456;&#21516;&#30340;&#8220;&#39592;&#26550;&#8221;&#65288;&#24213;&#23618;&#26080;&#21521;&#22270;&#65289;&#21644;v-&#32467;&#26500;&#65288;&#24418;&#24335;&#20026;$a\rightarrow b \leftarrow c$&#30340;&#35825;&#23548;&#23376;&#22270;&#65289;&#12290;&#36825;&#20123;&#32452;&#21512;&#29305;&#24449;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#33258;&#28982;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.00081</link><description>&lt;p&gt;
&#20026;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26500;&#24314;&#35821;&#20041;&#20016;&#23500;&#30340;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Semantically Enriched Embeddings for Knowledge Graph Completion. (arXiv:2308.00081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#31639;&#27861;&#23558;&#30693;&#35782;&#22270;&#35889;&#35270;&#20026;&#19968;&#20010;&#22810;&#21521;&#26631;&#35760;&#22270;&#65292;&#32570;&#20047;&#25429;&#25417;&#24213;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25429;&#33719;&#20102;&#22823;&#37327;&#20449;&#24687;&#65292;&#36825;&#19968;&#25429;&#33719;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;&#30693;&#35782;&#22270;&#35889;&#21487;&#20197;&#20174;LLMs&#20013;&#21463;&#30410;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#19981;&#21516;&#29983;&#25104;&#23884;&#20837;&#27169;&#22411;&#21464;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#21508;&#31181;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#65292;&#22914;&#36716;&#23548;&#21644;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20197;&#21450;&#23454;&#20307;&#31867;&#22411;&#39044;&#27979;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#31867;&#22411;&#20449;&#24687;&#12289;LLMs&#20197;&#21450;&#25429;&#25417;&#19981;&#21516;&#25551;&#36848;&#36923;&#36753;&#20844;&#29702;&#20013;&#30340;&#35821;&#20041;&#30340;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#31639;&#27861;&#30340;&#20851;&#38190;&#21453;&#24605;&#23545;&#35770;&#25991;&#36827;&#34892;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding based Knowledge Graph (KG) Completion has gained much attention over the past few years. Most of the current algorithms consider a KG as a multidirectional labeled graph and lack the ability to capture the semantics underlying the schematic information. In a separate development, a vast amount of information has been captured within the Large Language Models (LLMs) which has revolutionized the field of Artificial Intelligence. KGs could benefit from these LLMs and vice versa. This vision paper discusses the existing algorithms for KG completion based on the variations for generating KG embeddings. It starts with discussing various KG completion algorithms such as transductive and inductive link prediction and entity type prediction algorithms. It then moves on to the algorithms utilizing type information within the KGs, LLMs, and finally to algorithms capturing the semantics represented in different description logic axioms. We conclude the paper with a critical reflection on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRD&#31639;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#33258;&#25105;&#25552;&#21319;&#21644;&#20301;&#32622;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02762</link><description>&lt;p&gt;
PRD: &#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations. (arXiv:2307.02762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRD&#31639;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#33258;&#25105;&#25552;&#21319;&#21644;&#20301;&#32622;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#22238;&#31572;&#36136;&#37327;&#22312;&#33258;&#21160;&#21270;&#26041;&#38754;&#24456;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#24182;&#20027;&#35201;&#20351;&#29992;LLMs&#20316;&#20026;&#26080;&#21442;&#32771;&#24230;&#37327;&#34913;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21442;&#32771;&#25351;&#26631;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20182;&#20204;&#20197;&#34987;&#35748;&#20026;&#26159;&#8220;&#26368;&#24378;&#8221;&#30340;LLM&#20316;&#20026;&#35780;&#20272;&#22120;&#65292;&#23545;&#20505;&#36873;&#27169;&#22411;&#30340;&#31572;&#26696;&#36827;&#34892;&#20004;&#20004;&#27604;&#36739;&#24182;&#25552;&#20379;&#25490;&#21517;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#23384;&#22312;&#22810;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#24102;&#26469;&#33258;&#25105;&#25552;&#21319;&#65288;&#38738;&#30544;&#33258;&#24049;&#30340;&#31572;&#26696;&#65289;&#21644;&#20301;&#32622;&#20559;&#35265;&#12290;&#25105;&#20204;&#20174;&#25945;&#32946;&#39046;&#22495;&#65288;Cho and MacArthur, 2011&#65307;Walsh, 2014&#65289;&#20013;&#27762;&#21462;&#35265;&#35299;&#21644;&#25945;&#35757;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#21516;&#34892;&#35780;&#32423;&#65288;PR&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32771;&#34385;&#27599;&#20010;&#21516;&#34892;LLM&#23545;&#25152;&#26377;&#31572;&#26696;&#23545;&#30340;&#20004;&#20004;&#20559;&#22909;&#65292;&#24182;&#36755;&#20986;&#27169;&#22411;&#30340;&#26368;&#32456;&#25490;&#21517;&#65307;&#20197;&#21450;&#65288;2&#65289;&#21516;&#34892;&#35752;&#35770;&#65288;PD&#65289;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#20419;&#20351;&#20004;&#20010;LLMs&#36827;&#34892;&#35752;&#35770;&#24182;&#23581;&#35797;&#23601;&#20004;&#20010;&#20559;&#22909;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the quality of responses generated by different modern large language models (LLMs) are hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs as a reference-free metric for open-ended question answering. More specifically, they use the recognized "strongest" LLM as the evaluator, which conducts pairwise comparisons of candidate models' answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose the (1) peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on preferences of two an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24503;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20984;&#21253;&#26368;&#20415;&#23452;&#25554;&#20837;&#21551;&#21457;&#24335;&#35299;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#32553;&#25918;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#30340;&#28857;&#36817;&#20284;&#21040;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65292;&#29983;&#25104;&#20102;&#21021;&#22987;&#21270;&#31639;&#27861;&#30340;&#20984;&#21253;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;&#35813;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#26368;&#37051;&#36817;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.06582</link><description>&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24503;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20984;&#21253;&#26368;&#20415;&#23452;&#25554;&#20837;&#21551;&#21457;&#24335;&#35299;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Convex Hull Cheapest Insertion Heuristic for the Non-Euclidean TSP. (arXiv:2302.06582v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24503;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20984;&#21253;&#26368;&#20415;&#23452;&#25554;&#20837;&#21551;&#21457;&#24335;&#35299;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#32553;&#25918;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#30340;&#28857;&#36817;&#20284;&#21040;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65292;&#29983;&#25104;&#20102;&#21021;&#22987;&#21270;&#31639;&#27861;&#30340;&#20984;&#21253;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;&#35813;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#26368;&#37051;&#36817;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20984;&#21253;&#26368;&#20415;&#23452;&#25554;&#20837;&#21551;&#21457;&#24335;&#31639;&#27861;&#21487;&#20197;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#20135;&#29983;&#33391;&#22909;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36824;&#26410;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#24773;&#20917;&#19979;&#36827;&#34892;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#22788;&#29702;&#38556;&#30861;&#29289;&#30340;&#22256;&#38590;&#65292;&#25552;&#20986;&#30340;&#25913;&#36827;&#26041;&#27861;&#20351;&#29992;&#22810;&#32500;&#32553;&#25918;&#23558;&#36825;&#20123;&#28857;&#39318;&#20808;&#36817;&#20284;&#21040;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#21021;&#22987;&#21270;&#31639;&#27861;&#30340;&#20984;&#21253;&#12290;&#36890;&#36807;&#20462;&#25913;TSPLIB&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21521;&#20854;&#20013;&#28155;&#21152;&#19981;&#21487;&#36890;&#36807;&#30340;&#20998;&#21106;&#22120;&#26469;&#20135;&#29983;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;&#22312;&#25152;&#30740;&#31350;&#30340;&#26696;&#20363;&#20013;&#65292;&#35813;&#31639;&#27861;&#34920;&#29616;&#20986;&#20248;&#20110;&#24120;&#29992;&#30340;&#26368;&#37051;&#36817;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;96%&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convex hull cheapest insertion heuristic is known to generate good solutions to the Traveling Salesperson Problem in Euclidean spaces, but it has not been extended to the non-Euclidean case. To address the difficulty of dealing with obstacles in the non-Euclidean space, the proposed adaptation uses multidimensional scaling to first approximate these points in a Euclidean space, thereby enabling the generation of the convex hull that initializes the algorithm. To evaluate the proposed algorithm, the TSPLIB benchmark data-set is modified by adding impassable separators that produce non-Euclidean spaces. The algorithm is demonstrated to outperform the commonly used Nearest Neighbor algorithm in 96% of the cases studied.
&lt;/p&gt;</description></item></channel></rss>