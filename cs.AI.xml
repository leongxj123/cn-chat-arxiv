<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2404.00656</link><description>&lt;p&gt;
WavLLM&#65306;&#38754;&#21521;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WavLLM: Towards Robust and Adaptive Speech Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00656
&lt;/p&gt;
&lt;p&gt;
WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36880;&#28176;&#25299;&#23485;&#20102;&#23427;&#20204;&#30340;&#33539;&#22260;&#21040;&#22810;&#27169;&#24577;&#24863;&#30693;&#21644;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#21548;&#35273;&#33021;&#21147;&#25972;&#21512;&#21040;LLMs&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#36328;&#19981;&#21516;&#35821;&#22659;&#21644;&#25191;&#34892;&#22797;&#26434;&#21548;&#35273;&#20219;&#21153;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WavLLM&#65292;&#19968;&#20010;&#20855;&#26377;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#30340;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#21033;&#29992;Whisper&#32534;&#30721;&#22120;&#22788;&#29702;&#35821;&#38899;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#21033;&#29992;WavLM&#32534;&#30721;&#22120;&#25429;&#25417;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#22312;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;WavLLM&#39318;&#20808;&#36890;&#36807;&#28151;&#21512;&#35201;&#32032;&#36827;&#34892;&#20248;&#21270;&#26469;&#24314;&#31435;&#20854;&#22522;&#30784;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00656v1 Announce Type: cross  Abstract: The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elemen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#21644;&#20462;&#21098;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.19631</link><description>&lt;p&gt;
&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26816;&#32034;&#22686;&#24378;&#30693;&#35782;&#32534;&#36753;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#21644;&#20462;&#21098;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#39640;&#25928;&#33021;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#25972;&#21512;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#65292;&#23548;&#33268;&#21487;&#33021;&#36807;&#26102;&#25110;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#24403;&#22788;&#29702;&#22810;&#36339;&#38382;&#39064;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;LLMs&#26356;&#26032;&#21644;&#25972;&#21512;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#30693;&#35782;&#29255;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#23450;&#21046;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#12290;RAE&#39318;&#20808;&#26816;&#32034;&#32534;&#36753;&#21518;&#30340;&#20107;&#23454;&#65292;&#28982;&#21518;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#23436;&#21892;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26816;&#32034;&#26041;&#27861;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#26469;&#35782;&#21035;&#38142;&#24335;&#20107;&#23454;&#65292;&#32780;&#22825;&#30495;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25628;&#32034;&#21487;&#33021;&#20250;&#24573;&#30053;&#36825;&#20123;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#37319;&#29992;&#20102;&#20462;&#21098;&#31574;&#30053;&#65292;&#20174;&#26816;&#32034;&#21040;&#30340;&#20107;&#23454;&#20013;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#65292;&#36825;&#22686;&#24378;&#20102;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19631v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that na\"ive similarity-based searches might miss. Additionally, our framework incorporates a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the edi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20351;&#29992;&#26469;&#33258;&#21363;&#25554;&#21363;&#29992;&#30340;&#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#20809;&#32447;&#21453;&#23556;&#21457;&#29983;&#30340;&#34920;&#38754;&#37325;&#26032;&#26500;&#24314;&#20026;&#34394;&#25311;&#38236;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#33719;&#21462;&#38544;&#34255;&#22330;&#26223;&#28145;&#24230;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19376</link><description>&lt;p&gt;
NIGHT -- &#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#25968;&#25454;&#30340;&#38750;&#35270;&#36317;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20351;&#29992;&#26469;&#33258;&#21363;&#25554;&#21363;&#29992;&#30340;&#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#20809;&#32447;&#21453;&#23556;&#21457;&#29983;&#30340;&#34920;&#38754;&#37325;&#26032;&#26500;&#24314;&#20026;&#34394;&#25311;&#38236;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#33719;&#21462;&#38544;&#34255;&#22330;&#26223;&#28145;&#24230;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38750;&#35270;&#35282;&#30456;&#26426;&#22806;&#37096;&#33719;&#21462;&#29289;&#20307;&#26159;&#19968;&#20010;&#38750;&#24120;&#24341;&#20154;&#27880;&#30446;&#20294;&#20063;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21033;&#29992;&#23450;&#21046;&#30340;&#30452;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#20135;&#29983;&#30340;&#30636;&#26102;&#25104;&#20687;&#25968;&#25454;&#65292;&#36825;&#20010;&#24819;&#27861;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20351;&#29992;&#26469;&#33258;&#21363;&#25554;&#21363;&#29992;&#30340;&#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#30828;&#20214;&#35201;&#27714;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#20809;&#32447;&#21453;&#23556;&#21457;&#29983;&#30340;&#34920;&#38754;&#37325;&#26032;&#26500;&#24314;&#20026;&#34394;&#25311;&#38236;&#23376;&#12290;&#36825;&#31181;&#24314;&#27169;&#20351;&#24471;&#20219;&#21153;&#26356;&#23481;&#26131;&#22788;&#29702;&#65292;&#20063;&#26377;&#21161;&#20110;&#26500;&#24314;&#24102;&#26377;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20174;&#33719;&#24471;&#30340;&#25968;&#25454;&#20013;&#65292;&#21487;&#20197;&#24674;&#22797;&#38544;&#34255;&#22330;&#26223;&#30340;&#28145;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#39318;&#21019;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#24819;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19376v1 Announce Type: cross  Abstract: The acquisition of objects outside the Line-of-Sight of cameras is a very intriguing but also extremely challenging research topic. Recent works showed the feasibility of this idea exploiting transient imaging data produced by custom direct Time of Flight sensors. In this paper, for the first time, we tackle this problem using only data from an off-the-shelf indirect Time of Flight sensor without any further hardware requirement. We introduced a Deep Learning model able to re-frame the surfaces where light bounces happen as a virtual mirror. This modeling makes the task easier to handle and also facilitates the construction of annotated training data. From the obtained data it is possible to retrieve the depth information of the hidden scene. We also provide a first-in-its-kind synthetic dataset for the task and demonstrate the feasibility of the proposed idea over it.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;SAT&#32534;&#30721;&#30340;&#20559;&#24207;&#27169;&#22411;&#29992;&#20110;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#23545;&#24102;&#23485;&#30528;&#33394;&#38382;&#39064;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.15961</link><description>&lt;p&gt;
SAT&#32534;&#30721;&#30340;&#20559;&#24207;&#27169;&#22411;&#29992;&#20110;&#22270;&#30528;&#33394;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SAT Encoding of Partial Ordering Models for Graph Coloring Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15961
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;SAT&#32534;&#30721;&#30340;&#20559;&#24207;&#27169;&#22411;&#29992;&#20110;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#23545;&#24102;&#23485;&#30528;&#33394;&#38382;&#39064;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20559;&#24207;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#27169;&#22411;&#30340;&#22270;&#30528;&#33394;&#38382;&#39064;&#65288;GCP&#65289;&#21644;&#24102;&#23485;&#30528;&#33394;&#38382;&#39064;&#65288;BCP&#65289;&#30340;&#26032;SAT&#32534;&#30721;&#12290; GCP&#35201;&#27714;&#32473;&#23450;&#22270;&#30340;&#39030;&#28857;&#20998;&#37197;&#26368;&#23569;&#25968;&#37327;&#30340;&#39068;&#33394;&#65292;&#20197;&#20415;&#27599;&#20004;&#20010;&#30456;&#37051;&#30340;&#39030;&#28857;&#24471;&#21040;&#19981;&#21516;&#30340;&#39068;&#33394;&#12290; BCP&#26159;&#19968;&#20010;&#27867;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#26465;&#36793;&#37117;&#26377;&#19968;&#20010;&#26435;&#37325;&#65292;&#35201;&#27714;&#20998;&#37197;&#30340;&#39068;&#33394;&#20043;&#38388;&#26377;&#26368;&#23567;&#30340;&#8220;&#36317;&#31163;&#8221;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#20351;&#29992;&#30340;&#8220;&#26368;&#22823;&#8221;&#39068;&#33394;&#12290; &#23545;&#20110;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;GCP&#65292;&#25105;&#20204;&#22312;DIMACS&#22522;&#20934;&#38598;&#19978;&#23454;&#39564;&#27604;&#36739;&#20102;&#25105;&#20204;&#26032;&#30340;SAT&#32534;&#30721;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290; &#25105;&#20204;&#30340;&#35780;&#20272;&#35777;&#23454;&#65292;&#36825;&#31181;SAT&#32534;&#30721;&#23545;&#20110;&#31232;&#30095;&#22270;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#29978;&#33267;&#22312;&#19968;&#20123;DIMACS&#31034;&#20363;&#19978;&#32988;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290; &#23545;&#20110;BCP&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22522;&#20110;&#20559;&#24207;&#30340;SAT&#21644;ILP&#20844;&#24335;&#30340;&#22823;&#23567;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#23567;&#20110;&#32463;&#20856;&#30340;&#35299;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15961v1 Announce Type: new  Abstract: In this paper, we suggest new SAT encodings of the partial-ordering based ILP model for the graph coloring problem (GCP) and the bandwidth coloring problem (BCP). The GCP asks for the minimum number of colors that can be assigned to the vertices of a given graph such that each two adjacent vertices get different colors. The BCP is a generalization, where each edge has a weight that enforces a minimal "distance" between the assigned colors, and the goal is to minimize the "largest" color used. For the widely studied GCP, we experimentally compare our new SAT encoding to the state-of-the-art approaches on the DIMACS benchmark set. Our evaluation confirms that this SAT encoding is effective for sparse graphs and even outperforms the state-of-the-art on some DIMACS instances. For the BCP, our theoretical analysis shows that the partial-ordering based SAT and ILP formulations have an asymptotically smaller size than that of the classical assi
&lt;/p&gt;</description></item><item><title>ADEdgeDrop&#25552;&#20986;&#20102;&#19968;&#31181;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#36793;&#32536;&#39044;&#27979;&#22120;&#25351;&#23548;&#36793;&#32536;&#21024;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09171</link><description>&lt;p&gt;
ADEdgeDrop&#65306;&#29992;&#20110;&#24378;&#20581;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;
&lt;/p&gt;
&lt;p&gt;
ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09171
&lt;/p&gt;
&lt;p&gt;
ADEdgeDrop&#25552;&#20986;&#20102;&#19968;&#31181;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#36793;&#32536;&#39044;&#27979;&#22120;&#25351;&#23548;&#36793;&#32536;&#21024;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#21508;&#31181;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#23637;&#31034;&#20102;&#20174;&#37051;&#36817;&#33410;&#28857;&#25910;&#38598;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#21644;&#20887;&#20313;&#30340;&#22270;&#25968;&#25454;&#36896;&#25104;&#30340;&#24046;&#30340;&#27867;&#21270;&#21644;&#33030;&#24369;&#30340;&#31283;&#20581;&#24615;&#38480;&#21046;&#20102;GNNs&#30340;&#24615;&#33021;&#12290;&#22312;Graph Augmentation Learning&#65288;GAL&#65289;&#20013;&#65292;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;GNNs&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#21024;&#38500;&#36793;&#32536;&#36890;&#24120;&#20250;&#32469;&#36807;&#20851;&#38190;&#36793;&#32536;&#65292;&#20174;&#32780;&#21066;&#24369;&#28040;&#24687;&#20256;&#36882;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#65288;ADEdgeDrop&#65289;&#65292;&#21033;&#29992;&#25932;&#23545;&#36793;&#32536;&#39044;&#27979;&#22120;&#24341;&#23548;&#36793;&#32536;&#21024;&#38500;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#25972;&#21512;&#21040;&#19981;&#21516;&#30340;GNN&#20027;&#24178;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09171v1 Announce Type: cross  Abstract: Although Graph Neural Networks (GNNs) have exhibited the powerful ability to gather graph-structured information from neighborhood nodes via various message-passing mechanisms, the performance of GNNs is limited by poor generalization and fragile robustness caused by noisy and redundant graph data. As a prominent solution, Graph Augmentation Learning (GAL) has recently received increasing attention. Among prior GAL approaches, edge-dropping methods that randomly remove edges from a graph during training are effective techniques to improve the robustness of GNNs. However, randomly dropping edges often results in bypassing critical edges, consequently weakening the effectiveness of message passing. In this paper, we propose a novel adversarial edge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor guiding the removal of edges, which can be flexibly incorporated into diverse GNN backbones. Employing an adversarial 
&lt;/p&gt;</description></item><item><title>RepoHyper&#25552;&#20986;&#20102;Repo&#32423;&#35821;&#20041;&#22270;&#65288;RSG&#65289;&#21644;Expand&#21644;Refine&#26816;&#32034;&#26041;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#28385;&#36275;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30340;&#38656;&#27714;</title><link>https://arxiv.org/abs/2403.06095</link><description>&lt;p&gt;
RepoHyper&#65306;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#26159;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06095
&lt;/p&gt;
&lt;p&gt;
RepoHyper&#25552;&#20986;&#20102;Repo&#32423;&#35821;&#20041;&#22270;&#65288;RSG&#65289;&#21644;Expand&#21644;Refine&#26816;&#32034;&#26041;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#28385;&#36275;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06095v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CodeLLMs&#65289;&#22312;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#23436;&#20840;&#29702;&#35299;&#39033;&#30446;&#20179;&#24211;&#30340;&#24191;&#27867;&#19978;&#19979;&#25991;&#65292;&#27604;&#22914;&#30456;&#20851;&#25991;&#20214;&#21644;&#31867;&#23618;&#27425;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#34917;&#20840;&#19981;&#22815;&#31934;&#30830;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepoHyper&#65292;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#19982;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30456;&#20851;&#30340;&#22797;&#26434;&#25361;&#25112;&#30340;&#22810;&#26041;&#38754;&#26694;&#26550;&#12290;RepoHyper&#30340;&#26680;&#24515;&#26159;Repo&#32423;&#35821;&#20041;&#22270;&#65288;RSG&#65289;&#65292;&#19968;&#31181;&#23553;&#35013;&#20195;&#30721;&#20179;&#24211;&#24191;&#27867;&#19978;&#19979;&#25991;&#30340;&#26032;&#39062;&#35821;&#20041;&#22270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;RepoHyper&#21033;&#29992;&#25193;&#23637;&#21644;&#32454;&#21270;&#26816;&#32034;&#26041;&#27861;&#65292;&#21253;&#25324;&#24212;&#29992;&#20110;RSG&#30340;&#22270;&#25193;&#23637;&#21644;&#38142;&#25509;&#39044;&#27979;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#30456;&#20851;&#20195;&#30721;&#29255;&#27573;&#30340;&#26377;&#25928;&#26816;&#32034;&#21644;&#20248;&#20808;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;RepoHyper&#22312;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06095v1 Announce Type: cross  Abstract: Code Large Language Models (CodeLLMs) have demonstrated impressive proficiency in code completion tasks. However, they often fall short of fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions. To overcome these limitations, we present RepoHyper, a multifaceted framework designed to address the complex challenges associated with repository-level code completion. Central to RepoHyper is the Repo-level Semantic Graph (RSG), a novel semantic graph structure that encapsulates the vast context of code repositories. Furthermore, RepoHyper leverages Expand and Refine retrieval method, including a graph expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets. Our evaluations show that RepoHyper markedly outperforms existing techniques in re
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#32622;&#20195;&#29702;&#30340;&#32593;&#32476;&#21644;&#25552;&#28860;&#30693;&#35782;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05066</link><description>&lt;p&gt;
&#22797;&#20301;&#21644;&#25552;&#28860;&#65306;&#20811;&#26381;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reset &amp; Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05066
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#32622;&#20195;&#29702;&#30340;&#32593;&#32476;&#21644;&#25552;&#28860;&#30693;&#35782;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#21457;&#23637;&#26377;&#25928;&#30340;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#31639;&#27861;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#24403;&#38656;&#35201;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#21457;&#29983;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#38382;&#39064;&#22312;CRL&#20013;&#32463;&#24120;&#23384;&#22312;&#65292;&#24182;&#19988;&#26080;&#27861;&#36890;&#36807;&#26368;&#36817;&#19968;&#20123;&#26088;&#22312;&#20943;&#36731;RL&#20195;&#29702;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#30340;&#24037;&#20316;&#26469;&#26377;&#25928;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20811;&#26381;CRL&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;R&amp;D&#32467;&#21512;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#21363;&#37325;&#32622;&#20195;&#29702;&#30340;&#22312;&#32447;&#28436;&#21592;&#21644;&#35780;&#35770;&#32593;&#32476;&#20197;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20197;&#21450;&#31163;&#32447;&#23398;&#20064;&#27493;&#39588;&#65292;&#29992;&#20110;&#25552;&#28860;&#22312;&#32447;&#28436;&#21592;&#21644;&#20197;&#21069;&#19987;&#23478;&#21160;&#20316;&#27010;&#29575;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;Meta-World&#20219;&#21153;&#30340;&#38271;&#24207;&#21015;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#26368;&#36817;&#30340;&#22522;&#32447;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05066v1 Announce Type: cross  Abstract: We argue that one of the main obstacles for developing effective Continual Reinforcement Learning (CRL) algorithms is the negative transfer issue occurring when the new task to learn arrives. Through comprehensive experimental validation, we demonstrate that such issue frequently exists in CRL and cannot be effectively addressed by several recent work on mitigating plasticity loss of RL agents. To that end, we develop Reset &amp; Distill (R&amp;D), a simple yet highly effective method, to overcome the negative transfer problem in CRL. R&amp;D combines a strategy of resetting the agent's online actor and critic networks to learn a new task and an offline learning step for distilling the knowledge from the online actor and previous expert's action probabilities. We carried out extensive experiments on long sequence of Meta-World tasks and show that our method consistently outperforms recent baselines, achieving significantly higher success rates acr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00816</link><description>&lt;p&gt;
CFRet-DVQA&#65306;&#31895;&#21040;&#31934;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#29992;&#20110;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#65288;DVQA&#65289;&#26159;&#19968;&#20010;&#28041;&#21450;&#26681;&#25454;&#22270;&#20687;&#20869;&#23481;&#22238;&#31572;&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#24037;&#20316;&#20165;&#38480;&#20110;&#23450;&#20301;&#21333;&#39029;&#20869;&#30340;&#20449;&#24687;&#65292;&#19981;&#25903;&#25345;&#36328;&#39029;&#38754;&#38382;&#31572;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#23545;&#27169;&#22411;&#36755;&#20837;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#37096;&#20998;&#34987;&#25130;&#26029;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#23398;&#65292;&#31216;&#20026;CFRet-DVQA&#65292;&#37325;&#28857;&#25918;&#22312;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#19978;&#65292;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#19982;&#25152;&#25552;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#29255;&#27573;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#25991;&#26723;&#26631;&#31614;&#30340;&#39118;&#26684;&#30456;&#31526;&#12290;&#23454;&#39564;&#28436;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00816v1 Announce Type: cross  Abstract: Document Visual Question Answering (DVQA) is a task that involves responding to queries based on the content of images. Existing work is limited to locating information within a single page and does not facilitate cross-page question-and-answer interaction. Furthermore, the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer. In this study, we introduce a simple but effective methodology called CFRet-DVQA, which focuses on retrieval and efficient tuning to address this critical issue effectively. For that, we initially retrieve multiple segments from the document that correlate with the question at hand. Subsequently, we leverage the advanced reasoning abilities of the large language model (LLM), further augmenting its performance through instruction tuning. This approach enables the generation of answers that align with the style of the document labels. The experiments demo
&lt;/p&gt;</description></item><item><title>$\texttt{COSMIC}$&#26159;&#19968;&#31181;&#20197;&#30456;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#26032;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#31454;&#20105;&#24615;&#33021;&#20248;&#20110;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#12290;</title><link>https://arxiv.org/abs/2402.19457</link><description>&lt;p&gt;
$\texttt{COSMIC}$: &#30456;&#20114;&#20449;&#24687;&#29992;&#20110;&#20219;&#21153;&#26080;&#20851;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19457
&lt;/p&gt;
&lt;p&gt;
$\texttt{COSMIC}$&#26159;&#19968;&#31181;&#20197;&#30456;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#26032;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#31454;&#20105;&#24615;&#33021;&#20248;&#20110;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24635;&#32467;&#36136;&#37327;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#26681;&#25454;&#24635;&#32467;&#22120;&#29983;&#25104;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#19988;&#20445;&#30041;&#20219;&#21153;&#32467;&#26524;&#30340;&#25688;&#35201;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#36825;&#20123;&#20219;&#21153;&#30340;&#32467;&#26524;&#38169;&#35823;&#27010;&#29575;&#19982;&#28304;&#25991;&#26412;&#21644;&#29983;&#25104;&#25688;&#35201;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;$\texttt{COSMIC}$&#20316;&#20026;&#36825;&#19968;&#24230;&#37327;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#23637;&#31034;&#20102;&#23427;&#19982;&#22522;&#20110;&#20154;&#31867;&#21028;&#26029;&#30340;&#24230;&#37327;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;&#24050;&#24314;&#31435;&#30340;&#24230;&#37327;&#22914;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#30340;&#27604;&#36739;&#20998;&#26512;&#20984;&#26174;&#20102;$\texttt{COSMIC}$&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19457v1 Announce Type: cross  Abstract: Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks, while preserving task outcomes. We theoretically establish a direct relationship between the resulting error probability of these tasks and the mutual information between source texts and generated summaries. We introduce $\texttt{COSMIC}$ as a practical implementation of this metric, demonstrating its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$ highlight the competitive performance of $\texttt{COSMIC}$.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#36951;&#24536;&#65288;BlockFUL&#65289;&#65292;&#20351;&#29992;Chameleon Hash&#65288;CH&#65289;&#25216;&#26415;&#37325;&#26032;&#35774;&#35745;&#21306;&#22359;&#38142;&#32467;&#26500;&#65292;&#20943;&#23569;&#27169;&#22411;&#26356;&#26032;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16294</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Unlearning on Blockchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16294
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#36951;&#24536;&#65288;BlockFUL&#65289;&#65292;&#20351;&#29992;Chameleon Hash&#65288;CH&#65289;&#25216;&#26415;&#37325;&#26032;&#35774;&#35745;&#21306;&#22359;&#38142;&#32467;&#26500;&#65292;&#20943;&#23569;&#27169;&#22411;&#26356;&#26032;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#30830;&#20445;FL&#36807;&#31243;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#21306;&#22359;&#38142;FL&#28041;&#21450;&#21442;&#19982;&#32773;&#22312;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#38543;&#21518;&#23558;&#27169;&#22411;&#21457;&#24067;&#21040;&#21306;&#22359;&#38142;&#19978;&#65292;&#24418;&#25104;&#34920;&#31034;&#27169;&#22411;&#20851;&#31995;&#30340;&#31867;&#20284;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#30340;&#32487;&#25215;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;DAG&#30340;&#32467;&#26500;&#22312;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#26102;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#28041;&#21450;&#30340;&#22797;&#26434;&#24615;&#21644;&#24320;&#38144;&#36739;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#36951;&#24536;&#65288;BlockFUL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;&#21464;&#33394;&#40857;&#21704;&#24076;&#65288;CH&#65289;&#25216;&#26415;&#37325;&#26032;&#35774;&#35745;&#21306;&#22359;&#38142;&#32467;&#26500;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#26356;&#26032;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#36951;&#24536;&#20219;&#21153;&#30340;&#35745;&#31639;&#21644;&#20849;&#35782;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;BlockFUL&#25903;&#25345;&#21508;&#31181;&#32852;&#37030;&#36951;&#24536;&#26041;&#27861;&#65292;&#30830;&#20445;&#27169;&#22411;&#26356;&#26032;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16294v1 Announce Type: cross  Abstract: Blockchained Federated Learning (FL) has been gaining traction for ensuring the integrity and traceability of FL processes. Blockchained FL involves participants training models locally with their data and subsequently publishing the models on the blockchain, forming a Directed Acyclic Graph (DAG)-like inheritance structure that represents the model relationship. However, this particular DAG-based structure presents challenges in updating models with sensitive data, due to the complexity and overhead involved. To address this, we propose Blockchained Federated Unlearning (BlockFUL), a generic framework that redesigns the blockchain structure using Chameleon Hash (CH) technology to mitigate the complexity of model updating, thereby reducing the computational and consensus costs of unlearning tasks.Furthermore, BlockFUL supports various federated unlearning methods, ensuring the integrity and traceability of model updates, whether conduc
&lt;/p&gt;</description></item><item><title>V-STaR&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#25913;&#36827;&#21644;&#39564;&#35777;&#26041;&#27861;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.06457</link><description>&lt;p&gt;
V-STaR: &#33258;&#23398;&#25512;&#29702;&#22120;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
V-STaR: Training Verifiers for Self-Taught Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06457
&lt;/p&gt;
&lt;p&gt;
V-STaR&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#25913;&#36827;&#21644;&#39564;&#35777;&#26041;&#27861;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#33258;&#25105;&#25913;&#36827;&#26041;&#27861;&#65292;&#20363;&#22914;STaR&#65288;Zelikman&#31561;&#20154;&#65292;2022&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#36845;&#20195;&#24494;&#35843;LLM&#20197;&#25552;&#39640;&#20854;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27492;&#36807;&#31243;&#20013;&#20002;&#24323;&#20102;&#22823;&#37327;&#30340;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#33021;&#24573;&#30053;&#20102;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;V-STaR&#65292;&#23427;&#21033;&#29992;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20351;&#29992;DPO&#35757;&#32451;&#19968;&#20010;&#21028;&#26029;&#27169;&#22411;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#27491;&#30830;&#24615;&#30340;&#39564;&#35777;&#22120;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#36825;&#20010;&#39564;&#35777;&#22120;&#29992;&#26469;&#22312;&#20247;&#22810;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#20013;&#36873;&#25321;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22810;&#27425;&#36816;&#34892;V-STaR&#20250;&#36880;&#27493;&#20135;&#29983;&#26356;&#22909;&#30340;&#25512;&#29702;&#22120;&#21644;&#39564;&#35777;&#22120;&#65292;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;LLaMA2&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common self-improvement approaches for large language models (LLMs), such as STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.
&lt;/p&gt;</description></item><item><title>DFML&#26159;&#19968;&#20010;&#26080;&#26381;&#21153;&#22120;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20840;&#23616;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01863</link><description>&lt;p&gt;
DFML&#65306;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DFML: Decentralized Federated Mutual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01863
&lt;/p&gt;
&lt;p&gt;
DFML&#26159;&#19968;&#20010;&#26080;&#26381;&#21153;&#22120;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20840;&#23616;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#35774;&#22791;&#39046;&#22495;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#21644;&#23481;&#26131;&#21463;&#21040;&#21333;&#28857;&#25925;&#38556;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#35774;&#22791;&#22266;&#26377;&#22320;&#34920;&#29616;&#20986;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#29616;&#26377;&#24037;&#20316;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#27492;&#24322;&#36136;&#24615;&#19988;&#19981;&#26045;&#21152;&#26550;&#26500;&#38480;&#21046;&#25110;&#20551;&#23450;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#30340;&#20998;&#25955;&#24335;FL&#65288;DFL&#65289;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#65288;DFML&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#26080;&#26381;&#21153;&#22120;&#30340;&#65292;&#25903;&#25345;&#38750;&#38480;&#21046;&#24615;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20381;&#36182;&#20844;&#20849;&#25968;&#25454;&#12290;DFML&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#24182;&#24490;&#29615;&#25913;&#21464;&#30417;&#30563;&#21644;&#25552;&#21462;&#20449;&#21495;&#30340;&#25968;&#37327;&#26469;&#26377;&#25928;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DFML&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#20840;&#23616;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#19968;&#33268;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#26222;&#36941;&#23384;&#22312;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of real-world devices, centralized servers in Federated Learning (FL) present challenges including communication bottlenecks and susceptibility to a single point of failure. Additionally, contemporary devices inherently exhibit model and data heterogeneity. Existing work lacks a Decentralized FL (DFL) framework capable of accommodating such heterogeneity without imposing architectural restrictions or assuming the availability of public data. To address these issues, we propose a Decentralized Federated Mutual Learning (DFML) framework that is serverless, supports nonrestrictive heterogeneous models, and avoids reliance on public data. DFML effectively handles model and data heterogeneity through mutual learning, which distills knowledge between clients, and cyclically varying the amount of supervision and distillation signals. Extensive experimental results demonstrate consistent effectiveness of DFML in both convergence speed and global accuracy, outperforming prevalent b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01766</link><description>&lt;p&gt;
LLM&#25237;&#31080;&#65306;&#20154;&#31867;&#36873;&#25321;&#21644;AI&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
LLM Voting: Human Choices and AI Collective Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#19982;&#20154;&#31867;&#25237;&#31080;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36827;&#34892;&#20154;&#31867;&#25237;&#31080;&#23454;&#39564;&#20197;&#24314;&#31435;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#19982;LLM&#20195;&#29702;&#36827;&#34892;&#24179;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#38598;&#20307;&#32467;&#26524;&#21644;&#20010;&#20307;&#20559;&#22909;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#21644;LLMs&#20043;&#38388;&#22312;&#20915;&#31574;&#21644;&#22266;&#26377;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#22312;&#20559;&#22909;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#30456;&#27604;&#20154;&#31867;&#36873;&#27665;&#30340;&#22810;&#26679;&#20559;&#22909;&#65292;LLMs&#26377;&#26356;&#36235;&#21521;&#20110;&#19968;&#33268;&#36873;&#25321;&#30340;&#20542;&#21521;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;</title><link>https://arxiv.org/abs/2305.16877</link><description>&lt;p&gt;
&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning with Dual Expectile-Quantile Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.16877
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36817;&#20284;&#25972;&#20010;&#22238;&#25253;&#20998;&#24067;&#65292;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#29615;&#22659;&#26679;&#26412;&#12290;&#24120;&#29992;&#30340;&#22522;&#20110;&#19981;&#23545;&#31216;$L_1$&#25439;&#22833;&#30340;&#20998;&#24067;&#24335;RL&#30340;&#20998;&#20301;&#22238;&#24402;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;&#30340;&#26041;&#24335;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#25928;&#30340;&#28151;&#21512;&#19981;&#23545;&#31216;$L_1$-$L_2$ Huber&#25439;&#22833;&#26469;&#25913;&#36827;&#24448;&#24448;&#20250;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#20998;&#24067;&#20272;&#35745;&#20445;&#35777;&#28040;&#22833;&#20102;&#65292;&#25105;&#20204;&#23454;&#35777;&#35266;&#23519;&#21040;&#20272;&#35745;&#30340;&#20998;&#24067;&#20250;&#36805;&#36895;&#25910;&#25947;&#21040;&#20854;&#22343;&#20540;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#26399;&#26395;&#22238;&#24402;&#30456;&#23545;&#24212;&#30340;&#19981;&#23545;&#31216;$L_2$&#25439;&#22833;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#20998;&#24067;&#24335;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#12290;&#21463;&#21040;$L_2$&#20026;&#22522;&#30784;&#23398;&#20064;&#25928;&#29575;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#23398;&#20064;&#22238;&#25253;&#20998;&#24067;&#30340;&#26399;&#26395;&#20540;&#21644;&#20998;&#20301;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.16877v2 Announce Type: replace-cross  Abstract: Distributional reinforcement learning (RL) has proven useful in multiple benchmarks as it enables approximating the full distribution of returns and makes a better use of environment samples. The commonly used quantile regression approach to distributional RL -- based on asymmetric $L_1$ losses -- provides a flexible and effective way of learning arbitrary return distributions. In practice, it is often improved by using a more efficient, hybrid asymmetric $L_1$-$L_2$ Huber loss for quantile regression. However, by doing so, distributional estimation guarantees vanish, and we empirically observe that the estimated distribution rapidly collapses to its mean. Indeed, asymmetric $L_2$ losses, corresponding to expectile regression, cannot be readily used for distributional temporal difference learning. Motivated by the efficiency of $L_2$-based learning, we propose to jointly learn expectiles and quantiles of the return distribution
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03710</link><description>&lt;p&gt;
&#20195;&#29702;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36890;&#29992;&#30340;&#38646;-shot&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Agent Instructs Large Language Models to be General Zero-Shot Reasoners. (arXiv:2310.03710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#36827;&#19968;&#27493;&#37322;&#25918;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#26356;&#22810;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#29983;&#25104;&#12289;&#20998;&#31867;&#21644;&#25512;&#29702;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#65292;&#24182;&#22312;&#25105;&#20204;&#35780;&#20272;&#30340;29&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#22312;20&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;Vicuna-13b&#65288;13.3%&#65289;&#65292;Llama-2-70b-chat&#65288;23.2%&#65289;&#21644;GPT-3.5 Turbo&#65288;17.0%&#65289;&#12290;&#19982;&#38646;-shot&#24605;&#32500;&#38142;&#30456;&#27604;&#65292;&#25105;&#20204;&#23545;&#25512;&#29702;&#30340;&#25913;&#36827;&#24456;&#26126;&#26174;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;10.5%&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Llama-2-70b-chat&#30340;&#24615;&#33021;&#36229;&#36807;&#38646;-shot GPT-3.5 Turbo 10.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.16414</link><description>&lt;p&gt;
AutoCLIP: &#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models. (arXiv:2309.16414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26500;&#24314;&#30340;&#20998;&#31867;&#22120;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#26681;&#25454;&#25552;&#31034;&#27169;&#26495;&#33258;&#21160;&#21019;&#24314;&#27599;&#20010;&#31867;&#21035;&#30340;&#25551;&#36848;&#31526;&#38598;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#21253;&#25324;&#25163;&#24037;&#35774;&#35745;&#30340;&#27169;&#26495;&#12289;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#30340;&#27169;&#26495;&#20197;&#21450;&#20174;&#38543;&#26426;&#21333;&#35789;&#21644;&#23383;&#31526;&#26500;&#24314;&#30340;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#20174;&#30456;&#24212;&#30340;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#23548;&#20986;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65306;&#23558;&#22270;&#20687;&#30340;&#24179;&#22343;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#19982;&#32534;&#30721;&#22270;&#20687;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26368;&#22823;&#21270;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#26576;&#20123;&#25551;&#36848;&#31526;&#27604;&#20854;&#20182;&#25551;&#36848;&#31526;&#26356;&#22909;&#22320;&#21305;&#37197;&#32473;&#23450;&#22270;&#20687;&#19978;&#30340;&#35270;&#35273;&#32447;&#32034;&#26102;&#65292;&#23558;&#25152;&#26377;&#31867;&#21035;&#25551;&#36848;&#31526;&#31561;&#26435;&#37325;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35843;&#35856;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;AutoCLIP&#12290;AutoCLIP&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#20102;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#20174;s
&lt;/p&gt;
&lt;p&gt;
Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. In contrast, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, that is: classify to the class that maximizes the cosine similarity between its averaged encoded class descriptors and the encoded image. However, weighting all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP assigns to each prompt template per-image weights, which are derived from s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34917;&#19969;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;Patch AE&#65289;&#26694;&#26550;&#26469;&#22686;&#24378;&#33258;&#32534;&#30721;&#22120;&#23545;&#24322;&#24120;&#30340;&#37325;&#26500;&#33021;&#21147;&#65292;&#24182;&#22312;Mvtec AD&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#20855;&#26377;&#22312;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00429</link><description>&lt;p&gt;
&#22522;&#20110;&#34917;&#19969;&#21270;&#33258;&#32534;&#30721;&#22120;&#30340;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Patch-wise Auto-Encoder for Visual Anomaly Detection. (arXiv:2308.00429v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34917;&#19969;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;Patch AE&#65289;&#26694;&#26550;&#26469;&#22686;&#24378;&#33258;&#32534;&#30721;&#22120;&#23545;&#24322;&#24120;&#30340;&#37325;&#26500;&#33021;&#21147;&#65292;&#24182;&#22312;Mvtec AD&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#20855;&#26377;&#22312;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#24322;&#24120;&#20808;&#39564;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#65292;&#20256;&#32479;&#30340;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#22312;&#20165;&#36890;&#36807;&#27491;&#24120;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#26102;&#20542;&#21521;&#20110;&#22833;&#36133;&#65292;&#22240;&#20026;&#27169;&#22411;&#23558;&#26080;&#27861;&#27491;&#30830;&#37325;&#26500;&#24322;&#24120;&#22270;&#20687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34917;&#19969;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;Patch AE&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;AE&#23545;&#24322;&#24120;&#30340;&#37325;&#26500;&#33021;&#21147;&#32780;&#19981;&#26159;&#21066;&#24369;&#23427;&#12290;&#22270;&#20687;&#30340;&#27599;&#20010;&#34917;&#19969;&#37117;&#36890;&#36807;&#30456;&#24212;&#30340;&#31354;&#38388;&#20998;&#24067;&#29305;&#24449;&#21521;&#37327;&#30340;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#37325;&#26500;&#65292;&#21363;&#34917;&#19969;&#21270;&#37325;&#26500;&#65292;&#36825;&#30830;&#20445;&#20102;AE&#23545;&#24322;&#24120;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#39640;&#25928;&#12290;&#23427;&#22312;Mvtec AD&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#22312;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection without priors of the anomalies is challenging. In the field of unsupervised anomaly detection, traditional auto-encoder (AE) tends to fail based on the assumption that by training only on normal images, the model will not be able to reconstruct abnormal images correctly. On the contrary, we propose a novel patch-wise auto-encoder (Patch AE) framework, which aims at enhancing the reconstruction ability of AE to anomalies instead of weakening it. Each patch of image is reconstructed by corresponding spatially distributed feature vector of the learned feature representation, i.e., patch-wise reconstruction, which ensures anomaly-sensitivity of AE. Our method is simple and efficient. It advances the state-of-the-art performances on Mvtec AD benchmark, which proves the effectiveness of our model. It shows great potential in practical industrial application scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item></channel></rss>