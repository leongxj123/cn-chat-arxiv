<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#31867;&#20284;&#22823;&#33041;&#22238;&#25918;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#36825;&#19968;&#21457;&#29616;&#25552;&#20379;&#20102;&#29702;&#35299;&#22238;&#25918;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01467</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20869;&#20986;&#29616;&#31867;&#20284;&#22823;&#33041;&#22238;&#25918;&#30340;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#31867;&#20284;&#22823;&#33041;&#22238;&#25918;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#36825;&#19968;&#21457;&#29616;&#25552;&#20379;&#20102;&#29702;&#35299;&#22238;&#25918;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#21306;&#22495;&#20013;&#26222;&#36941;&#35266;&#23519;&#21040;&#30340;&#22238;&#25918;&#29616;&#35937;&#26159;&#21542;&#33021;&#22815;&#22312;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#20013;&#33258;&#28982;&#20135;&#29983;&#65311;&#22914;&#26524;&#26159;&#30340;&#35805;&#65292;&#23427;&#26159;&#21542;&#23545;&#20219;&#21153;&#26377;&#25152;&#36129;&#29486;&#65311;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#20219;&#21153;&#20248;&#21270;&#30340;&#33539;&#24335;&#19979;&#21457;&#29616;&#20102;&#22238;&#25918;&#30340;&#33258;&#28982;&#20986;&#29616;&#65292;&#27169;&#22411;&#27169;&#25311;&#20102;&#28023;&#39532;&#20307;&#21644;&#21069;&#39069;&#21494;&#30382;&#23618;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#27807;&#36890;&#21644;&#24863;&#35273;&#30382;&#23618;&#30340;&#36755;&#20837;&#12290;&#28023;&#39532;&#20307;&#20013;&#30340;&#22238;&#25918;&#26159;&#30001;&#20110;&#24773;&#26223;&#35760;&#24518;&#12289;&#35748;&#30693;&#22320;&#22270;&#20197;&#21450;&#29615;&#22659;&#35266;&#23519;&#32780;&#20135;&#29983;&#30340;&#65292;&#19982;&#21160;&#29289;&#23454;&#39564;&#25968;&#25454;&#30456;&#20284;&#65292;&#24182;&#19988;&#26159;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#25928;&#25351;&#26631;&#12290;&#35813;&#27169;&#22411;&#36824;&#25104;&#21151;&#22320;&#37325;&#29616;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30340;&#22238;&#25918;&#65292;&#19982;&#20154;&#31867;&#23454;&#39564;&#25968;&#25454;&#30456;&#31526;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#29702;&#35299;&#22238;&#25918;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can replay, as a widely observed neural activity pattern in brain regions, particularly in the hippocampus and neocortex, emerge in an artificial agent? If yes, does it contribute to the tasks? In this work, without heavy dependence on complex assumptions, we discover naturally emergent replay under task-optimized paradigm using a recurrent neural network-based reinforcement learning model, which mimics the hippocampus and prefrontal cortex, as well as their intercommunication and the sensory cortex input. The emergent replay in the hippocampus, which results from the episodic memory and cognitive map as well as environment observations, well resembles animal experimental data and serves as an effective indicator of high task performance. The model also successfully reproduces local and nonlocal replay, which matches the human experimental data. Our work provides a new avenue for understanding the mechanisms behind replay.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02817</link><description>&lt;p&gt;
&#20248;&#21270;&#22411;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#32508;&#36848;&#65306;&#20174;&#32463;&#20856;&#21040;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#23558;&#39640;&#23618;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#23618;&#36816;&#21160;&#35268;&#21010;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#35299;&#20915;&#38271;&#26102;&#22495;&#12289;&#21160;&#24577;&#20219;&#21153;&#12290;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#19987;&#27880;&#20110;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30446;&#26631;&#26465;&#20214;&#30340;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#24320;&#25918;&#24335;&#30446;&#26631;&#12289;&#26426;&#22120;&#20154;&#21160;&#24577;&#21644;&#26426;&#22120;&#20154;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#29289;&#29702;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#29305;&#21035;&#36866;&#21512;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#65292;&#28085;&#30422;&#20102;&#65288;i&#65289;&#35268;&#21010;&#39046;&#22495;&#34920;&#31034;&#65292;&#21253;&#25324;&#21160;&#20316;&#25551;&#36848;&#35821;&#35328;&#21644;&#26102;&#24577;&#36923;&#36753;&#65292;&#65288;ii&#65289;TAMP&#21508;&#32452;&#20214;&#30340;&#20010;&#21035;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;&#20154;&#24037;&#26234;&#33021;&#35268;&#21010;&#21644;&#36712;&#36857;&#20248;&#21270;&#65288;TO&#65289;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22522;&#20110;&#36923;&#36753;&#30340;&#20219;&#21153;&#35268;&#21010;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;TO&#20043;&#38388;&#30340;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02817v1 Announce Type: cross  Abstract: Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A 
&lt;/p&gt;</description></item><item><title>&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02151</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#36234;&#29425;&#21151;&#33021;&#23545;&#40784;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02151
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#23433;&#20840;&#23545;&#40784;&#30340;LLM&#20063;&#19981;&#20855;&#26377;&#25269;&#25239;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#21033;&#29992;&#23545;logprobs&#30340;&#35775;&#38382;&#36827;&#34892;&#36234;&#29425;&#65306;&#25105;&#20204;&#26368;&#21021;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#27169;&#26495;&#65288;&#26377;&#26102;&#20250;&#36866;&#24212;&#30446;&#26631;LLM&#65289;&#65292;&#28982;&#21518;&#25105;&#20204;&#22312;&#21518;&#32512;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#20197;&#26368;&#22823;&#21270;&#30446;&#26631;logprob&#65288;&#20363;&#22914;token&#8220;Sure&#8221;&#65289;&#65292;&#21487;&#33021;&#20250;&#36827;&#34892;&#22810;&#27425;&#37325;&#21551;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;GPT-3.5/4&#12289;Llama-2-Chat-7B/13B/70B&#12289;Gemma-7B&#21644;&#38024;&#23545;GCG&#25915;&#20987;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#30340;HarmBench&#19978;&#30340;R2D2&#31561;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;--&#26681;&#25454;GPT-4&#30340;&#35780;&#21028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36716;&#31227;&#25110;&#39044;&#22635;&#20805;&#25915;&#20987;&#20197;100%&#30340;&#25104;&#21151;&#29575;&#23545;&#25152;&#26377;&#19981;&#26292;&#38706;logprobs&#30340;Claude&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#23545;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;token&#25191;&#34892;&#38543;&#26426;&#25628;&#32034;&#20197;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;--&#36825;&#39033;&#20219;&#21153;&#19982;&#35768;&#22810;&#20854;&#20182;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
&lt;/p&gt;</description></item><item><title>Evalverse&#26159;&#19968;&#20010;&#32479;&#19968;&#21644;&#26131;&#29992;&#30340;&#24211;&#65292;&#31616;&#21270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#38598;&#20013;&#19988;&#26131;&#20110;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.00943</link><description>&lt;p&gt;
Evalverse: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32479;&#19968;&#21644;&#26131;&#29992;&#24211;
&lt;/p&gt;
&lt;p&gt;
Evalverse: Unified and Accessible Library for Large Language Model Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00943
&lt;/p&gt;
&lt;p&gt;
Evalverse&#26159;&#19968;&#20010;&#32479;&#19968;&#21644;&#26131;&#29992;&#30340;&#24211;&#65292;&#31616;&#21270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#38598;&#20013;&#19988;&#26131;&#20110;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Evalverse&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24211;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#35780;&#20272;&#24037;&#20855;&#32479;&#19968;&#21040;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#20013;&#65292;&#31616;&#21270;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#12290;Evalverse&#20351;&#24471;&#23545;&#20154;&#24037;&#26234;&#33021;&#20102;&#35299;&#26377;&#38480;&#30340;&#20010;&#20154;&#21487;&#20197;&#36731;&#26494;&#35831;&#27714;LLMs&#35780;&#20272;&#24182;&#25910;&#21040;&#35814;&#32454;&#25253;&#21578;&#65292;&#21033;&#29992;&#19982;Slack&#31561;&#36890;&#20449;&#24179;&#21488;&#30340;&#38598;&#25104;&#12290;&#22240;&#27492;&#65292;Evalverse&#20316;&#20026;LLMs&#30340;&#20840;&#38754;&#35780;&#20272;&#24378;&#22823;&#24037;&#20855;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#38598;&#20013;&#19988;&#26131;&#20110;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;Evalverse&#30340;&#28436;&#31034;&#35270;&#39057;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#21151;&#33021;&#21644;&#23454;&#29616;&#26041;&#24335;&#65292;&#20197;&#20004;&#20998;&#38047;&#30340;&#26684;&#24335;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00943v1 Announce Type: cross  Abstract: This paper introduces Evalverse, a novel library that streamlines the evaluation of Large Language Models (LLMs) by unifying disparate evaluation tools into a single, user-friendly framework. Evalverse enables individuals with limited knowledge of artificial intelligence to easily request LLM evaluations and receive detailed reports, facilitated by an integration with communication platforms like Slack. Thus, Evalverse serves as a powerful tool for the comprehensive assessment of LLMs, offering both researchers and practitioners a centralized and easily accessible evaluation framework. Finally, we also provide a demo video for Evalverse, showcasing its capabilities and implementation in a two-minute format.
&lt;/p&gt;</description></item><item><title>sDPO&#26159;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#20998;&#27493;&#21033;&#29992;&#20559;&#22909;&#25968;&#25454;&#38598;&#32780;&#38750;&#19968;&#27425;&#24615;&#20351;&#29992;&#65292;&#20419;&#36827;&#26356;&#31934;&#30830;&#23545;&#40784;&#21442;&#32771;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#24182;&#35757;&#32451;&#20986;&#24615;&#33021;&#26356;&#20248;&#30340;&#26368;&#32456;&#27169;&#22411;&#65292;&#29978;&#33267;&#32988;&#36807;&#20854;&#20182;&#20855;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#27969;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19270</link><description>&lt;p&gt;
sDPO&#65306;&#19981;&#35201;&#19968;&#27425;&#24615;&#20351;&#29992;&#24744;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
sDPO: Don't Use Your Data All at Once
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19270
&lt;/p&gt;
&lt;p&gt;
sDPO&#26159;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#20998;&#27493;&#21033;&#29992;&#20559;&#22909;&#25968;&#25454;&#38598;&#32780;&#38750;&#19968;&#27425;&#24615;&#20351;&#29992;&#65292;&#20419;&#36827;&#26356;&#31934;&#30830;&#23545;&#40784;&#21442;&#32771;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#24182;&#35757;&#32451;&#20986;&#24615;&#33021;&#26356;&#20248;&#30340;&#26368;&#32456;&#27169;&#22411;&#65292;&#29978;&#33267;&#32988;&#36807;&#20854;&#20182;&#20855;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#27969;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#23558;&#23427;&#20204;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#27493;DPO&#65288;sDPO&#65289;&#65292;&#36825;&#26159;&#23545;&#26368;&#36817;&#27969;&#34892;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#36827;&#34892;&#35843;&#25972;&#30340;&#19968;&#20010;&#25193;&#23637;&#12290;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#23558;&#21487;&#29992;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#20998;&#21106;&#65292;&#24182;&#20197;&#20998;&#27493;&#26041;&#24335;&#21033;&#29992;&#23427;&#20204;&#65292;&#32780;&#19981;&#26159;&#19968;&#27425;&#24615;&#20351;&#29992;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#20419;&#36827;&#20102;&#26356;&#31934;&#30830;&#23545;&#40784;&#21442;&#32771;&#27169;&#22411;&#22312;DPO&#35757;&#32451;&#26694;&#26550;&#20869;&#30340;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;sDPO&#35757;&#32451;&#26368;&#32456;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#29978;&#33267;&#32988;&#36807;&#25317;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#20854;&#20182;&#27969;&#34892;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19270v1 Announce Type: cross  Abstract: As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important. We propose stepwise DPO (sDPO), an extension of the recently popularized direct preference optimization (DPO) for alignment tuning. This approach involves dividing the available preference datasets and utilizing them in a stepwise manner, rather than employing it all at once. We demonstrate that this method facilitates the use of more precisely aligned reference models within the DPO training framework. Furthermore, sDPO trains the final model to be more performant, even outperforming other popular LLMs with more parameters.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#31216;&#20026;&#27491;&#20132;&#38170;&#28857;&#22238;&#24402;&#25439;&#22833;&#65292;&#29992;&#20110;&#35299;&#24320;&#23884;&#20837;&#32858;&#31867;&#65292;&#26174;&#33879;&#22686;&#24378;&#23884;&#20837;&#30340;&#29420;&#29305;&#24615;</title><link>https://arxiv.org/abs/2403.18699</link><description>&lt;p&gt;
&#20855;&#26377;&#27491;&#20132;&#38170;&#28857;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;CLOA&#65289;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning with Orthonormal Anchors (CLOA)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#31216;&#20026;&#27491;&#20132;&#38170;&#28857;&#22238;&#24402;&#25439;&#22833;&#65292;&#29992;&#20110;&#35299;&#24320;&#23884;&#20837;&#32858;&#31867;&#65292;&#26174;&#33879;&#22686;&#24378;&#23884;&#20837;&#30340;&#29420;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#26816;&#26597;InfoNCE&#25439;&#22833;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#65292;&#21363;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#34920;&#29616;&#20986;&#38480;&#21046;&#24615;&#34892;&#20026;&#65292;&#23548;&#33268;&#23884;&#20837;&#36235;&#20110;&#34701;&#21512;&#20026;&#19968;&#20010;&#22855;&#24322;&#28857;&#30340;&#25910;&#25947;&#29616;&#35937;&#12290;&#36825;&#31181;&#8220;&#36807;&#24230;&#34701;&#21512;&#8221;&#25928;&#24212;&#23545;&#21518;&#32493;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23884;&#20837;&#22312;&#31561;&#20110;&#25110;&#23616;&#38480;&#20110;&#31209;-1&#32447;&#24615;&#23376;&#31354;&#38388;&#26102;&#34920;&#31034;InfoNCE&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#19982;&#24494;&#35843;&#38454;&#27573;&#20856;&#22411;&#20351;&#29992;&#30340;&#30456;&#21516;&#25110;&#26356;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#27491;&#20132;&#38170;&#28857;&#22238;&#24402;&#25439;&#22833;&#65292;&#26088;&#22312;&#35299;&#24320;&#23884;&#20837;&#32858;&#31867;&#65292;&#26174;&#33879;&#22686;&#24378;&#27599;&#20010;&#23884;&#20837;&#30340;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18699v1 Announce Type: cross  Abstract: This study focuses on addressing the instability issues prevalent in contrastive learning, specifically examining the InfoNCE loss function and its derivatives. We reveal a critical observation that these loss functions exhibit a restrictive behavior, leading to a convergence phenomenon where embeddings tend to merge into a singular point. This "over-fusion" effect detrimentally affects classification accuracy in subsequent supervised-learning tasks. Through theoretical analysis, we demonstrate that embeddings, when equalized or confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In response to this challenge, our research introduces an innovative strategy that leverages the same or fewer labeled data than typically used in the fine-tuning phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to disentangle embedding clusters, significantly enhancing the distinctiveness of each embedding 
&lt;/p&gt;</description></item><item><title>TransFusion&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#23450;&#20041;&#20102;&#23545;&#27604;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#29702;&#35770;&#26497;&#38480;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#20197;&#25913;&#21892;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.18681</link><description>&lt;p&gt;
TransFusion&#65306;&#20855;&#26377;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TransFusion: Contrastive Learning with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18681
&lt;/p&gt;
&lt;p&gt;
TransFusion&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#23450;&#20041;&#20102;&#23545;&#27604;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#29702;&#35770;&#26497;&#38480;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#20197;&#25913;&#21892;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;TransFusion&#65292;&#26088;&#22312;&#20351;&#23545;&#27604;&#23398;&#20064;&#30340;&#36807;&#31243;&#26356;&#20855;&#20998;&#26512;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290; TransFusion&#30001;&#27880;&#24847;&#21147;&#22359;&#32452;&#25104;&#65292;&#20854;&#20013;&#30340;softmax&#34987;&#26367;&#25442;&#20026;ReLU&#65292;&#24182;&#19988;&#20854;&#26368;&#32456;&#22359;&#30340;&#21152;&#26435;&#21644;&#25805;&#20316;&#34987;&#25130;&#26029;&#65292;&#20197;&#20351;&#37051;&#25509;&#30697;&#38453;&#25104;&#20026;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#36755;&#20986;&#19982;&#30446;&#26631;&#20851;&#32852;&#30697;&#38453;&#20043;&#38388;&#30340;Jensen-Shannon&#25955;&#24230;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#30697;&#38453;&#25351;&#31034;&#27599;&#23545;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#30456;&#21516;&#31867;&#21035;&#25110;&#19981;&#21516;&#31867;&#21035;&#12290; TransFusion&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23450;&#20041;&#20102;&#22238;&#31572;&#35813;&#39046;&#22495;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#29702;&#35770;&#26497;&#38480;&#65306;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#22823;&#32423;&#21035;&#21644;&#26377;&#25928;&#23545;&#27604;&#23398;&#20064;&#25152;&#38656;&#30340;&#26368;&#23567;&#25209;&#37327;&#22823;&#23567;&#12290; &#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TransFusion&#25104;&#21151;&#22320;&#25552;&#21462;&#20986;&#33021;&#22815;&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#20998;&#31163;&#38598;&#32676;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18681v1 Announce Type: cross  Abstract: This paper proposes a novel framework, TransFusion, designed to make the process of contrastive learning more analytical and explainable. TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block's weighted-sum operation is truncated to leave the adjacency matrix as the output. The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes. The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of data augmentation and the minimum batch size required for effective contrastive learning. Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world data, leading to improved classification accuracy 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#21333;&#27493;&#32452;&#35013;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#21644;LEGO&#38169;&#35823;&#26657;&#27491;&#32452;&#35013;&#25968;&#25454;&#38598;&#65288;LEGO-ECA&#65289;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#65288;SCANet&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.18195</link><description>&lt;p&gt;
&#29992;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#32416;&#27491;LEGO&#32452;&#35013;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18195
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#21333;&#27493;&#32452;&#35013;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#21644;LEGO&#38169;&#35823;&#26657;&#27491;&#32452;&#35013;&#25968;&#25454;&#38598;&#65288;LEGO-ECA&#65289;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#65288;SCANet&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#21644;3D&#35270;&#35273;&#20013;&#65292;&#33258;&#20027;&#32452;&#35013;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#30830;&#20445;&#32452;&#35013;&#27491;&#30830;&#24615;&#12290;&#20027;&#27969;&#26041;&#27861;&#22914;MEPNet&#30446;&#21069;&#19987;&#27880;&#20110;&#22522;&#20110;&#25163;&#21160;&#25552;&#20379;&#30340;&#22270;&#20687;&#36827;&#34892;&#32452;&#20214;&#32452;&#35013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#24448;&#24448;&#38590;&#20197;&#21462;&#24471;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#21516;&#19968;&#26102;&#38388;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25972;&#21512;&#33258;&#26657;&#27491;&#27169;&#22359;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#21463;&#27492;&#38382;&#39064;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21333;&#27493;&#32452;&#35013;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#35782;&#21035;&#21644;&#32416;&#27491;&#32452;&#20214;&#32452;&#35013;&#38169;&#35823;&#12290;&#20026;&#25903;&#25345;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEGO&#38169;&#35823;&#26657;&#27491;&#32452;&#35013;&#25968;&#25454;&#38598;&#65288;LEGO-ECA&#65289;&#65292;&#21253;&#25324;&#29992;&#20110;&#32452;&#35013;&#27493;&#39588;&#21644;&#32452;&#35013;&#22833;&#36133;&#23454;&#20363;&#30340;&#25163;&#21160;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#65288;SCANet&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#12290;SCANet&#23558;&#32452;&#35013;&#30340;&#37096;&#20214;&#35270;&#20026;&#26597;&#35810;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18195v1 Announce Type: cross  Abstract: Autonomous assembly in robotics and 3D vision presents significant challenges, particularly in ensuring assembly correctness. Presently, predominant methods such as MEPNet focus on assembling components based on manually provided images. However, these approaches often fall short in achieving satisfactory results for tasks requiring long-term planning. Concurrently, we observe that integrating a self-correction module can partially alleviate such issues. Motivated by this concern, we introduce the single-step assembly error correction task, which involves identifying and rectifying misassembled components. To support research in this area, we present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising manual images for assembly steps and instances of assembly failures. Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel method to address this task. SCANet treats assembled components as queries, de
&lt;/p&gt;</description></item><item><title>MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;</title><link>https://arxiv.org/abs/2403.17141</link><description>&lt;p&gt;
MetaAligner&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#36890;&#29992;&#22810;&#30446;&#26631;&#23545;&#40784;&#30340;&#26465;&#20214;&#20174;&#24369;&#21040;&#24378;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17141
&lt;/p&gt;
&lt;p&gt;
MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#26088;&#22312;&#36890;&#36807;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26469;&#35299;&#20915;&#24322;&#36136;&#20154;&#31867;&#26399;&#26395;&#21644;&#20215;&#20540;&#35266;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#21040;&#31574;&#30053;&#27169;&#22411;&#30340;&#21442;&#25968;&#38480;&#21046;&#65292;&#23548;&#33268;&#20004;&#20010;&#20851;&#38190;&#23616;&#38480;&#24615;&#65306;&#65288;1&#65289;&#23427;&#20204;&#30340;&#23545;&#40784;&#31639;&#27861;&#23545;&#20110;&#27599;&#20010;&#26032;&#30446;&#26631;&#27169;&#22411;&#30340;&#37325;&#22797;&#25104;&#26412;&#24456;&#39640;&#65307;&#65288;2&#65289;&#30001;&#20110;&#20854;&#38745;&#24577;&#23545;&#40784;&#30446;&#26631;&#65292;&#23427;&#20204;&#26080;&#27861;&#25193;&#23637;&#21040;&#26410;&#35265;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-Objective Aligner&#65288;MetaAligner&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25191;&#34892;&#26465;&#20214;&#20174;&#24369;&#21040;&#24378;&#26657;&#27491;&#20197;&#36924;&#36817;&#24378;&#21709;&#24212;&#30340;&#27169;&#22411;&#12290;MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MetaAligner&#21462;&#24471;&#20102;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17141v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses. MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates zero-shot preference alignment for unseen objectives via in-context learning. Experimental results show that MetaAligner achieves sign
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.12510</link><description>&lt;p&gt;
&#22270;&#20687;&#25805;&#20316;&#30340;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Consistency Trajectory Models for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#21644;&#24674;&#22797;&#31561;&#24212;&#29992;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#22312;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#24615;&#36136;&#65306;&#25193;&#25955;&#23558;&#23558;&#22122;&#22768;&#21040;&#25968;&#25454;&#30340;&#22797;&#26434;&#26144;&#23556;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#21435;&#22122;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#27880;&#20837;&#24341;&#23548;&#39033;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#36807;&#31243;&#20063;&#24120;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#25968;&#21313;&#27425;&#29978;&#33267;&#25968;&#21315;&#27425;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTMs&#65289;&#21487;&#20197;&#22312;&#27010;&#29575;&#27969;ODE&#65288;PFODE&#65289;&#19978;&#20219;&#24847;&#26102;&#38388;&#28857;&#20043;&#38388;&#36827;&#34892;&#36941;&#21382;&#65292;&#24182;&#19988;&#36890;&#36807;&#21333;&#27425;&#20989;&#25968;&#35780;&#20272;&#36827;&#34892;&#24471;&#20998;&#25512;&#23548;&#65292;&#20294;CTMs&#20165;&#20801;&#35768;&#20174;&#39640;&#26031;&#22122;&#22768;&#36716;&#25442;&#20026;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#24191;&#20041;CTMs&#65288;GCTMs&#65289;&#26469;&#21457;&#25381;CTMs&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23454;&#29616;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
&lt;/p&gt;</description></item><item><title>HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11456</link><description>&lt;p&gt;
HateCOT&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27867;&#21270;&#25915;&#20987;&#24615;&#35328;&#35770;&#26816;&#27979;&#30340;&#35299;&#37322;&#22686;&#24378;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11456
&lt;/p&gt;
&lt;p&gt;
HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#23545;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#21487;&#38752;&#39640;&#25928;&#26816;&#27979;&#30340;&#38656;&#27714;&#65292;&#20026;&#20102;&#38480;&#21046;&#20854;&#26377;&#23475;&#24433;&#21709;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#19982;&#26816;&#27979;&#25915;&#20987;&#24615;&#20869;&#23481;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HateCOT&#65292;&#36825;&#26159;&#20174;&#22810;&#26679;&#21270;&#29616;&#26377;&#26469;&#28304;&#20013;&#25277;&#21462;&#30340;5.2&#19975;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30001;GPT-3.5-Turbo&#21644;&#20154;&#24037;&#31934;&#24515;&#21046;&#20316;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;HateCOT&#19978;&#20026;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#38646;-shot&#21644;few-shot&#35774;&#32622;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#23613;&#31649;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11456v1 Announce Type: cross  Abstract: The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects. This has led to a proliferation of datasets and models related to detecting offensive content. While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how "offensive content" is conceptualized, and the resulting differences in how these datasets are labeled. In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by GPT-3.5-Turbo and human-curated. We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three benchmark datasets in both zero and few-shot settings, despite differences in domain and task.} We further find that HateCOT enables effective K-shot fin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;TAMP&#31574;&#30053;&#65288;TAMPURA&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#21021;&#22987;&#29366;&#24577;&#21644;&#21160;&#20316;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#30340;&#38271;&#26102;&#31243;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#20449;&#24687;&#25910;&#38598;&#21644;&#36991;&#20813;&#19981;&#33391;&#21644;&#19981;&#21487;&#36870;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10454</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Task and Motion Planning with Uncertainty and Risk Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10454
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;TAMP&#31574;&#30053;&#65288;TAMPURA&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#21021;&#22987;&#29366;&#24577;&#21644;&#21160;&#20316;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#30340;&#38271;&#26102;&#31243;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#20449;&#24687;&#25910;&#38598;&#21644;&#36991;&#20813;&#19981;&#33391;&#21644;&#19981;&#21487;&#36870;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#30340;&#38271;&#26102;&#31243;&#26426;&#22120;&#20154;&#25805;&#32437;&#21644;&#23548;&#33322;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;TAMP&#38382;&#39064;&#20844;&#24335;&#21270;&#20551;&#35774;&#23436;&#20840;&#21487;&#35266;&#27979;&#21644;&#30830;&#23450;&#24615;&#21160;&#20316;&#25928;&#26524;&#12290;&#36825;&#20123;&#20551;&#35774;&#38480;&#21046;&#20102;&#35268;&#21010;&#32773;&#33719;&#21462;&#20449;&#24687;&#21644;&#20570;&#20986;&#20855;&#26377;&#39118;&#38505;&#24847;&#35782;&#30340;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;TAMP&#31574;&#30053;&#65288;TAMPURA&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#21021;&#22987;&#29366;&#24577;&#21644;&#21160;&#20316;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#30340;&#38271;&#26102;&#31243;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#20449;&#24687;&#25910;&#38598;&#21644;&#36991;&#20813;&#19981;&#33391;&#21644;&#19981;&#21487;&#36870;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35268;&#21010;&#32773;&#22312;&#25277;&#35937;&#20219;&#21153;&#32423;&#21035;&#21644;&#36830;&#32493;&#25511;&#21046;&#22120;&#32423;&#21035;&#22343;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#36827;&#34892;&#25512;&#29702;&#12290;&#37492;&#20110;&#19968;&#32452;&#22312;&#22522;&#26412;&#21160;&#20316;&#31354;&#38388;&#20013;&#36816;&#34892;&#30340;&#38381;&#29615;&#30446;&#26631;&#39537;&#21160;&#25511;&#21046;&#22120;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#20204;&#30340;&#21069;&#25552;&#26465;&#20214;&#21644;&#28508;&#22312;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10454v1 Announce Type: cross  Abstract: Integrated task and motion planning (TAMP) has proven to be a valuable approach to generalizable long-horizon robotic manipulation and navigation problems. However, the typical TAMP problem formulation assumes full observability and deterministic action effects. These assumptions limit the ability of the planner to gather information and make decisions that are risk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness (TAMPURA) that is capable of efficiently solving long-horizon planning problems with initial-state and action outcome uncertainty, including problems that require information gathering and avoiding undesirable and irreversible outcomes. Our planner reasons under uncertainty at both the abstract task level and continuous controller level. Given a set of closed-loop goal-conditioned controllers operating in the primitive action space and a description of their preconditions and potential capabilities, w
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASK&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#36890;&#36807;&#38750;&#35328;&#35821;&#20114;&#21160;&#19982;&#35266;&#20247;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#29366;&#24577;&#26426;&#32467;&#26500;&#35843;&#25972;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#23454;&#29616;&#22810;&#31181;&#19981;&#21516;&#35282;&#33394;&#30340;&#21160;&#24577;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2403.10041</link><description>&lt;p&gt;
&#22312;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#20013;&#23884;&#20837;&#21160;&#24577;&#35282;&#33394;: &#20266;&#35013;&#21160;&#30011;&#31038;&#20132;&#36816;&#21160;&#23398;&#65288;MASK&#65289;
&lt;/p&gt;
&lt;p&gt;
Towards Embedding Dynamic Personas in Interactive Robots: Masquerading Animated Social Kinematics (MASK)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10041
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASK&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#36890;&#36807;&#38750;&#35328;&#35821;&#20114;&#21160;&#19982;&#35266;&#20247;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#29366;&#24577;&#26426;&#32467;&#26500;&#35843;&#25972;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#23454;&#29616;&#22810;&#31181;&#19981;&#21516;&#35282;&#33394;&#30340;&#21160;&#24577;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#65292;&#20197;&#22686;&#24378;&#35266;&#20247;&#21442;&#19982;&#24230;&#65292;&#20351;&#29992;&#31867;&#20284;&#35282;&#33394;&#30340;&#20154;&#29289;&#24418;&#35937;&#12290;&#22522;&#20110;&#20197;&#35282;&#33394;&#20026;&#39537;&#21160;&#30340;&#23545;&#35805;&#20195;&#29702;&#31995;&#32479;&#65292;&#26412;&#30740;&#31350;&#23558;&#35813;&#20195;&#29702;&#24212;&#29992;&#25193;&#23637;&#21040;&#20102;&#29289;&#29702;&#39046;&#22495;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#25552;&#20379;&#26356;&#20855;&#27785;&#28024;&#24863;&#21644;&#20114;&#21160;&#20307;&#39564;&#12290;&#25552;&#20986;&#30340;&#31995;&#32479;&#21517;&#20026;Masquerading Animated Social Kinematics (MASK)&#65292;&#21033;&#29992;&#31867;&#20154;&#26426;&#22120;&#20154;&#36890;&#36807;&#38750;&#35328;&#35821;&#20114;&#21160;&#19982;&#23458;&#20154;&#20114;&#21160;&#65292;&#21253;&#25324;&#38754;&#37096;&#34920;&#24773;&#21644;&#25163;&#21183;&#12290;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#29366;&#24577;&#26426;&#32467;&#26500;&#30340;&#34892;&#20026;&#29983;&#25104;&#31995;&#32479;&#26377;&#25928;&#22320;&#35843;&#25972;&#26426;&#22120;&#20154;&#34892;&#20026;&#20197;&#20256;&#36798;&#19981;&#21516;&#30340;&#20154;&#29289;&#35282;&#33394;&#12290;MASK&#26694;&#26550;&#38598;&#25104;&#20102;&#24863;&#30693;&#24341;&#25806;&#12289;&#34892;&#20026;&#36873;&#25321;&#24341;&#25806;&#21644;&#32508;&#21512;&#21160;&#20316;&#24211;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#34892;&#20026;&#35774;&#35745;&#20013;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#24178;&#39044;&#22320;&#23454;&#29616;&#23454;&#26102;&#12289;&#21160;&#24577;&#20114;&#21160;&#12290;&#22312;&#29992;&#25143;&#23545;&#35937;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#25506;&#35752;&#20102;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#20197;&#28608;&#21457;&#26410;&#26469;&#30740;&#31350;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10041v1 Announce Type: cross  Abstract: This paper presents the design and development of an innovative interactive robotic system to enhance audience engagement using character-like personas. Built upon the foundations of persona-driven dialog agents, this work extends the agent application to the physical realm, employing robots to provide a more immersive and interactive experience. The proposed system, named the Masquerading Animated Social Kinematics (MASK), leverages an anthropomorphic robot which interacts with guests using non-verbal interactions, including facial expressions and gestures. A behavior generation system based upon a finite-state machine structure effectively conditions robotic behavior to convey distinct personas. The MASK framework integrates a perception engine, a behavior selection engine, and a comprehensive action library to enable real-time, dynamic interactions with minimal human intervention in behavior design. Throughout the user subject studi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.06259</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Conceptual Knowledge for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21644;&#35780;&#20272;&#20165;&#25506;&#35752;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#28982;&#32780;LLMs&#26159;&#21542;&#20855;&#26377;&#20462;&#25913;&#27010;&#24565;&#30340;&#33021;&#21147;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;LLMs&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;ConceptEdit&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#32423;&#21035;&#30340;&#23450;&#20041;&#65292;&#20294;&#23427;&#20204;&#20063;&#26377;&#28508;&#21147;&#25197;&#26354;LLMs&#20013;&#30456;&#20851;&#30340;&#23454;&#20363;&#30693;&#35782;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#21487;&#20197;&#28608;&#21457;&#23545;&#26356;&#22909;&#29702;&#35299;LLMs&#30340;&#36827;&#19968;&#27493;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#20027;&#39029;&#20301;&#20110;https://zjunlp.github.io/project/ConceptEdit&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06259v1 Announce Type: cross  Abstract: Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.
&lt;/p&gt;</description></item><item><title>RFWave&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#27874;&#24418;&#65292;&#20165;&#38656;10&#20010;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#23454;&#29616;&#20986;&#33394;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.05010</link><description>&lt;p&gt;
RFWave&#65306;&#29992;&#20110;&#38899;&#39057;&#27874;&#24418;&#37325;&#24314;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05010
&lt;/p&gt;
&lt;p&gt;
RFWave&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#27874;&#24418;&#65292;&#20165;&#38656;10&#20010;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#23454;&#29616;&#20986;&#33394;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#23637;&#22312;&#20174;&#19981;&#21516;&#34920;&#31034;&#20013;&#37325;&#24314;&#38899;&#39057;&#27874;&#24418;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#37325;&#24314;&#38899;&#39057;&#27874;&#24418;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#22312;&#20010;&#21035;&#26679;&#26412;&#28857;&#32423;&#21035;&#36827;&#34892;&#25805;&#20316;&#24182;&#19988;&#38656;&#35201;&#30456;&#23545;&#36739;&#22823;&#25968;&#37327;&#30340;&#37319;&#26679;&#27493;&#39588;&#65292;&#22240;&#27492;&#23427;&#20204;&#24448;&#24448;&#20250;&#20986;&#29616;&#24310;&#36831;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RFWave&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;&#26041;&#27861;&#65292;&#23427;&#20174;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#27874;&#24418;&#12290;RFWave&#22312;&#29983;&#25104;&#22797;&#26434;&#39057;&#35889;&#22270;&#24182;&#22312;&#24103;&#32423;&#21035;&#36816;&#34892;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#24615;&#65292;&#21516;&#26102;&#22788;&#29702;&#25152;&#26377;&#23376;&#24102;&#20197;&#22686;&#24378;&#25928;&#29575;&#12290;&#30001;&#20110;&#24076;&#26395;&#33719;&#24471;&#24179;&#32531;&#20256;&#36755;&#36712;&#36857;&#30340;&#25972;&#27969;&#27969;&#21160;&#65292;RFWave&#20165;&#38656;10&#20010;&#37319;&#26679;&#27493;&#39588;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;RFWave&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#29983;&#25104;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05010v1 Announce Type: cross  Abstract: Recent advancements in generative modeling have led to significant progress in audio waveform reconstruction from diverse representations. Although diffusion models have been used for reconstructing audio waveforms, they tend to exhibit latency issues because they operate at the level of individual sample points and require a relatively large number of sampling steps. In this study, we introduce RFWave, a novel multi-band Rectified Flow approach that reconstructs high-fidelity audio waveforms from Mel-spectrograms. RFWave is distinctive for generating complex spectrograms and operating at the frame level, processing all subbands concurrently to enhance efficiency. Thanks to Rectified Flow, which aims for a flat transport trajectory, RFWave requires only 10 sampling steps. Empirical evaluations demonstrate that RFWave achieves exceptional reconstruction quality and superior computational efficiency, capable of generating audio at a spee
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;85%&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.00894</link><description>&lt;p&gt;
&#23545;&#20110;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A systematic evaluation of large language models for generating programming code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00894
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;85%&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#31995;&#32479;&#35780;&#20272;&#20102;&#19971;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#12289;&#32534;&#31243;&#35821;&#35328;&#21644;&#20219;&#21153;&#38590;&#24230;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26102;&#30340;&#24615;&#33021;&#12290;GPT-4&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;Gemini Ultra&#21644;Claude 2&#12290;GPT-4&#30340;&#32534;&#30721;&#24615;&#33021;&#38543;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#32780;&#21464;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#35780;&#20272;&#30340;&#22823;&#22810;&#25968;LeetCode&#21644;GeeksforGeeks&#32534;&#31243;&#27604;&#36187;&#20013;&#65292;&#37319;&#29992;&#26368;&#20339;&#25552;&#31034;&#31574;&#30053;&#30340;GPT-4&#32988;&#36807;85%&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#12290;&#27492;&#22806;&#65292;GPT-4&#34920;&#29616;&#20986;&#22312;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#20195;&#30721;&#21644;&#20174;&#36807;&#21435;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#30001;GPT-4&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#35745;&#31639;&#25928;&#29575;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30456;&#24403;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#26377;&#28508;&#21147;&#25104;&#20026;&#22312;&#32534;&#31243;&#20195;&#30721;&#29983;&#25104;&#21644;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#21487;&#38752;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00894v1 Announce Type: cross  Abstract: We systematically evaluated the performance of seven large language models in generating programming code using various prompt strategies, programming languages, and task difficulties. GPT-4 substantially outperforms other large language models, including Gemini Ultra and Claude 2. The coding performance of GPT-4 varies considerably with different prompt strategies. In most LeetCode and GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the optimal prompt strategy outperforms 85 percent of human participants. Additionally, GPT-4 demonstrates strong capabilities in translating code between different programming languages and in learning from past errors. The computational efficiency of the code generated by GPT-4 is comparable to that of human programmers. These results suggest that GPT-4 has the potential to serve as a reliable assistant in programming code generation and software development.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#35789;&#22120;PathPiece&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#23569;&#37327;&#26631;&#35760;&#24182;&#19981;&#33021;&#23548;&#33268;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110; Tokenization &#30340;&#26377;&#25928;&#24615;&#29702;&#35299;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>https://arxiv.org/abs/2402.18376</link><description>&lt;p&gt;
Tokenization&#36229;&#36234;&#20102;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Tokenization Is More Than Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18376
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#35789;&#22120;PathPiece&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#23569;&#37327;&#26631;&#35760;&#24182;&#19981;&#33021;&#23548;&#33268;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110; Tokenization &#30340;&#26377;&#25928;&#24615;&#29702;&#35299;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tokenization&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#22522;&#30784;&#27493;&#39588;&#65292;&#23427;&#36830;&#25509;&#20102;&#21407;&#22987;&#25991;&#26412;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;Tokenization&#26041;&#27861;&#65292;&#22914;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;Byte-Pair Encoding&#65292;BPE&#65289;&#65292;&#28304;&#33258;&#25968;&#25454;&#21387;&#32553;&#39046;&#22495;&#65292;&#24182;&#26377;&#20154;&#35748;&#20026;BPE&#30340;&#26377;&#25928;&#24615;&#28304;&#20110;&#20854;&#23558;&#25991;&#26412;&#21387;&#32553;&#20026;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;PathPiece&#26469;&#27979;&#35797;&#8220;&#26356;&#23569;&#30340;&#26631;&#35760;&#26159;&#21542;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#8221;&#36825;&#19968;&#20551;&#35774;&#65292;PathPiece&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#35789;&#22120;&#65292;&#26681;&#25454;&#32473;&#23450;&#35789;&#27719;&#23558;&#25991;&#26723;&#25991;&#26412;&#21010;&#20998;&#20026;&#26368;&#23569;&#25968;&#37327;&#30340;&#26631;&#35760;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#19968;&#20551;&#35774;&#24182;&#38750;&#25104;&#31435;&#65292;&#23545;&#26377;&#25928;Tokenization&#21407;&#22240;&#30340;&#29702;&#35299;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#20026;&#20102;&#26816;&#26597;&#21738;&#20123;&#20854;&#20182;&#22240;&#32032;&#36215;&#21040;&#20316;&#29992;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Tokenization&#30340;&#25152;&#26377;&#19977;&#20010;&#38454;&#27573;&#65288;&#39044;&#20998;&#35789;&#12289;&#35789;&#27719;&#26500;&#36896;&#21644;&#20998;&#21106;&#65289;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#35774;&#35745;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18376v1 Announce Type: cross  Abstract: Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;</title><link>https://arxiv.org/abs/2402.17840</link><description>&lt;p&gt;
&#36981;&#24490;&#25105;&#30340;&#25351;&#31034;&#24182;&#35828;&#20986;&#30495;&#30456;&#65306;&#26469;&#33258;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17840
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#36866;&#24212;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;Retrieval-In-Context RAG&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23545;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LMs&#26500;&#24314;&#30340;RAG&#31995;&#32479;&#36827;&#34892;&#25552;&#31034;&#27880;&#20837;&#26102;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#12290;&#36825;&#31181;&#28431;&#27934;&#23384;&#22312;&#20110;&#35206;&#30422;Llama2&#12289;Mistral/Mixtral&#12289;Vicuna&#12289;SOLAR&#12289;WizardLM&#12289;Qwen1.5&#21644;Platypus2&#31561;&#22810;&#31181;&#29616;&#20195;LMs&#30340;&#24191;&#27867;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#21033;&#29992;&#33021;&#21147;&#21152;&#21095;&#12290;&#23558;&#30740;&#31350;&#25193;&#23637;&#21040;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25915;&#20987;&#65292;&#21487;&#20197;&#22312;&#23545;25&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#23450;&#21046;GPTs&#26045;&#21152;&#26368;&#22810;2&#20010;&#26597;&#35810;&#26102;&#20197;100%&#25104;&#21151;&#29575;&#23548;&#33268;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#20197;77,000&#23383;&#30340;&#20070;&#31821;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;41%&#65292;&#20197;&#21450;&#22312;&#21547;&#26377;1,569,00&#35789;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17840v1 Announce Type: cross  Abstract: Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,00
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26816;&#27979;&#28508;&#22312;&#24187;&#35273;&#24182;&#24314;&#35758;&#26367;&#20195;&#26041;&#26696;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#25104;&#21151;&#20943;&#23569;&#20154;&#31867;&#23548;&#33322;&#38169;&#35823;&#39640;&#36798;29%&#32780;&#19981;&#22686;&#21152;&#35748;&#30693;&#36127;&#25285;</title><link>https://arxiv.org/abs/2402.16973</link><description>&lt;p&gt;
&#36890;&#36807;&#31361;&#20986;&#28508;&#22312;&#38169;&#35823;&#24182;&#24314;&#35758;&#32416;&#27491;&#25104;&#21151;&#24341;&#23548;&#20154;&#31867;&#20570;&#20986;&#20915;&#31574;&#30340;&#19981;&#23436;&#32654;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16973
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26816;&#27979;&#28508;&#22312;&#24187;&#35273;&#24182;&#24314;&#35758;&#26367;&#20195;&#26041;&#26696;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#25104;&#21151;&#20943;&#23569;&#20154;&#31867;&#23548;&#33322;&#38169;&#35823;&#39640;&#36798;29%&#32780;&#19981;&#22686;&#21152;&#35748;&#30693;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#21033;&#29992;&#19981;&#23436;&#32654;&#35821;&#35328;&#27169;&#22411;&#26469;&#22312;&#22522;&#20110;&#23450;&#20301;&#23548;&#33322;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#24341;&#23548;&#20154;&#31867;&#20915;&#31574;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#23436;&#32654;&#30340;&#35828;&#26126;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26377;&#25928;&#30340;&#36890;&#20449;&#26426;&#21046;&#26469;&#26356;&#25104;&#21151;&#22320;&#24341;&#23548;&#20154;&#31867;&#12290;&#25105;&#20204;&#26500;&#24314;&#30340;&#36890;&#20449;&#26426;&#21046;&#21253;&#25324;&#21487;&#20197;&#26816;&#27979;&#35828;&#26126;&#20013;&#28508;&#22312;&#24187;&#35273;&#24182;&#24314;&#35758;&#23454;&#38469;&#26367;&#20195;&#26041;&#26696;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#30452;&#35266;&#30340;&#30028;&#38754;&#23558;&#35813;&#20449;&#24687;&#21576;&#29616;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#20154;&#31867;&#23548;&#33322;&#38169;&#35823;&#38477;&#20302;&#39640;&#36798;29%&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35748;&#30693;&#36127;&#25285;&#12290;&#36825;&#19968;&#32467;&#26524;&#31361;&#26174;&#20102;&#23558;&#22810;&#26679;&#21270;&#30340;&#36890;&#20449;&#28192;&#36947;&#25972;&#21512;&#21040;AI&#31995;&#32479;&#20013;&#26469;&#24357;&#34917;&#20854;&#32570;&#38519;&#24182;&#22686;&#24378;&#20854;&#23545;&#20154;&#31867;&#30340;&#23454;&#29992;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16973v1 Announce Type: new  Abstract: This paper addresses the challenge of leveraging imperfect language models to guide human decision-making in the context of a grounded navigation task. We show that an imperfect instruction generation model can be complemented with an effective communication mechanism to become more successful at guiding humans. The communication mechanism we build comprises models that can detect potential hallucinations in instructions and suggest practical alternatives, and an intuitive interface to present that information to users. We show that this approach reduces the human navigation error by up to 29% with no additional cognitive burden. This result underscores the potential of integrating diverse communication channels into AI systems to compensate for their imperfections and enhance their utility for humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#29992;&#20110;&#27491;&#24335;&#35748;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#35777;&#20070;&#23450;&#37327;&#21270;&#19988;&#21253;&#21547;&#39640;&#32622;&#20449;&#24230;&#30340;&#27010;&#29575;&#30028;&#38480;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#25552;&#39640;&#65292;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15929</link><description>&lt;p&gt;
QuaCer-C&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#23450;&#37327;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#29992;&#20110;&#27491;&#24335;&#35748;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#35777;&#20070;&#23450;&#37327;&#21270;&#19988;&#21253;&#21547;&#39640;&#32622;&#20449;&#24230;&#30340;&#27010;&#29575;&#30028;&#38480;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#25552;&#39640;&#65292;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30740;&#31350;&#24182;&#26410;&#23545;LLMs&#30340;&#34920;&#29616;&#25552;&#20379;&#27491;&#24335;&#30340;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#25105;&#20204;&#22312;&#27492;&#23545;&#30693;&#21517;LLMs&#30340;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#27491;&#24335;&#35748;&#35777;&#12290;&#25105;&#20204;&#30340;&#35777;&#20070;&#26159;&#23450;&#37327;&#30340; - &#23427;&#20204;&#21253;&#25324;&#23545;&#30446;&#26631;LLM&#22312;&#20219;&#20309;&#30456;&#20851;&#30693;&#35782;&#29702;&#35299;&#25552;&#31034;&#19978;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#27010;&#29575;&#30340;&#39640;&#32622;&#20449;&#24230;&#32039;&#23494;&#30028;&#38480;&#12290;&#25105;&#20204;&#38024;&#23545;Llama&#12289;Vicuna&#21644;Mistral LLMs&#30340;&#35777;&#20070;&#34920;&#26126;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#38543;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#24182;&#19988;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15929v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive performance on several benchmarks. However, traditional studies do not provide formal guarantees on the performance of LLMs. In this work, we propose a novel certification framework for LLM, QuaCer-C, wherein we formally certify the knowledge-comprehension capabilities of popular LLMs. Our certificates are quantitative - they consist of high-confidence, tight bounds on the probability that the target LLM gives the correct answer on any relevant knowledge comprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs indicate that the knowledge comprehension capability improves with an increase in the number of parameters and that the Mistral model is less performant than the rest in this evaluation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15290</link><description>&lt;p&gt;
&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Linear Dynamics-embedded Neural Network for Long-Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15290
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#25104;&#20026;&#29942;&#39048;&#65292;&#21463;&#21040;&#25511;&#21046;&#29702;&#35770;&#20013;&#20855;&#26377;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#12290; SSM&#30340;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#21367;&#31215;&#23646;&#24615;&#20351;LDNN&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#30340;&#25512;&#26029;&#21644;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290; &#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26377;&#25928;&#31574;&#30053;&#65292;&#23545;&#35282;&#21270;&#21644;&#8220;&#35299;&#32806;&#28982;&#21518;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#8221;&#65292;&#20197;&#23558;&#21367;&#31215;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$O(LNH\max\{L, N\})$&#38477;&#20302;&#21040;$O(LN\max\{H, \log L\})$&#12290; &#25105;&#20204;&#36890;&#36807;&#21452;&#21521;&#38750;&#22240;&#26524;&#21644;&#22810;&#22836;&#35774;&#32622;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;LDNN&#65292;&#20197;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290; &#23545;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#65288;LRA&#65289;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;LDNN&#30340;&#26377;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15290v1 Announce Type: cross  Abstract: The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to $O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#24847;&#22270;&#29702;&#35299;&#30340;&#19968;&#20010;&#26032;&#35270;&#35282;&#65292;&#19981;&#20381;&#36182;&#20110;&#20135;&#21697;&#26412;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#20135;&#21697;&#24674;&#22797;&#22522;&#20934;&#39564;&#35777;&#20102;&#24403;&#21069;&#24847;&#22270;&#30693;&#35782;&#22270;&#30340;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.14901</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20013;&#24847;&#22270;&#29702;&#35299;&#30340;&#20351;&#29992;&#20013;&#24515;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Usage-centric Take on Intent Understanding in E-Commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#24847;&#22270;&#29702;&#35299;&#30340;&#19968;&#20010;&#26032;&#35270;&#35282;&#65292;&#19981;&#20381;&#36182;&#20110;&#20135;&#21697;&#26412;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#20135;&#21697;&#24674;&#22797;&#22522;&#20934;&#39564;&#35777;&#20102;&#24403;&#21069;&#24847;&#22270;&#30693;&#35782;&#22270;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#26159;&#30005;&#23376;&#21830;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#24847;&#22270;&#29702;&#35299;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#23450;&#20041;&#24182;&#19981;&#19968;&#33268;&#65292;&#19988;&#32570;&#20047;&#20934;&#30830;&#30340;&#22522;&#20934;&#12290;&#26412;&#25991;&#20851;&#27880;&#23558;&#29992;&#25143;&#24847;&#22270;&#23450;&#20041;&#20026;"&#39038;&#23458;&#22914;&#20309;&#20351;&#29992;&#20135;&#21697;"&#30340;&#39044;&#27979;&#24615;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#23558;&#24847;&#22270;&#29702;&#35299;&#35270;&#20026;&#19968;&#39033;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#29420;&#31435;&#20110;&#20135;&#21697;&#26412;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;FolkScope&#30340;&#20004;&#20010;&#24369;&#28857;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#30005;&#23376;&#21830;&#21153;&#24847;&#22270;&#30693;&#35782;&#22270;&#65292;&#38480;&#21046;&#20102;&#20854;&#25512;&#29702;&#29992;&#25143;&#24847;&#22270;&#21644;&#25512;&#33616;&#22810;&#26679;&#26377;&#29992;&#20135;&#21697;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20135;&#21697;&#24674;&#22797;&#22522;&#20934;&#65292;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#20010;&#31034;&#20363;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#19978;&#36848;FolkScope&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14901v1 Announce Type: cross  Abstract: Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its popularity, intent understanding has not been consistently defined or accurately benchmarked. In this paper, we focus on predicative user intents as "how a customer uses a product", and pose intent understanding as a natural language reasoning task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph, that limit its capacity to reason about user intents and to recommend diverse useful products. Following these observations, we introduce a Product Recovery Benchmark including a novel evaluation framework and an example dataset. We further validate the above FolkScope weaknesses on this benchmark.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#20462;&#22797;&#21644;&#38899;&#20048;&#25490;&#21015;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22810;&#20010;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;AI&#39537;&#21160;&#38899;&#20048;&#32534;&#36753;&#24037;&#20855;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.09508</link><description>&lt;p&gt;
&#25490;&#21015;&#12289;&#20462;&#22797;&#21644;&#25913;&#36827;&#65306;&#36890;&#36807;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#23454;&#29616;&#21487;&#25805;&#25511;&#30340;&#38271;&#26399;&#38899;&#20048;&#38899;&#39057;&#29983;&#25104;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09508
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#20462;&#22797;&#21644;&#38899;&#20048;&#25490;&#21015;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22810;&#20010;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;AI&#39537;&#21160;&#38899;&#20048;&#32534;&#36753;&#24037;&#20855;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#38899;&#20048;&#29983;&#25104;&#22312;&#20154;&#26426;&#38899;&#20048;&#20849;&#21019;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#38899;&#20048;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#33258;&#22238;&#24402;&#29983;&#25104;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#22320;&#35299;&#20915;&#38899;&#20048;&#20462;&#22797;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;PEFT&#26041;&#27861;&#38598;&#25104;&#20102;&#22522;&#20110;&#24103;&#32423;&#20869;&#23481;&#30340;&#25511;&#21046;&#65292;&#20419;&#36827;&#20102;&#36712;&#36947;&#26465;&#20214;&#38899;&#20048;&#30340;&#31934;&#28860;&#21644;&#20998;&#25968;&#26465;&#20214;&#38899;&#20048;&#30340;&#25490;&#21015;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;MusicGen&#65292;&#19968;&#20010;&#39046;&#20808;&#30340;&#33258;&#22238;&#24402;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#22810;&#20010;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20026;&#26410;&#26469;&#30340;AI&#39537;&#21160;&#38899;&#20048;&#32534;&#36753;&#24037;&#20855;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09508v1 Announce Type: cross  Abstract: Controllable music generation plays a vital role in human-AI music co-creation. While Large Language Models (LLMs) have shown promise in generating high-quality music, their focus on autoregressive generation limits their utility in music editing tasks. To bridge this gap, we introduce a novel Parameter-Efficient Fine-Tuning (PEFT) method. This approach enables autoregressive language models to seamlessly address music inpainting tasks. Additionally, our PEFT method integrates frame-level content-based controls, facilitating track-conditioned music refinement and score-conditioned music arrangement. We apply this method to fine-tune MusicGen, a leading autoregressive music generation model. Our experiments demonstrate promising results across multiple music editing tasks, offering more flexible controls for future AI-driven music editing tools. A demo page\footnote{\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and source 
&lt;/p&gt;</description></item><item><title>UrbanKGent&#26159;&#19968;&#20010;&#29992;&#20110;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#24322;&#26500;&#24863;&#30693;&#21644;&#22320;&#29702;&#31354;&#38388;&#27880;&#20837;&#26500;&#24314;&#30693;&#35782;&#21270;&#25351;&#20196;&#38598;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#36712;&#36857;&#32454;&#21270;&#27169;&#22359;&#26469;&#25552;&#21319;&#36712;&#36857;&#30340;&#36136;&#37327;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#20102;UrbanKGent&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06861</link><description>&lt;p&gt;
UrbanKGent&#65306;&#29992;&#20110;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06861
&lt;/p&gt;
&lt;p&gt;
UrbanKGent&#26159;&#19968;&#20010;&#29992;&#20110;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#24322;&#26500;&#24863;&#30693;&#21644;&#22320;&#29702;&#31354;&#38388;&#27880;&#20837;&#26500;&#24314;&#30693;&#35782;&#21270;&#25351;&#20196;&#38598;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#36712;&#36857;&#32454;&#21270;&#27169;&#22359;&#26469;&#25552;&#21319;&#36712;&#36857;&#30340;&#36136;&#37327;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#20102;UrbanKGent&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#25552;&#21462;&#20851;&#38190;&#30693;&#35782;&#65292;&#29992;&#20110;&#21508;&#31181;&#22478;&#24066;&#24212;&#29992;&#22330;&#26223;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#20294;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#25163;&#21160;&#24037;&#20316;&#65292;&#38459;&#30861;&#20102;&#20854;&#28508;&#22312;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;UrbanKGent&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24322;&#26500;&#24863;&#30693;&#21644;&#22320;&#29702;&#31354;&#38388;&#27880;&#20837;&#26469;&#26500;&#24314;UrbanKGC&#20219;&#21153;&#65288;&#22914;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#21644;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65289;&#30340;&#30693;&#35782;&#21270;&#25351;&#20196;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#22686;&#24378;&#30340;&#36845;&#20195;&#36712;&#36857;&#32454;&#21270;&#27169;&#22359;&#65292;&#20197;&#22686;&#24378;&#21644;&#20248;&#21270;&#20174;GPT-4&#20013;&#25552;&#21462;&#30340;&#36712;&#36857;&#12290;&#36890;&#36807;&#22312;Llama-2-13B&#19978;&#36827;&#34892;&#22686;&#24378;&#36712;&#36857;&#30340;&#28151;&#21512;&#25351;&#20196;&#24494;&#35843;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;UrbanKGC&#20195;&#29702;UrbanKGent-13B&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama-2-13B, we obtain the UrbanKGC agent, UrbanKGent-13B. We perform a comprehensive evaluation on two real-world da
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03469</link><description>&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#19982;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Preference-free Alignment Learning with Regularized Relevance Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03469
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20542;&#21521;&#20110;&#32473;&#38271;&#30340;&#19982;&#20027;&#39064;&#26080;&#20851;&#30340;&#22238;&#22797;&#26356;&#39640;&#30340;&#20998;&#25968;&#65292;&#32780;&#32473;&#30701;&#30340;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#22238;&#22797;&#36739;&#20302;&#20998;&#12290;&#22312;&#36825;&#19968;&#35266;&#23519;&#30340;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26080;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#8220;&#30456;&#20851;&#24615;&#8221;&#20316;&#20026;&#23545;&#40784;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#36890;&#36807;&#26816;&#32034;&#24471;&#21040;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#30340;&#24433;&#21709;&#65292;&#21363;&#36807;&#24230;&#20248;&#21270;&#21040;&#19981;&#26399;&#26395;&#30340;&#25463;&#24452;&#19978;&#65292;&#24403;&#25105;&#20204;&#23558;&#35813;&#24471;&#20998;&#20316;&#20026;&#22870;&#21169;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#25972;&#21512;&#21040;&#24120;&#35268;&#30340;&#30456;&#20851;&#24615;&#20013;&#65292;&#20114;&#30456;&#27491;&#21017;&#21270;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#28151;&#21512;&#22870;&#21169;&#20989;&#25968;&#65306;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#65288;$R^3$&#65289;&#12290;$R^3$&#36890;&#36807;&#25552;&#20379;&#31283;&#20581;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;$R^3$&#19981;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#30340;&#20004;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#35813;&#25915;&#20987;&#23545; GPT &#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616; GPT-4 &#23545;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02987</link><description>&lt;p&gt;
GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Conversation Reconstruction Attack Against GPT Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#30340;&#20004;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#35813;&#25915;&#20987;&#23545; GPT &#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616; GPT-4 &#23545;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20854;&#20013; GPT &#31995;&#21015;&#27169;&#22411;&#20195;&#34920;&#30528;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25104;&#26524;&#12290;&#20026;&#20102;&#20248;&#21270;&#20219;&#21153;&#25191;&#34892;&#65292;&#29992;&#25143;&#32463;&#24120;&#19982;&#25176;&#31649;&#22312;&#20113;&#29615;&#22659;&#20013;&#30340; GPT &#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#36825;&#20123;&#22810;&#36718;&#23545;&#35805;&#24448;&#24448;&#21253;&#21547;&#31169;&#20154;&#20449;&#24687;&#65292;&#38656;&#35201;&#22312;&#20113;&#20013;&#36827;&#34892;&#20256;&#36755;&#21644;&#23384;&#20648;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25805;&#20316;&#27169;&#24335;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#25915;&#20987;&#38754;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#29305;&#23450;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65306;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#24403; GPT &#27169;&#22411;&#36973;&#21463;&#35813;&#25915;&#20987;&#26102;&#23545;&#35805;&#20013;&#22266;&#26377;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#35814;&#23613;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;GPT-4 &#23545;&#20110;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#39640;&#32423;&#25915;&#20987;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#37325;&#26500;&#20197;&#21069;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, significant advancements have been made in the field of large language models (LLMs), represented by GPT series models. To optimize task execution, users often engage in multi-round conversations with GPT models hosted in cloud environments. These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud. However, this operational paradigm introduces additional attack surfaces. In this paper, we first introduce a specific Conversation Reconstruction Attack targeting GPT models. Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations. Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when GPT models are subjected to the proposed attack. However, GPT-4 demonstrates certain robustness to the proposed attacks. We then introduce two advanced attacks aimed at better reconstructing previous c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#31995;&#32479;&#38169;&#35823;&#65292;&#36890;&#36807;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#26469;&#35780;&#20272;&#20854;&#23545;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;</title><link>https://arxiv.org/abs/2402.02680</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#22320;&#29702;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Geographically Biased
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#31995;&#32479;&#38169;&#35823;&#65292;&#36890;&#36807;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#26469;&#35780;&#20272;&#20854;&#23545;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#22312;&#22320;&#21547;&#26377;&#20854;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#20559;&#35265;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#20260;&#23475;&#30340;&#25345;&#32493;&#23384;&#22312;&#12290;&#38543;&#30528;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#29702;&#35299;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20559;&#35265;&#23545;&#20110;&#23454;&#29616;&#20844;&#27491;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22320;&#29702;&#35270;&#35282;&#30740;&#31350;LLMs&#23545;&#25105;&#20204;&#25152;&#29983;&#27963;&#30340;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#24378;&#22823;&#65292;&#22240;&#20026;&#23545;&#20154;&#31867;&#29983;&#27963;&#20013;&#35832;&#22810;&#19982;&#22320;&#29702;&#31354;&#38388;&#30456;&#20851;&#30340;&#26041;&#38754;&#65288;&#22914;&#25991;&#21270;&#12289;&#31181;&#26063;&#12289;&#35821;&#35328;&#12289;&#25919;&#27835;&#21644;&#23447;&#25945;&#65289;&#26377;&#30528;&#26126;&#26174;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#38382;&#39064;&#22320;&#29702;&#20559;&#35265;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#20013;&#30340;&#31995;&#32479;&#38169;&#35823;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;LLMs&#33021;&#22815;&#36827;&#34892;&#31934;&#30830;&#30340;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#65292;&#20197;&#35780;&#32423;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20854;&#19982;&#30495;&#23454;&#24773;&#20917;&#20043;&#38388;&#21576;&#29616;&#20986;&#24378;&#28872;&#30340;&#21333;&#35843;&#30456;&#20851;&#24615;&#65288;Spearman's &#961;&#26368;&#39640;&#21487;&#36798;0.89&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#20010;&#23458;&#35266;&#21644;&#23376;&#39046;&#22495;&#19978;&#34920;&#29616;&#20986;&#20849;&#21516;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and sub
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2312.11834</link><description>&lt;p&gt;
&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#22312;&#34892;&#20154;&#21160;&#24577;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11834
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#27169;&#25311;&#34892;&#20154;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36947;&#36335;&#65292;&#24182;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#21644;&#26368;&#23567;&#20108;&#20056;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#30340;MARL&#20195;&#29702;&#12290;&#22312;&#36825;&#20010;&#29615;&#22659;&#19979;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#20195;&#29702;&#23398;&#20064;&#36991;&#24320;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20219;&#21153;&#65306;&#31364;&#30452;&#25509;&#36335;&#24452;&#21644;&#23485;&#32469;&#36947;&#20043;&#38388;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#36208;&#24266;&#20013;&#30340;&#21452;&#21521;&#34892;&#20154;&#27969;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20195;&#29702;&#23494;&#24230;&#19981;&#22826;&#39640;&#26102;&#65292;&#23398;&#20064;&#26159;&#25104;&#21151;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11834v2 Announce Type: replace-cross  Abstract: In recent years, simulations of pedestrians using the multi-agent reinforcement learning (MARL) have been studied. This study considered the roads on a grid-world environment, and implemented pedestrians as MARL agents using an echo-state network and the least squares policy iteration method. Under this environment, the ability of these agents to learn to move forward by avoiding other agents was investigated. Specifically, we considered two types of tasks: the choice between a narrow direct route and a broad detour, and the bidirectional pedestrian flow in a corridor. The simulations results indicated that the learning was successful when the density of the agents was not that high.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#24120;&#29992;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#22810;&#26679;&#21270;&#20559;&#22909;&#23545;&#22870;&#21169;&#24314;&#27169;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#22870;&#21169;&#23398;&#20064;&#26041;&#27861;&#20197;&#22686;&#24378;&#26657;&#20934;&#24615;&#33021;</title><link>https://arxiv.org/abs/2312.07401</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#22810;&#26679;&#21270;&#20559;&#22909;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Diversified Preferences of Large Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#24120;&#29992;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#22810;&#26679;&#21270;&#20559;&#22909;&#23545;&#22870;&#21169;&#24314;&#27169;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#22870;&#21169;&#23398;&#20064;&#26041;&#27861;&#20197;&#22686;&#24378;&#26657;&#20934;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#34987;&#35748;&#20026;&#26159;&#25552;&#39640;LLMs&#20132;&#20114;&#36136;&#37327;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#22810;&#20803;&#21270;&#30340;&#19990;&#30028;&#20013;&#65292;&#30001;&#20110;&#26631;&#27880;&#32773;&#30340;&#19981;&#21516;&#20559;&#22909;&#65292;&#20154;&#31867;&#20559;&#22909;&#21487;&#33021;&#20250;&#22810;&#26679;&#21270;&#65292;&#36825;&#38459;&#30861;&#20102;LLM&#23545;&#40784;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;&#24120;&#29992;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#38598;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#65292;&#20197;&#30740;&#31350;&#22810;&#26679;&#21270;&#20559;&#22909;&#23545;&#22870;&#21169;&#24314;&#27169;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22870;&#21169;&#27169;&#22411;&#65288;RMs&#65289;&#30340;&#26657;&#20934;&#24615;&#33021;&#19982;LLMs&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22810;&#26679;&#21270;&#20559;&#22909;&#25968;&#25454;&#23545;&#20154;&#31867;&#20849;&#20139;&#20559;&#22909;&#65288;&#22914;&#8220;&#26080;&#23475;&#21644;&#26377;&#24110;&#21161;&#8221;&#65289;&#19978;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;LLMs&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#31181;&#26080;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#22870;&#21169;&#23398;&#20064;&#26041;&#27861;&#65288;MORE&#65289;&#20197;&#22686;&#24378;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07401v3 Announce Type: replace  Abstract: Aligning large language models (LLMs) with human preferences has been recognized as the key to improving LLMs' interaction quality. However, in this pluralistic world, human preferences can be diversified due to annotators' different tastes, which hinders the effectiveness of LLM alignment methods. This paper presents the first quantitative analysis of commonly used human feedback datasets to investigate the impact of diversified preferences on reward modeling. Our analysis reveals a correlation between the calibration performance of reward models (RMs) and the alignment performance of LLMs. We find that diversified preference data negatively affect the calibration performance of RMs on human-shared preferences, such as \textit{Harmless\&amp;Helpful}, thereby impairing the alignment performance of LLMs. To address the ineffectiveness, we propose a novel Multi-Objective Reward learning method (MORE) to enhance the calibration performance 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.18022</link><description>&lt;p&gt;
&#21033;&#29992;&#25351;&#25968;&#23610;&#24230;&#30340;&#28145;&#24230;&#24378;&#21270;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#30475;&#20316;&#26159;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#32452;&#21512;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#38543;&#30528;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#34920;&#36798;&#22312;&#36755;&#20837;&#22495;&#19978;&#30340;&#19981;&#21516;&#32447;&#24615;&#21306;&#22495;&#30340;&#25968;&#37327;&#26377;&#21487;&#33021;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20294;&#24403;&#21021;&#22987;&#21442;&#25968;&#36873;&#25321;&#38543;&#26426;&#26102;&#65292;&#19981;&#22826;&#21487;&#33021;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#19981;&#33391;&#30340;&#23610;&#24230;&#33021;&#22815;&#23548;&#33268;&#21363;&#20351;&#26159;&#31616;&#21333;&#20989;&#25968;&#20063;&#38656;&#35201;&#20351;&#29992;&#36807;&#22823;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#20197;&#19968;&#31181;&#26041;&#24335;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#12290;&#22312;&#36825;&#20123;&#26032;&#21442;&#25968;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#21021;&#22987;&#35299;&#65292;&#31245;&#21518;&#36890;&#36807;&#26356;&#26032;&#24213;&#23618;&#27169;&#22411;&#26435;&#37325;&#26469;&#25913;&#36827;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20135;&#29983;&#27604;&#38543;&#26426;&#21021;&#22987;&#21270;&#23545;&#24212;&#30340;&#20989;&#25968;&#36924;&#36817;&#22909;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A neural network with ReLU activations may be viewed as a composition of piecewise linear functions. For such networks, the number of distinct linear regions expressed over the input domain has the potential to scale exponentially with depth, but it is not expected to do so when the initial parameters are chosen randomly. This poor scaling can necessitate the use of overly large models to approximate even simple functions. To address this issue, we introduce a novel training strategy: we first reparameterize the network weights in a manner that forces an exponential number of activation patterns to manifest. Training first on these new parameters provides an initial solution that can later be refined by updating the underlying model weights. This approach allows us to produce function approximations that are several orders of magnitude better than their randomly initialized counterparts.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2309.02233</link><description>&lt;p&gt;
&#29992;&#21307;&#23398;&#25945;&#31185;&#20070;&#22686;&#24378;&#40657;&#30418;LLMs&#36827;&#34892;&#20020;&#24202;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.02233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#31034;&#20986;&#26681;&#25454;&#20154;&#31867;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#32570;&#20047;&#29305;&#23450;&#12289;&#28145;&#20837;&#30340;&#30693;&#35782;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;LLM-AMT&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;&#19968;&#20010;&#26597;&#35810;&#22686;&#24378;&#22120;&#12289;&#19968;&#20010;&#28151;&#21512;&#25945;&#31185;&#20070;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#30693;&#35782;&#33258;&#25105;&#23436;&#21892;&#12290;&#23427;&#20204;&#20849;&#21516;&#25972;&#21512;&#26435;&#23041;&#21307;&#23398;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;LLMs&#38405;&#35835;&#22120;&#26377;&#21161;&#20110;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMAMT&#26174;&#33879;&#25552;&#39640;&#20102;&#21709;&#24212;&#36136;&#37327;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.6%&#21040;16.6%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20197;GPT-4-Turbo&#20026;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.02233v2 Announce Type: replace-cross  Abstract: Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions. However, their use in the medical field can be challenging due to their lack of specific, in-depth knowledge. In this study, we present a system called LLMs Augmented with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in specialized domains. LLM-AMT integrates authoritative medical textbooks into the LLMs' framework using plug-and-play modules. These modules include a Query Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together, they incorporate authoritative medical knowledge. Additionally, an LLM Reader aids in contextual understanding. Our experimental results on three medical QA tasks demonstrate that LLMAMT significantly improves response quality, with accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the base mod
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;R-Judge&#65292;&#36890;&#36807;&#23545;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;72.29%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10019</link><description>&lt;p&gt;
R-Judge: &#35780;&#20272;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
R-Judge: Benchmarking Safety Risk Awareness for LLM Agents. (arXiv:2401.10019v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;R-Judge&#65292;&#36890;&#36807;&#23545;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;72.29%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#23436;&#25104;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLM&#20195;&#29702;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#25805;&#20316;&#26102;&#20250;&#24341;&#20837;&#24847;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#19982;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#23433;&#20840;&#24615;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#23433;&#20840;&#24615;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;R-Judge&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#22312;&#32473;&#23450;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#26102;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#30340;&#33021;&#21147;&#12290;R-Judge&#21253;&#25324;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#65292;&#28085;&#30422;7&#20010;&#24212;&#29992;&#39046;&#22495;&#21644;10&#31181;&#39118;&#38505;&#31867;&#22411;&#30340;27&#20010;&#20851;&#38190;&#39118;&#38505;&#22330;&#26223;&#12290;&#23427;&#32467;&#21512;&#20102;&#20154;&#31867;&#23545;&#23433;&#20840;&#24615;&#30340;&#20849;&#35782;&#65292;&#24182;&#20855;&#26377;&#26631;&#35760;&#30340;&#23433;&#20840;&#39118;&#38505;&#26631;&#31614;&#21644;&#39640;&#36136;&#37327;&#30340;&#39118;&#38505;&#25551;&#36848;&#12290;&#21033;&#29992;R-Judge&#65292;&#25105;&#20204;&#23545;8&#31181;&#24120;&#29992;&#20316;&#20195;&#29702;&#39592;&#24178;&#30340;&#33879;&#21517;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4&#23454;&#29616;&#20102;72.29%&#30340;&#23545;&#27604;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging safety risks given agent interaction records. R-Judge comprises 162 agent interaction records, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety risk labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.05930</link><description>&lt;p&gt;
SH2: &#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#24110;&#21161;&#24744;&#26356;&#20934;&#30830;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05930
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#26041;&#27861;&#65292;&#21363;&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;(SH2)&#65292;&#20197;&#24110;&#21161;LLMs&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;SH2&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#20013;&#19968;&#20010;&#31616;&#21333;&#30340;&#20107;&#23454;&#65292;&#21363;&#23545;&#20110;LLMs&#32780;&#35328;&#65292;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#24448;&#24448;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLMs&#32473;&#20104;&#36739;&#20302;&#27010;&#29575;&#30340;&#26631;&#35760;&#26356;&#26377;&#21487;&#33021;&#19982;&#20107;&#23454;&#20449;&#24687;&#65288;&#22914;&#21517;&#35789;&#12289;&#19987;&#26377;&#21517;&#35789;&#21644;&#24418;&#23481;&#35789;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#36873;&#25321;&#27010;&#29575;&#26368;&#20302;&#30340;&#26631;&#35760;&#24182;&#23558;&#20854;&#36830;&#25509;&#21040;&#21407;&#22987;&#19978;&#19979;&#25991;&#20013;&#26469;&#8220;&#31361;&#20986;&#8221;&#20107;&#23454;&#20449;&#24687;&#65292;&#20174;&#32780;&#36843;&#20351;&#27169;&#22411;&#22312;&#29983;&#25104;&#20043;&#21069;&#22810;&#27425;&#38405;&#35835;&#21644;&#29369;&#35947;&#36825;&#20123;&#26631;&#35760;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#23545;&#27604;&#35299;&#30721;&#30340;&#26041;&#24335;&#26469;&#24378;&#35843;&#30001;&#29369;&#35947;&#24102;&#26469;&#30340;&#36755;&#20986;&#27010;&#29575;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation.
&lt;/p&gt;</description></item><item><title>&#35745;&#31639;&#21307;&#30103;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23433;&#20840;&#24615;&#12289;&#35780;&#20272;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;FM&#30340;&#20998;&#26512;&#26377;&#26395;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02458</link><description>&lt;p&gt;
&#35745;&#31639;&#21307;&#30103;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Foundation Models in Computational Healthcare: A Survey. (arXiv:2401.02458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02458
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#21307;&#30103;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23433;&#20840;&#24615;&#12289;&#35780;&#20272;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;FM&#30340;&#20998;&#26512;&#26377;&#26395;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#22871;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#30340;&#20986;&#29616;&#20026;&#35745;&#31639;&#21307;&#30103;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#26426;&#36935;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#24615;&#30001;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#20154;&#31867;&#25351;&#20196;&#24341;&#23548;&#65292;&#24341;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#24378;&#35843;&#26356;&#22909;&#30340;&#25968;&#25454;&#34920;&#24449;&#12289;&#36136;&#37327;&#21644;&#35268;&#27169;&#12290;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#33719;&#21462;&#21644;&#22788;&#29702;&#39640;&#36136;&#37327;&#30340;&#20020;&#24202;&#25968;&#25454;&#35760;&#24405;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#25968;&#25454;&#25968;&#37327;&#12289;&#27880;&#37322;&#12289;&#24739;&#32773;&#38544;&#31169;&#21644;&#20262;&#29702;&#31561;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;FM&#26102;&#20195;&#30340;&#24191;&#27867;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#65288;&#20174;&#27169;&#22411;&#39044;&#35757;&#32451;&#21040;&#25512;&#29702;&#65289;&#65292;&#20197;&#25913;&#36827;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#12289;&#35780;&#20272;&#20197;&#21450;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#30340;&#20851;&#38190;&#35266;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#22522;&#20110;FM&#30340;&#20998;&#26512;&#22312;&#21307;&#30103;&#21644;&#21307;&#33647;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#30340;&#26684;&#23616;&#20013;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#34920;&#29616;&#30340;&#21069;&#26223;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#26032;&#30340;&#21307;&#30103;&#28165;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models (FMs) as an emerging suite of AI techniques has struck a wave of opportunities in computational healthcare. The interactive nature of these models, guided by pre-training data and human instructions, has ignited a data-centric AI paradigm that emphasizes better data characterization, quality, and scale. In healthcare AI, obtaining and processing high-quality clinical data records has been a longstanding challenge, ranging from data quantity, annotation, patient privacy, and ethics. In this survey, we investigate a wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow. We discuss key perspectives in AI security, assessment, and alignment with human values. Finally, we offer a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine. We provide an up-to-date list of healthcare
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01851</link><description>&lt;p&gt;
&#35757;&#32451;&#30340;&#21147;&#37327;&#65306;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#32622;&#23545;&#33021;&#28304;&#38656;&#27714;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#21464;&#21270;&#23545;&#30456;&#24212;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#25552;&#39640;&#21644;&#39640;&#24615;&#33021;&#30828;&#20214;&#30340;&#21019;&#26032;&#25512;&#21160;&#20102;&#22797;&#26434;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20294;&#20063;&#25903;&#25345;&#20102;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#30340;&#28040;&#38544;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22686;&#21152;&#20154;&#20204;&#23545;&#19968;&#33324;&#35757;&#32451;&#21442;&#25968;&#21644;&#36807;&#31243;&#65288;&#20174;&#23398;&#20064;&#29575;&#21040;&#25209;&#37327;&#22823;&#23567;&#20877;&#21040;&#30693;&#35782;&#20256;&#36755;&#65289;&#30340;&#33021;&#28304;&#24433;&#21709;&#30340;&#35748;&#35782;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21021;&#22987;&#21270;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#30828;&#20214;&#37197;&#32622;&#19978;&#35780;&#20272;&#22810;&#31181;&#35774;&#32622;&#65292;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#22312;&#22522;&#20934;&#32467;&#26524;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.13538</link><description>&lt;p&gt;
&#23398;&#20250;&#35828;&#27597;&#35821;&#65306;&#20197;&#27597;&#35821;&#39118;&#26684;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#29616;&#20195;&#24037;&#20855;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#25991;&#26412;&#39118;&#26684;&#22914;&#20309;&#24433;&#21709;LLMs&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;LLMs&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290; "&#27597;&#35821;"&#26159;&#25351;LLMs&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#21487;&#20197;&#36890;&#36807;&#38646;-shot&#22330;&#26223;&#25506;&#27979;&#12290; AlignedCoT&#24191;&#27867;&#36866;&#29992;&#20110;ICL&#26041;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#38382;&#31572;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#25991;&#26412;&#29702;&#35299;&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#32780;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;AlignedCoT&#30456;&#27604;&#31934;&#24515;&#25163;&#24037;&#21046;&#20316;&#30340;&#28436;&#31034;&#25991;&#31295;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specificall
&lt;/p&gt;</description></item><item><title>NOD-TAMP&#26159;&#19968;&#20010;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#26469;&#35299;&#20915;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#36712;&#36857;&#24182;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01530</link><description>&lt;p&gt;
NOD-TAMP:&#22810;&#27493;&#39588;&#25805;&#32437;&#35268;&#21010;&#20013;&#30340;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;
&lt;/p&gt;
&lt;p&gt;
NOD-TAMP: Multi-Step Manipulation Planning with Neural Object Descriptors. (arXiv:2311.01530v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01530
&lt;/p&gt;
&lt;p&gt;
NOD-TAMP&#26159;&#19968;&#20010;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#26469;&#35299;&#20915;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#36712;&#36857;&#24182;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23478;&#23621;&#21644;&#24037;&#21378;&#29615;&#22659;&#20013;&#24320;&#21457;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38271;&#26102;&#31243;&#20219;&#21153;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#32437;&#20197;&#21450;&#38656;&#35201;&#22312;&#21508;&#31181;&#29289;&#20307;&#24418;&#29366;&#21644;&#22330;&#26223;&#24067;&#23616;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#34429;&#28982;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#30340;&#20551;&#35774;&#65292;&#22914;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#26032;&#39062;&#32972;&#26223;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#65288;NODs&#65289;&#22312;&#29289;&#20307;&#21644;&#22330;&#26223;&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#24191;&#27867;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;NOD-TAMP&#20174;&#23569;&#25968;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#30701;&#30340;&#25805;&#32437;&#36712;&#36857;&#65292;&#20351;&#29992;NOD&#29305;&#24449;&#26469;&#35843;&#25972;&#36825;&#20123;&#36712;&#36857;&#65292;&#24182;&#32452;&#21512;&#23427;&#20204;&#26469;&#35299;&#20915;&#24191;&#27867;&#30340;&#38271;&#26102;&#31243;&#20219;&#21153;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#39564;&#35777;&#21518;&#65292;NOD-TAMP&#26377;&#25928;&#24212;&#23545;&#21508;&#31181;&#25361;&#25112;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#25805;&#32437;&#35268;&#21010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing intelligent robots for complex manipulation tasks in household and factory settings remains challenging due to long-horizon tasks, contact-rich manipulation, and the need to generalize across a wide variety of object shapes and scene layouts. While Task and Motion Planning (TAMP) offers a promising solution, its assumptions such as kinodynamic models limit applicability in novel contexts. Neural object descriptors (NODs) have shown promise in object and scene generalization but face limitations in addressing broader tasks. Our proposed TAMP-based framework, NOD-TAMP, extracts short manipulation trajectories from a handful of human demonstrations, adapts these trajectories using NOD features, and composes them to solve broad long-horizon tasks. Validated in a simulation environment, NOD-TAMP effectively tackles varied challenges and outperforms existing methods, establishing a cohesive framework for manipulation planning. For videos and other supplemental material, see the pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#38899;&#39640;&#12289;&#21644;&#24358;&#21644;&#40723;&#20048;&#31561;&#22266;&#26377;&#38899;&#20048;&#35821;&#35328;&#30340;&#30452;&#25509;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#27604;&#21407;&#22987;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23569;&#20110;4%&#12290;</title><link>http://arxiv.org/abs/2310.17162</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#30340;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Content-based Controls For Music Large Language Modeling. (arXiv:2310.17162v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#38899;&#39640;&#12289;&#21644;&#24358;&#21644;&#40723;&#20048;&#31561;&#22266;&#26377;&#38899;&#20048;&#35821;&#35328;&#30340;&#30452;&#25509;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#27604;&#21407;&#22987;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23569;&#20110;4%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#38899;&#20048;&#38899;&#39057;&#39046;&#22495;&#20986;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36805;&#36895;&#22686;&#38271;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#24471;&#33021;&#22815;&#36827;&#34892;&#39640;&#36136;&#37327;&#38899;&#20048;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#65292;&#24182;&#19988;&#19968;&#20123;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#22312;&#38899;&#20048;&#19978;&#30340;&#25511;&#21046;&#33021;&#21147;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#36890;&#36807;&#20803;&#25968;&#25454;&#65288;&#22914;&#27468;&#25163;&#21644;&#20048;&#22120;&#65289;&#25110;&#39640;&#32423;&#34920;&#31034;&#65288;&#22914;&#27969;&#27966;&#21644;&#24773;&#24863;&#65289;&#38388;&#25509;&#22320;&#25551;&#36848;&#38899;&#20048;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36827;&#19968;&#27493;&#25552;&#20379;&#23545;&#38899;&#39640;&#12289;&#21644;&#24358;&#21644;&#40723;&#20048;&#31561;&#22266;&#26377;&#38899;&#20048;&#35821;&#35328;&#30340;&#30452;&#25509;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Coco-Mulla&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#20102;&#38024;&#23545;&#22522;&#20110;Transformer&#30340;&#38899;&#39057;&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#30456;&#27604;&#21407;&#22987;&#27169;&#22411;&#65292;&#21442;&#25968;&#35843;&#20248;&#30340;&#27604;&#20363;&#19981;&#21040;4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieved high-quality music generation with low-resource semi-supervised learning, tuning with less than 4% parameters compared to the original model and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13391</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;Hebbian Temporal Memory&#23398;&#20064;&#32487;&#20219;&#32773;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Successor Representations with Distributed Hebbian Temporal Memory. (arXiv:2310.13391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#22312;&#19981;&#31283;&#23450;&#30340;&#12289;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#20998;&#24067;&#24335;Hebbian Temporal Memory (DHTM)&#65292;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;DHTM&#26088;&#22312;&#25429;&#25417;&#39034;&#24207;&#25968;&#25454;&#20851;&#31995;&#24182;&#23545;&#26410;&#26469;&#35266;&#23519;&#20316;&#20986;&#32047;&#31215;&#39044;&#27979;&#65292;&#24418;&#25104;&#32487;&#20219;&#32773;&#34920;&#31034;&#12290;&#21463;&#26032;&#30382;&#23618;&#30340;&#31070;&#32463;&#29983;&#29702;&#23398;&#27169;&#22411;&#21551;&#21457;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#20811;&#26381;&#20102;&#20256;&#32479;&#26102;&#38388;&#35760;&#24518;&#31639;&#27861;&#65288;&#22914;RNN&#21644;HMM&#65289;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#24930;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#20248;&#20110;&#32463;&#20856;&#30340;LSTM&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#21152;&#36895;&#20102;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#30340;&#31639;&#27861;&#24211;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.12567</link><description>&lt;p&gt;
Safety-Gymnasion&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark. (arXiv:2310.12567v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#30340;&#31639;&#27861;&#24211;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25317;&#26377;&#25512;&#21160;&#31038;&#20250;&#36827;&#27493;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#32463;&#24120;&#38754;&#20020;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;(SafeRL)&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#21487;&#20197;&#22312;&#21516;&#26102;&#36981;&#23432;&#22810;&#20010;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25509;&#21463;&#21521;&#37327;&#21644;&#20165;&#35270;&#35273;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Safe Policy Optimization&#65288;SafePO&#65289;&#30340;&#31639;&#27861;&#24211;&#65292;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#12290;&#36825;&#20010;&#32508;&#21512;&#24615;&#24211;&#21487;&#20197;&#20316;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#20010;&#22522;&#20934;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20174;&#32780;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#36890;&#36807;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#65292;&#21462;&#20195;&#20102;&#20840;&#23616;&#22238;&#25253;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#36807;&#20110;&#20048;&#35266;&#30340;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.16397</link><description>&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Decision Transformer for Stochastic Driving Environments. (arXiv:2309.16397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#36890;&#36807;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#65292;&#21462;&#20195;&#20102;&#20840;&#23616;&#22238;&#25253;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#36807;&#20110;&#20048;&#35266;&#30340;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#20027;&#21160;&#20132;&#20114;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#26694;&#26550;&#65292;&#22240;&#27492;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#23588;&#20854;&#21560;&#24341;&#20154;&#12290;&#26368;&#36817;Transformers&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#23558;&#31163;&#32447;RL&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#36825;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#36807;&#20110;&#20048;&#35266;&#65292;&#38169;&#35823;&#22320;&#20551;&#35774;&#30456;&#21516;&#30340;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#30456;&#21516;&#30340;&#21160;&#20316;&#19968;&#33268;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#36716;&#25442;&#27169;&#22411;&#25110;&#22797;&#26434;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UNREST&#36890;&#36807;&#36716;&#25442;&#19982;&#22238;&#25253;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#26469;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#12290;&#36890;&#36807;&#21457;&#29616;&#39550;&#39542;&#29615;&#22659;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#32047;&#31215;&#8221;&#21644;&#8220;&#26102;&#38388;&#23616;&#37096;&#24615;&#8221;&#29305;&#24615;&#65292;UNREST&#23558;&#20915;&#31574;Transformer&#20013;&#30340;&#20840;&#23616;&#22238;&#25253;&#26367;&#25442;&#20026;&#36739;&#23569;&#30340;&#37096;&#20998;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) has emerged as a promising framework for learning policies without active interactions, making it especially appealing for autonomous driving tasks. Recent successes of Transformers inspire casting offline RL as sequence modeling, which performs well in long-horizon tasks. However, they are overly optimistic in stochastic environments with incorrect assumptions that the same goal can be consistently achieved by identical actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in stochastic driving environments without introducing additional transition or complex generative models. Specifically, UNREST estimates state uncertainties by the conditional mutual information between transitions and returns, and segments sequences accordingly. Discovering the `uncertainty accumulation' and `temporal locality' properties of driving environments, UNREST replaces the global returns in decision transformers with less 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#30740;&#20102;&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#39046;&#22495;&#30340;&#24403;&#21069;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#32597;&#35265;&#20107;&#20214;&#25968;&#25454;&#12289;&#25968;&#25454;&#22788;&#29702;&#12289;&#31639;&#27861;&#26041;&#27861;&#21644;&#35780;&#20272;&#26041;&#27861;&#22235;&#20010;&#32500;&#24230;&#65292;&#24635;&#32467;&#20986;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#35299;&#20915;&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#38382;&#39064;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11356</link><description>&lt;p&gt;
&#38024;&#23545;&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Rare Event Prediction. (arXiv:2309.11356v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#30740;&#20102;&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#39046;&#22495;&#30340;&#24403;&#21069;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#32597;&#35265;&#20107;&#20214;&#25968;&#25454;&#12289;&#25968;&#25454;&#22788;&#29702;&#12289;&#31639;&#27861;&#26041;&#27861;&#21644;&#35780;&#20272;&#26041;&#27861;&#22235;&#20010;&#32500;&#24230;&#65292;&#24635;&#32467;&#20986;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#35299;&#20915;&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#38382;&#39064;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#28041;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#35782;&#21035;&#21644;&#39044;&#27979;&#20302;&#27010;&#29575;&#20107;&#20214;&#12290;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#65292;&#26222;&#36890;&#20107;&#20214;&#30340;&#39057;&#29575;&#36828;&#36828;&#36229;&#36807;&#32597;&#35265;&#20107;&#20214;&#30340;&#39057;&#29575;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#27599;&#20010;&#27493;&#39588;&#20013;&#20351;&#29992;&#19987;&#38376;&#30340;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#31639;&#27861;&#21040;&#35780;&#20272;&#21327;&#35758;&#12290;&#39044;&#27979;&#32597;&#35265;&#20107;&#20214;&#30340;&#21457;&#29983;&#23545;&#20110;&#24037;&#19994;4.0&#31561;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20063;&#26159;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#32597;&#35265;&#20107;&#20214;&#39044;&#27979;&#30340;&#24403;&#21069;&#26041;&#27861;&#65292;&#20174;&#32597;&#35265;&#20107;&#20214;&#25968;&#25454;&#12289;&#25968;&#25454;&#22788;&#29702;&#12289;&#31639;&#27861;&#26041;&#27861;&#21644;&#35780;&#20272;&#26041;&#27861;&#22235;&#20010;&#32500;&#24230;&#32771;&#34385;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;73&#20010;&#25968;&#25454;&#38598;&#65288;&#25968;&#20540;&#12289;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#65292;&#22235;&#20010;&#20027;&#35201;&#30340;&#25968;&#25454;&#22788;&#29702;&#20998;&#31867;&#65292;&#20116;&#20010;&#20027;&#35201;&#30340;&#31639;&#27861;&#20998;&#31867;&#21644;&#20004;&#20010;&#26356;&#24191;&#27867;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rare event prediction involves identifying and forecasting events with a low probability using machine learning and data analysis. Due to the imbalanced data distributions, where the frequency of common events vastly outweighs that of rare events, it requires using specialized methods within each step of the machine learning pipeline, i.e., from data processing to algorithms to evaluation protocols. Predicting the occurrences of rare events is important for real-world applications, such as Industry 4.0, and is an active research area in statistical and machine learning. This paper comprehensively reviews the current approaches for rare event prediction along four dimensions: rare event data, data processing, algorithmic approaches, and evaluation approaches. Specifically, we consider 73 datasets from different modalities (i.e., numerical, image, text, and audio), four major categories of data processing, five major algorithmic groupings, and two broader evaluation approaches. This pape
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#35270;&#39057;&#25968;&#25454;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#35821;&#35328;&#32534;&#30721;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.05822</link><description>&lt;p&gt;
&#32534;&#30721;-&#23384;&#20648;-&#26816;&#32034;&#65306;&#36890;&#36807;&#35821;&#35328;&#32534;&#30721;&#30340;&#33258;&#25105;&#20013;&#24515;&#24863;&#30693;&#22686;&#24378;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception. (arXiv:2308.05822v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#35270;&#39057;&#25968;&#25454;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#35821;&#35328;&#32534;&#30721;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20381;&#36182;&#20110;&#33258;&#24049;&#30340;&#35760;&#24518;&#26469;&#32534;&#30721;&#12289;&#23384;&#20648;&#21644;&#26816;&#32034;&#25105;&#20204;&#30340;&#32463;&#21382;&#12290;&#28982;&#32780;&#65292;&#35760;&#24518;&#38388;&#38548;&#26377;&#26102;&#20250;&#21457;&#29983;&#12290;&#23454;&#29616;&#35760;&#24518;&#22686;&#24378;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#22836;&#25140;&#24335;&#26174;&#31034;&#35774;&#22791;&#26469;&#25429;&#25417;&#21644;&#20445;&#30041;&#33258;&#25105;&#20013;&#24515;&#30340;&#35270;&#39057;&#65292;&#36825;&#31181;&#20570;&#27861;&#36890;&#24120;&#34987;&#31216;&#20026;&#29983;&#27963;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#25216;&#26415;&#32570;&#20047;&#39640;&#25928;&#32534;&#30721;&#21644;&#23384;&#20648;&#22914;&#27492;&#22823;&#37327;&#30340;&#35270;&#39057;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20174;&#24222;&#22823;&#30340;&#35270;&#39057;&#23384;&#26723;&#20013;&#26816;&#32034;&#29305;&#23450;&#20449;&#24687;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#22797;&#26434;&#20102;&#24555;&#36895;&#35775;&#38382;&#25152;&#38656;&#20869;&#23481;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as life logging. However, a significant challenge arises from the sheer volume of video data generated through life logging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation system that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05385</link><description>&lt;p&gt;
&#23398;&#20064;&#26680;&#25216;&#26415;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#25239;(PPG)&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#26041;&#27861;&#26469;&#25345;&#32493;&#30417;&#27979;&#21508;&#31181;&#24515;&#34880;&#31649;&#21442;&#25968;&#12290;PPG&#20449;&#21495;&#30001;&#21487;&#31359;&#25140;&#35774;&#22791;&#20135;&#29983;&#65292;&#24120;&#24120;&#21253;&#21547;&#30001;&#22806;&#37096;&#22240;&#32032;(&#22914;&#20154;&#20307;&#36816;&#21160;)&#24341;&#36215;&#30340;&#22823;&#22411;&#20266;&#24433;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#29983;&#29702;&#21442;&#25968;&#36827;&#34892;&#31283;&#20581;&#21644;&#20934;&#30830;&#30340;&#25552;&#21462;&#65292;&#20449;&#21495;&#30340;&#25439;&#22351;&#21306;&#22495;&#38656;&#35201;&#34987;&#27491;&#30830;&#22320;&#35782;&#21035;&#21644;&#22788;&#29702;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#25163;&#24037;&#29305;&#24449;&#26816;&#27979;&#22120;&#25110;&#20449;&#21495;&#24230;&#37327;&#65292;&#32467;&#26524;&#24615;&#33021;&#19981;&#20339;&#65292;&#25110;&#20381;&#38752;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#23567;&#32452;&#21487;&#35299;&#37322;&#30340;&#21367;&#31215;&#26680;&#65292;&#20854;&#24615;&#33021;&#19982;&#29616;&#26377;&#25216;&#26415;DNN&#26041;&#27861;&#30456;&#20284;&#65292;&#29978;&#33267;&#26356;&#22909;&#65292;&#32780;&#21442;&#25968;&#25968;&#37327;&#27604;DNN&#26041;&#27861;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#39033;&#24037;&#20316;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;FHE&#21152;&#23494;&#25216;&#26415;&#65292;&#26082;&#21487;&#20197;&#20445;&#25252;&#27169;&#22411;&#26356;&#26032;&#30340;&#38544;&#31169;&#65292;&#21448;&#21487;&#20197;&#38450;&#27490;&#24694;&#24847;&#29992;&#25143;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05112</link><description>&lt;p&gt;
FheFL&#65306;&#25903;&#25345;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#19982;&#25308;&#21344;&#24237;&#29992;&#25143;
&lt;/p&gt;
&lt;p&gt;
FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving Federated Learning with Byzantine Users. (arXiv:2306.05112v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;FHE&#21152;&#23494;&#25216;&#26415;&#65292;&#26082;&#21487;&#20197;&#20445;&#25252;&#27169;&#22411;&#26356;&#26032;&#30340;&#38544;&#31169;&#65292;&#21448;&#21487;&#20197;&#38450;&#27490;&#24694;&#24847;&#29992;&#25143;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25216;&#26415;&#26368;&#21021;&#26159;&#20026;&#20102;&#32531;&#35299;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#12290;&#23613;&#31649;FL&#30830;&#20445;&#29992;&#25143;&#30340;&#25968;&#25454;&#22987;&#32456;&#20445;&#30041;&#22312;&#29992;&#25143;&#25163;&#20013;&#65292;&#20294;&#23616;&#37096;&#35757;&#32451;&#27169;&#22411;&#30340;&#26799;&#24230;&#24517;&#39035;&#19982;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#36890;&#20449;&#20197;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#36825;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#65292;&#20351;&#24471;&#26381;&#21153;&#22120;&#21487;&#20197;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#25512;&#26029;&#20986;&#29992;&#25143;&#25968;&#25454;&#30340;&#31169;&#23494;&#20449;&#24687;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#32570;&#38519;&#65292;&#19979;&#19968;&#20195;FL&#26550;&#26500;&#25552;&#20986;&#20102;&#21152;&#23494;&#21644;&#21311;&#21517;&#21270;&#25216;&#26415;&#65292;&#20197;&#20445;&#25252;&#27169;&#22411;&#26356;&#26032;&#20813;&#21463;&#26381;&#21153;&#22120;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#24102;&#26469;&#20854;&#20182;&#25361;&#25112;&#65292;&#20363;&#22914;&#24694;&#24847;&#29992;&#25143;&#21487;&#33021;&#36890;&#36807;&#20849;&#20139;&#34394;&#20551;&#26799;&#24230;&#26469;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#12290;&#30001;&#20110;&#26799;&#24230;&#34987;&#21152;&#23494;&#65292;&#26381;&#21153;&#22120;&#26080;&#27861;&#35782;&#21035;&#21644;&#25490;&#38500;&#19981;&#33391;&#29992;&#25143;&#20197;&#20445;&#25252;&#20840;&#23616;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#31181;&#25915;&#20987;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#65288;FHE&#65289;&#30340;&#26032;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The federated learning (FL) technique was initially developed to mitigate data privacy issues that can arise in the traditional machine learning paradigm. While FL ensures that a user's data always remain with the user, the gradients of the locally trained models must be communicated with the centralized server to build the global model. This results in privacy leakage, where the server can infer private information of the users' data from the shared gradients. To mitigate this flaw, the next-generation FL architectures proposed encryption and anonymization techniques to protect the model updates from the server. However, this approach creates other challenges, such as a malicious user might sabotage the global model by sharing false gradients. Since the gradients are encrypted, the server is unable to identify and eliminate rogue users which would protect the global model. Therefore, to mitigate both attacks, this paper proposes a novel fully homomorphic encryption (FHE) based scheme 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#30340;&#29983;&#25104;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#25928;&#26524;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;DynamicGR&#22312;&#26032;&#30340;&#35821;&#26009;&#24211;&#19978;&#23637;&#29616;&#20986;&#20102;&#24847;&#22806;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18952</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#25345;&#32493;&#26356;&#26032;&#29983;&#25104;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Continually Updating Generative Retrieval on Dynamic Corpora. (arXiv:2305.18952v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#30340;&#29983;&#25104;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#25928;&#26524;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;DynamicGR&#22312;&#26032;&#30340;&#35821;&#26009;&#24211;&#19978;&#23637;&#29616;&#20986;&#20102;&#24847;&#22806;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#20449;&#24687;&#26816;&#32034;(IR)&#30340;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#35821;&#26009;&#24211;&#26159;&#38745;&#24577;&#30340;&#65292;&#32780;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#26159;&#19981;&#26029;&#26356;&#26032;&#30340;&#12290;&#26412;&#25991;&#23558;&#30693;&#35782;&#30340;&#21160;&#24577;&#24615;&#24341;&#20837;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#23558;&#26816;&#32034;&#35270;&#20026;&#21160;&#24577;&#30340;&#30693;&#35782;&#24211;&#65292;&#26356;&#31526;&#21512;&#30495;&#23454;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21452;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#26816;&#32034;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#21033;&#29992;StreamingQA&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#26102;&#24577;&#30693;&#35782;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24403;&#25105;&#20204;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22686;&#24378;&#29983;&#25104;&#26816;&#32034;&#23545;&#26032;&#35821;&#26009;&#24211;&#30340;&#36866;&#24212;&#24615;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;Dynamic Generative Retrieval (DynamicGR)&#23637;&#29616;&#20986;&#24847;&#22806;&#30340;&#21457;&#29616;&#12290;&#23427;&#33021;&#22815;&#22312;&#20854;&#20869;&#37096;&#32034;&#24341;&#20013;&#39640;&#25928;&#21387;&#32553;&#26032;&#30340;&#30693;&#35782;&#65292;
&lt;/p&gt;
&lt;p&gt;
The majority of prior work on information retrieval (IR) assumes that the corpus is static, whereas in the real world, the documents are continually updated. In this paper, we incorporate often overlooked dynamic nature of knowledge into the retrieval systems. Our work treats retrieval not as static archives but as dynamic knowledge bases better aligned with real-world environments. We conduct a comprehensive evaluation of dual encoders and generative retrieval, utilizing the StreamingQA benchmark designed for the temporal knowledge updates. Our initial results show that while generative retrieval outperforms dual encoders in static settings, the opposite is true in dynamic settings. Surprisingly, however, when we utilize a parameter-efficient pre-training method to enhance adaptability of generative retrieval to new corpora, our resulting model, Dynamic Generative Retrieval (DynamicGR), exhibits unexpected findings. It (1) efficiently compresses new knowledge in their internal index, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.18256</link><description>&lt;p&gt;
&#29992;Transformer&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#21644;&#25968;&#20540;&#30693;&#35782;&#22270;&#20013;&#30340;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#20102;&#19968;&#20010;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#19977;&#20803;&#32452;&#19982;&#38480;&#23450;&#35789;&#38598;&#21512;&#30456;&#20851;&#32852;; &#19968;&#20010;&#38480;&#23450;&#35789;&#30001;&#20851;&#31995;&#21644;&#23454;&#20307;&#32452;&#25104;&#65292;&#20026;&#19977;&#20803;&#32452;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20551;&#23450;&#23454;&#20307;&#26159;&#31163;&#25955;&#23545;&#35937;&#65292;&#20294;&#26377;&#20123;&#20449;&#24687;&#24212;&#20351;&#29992;&#25968;&#20540;&#34920;&#31034;&#65292;&#20363;&#22914;(J.R.R.&#65292;&#20986;&#29983;&#20110;&#65292;1892)&#12290;&#21516;&#26102;&#65292;&#19977;&#20803;&#32452;(J.R.R.&#65292;&#23601;&#35835;&#20110;&#65292;&#29275;&#27941;&#22823;&#23398;)&#21487;&#20197;&#19982;&#38480;&#23450;&#35789;(&#24320;&#22987;&#26102;&#38388;&#65292;1911)&#30456;&#20851;&#32852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21253;&#21547;&#19977;&#20803;&#32452;&#25110;&#38480;&#23450;&#35789;&#20013;&#25968;&#20540;&#25991;&#23383;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;Transformer&#21644;&#19968;&#20010;&#39044;&#27979;Transformer&#65292;&#26469;&#23398;&#20064;&#34920;&#31034;&#65292;&#19981;&#20165;&#22522;&#20110;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36824;&#22522;&#20110;&#25968;&#20540;&#20449;&#24687;&#12290;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#38480;&#23450;&#35789;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#39304;&#36865;&#32473;Transformer&#26469;&#33719;&#24471;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
&lt;/p&gt;</description></item><item><title>PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.07514</link><description>&lt;p&gt;
PI-FL&#65306;&#20010;&#24615;&#21270;&#21644;&#28608;&#21169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PI-FL: Personalized and Incentivized Federated Learning. (arXiv:2304.07514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07514
&lt;/p&gt;
&lt;p&gt;
PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24212;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#32771;&#34385;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#20010;&#24615;&#21270;&#36807;&#31243;&#20197;&#20445;&#25252;&#20854;&#33258;&#27835;&#26435;&#12290;&#20801;&#35768;&#23458;&#25143;&#31471;&#21442;&#19982;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20915;&#31574;&#21464;&#24471;&#37325;&#35201;&#65292;&#22240;&#20026;&#23384;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#26080;&#27861;&#33258;&#30001;&#20849;&#20139;&#29983;&#25104;&#33391;&#22909;&#36136;&#37327;&#20010;&#24615;&#21270;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#36164;&#28304;&#30340;&#23458;&#25143;&#31471;&#19981;&#24895;&#24847;&#22312;&#27809;&#26377;&#21512;&#29702;&#28608;&#21169;&#30340;&#24773;&#20917;&#19979;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PI-FL&#65292;&#36825;&#26159;&#19968;&#20010;&#19968;&#27425;&#24615;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#37197;&#21512;&#19968;&#20010;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#12290;PI-FL&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized FL has been widely used to cater to heterogeneity challenges with non-IID data. A primary obstacle is considering the personalization process from the client's perspective to preserve their autonomy. Allowing the clients to participate in personalized FL decisions becomes significant due to privacy and security concerns, where the clients may not be at liberty to share private information necessary for producing good quality personalized models. Moreover, clients with high-quality data and resources are reluctant to participate in the FL process without reasonable incentive. In this paper, we propose PI-FL, a one-shot personalization solution complemented by a token-based incentive mechanism that rewards personalized training. PI-FL outperforms other state-of-the-art approaches and can generate good-quality personalized models while respecting clients' privacy.
&lt;/p&gt;</description></item></channel></rss>