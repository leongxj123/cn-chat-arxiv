<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411; CF-CBMs&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12289;&#35299;&#37322;&#21644;&#24819;&#35937;&#33021;&#21147;&#30340;&#19981;&#36275;&#65292;&#20026;&#37096;&#32626;&#21487;&#38752;&#30340;AI&#20195;&#29702;&#12289;&#26657;&#20934;&#20154;&#31867;&#20449;&#20219;&#21644;&#21152;&#28145;&#20154;&#26426;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01408</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#25856;&#30331;&#35299;&#37322;&#24615;&#30340;&#38454;&#26799;
&lt;/p&gt;
&lt;p&gt;
Climbing the Ladder of Interpretability with Counterfactual Concept Bottleneck Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411; CF-CBMs&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12289;&#35299;&#37322;&#21644;&#24819;&#35937;&#33021;&#21147;&#30340;&#19981;&#36275;&#65292;&#20026;&#37096;&#32626;&#21487;&#38752;&#30340;AI&#20195;&#29702;&#12289;&#26657;&#20934;&#20154;&#31867;&#20449;&#20219;&#21644;&#21152;&#28145;&#20154;&#26426;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27809;&#26377;&#21516;&#26102;&#35299;&#20915;&#19977;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#35774;&#35745;&#65306;&#39044;&#27979;&#31867;&#21035;&#26631;&#31614;&#20197;&#35299;&#20915;&#32473;&#23450;&#30340;&#20998;&#31867;&#20219;&#21153;&#65288;&#8220;&#26159;&#20160;&#20040;&#65311;&#8221;&#65289;&#65292;&#35299;&#37322;&#20219;&#21153;&#39044;&#27979;&#65288;&#8220;&#20026;&#20160;&#20040;&#65311;&#8221;&#65289;&#65292;&#24182;&#24819;&#35937;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#39044;&#27979;&#30340;&#26367;&#20195;&#24773;&#26223;&#65288;&#8220;&#22914;&#26524;&#24590;&#26679;&#65311;&#8221;&#65289;&#12290;&#26080;&#27861;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20195;&#34920;&#20102;&#37096;&#32626;&#21487;&#38752;&#30340;AI&#20195;&#29702;&#12289;&#26657;&#20934;&#20154;&#31867;&#20449;&#20219;&#21644;&#21152;&#28145;&#20154;&#26426;&#20132;&#20114;&#30340;&#20851;&#38190;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CF-CBMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31867;&#33021;&#22815;&#39640;&#25928;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#26597;&#35810;&#32780;&#26080;&#38656;&#36827;&#34892;&#20107;&#21518;&#25628;&#32034;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CF-CBMs&#33021;&#22815;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#65288;&#8220;&#26159;&#20160;&#20040;&#65311;&#8221;&#65289;&#65292;&#23545;&#20219;&#21153;&#39044;&#27979;&#25552;&#20379;&#31616;&#21333;&#30340;&#35299;&#37322;&#65288;&#8220;&#20026;&#20160;&#20040;&#65311;&#8221;&#65289;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#65288;&#8220;&#22914;&#26524;&#24590;&#26679;&#65311;&#8221;&#65289;&#12290;CF-CBMs&#36824;&#21487;&#20197;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#24433;&#21709;&#36827;&#34892;&#37319;&#26679;&#25110;&#20272;&#35745;&#26368;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#65292;&#20197;&#35299;&#37322;&#20107;&#20214;&#65292;&#24182;&#20248;&#21270;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#21453;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the "What?"), explain task predictions (the "Why?"), and imagine alternative scenarios that could result in different predictions (the "What if?"). The inability to answer these questions represents a crucial gap in deploying reliable AI agents, calibrating human trust, and deepening human-machine interaction. To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches. Our results show that CF-CBMs produce: accurate predictions (the "What?"), simple explanations for task predictions (the "Why?"), and interpretable counterfactuals (the "What if?"). CF-CBMs can also sample or estimate the most probable counterfactual to: (i) explain the effect of concept interventions on tasks, (ii) sh
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102; PeersimGym &#29615;&#22659;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#20223;&#30495;&#29615;&#22659;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.17637</link><description>&lt;p&gt;
PeersimGym&#65306;&#29992;&#20110;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17637
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102; PeersimGym &#29615;&#22659;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#20223;&#30495;&#29615;&#22659;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21368;&#36733;&#23545;&#20110;&#22312;&#35832;&#22914;&#29289;&#32852;&#32593;&#20043;&#31867;&#30340;&#32593;&#32476;&#20013;&#24179;&#34913;&#35774;&#22791;&#30340;&#35745;&#31639;&#36127;&#36733;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#38754;&#20020;&#30528;&#35832;&#22914;&#22312;&#20005;&#26684;&#30340;&#36890;&#20449;&#21644;&#23384;&#20648;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#24310;&#36831;&#21644;&#33021;&#28304;&#20351;&#29992;&#31561;&#37325;&#35201;&#20248;&#21270;&#25361;&#25112;&#12290;&#20256;&#32479;&#20248;&#21270;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65307;&#21551;&#21457;&#24335;&#26041;&#27861;&#32570;&#20047;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#20801;&#35768;&#36890;&#36807;&#36845;&#20195;&#20132;&#20114;&#23398;&#20064;&#26368;&#20339;&#21368;&#36733;&#31574;&#30053;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;RL &#30340;&#21151;&#25928;&#21462;&#20915;&#20110;&#23545;&#20016;&#23500;&#25968;&#25454;&#38598;&#21644;&#23450;&#21046;&#30340;&#29616;&#23454;&#35757;&#32451;&#29615;&#22659;&#30340;&#35775;&#38382;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; PeersimGym&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;PeersimGym &#25903;&#25345;&#21508;&#31181;&#32593;&#32476;&#25299;&#25169;&#21644;&#35745;&#31639;&#32422;&#26463;&#65292;&#24182;&#25972;&#21512;&#20102;&#19968;&#31181;"PettingZo"&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#37197;&#32622;&#20223;&#30495;&#21442;&#25968;&#21644;&#30417;&#25511;&#20223;&#30495;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17637v1 Announce Type: cross  Abstract: Task offloading, crucial for balancing computational loads across devices in networks such as the Internet of Things, poses significant optimization challenges, including minimizing latency and energy usage under strict communication and storage constraints. While traditional optimization falls short in scalability; and heuristic approaches lack in achieving optimal outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the learning of optimal offloading strategies through iterative interactions. However, the efficacy of RL hinges on access to rich datasets and custom-tailored, realistic training environments. To address this, we introduce PeersimGym, an open-source, customizable simulation environment tailored for developing and optimizing task offloading strategies within computational networks. PeersimGym supports a wide range of network topologies and computational constraints and integrates a \textit{PettingZo
&lt;/p&gt;</description></item><item><title>QKFormer&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#33033;&#20914;&#24418;&#24335;Q-K&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#20998;&#23618;&#32467;&#26500;&#21644;&#34917;&#19969;&#23884;&#20837;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#33033;&#20914;&#21464;&#21387;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16552</link><description>&lt;p&gt;
QKFormer: &#20351;&#29992;Q-K&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#33033;&#20914;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
QKFormer: Hierarchical Spiking Transformer using Q-K Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16552
&lt;/p&gt;
&lt;p&gt;
QKFormer&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#33033;&#20914;&#24418;&#24335;Q-K&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#20998;&#23618;&#32467;&#26500;&#21644;&#34917;&#19969;&#23884;&#20837;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#33033;&#20914;&#21464;&#21387;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#21464;&#21387;&#22120;&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#21464;&#21387;&#22120;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#30001;&#20110;&#20854;&#33410;&#33021;&#39640;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#29616;&#26377;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#39033;&#21019;&#26032;&#65306;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;SNNs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#33033;&#20914;&#24418;&#24335;Q-K&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24615;&#30340;&#20108;&#36827;&#21046;&#21521;&#37327;&#26377;&#25928;&#22320;&#24314;&#27169;&#20196;&#29260;&#25110;&#36890;&#36947;&#32500;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;ii&#65289;&#25105;&#20204;&#23558;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#30340;&#20998;&#23618;&#32467;&#26500;&#24341;&#20837;&#33033;&#20914;&#21464;&#21387;&#22120;&#65292;&#20174;&#32780;&#33719;&#24471;&#22810;&#23610;&#24230;&#33033;&#20914;&#34920;&#31034;&#65292;&#36825;&#23545;&#22823;&#33041;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#37117;&#26377;&#26174;&#30528;&#22909;&#22788;&#12290;iii&#65289;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#24378;&#22823;&#30340;&#34917;&#19969;&#23884;&#20837;&#27169;&#22359;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#33033;&#20914;&#21464;&#21387;&#22120;&#35774;&#35745;&#30340;&#21464;&#24418;&#24555;&#25463;&#26041;&#24335;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;QKFormer&#65292;&#19968;&#31181;&#20998;&#23618;&#33033;&#20914;&#21464;&#21387;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16552v1 Announce Type: cross  Abstract: Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance. However, existing models in this domain still suffer from suboptimal performance. We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity. ii) We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation. iii) We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers. Together, we develop QKFormer, a hierarchical spiking transformer
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#23454;&#29616;&#33041;&#30005;&#20449;&#21495;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;Signal-JEPA&#29992;&#20110;&#34920;&#31034;&#33041;&#30005;&#35760;&#24405;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#31934;&#30830;&#19979;&#28216;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11772</link><description>&lt;p&gt;
S-JEPA&#65306;&#36890;&#36807;&#21160;&#24577;&#31354;&#38388;&#27880;&#24847;&#21147;&#23454;&#29616;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#23454;&#29616;&#33041;&#30005;&#20449;&#21495;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;Signal-JEPA&#29992;&#20110;&#34920;&#31034;&#33041;&#30005;&#35760;&#24405;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#31934;&#30830;&#19979;&#28216;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#33041;&#30005;&#20449;&#21495;&#22788;&#29702;&#20013;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#36801;&#31227;&#23398;&#20064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#33041;&#30005;&#20449;&#21495;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#34920;&#31034;&#33041;&#30005;&#35760;&#24405;&#30340;Signal-JEPA&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#29305;&#23450;&#31354;&#38388;&#22359;&#25513;&#34109;&#31574;&#30053;&#21644;&#19977;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#19979;&#28216;&#20998;&#31867;&#30340;&#26550;&#26500;&#12290;&#35813;&#30740;&#31350;&#22312;&#19968;&#20010;54&#20010;&#21463;&#35797;&#32773;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;BCI&#33539;&#24335;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;&#36816;&#21160;&#24819;&#35937;&#12289;ERP&#21644;SSVEP&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;JEPAs&#22312;&#33041;&#30005;&#20449;&#21495;&#32534;&#30721;&#20013;&#30340;&#28508;&#21147;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#31354;&#38388;&#28388;&#27874;&#23545;&#20934;&#30830;&#19979;&#28216;&#20998;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11772v1 Announce Type: cross  Abstract: Motivated by the challenge of seamless cross-dataset transfer in EEG signal processing, this article presents an exploratory study on the use of Joint Embedding Predictive Architectures (JEPAs). In recent years, self-supervised learning has emerged as a promising approach for transfer learning in various domains. However, its application to EEG signals remains largely unexplored. In this article, we introduce Signal-JEPA for representing EEG recordings which includes a novel domain-specific spatial block masking strategy and three novel architectures for downstream classification. The study is conducted on a 54~subjects dataset and the downstream performance of the models is evaluated on three different BCI paradigms: motor imagery, ERP and SSVEP. Our study provides preliminary evidence for the potential of JEPAs in EEG signal encoding. Notably, our results highlight the importance of spatial filtering for accurate downstream classific
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#24341;&#23548;&#30340;&#28508;&#22312;&#19968;&#33268;&#24615;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LCD&#36807;&#31243;&#20013;&#25972;&#21512;&#22870;&#21169;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#39640;&#20445;&#30495;&#22270;&#20687;&#29983;&#25104;&#26102;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.11027</link><description>&lt;p&gt;
&#22870;&#21169;&#24341;&#23548;&#30340;&#28508;&#22312;&#19968;&#33268;&#24615;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Reward Guided Latent Consistency Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#24341;&#23548;&#30340;&#28508;&#22312;&#19968;&#33268;&#24615;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LCD&#36807;&#31243;&#20013;&#25972;&#21512;&#22870;&#21169;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#39640;&#20445;&#30495;&#22270;&#20687;&#29983;&#25104;&#26102;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#19968;&#33268;&#24615;&#33976;&#39311;(LCD)&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#33539;&#24335;&#12290;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#20013;&#33976;&#39311;&#20986;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;(LCM)&#65292;LCD&#22312;&#20165;&#38656;2&#21040;4&#20010;&#25512;&#29702;&#27493;&#39588;&#20869;&#20419;&#36827;&#20102;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;LCM&#30340;&#39640;&#25928;&#25512;&#29702;&#26159;&#20197;&#26679;&#26412;&#36136;&#37327;&#20026;&#20195;&#20215;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;LCM&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#26469;&#34917;&#20607;&#36136;&#37327;&#25439;&#22833;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#22870;&#21169;&#24341;&#23548;&#30340;LCD(RG-LCD)&#65292;&#36890;&#36807;&#23558;&#22870;&#21169;&#27169;&#22411;(RM)&#30340;&#21453;&#39304;&#25972;&#21512;&#21040;LCD&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;LCD&#25439;&#22833;&#19982;&#26368;&#22823;&#21270;&#19982;LCM&#21333;&#27493;&#29983;&#25104;&#30456;&#20851;&#32852;&#30340;&#22870;&#21169;&#30340;&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#26469;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#65292;&#24403;&#20351;&#29992;&#33391;&#22909;RM&#30340;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;RG-LCM&#30340;2&#27493;&#29983;&#25104;&#34987;&#20154;&#31867;&#38738;&#30544;&#65292;&#36229;&#36807;&#20102;50&#27493;DDIM&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11027v1 Announce Type: cross  Abstract: Latent Consistency Distillation (LCD) has emerged as a promising paradigm for efficient text-to-image synthesis. By distilling a latent consistency model (LCM) from a pre-trained teacher latent diffusion model (LDM), LCD facilitates the generation of high-fidelity images within merely 2 to 4 inference steps. However, the LCM's efficient inference is obtained at the cost of the sample quality. In this paper, we propose compensating the quality loss by aligning LCM's output with human preference during training. Specifically, we introduce Reward Guided LCD (RG-LCD), which integrates feedback from a reward model (RM) into the LCD process by augmenting the original LCD loss with the objective of maximizing the reward associated with LCM's single-step generation. As validated through human evaluation, when trained with the feedback of a good RM, the 2-step generations from our RG-LCM are favored by humans over the 50-step DDIM samples from 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#21644;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#65292;DSEG-LIME&#25913;&#36827;&#20102;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07733</link><description>&lt;p&gt;
DSEG-LIME -- &#36890;&#36807;&#23618;&#27425;&#21270;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#25552;&#21319;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DSEG-LIME -- Improving Image Explanation by Hierarchical Data-Driven Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#21644;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#65292;DSEG-LIME&#25913;&#36827;&#20102;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#25581;&#31034;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;LIME (Local Interpretable Model-agnostic Explanations) &#26159;&#19968;&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#29992;&#20110;&#22270;&#20687;&#20998;&#26512;&#30340;XAI&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#26469;&#21019;&#24314;&#29305;&#24449;&#20197;&#35782;&#21035;&#30456;&#20851;&#30340;&#20998;&#31867;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36739;&#24046;&#30340;&#20998;&#21106;&#21487;&#33021;&#20250;&#24433;&#21709;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#24182;&#21066;&#24369;&#21508;&#20010;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#25972;&#20307;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DSEG-LIME (Data-Driven Segmentation LIME)&#65292;&#20855;&#26377;: i) &#29992;&#20110;&#29983;&#25104;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#30340;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;, &#21644; ii) &#36890;&#36807;&#32452;&#21512;&#23454;&#29616;&#30340;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#20351;&#29992;&#26469;&#33258;ImageNet&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#23545;DSEG-LIME&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;-&#36825;&#20123;&#24773;&#26223;&#19981;&#21253;&#21547;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#20998;&#26512;&#21253;&#25324;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;XAI&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07733v1 Announce Type: cross  Abstract: Explainable Artificial Intelligence is critical in unraveling decision-making processes in complex machine learning models. LIME (Local Interpretable Model-agnostic Explanations) is a well-known XAI framework for image analysis. It utilizes image segmentation to create features to identify relevant areas for classification. Consequently, poor segmentation can compromise the consistency of the explanation and undermine the importance of the segments, affecting the overall interpretability. Addressing these challenges, we introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a data-driven segmentation for human-recognized feature generation, and ii) a hierarchical segmentation procedure through composition. We benchmark DSEG-LIME on pre-trained models with images from the ImageNet dataset - scenarios without domain-specific knowledge. The analysis includes a quantitative evaluation using established XAI metrics, complemented
&lt;/p&gt;</description></item><item><title>&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07404</link><description>&lt;p&gt;
&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#20943;&#23569;&#36951;&#24536;&#65306;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#21452;&#37325;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07404
&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#21463;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#28304;&#39640;&#25928;&#21033;&#29992;&#38656;&#27714;&#39537;&#21160;&#65292;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#31574;&#30053;&#36890;&#36807;&#22312;&#32593;&#32476;&#26089;&#26399;&#20570;&#20986;&#20915;&#23450;&#65292;&#23454;&#29616;&#24555;&#36895;&#39044;&#27979;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#20165;&#38024;&#23545;&#38745;&#24577;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20102;&#24320;&#21457;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#25345;&#32493;&#38750;&#38745;&#24577;&#25968;&#25454;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#25105;&#20204;&#25913;&#32534;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#20197;&#36866;&#24212;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#25345;&#32493;&#35774;&#32622;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#26089;&#26399;&#32593;&#32476;&#23618;&#34920;&#29616;&#20986;&#20943;&#23569;&#36951;&#24536;&#65292;&#21363;&#20351;&#20351;&#29992;&#30340;&#36164;&#28304;&#26174;&#33879;&#26356;&#23569;&#65292;&#20063;&#33021;&#32988;&#36807;&#26631;&#20934;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20219;&#21153;&#26368;&#36817;&#24615;&#20559;&#24046;&#23545;&#26089;&#26399;&#36864;&#20986;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20219;&#21153;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 Announce Type: cross  Abstract: Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#24314;&#20102;ROME&#65292;&#25552;&#20379;&#20102;&#26356;&#31283;&#23450;&#30340;r-ROME&#23454;&#29616;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07175</link><description>&lt;p&gt;
&#37325;&#24314;ROME: &#35299;&#20915;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#24314;&#20102;ROME&#65292;&#25552;&#20379;&#20102;&#26356;&#31283;&#23450;&#30340;r-ROME&#23454;&#29616;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;Rank-One Model Editing (ROME)&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26377;&#19968;&#20123;&#20107;&#23454;&#34920;&#26126;&#35813;&#31639;&#27861;&#26080;&#27861;&#36827;&#34892;&#32534;&#36753;&#32780;&#19981;&#30772;&#22351;&#27169;&#22411;&#12290;&#36825;&#20123;&#32534;&#36753;&#20197;&#21069;&#34987;&#31216;&#20026;&#31105;&#29992;&#32534;&#36753;&#12290;&#36825;&#20123;&#31105;&#29992;&#32534;&#36753;&#20250;&#23548;&#33268;&#31435;&#21363;&#27169;&#22411;&#23849;&#28291;&#65292;&#24182;&#38480;&#21046;&#20102;ROME&#29992;&#20110;&#39034;&#24207;&#32534;&#36753;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20351;&#29992;CounterFact&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#36753;&#26102;&#65292;ROME&#20165;&#22312;&#27492;&#26102;&#21457;&#29983;&#27169;&#22411;&#23849;&#28291;&#65292;&#24182;&#22312;&#20351;&#29992;zsRE&#25968;&#25454;&#38598;&#26102;&#19981;&#20250;&#21457;&#29983;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#31105;&#29992;&#32534;&#36753;&#26159;ROME&#21407;&#22987;&#23454;&#29616;&#30340;&#20135;&#29289;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#31283;&#23450;&#30340;&#23454;&#29616;ROME&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;r-ROME&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#22312;&#20351;&#29992;ROME&#36827;&#34892;&#22823;&#35268;&#27169;&#39034;&#24207;&#32534;&#36753;&#26102;&#19981;&#20877;&#35266;&#23519;&#21040;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07175v1 Announce Type: cross  Abstract: Recent work on model editing using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we make two main contributions. Firstly, we show that model collapse with ROME only happens when making edits using the CounterFact dataset and does not happen when using the zsRE dataset. Secondly, we find that disabling edits are an artifact of the original implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that we no longer observe model collapse when making large scale sequential edits with ROME.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#23545;&#20110;&#29289;&#20307;&#19982;&#32972;&#26223;&#20043;&#38388;&#22810;&#26679;&#21270;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#24341;&#20837;&#19981;&#21516;&#23545;&#35937;&#26041;&#38754;&#21464;&#21270;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.04701</link><description>&lt;p&gt;
ObjectCompose: &#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#22312;&#29289;&#20307;&#19982;&#32972;&#26223;&#32452;&#21512;&#21464;&#21270;&#19978;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04701
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#23545;&#20110;&#29289;&#20307;&#19982;&#32972;&#26223;&#20043;&#38388;&#22810;&#26679;&#21270;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#24341;&#20837;&#19981;&#21516;&#23545;&#35937;&#26041;&#38754;&#21464;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35757;&#32451;&#24182;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#20102;&#35299;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#31243;&#24230;&#23545;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#38024;&#23545;&#19981;&#21516;&#30340;&#29289;&#20307;&#19982;&#32972;&#26223;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#38887;&#24615;&#12290;&#22823;&#22810;&#25968;&#40065;&#26834;&#24615;&#35780;&#20272;&#26041;&#27861;&#24341;&#20837;&#20102;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#35825;&#23548;&#29289;&#20307;&#29305;&#24449;&#65288;&#35270;&#28857;&#12289;&#23610;&#24230;&#12289;&#39068;&#33394;&#65289;&#30340;&#21464;&#21270;&#65292;&#25110;&#32773;&#21033;&#29992;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#65288;&#23545;&#25239;&#24615;&#21464;&#21270;&#12289;&#24120;&#35265;&#30772;&#22351;&#65289;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#27169;&#25311;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#32972;&#26223;&#30340;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#22312;&#25552;&#20379;&#23545;&#35201;&#36827;&#34892;&#30340;&#26356;&#25913;&#30340;&#25511;&#21046;&#26041;&#38754;&#19981;&#36275;&#65292;&#35201;&#20040;&#25197;&#26354;&#20102;&#29289;&#20307;&#30340;&#35821;&#20041;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24341;&#20837;&#21508;&#31181;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04701v1 Announce Type: cross  Abstract: Given the large-scale multi-modal training of recent vision-based models and their generalization capabilities, understanding the extent of their robustness is critical for their real-world deployment. In this work, we evaluate the resilience of current vision-based models against diverse object-to-background context variations. The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions. Recent works have explored leveraging large language models and diffusion models to generate changes in the background. However, these methods either lack in offering control over the changes to be made or distort the object semantics, making them unsuitable for the task. Our method, on the other hand, can induce diverse objec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02966</link><description>&lt;p&gt;
&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#29992;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;-shot&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#24615;&#33021;&#65292;&#28982;&#32780;&#32467;&#26500;&#21270;&#30340;KG&#24418;&#24335;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#19977;&#20803;&#32452;&#24418;&#24335;&#25110;&#19977;&#20803;&#32452;&#20107;&#23454;&#30340;&#33258;&#30001;&#25991;&#26412;&#36716;&#25442;&#65292;&#36935;&#21040;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#30001;&#20110;&#37325;&#22797;&#23454;&#20307;&#25110;&#20851;&#31995;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#23494;&#24230;&#38477;&#20302;&#65292;&#20197;&#21450;&#30001;&#20110;&#26080;&#27861;&#24378;&#35843;&#20851;&#38190;&#35777;&#25454;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#28165;&#26224;&#24230;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EFSum&#65292;&#19968;&#20010;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;LLMs&#22686;&#24378;QA&#12290;&#25105;&#20204;&#36890;&#36807;&#33976;&#39311;&#21644;&#20559;&#22909;&#23545;&#40784;&#26469;&#20248;&#21270;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#20316;&#20026;&#20107;&#23454;&#25688;&#35201;&#22120;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;EFSum&#25552;&#39640;&#20102;LLM&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#30830;&#20445;&#25688;&#35201;&#30340;&#21516;&#26102;&#26377;&#30410;&#21644;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.01121</link><description>&lt;p&gt;
OpenGraph: &#36808;&#21521;&#24320;&#25918;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenGraph: Towards Open Graph Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#20132;&#20114;   &#25688;&#35201;: &#22270;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#37322;&#21644;&#21033;&#29992;&#21508;&#39046;&#22495;&#30340;&#20851;&#31995;&#25968;&#25454;&#30340;&#19981;&#21487;&#25110;&#32570;&#37096;&#20998;&#65292;&#20174;&#25512;&#33616;&#31995;&#32479;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#21508;&#31181;GNN&#24050;&#32463;&#25104;&#20026;&#32534;&#30721;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#36825;&#20123;GNN&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22686;&#24378;&#22270;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;: &#36825;&#20123;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#22312;&#23558;&#26174;&#33879;&#19981;&#21516;&#20110;&#35757;&#32451;&#23454;&#20363;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#27867;&#21270;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#26469;&#25512;&#36827;&#22270;&#23398;&#20064;&#33539;&#24335;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#29702;&#35299;&#22810;&#26679;&#22270;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#25299;&#25169;&#27169;&#24335;&#65292;&#20351;&#20854;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 Announce Type: cross  Abstract: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#29983;&#25104;&#20102;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65292;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32467;&#26500;</title><link>https://arxiv.org/abs/2402.15487</link><description>&lt;p&gt;
RoboEXP: &#36890;&#36807;&#20132;&#20114;&#24335;&#25506;&#32034;&#23454;&#29616;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#29983;&#25104;&#20102;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65292;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#25506;&#32034;&#21608;&#22260;&#29615;&#22659;&#20197;&#36866;&#24212;&#24182;&#24212;&#23545;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#24182;&#29983;&#25104;&#19968;&#20010;&#25429;&#25417;&#22522;&#30784;&#29615;&#22659;&#32467;&#26500;&#30340;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65288;ACSG&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15487v1 Announce Type: cross  Abstract: Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments. Prior work has proposed building scene graphs of the environment but typically assumes that the environment is static, omitting regions that require active interactions. This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments. In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment. The ACSG accounts for both low-level information, such as geometry and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene. To this end, we present the Robotic Explo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;</title><link>https://arxiv.org/abs/2402.12365</link><description>&lt;p&gt;
&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Universal Physics Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26367;&#20195;&#32773;&#36817;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#23427;&#20204;&#30340;&#25968;&#20540;&#23545;&#24212;&#29289;&#65292;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21363;&#20351;&#31995;&#32479;&#30340;&#22522;&#30784;&#21160;&#24577;&#30456;&#20284;&#12290;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#26159;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#34920;&#36848;&#65292;&#36825;&#20026;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#24314;&#27169;&#22522;&#20110;&#31890;&#23376;&#32780;&#19981;&#26159;&#32593;&#26684;&#30340;&#21160;&#24577;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#27169;&#25311;&#20102;&#19968;&#31995;&#21015;&#26102;&#31354;&#38382;&#39064; - &#23545;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#12290;UPTs&#22312;&#27809;&#26377;&#22522;&#20110;&#32593;&#26684;&#25110;&#22522;&#20110;&#31890;&#23376;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#20174;&#32780;&#22312;&#32593;&#26684;&#21644;&#31890;&#23376;&#20043;&#38388;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;UPTs&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#20256;&#25773;&#21160;&#24577;&#65292;&#24378;&#35843;&#20102;&#36870;&#32534;&#30721;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;UPTs&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12365v1 Announce Type: cross  Abstract: Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space repre
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12265</link><description>&lt;p&gt;
&#35770;&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Byzantine-Resilience of Distillation-Based Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12265
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#38544;&#31169;&#12289;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31639;&#27861;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;KD&#30340;FL&#31639;&#27861;&#30456;&#24403;&#20855;&#26377;&#24377;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#36807;&#31243;&#30456;&#23545;&#20110;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#12290;&#26681;&#25454;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#23545;&#20808;&#21069;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FilterExp&#65292;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12265v1 Announce Type: cross  Abstract: Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilien
&lt;/p&gt;</description></item><item><title>&#20998;&#38548;&#31526;&#30340;&#24341;&#20837;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.10645</link><description>&lt;p&gt;
&#20998;&#38548;&#31526;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#25928;&#26524;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Separators Improve Chain-of-Thought Prompting?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10645
&lt;/p&gt;
&lt;p&gt;
&#20998;&#38548;&#31526;&#30340;&#24341;&#20837;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;CoT&#30340;&#22522;&#26412;&#29702;&#24565;&#26159;&#36890;&#36807;&#23558;&#31034;&#20363;&#25918;&#22312;&#36755;&#20837;&#25552;&#31034;&#20013;&#65292;&#35753;LLMs&#36880;&#27493;&#25286;&#35299;&#20182;&#20204;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;CoT&#25552;&#31034;&#30340;&#23494;&#38598;&#32467;&#26500;&#21487;&#33021;&#23548;&#33268;LLMs&#30340;&#35748;&#30693;&#36127;&#33655;&#36807;&#37325;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoT-Sep&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;CoT&#25552;&#31034;&#20013;&#27599;&#20010;&#31034;&#20363;&#30340;&#26411;&#23614;&#31574;&#30053;&#24615;&#22320;&#24212;&#29992;&#20998;&#38548;&#31526;&#12290;&#36825;&#20123;&#20998;&#38548;&#31526;&#26088;&#22312;&#24110;&#21161;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#29702;&#35299;&#20182;&#20204;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#19981;&#20351;&#29992;&#20998;&#38548;&#31526;&#30340;&#26222;&#36890;CoT&#30456;&#27604;&#65292;CoT-Sep&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;GSM-8K&#12289;AQuA&#12289;CSQA&#65289;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#20998;&#38548;&#31526;&#23545;&#22810;&#20010;LLMs&#65288;&#21253;&#25324;GPT-3.5-Turbo&#12289;GPT-4&#21644;LLaMA-27&#65289;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10645v1 Announce Type: cross  Abstract: Chain-of-thought (CoT) prompting is a simple and effective method for improving the reasoning capabilities of Large language models (LLMs). The basic idea of CoT is to let LLMs break down their thought processes step-by-step by putting exemplars in the input prompt. However, the densely structured prompt exemplars of CoT may cause the cognitive overload of LLMs. Inspired by human cognition, we introduce CoT-Sep, a novel method that strategically employs separators at the end of each exemplar in CoT prompting. These separators are designed to help the LLMs understand their thought processes better while reasoning. It turns out that CoT-Sep significantly improves the LLMs' performances on complex reasoning tasks (e.g., GSM-8K, AQuA, CSQA), compared with the vanilla CoT, which does not use separators. We also study the effects of the type and the location of separators tested on multiple LLMs, including GPT-3.5-Turbo, GPT-4, and LLaMA-2 7
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#22270;&#23376;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#20943;&#23567;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10206</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#29305;&#23450;&#20219;&#21153;&#22270;&#23376;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Ising on the Graph: Task-specific Graph Subsampling via the Ising Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#22270;&#23376;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#20943;&#23567;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#22270;&#30340;&#22823;&#23567;&#21516;&#26102;&#20445;&#25345;&#20854;&#25972;&#20307;&#32467;&#26500;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#20943;&#23567;&#22270;&#30340;&#26041;&#27861;&#35201;&#20040;&#21024;&#38500;&#36793;&#32536;&#65288;&#31232;&#30095;&#21270;&#65289;&#65292;&#35201;&#20040;&#21512;&#24182;&#33410;&#28857;&#65288;&#31895;&#21270;&#65289;&#65292;&#32780;&#27809;&#26377;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22312;&#33410;&#28857;&#25110;&#36793;&#19978;&#23450;&#20041;&#30340;&#20234;&#36763;&#27169;&#22411;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#23376;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#22914;&#20309;&#20026;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#12290;&#25152;&#20351;&#29992;&#30340;&#20219;&#21153;&#25439;&#22833;&#20989;&#25968;&#29978;&#33267;&#19981;&#38656;&#35201;&#21487;&#24494;&#20998;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24212;&#29992;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#65306;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10206v1 Announce Type: cross  Abstract: Reducing a graph while preserving its overall structure is an important problem with many applications. Typically, the reduction approaches either remove edges (sparsification) or merge nodes (coarsening) in an unsupervised way with no specific downstream task in mind. In this paper, we present an approach for subsampling graph structures using an Ising model defined on either the nodes or edges and learning the external magnetic field of the Ising model using a graph neural network. Our approach is task-specific as it can learn how to reduce a graph for a specific downstream task in an end-to-end fashion. The utilized loss function of the task does not even have to be differentiable. We showcase the versatility of our approach on three distinct applications: image segmentation, 3D shape sparsification, and sparse approximate matrix inverse determination.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;GPS&#36712;&#36857;&#26469;&#32472;&#21046;&#24314;&#31569;&#24037;&#22320;&#36947;&#36335;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#20851;&#38190;&#30340;&#20132;&#21449;&#21475;&#24182;&#36830;&#25509;&#23427;&#20204;&#65292;&#29983;&#25104;&#36947;&#36335;&#22270;&#65292;&#20026;&#35268;&#21010;&#21644;&#20219;&#21153;&#20998;&#37197;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.09919</link><description>&lt;p&gt;
&#36947;&#36335;&#22270;&#29983;&#25104;&#22120;&#65306;&#20174;GPS&#25968;&#25454;&#20013;&#29983;&#25104;&#24314;&#31569;&#24037;&#22320;&#36947;&#36335;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Road Graph Generator: Mapping roads at construction sites from GPS data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;GPS&#36712;&#36857;&#26469;&#32472;&#21046;&#24314;&#31569;&#24037;&#22320;&#36947;&#36335;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#20851;&#38190;&#30340;&#20132;&#21449;&#21475;&#24182;&#36830;&#25509;&#23427;&#20204;&#65292;&#29983;&#25104;&#36947;&#36335;&#22270;&#65292;&#20026;&#35268;&#21010;&#21644;&#20219;&#21153;&#20998;&#37197;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;GPS&#36712;&#36857;&#20013;&#25512;&#27979;&#36947;&#36335;&#20197;&#32472;&#21046;&#24314;&#31569;&#24037;&#22320;&#22320;&#22270;&#30340;&#26041;&#27861;&#12290;&#36825;&#39033;&#20219;&#21153;&#30001;&#20110;&#24314;&#31569;&#26426;&#26800;&#30340;&#19981;&#35268;&#21017;&#21644;&#38750;&#26631;&#20934;&#36816;&#21160;&#27169;&#24335;&#19982;&#24050;&#24314;&#31435;&#36947;&#36335;&#19978;&#30340; typcial &#36710;&#36742;&#20132;&#36890;&#26174;&#33879;&#19981;&#21516;&#65292;&#22240;&#27492;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#35782;&#21035;&#36947;&#36335;&#32593;&#32476;&#20013;&#20316;&#20026;&#20851;&#38190;&#20915;&#31574;&#28857;&#30340;&#20132;&#21449;&#21475;&#65292;&#28982;&#21518;&#36830;&#25509;&#23427;&#20204;&#20197;&#24418;&#25104;&#19968;&#20010;&#22270;&#65292;&#38543;&#21518;&#21487;&#20197;&#29992;&#20110;&#35268;&#21010;&#21644;&#20219;&#21153;&#20998;&#37197;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#25386;&#23041;&#30340;&#19968;&#20010;&#23454;&#38469;&#24314;&#31569;&#24037;&#22320;&#32472;&#21046;&#36947;&#36335;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09919v1 Announce Type: new  Abstract: We present a method for road inference from GPS trajectories to map construction sites. This task introduces a unique challenge due to the erratic and non-standard movement patterns of construction machinery, which diverge significantly from typical vehicular traffic on established roads. Our method first identifies intersections in the road network that serve as critical decision points, and later connects them with edges, producing a graph, which subsequently can be used for planning and task-allocation. We demonstrate the effectiveness of our approach by mapping roads at a real-life construction site in Norway.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#21644;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#65292;&#20197;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#26469;&#23454;&#29616;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23454;&#35777;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09281</link><description>&lt;p&gt;
&#25552;&#21319;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#21327;&#26041;&#24046;&#21644;Hessian&#30697;&#38453;&#30340;&#21327;&#21516;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#21644;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#65292;&#20197;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#26469;&#23454;&#29616;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23454;&#35777;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#21327;&#26041;&#24046;&#21644;Hessian&#30697;&#38453;&#20998;&#21035;&#34987;&#21333;&#29420;&#20998;&#26512;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#30697;&#38453;&#38598;&#25104;&#36215;&#26469;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#35777;&#26126;&#20102;&#23427;&#21487;&#20197;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#24182;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#25353;&#29031;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#30340;&#26631;&#20934;&#23454;&#29616;&#20102;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#21644;&#20581;&#24247;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#39564;&#35777;&#22987;&#32456;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09281v1 Announce Type: cross Abstract: Covariance and Hessian matrices have been analyzed separately in the literature for classification problems. However, integrating these matrices has the potential to enhance their combined power in improving classification performance. We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks. Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances. By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria. Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#38544;&#31169;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#24635;&#32467;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;</title><link>https://arxiv.org/abs/2402.05525</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Model-Based Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#38544;&#31169;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#24635;&#32467;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20855;&#26377;&#38544;&#31169;&#20445;&#35777;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#35757;&#32451;&#19968;&#20010;&#30456;&#23545;&#20110;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#36712;&#36857;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DP-MORL&#65292;&#19968;&#31181;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;MBRL&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;DP-FedAvg&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#29615;&#22659;&#30340;&#38544;&#31169;&#27169;&#22411;&#65292;DP-FedAvg&#26159;&#19968;&#31181;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#36712;&#36857;&#32423;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#20174;&#65288;&#21463;&#32602;&#30340;&#65289;&#38544;&#31169;&#27169;&#22411;&#20013;&#25512;&#23548;&#20986;&#31574;&#30053;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#19982;&#31995;&#32479;&#20132;&#20114;&#25110;&#35775;&#38382;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;DP-MORL&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;RL&#20195;&#29702;&#65292;&#24182;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address offline reinforcement learning with privacy guarantees, where the goal is to train a policy that is differentially private with respect to individual trajectories in the dataset. To achieve this, we introduce DP-MORL, an MBRL algorithm coming with differential privacy guarantees. A private model of the environment is first learned from offline data using DP-FedAvg, a training method for neural networks that provides differential privacy guarantees at the trajectory level. Then, we use model-based policy optimization to derive a policy from the (penalized) private model, without any further interaction with the system or access to the input data. We empirically show that DP-MORL enables the training of private RL agents from offline data and we furthermore outline the price of privacy in this setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20462;&#25913;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#38480;&#21046;&#21069;K&#20010;softmax&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#33021;&#22815;&#26377;&#25928;&#25269;&#24481;&#24120;&#35265;&#30340;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.03627</link><description>&lt;p&gt;
&#36817;&#20284;&#30340;&#20013;&#24515;&#21270;softmax&#25439;&#22833;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Partially Recentralization Softmax Loss for Vision-Language Models Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20462;&#25913;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#38480;&#21046;&#21069;K&#20010;softmax&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#33021;&#22815;&#26377;&#25928;&#25269;&#24481;&#24120;&#35265;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#31361;&#30772;&#65292;&#22810;&#27169;&#24577;&#25216;&#26415;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#21363;&#27169;&#22411;&#30340;&#36755;&#20986;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#32780;&#21457;&#29983;&#24040;&#22823;&#21464;&#21270;&#12290;&#34429;&#28982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#38450;&#24481;&#25216;&#26415;&#65292;&#20294;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36824;&#27809;&#26377;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#20462;&#25913;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38480;&#21046;&#21069;K&#20010;softmax&#36755;&#20986;&#26469;&#25552;&#20379;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#35780;&#20272;&#21644;&#35780;&#20998;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#65292;&#23545;&#25239;&#24120;&#35265;&#30340;&#25915;&#20987;&#26377;&#25928;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#24212;&#35813;&#25506;&#32034;&#36825;&#31867;&#25439;&#22833;&#20989;&#25968;&#30340;&#36755;&#20986;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;&#20043;&#21518;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models make a breakthrough in natural language processing tasks (NLP), multimodal technique becomes extremely popular. However, it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input. While several defense techniques have been proposed both in computer vision and NLP models, the multimodal robustness of models have not been fully explored. In this paper, we study the adversarial robustness provided by modifying loss function of pre-trained multimodal models, by restricting top K softmax outputs. Based on the evaluation and scoring, our experiments show that after a fine-tuning, adversarial robustness of pre-trained models can be significantly improved, against popular attacks. Further research should be studying, such as output diversity, generalization and the robustness-performance trade-off of this kind of loss functions. Our code will be available after th
&lt;/p&gt;</description></item><item><title>SWAG&#26159;&#19968;&#31181;&#26032;&#30340;&#25925;&#20107;&#35762;&#36848;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25925;&#20107;&#20889;&#20316;&#31616;&#21270;&#20026;&#25628;&#32034;&#38382;&#39064;&#65292;&#20351;&#29992;&#20004;&#20010;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#26469;&#25351;&#23548;&#25925;&#20107;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#22312;GPT-4&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;SWAG&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#20351;&#29992;&#20165;&#24320;&#28304;&#27169;&#22411;&#30340;SWAG&#27969;&#31243;&#36229;&#36807;&#20102;GPT-3.5-Turbo&#12290;</title><link>https://arxiv.org/abs/2402.03483</link><description>&lt;p&gt;
SWAG: &#24102;&#26377;&#34892;&#21160;&#25351;&#23548;&#30340;&#25925;&#20107;&#35762;&#36848;
&lt;/p&gt;
&lt;p&gt;
SWAG: Storytelling With Action Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03483
&lt;/p&gt;
&lt;p&gt;
SWAG&#26159;&#19968;&#31181;&#26032;&#30340;&#25925;&#20107;&#35762;&#36848;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25925;&#20107;&#20889;&#20316;&#31616;&#21270;&#20026;&#25628;&#32034;&#38382;&#39064;&#65292;&#20351;&#29992;&#20004;&#20010;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#26469;&#25351;&#23548;&#25925;&#20107;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#22312;GPT-4&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;SWAG&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#20351;&#29992;&#20165;&#24320;&#28304;&#27169;&#22411;&#30340;SWAG&#27969;&#31243;&#36229;&#36807;&#20102;GPT-3.5-Turbo&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#38271;&#31687;&#25925;&#20107;&#29983;&#25104;&#36890;&#24120;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19968;&#27425;&#24615;&#21019;&#24314;&#65292;&#23427;&#21487;&#20197;&#20135;&#29983;&#36830;&#36143;&#20294;&#19981;&#19968;&#23450;&#24341;&#20154;&#20837;&#32988;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#26377;&#34892;&#21160;&#25351;&#23548;&#30340;&#25925;&#20107;&#35762;&#36848;&#65288;SWAG&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#23558;&#25925;&#20107;&#20889;&#20316;&#31616;&#21270;&#20026;&#19968;&#20010;&#25628;&#32034;&#38382;&#39064;&#65306;&#19968;&#20010;LLM&#29983;&#25104;&#25925;&#20107;&#20869;&#23481;&#65292;&#21478;&#19968;&#20010;&#36741;&#21161;LLM&#29992;&#20110;&#36873;&#25321;&#19979;&#19968;&#20010;&#26368;&#20339;&#30340;&#8220;&#34892;&#21160;&#8221;&#65292;&#20197;&#24341;&#23548;&#25925;&#20107;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;GPT-4&#21644;&#20154;&#24037;&#35780;&#20272;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;SWAG&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#30340;&#31471;&#21040;&#31471;&#25925;&#20107;&#29983;&#25104;&#25216;&#26415;&#65292;&#24182;&#19988;&#25105;&#20204;&#21482;&#20351;&#29992;&#24320;&#28304;&#27169;&#22411;&#30340;SWAG&#27969;&#31243;&#36229;&#36234;&#20102;GPT-3.5-Turbo&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated long-form story generation typically employs long-context large language models (LLMs) for one-shot creation, which can produce cohesive but not necessarily engaging content. We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach reduces story writing to a search problem through a two-model feedback loop: one LLM generates story content, and another auxiliary LLM is used to choose the next best "action" to steer the story's future direction. Our results show that SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by GPT-4 and through human evaluation, and our SWAG pipeline using only open-source models surpasses GPT-3.5-Turbo.
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#20174;&#20559;&#22909;&#27604;&#36739;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#26469;&#23545;&#30446;&#26631;&#32467;&#26524;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#31639;&#27861;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01920</link><description>&lt;p&gt;
&#23545;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#30340;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Preference Poisoning Attacks on Reward Model Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01920
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20174;&#20559;&#22909;&#27604;&#36739;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#26469;&#23545;&#30446;&#26631;&#32467;&#26524;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#31639;&#27861;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20004;&#20004;&#27604;&#36739;&#20013;&#23398;&#20064;&#25928;&#29992;&#25110;&#22870;&#21169;&#27169;&#22411;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#26041;&#27861;&#20174;&#26412;&#36136;&#19978;&#38656;&#35201;&#20174;&#20154;&#20204;&#37027;&#37324;&#25910;&#38598;&#20559;&#22909;&#20449;&#24687;&#65292;&#32780;&#21453;&#39304;&#36890;&#24120;&#26159;&#21311;&#21517;&#25552;&#20379;&#30340;&#12290;&#30001;&#20110;&#20559;&#22909;&#26159;&#20027;&#35266;&#30340;&#65292;&#27809;&#26377;&#21487;&#20197;&#27604;&#36739;&#30340;&#40644;&#37329;&#26631;&#20934;&#65307;&#28982;&#32780;&#65292;&#23545;&#20559;&#22909;&#23398;&#20064;&#30340;&#39640;&#24433;&#21709;&#31995;&#32479;&#30340;&#20381;&#36182;&#24615;&#20026;&#24694;&#24847;&#34892;&#20026;&#32773;&#20542;&#21521;&#20110;&#25197;&#26354;&#20197;&#36798;&#21040;&#20854;&#30446;&#30340;&#32780;&#37319;&#38598;&#30340;&#25968;&#25454;&#21019;&#36896;&#20102;&#24378;&#28872;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#31181;&#23041;&#32961;&#27169;&#22411;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#36825;&#31181;&#28431;&#27934;&#30340;&#24615;&#36136;&#21644;&#31243;&#24230;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#21487;&#20197;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#65292;&#20197;&#20419;&#36827;&#25110;&#36140;&#20302;&#30446;&#26631;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#29992;&#20110;&#36825;&#20123;&#25915;&#20987;&#30340;&#31639;&#27861;&#26041;&#27861;&#65306;&#22522;&#20110;&#21407;&#21017;&#30340;&#26799;&#24230;&#26694;&#26550;&#21644;&#20960;&#31181;&#21464;&#31181;&#30340;&#25353;&#36317;&#31163;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31867;&#26368;&#20339;&#25915;&#20987;&#22312;&#25104;&#21151;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning utility, or reward, models from pairwise comparisons is a fundamental component in a number of application domains. These approaches inherently entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability systematically by considering a threat model in which an attacker can flip a small subset of preference comparisons with the goal of either promoting or demoting a target outcome. First, we propose two classes of algorithmic approaches for these attacks: a principled gradient-based framework, and several variants of rank-by-distance methods. Next, we demonstrate the efficacy of best attacks in both these classes in successfully achieving malicio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.00795</link><description>&lt;p&gt;
LLMs&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#25511;&#21046;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;-shot&#20219;&#21153;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#29702;&#35299;&#20854;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#21463;&#29289;&#29702;&#21407;&#29702;&#25511;&#21046;&#30340;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#35201;&#22312;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;LLaMA 2&#22312;&#27809;&#26377;&#24494;&#35843;&#25110;&#25552;&#31034;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#38543;&#30528;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;LLMs&#20013;&#25552;&#21462;&#22810;&#20301;&#25968;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;LM-HT&#27169;&#22411;&#65292;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#22810;&#23618;&#27425;&#38408;&#20540;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#24615;&#33021;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.00411</link><description>&lt;p&gt;
LM-HT SNN: &#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#22810;&#23618;&#27425;&#38408;&#20540;&#27169;&#22411;&#22686;&#24378;SNN&#19982;ANN&#30340;&#24615;&#33021;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;LM-HT&#27169;&#22411;&#65292;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#22810;&#23618;&#27425;&#38408;&#20540;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#24615;&#33021;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#27604;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22240;&#20854;&#26356;&#20855;&#29983;&#29289;&#21551;&#21457;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#20449;&#24687;&#20256;&#36882;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#23398;&#26415;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20043;&#21069;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#23545;SNN&#30340;&#23398;&#20064;&#26799;&#24230;&#21644;&#27169;&#22411;&#32467;&#26500;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20294;&#22312;&#24615;&#33021;&#26041;&#38754;SNN&#20173;&#28982;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33853;&#21518;&#20110;ANN&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22810;&#38408;&#20540;&#27169;&#22411;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;SNN&#30340;&#23398;&#20064;&#33021;&#21147;&#25552;&#20379;&#20102;&#26356;&#22810;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#23398;&#30340;&#35282;&#24230;&#20005;&#26684;&#20998;&#26512;&#20102;&#22810;&#38408;&#20540;&#27169;&#22411;&#12289;&#21407;&#22987;&#33033;&#20914;&#27169;&#22411;&#21644;&#37327;&#21270;ANN&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LM-HT&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#31561;&#36317;&#22810;&#23618;&#27425;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#21160;&#24577;&#35843;&#33410;&#20840;&#23616;&#36755;&#20837;&#30005;&#27969;&#21644;&#33180;&#30005;&#20301;&#27844;&#28431;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#22522;&#20110;LM-HT&#27169;&#22411;&#30340;&#30452;&#25509;&#35757;&#32451;&#31639;&#27861;&#21487;&#20197;&#26080;&#32541;&#22320;&#36830;&#25509;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more biological-inspired and energy-efficient manner. However, despite previous efforts to optimize the learning gradients and model structure of SNNs through various methods, SNNs still lag behind ANNs in terms of performance to some extent. The recently proposed multi-threshold model provides more possibilities for further enhancing the learning capability of SNNs. In this paper, we rigorously analyze the relationship among the multi-threshold model, vanilla spiking model and quantized ANNs from a mathematical perspective, then propose a novel LM-HT model, which is an equidistant multi-hierarchical model that can dynamically regulate the global input current and membrane potential leakage on the time dimension. In addition, we note that the direct training algorithm based on the LM-HT model can seamlessl
&lt;/p&gt;</description></item><item><title>ZS4C&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#36827;&#34892;&#38646;&#23556;&#20987;&#21512;&#25104;&#21487;&#32534;&#35793;&#20195;&#30721;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#24110;&#21161;&#29992;&#25143;&#37325;&#29992;&#25110;&#20998;&#26512;&#19981;&#23436;&#25972;&#30340;Q&amp;A&#20195;&#30721;&#29255;&#27573;&#65292;&#36890;&#36807;&#35782;&#21035;&#32570;&#22833;&#30340;&#23548;&#20837;&#35821;&#21477;&#24182;&#20462;&#22797;&#32534;&#35793;&#38169;&#35823;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14279</link><description>&lt;p&gt;
ZS4C: &#20351;&#29992;ChatGPT&#36827;&#34892;&#38646;&#23556;&#20987;&#21512;&#25104;&#19981;&#23436;&#25972;&#20195;&#30721;&#29255;&#27573;&#30340;&#21487;&#32534;&#35793;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT. (arXiv:2401.14279v1 [cs.SE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14279
&lt;/p&gt;
&lt;p&gt;
ZS4C&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#36827;&#34892;&#38646;&#23556;&#20987;&#21512;&#25104;&#21487;&#32534;&#35793;&#20195;&#30721;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#24110;&#21161;&#29992;&#25143;&#37325;&#29992;&#25110;&#20998;&#26512;&#19981;&#23436;&#25972;&#30340;Q&amp;A&#20195;&#30721;&#29255;&#27573;&#65292;&#36890;&#36807;&#35782;&#21035;&#32570;&#22833;&#30340;&#23548;&#20837;&#35821;&#21477;&#24182;&#20462;&#22797;&#32534;&#35793;&#38169;&#35823;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#38382;&#31572;&#65288;Q&amp;A&#65289;&#32593;&#31449;&#22914;Stack Overflow&#24050;&#25104;&#20026;&#36719;&#20214;&#24320;&#21457;&#32773;&#23547;&#27714;&#30693;&#35782;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;Q&amp;A&#32593;&#31449;&#19978;&#30340;&#20195;&#30721;&#29255;&#27573;&#36890;&#24120;&#30001;&#20110;&#26410;&#35299;&#26512;&#30340;&#31867;&#22411;&#21644;&#32570;&#22833;&#30340;&#20381;&#36182;&#24211;&#32780;&#26080;&#27861;&#32534;&#35793;&#21644;&#35821;&#20041;&#19978;&#19981;&#23436;&#25972;&#65292;&#36825;&#22686;&#21152;&#20102;&#29992;&#25143;&#37325;&#29992;&#25110;&#20998;&#26512;Q&amp;A&#20195;&#30721;&#29255;&#27573;&#30340;&#38556;&#30861;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#19981;&#36866;&#29992;&#20110;&#21512;&#25104;&#21487;&#32534;&#35793;&#20195;&#30721;&#65292;&#35201;&#20040;&#32534;&#35793;&#25104;&#21151;&#29575;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ZS4C&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#19981;&#23436;&#25972;&#30340;&#20195;&#30721;&#29255;&#27573;&#20013;&#36827;&#34892;&#38646;&#23556;&#20987;&#21512;&#25104;&#21487;&#32534;&#35793;&#20195;&#30721;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#12290;ZS4C&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;ZS4C&#21033;&#29992;&#19968;&#20010;LLM&#65292;&#21363;ChatGPT&#65292;&#26681;&#25454;&#25105;&#20204;&#35774;&#35745;&#30340;&#19987;&#29992;&#20219;&#21153;&#25552;&#31034;&#27169;&#26495;&#65292;&#20026;&#32473;&#23450;&#30340;&#20195;&#30721;&#29255;&#27573;&#35782;&#21035;&#32570;&#22833;&#30340;&#23548;&#20837;&#35821;&#21477;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;ZS4C&#36890;&#36807;&#20462;&#22797;&#30001;&#20110;&#19981;&#27491;&#30830;&#30340;&#23548;&#20837;&#35821;&#21477;&#21644;&#35821;&#27861;&#38169;&#35823;&#24341;&#36215;&#30340;&#32534;&#35793;&#38169;&#35823;&#26469;&#20462;&#22797;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Technical question and answering (Q&amp;A) sites such as Stack Overflow have become an important source for software developers to seek knowledge. However, code snippets on Q&amp;A sites are usually uncompilable and semantically incomplete for compilation due to unresolved types and missing dependent libraries, which raises the obstacle for users to reuse or analyze Q&amp;A code snippets. Prior approaches either are not designed for synthesizing compilable code or suffer from a low compilation success rate. To address this problem, we propose ZS4C, a lightweight approach to perform zero-shot synthesis of compilable code from incomplete code snippets using Large Language Model (LLM). ZS4C operates in two stages. In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify missing import statements for a given code snippet, leveraging our designed task-specific prompt template. In the second stage, ZS4C fixes compilation errors caused by incorrect import statements and syntax errors through 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.18304</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;&#31283;&#24577;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;&#24615;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Stability Principle for Learning under Non-Stationarity. (arXiv:2310.18304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#26469;&#36873;&#25321;&#19968;&#20010;&#22238;&#28335;&#31383;&#21475;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#23558;&#32047;&#31215;&#20559;&#24046;&#20445;&#25345;&#22312;&#19982;&#38543;&#26426;&#35823;&#24046;&#30456;&#23545;&#21487;&#25509;&#21463;&#30340;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#26410;&#30693;&#38750;&#31283;&#23450;&#24615;&#30340;&#36866;&#24212;&#24615;&#12290;&#24403;&#20154;&#21475;&#25439;&#22833;&#20989;&#25968;&#24378;&#20984;&#25110;&#20165;&#28385;&#36275;Lipschitz&#26465;&#20214;&#26102;&#65292;&#36951;&#25022;&#30028;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#65292;&#20165;&#21463;&#23545;&#25968;&#22240;&#23376;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26680;&#24515;&#26159;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#20989;&#25968;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#23558;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#20026;&#20934;&#31283;&#24577;&#29255;&#27573;&#30340;&#20998;&#21106;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptability of this approach to unknown non-stationarity. The regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.
&lt;/p&gt;</description></item><item><title>netFound&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#35757;&#32451;&#25429;&#25417;&#32593;&#32476;&#27969;&#37327;&#30340;&#23618;&#27425;&#21270;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#36136;&#37327;&#20302;&#12289;&#26377;&#38480;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2310.17025</link><description>&lt;p&gt;
netFound: &#32593;&#32476;&#23433;&#20840;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
netFound: Foundation Model for Network Security. (arXiv:2310.17025v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17025
&lt;/p&gt;
&lt;p&gt;
netFound&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#35757;&#32451;&#25429;&#25417;&#32593;&#32476;&#27969;&#37327;&#30340;&#23618;&#27425;&#21270;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#36136;&#37327;&#20302;&#12289;&#26377;&#38480;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20256;&#32479;&#24037;&#20316;&#27969;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#65292;&#20294;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#21644;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#38459;&#30861;&#20102;&#29305;&#24449;&#36873;&#25321;&#65292;&#23548;&#33268;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#20851;&#38190;&#20851;&#31995;&#21644;&#26377;&#25928;&#27867;&#21270;&#12290;&#21463;&#21040;GPT-4&#21644;Vision Transformers&#31561;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;netFound&#65292;&#19968;&#20010;&#32593;&#32476;&#23433;&#20840;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;&#23545;&#29616;&#26377;&#30340;&#26410;&#26631;&#35760;&#32593;&#32476;&#25968;&#25454;&#21253;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;netFound&#30340;&#35774;&#35745;&#34701;&#21512;&#20102;&#32593;&#32476;&#27969;&#37327;&#30340;&#23618;&#27425;&#21270;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#26377;&#25928;&#25429;&#25417;&#20102;&#38544;&#34255;&#30340;&#32593;&#32476;&#19978;&#19979;&#25991;&#65292;&#21253;&#25324;&#24212;&#29992;&#36923;&#36753;&#12289;&#36890;&#20449;&#21327;&#35758;&#21644;&#32593;&#32476;&#26465;&#20214;&#12290;&#26377;&#20102;&#36825;&#20010;&#39044;&#35757;&#32451;&#22522;&#30784;&#65292;&#21363;&#20351;&#22788;&#29702;&#36136;&#37327;&#20302;&#12289;&#26377;&#38480;&#21644;&#22024;&#26434;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#23545;netFound&#36827;&#34892;&#24494;&#35843;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;netFound&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ML for network security, traditional workflows rely on high-quality labeled data and manual feature engineering, but limited datasets and human expertise hinder feature selection, leading to models struggling to capture crucial relationships and generalize effectively. Inspired by recent advancements in ML application domains like GPT-4 and Vision Transformers, we have developed netFound, a foundational model for network security. This model undergoes pre-training using self-supervised algorithms applied to readily available unlabeled network packet traces. netFound's design incorporates hierarchical and multi-modal attributes of network traffic, effectively capturing hidden networking contexts, including application logic, communication protocols, and network conditions.  With this pre-trained foundation in place, we can fine-tune netFound for a wide array of downstream tasks, even when dealing with low-quality, limited, and noisy labeled data. Our experiments demonstrate netFound'
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLDM&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687; Latent Diffusion Model&#65288;LDM&#65289;&#21644;&#35270;&#39057; LDM&#65292;&#22312;&#35270;&#39057;&#32534;&#36753;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#39057;&#32534;&#36753;&#12290;&#36825;&#19968;&#26041;&#27861;&#26082;&#20445;&#25345;&#20102;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#21448;&#21033;&#29992;&#20102;&#22270;&#20687; LDM &#30340;&#39640;&#20445;&#30495;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#19982;&#21487;&#26367;&#25442;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16400</link><description>&lt;p&gt;
&#34701;&#21512;&#28508;&#21464;&#25193;&#25955;&#27169;&#22411;&#30340;&#35270;&#39057;&#32534;&#36753;&#65306;&#22810;&#28304;&#28508;&#21464;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models. (arXiv:2310.16400v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLDM&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687; Latent Diffusion Model&#65288;LDM&#65289;&#21644;&#35270;&#39057; LDM&#65292;&#22312;&#35270;&#39057;&#32534;&#36753;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#39057;&#32534;&#36753;&#12290;&#36825;&#19968;&#26041;&#27861;&#26082;&#20445;&#25345;&#20102;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#21448;&#21033;&#29992;&#20102;&#22270;&#20687; LDM &#30340;&#39640;&#20445;&#30495;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#19982;&#21487;&#26367;&#25442;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#21464;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#20197;&#20854;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#21512;&#25104;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#35270;&#39057;&#32534;&#36753;&#26041;&#27861;&#23384;&#22312;&#30528;&#39044;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#25110;&#35270;&#39057;&#36880;&#24103;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FLDM&#65288;&#34701;&#21512;&#28508;&#21464;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35270;&#39057;LDM&#20013;&#24212;&#29992;&#29616;&#25104;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#26469;&#23454;&#29616;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#32534;&#36753;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FLDM&#22312;&#21435;&#22122;&#36807;&#31243;&#20013;&#34701;&#21512;&#20102;&#22270;&#20687;LDM&#21644;&#35270;&#39057;LDM&#30340;&#28508;&#21464;&#12290;&#36825;&#26679;&#65292;&#21487;&#20197;&#20445;&#25345;&#35270;&#39057;LDM&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#21033;&#29992;&#22270;&#20687;LDM&#30340;&#39640;&#20445;&#30495;&#24230;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#22270;&#20687;LDM&#21644;&#35270;&#39057;LDM&#37117;&#21487;&#20197;&#26367;&#25442;&#65292;&#25152;&#20197;FLDM&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#39640;&#32423;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#22914;InstructPix2Pix&#21644;ControlNet&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FLDM&#26159;&#31532;&#19968;&#31181;&#23558;&#29616;&#25104;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#24212;&#29992;&#20110;&#35270;&#39057;LDM&#36827;&#34892;&#35270;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#12290;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Models (LDMs) are renowned for their powerful capabilities in image and video synthesis. Yet, video editing methods suffer from insufficient pre-training data or video-by-video re-training cost. In addressing this gap, we propose FLDM (Fused Latent Diffusion Model), a training-free framework to achieve text-guided video editing by applying off-the-shelf image editing methods in video LDMs. Specifically, FLDM fuses latents from an image LDM and an video LDM during the denoising process. In this way, temporal consistency can be kept with video LDM while high-fidelity from the image LDM can also be exploited. Meanwhile, FLDM possesses high flexibility since both image LDM and video LDM can be replaced so advanced image editing methods such as InstructPix2Pix and ControlNet can be exploited. To the best of our knowledge, FLDM is the first method to adapt off-the-shelf image editing methods into video LDMs for video editing. Extensive quantitative and qualitative experiment
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#24182;&#24378;&#35843;&#21487;&#39044;&#27979;&#24615;&#23545;&#20110;&#25552;&#39640;&#20449;&#20219;&#12289;&#36131;&#20219;&#12289;&#25511;&#21046;&#12289;&#21327;&#35843;&#21644;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06167</link><description>&lt;p&gt;
&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predictable Artificial Intelligence. (arXiv:2310.06167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06167
&lt;/p&gt;
&lt;p&gt;
&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#24182;&#24378;&#35843;&#21487;&#39044;&#27979;&#24615;&#23545;&#20110;&#25552;&#39640;&#20449;&#20219;&#12289;&#36131;&#20219;&#12289;&#25511;&#21046;&#12289;&#21327;&#35843;&#21644;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#26412;&#24605;&#24819;&#21644;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#25506;&#32034;&#22914;&#20309;&#39044;&#27979;&#29616;&#26377;&#21644;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#20851;&#38190;&#25351;&#26631;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23454;&#29616;&#21487;&#39044;&#27979;&#24615;&#23545;&#20110;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#20449;&#20219;&#12289;&#36131;&#20219;&#12289;&#25511;&#21046;&#12289;&#21327;&#35843;&#21644;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#24212;&#20248;&#20808;&#32771;&#34385;&#32780;&#38750;&#24615;&#33021;&#12290;&#23613;&#31649;&#19982;&#20854;&#20182;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#19982;&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#38382;&#39064;&#12289;&#20551;&#35774;&#21644;&#25361;&#25112;&#23578;&#26410;&#34987;&#28165;&#26970;&#25551;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#36825;&#20123;&#38382;&#39064;&#65292;&#21628;&#21505;&#25214;&#21040;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#21487;&#39044;&#27979;&#24615;&#30340;&#36335;&#24452;&#65292;&#24182;&#27010;&#36848;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the fundamental ideas and challenges of Predictable AI, a nascent research area that explores the ways in which we can anticipate key indicators of present and future AI ecosystems. We argue that achieving predictability is crucial for fostering trust, liability, control, alignment and safety of AI ecosystems, and thus should be prioritised over performance. While distinctive from other areas of technical and non-technical AI research, the questions, hypotheses and challenges relevant to Predictable AI were yet to be clearly described. This paper aims to elucidate them, calls for identifying paths towards AI predictability and outlines the potential impact of this emergent field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#28040;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04055</link><description>&lt;p&gt;
&#25226;&#22351;&#20154;&#36386;&#20986;&#21435;&#65281;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning. (arXiv:2310.04055v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#28040;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25915;&#20987;&#65292;&#20182;&#20204;&#36890;&#36807;&#25552;&#20132;&#31713;&#25913;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#36798;&#21040;&#23545;&#25239;&#30446;&#26631;&#65292;&#27604;&#22914;&#38459;&#27490;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#25110;&#32773;&#23548;&#33268;&#20840;&#23616;&#27169;&#22411;&#23545;&#26576;&#20123;&#25968;&#25454;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20808;&#30693;&#36947;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#65292;&#25110;&#32773;&#20381;&#36182;&#37325;&#26032;&#21152;&#26435;&#25110;&#20462;&#25913;&#25552;&#20132;&#30340;&#26041;&#24335;&#12290;&#36825;&#26159;&#22240;&#20026;&#25915;&#20987;&#32773;&#36890;&#24120;&#19981;&#20250;&#22312;&#25915;&#20987;&#20043;&#21069;&#23459;&#24067;&#20182;&#20204;&#30340;&#24847;&#22270;&#65292;&#32780;&#37325;&#26032;&#21152;&#26435;&#21487;&#33021;&#20250;&#25913;&#21464;&#32858;&#21512;&#32467;&#26524;&#65292;&#21363;&#20351;&#27809;&#26377;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#23574;&#31471;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;i&#65289;&#20165;&#22312;&#21457;&#29983;&#25915;&#20987;&#26102;&#26816;&#27979;&#25915;&#20987;&#30340;&#21457;&#29983;&#24182;&#36827;&#34892;&#38450;&#24481;&#25805;&#20316;&#65307;ii&#65289;&#19968;&#26086;&#21457;&#29983;&#25915;&#20987;&#65292;&#36827;&#19968;&#27493;&#26816;&#27979;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#24182;&#23558;&#20854;&#28040;&#38500;&#65292;&#32780;&#19981;&#20250;&#23545;&#27491;&#24120;&#27169;&#22411;&#36896;&#25104;&#20260;&#23475;&#65307;iii&#65289;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) systems are vulnerable to malicious clients that submit poisoned local models to achieve their adversarial goals, such as preventing the convergence of the global model or inducing the global model to misclassify some data. Many existing defense mechanisms are impractical in real-world FL systems, as they require prior knowledge of the number of malicious clients or rely on re-weighting or modifying submissions. This is because adversaries typically do not announce their intentions before attacking, and re-weighting might change aggregation results even in the absence of attacks. To address these challenges in real FL systems, this paper introduces a cutting-edge anomaly detection approach with the following features: i) Detecting the occurrence of attacks and performing defense operations only when attacks happen; ii) Upon the occurrence of an attack, further detecting the malicious client models and eliminating them without harming the benign ones; iii) Ensuri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05516</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;LLMs&#37327;&#21270;&#20013;&#30340;&#26435;&#37325;&#33293;&#20837;
&lt;/p&gt;
&lt;p&gt;
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25191;&#34892;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#38543;&#30528;&#20301;&#25968;&#30340;&#20943;&#23569;&#65292;&#37327;&#21270;&#32593;&#26684;&#21464;&#24471;&#26356;&#21152;&#23485;&#27867;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19978;&#19979;&#33293;&#20837;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#32454;&#35843;&#19978;&#19979;&#33293;&#20837;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21046;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#31934;&#30830;&#19988;&#26377;&#38480;&#30340;&#36793;&#30028;&#65292;&#21482;&#26377;&#25913;&#21464;&#33293;&#20837;&#20540;&#30340;&#38408;&#20540;&#25165;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#39640;&#25928;&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;SignRound&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#30340;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;HC3 Plus&#65292;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#35821;&#26009;&#24211;&#32771;&#34385;&#20102;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#20013;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#21644;Tk-instruct&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.02731</link><description>&lt;p&gt;
HC3 Plus&#65306;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus. (arXiv:2309.02731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HC3 Plus&#65292;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#35821;&#26009;&#24211;&#32771;&#34385;&#20102;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#20013;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#21644;Tk-instruct&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22240;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#28508;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#26816;&#27979;&#36234;&#26469;&#36234;&#20851;&#27880;&#65292;&#36825;&#23545;&#26410;&#32463;&#35757;&#32451;&#30340;&#20154;&#31867;&#26469;&#35828;&#24448;&#24448;&#24456;&#38590;&#35782;&#21035;&#12290;&#30446;&#21069;&#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#38382;&#31572;&#26041;&#38754;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#20855;&#26377;&#35821;&#20041;&#19981;&#21464;&#24615;&#30340;&#20219;&#21153;&#65292;&#22914;&#25688;&#35201;&#12289;&#32763;&#35793;&#21644;&#25913;&#20889;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#19978;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#12289;&#26356;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#32463;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#20197;&#21069;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25351;&#23548;&#24494;&#35843;&#20102;Tk-instruct&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has gained significant interest due to its impressive performance, but people are increasingly concerned about its potential risks, particularly around the detection of AI-generated content (AIGC), which is often difficult for untrained humans to identify. Current datasets utilized for detecting ChatGPT-generated text primarily center around question-answering, yet they tend to disregard tasks that possess semantic-invariant properties, such as summarization, translation, and paraphrasing. Our primary studies demonstrate that detecting model-generated text on semantic-invariant tasks is more difficult. To fill this gap, we introduce a more extensive and comprehensive dataset that considers more types of tasks than previous work, including semantic-invariant tasks. In addition, the model after a large number of task instruction fine-tuning shows a strong powerful performance. Owing to its previous success, we further instruct fine-tuning Tk-instruct and built a more powerful det
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02293</link><description>&lt;p&gt;
&#29992;&#27491;&#21017;&#21270;&#39640;&#38454;&#24635;&#21464;&#24046;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation. (arXiv:2308.02293v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02293
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#39640;&#24230;&#34920;&#36798;&#30340;&#21442;&#25968;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#22797;&#26434;&#27010;&#24565;&#65292;&#20294;&#35757;&#32451;&#36825;&#31181;&#39640;&#24230;&#38750;&#32447;&#24615;&#27169;&#22411;&#24050;&#30693;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;k&#38454;&#24635;&#21464;&#24046;&#65288;k-TV&#65289;&#27491;&#21017;&#21270;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;&#35201;&#35757;&#32451;&#30340;&#21442;&#25968;&#27169;&#22411;&#30340;k&#38454;&#23548;&#25968;&#30340;&#24179;&#26041;&#31215;&#20998;&#65292;&#36890;&#36807;&#24809;&#32602;k-TV&#26469;&#20135;&#29983;&#19968;&#20010;&#26356;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#23613;&#31649;&#23558;k-TV&#39033;&#24212;&#29992;&#20110;&#19968;&#33324;&#30340;&#21442;&#25968;&#27169;&#22411;&#30001;&#20110;&#31215;&#20998;&#32780;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#24102;&#26377;k-TV&#27491;&#21017;&#21270;&#30340;&#19968;&#33324;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26174;&#24335;&#30340;&#25968;&#20540;&#31215;&#20998;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#32467;&#26500;&#20219;&#24847;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#21363;&#21487;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $k$th order total variation ($k$-TV) regularization, which is defined as the squared integral of the $k$th order derivative of the parametric models to be trained; penalizing the $k$-TV is expected to yield a smoother function, which is expected to avoid overfitting. While the $k$-TV terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $k$-TV regularization without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradien
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#35745;&#31639;&#20195;&#29702;&#20851;&#38190;&#24615;&#25351;&#26631;&#26469;&#29983;&#25104;&#23433;&#20840;&#36793;&#30028;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#21487;&#33021;&#30340;&#38169;&#35823;&#34892;&#20026;&#30340;&#21518;&#26524;&#19982;&#25972;&#20307;&#24615;&#33021;&#30340;&#39044;&#26399;&#25439;&#22833;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20195;&#29702;&#25509;&#36817;&#22833;&#36133;&#29366;&#24577;&#65292;&#23433;&#20840;&#36793;&#30028;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.13642</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#23433;&#20840;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Safety Margins for Reinforcement Learning. (arXiv:2307.13642v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#35745;&#31639;&#20195;&#29702;&#20851;&#38190;&#24615;&#25351;&#26631;&#26469;&#29983;&#25104;&#23433;&#20840;&#36793;&#30028;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#21487;&#33021;&#30340;&#38169;&#35823;&#34892;&#20026;&#30340;&#21518;&#26524;&#19982;&#25972;&#20307;&#24615;&#33021;&#30340;&#39044;&#26399;&#25439;&#22833;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20195;&#29702;&#25509;&#36817;&#22833;&#36133;&#29366;&#24577;&#65292;&#23433;&#20840;&#36793;&#30028;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#33258;&#20027;&#25511;&#21046;&#22120;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#37117;&#21487;&#33021;&#19981;&#23433;&#20840;&#12290;&#33021;&#22815;&#23450;&#37327;&#22320;&#30830;&#23450;&#20309;&#26102;&#20250;&#21457;&#29983;&#36825;&#20123;&#19981;&#23433;&#20840;&#24773;&#20917;&#23545;&#20110;&#21450;&#26102;&#24341;&#20837;&#20154;&#31867;&#30417;&#30563;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#36135;&#36816;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20195;&#29702;&#30340;&#24773;&#20917;&#30340;&#30495;&#27491;&#20851;&#38190;&#24615;&#21487;&#20197;&#34987;&#31283;&#20581;&#22320;&#23450;&#20041;&#20026;&#22312;&#19968;&#20123;&#38543;&#26426;&#21160;&#20316;&#19979;&#22870;&#21169;&#30340;&#24179;&#22343;&#20943;&#23569;&#12290;&#21487;&#20197;&#23558;&#23454;&#26102;&#21487;&#35745;&#31639;&#30340;&#20195;&#29702;&#20851;&#38190;&#24615;&#25351;&#26631;&#65288;&#21363;&#65292;&#26080;&#38656;&#23454;&#38469;&#27169;&#25311;&#38543;&#26426;&#21160;&#20316;&#30340;&#24433;&#21709;&#65289;&#19982;&#30495;&#27491;&#30340;&#20851;&#38190;&#24615;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20195;&#29702;&#25351;&#26631;&#29983;&#25104;&#23433;&#20840;&#36793;&#30028;&#65292;&#23558;&#28508;&#22312;&#38169;&#35823;&#34892;&#20026;&#30340;&#21518;&#26524;&#30452;&#25509;&#19982;&#25972;&#20307;&#24615;&#33021;&#30340;&#39044;&#26399;&#25439;&#22833;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;Atari&#29615;&#22659;&#20013;&#36890;&#36807;APE-X&#21644;A3C&#30340;&#23398;&#20064;&#31574;&#30053;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#38543;&#30528;&#20195;&#29702;&#25509;&#36817;&#22833;&#36133;&#29366;&#24577;&#65292;&#23433;&#20840;&#36793;&#30028;&#30340;&#20943;&#23567;&#12290;&#23558;&#23433;&#20840;&#36793;&#30028;&#25972;&#21512;&#21040;&#30417;&#25511;&#31243;&#24207;&#20013;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Any autonomous controller will be unsafe in some situations. The ability to quantitatively identify when these unsafe situations are about to occur is crucial for drawing timely human oversight in, e.g., freight transportation applications. In this work, we demonstrate that the true criticality of an agent's situation can be robustly defined as the mean reduction in reward given some number of random actions. Proxy criticality metrics that are computable in real-time (i.e., without actually simulating the effects of random actions) can be compared to the true criticality, and we show how to leverage these proxy metrics to generate safety margins, which directly tie the consequences of potentially incorrect actions to an anticipated loss in overall performance. We evaluate our approach on learned policies from APE-X and A3C within an Atari environment, and demonstrate how safety margins decrease as agents approach failure states. The integration of safety margins into programs for monit
&lt;/p&gt;</description></item><item><title>GOOSE&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#40517;&#30340;&#34892;&#20026;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#24037;&#31243;&#25361;&#25112;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10420</link><description>&lt;p&gt;
GOOSE&#31639;&#27861;: &#19968;&#20010;&#24378;&#22823;&#30340;&#20248;&#21270;&#24037;&#20855;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24037;&#31243;&#25361;&#25112;&#21450;&#26356;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
GOOSE Algorithm: A Powerful Optimization Tool for Real-World Engineering Challenges and Beyond. (arXiv:2307.10420v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10420
&lt;/p&gt;
&lt;p&gt;
GOOSE&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#40517;&#30340;&#34892;&#20026;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#24037;&#31243;&#25361;&#25112;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;GOOSE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#40517;&#22312;&#20241;&#24687;&#21644;&#35269;&#39135;&#26102;&#30340;&#34892;&#20026;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#40517;&#38752;&#30528;&#19968;&#21482;&#33151;&#20445;&#25345;&#24179;&#34913;&#65292;&#20197;&#23432;&#25252;&#21644;&#20445;&#25252;&#32676;&#20307;&#20013;&#30340;&#20854;&#20182;&#20010;&#20307;&#12290;GOOSE&#31639;&#27861;&#22312;19&#20010;&#30693;&#21517;&#30340;&#22522;&#20934;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#19982;&#36951;&#20256;&#31639;&#27861;(GA)&#12289;&#31890;&#23376;&#32676;&#20248;&#21270;(PSO)&#12289;&#34619;&#34579;&#31639;&#27861;(DA)&#21644;&#36866;&#24212;&#24615;&#20381;&#36182;&#20248;&#21270;&#22120;(FDO)&#30340;&#27604;&#36739;&#30740;&#31350;&#26469;&#39564;&#35777;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#22312;10&#20010;&#29616;&#20195;&#22522;&#20934;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#34619;&#34579;&#31639;&#27861;&#12289;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;(WOA)&#21644;&#40144;&#40060;&#32676;&#31639;&#27861;(SSA)&#31561;&#19977;&#20010;&#26368;&#36817;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;GOOSE&#31639;&#27861;&#36824;&#22312;5&#20010;&#32463;&#20856;&#22522;&#20934;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23558;&#25152;&#24471;&#32467;&#26524;&#19982;&#36866;&#24212;&#24615;&#20381;&#36182;&#20248;&#21270;&#22120;(FDO)&#12289;FOX&#20248;&#21270;&#22120;&#12289;&#34678;&#20248;&#21270;&#31639;&#27861;(BOA)&#12289;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#12289;&#20154;&#24037;&#34562;&#32676;&#31639;&#27861;&#21644;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#31561;&#20845;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes the GOOSE algorithm as a novel metaheuristic algorithm based on the goose's behavior during rest and foraging. The goose stands on one leg and keeps his balance to guard and protect other individuals in the flock. The GOOSE algorithm is benchmarked on 19 well-known benchmark test functions, and the results are verified by a comparative study with genetic algorithm (GA), particle swarm optimization (PSO), dragonfly algorithm (DA), and fitness dependent optimizer (FDO). In addition, the proposed algorithm is tested on 10 modern benchmark functions, and the gained results are compared with three recent algorithms, such as the dragonfly algorithm, whale optimization algorithm (WOA), and salp swarm algorithm (SSA). Moreover, the GOOSE algorithm is tested on 5 classical benchmark functions, and the obtained results are evaluated with six algorithms, such as fitness dependent optimizer (FDO), FOX optimizer, butterfly optimization algorithm (BOA), whale optimization algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39564;&#35777;&#20102;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#22312;&#20855;&#26377;&#19981;&#20934;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24773;&#20917;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;LAF&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;LAF&#22312;MHWSIA&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02709</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#39564;&#35777;&#23545;&#19981;&#20934;&#30830;&#22320;&#30495;&#23454;&#26631;&#31614;&#30340;&#35780;&#20272;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Validation of the Practicability of Logical Assessment Formula for Evaluations with Inaccurate Ground-Truth Labels. (arXiv:2307.02709v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39564;&#35777;&#20102;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#22312;&#20855;&#26377;&#19981;&#20934;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24773;&#20917;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;LAF&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;LAF&#22312;MHWSIA&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#65288;LAF&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#19981;&#20934;&#30830;&#22320;&#30495;&#23454;&#26631;&#31614;&#65288;IAGTLs&#65289;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#26032;&#29702;&#35770;&#65292;&#29992;&#20110;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;LAF&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23545;&#20110;&#20855;&#26377;IAGTLs&#30340;&#35780;&#20272;&#30340;&#23454;&#29992;&#24615;&#23578;&#26410;&#24471;&#21040;&#39564;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;LAF&#24212;&#29992;&#20110;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#65288;TSfBC&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#26174;&#31034;LAF&#22312;TSfBC&#20013;&#23545;&#20110;&#20855;&#26377;IAGTLs&#30340;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21453;&#26144;&#20102;&#23558;LAF&#24212;&#29992;&#20110;MHWSIA&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical assessment formula (LAF) is a new theory proposed for evaluations with inaccurate ground-truth labels (IAGTLs) to assess the predictive models for various artificial intelligence applications. However, the practicability of LAF for evaluations with IAGTLs has not yet been validated in real-world practice. In this paper, to address this issue, we applied LAF to tumour segmentation for breast cancer (TSfBC) in medical histopathology whole slide image analysis (MHWSIA). Experimental results and analysis show the validity of LAF for evaluations with IAGTLs in the case of TSfBC and reflect the potentials of LAF applied to MHWSIA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500; GPTrans&#65292;&#20197;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#20026;&#22522;&#30784;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11424</link><description>&lt;p&gt;
&#22270;&#20256;&#25773;&#21464;&#25442;&#22120;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Propagation Transformer for Graph Representation Learning. (arXiv:2305.11424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500; GPTrans&#65292;&#20197;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#20026;&#22522;&#30784;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#35265;&#35299;&#26159;&#22312;&#26500;&#24314;&#21464;&#25442;&#22120;&#22359;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#26102;&#65292;&#20805;&#20998;&#32771;&#34385;&#22270;&#20013;&#33410;&#28857;&#21644;&#36793;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#31216;&#20026;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#65288;GPA&#65289;&#65292;&#23427;&#23558;&#20449;&#24687;&#22312;&#33410;&#28857;&#21644;&#36793;&#20043;&#38388;&#20197;&#19977;&#31181;&#26041;&#24335;&#26126;&#30830;&#20256;&#36882;&#65292;&#21363;&#20174;&#33410;&#28857;&#21040;&#33410;&#28857;&#65292;&#20174;&#33410;&#28857;&#21040;&#36793;&#21644;&#20174;&#36793;&#21040;&#33410;&#28857;&#65292;&#36825;&#23545;&#20110;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#20256;&#25773;&#21464;&#25442;&#22120;&#65288;GPTrans&#65289;&#30340;&#26377;&#25928;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#36827;&#19968;&#27493;&#24110;&#21161;&#23398;&#20064;&#22270;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#22270;&#23398;&#20064;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;GPTrans&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26356;&#22909;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;&#20195;&#30721;&#23558;&#22312;https://github.com/czczup/GPTrans&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GPTrans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-of-the-art transformer-based graph models with better performance. The code will be released at https://github.com/czczup/GPTrans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23383;&#23402;&#29983;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#26500;&#24314;&#22810;&#20010;&#36890;&#29992;&#20223;&#30495;&#22120;&#65292;&#24378;&#21270;&#20102;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#30340;&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08060</link><description>&lt;p&gt;
&#20004;&#20010;&#20248;&#20110;&#19968;&#20010;&#65306;&#25968;&#23383;&#23402;&#29983;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Two is Better Than One: Digital Siblings to Improve Autonomous Driving Testing. (arXiv:2305.08060v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23383;&#23402;&#29983;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#26500;&#24314;&#22810;&#20010;&#36890;&#29992;&#20223;&#30495;&#22120;&#65292;&#24378;&#21270;&#20102;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#30340;&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#26159;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#23454;&#38469;&#20013;&#65292;&#24403;&#20225;&#19994;&#20381;&#36182;&#31532;&#19977;&#26041;&#36890;&#29992;&#20223;&#30495;&#22120;&#36827;&#34892;&#20869;&#37096;&#25110;&#22806;&#21253;&#27979;&#35797;&#26102;&#65292;&#27979;&#35797;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#21463;&#21040;&#23041;&#32961;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#30340;&#27010;&#24565;&#21152;&#24378;&#20102;&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;AV&#22312;&#22810;&#20010;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#26500;&#24314;&#30340;&#36890;&#29992;&#20223;&#30495;&#22120;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;&#27599;&#20010;&#21333;&#29420;&#30340;&#20223;&#30495;&#22120;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23558;&#27979;&#35797;&#36801;&#31227;&#33267;&#21508;&#20010;&#20223;&#30495;&#22120;&#20043;&#38388;&#65292;&#20197;&#34920;&#24449;&#25152;&#36827;&#34892;&#30340;&#34892;&#39542;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#35745;&#31639;&#32852;&#21512;&#39044;&#27979;&#22833;&#25928;&#27010;&#29575;&#65292;&#24182;&#20165;&#22312;&#23402;&#29983;&#20043;&#38388;&#36798;&#25104;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#25253;&#21578;&#25925;&#38556;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#24320;&#28304;&#20223;&#30495;&#22120;&#23454;&#29616;&#20102;&#35813;&#26694;&#26550;&#65292;&#24182;&#22312;&#25968;&#23383;&#23402;&#29983;&#30340;&#29289;&#29702;&#27604;&#20363;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-based testing represents an important step to ensure the reliability of autonomous driving software. In practice, when companies rely on third-party general-purpose simulators, either for in-house or outsourced testing, the generalizability of testing results to real autonomous vehicles is at stake.  In this paper, we strengthen simulation-based testing by introducing the notion of digital siblings, a novel framework in which the AV is tested on multiple general-purpose simulators, built with different technologies. First, test cases are automatically generated for each individual simulator. Then, tests are migrated between simulators, using feature maps to characterize of the exercised driving conditions. Finally, the joint predicted failure probability is computed and a failure is reported only in cases of agreement among the siblings.  We implemented our framework using two open-source simulators and we empirically compared it against a digital twin of a physical scaled a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#35813;&#31639;&#27861;&#23588;&#20854;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#31574;&#30053;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#20197;&#36798;&#21040;&#20219;&#21153;&#30340;&#26368;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12785</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;&#65306;&#25910;&#25947;&#24615;&#19982;&#20840;&#23616;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and Global Optimality. (arXiv:2303.12785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#35813;&#31639;&#27861;&#23588;&#20854;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#31574;&#30053;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#20197;&#36798;&#21040;&#20219;&#21153;&#30340;&#26368;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#20195;&#29702;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#38500;&#20102;&#32047;&#35745;&#22870;&#21169;&#22806;&#30340;&#29109;&#22870;&#21169;&#12290;MPG&#19982;&#26631;&#20934;PG&#30340;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#23427;&#35757;&#32451;&#19968;&#31995;&#21015;&#31574;&#30053;&#21516;&#26102;&#23398;&#20064;&#26377;&#38480;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#21333;&#19968;&#30340;&#26631;&#20934;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31574;&#30053;&#12290;&#23545;&#20110;softmax&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MPG&#30340;&#25910;&#25947;&#24615;&#21644;&#26497;&#38480;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#36890;&#36807;&#35777;&#26126;MPG&#30446;&#26631;&#30340;&#21807;&#19968;&#20020;&#30028;&#28857;&#26159;&#26368;&#20248;&#31574;&#30053;&#65307;&#21363;&#20351;&#22312;&#36830;&#32493;&#32039;&#33268;&#29366;&#24577;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#32467;&#26524;&#20173;&#28982;&#25104;&#31435;&#12290;MPG&#30452;&#35266;&#12289;&#29702;&#35770;&#19978;Sound&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26631;&#20934;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#26368;&#20248;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;MPG&#26694;&#26550;&#30340;&#26368;&#20248;&#31574;&#30053;&#36827;&#34892;&#20219;&#24847;&#31934;&#24230;&#30340;&#36924;&#36817;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#31574;&#30053;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;MPG&#38750;&#24120;&#36866;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel Policy Gradient (PG) algorithm, called Matryoshka Policy Gradient (MPG), is introduced and studied, in the context of max-entropy reinforcement learning, where an agent aims at maximising entropy bonuses additional to its cumulative rewards. MPG differs from standard PG in that it trains a sequence of policies to learn finite horizon tasks simultaneously, instead of a single policy for the single standard objective. For softmax policies, we prove convergence of MPG and global optimality of the limit by showing that the only critical point of the MPG objective is the optimal policy; these results hold true even in the case of continuous compact state space. MPG is intuitive, theoretically sound and we furthermore show that the optimal policy of the standard max-entropy objective can be approximated arbitrarily well by the optimal policy of the MPG framework. Finally, we justify that MPG is well suited when the policies are parametrized with neural networks and we provide an simp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07865</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25512;&#25991;/&#29992;&#25143;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22788;&#29702;&#25991;&#26412;&#22823;&#25968;&#25454;&#22320;&#29702;&#26631;&#35760;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#20272;&#35745;&#22352;&#26631;&#23545;&#65288;&#32463;&#24230;&#65292;&#32428;&#24230;&#65289;&#21644;&#20108;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#33539;&#22260;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#12290;&#24615;&#33021;&#25351;&#26631;&#34920;&#26126;&#65292;&#23545;&#20110;&#22312;&#25512;&#25991;&#20869;&#23481;&#21644;&#20803;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;30&#20844;&#37324;&#65292;&#32654;&#22269;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;15&#20844;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
&lt;/p&gt;</description></item><item><title>&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;&#34920;&#31034;&#37319;&#29992;&#20102;Mittag-Leffler&#20989;&#25968;&#65292;&#21487;&#20197;&#25554;&#20540;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#12289;&#20943;&#36731;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.11007</link><description>&lt;p&gt;
&#27969;&#34892;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unification of popular artificial neural network activation functions. (arXiv:2302.11007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11007
&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;&#34920;&#31034;&#37319;&#29992;&#20102;Mittag-Leffler&#20989;&#25968;&#65292;&#21487;&#20197;&#25554;&#20540;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#12289;&#20943;&#36731;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#27969;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#34920;&#31034;&#12290;&#37319;&#29992;&#20102;&#20998;&#25968;&#24494;&#31215;&#20998;&#30340;Mittag-Leffler&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#32039;&#20945;&#30340;&#21151;&#33021;&#24418;&#24335;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#28608;&#27963;&#20989;&#25968;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#24182;&#20943;&#36731;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#22914;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#12290;&#25152;&#25552;&#20986;&#30340;&#38376;&#25511;&#34920;&#31034;&#25193;&#23637;&#20102;&#22266;&#23450;&#24418;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#33539;&#22260;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#33258;&#36866;&#24212;&#23545;&#24212;&#29289;&#65292;&#20854;&#24418;&#29366;&#21487;&#20197;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#20989;&#25968;&#24418;&#24335;&#30340;&#23548;&#25968;&#20063;&#21487;&#20197;&#29992;Mittag-Leffler&#20989;&#25968;&#34920;&#31034;&#65292;&#22240;&#27492;&#23427;&#26159;&#26799;&#24230;&#19979;&#38477;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#21512;&#36866;&#20505;&#36873;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#32479;&#19968;&#30340;&#38376;&#25511;&#28608;&#27963;&#20989;&#25968;&#34920;&#31034;&#20026;&#21508;&#31181;&#20869;&#32622;&#23454;&#29616;&#30340;&#32463;&#27982;&#30340;&#21644;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified representation of the most popular neural network activation functions. Adopting Mittag-Leffler functions of fractional calculus, we propose a flexible and compact functional form that is able to interpolate between various activation functions and mitigate common problems in training neural networks such as vanishing and exploding gradients. The presented gated representation extends the scope of fixed-shape activation functions to their adaptive counterparts whose shape can be learnt from the training data. The derivatives of the proposed functional form can also be expressed in terms of Mittag-Leffler functions making it a suitable candidate for gradient-based backpropagation algorithms. By training multiple neural networks of different complexities on various datasets with different sizes, we demonstrate that adopting a unified gated representation of activation functions offers a promising and affordable alternative to individual built-in implementations of ac
&lt;/p&gt;</description></item></channel></rss>