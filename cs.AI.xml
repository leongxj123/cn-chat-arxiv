<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01343</link><description>&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01343
&lt;/p&gt;
&lt;p&gt;
CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#21644;&#36719;&#20214;&#24179;&#21488;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#20351;&#29992;&#20687;GPT-3.5&#12289;GPT-4&#12289;GLM-3&#21644;LLaMa-2&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23458;&#25143;&#26381;&#21153;&#30340;&#32842;&#22825;&#36741;&#21161;&#25110;&#25512;&#29702;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#23458;&#25143;&#26381;&#21153;&#27169;&#22411;&#22312;&#19982;&#23458;&#25143;&#37197;&#32622;&#25991;&#20214;&#30340;&#38598;&#25104;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#24182;&#19988;&#32570;&#20047;&#26377;&#25928;&#26381;&#21153;&#25152;&#38656;&#30340;&#25805;&#20316;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#65288;CHat with custOmer Profile in existing System&#65289;&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#65306;&#65288;1&#65289;&#39640;&#25928;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#20197;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#25110;&#25353;&#29031;&#29616;&#26377;&#25351;&#21335;&#19982;&#36825;&#20123;&#31995;&#32479;&#20132;&#20114;&#65307;&#65288;2&#65289;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#22312;&#31995;&#32479;&#20013;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#65307;&#65288;3&#65289;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01343v1 Announce Type: cross  Abstract: Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverag
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#35299;&#20915;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#31215;&#21387;&#38382;&#39064;&#20855;&#26377;&#25112;&#30053;&#24212;&#23545;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#22312;&#39564;&#35777;&#21644;&#22686;&#24378;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16424</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#20026;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#25351;&#23450;LCSH&#20027;&#39064;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#35299;&#20915;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#31215;&#21387;&#38382;&#39064;&#20855;&#26377;&#25112;&#30053;&#24212;&#23545;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#22312;&#39564;&#35777;&#21644;&#22686;&#24378;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#65288;LCSH&#65289;&#30340;&#28508;&#21147;&#12290;&#20316;&#32773;&#20351;&#29992;ChatGPT&#26681;&#25454;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#30340;&#26631;&#39064;&#21644;&#25688;&#35201;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#19968;&#20123;&#29983;&#25104;&#30340;&#20027;&#39064;&#26631;&#22836;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23384;&#22312;&#29305;&#23450;&#24615;&#21644;&#35814;&#23613;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#20316;&#20026;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#30340;&#25112;&#30053;&#24615;&#24212;&#23545;&#25514;&#26045;&#65292;&#21516;&#26102;&#20063;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#24555;&#36895;&#29983;&#25104;LCSH&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#26159;&#39564;&#35777;&#21644;&#22686;&#24378;LLMs&#29983;&#25104;&#30340;LCSH&#30340;&#26377;&#25928;&#24615;&#12289;&#35814;&#23613;&#24615;&#21644;&#29305;&#23450;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16424v1 Announce Type: new  Abstract: This study delves into the potential use of Large Language Models (LLMs) for generating Library of Congress Subject Headings (LCSH). The authors employed ChatGPT to generate subject headings for electronic theses and dissertations (ETDs) based on their titles and summaries. The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness. The study showcases that LLMs can serve as a strategic response to the backlog of items awaiting cataloging in academic libraries, while also offering a cost-effective approach for promptly generating LCSH. Nonetheless, human catalogers remain essential for verifying and enhancing the validity, exhaustiveness, and specificity of LCSH generated by LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#29031;&#26009;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#20154;&#24037;&#24515;&#26234;&#29702;&#35770;&#26469;&#29468;&#27979;&#20154;&#31867;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#21361;&#38505;&#24773;&#20917;&#24182;&#23454;&#26102;&#28040;&#38500;&#21361;&#38505;&#30340;&#31639;&#27861;&#65292;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39640;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.16291</link><description>&lt;p&gt;
&#22312;&#29031;&#26009;&#26426;&#22120;&#20154;&#20013;&#29468;&#27979;&#20154;&#31867;&#24847;&#22270;&#20197;&#36991;&#20813;&#21361;&#38505;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Guessing human intentions to avoid dangerous situations in caregiving robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#29031;&#26009;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#20154;&#24037;&#24515;&#26234;&#29702;&#35770;&#26469;&#29468;&#27979;&#20154;&#31867;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#21361;&#38505;&#24773;&#20917;&#24182;&#23454;&#26102;&#28040;&#38500;&#21361;&#38505;&#30340;&#31639;&#27861;&#65292;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39640;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#27714;&#26426;&#22120;&#20154;&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#65292;&#23427;&#20204;&#24517;&#39035;&#20934;&#30830;&#35299;&#37322;&#20154;&#31867;&#24847;&#22270;&#24182;&#39044;&#27979;&#28508;&#22312;&#32467;&#26524;&#12290;&#23545;&#20110;&#20026;&#20154;&#31867;&#25252;&#29702;&#35774;&#35745;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#32780;&#35328;&#23588;&#20026;&#37325;&#35201;&#65292;&#21487;&#33021;&#20250;&#38754;&#20020;&#20154;&#31867;&#30340;&#21361;&#38505;&#24773;&#20917;&#65292;&#27604;&#22914;&#26410;&#35265;&#38556;&#30861;&#29289;&#65292;&#24212;&#35813;&#20104;&#20197;&#36991;&#20813;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#24515;&#26234;&#29702;&#35770;&#65288;ATM&#65289;&#26041;&#27861;&#26469;&#25512;&#26029;&#21644;&#35299;&#37322;&#20154;&#31867;&#24847;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#20154;&#31867;&#39118;&#38505;&#24773;&#20917;&#30340;&#31639;&#27861;&#65292;&#36873;&#25321;&#23454;&#26102;&#28040;&#38500;&#21361;&#38505;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;ATM&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#8220;&#20687;&#25105;&#19968;&#26679;&#8221;&#30340;&#31574;&#30053;&#23558;&#24847;&#22270;&#21644;&#21160;&#20316;&#20998;&#37197;&#32473;&#20154;&#31867;&#12290;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#26426;&#22120;&#20154;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#21487;&#20197;&#39640;&#25104;&#21151;&#29575;&#22320;&#26816;&#27979;&#21644;&#34892;&#21160;&#12290;&#35813;&#31639;&#27861;&#24050;&#32463;&#20316;&#20026;&#29616;&#26377;&#26426;&#22120;&#20154;&#35748;&#30693;&#26550;&#26500;&#30340;&#19968;&#37096;&#20998;&#23454;&#26045;&#65292;&#24182;&#22312;&#27169;&#25311;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#36827;&#34892;&#20102;&#19977;&#20010;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16291v1 Announce Type: cross  Abstract: For robots to interact socially, they must interpret human intentions and anticipate their potential outcomes accurately. This is particularly important for social robots designed for human care, which may face potentially dangerous situations for people, such as unseen obstacles in their way, that should be avoided. This paper explores the Artificial Theory of Mind (ATM) approach to inferring and interpreting human intentions. We propose an algorithm that detects risky situations for humans, selecting a robot action that removes the danger in real time. We use the simulation-based approach to ATM and adopt the 'like-me' policy to assign intentions and actions to people. Using this strategy, the robot can detect and act with a high rate of success under time-constrained situations. The algorithm has been implemented as part of an existing robotics cognitive architecture and tested in simulation scenarios. Three experiments have been co
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#65292;&#22312;&#21508;&#31181;&#32422;&#26463;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.13244</link><description>&lt;p&gt;
&#20351;&#29992;&#24072;&#29983;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13244
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#65292;&#22312;&#21508;&#31181;&#32422;&#26463;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#27169;&#22411;&#21644;&#35745;&#31639;&#24037;&#20855;&#29992;&#20110;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#20998;&#26512;&#65292;&#20294;&#29983;&#25104;&#31526;&#21512;&#25152;&#26377;&#26399;&#26395;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#20998;&#23376;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#31867;&#20284;&#20110;&#23398;&#29983;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#26469;&#33258;&#21508;&#31181;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#65288;&#21363;&#8220;&#32769;&#24072;&#8221;&#65289;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35757;&#32451;TSMMG&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#36825;&#20123;&#8216;&#32769;&#24072;&#8217;&#20013;&#25552;&#21462;&#30340;&#20998;&#23376;&#30693;&#35782;&#26500;&#24314;&#20102;&#22823;&#37327;&#25991;&#26412;-&#20998;&#23376;&#23545;&#65292;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#21508;&#31181;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;TSMMG&#22312;&#29983;&#25104;&#31526;&#21512;&#22797;&#26434;&#12289;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20004;&#12289;&#19977;&#21644;&#22235;&#32422;&#26463;&#20219;&#21153;&#30340;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24179;&#22343;&#20998;&#23376;&#26377;&#25928;&#24615;&#36229;&#36807;99&#65285;&#65292;&#25104;&#21151;&#29575;&#20998;&#21035;&#20026;88.08&#65285;&#12289;65.27&#65285;&#21644;61.44&#65285;&#12290;&#35813;&#27169;&#22411;&#36824;ex
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13244v1 Announce Type: new  Abstract: While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge. Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'. To train TSMMG, we construct a large set of text-molecule pairs by extracting molecular knowledge from these 'teachers', enabling it to generate novel molecules that conform to the descriptions through various text prompts. We experimentally show that TSMMG remarkably performs in generating molecules meeting complex, natural language-described property requirements across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 88.08%, 65.27%, and 61.44%, respectively. The model also ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RASP&#65292;&#19968;&#20010;&#21487;&#22312;25&#31186;&#20869;&#33258;&#20027;&#26356;&#25442;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#26500;&#20256;&#24863;&#21644;&#20316;&#21160;&#24179;&#21488;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#24555;&#36895;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#20154;&#21161;&#29702;&#31995;&#32479;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.12853</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#29615;&#22659;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#37325;&#26500;&#20316;&#21160;&#21644;&#20256;&#24863;&#24179;&#21488;RASP
&lt;/p&gt;
&lt;p&gt;
RASP: A Drone-based Reconfigurable Actuation and Sensing Platform Towards Ambient Intelligent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RASP&#65292;&#19968;&#20010;&#21487;&#22312;25&#31186;&#20869;&#33258;&#20027;&#26356;&#25442;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#26500;&#20256;&#24863;&#21644;&#20316;&#21160;&#24179;&#21488;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#24555;&#36895;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#20154;&#21161;&#29702;&#31995;&#32479;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#28040;&#36153;&#32423;&#26080;&#20154;&#26426;&#19982;&#25105;&#20204;&#23478;&#20013;&#30340;&#21560;&#23576;&#26426;&#22120;&#20154;&#25110;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#20010;&#20154;&#26234;&#33021;&#25163;&#26426;&#19968;&#26679;&#26377;&#29992;&#65292;&#38656;&#35201;&#26080;&#20154;&#26426;&#33021;&#24863;&#30693;&#12289;&#39537;&#21160;&#21644;&#21709;&#24212;&#21487;&#33021;&#20986;&#29616;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#24895;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RASP&#65292;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#26500;&#30340;&#20256;&#24863;&#21644;&#20316;&#21160;&#24179;&#21488;&#65292;&#20801;&#35768;&#26080;&#20154;&#26426;&#22312;&#20165;25&#31186;&#20869;&#33258;&#20027;&#26356;&#25442;&#26426;&#36733;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#20351;&#21333;&#20010;&#26080;&#20154;&#26426;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#12290;RASP&#21253;&#25324;&#19968;&#20010;&#26426;&#26800;&#23618;&#65292;&#29992;&#20110;&#29289;&#29702;&#26356;&#25442;&#20256;&#24863;&#22120;&#27169;&#22359;&#65292;&#19968;&#20010;&#30005;&#27668;&#23618;&#65292;&#29992;&#20110;&#32500;&#25252;&#20256;&#24863;&#22120;/&#25191;&#34892;&#22120;&#30340;&#30005;&#28304;&#21644;&#36890;&#20449;&#32447;&#36335;&#65292;&#20197;&#21450;&#19968;&#20010;&#36719;&#20214;&#23618;&#65292;&#29992;&#20110;&#22312;&#26080;&#20154;&#26426;&#21644;&#25105;&#20204;&#24179;&#21488;&#19978;&#30340;&#20219;&#20309;&#20256;&#24863;&#22120;&#27169;&#22359;&#20043;&#38388;&#32500;&#25252;&#19968;&#20010;&#20844;&#20849;&#25509;&#21475;&#12290;&#21033;&#29992;&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;RASP&#30340;&#20010;&#20154;&#21161;&#29702;&#31995;&#32479;&#30340;&#26550;&#26500;&#12289;&#23454;&#29616;&#21644;&#29616;&#23454;&#19990;&#30028;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12853v1 Announce Type: cross  Abstract: Realizing consumer-grade drones that are as useful as robot vacuums throughout our homes or personal smartphones in our daily lives requires drones to sense, actuate, and respond to general scenarios that may arise. Towards this vision, we propose RASP, a modular and reconfigurable sensing and actuation platform that allows drones to autonomously swap onboard sensors and actuators in only 25 seconds, allowing a single drone to quickly adapt to a diverse range of tasks. RASP consists of a mechanical layer to physically swap sensor modules, an electrical layer to maintain power and communication lines to the sensor/actuator, and a software layer to maintain a common interface between the drone and any sensor module in our platform. Leveraging recent advances in large language and visual language models, we further introduce the architecture, implementation, and real-world deployments of a personal assistant system utilizing RASP. We demo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33455;&#29255;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;Verilog&#25968;&#25454;&#21294;&#20047;&#21644;&#35757;&#32451;&#25968;&#25454;&#20934;&#22791;&#26102;&#38388;&#38271;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.11202</link><description>&lt;p&gt;
&#25968;&#25454;&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65306;&#36890;&#36807;&#33258;&#21160;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#23545;LLM&#36827;&#34892;&#33455;&#29255;&#35774;&#35745;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33455;&#29255;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;Verilog&#25968;&#25454;&#21294;&#20047;&#21644;&#35757;&#32451;&#25968;&#25454;&#20934;&#22791;&#26102;&#38388;&#38271;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#39640;&#23618;&#25552;&#31034;&#19979;&#33258;&#21160;&#29983;&#25104;&#30828;&#20214;&#25551;&#36848;&#35821;&#35328;&#65288;HDL&#65289;&#20195;&#30721;&#30340;&#28508;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#24494;&#35843;&#26469;&#22686;&#24378;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;Verilog&#25968;&#25454;&#38459;&#30861;&#20102;LLMs&#22312;Verilog&#29983;&#25104;&#36136;&#37327;&#19978;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#32570;&#23569;Verilog&#21644;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#33050;&#26412;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#26174;&#30528;&#22686;&#21152;&#20102;&#20026;LLM&#35757;&#32451;&#22120;&#20934;&#22791;&#35757;&#32451;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#23427;&#29983;&#25104;&#19982;Verilog&#21644;EDA&#33050;&#26412;&#23545;&#40784;&#30340;&#22823;&#37327;&#39640;&#36136;&#37327;&#33258;&#28982;&#35821;&#35328;&#12290;&#23545;&#20110;Verilog&#29983;&#25104;&#65292;&#23427;&#23558;Verilog&#25991;&#20214;&#36716;&#25442;&#20026;&#25277;&#35937;&#35821;&#27861;&#26641;&#65292;&#28982;&#21518;&#23558;&#33410;&#28857;&#26144;&#23556;&#21040;&#20855;&#26377;&#39044;&#23450;&#20041;&#27169;&#26495;&#30340;&#33258;&#28982;&#35821;&#35328;&#12290;&#23545;&#20110;Verilog&#20462;&#22797;&#65292;&#23427;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11202v1 Announce Type: cross  Abstract: Recent advances in large language models have demonstrated their potential for automated generation of hardware description language (HDL) code from high-level prompts. Researchers have utilized fine-tuning to enhance the ability of these large language models (LLMs) in the field of Chip Design. However, the lack of Verilog data hinders further improvement in the quality of Verilog generation by LLMs. Additionally, the absence of a Verilog and Electronic Design Automation (EDA) script data augmentation framework significantly increases the time required to prepare the training dataset for LLM trainers. This paper proposes an automated design-data augmentation framework, which generates high-volume and high-quality natural language aligned with Verilog and EDA scripts. For Verilog generation, it translates Verilog files to an abstract syntax tree and then maps nodes to natural language with a predefined template. For Verilog repair, it 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09974</link><description>&lt;p&gt;
GET&#65306;&#35299;&#38145;CLIP&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#65292;&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#21253;&#21547;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#26088;&#22312;&#20934;&#30830;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#24182;&#27491;&#30830;&#20998;&#31867;&#26087;&#31867;&#21035;&#65292;&#21033;&#29992;&#20174;&#26377;&#26631;&#31614;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#31867;&#21035;&#27010;&#24565;&#12290;&#24403;&#21069;&#30340;GCD&#26041;&#27861;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#20449;&#24687;&#27169;&#24577;&#65292;&#23548;&#33268;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#31867;&#21035;&#30340;&#20998;&#31867;&#25928;&#26524;&#19981;&#20339;&#12290;&#34429;&#28982;&#26576;&#20123;&#31867;&#21035;&#22312;&#35270;&#35273;&#19978;&#23481;&#26131;&#28151;&#28102;&#65292;&#20294;&#23427;&#20204;&#30340;&#25991;&#26412;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#21516;&#30340;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#25991;&#26412;&#20449;&#24687;&#24341;&#20837;&#21040;GCD&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#26080;&#26631;&#31614;&#25968;&#25454;&#32570;&#20047;&#31867;&#21035;&#21517;&#31216;&#65292;&#20351;&#24471;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;TES&#21033;&#29992;CLIP&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#36825;&#19968;&#29305;&#24615;&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#36716;&#25442;&#20026;CLIP&#25991;&#26412;&#27169;&#22411;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09974v1 Announce Type: cross  Abstract: Given unlabelled datasets containing both old and new categories, generalized category discovery (GCD) aims to accurately discover new classes while correctly classifying old classes, leveraging the class concepts learned from labeled samples. Current GCD methods only use a single visual modality of information, resulting in poor classification of visually similar classes. Though certain classes are visually confused, their text information might be distinct, motivating us to introduce text information into the GCD task. However, the lack of class names for unlabelled data makes it impractical to utilize text information. To tackle this challenging problem, in this paper, we propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings for unlabelled samples. Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Ensembling Prioritized Hybrid Policies (EPH)&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#21644;&#19977;&#31181;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07559</link><description>&lt;p&gt;
&#20026;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38598;&#25104;&#20248;&#20808;&#32423;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07559
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Ensembling Prioritized Hybrid Policies (EPH)&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#21644;&#19977;&#31181;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#36817;&#26469;&#22240;&#20854;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Ensembling Prioritized Hybrid Policies (EPH)&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22522;&#20110;&#36890;&#20449;&#30340;MARL-MAPF&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#65292;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#25910;&#38598;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#26234;&#33021;&#20307;&#21327;&#35843;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Q-learning&#30340;&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07559v1 Announce Type: cross  Abstract: Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Se
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Composition Score&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#21306;&#22495;&#30456;&#20851;&#65292;&#25581;&#31034;&#20102;&#21547;&#20041;&#21512;&#25104;&#22312;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04325</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20998;&#25968;&#27979;&#37327;&#20154;&#33041;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04325
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Composition Score&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#21306;&#22495;&#30456;&#20851;&#65292;&#25581;&#31034;&#20102;&#21547;&#20041;&#21512;&#25104;&#22312;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21547;&#20041;&#21512;&#25104;&#30340;&#36807;&#31243;&#26159;&#25351;&#26356;&#23567;&#30340;&#21333;&#20301;&#22914;&#35821;&#32032;&#25110;&#21333;&#35789;&#32452;&#21512;&#24418;&#25104;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#23545;&#20110;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#31070;&#32463;&#35821;&#35328;&#23398;&#23545;&#28041;&#21450;&#21547;&#20041;&#21512;&#25104;&#30340;&#22823;&#33041;&#21306;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20173;&#32570;&#20047;&#19968;&#31181;&#35745;&#31639;&#24230;&#37327;&#26469;&#37327;&#21270;&#21512;&#25104;&#30340;&#31243;&#24230;&#12290;&#20511;&#37492;&#21464;&#21387;&#22120;&#21069;&#39304;&#32593;&#32476;&#22359;&#30340;&#38190;&#20540;&#20869;&#23384;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32452;&#21512;&#20998;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#36807;&#31243;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#31751;&#30456;&#20851;&#32852;&#65292;&#36825;&#20123;&#22823;&#33041;&#31751;&#19982;&#35789;&#39057;&#29575;&#12289;&#32467;&#26500;&#22788;&#29702;&#21644;&#23545;&#21333;&#35789;&#30340;&#19968;&#33324;&#25935;&#24863;&#24615;&#26377;&#20851;&#65292;&#36825;&#34920;&#26126;&#20102;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#36807;&#31243;&#20013;&#21547;&#20041;&#21512;&#25104;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04325v1 Announce Type: cross  Abstract: The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#20174;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.02502</link><description>&lt;p&gt;
&#35797;&#38169;&#27861;&#65306;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02502
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#20174;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#20027;&#20195;&#29702;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;ETO&#12290;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#24320;&#25918;LLM&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#19982;&#20808;&#21069;&#19987;&#38376;&#35757;&#32451;&#25104;&#21151;&#19987;&#23478;&#36712;&#36857;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20195;&#29702;&#20174;&#20854;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#12290;&#36825;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;&#22312;&#25506;&#32034;&#38454;&#27573;&#65292;&#20195;&#29702;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#23436;&#25104;&#25351;&#23450;&#20219;&#21153;&#65292;&#25910;&#38598;&#22833;&#36133;&#36712;&#36857;&#20197;&#21019;&#24314;&#23545;&#27604;&#36712;&#36857;&#23545;&#12290;&#22312;&#38543;&#21518;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#20195;&#29702;&#21033;&#29992;&#36825;&#20123;&#36712;&#36857;&#20559;&#22909;&#23545;&#26356;&#26032;&#20854;&#31574;&#30053;&#65292;&#20351;&#29992;&#31867;&#20284;DPO&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#25506;&#32034;&#21644;&#35757;&#32451;&#30340;&#36845;&#20195;&#24490;&#29615;&#20419;&#36827;&#20102;&#20195;&#29702;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02502v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments o
&lt;/p&gt;</description></item><item><title>&#36755;&#20837;&#38271;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#38477;&#32423;&#36235;&#21183;&#20986;&#29616;&#22312;&#27604;&#25216;&#26415;&#26368;&#22823;&#20540;&#30701;&#24471;&#22810;&#30340;&#36755;&#20837;&#38271;&#24230;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.14848</link><description>&lt;p&gt;
&#20219;&#21153;&#30456;&#21516;&#65292;&#20196;&#29260;&#26356;&#22810;&#65306;&#36755;&#20837;&#38271;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14848
&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#38271;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#38477;&#32423;&#36235;&#21183;&#20986;&#29616;&#22312;&#27604;&#25216;&#26415;&#26368;&#22823;&#20540;&#30701;&#24471;&#22810;&#30340;&#36755;&#20837;&#38271;&#24230;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25193;&#23637;&#36755;&#20837;&#38271;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;LLMs&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#19981;&#21516;&#36755;&#20837;&#38271;&#24230;&#19979;&#30340;&#24615;&#33021;&#19968;&#33268;&#24615;&#23578;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#38382;&#31572;&#25512;&#29702;&#26694;&#26550;&#26469;&#30740;&#31350;&#27492;&#26041;&#38754;&#65292;&#35813;&#26694;&#26550;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#36755;&#20837;&#38271;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21516;&#19968;&#26679;&#26412;&#30340;&#22810;&#20010;&#29256;&#26412;&#65292;&#27599;&#20010;&#29256;&#26412;&#37117;&#36890;&#36807;&#19981;&#21516;&#38271;&#24230;&#12289;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#22635;&#20805;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20174;&#32780;&#20998;&#31163;&#20102;&#36755;&#20837;&#38271;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27604;&#23427;&#20204;&#30340;&#25216;&#26415;&#26368;&#22823;&#20540;&#30701;&#24471;&#22810;&#30340;&#36755;&#20837;&#38271;&#24230;&#19979;&#65292;LLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#26126;&#26174;&#38477;&#20302;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38477;&#32423;&#36235;&#21183;&#20986;&#29616;&#22312;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#27599;&#20010;&#29256;&#26412;&#20013;&#65292;&#23613;&#31649;&#24378;&#24230;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20256;&#32479;&#30340;&#22256;&#24785;&#24230;&#24230;&#37327;&#19982;LLMs&#22312;&#38271;&#36755;&#20837;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#27809;&#26377;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24182;&#35782;&#21035;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14848v1 Announce Type: cross  Abstract: This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that traditional perplexity metrics do not correlate with performance of LLMs' in long input reasoning tasks. We analyse our results and identif
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PEOs&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#38656;&#35201;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11354</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PEOs&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#38656;&#35201;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.11354v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;ANNS&#30340;&#20248;&#36234;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#21508;&#31181;&#22522;&#20110;&#22270;&#30340;ANNS&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#32570;&#20047;&#27491;&#24335;&#29702;&#35770;&#25903;&#25345;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;&#22270;&#30340;ANNS&#20013;&#30340;&#36335;&#30001;&#65292;&#35813;&#26041;&#27861;&#22312;&#25506;&#32034;&#22270;&#20013;&#33410;&#28857;&#30340;&#37051;&#23621;&#26102;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#27010;&#29575;&#36335;&#30001;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#25935;&#24863;&#25216;&#26415;&#24320;&#21457;&#20102;&#20004;&#31181;&#22522;&#20934;&#31574;&#30053;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PEOs&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#24212;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11354v1 Announce Type: cross  Abstract: Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a pivotal challenge in the field of machine learning. In recent years, graph-based methods have emerged as the superior approach to ANNS, establishing a new state of the art. Although various optimizations for graph-based ANNS have been introduced, they predominantly rely on heuristic methods that lack formal theoretical backing. This paper aims to enhance routing within graph-based ANNS by introducing a method that offers a probabilistic guarantee when exploring a node's neighbors in the graph. We formulate the problem as probabilistic routing and develop two baseline strategies by incorporating locality-sensitive techniques. Subsequently, we introduce PEOs, a novel approach that efficiently identifies which neighbors in the graph should be considered for exact distance computation, thus significantly improving efficiency in practice. Our experiments demonstrate 
&lt;/p&gt;</description></item><item><title>Jack of All Trades (JAT)&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26435;&#37325;&#38598;&#65292;&#23637;&#29616;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#22343;&#33021;&#21462;&#24471;&#24378;&#22823;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#23427;&#26159;&#39318;&#20010;&#23454;&#29616;&#35813;&#30446;&#26631;&#30340;&#24320;&#25918;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09844</link><description>&lt;p&gt;
&#35832;&#22810;&#25165;&#33402;&#65292;&#20854;&#20013;&#19968;&#20123;&#26159;&#22823;&#24072;&#65306;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#36716;&#25442;&#22120;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09844
&lt;/p&gt;
&lt;p&gt;
Jack of All Trades (JAT)&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26435;&#37325;&#38598;&#65292;&#23637;&#29616;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#22343;&#33021;&#21462;&#24471;&#24378;&#22823;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#23427;&#26159;&#39318;&#20010;&#23454;&#29616;&#35813;&#30446;&#26631;&#30340;&#24320;&#25918;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#23547;&#25214;&#19968;&#20010;&#33021;&#22815;&#22312;&#22810;&#20010;&#39046;&#22495;&#26080;&#32541;&#36816;&#20316;&#30340;&#36890;&#29992;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#20027;&#27969;&#30340;&#26041;&#27861;&#24448;&#24448;&#23558;&#27169;&#22411;&#38480;&#21046;&#22312;&#21333;&#19968;&#20219;&#21153;&#21644;&#21333;&#27169;&#24577;&#26694;&#26550;&#20013;&#65292;&#36825;&#19968;&#38480;&#21046;&#19982;&#36890;&#29992;&#30340;&#12289;&#22810;&#39046;&#22495;&#27169;&#22411;&#30340;&#24191;&#38420;&#24895;&#26223;&#30456;&#30683;&#30462;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Jack of All Trades (JAT) &#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20854;&#29420;&#29305;&#35774;&#35745;&#20248;&#21270;&#20102;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#31867;&#22411;&#30340;&#33021;&#21147;&#12290;JAT&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26435;&#37325;&#38598;&#65292;&#22312;&#38750;&#24120;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#29616;&#20102;&#20854;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;JAT&#27169;&#22411;&#26159;&#26397;&#30528;&#26356;&#36890;&#29992;&#12289;&#36328;&#39046;&#22495;&#30340;AI&#27169;&#22411;&#35774;&#35745;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#24182;&#19988;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#26159;&#39318;&#20010;&#23436;&#20840;&#24320;&#25918;&#30340;&#36825;&#19968;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09844v1 Announce Type: new  Abstract: The search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research. The prevailing methodology in Reinforcement Learning (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model. In this paper, we present Jack of All Trades (JAT), a transformer-based model with a unique design optimized for handling sequential decision-making tasks and multimodal data types. The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL benchmarks, along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights. The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;Large Language Model (LLM)&#20026;&#22522;&#30784;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#65292;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;LLM&#31649;&#36947;&#65292;&#21457;&#29616;GPT-2 + BART&#31649;&#36947;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#12290;&#36825;&#31181;&#30740;&#31350;&#26377;&#21161;&#20110;&#25913;&#21892;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#31038;&#20132;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00260</link><description>&lt;p&gt;
&#20197;LLM&#20026;&#22522;&#30784;&#23454;&#29616;&#38754;&#21521;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#30340;&#21487;&#25193;&#23637;&#26426;&#22120;&#20154;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;Large Language Model (LLM)&#20026;&#22522;&#30784;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#65292;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;LLM&#31649;&#36947;&#65292;&#21457;&#29616;GPT-2 + BART&#31649;&#36947;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#12290;&#36825;&#31181;&#30740;&#31350;&#26377;&#21161;&#20110;&#25913;&#21892;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#31038;&#20132;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;&#36825;&#31181;&#20132;&#27969;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Large Language Model (LLM)&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#31038;&#20132;&#26426;&#22120;&#20154;NAO&#25198;&#28436;&#20102;&#19968;&#20010;&#21050;&#28608;&#22120;(&#21475;&#22836;&#25551;&#36848;&#31038;&#20132;&#24773;&#26223;&#24182;&#25552;&#38382;)&#12289;&#25552;&#31034;&#22120;(&#25552;&#20379;&#19977;&#20010;&#36873;&#25321;&#39033;&#20379;&#36873;&#25321;)&#21644;&#22870;&#21169;&#22120;(&#24403;&#31572;&#26696;&#27491;&#30830;&#26102;&#32473;&#20104;&#31216;&#36190;)&#30340;&#35282;&#33394;&#12290;&#23545;&#20110;&#21050;&#28608;&#22120;&#30340;&#35282;&#33394;&#65292;&#31038;&#20132;&#24773;&#22659;&#12289;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#26159;&#20351;&#29992;&#25105;&#20204;&#30340;LLM&#31649;&#36947;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;GPT-2 + BART&#21644;GPT-2 + GPT-2&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;GPT-2&#22312;&#31649;&#36947;&#20013;&#26159;&#29992;&#20110;&#26080;&#30417;&#30563;&#31038;&#20132;&#24773;&#22659;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;SOCIALIQA&#25968;&#25454;&#38598;&#23545;&#25152;&#26377;LLM&#31649;&#36947;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-2 + BART&#31649;&#36947;&#22312;&#36890;&#36807;&#32467;&#21512;&#21508;&#33258;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;BERTscore&#12290;&#36825;&#31181;&#35266;&#23519;&#32467;&#26524;&#20063;&#19982;&#20799;&#31461;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27700;&#24179;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a social robot capable of verbally interacting with children with Autism Spectrum Disorder (ASD). This communication is meant to teach perspective-taking using text generated using a Large Language Model (LLM) pipeline. The social robot NAO acts as a stimulator (verbally describes a social situation and asks a question), prompter (presents three options to choose from), and reinforcer (praises when the answer is correct). For the role of the stimulator, the social situation, questions, and options are generated using our LLM pipeline. We compare two approaches: GPT-2 + BART and GPT-2 + GPT-2, where the first GPT-2 common between the pipelines is used for unsupervised social situation generation. We use the SOCIALIQA dataset to fine-tune all of our LLM pipelines. We found that the GPT-2 + BART pipeline had a better BERTscore for generating the questions and the options by combining their individual loss functions. This observation was also consistent with the h
&lt;/p&gt;</description></item><item><title>Agent Lumos&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35268;&#21010;&#27169;&#22359;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#65292;&#35757;&#32451;&#25509;&#22320;&#27169;&#22359;&#23558;&#20854;&#36716;&#21270;&#20026;&#21160;&#20316;&#65292;&#20419;&#36827;&#24191;&#27867;&#20114;&#21160;&#20219;&#21153;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.05657</link><description>&lt;p&gt;
Agent Lumos: &#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#35757;&#32451;&#24320;&#28304;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Agent Lumos: Unified and Modular Training for Open-Source Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05657
&lt;/p&gt;
&lt;p&gt;
Agent Lumos&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35268;&#21010;&#27169;&#22359;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#65292;&#35757;&#32451;&#25509;&#22320;&#27169;&#22359;&#23558;&#20854;&#36716;&#21270;&#20026;&#21160;&#20316;&#65292;&#20419;&#36827;&#24191;&#27867;&#20114;&#21160;&#20219;&#21153;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;&#20195;&#29702;&#23384;&#22312;&#35832;&#22810;&#38382;&#39064;&#65292;&#22914;&#32570;&#20047;&#36127;&#25285;&#24471;&#36215;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#30340;&#20114;&#21160;&#20219;&#21153;&#20013;&#12290;&#36825;&#20419;&#20351;&#20102;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102; LUMOS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20026;&#35757;&#32451;&#24320;&#28304; LLM-based &#20195;&#29702;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#20043;&#19968;&#12290;LUMOS&#20855;&#26377;&#21487;&#23398;&#20064;&#12289;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#30340;&#35268;&#21010;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#26377;&#32032;&#30340;&#25509;&#22320;&#27169;&#22359;&#65292;&#29992;&#20110;&#20351;&#29992;&#25191;&#34892;&#27169;&#22359;&#20013;&#30340;&#21508;&#31181;&#24037;&#20855;&#23558;&#36825;&#20123;&#36716;&#21270;&#20026;&#21160;&#20316;&#12290;&#36825;&#31181;&#35774;&#35745;&#20801;&#35768;&#27169;&#22359;&#21270;&#21319;&#32423;&#65292;&#24182;&#26356;&#24191;&#27867;&#22320;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20114;&#21160;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;&#36890;&#29992;&#20195;&#29702;&#23398;&#20064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#28304;&#33258;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#20219;&#21153;&#20013;&#19981;&#21516;&#22320;&#38754;&#30495;&#23454;&#25512;&#29702;&#21407;&#29702;&#30340;&#22823;&#35268;&#27169;&#12289;&#32479;&#19968;&#21644;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#27880;&#37322;&#12290;&#22312;9&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;LUMOS&#34920;&#29616;&#20986;&#20102;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#65288;1&#65289;LUMOS&#22312;&#22810;&#20010;&#36739;&#22823;&#30340;&#24320;&#28304;a
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05657v2 Announce Type: replace  Abstract: Closed-source agents suffer from several issues such as a lack of affordability, transparency, and reproducibility, particularly on complex interactive tasks. This motivates the development of open-source alternatives. We introduce LUMOS, one of the first frameworks for training open-source LLM-based agents. LUMOS features a learnable, unified, and modular architecture with a planning module that learns high-level subgoal generation, and a grounding module trained to translate these into actions using various tools in the execution module. The design allows for modular upgrades and wider applicability to diverse interactive tasks. To foster generalizable agent learning, we collect large-scale, unified, and high-quality training annotations derived from diverse ground-truth reasoning rationales across various complex interactive tasks. On 9 datasets, LUMOS exhibits several key advantages: (1) LUMOS excels multiple larger open-source a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24038;&#21491;&#21322;&#29699;&#19987;&#38376;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19981;&#21516;&#30340;&#36816;&#21160;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21327;&#35843;&#24615;&#12289;&#36816;&#21160;&#25928;&#29575;&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.14057</link><description>&lt;p&gt;
&#24038;/&#21491;&#33041;&#12289;&#20154;&#31867;&#36816;&#21160;&#25511;&#21046;&#21450;&#23545;&#26426;&#22120;&#20154;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Left/Right Brain, human motor control and the implications for robotics. (arXiv:2401.14057v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24038;&#21491;&#21322;&#29699;&#19987;&#38376;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19981;&#21516;&#30340;&#36816;&#21160;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21327;&#35843;&#24615;&#12289;&#36816;&#21160;&#25928;&#29575;&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36816;&#21160;&#25511;&#21046;&#22120;&#30456;&#23545;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#21508;&#31181;&#20248;&#28857;&#65292;&#28982;&#32780;&#30001;&#20110;&#20854;&#26080;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#31934;&#30830;&#36816;&#21160;&#65292;&#22240;&#27492;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#37319;&#29992;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#21452;&#20391;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20316;&#20026;&#36816;&#21160;&#20219;&#21153;&#30340;&#25511;&#21046;&#31995;&#32479;&#12290;&#25105;&#20204;&#26088;&#22312;&#23454;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#21322;&#29699;&#19987;&#38376;&#21270;&#65306;&#20248;&#21183;&#31995;&#32479;&#65288;&#36890;&#24120;&#26159;&#21491;&#25163;&#12289;&#24038;&#21322;&#29699;&#65289;&#25797;&#38271;&#21327;&#35843;&#21644;&#36816;&#21160;&#25928;&#29575;&#30340;&#20219;&#21153;&#65292;&#32780;&#38750;&#20248;&#21183;&#31995;&#32479;&#22312;&#38656;&#35201;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#21322;&#29699;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#19987;&#38376;&#21270;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20855;&#26377;&#19987;&#38376;&#21270;&#21322;&#29699;&#21644;&#26080;&#19987;&#38376;&#21270;&#21322;&#29699;&#12289;&#20855;&#26377;&#21322;&#29699;&#38388;&#36830;&#25509;&#65288;&#20195;&#34920;&#29983;&#29289;&#23398;&#33041;&#26725;&#65289;&#21644;&#26080;&#21322;&#29699;&#38388;&#36830;&#25509;&#12289;&#20855;&#26377;&#19987;&#38376;&#21270;&#21644;&#26080;&#19987;&#38376;&#21270;&#30340;&#21333;&#20391;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Network movement controllers promise a variety of advantages over conventional control methods however they are not widely adopted due to their inability to produce reliably precise movements. This research explores a bilateral neural network architecture as a control system for motor tasks. We aimed to achieve hemispheric specialisation similar to what is observed in humans across different tasks; the dominant system (usually the right hand, left hemisphere) excels at tasks involving coordination and efficiency of movement, and the non-dominant system performs better at tasks requiring positional stability. Specialisation was achieved by training the hemispheres with different loss functions tailored toward the expected behaviour of the respective hemispheres. We compared bilateral models with and without specialised hemispheres, with and without inter-hemispheric connectivity (representing the biological Corpus Callosum), and unilateral models with and without specialisation. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#25506;&#35752;&#20102;&#19982;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#30456;&#20851;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;&#20998;&#26512;&#20102;&#20116;&#31181;&#20027;&#35201;&#23041;&#32961;&#30340;&#20262;&#29702;&#21518;&#26524;&#65292;&#24182;&#24378;&#35843;&#20102;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#22312;&#20262;&#29702;&#35268;&#33539;&#33539;&#22260;&#20869;&#36816;&#20316;&#30340;&#32039;&#36843;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12273</link><description>&lt;p&gt;
&#20132;&#20114;&#30340;&#20262;&#29702;&#38382;&#39064;&#65306;&#32531;&#35299;LLMs&#20013;&#30340;&#23433;&#20840;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
The Ethics of Interaction: Mitigating Security Threats in LLMs. (arXiv:2401.12273v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#25506;&#35752;&#20102;&#19982;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#30456;&#20851;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;&#20998;&#26512;&#20102;&#20116;&#31181;&#20027;&#35201;&#23041;&#32961;&#30340;&#20262;&#29702;&#21518;&#26524;&#65292;&#24182;&#24378;&#35843;&#20102;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#22312;&#20262;&#29702;&#35268;&#33539;&#33539;&#22260;&#20869;&#36816;&#20316;&#30340;&#32039;&#36843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#19982;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#30456;&#20851;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;&#36825;&#20123;&#22797;&#26434;&#30340;&#25968;&#23383;&#23384;&#20648;&#24211;&#26085;&#30410;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#22240;&#27492;&#25104;&#20026;&#25915;&#20987;&#30340;&#20027;&#35201;&#30446;&#26631;&#65292;&#21487;&#33021;&#21361;&#21450;&#20854;&#35757;&#32451;&#25968;&#25454;&#21644;&#25968;&#25454;&#28304;&#30340;&#26426;&#23494;&#24615;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#36825;&#20123;&#23433;&#20840;&#23041;&#32961;&#23545;&#31038;&#20250;&#21644;&#20010;&#20154;&#38544;&#31169;&#30340;&#24494;&#22937;&#20262;&#29702;&#24433;&#21709;&#12290;&#25105;&#20204;&#23545;&#20116;&#20010;&#20027;&#35201;&#23041;&#32961;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65306;&#25552;&#31034;&#27880;&#20837;&#12289;&#36234;&#29425;&#12289;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#26333;&#38706;&#12289;&#24615;&#21035;&#26174;&#38706;&#20869;&#23481;&#21644;&#22522;&#20110;&#20167;&#24680;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#19981;&#20165;&#20165;&#36827;&#34892;&#20102;&#35782;&#21035;&#65292;&#36824;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#20262;&#29702;&#21518;&#26524;&#20197;&#21450;&#23545;&#24378;&#21270;&#38450;&#24481;&#31574;&#30053;&#30340;&#32039;&#36843;&#24615;&#12290;&#23545;LLMs&#30340;&#19981;&#26029;&#20381;&#36182;&#20984;&#26174;&#20102;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#22312;&#20262;&#29702;&#35268;&#33539;&#33539;&#22260;&#20869;&#36816;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#28389;&#29992;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#31038;&#20250;&#21644;&#20010;&#20154;&#20260;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#36825;&#20123;&#31995;&#32479;&#27010;&#24565;&#21270;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper comprehensively explores the ethical challenges arising from security threats to Language Learning Models (LLMs). These intricate digital repositories are increasingly integrated into our daily lives, making them prime targets for attacks that can compromise their training data and the confidentiality of their data sources. The paper delves into the nuanced ethical repercussions of such security threats on society and individual privacy. We scrutinize five major threats: prompt injection, jailbreaking, Personal Identifiable Information (PII) exposure, sexually explicit content, and hate based content, going beyond mere identification to assess their critical ethical consequences and the urgency they create for robust defensive strategies. The escalating reliance on LLMs underscores the crucial need for ensuring these systems operate within the bounds of ethical norms, particularly as their misuse can lead to significant societal and individual harm. We propose conceptualizin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#19981;&#24895;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#65292;&#23548;&#33268;&#39640;&#38169;&#35823;&#29575;&#12290;&#23454;&#39564;&#36824;&#34920;&#26126;&#29992;&#25143;&#26080;&#35770;&#26159;&#21542;&#26631;&#35760;&#20102;&#30830;&#23450;&#24615;&#37117;&#20250;&#20005;&#37325;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06730</link><description>&lt;p&gt;
&#19981;&#21487;&#38752;&#30340;&#20381;&#36182;&#65306;&#35821;&#35328;&#27169;&#22411;&#19981;&#24895;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty. (arXiv:2401.06730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#19981;&#24895;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#65292;&#23548;&#33268;&#39640;&#38169;&#35823;&#29575;&#12290;&#23454;&#39564;&#36824;&#34920;&#26126;&#29992;&#25143;&#26080;&#35770;&#26159;&#21542;&#26631;&#35760;&#20102;&#30830;&#23450;&#24615;&#37117;&#20250;&#20005;&#37325;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#30340;&#40664;&#35748;&#25509;&#21475;&#65292;&#35821;&#35328;&#27169;&#22411;&#36866;&#24403;&#22320;&#20256;&#36798;&#19979;&#28216;&#24212;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#23545;&#20854;&#22238;&#31572;&#30340;&#32622;&#20449;&#24230;&#65292;&#20197;&#21450;&#19979;&#28216;&#29992;&#25143;&#23545;&#35821;&#35328;&#27169;&#22411;&#34920;&#36798;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#21453;&#24212;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20844;&#24320;&#37096;&#32626;&#30340;&#27169;&#22411;&#65292;&#21457;&#29616;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#21363;&#20351;&#20135;&#29983;&#20102;&#38169;&#35823;&#31572;&#26696;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#26080;&#27861;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#12290;&#34429;&#28982;&#21487;&#20197;&#26126;&#30830;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#34920;&#36798;&#32622;&#20449;&#24230;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#65292;&#23548;&#33268;&#22312;&#32622;&#20449;&#30340;&#22238;&#31572;&#20013;&#38169;&#35823;&#29575;&#39640;&#36798;&#24179;&#22343;47%&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#31867;&#23454;&#39564;&#27979;&#35797;&#20102;&#35821;&#35328;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#30340;&#39118;&#38505;&#65292;&#24182;&#35777;&#26126;&#29992;&#25143;&#26080;&#35770;&#26159;&#21542;&#26631;&#35760;&#20102;&#30830;&#23450;&#24615;&#37117;&#20250;&#20005;&#37325;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;RLHF&#23545;&#40784;&#20013;&#20351;&#29992;&#30340;&#20559;&#22909;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#23545;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25991;&#26412;&#26377;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As natural language becomes the default interface for human-AI interaction, there is a critical need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence about their responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are unable to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (on average 47%) among confident responses. We test the risks of LM overconfidence by running human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in RLHF alignment and find that humans have a bias against texts with uncertainty. Our work hig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06122</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#24377;&#23556;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#32780;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#24615;&#36136;&#20173;&#28982;&#26410;&#30693;&#12290;&#35299;&#37322;DNNs&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#28608;&#27963;&#26368;&#22823;&#21270;(AM)&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#21512;&#25104;&#30340;&#36755;&#20837;&#20449;&#21495;&#65292;&#26368;&#22823;&#21270;&#28608;&#27963;&#32593;&#32476;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#32780;&#19981;&#25913;&#21464;&#27169;&#22411;&#32467;&#26500;&#25110;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20960;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#33021;&#21147;&#65292;&#22312;&#27169;&#22411;&#23457;&#26680;&#36807;&#31243;&#20013;&#20351;&#29992;&#36873;&#25321;&#30340;&#30446;&#26631;&#35299;&#37322;&#23631;&#34109;&#20102;&#21407;&#22987;&#35299;&#37322;&#12290;&#20316;&#20026;&#19968;&#31181;&#34917;&#25937;&#25514;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#27490;&#36825;&#31181;&#25805;&#32437;&#30340;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;HTML&#20869;&#23481;&#30340;&#39640;&#32423;&#26816;&#27979;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#34701;&#21512;&#26041;&#27861;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#36825;&#39033;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.04820</link><description>&lt;p&gt;
&#36890;&#36807;HTML&#20869;&#23481;&#30340;&#22810;&#27169;&#22411;&#20998;&#26512;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;
&lt;/p&gt;
&lt;p&gt;
Phishing Website Detection through Multi-Model Analysis of HTML Content. (arXiv:2401.04820v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;HTML&#20869;&#23481;&#30340;&#39640;&#32423;&#26816;&#27979;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#34701;&#21512;&#26041;&#27861;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#36825;&#39033;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#20852;&#36215;&#65292;&#25105;&#20204;&#30340;&#36890;&#20449;&#21644;&#24037;&#20316;&#26041;&#24335;&#21457;&#29983;&#20102;&#24040;&#22823;&#30340;&#21464;&#21270;&#12290;&#34429;&#28982;&#23427;&#20026;&#25105;&#20204;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#32593;&#32476;&#23041;&#32961;&#12290;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#19988;&#20005;&#37325;&#30340;&#23041;&#32961;&#26159;&#32593;&#32476;&#38035;&#40060;&#65292;&#40657;&#23458;&#20351;&#29992;&#27450;&#39575;&#24615;&#26041;&#27861;&#31363;&#21462;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;HTML&#20869;&#23481;&#30340;&#20808;&#36827;&#26816;&#27979;&#27169;&#22411;&#65292;&#38024;&#23545;&#32593;&#32476;&#38035;&#40060;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#29992;&#20110;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#19987;&#38376;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#27169;&#22411;&#21644;&#20004;&#20010;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#27169;&#22411;&#65292;&#20197;&#20998;&#26512;&#39029;&#38754;&#26631;&#39064;&#21644;&#20869;&#23481;&#31561;&#25991;&#26412;&#29305;&#24449;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#34701;&#21512;&#36807;&#31243;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#23884;&#20837;&#21521;&#37327;&#34987;&#21644;&#35856;&#22320;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#36755;&#20837;&#21040;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#12290;&#37492;&#20110;&#30446;&#21069;&#32570;&#20047;&#20840;&#38754;&#30340;&#32593;&#32476;&#38035;&#40060;&#30740;&#31350;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#36824;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The way we communicate and work has changed significantly with the rise of the Internet. While it has opened up new opportunities, it has also brought about an increase in cyber threats. One common and serious threat is phishing, where cybercriminals employ deceptive methods to steal sensitive information.This study addresses the pressing issue of phishing by introducing an advanced detection model that meticulously focuses on HTML content. Our proposed approach integrates a specialized Multi-Layer Perceptron (MLP) model for structured tabular data and two pretrained Natural Language Processing (NLP) models for analyzing textual features such as page titles and content. The embeddings from these models are harmoniously combined through a novel fusion process. The resulting fused embeddings are then input into a linear classifier. Recognizing the scarcity of recent datasets for comprehensive phishing research, our contribution extends to the creation of an up-to-date dataset, which we o
&lt;/p&gt;</description></item><item><title>SecureReg&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22495;&#21517;&#27880;&#20876;&#36807;&#31243;&#20013;&#20027;&#21160;&#26292;&#38706;&#24694;&#24847;&#22495;&#21517;&#27880;&#20876;&#65292;&#25552;&#20379;&#20102;&#26089;&#26399;&#23041;&#32961;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#28431;&#27934;&#31383;&#21475;&#65292;&#24182;&#20026;&#20027;&#21160;&#39044;&#38450;&#24615;&#25805;&#20316;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.03196</link><description>&lt;p&gt;
SecureReg:&#19968;&#20010;&#32467;&#21512;&#26041;&#27861;&#29992;&#20110;&#20027;&#21160;&#26292;&#38706;&#24694;&#24847;&#22495;&#21517;&#27880;&#20876;
&lt;/p&gt;
&lt;p&gt;
SecureReg: A Combined Framework for Proactively Exposing Malicious Domain Name Registrations. (arXiv:2401.03196v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03196
&lt;/p&gt;
&lt;p&gt;
SecureReg&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22495;&#21517;&#27880;&#20876;&#36807;&#31243;&#20013;&#20027;&#21160;&#26292;&#38706;&#24694;&#24847;&#22495;&#21517;&#27880;&#20876;&#65292;&#25552;&#20379;&#20102;&#26089;&#26399;&#23041;&#32961;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#28431;&#27934;&#31383;&#21475;&#65292;&#24182;&#20026;&#20027;&#21160;&#39044;&#38450;&#24615;&#25805;&#20316;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#19981;&#26029;&#22686;&#21152;&#65292;&#19981;&#27861;&#20998;&#23376;&#27599;&#22825;&#27880;&#20876;&#25968;&#21315;&#20010;&#26032;&#22495;&#21517;&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#12289;&#32593;&#32476;&#38035;&#40060;&#21644;&#39537;&#21160;&#19979;&#36733;&#31561;&#20114;&#32852;&#32593;&#25915;&#20987;&#65292;&#24378;&#35843;&#20102;&#21019;&#26032;&#26816;&#27979;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27880;&#20876;&#36807;&#31243;&#24320;&#22987;&#26102;&#35782;&#21035;&#21487;&#30097;&#22495;&#21517;&#12290;&#38468;&#24102;&#30340;&#25968;&#25454;&#27969;&#31243;&#36890;&#36807;&#27604;&#36739;&#26032;&#22495;&#21517;&#19982;&#27880;&#20876;&#22495;&#21517;&#20135;&#29983;&#20851;&#38190;&#29305;&#24449;&#65292;&#24378;&#35843;&#20102;&#20851;&#38190;&#30456;&#20284;&#24230;&#24471;&#20998;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;Canine&#27169;&#22411;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20998;&#26512;&#35821;&#20041;&#21644;&#25968;&#20540;&#23646;&#24615;&#65292;&#20026;&#26089;&#26399;&#23041;&#32961;&#26816;&#27979;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#21512;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#28431;&#27934;&#31383;&#21475;&#65292;&#21152;&#24378;&#20102;&#23545;&#28508;&#22312;&#23041;&#32961;&#30340;&#38450;&#24481;&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#32508;&#21512;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#20027;&#21160;&#39044;&#38450;&#24615;&#25805;&#20316;&#30340;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rising cyber threats, with miscreants registering thousands of new domains daily for Internet-scale attacks like spam, phishing, and drive-by downloads, emphasize the need for innovative detection methods. This paper introduces a cutting-edge approach for identifying suspicious domains at the onset of the registration process. The accompanying data pipeline generates crucial features by comparing new domains to registered domains,emphasizing the crucial similarity score. Leveraging a novel combination of Natural Language Processing (NLP) techniques, including a pretrained Canine model, and Multilayer Perceptron (MLP) models, our system analyzes semantic and numerical attributes, providing a robust solution for early threat detection. This integrated approach significantly reduces the window of vulnerability, fortifying defenses against potential threats. The findings demonstrate the effectiveness of the integrated approach and contribute to the ongoing efforts in developing proactive s
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20114;&#34917;&#20449;&#24687;&#30456;&#20114;&#23398;&#20064;&#65288;CIML&#65289;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#21106;&#20013;&#35299;&#20915;&#20102;&#27169;&#24577;&#38388;&#20887;&#20313;&#20449;&#24687;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#37319;&#29992;&#21152;&#27861;&#21644;&#20219;&#21153;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;CIML&#25104;&#21151;&#22320;&#28040;&#38500;&#20102;&#20887;&#20313;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02717</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#21106;&#30340;&#20114;&#34917;&#20449;&#24687;&#30456;&#20114;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Complementary Information Mutual Learning for Multimodality Medical Image Segmentation. (arXiv:2401.02717v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20114;&#34917;&#20449;&#24687;&#30456;&#20114;&#23398;&#20064;&#65288;CIML&#65289;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#21106;&#20013;&#35299;&#20915;&#20102;&#27169;&#24577;&#38388;&#20887;&#20313;&#20449;&#24687;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#37319;&#29992;&#21152;&#27861;&#21644;&#20219;&#21153;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;CIML&#25104;&#21151;&#22320;&#28040;&#38500;&#20102;&#20887;&#20313;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21307;&#23398;&#24433;&#20687;&#30340;&#23616;&#38480;&#24615;&#21644;&#32959;&#30244;&#20449;&#21495;&#30340;&#22810;&#26679;&#24615;&#65292;&#25918;&#23556;&#31185;&#21307;&#29983;&#24517;&#39035;&#21033;&#29992;&#22810;&#27169;&#24577;&#22270;&#20687;&#36827;&#34892;&#32959;&#30244;&#20998;&#21106;&#21644;&#35786;&#26029;&#12290;&#36825;&#23548;&#33268;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#20998;&#21106;&#20013;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#27169;&#24577;&#20043;&#38388;&#30340;&#20887;&#20313;&#24615;&#32473;&#29616;&#26377;&#30340;&#22522;&#20110;&#20943;&#27861;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#20363;&#22914;&#38169;&#35823;&#21028;&#26029;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#24573;&#35270;&#29305;&#23450;&#30340;&#27169;&#24577;&#20449;&#24687;&#65292;&#20197;&#21450;&#22686;&#21152;&#35748;&#30693;&#36127;&#33655;&#12290;&#36825;&#20123;&#26840;&#25163;&#30340;&#38382;&#39064;&#26368;&#32456;&#38477;&#20302;&#20102;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#24182;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20114;&#34917;&#20449;&#24687;&#30456;&#20114;&#23398;&#20064;&#65288;CIML&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#27169;&#24577;&#38388;&#20887;&#20313;&#20449;&#24687;&#30340;&#36127;&#38754;&#24433;&#21709;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#21644;&#35299;&#20915;&#12290;CIML&#37319;&#29992;&#20102;&#21152;&#27861;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#24402;&#32435;&#20559;&#32622;&#39537;&#21160;&#30340;&#20219;&#21153;&#20998;&#35299;&#21644;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#20887;&#20313;&#24615;&#36807;&#28388;&#26469;&#28040;&#38500;&#27169;&#24577;&#38388;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;CIML&#23558;&#22810;&#27169;&#24577;&#20998;&#21106;&#20219;&#21153;&#39318;&#20808;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Radiologists must utilize multiple modal images for tumor segmentation and diagnosis due to the limitations of medical imaging and the diversity of tumor signals. This leads to the development of multimodal learning in segmentation. However, the redundancy among modalities creates challenges for existing subtraction-based joint learning methods, such as misjudging the importance of modalities, ignoring specific modal information, and increasing cognitive load. These thorny issues ultimately decrease segmentation accuracy and increase the risk of overfitting. This paper presents the complementary information mutual learning (CIML) framework, which can mathematically model and address the negative impact of inter-modal redundant information. CIML adopts the idea of addition and removes inter-modal redundant information through inductive bias-driven task decomposition and message passing-based redundancy filtering. CIML first decomposes the multimodal segmentation task into multiple subta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#30340;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#23481;&#26131;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#31867;&#22411;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598; arXiv2Formal&#12290;</title><link>http://arxiv.org/abs/2310.07957</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#24418;&#24335;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Approach Towards Autoformalization. (arXiv:2310.07957v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#30340;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#23481;&#26131;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#31867;&#22411;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598; arXiv2Formal&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#25968;&#23398;&#35777;&#26126;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#26426;&#30340;&#36741;&#21161;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;&#33258;&#21160;&#24418;&#24335;&#21270;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#23398;&#33258;&#21160;&#36716;&#21270;&#20026;&#21487;&#20197;&#30001;&#31243;&#24207;&#39564;&#35777;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#23545;&#20110;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#39640;&#32423;&#25968;&#23398;&#26469;&#35828;&#12290;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#25968;&#23398;&#38656;&#35201;&#22823;&#37327;&#30340;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#26131;&#20110;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65306;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#65288;&#21253;&#21547;&#26410;&#38142;&#25509;&#30340;&#23450;&#20041;&#21644;&#23450;&#29702;&#30340;&#24418;&#24335;&#21270;&#65289;&#12289;&#23454;&#20307;&#38142;&#25509;&#65288;&#38142;&#25509;&#21040;&#27491;&#30830;&#30340;&#23450;&#29702;&#21644;&#23450;&#20041;&#65289;&#20197;&#21450;&#35843;&#25972;&#31867;&#22411;&#20197;&#36890;&#36807;&#31867;&#22411;&#26816;&#26597;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;arXiv2Formal&#65292;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;arXiv.org&#30340;&#35770;&#25991;&#20013;&#25277;&#21462;&#30340;50&#20010;&#23450;&#29702;&#22312;Lean&#23450;&#29702;&#35777;&#26126;&#22120;&#20013;&#36827;&#34892;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#27426;&#36814;&#20219;&#20309;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verifying mathematical proofs is difficult, but can be automated with the assistance of a computer. Autoformalization is the task of automatically translating natural language mathematics into a formal language that can be verified by a program. This is a challenging task, and especially for higher-level mathematics found in research papers. Research paper mathematics requires large amounts of background and context. In this paper, we propose an avenue towards tackling autoformalization for research-level mathematics, by breaking the task into easier and more approachable subtasks: unlinked formalization (formalization with unlinked definitions and theorems), entity linking (linking to the proper theorems and definitions), and finally adjusting types so it passes the type checker. In addition, we present arXiv2Formal, a benchmark dataset for unlinked formalization consisting of 50 theorems formalized for the Lean theorem prover sampled from papers on arXiv.org. We welcome any contribut
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;Transformer&#26550;&#26500;&#21644;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#20351;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.07402</link><description>&lt;p&gt;
NuTime: &#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#30340;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining. (arXiv:2310.07402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;Transformer&#26550;&#26500;&#21644;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#20351;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#30740;&#31350;&#26174;&#31034;&#20986;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#38480;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#25968;&#21315;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25968;&#20540;&#29305;&#24615;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#30334;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#37319;&#29992;Transformer&#26550;&#26500;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#21010;&#20998;&#20026;&#38750;&#37325;&#21472;&#31383;&#21475;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#31383;&#21475;&#30340;&#26631;&#20934;&#21270;&#24418;&#29366;&#21644;&#20004;&#20010;&#26631;&#37327;&#20540;&#34920;&#31034;&#27599;&#20010;&#31383;&#21475;&#20869;&#30340;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#12290;&#20026;&#20102;&#23558;&#21487;&#33021;&#20855;&#26377;&#20219;&#24847;&#25968;&#20540;&#23610;&#24230;&#30340;&#26631;&#37327;&#20540;&#23884;&#20837;&#21040;&#39640;&#32500;&#21521;&#37327;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#26522;&#20030;&#25152;&#26377;&#21487;&#33021;&#30340;&#26631;&#37327;&#20540;&#23610;&#24230;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#25552;&#20986;&#30340;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#37319;&#29992;&#31616;&#21333;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales to high-dimensional vectors, we propose a numerically multi-scaled embedding module enumerating all possible scales for the scalar values. The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#20302;&#36164;&#28304;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#30340;&#21487;&#33021;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#21487;&#20197;&#29992;&#20316;&#20302;&#36164;&#28304;&#35835;&#35299;&#20219;&#21153;&#20013;&#20154;&#24037;&#27880;&#37322;&#32773;&#30340;&#26367;&#20195;&#21697;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;LLMs&#20316;&#20026;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#22120;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#21457;&#24067;&#20102;&#22686;&#24378;&#29256;&#26412;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.12426</link><description>&lt;p&gt;
LLMs&#33021;&#22686;&#24378;&#20302;&#36164;&#28304;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#21527;&#65311;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges. (arXiv:2309.12426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#20302;&#36164;&#28304;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#30340;&#21487;&#33021;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#21487;&#20197;&#29992;&#20316;&#20302;&#36164;&#28304;&#35835;&#35299;&#20219;&#21153;&#20013;&#20154;&#24037;&#27880;&#37322;&#32773;&#30340;&#26367;&#20195;&#21697;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;LLMs&#20316;&#20026;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#22120;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#21457;&#24067;&#20102;&#22686;&#24378;&#29256;&#26412;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#33021;&#22815;&#36827;&#34892;&#25512;&#29702;&#21644;&#24212;&#29992;&#24120;&#35782;&#12290;&#19968;&#20010;&#30456;&#20851;&#30340;&#24212;&#29992;&#26159;&#23558;&#23427;&#20204;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#20379;&#21518;&#32493;&#20219;&#21153;&#20351;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;GPT-4&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#25277;&#21462;&#24335;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#27880;&#37322;&#36807;&#31243;&#26377;&#28508;&#21147;&#33410;&#30465;&#22823;&#37327;&#26102;&#38388;&#12289;&#37329;&#38065;&#21644;&#31934;&#21147;&#65292;&#36825;&#20123;&#37117;&#26159;&#29992;&#20110;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#24494;&#35843;&#21518;&#30340;&#24615;&#33021;&#20197;&#21450;&#27880;&#37322;&#30340;&#25104;&#26412;&#65292;&#35780;&#20272;&#20102;GPT-4&#20316;&#20026;&#20302;&#36164;&#28304;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#20154;&#24037;&#27880;&#37322;&#26367;&#20195;&#32773;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#23545;LLMs&#20316;&#20026;QA&#31995;&#32479;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#22120;&#30340;&#39318;&#27425;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#29420;&#29305;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#36825;&#23558;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#37325;&#26032;&#35780;&#20272;LLMs&#22312;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive zero shot performance on a wide range of NLP tasks, demonstrating the ability to reason and apply commonsense. A relevant application is to use them for creating high quality synthetic datasets for downstream tasks. In this work, we probe whether GPT-4 can be used to augment existing extractive reading comprehension datasets. Automating data annotation processes has the potential to save large amounts of time, money and effort that goes into manually labelling datasets. In this paper, we evaluate the performance of GPT-4 as a replacement for human annotators for low resource reading comprehension tasks, by comparing performance after fine tuning, and the cost associated with annotation. This work serves to be the first analysis of LLMs as synthetic data augmenters for QA systems, highlighting the unique opportunities and challenges. Additionally, we release augmented versions of low resource datasets, that will allow the researc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;LLMs&#26469;&#29983;&#25104;&#30828;&#20214;&#30340;&#23433;&#20840;&#26029;&#35328;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;SystemVerilog&#26029;&#35328;&#26469;&#26367;&#20195;&#32534;&#20889;&#20855;&#26377;&#25361;&#25112;&#30340;&#23433;&#20840;&#26029;&#35328;&#12290;</title><link>http://arxiv.org/abs/2306.14027</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#30828;&#20214;&#26029;&#35328;&#29983;&#25104;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
LLM-assisted Generation of Hardware Assertions. (arXiv:2306.14027v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;LLMs&#26469;&#29983;&#25104;&#30828;&#20214;&#30340;&#23433;&#20840;&#26029;&#35328;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;SystemVerilog&#26029;&#35328;&#26469;&#26367;&#20195;&#32534;&#20889;&#20855;&#26377;&#25361;&#25112;&#30340;&#23433;&#20840;&#26029;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#36890;&#24120;&#20381;&#36182;&#20110;&#30828;&#20214;&#30340;&#23433;&#20840;&#24615;&#12290;&#30828;&#20214;&#28431;&#27934;&#23545;&#31995;&#32479;&#26377;&#20005;&#37325;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#25216;&#26415;&#25903;&#25345;&#23433;&#20840;&#39564;&#35777;&#27963;&#21160;&#12290;&#26029;&#35328;&#39564;&#35777;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#39564;&#35777;&#25216;&#26415;&#65292;&#23427;&#28041;&#21450;&#22312;&#19968;&#32452;&#26029;&#35328;&#20013;&#25429;&#25417;&#35774;&#35745;&#24847;&#22270;&#65292;&#36825;&#20123;&#26029;&#35328;&#21487;&#29992;&#20110;&#24418;&#24335;&#39564;&#35777;&#25110;&#22522;&#20110;&#27979;&#35797;&#30340;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#32534;&#20889;&#20197;&#23433;&#20840;&#20026;&#20013;&#24515;&#30340;&#26029;&#35328;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30828;&#20214;&#26029;&#35328;&#29983;&#25104;&#30340;&#20195;&#30721;&#29983;&#25104;&#25216;&#26415;&#65292;&#20854;&#20013;&#20027;&#35201;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65288;&#20363;&#22914;&#22312;&#26029;&#35328;&#25991;&#20214;&#20013;&#30475;&#21040;&#30340;&#20195;&#30721;&#27880;&#37322;&#65289;&#29983;&#25104;SystemVerilog&#26029;&#35328;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#27969;&#34892;&#30340;LLM&#65292;&#24182;&#23545;&#20854;&#22312;&#32473;&#23450;&#19981;&#21516;&#35814;&#32454;&#32423;&#21035;&#30340;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#32534;&#20889;&#26029;&#35328;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29983;&#25104;&#21508;&#31181;LLM&#36741;&#21161;&#19979;&#20135;&#29983;&#30340;&#31995;&#32479;&#26029;&#35328;&#24418;&#24335;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The security of computer systems typically relies on a hardware root of trust. As vulnerabilities in hardware can have severe implications on a system, there is a need for techniques to support security verification activities. Assertion-based verification is a popular verification technique that involves capturing design intent in a set of assertions that can be used in formal verification or testing-based checking. However, writing security-centric assertions is a challenging task. In this work, we investigate the use of emerging large language models (LLMs) for code generation in hardware assertion generation for security, where primarily natural language prompts, such as those one would see as code comments in assertion files, are used to produce SystemVerilog assertions. We focus our attention on a popular LLM and characterize its ability to write assertions out of the box, given varying levels of detail in the prompt. We design an evaluation framework that generates a variety of 
&lt;/p&gt;</description></item><item><title>CodeGeeX&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;130&#20159;&#21442;&#25968;&#65292;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CodeGeeX&#22312;HumanEval-X&#19978;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;CodeGeeX&#21487;&#20197;&#23558;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;22%&#12290;</title><link>http://arxiv.org/abs/2303.17568</link><description>&lt;p&gt;
CodeGeeX&#65306;&#22810;&#35821;&#35328;&#35780;&#20272;&#19979;&#30340;&#39044;&#35757;&#32451;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X. (arXiv:2303.17568v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17568
&lt;/p&gt;
&lt;p&gt;
CodeGeeX&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;130&#20159;&#21442;&#25968;&#65292;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CodeGeeX&#22312;HumanEval-X&#19978;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;CodeGeeX&#21487;&#20197;&#23558;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;22%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;OpenAI Codex&#65289;&#21487;&#20197;&#29983;&#25104;&#27491;&#30830;&#35821;&#27861;&#21644;&#21151;&#33021;&#30340;&#20195;&#30721;&#65292;&#20351;&#31243;&#24207;&#21592;&#30340;&#32534;&#30721;&#26356;&#21152;&#39640;&#25928;&#65292;&#20351;&#25105;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36861;&#27714;&#26356;&#21152;&#36148;&#36817;&#29616;&#23454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CodeGeeX&#65292;&#19968;&#20010;&#20855;&#26377;130&#20159;&#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;CodeGeeX&#22312;2022&#24180;6&#26376;&#26102;&#22522;&#20110;23&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;8500&#20159;&#20196;&#29260;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;CodeGeeX&#22312;HumanEval-X&#19978;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#35268;&#27169;&#30456;&#20284;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#27169;&#22411;&#12290;&#22312;HumanEval&#65288;&#20165;&#38480;Python&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;HumanEval-X&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#25163;&#20889;C ++&#12289;Java&#12289;JavaScript&#21644;Go&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35780;&#20272;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;Visual Studio Code&#12289;JetBrains&#21644;Cloud Studio&#19978;&#26500;&#24314;&#20102;&#22522;&#20110;CodeGeeX&#30340;&#25193;&#23637;&#65292;&#27599;&#21608;&#20026;&#25968;&#20197;&#19975;&#35745;&#30340;&#27963;&#36291;&#29992;&#25143;&#29983;&#25104;47&#20159;&#20196;&#29260;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;CodeGeeX&#21487;&#20197;&#23558;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;22%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#22312;&#31038;&#20250;&#32972;&#26223;&#19979;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#39044;&#27979;&#22120;&#36827;&#34892;&#27169;&#22411;&#22810;&#26679;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#20010;&#20154;&#21463;&#20260;&#23475;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2203.07139</link><description>&lt;p&gt;
&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#65306;&#22810;&#27169;&#22411;&#24773;&#20917;&#19979;&#30340;&#20844;&#24179;&#24615;&#19982;&#20262;&#29702;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cross-model Fairness: Empirical Study of Fairness and Ethics Under Model Multiplicity. (arXiv:2203.07139v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#22312;&#31038;&#20250;&#32972;&#26223;&#19979;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#39044;&#27979;&#22120;&#36827;&#34892;&#27169;&#22411;&#22810;&#26679;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#20010;&#20154;&#21463;&#20260;&#23475;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#27979;&#27169;&#22411;&#26159;&#19968;&#20010;&#20005;&#26684;&#30340;&#25216;&#26415;&#26500;&#36896;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#22312;&#31038;&#20250;&#32972;&#26223;&#19979;&#36816;&#20316;&#65292;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#21892;&#24847;&#30340;&#24037;&#31243;&#36873;&#25321;&#21487;&#33021;&#24102;&#26469;&#38544;&#21547;&#30340;&#12289;&#38388;&#25509;&#30340;&#21644;&#24847;&#24819;&#19981;&#21040;&#30340;&#29616;&#23454;&#21518;&#26524;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#65292;&#28041;&#21450;&#21040;&#20010;&#20154;&#21644;&#32676;&#20307;&#65292;&#26159;&#19968;&#20010;&#30456;&#20851;&#30340;&#32771;&#34385;&#22240;&#32032;&#65307;&#23427;&#22312;&#25968;&#25454;&#25429;&#25417;&#21487;&#23548;&#33268;&#20154;&#20204;&#21463;&#21040;&#27495;&#35270;&#30340;&#21463;&#20445;&#25252;&#29305;&#24449;&#26102;&#20986;&#29616;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#27010;&#24565;&#20027;&#35201;&#38024;&#23545;&#22266;&#23450;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#20998;&#31867;&#38408;&#20540;&#19979;&#36827;&#34892;&#30740;&#31350;&#65292;&#21147;&#22270;&#35782;&#21035;&#21644;&#28040;&#38500;&#20854;&#36816;&#20316;&#20013;&#19981;&#24076;&#26395;&#30340;&#12289;&#20855;&#26377;&#27495;&#35270;&#24615;&#21644;&#21487;&#33021;&#36829;&#27861;&#30340;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#28335;&#20102;&#36825;&#20010;&#22266;&#23450;&#27169;&#22411;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#24182;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#21363;&#22312;&#20174;&#19968;&#32452;&#34920;&#29616;&#21516;&#26679;&#20986;&#33394;&#30340;&#27169;&#22411;&#20013;&#29305;&#23450;&#36873;&#25321;&#39044;&#27979;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20010;&#20154;&#21487;&#33021;&#21463;&#21040;&#20260;&#23475;&#65292;&#21363;&#22312;&#22522;&#20110;&#25928;&#29992;&#30340;&#27169;&#22411;&#22810;&#26679;&#24615;&#30340;&#35270;&#22270;&#19979;&#12290;&#30001;&#20110;&#19968;&#20010;&#20154;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#19979;&#21487;&#33021;&#34987;&#20998;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
While data-driven predictive models are a strictly technological construct, they may operate within a social context in which benign engineering choices entail implicit, indirect and unexpected real-life consequences. Fairness of such systems -- pertaining both to individuals and groups -- is one relevant consideration in this space; it arises when data capture protected characteristics upon which people may be discriminated. To date, this notion has predominantly been studied for a fixed model, often under different classification thresholds, striving to identify and eradicate undesirable, discriminative and possibly unlawful aspects of its operation. Here, we backtrack on this fixed model assumption to propose and explore a novel definition of cross-model fairness where individuals can be harmed when one predictor is chosen ad hoc from a group of equally-well performing models, i.e., in view of utility-based model multiplicity. Since a person may be classified differently across mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LARA&#30340;&#26032;&#22411;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#23616;&#37096;&#24863;&#30693;&#27880;&#24847;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LARA&#22312;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2107.11972</link><description>&lt;p&gt;
&#24403;&#26426;&#20250;&#26469;&#20020;&#26102;&#36827;&#34892;&#20132;&#26131;&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#30340;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Iterative Refinement Labeling. (arXiv:2107.11972v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.11972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LARA&#30340;&#26032;&#22411;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#23616;&#37096;&#24863;&#30693;&#27880;&#24847;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LARA&#22312;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26088;&#22312;&#26681;&#25454;&#24403;&#21069;&#24066;&#22330;&#24773;&#20917;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#39044;&#27979;&#37329;&#34701;&#36164;&#20135;&#30340;&#26410;&#26469;&#36235;&#21183;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37329;&#34701;&#25968;&#25454;&#30340;&#20302;&#20449;&#22122;&#27604;&#21644;&#38543;&#26426;&#24615;&#26497;&#24378;&#65292;&#22909;&#30340;&#20132;&#26131;&#26426;&#20250;&#26497;&#20026;&#31232;&#23569;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#19981;&#20180;&#32454;&#36873;&#25321;&#28508;&#22312;&#30340;&#30408;&#21033;&#26679;&#26412;&#65292;&#36825;&#20123;ML&#26041;&#27861;&#23481;&#26131;&#25429;&#25417;&#21040;&#22122;&#22768;&#32780;&#19981;&#26159;&#30495;&#23454;&#20449;&#21495;&#30340;&#27169;&#24335;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LARA&#30340;&#26032;&#22411;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#23616;&#37096;&#24863;&#30693;&#27880;&#24847;&#21147;&#65288;LA-Attention&#65289;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#65288;IRL&#65289;&#12290;LA-Attention&#26088;&#22312;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#37329;&#34701;&#25968;&#25454;&#20013;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#32780;IRL&#21017;&#26088;&#22312;&#36845;&#20195;&#22320;&#32454;&#21270;&#26631;&#27880;&#36807;&#31243;&#65292;&#36807;&#28388;&#25481;&#22122;&#22768;&#21644;&#26080;&#20851;&#26679;&#26412;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LARA&#22312;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Price movement forecasting aims at predicting the future trends of financial assets based on the current market conditions and other relevant information. Recently, machine learning (ML) methods have become increasingly popular and achieved promising results for price movement forecasting in both academia and industry. Most existing ML solutions formulate the forecasting problem as a classification (to predict the direction) or a regression (to predict the return) problem over the entire set of training data. However, due to the extremely low signal-to-noise ratio and stochastic nature of financial data, good trading opportunities are extremely scarce. As a result, without careful selection of potentially profitable samples, such ML methods are prone to capture the patterns of noises instead of real signals. To address this issue, we propose a novel price movement forecasting framework named LARA consisting of two main components: Locality-Aware Attention (LA-Attention) and Iterative R
&lt;/p&gt;</description></item></channel></rss>