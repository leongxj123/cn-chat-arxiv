<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;</title><link>https://rss.arxiv.org/abs/2402.01295</link><description>&lt;p&gt;
ExtremeCast: &#25552;&#21319;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#30340;&#26497;&#20540;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01295
&lt;/p&gt;
&lt;p&gt;
ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#22312;&#20840;&#29699;&#20013;&#26399;&#39044;&#25253;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26497;&#31471;&#20540;&#39044;&#27979;&#19982;&#27492;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#23545;&#31216;&#25439;&#22833;&#65292;&#22914;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#26377;&#20559;&#24046;&#24182;&#20302;&#20272;&#26497;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Exloss&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#20248;&#21270;&#31361;&#20986;&#26497;&#20540;&#65292;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#26497;&#31471;&#22825;&#27668;&#39044;&#25253;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#23427;&#22686;&#21152;&#20102;&#20687;&#32032;&#20540;&#30340;&#26041;&#24046;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#21512;&#20808;&#36827;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models. However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction. Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values. To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast. Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness. Combined with an advanced global weather forecast model, extensive experiments show that our sol
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#32773;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#20256;&#32479;&#27169;&#22411;&#38598;&#25104;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16260</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#22810;&#29702;&#35299;&#38598;&#25104;&#23454;&#29616;&#36234;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16260
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#32773;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#20256;&#32479;&#27169;&#22411;&#38598;&#25104;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#36234;&#30028;&#65288;OOD&#65289;&#29305;&#24449;&#34920;&#31034;&#39046;&#22495;&#35268;&#27169;&#23545;&#27169;&#22411;&#22312;OOD&#26816;&#27979;&#20013;&#25928;&#26524;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#27169;&#22411;&#38598;&#25104;&#20316;&#20026;&#22686;&#24378;&#36825;&#19968;&#29305;&#24449;&#34920;&#31034;&#39046;&#22495;&#30340;&#31361;&#20986;&#31574;&#30053;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#26399;&#30340;&#27169;&#22411;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#30406;/&#38556;&#30861;&#21487;&#35270;&#21270;&#21644;&#33258;&#32806;&#21512;&#25351;&#25968;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#21253;&#21547;&#21487;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#30340;&#26435;&#37325;&#65292;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#21487;&#21464;&#24615;&#65292;&#20174;&#32780;&#26410;&#33021;&#23454;&#29616;&#29305;&#24449;&#34920;&#31034;&#20013;&#25152;&#38656;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16260v1 Announce Type: cross  Abstract: Recent research underscores the pivotal role of the Out-of-Distribution (OOD) feature representation field scale in determining the efficacy of models in OOD detection. Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity.   However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods. We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation.   To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into di
&lt;/p&gt;</description></item><item><title>Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03640</link><description>&lt;p&gt;
Apollo&#65306;&#36731;&#37327;&#32423;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65306;&#35753;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#26222;&#24800;60&#20159;&#20154;
&lt;/p&gt;
&lt;p&gt;
Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03640
&lt;/p&gt;
&lt;p&gt;
Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#29699;&#21307;&#23398;&#30693;&#35782;&#30340;&#24222;&#22823;&#23384;&#20648;&#24211;&#20027;&#35201;&#26159;&#20197;&#33521;&#35821;&#20026;&#20027;&#65292;&#20294;&#22312;&#20256;&#36882;&#37327;&#36523;&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#26041;&#38754;&#65292;&#26412;&#22320;&#35821;&#35328;&#23545;&#20110;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#23558;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#20154;&#32676;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#28085;&#30422;&#20840;&#29699;61&#20159;&#20154;&#21475;&#30340;&#20845;&#31181;&#26368;&#24120;&#29992;&#35821;&#35328;&#30340;&#21307;&#23398;LLMs&#12290;&#36825;&#19968;&#21162;&#21147;&#26368;&#32456;&#20419;&#25104;&#20102;ApolloCorpora&#22810;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#38598;&#21644;XMedBench&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#22312;&#22810;&#35821;&#35328;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21457;&#24067;&#30340;Apollo&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#30456;&#23545;&#36739;&#23567;&#23610;&#23544;&#65288;&#21363;0.5B&#12289;1.8B&#12289;2B&#12289;6B&#21644;7B&#65289;&#19978;&#21462;&#24471;&#20102;&#19982;&#21516;&#31561;&#22823;&#23567;&#27169;&#22411;&#26368;&#20339;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;Apollo-7B&#26159;&#36804;&#20170;&#20026;&#27490;&#36798;&#21040;70B&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#36739;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03640v1 Announce Type: cross  Abstract: Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a
&lt;/p&gt;</description></item><item><title>CollaFuse&#26159;&#19968;&#20010;&#21463;&#25286;&#20998;&#23398;&#20064;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#26381;&#21153;&#22120;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#22312;&#21327;&#20316;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26102;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#20174;&#32780;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.19105</link><description>&lt;p&gt;
CollaFuse&#65306;&#22312;&#21327;&#20316;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#23548;&#33322;&#26377;&#38480;&#36164;&#28304;&#21644;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19105
&lt;/p&gt;
&lt;p&gt;
CollaFuse&#26159;&#19968;&#20010;&#21463;&#25286;&#20998;&#23398;&#20064;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#26381;&#21153;&#22120;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#22312;&#21327;&#20316;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26102;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#20174;&#32780;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#25193;&#25955;&#24335;&#27169;&#22411;&#22312;&#25968;&#25454;&#38656;&#27714;&#21644;&#38544;&#31169;&#26041;&#38754;&#32473;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#24102;&#26469;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#32852;&#37030;&#23398;&#20064;&#20998;&#21457;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#20250;&#32473;&#20010;&#21035;&#23458;&#25143;&#24102;&#26469;&#21387;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#36793;&#32536;&#35774;&#22791;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CollaFuse&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#25286;&#20998;&#23398;&#20064;&#21551;&#21457;&#30340;&#26032;&#26694;&#26550;&#12290;&#20026;&#20102;&#26377;&#25928;&#21327;&#20316;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;CollaFuse&#23454;&#29616;&#20102;&#20849;&#20139;&#26381;&#21153;&#22120;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#36825;&#36890;&#36807;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#26412;&#22320;&#20445;&#30041;&#25968;&#25454;&#21644;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#30340;GPU&#36827;&#31243;&#65292;&#21516;&#26102;&#23558;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#30340;&#36827;&#31243;&#22806;&#21253;&#32473;&#20849;&#20139;&#26381;&#21153;&#22120;&#26469;&#23454;&#29616;&#12290;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#23637;&#31034;&#65292;CollaFuse&#36890;&#36807;&#22823;&#22823;&#20943;&#23569;&#23545;&#25935;&#24863;&#20449;&#24687;&#20849;&#20139;&#30340;&#38656;&#27714;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19105v1 Announce Type: cross  Abstract: In the landscape of generative artificial intelligence, diffusion-based models present challenges for socio-technical systems in data requirements and privacy. Traditional approaches like federated learning distribute the learning process but strain individual clients, especially with constrained resources (e.g., edge devices). In response to these challenges, we introduce CollaFuse, a novel framework inspired by split learning. Tailored for efficient and collaborative use of denoising diffusion probabilistic models, CollaFuse enables shared server training and inference, alleviating client computational burdens. This is achieved by retaining data and computationally inexpensive GPU processes locally at each client while outsourcing the computationally expensive processes to the shared server. Demonstrated in a healthcare context, CollaFuse enhances privacy by highly reducing the need for sensitive information sharing. These capabiliti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#34920;&#31034;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#26377;&#30528;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.16998</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21548;&#21040;&#20102;&#20160;&#20040;&#65311;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21548;&#35273;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
What Do Language Models Hear? Probing for Auditory Representations in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#34920;&#31034;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#26377;&#30528;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23545;&#29289;&#20307;&#30340;&#22768;&#38899;&#20855;&#26377;&#21547;&#20041;&#28145;&#21051;&#19988;&#22522;&#20110;&#23454;&#36136;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#27169;&#22411;&#32473;&#20986;&#19968;&#20010;&#23545;&#35937;&#30340;&#22768;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#32473;&#23450;&#19982;&#35813;&#23545;&#35937;&#30456;&#20851;&#30340;&#38899;&#39057;&#29255;&#27573;&#30340;&#24773;&#20917;&#19979;&#26816;&#32034;&#20986;&#35813;&#23545;&#35937;&#30340;&#27491;&#30830;&#25991;&#26412;&#34920;&#31034;&#12290;&#36825;&#20010;&#25506;&#38024;&#26159;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#25512;&#21160;&#23545;&#35937;&#30340;&#35821;&#35328;&#34920;&#31034;&#21644;&#22768;&#38899;&#34920;&#31034;&#24444;&#27492;&#25509;&#36817;&#12290;&#22312;&#35757;&#32451;&#20043;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#25506;&#38024;&#23545;&#20110;&#19968;&#20123;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#23545;&#35937;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38899;&#39057;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#25506;&#38024;&#30340;&#27867;&#21270;&#33021;&#21147;&#36229;&#36807;&#20102;&#38543;&#26426;&#29468;&#27979;&#30340;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#20855;&#26377;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16998v1 Announce Type: cross  Abstract: This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.
&lt;/p&gt;</description></item><item><title>ToolSword&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#32454;&#33268;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#25345;&#20037;&#23384;&#22312;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10753</link><description>&lt;p&gt;
ToolSword&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#36328;&#19977;&#20010;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10753
&lt;/p&gt;
&lt;p&gt;
ToolSword&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#32454;&#33268;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#25345;&#20037;&#23384;&#22312;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10753v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25277;&#35937;&#65306;&#24037;&#20855;&#23398;&#20064;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#30784;&#26041;&#27861;&#12290;&#23613;&#31649;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#21033;&#29992;&#24037;&#20855;&#26469;&#22686;&#24378;LLMs&#65292;&#20294;&#23427;&#32463;&#24120;&#24573;&#35270;&#19982;&#20854;&#24212;&#29992;&#30456;&#20851;&#30340;&#26032;&#20852;&#23433;&#20840;&#32771;&#34385;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$ToolSword$&#65292;&#36825;&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#32454;&#33268;&#35843;&#26597;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ToolSword&#21246;&#30011;&#20102;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#20845;&#20010;&#23433;&#20840;&#22330;&#26223;&#65292;&#21253;&#25324;&#36755;&#20837;&#38454;&#27573;&#30340;$&#24694;&#24847;$ $&#26597;&#35810;$&#21644;$&#36234;&#29425;$ $&#25915;&#20987;$&#65292;&#25191;&#34892;&#38454;&#27573;&#30340;$&#22122;&#22768;$ $&#35823;&#23548;$&#21644;$&#39118;&#38505;$ $&#32447;&#32034;$&#65292;&#20197;&#21450;&#36755;&#20986;&#38454;&#27573;&#30340;$&#26377;&#23475;$ $&#21453;&#39304;$&#21644;$&#38169;&#35823;$ $&#20914;&#31361;$&#12290;&#23545;11&#20010;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23384;&#22312;&#25345;&#20037;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#22914;&#22788;&#29702;&#26377;&#23475;&#26597;&#35810;&#12289;&#20351;&#29992;&#39118;&#38505;&#24037;&#20855;&#21644;&#25552;&#20379;&#26377;&#23475;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10753v1 Announce Type: cross  Abstract: Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present $ToolSword$, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and $harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedb
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.01787</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21361;&#23475;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Harm Amplification in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01787
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687; (T2I) &#27169;&#22411;&#24050;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23384;&#22312;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20351;&#29992;&#25143;&#36755;&#20837;&#30475;&#20284;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#22270;&#20687;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#21361;&#23475;&#25918;&#22823;&#65292;&#23427;&#27604;&#23545;&#25239;&#25552;&#31034;&#26356;&#20855;&#28508;&#22312;&#39118;&#38505;&#65292;&#20351;&#29992;&#25143;&#26080;&#24847;&#38388;&#36973;&#21463;&#20260;&#23475;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#36827;&#19968;&#27493;&#36129;&#29486;&#20110;&#24320;&#21457;&#29992;&#20110;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#22330;&#26223;&#65292;&#21253;&#25324;&#37327;&#21270;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#19981;&#21516;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#24037;&#20855;&#21435;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) models have emerged as a significant advancement in generative AI; however, there exist safety concerns regarding their potential to produce harmful image outputs even when users input seemingly safe prompts. This phenomenon, where T2I models generate harmful representations that were not explicit in the input, poses a potentially greater risk than adversarial prompts, leaving users unintentionally exposed to harms. Our paper addresses this issue by first introducing a formal definition for this phenomenon, termed harm amplification. We further contribute to the field by developing methodologies to quantify harm amplification in which we consider the harm of the model output in the context of user input. We then empirically examine how to apply these different methodologies to simulate real-world deployment scenarios including a quantification of disparate impacts across genders resulting from harm amplification. Together, our work aims to offer researchers tools to
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17542</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#21512;&#21307;&#23398;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Effective Learning: A Comprehensive Medical Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26088;&#22312;&#20197;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#25968;&#25454;&#26469;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#20854;&#28041;&#21450;&#20851;&#27880;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#25968;&#37327;&#30340;&#31574;&#30053;&#65292;&#30830;&#20445;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#20855;&#26377;&#39640;&#20449;&#24687;&#20215;&#20540;&#12290;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#22312;&#21152;&#24555;AI&#35757;&#32451;&#12289;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#33410;&#30465;&#25968;&#25454;&#23384;&#20648;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#22312;&#36817;&#24180;&#26469;&#21307;&#23398;&#25968;&#25454;&#30340;&#25968;&#37327;&#36229;&#20986;&#20102;&#35768;&#22810;&#20154;&#30340;&#39044;&#26399;&#26102;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21644;&#32508;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;31&#20010;&#21307;&#30103;&#20013;&#24515;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;(DataDEL)&#65292;&#29992;&#20110;&#27604;&#36739;&#30340;&#22522;&#20934;&#26041;&#27861;(MedDEL)&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#23458;&#35266;&#34913;&#37327;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#24615;&#33021;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;(NormDEL)&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#22312;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive e
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#20294;&#37325;&#35201;&#30340;&#31185;&#23398;&#35282;&#33394;&#65292;&#21363;AI&#20316;&#20026;&#25506;&#32034;&#12290;&#23427;&#24378;&#35843;&#36890;&#36807;&#21019;&#24314;&#21644;&#30740;&#31350;&#26234;&#33021;&#31995;&#32479;&#26469;&#25581;&#31034;&#21487;&#33021;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#26234;&#33021;&#24418;&#24335;&#19981;&#21516;&#30340;&#20505;&#36873;&#26500;&#24314;&#27169;&#22359;&#12290;&#35770;&#25991;&#36890;&#36807;&#35752;&#35770;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#26032;&#39062;&#21644;&#21019;&#36896;&#24615;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35828;&#26126;&#20102;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2401.07964</link><description>&lt;p&gt;
AI&#20316;&#20026;&#25506;&#32034;&#65306;&#23548;&#33322;&#26234;&#33021;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
AI-as-exploration: Navigating intelligence space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07964
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#20294;&#37325;&#35201;&#30340;&#31185;&#23398;&#35282;&#33394;&#65292;&#21363;AI&#20316;&#20026;&#25506;&#32034;&#12290;&#23427;&#24378;&#35843;&#36890;&#36807;&#21019;&#24314;&#21644;&#30740;&#31350;&#26234;&#33021;&#31995;&#32479;&#26469;&#25581;&#31034;&#21487;&#33021;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#26234;&#33021;&#24418;&#24335;&#19981;&#21516;&#30340;&#20505;&#36873;&#26500;&#24314;&#27169;&#22359;&#12290;&#35770;&#25991;&#36890;&#36807;&#35752;&#35770;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#26032;&#39062;&#21644;&#21019;&#36896;&#24615;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35828;&#26126;&#20102;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#25317;&#26377;&#35768;&#22810;&#29983;&#21629;&#30340;&#39046;&#22495;&#65292;&#36825;&#20010;&#26415;&#35821;&#24050;&#32463;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#31185;&#23398;&#21644;&#21830;&#19994;&#21162;&#21147;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#38416;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#19968;&#20010;&#34987;&#24573;&#35270;&#20294;&#21313;&#20998;&#37325;&#35201;&#30340;&#31185;&#23398;&#35282;&#33394;&#65292;&#21363;&#8220;AI&#20316;&#20026;&#25506;&#32034;&#8221;&#12290;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#21019;&#24314;&#21644;&#30740;&#31350;&#33021;&#22815;&#25581;&#31034;&#26234;&#33021;&#20505;&#36873;&#26500;&#24314;&#27169;&#22359;&#30340;&#31995;&#32479;&#65292;&#36825;&#20123;&#27169;&#22359;&#21487;&#33021;&#19981;&#21516;&#20110;&#25105;&#20204;&#29087;&#24713;&#30340;&#20154;&#31867;&#21644;&#21160;&#29289;&#26234;&#33021;&#24418;&#24335;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#35748;&#20026;&#20154;&#24037;&#26234;&#33021;&#26159;&#25506;&#32034;&#26234;&#33021;&#31354;&#38388;&#65292;&#21363;&#21487;&#33021;&#30340;&#26234;&#33021;&#31995;&#32479;&#31354;&#38388;&#65292;&#30340;&#26368;&#20339;&#24037;&#20855;&#20043;&#19968;&#12290;&#25105;&#36890;&#36807;&#20851;&#27880;&#19968;&#20010;&#20855;&#20307;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21363;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#26032;&#39062;&#21644;&#21019;&#36896;&#24615;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#26469;&#35828;&#26126;AI&#20316;&#20026;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#25105;&#23637;&#31034;&#20102;&#23613;&#31649;&#21518;&#32773;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#24456;&#21487;&#33021;&#20197;&#26681;&#26412;&#19981;&#21516;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence is a field that lives many lives, and the term has come to encompass a motley collection of scientific and commercial endeavours. In this paper, I articulate the contours of a rather neglected but central scientific role that AI has to play, which I dub `AI-as-exploration'.The basic thrust of AI-as-exploration is that of creating and studying systems that can reveal candidate building blocks of intelligence that may differ from the forms of human and animal intelligence we are familiar with. In other words, I suggest that AI is one of the best tools we have for exploring intelligence space, namely the space of possible intelligent systems. I illustrate the value of AI-as-exploration by focusing on a specific case study, i.e., recent work on the capacity to combine novel and invented concepts in humans and Large Language Models. I show that the latter, despite showing human-level accuracy in such a task, most probably solve it in ways radically different, but no 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;transformer&#32593;&#32476;&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#30382;&#23618;&#27874;&#22312;&#25552;&#21462;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14267</link><description>&lt;p&gt;
Transformers&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#65306;&#22312;&#26102;&#38388;&#19978;&#20256;&#36882;&#19978;&#19979;&#25991;&#30340;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers and Cortical Waves: Encoders for Pulling In Context Across Time. (arXiv:2401.14267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;transformer&#32593;&#32476;&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#30382;&#23618;&#27874;&#22312;&#25552;&#21462;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;ChatGPT&#21644;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;transformer&#32593;&#32476;&#30340;&#33021;&#21147;&#24050;&#32463;&#24341;&#36215;&#20102;&#19990;&#30028;&#30340;&#20851;&#27880;&#12290;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#23558;&#23436;&#25972;&#30340;&#36755;&#20837;&#24207;&#21015;&#65288;&#20363;&#22914;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#21333;&#35789;&#65289;&#36716;&#21270;&#20026;&#19968;&#20010;&#38271;&#30340;&#8220;&#32534;&#30721;&#21521;&#37327;&#8221;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#23398;&#20064;&#33258;&#28982;&#24207;&#21015;&#20013;&#30340;&#38271;&#31243;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#8220;&#33258;&#27880;&#24847;&#21147;&#8221;&#24212;&#29992;&#20110;&#36825;&#20010;&#32534;&#30721;&#21521;&#37327;&#65292;&#36890;&#36807;&#35745;&#31639;&#36755;&#20837;&#24207;&#21015;&#20013;&#21333;&#35789;&#23545;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#22686;&#24378;&#20102;transformer&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#35748;&#20026;&#31070;&#32463;&#27963;&#21160;&#22312;&#21333;&#20010;&#30382;&#23618;&#21306;&#22495;&#20869;&#25110;&#25972;&#20010;&#22823;&#33041;&#33539;&#22260;&#20869;&#20256;&#25773;&#30340;&#27874;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#32534;&#30721;&#21407;&#29702;&#12290;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#21051;&#23558;&#26368;&#36817;&#30340;&#36755;&#20837;&#21382;&#21490;&#23553;&#35013;&#20026;&#21333;&#20010;&#31354;&#38388;&#27169;&#24335;&#65292;&#30382;&#23618;&#27874;&#21487;&#20197;&#20174;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#25552;&#21462;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#36825;&#19982;&#35745;&#31639;&#21407;&#29702;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention. The crucial computational mechanism underlying their performance relies on transforming a complete input sequence - for example, all the words in a sentence into a long "encoding vector" - that allows transformers to learn long-range temporal dependencies in naturalistic sequences. Specifically, "self-attention" applied to this encoding vector enhances temporal context in transformers by computing associations between pairs of words in the input sequence. We suggest that waves of neural activity, traveling across single cortical regions or across multiple regions at the whole-brain scale, could implement a similar encoding principle. By encapsulating recent input history into a single spatial pattern at each moment in time, cortical waves may enable temporal context to be extracted from sequences of sensory inputs, the same computational principle used in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#32858;&#31867;&#21644;&#20998;&#24067;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#31163;&#25955;&#20998;&#24067;&#32858;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#26368;&#20248;&#20256;&#36755;&#26500;&#24314;&#30456;&#20284;&#24230;&#30697;&#38453;&#65292;&#25105;&#20204;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.13913</link><description>&lt;p&gt;
&#31163;&#25955;&#20998;&#24067;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spectral Clustering for Discrete Distributions. (arXiv:2401.13913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#32858;&#31867;&#21644;&#20998;&#24067;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#31163;&#25955;&#20998;&#24067;&#32858;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#26368;&#20248;&#20256;&#36755;&#26500;&#24314;&#30456;&#20284;&#24230;&#30697;&#38453;&#65292;&#25105;&#20204;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#20998;&#24067;&#32858;&#31867;&#65288;D2C&#65289;&#36890;&#24120;&#36890;&#36807;Wasserstein&#36136;&#24515;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#20551;&#35774;&#19979;&#24037;&#20316;&#65292;&#21363;&#32858;&#31867;&#21487;&#20197;&#36890;&#36807;&#36136;&#24515;&#24456;&#22909;&#22320;&#34920;&#31034;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#35889;&#32858;&#31867;&#21644;&#20998;&#24067;&#30456;&#20284;&#24230;&#24230;&#37327;&#65288;&#20363;&#22914;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;Wasserstein&#36317;&#31163;&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;D2C&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32447;&#24615;&#26368;&#20248;&#20256;&#36755;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39640;&#25928;&#22320;&#26500;&#24314;&#30456;&#20284;&#24230;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20445;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#32858;&#31867;&#20998;&#24067;&#26041;&#38754;&#30340;&#25104;&#21151;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete distribution clustering (D2C) was often solved by Wasserstein barycenter methods. These methods are under a common assumption that clusters can be well represented by barycenters, which may not hold in many real applications. In this work, we propose a simple yet effective framework based on spectral clustering and distribution affinity measures (e.g., maximum mean discrepancy and Wasserstein distance) for D2C. To improve the scalability, we propose to use linear optimal transport to construct affinity matrices efficiently on large datasets. We provide theoretical guarantees for the success of the proposed methods in clustering distributions. Experiments on synthetic and real data show that our methods outperform the baselines largely in terms of both clustering accuracy and computational efficiency.
&lt;/p&gt;</description></item><item><title>MIMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#21033;&#29992;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;Vision Transformers&#65288;ViTs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.04960</link><description>&lt;p&gt;
MIMIR: &#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness. (arXiv:2312.04960v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04960
&lt;/p&gt;
&lt;p&gt;
MIMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#21033;&#29992;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;Vision Transformers&#65288;ViTs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViTs&#65289;&#30456;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;ViTs&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#24314;&#31435;&#24378;&#22823;&#30340;CNN&#27169;&#22411;&#30340;&#26368;&#25104;&#21151;&#26041;&#27861;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;ViTs&#21644;CNNs&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#22914;&#26356;&#22909;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#38450;&#27490;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#21333;&#20010;&#22359;&#19978;&#65292;&#25110;&#20002;&#24323;&#20302;&#27880;&#24847;&#21147;&#30340;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#36981;&#24490;&#20256;&#32479;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#30340;&#35774;&#35745;&#65292;&#38480;&#21046;&#20102;&#23545;ViTs&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;MIMIR&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#20013;&#30340;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#26500;&#24314;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#25509;&#21463;&#23545;&#25239;&#24615;&#20363;&#23376;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#23558;&#24178;&#20928;&#30340;&#20363;&#23376;&#20316;&#20026;&#24314;&#27169;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20114;&#20449;&#24687;&#65288;MI&#65289;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) achieve superior performance on various tasks compared to convolutional neural networks (CNNs), but ViTs are also vulnerable to adversarial attacks. Adversarial training is one of the most successful methods to build robust CNN models. Thus, recent works explored new methodologies for adversarial training of ViTs based on the differences between ViTs and CNNs, such as better training strategies, preventing attention from focusing on a single block, or discarding low-attention embeddings. However, these methods still follow the design of traditional supervised adversarial training, limiting the potential of adversarial training on ViTs. This paper proposes a novel defense method, MIMIR, which aims to build a different adversarial training methodology by utilizing Masked Image Modeling at pre-training. We create an autoencoder that accepts adversarial examples as input but takes the clean examples as the modeling target. Then, we create a mutual information (MI
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#35774;&#35745;&#20102;Injectivity Bit Flip Attack&#26469;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#20854;&#23545;&#22270;&#32467;&#26500;&#30340;&#35782;&#21035;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#23545;&#20301;&#21453;&#36716;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01205</link><description>&lt;p&gt;
&#20351;&#29992;&#20301;&#21453;&#36716;&#25915;&#20987;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;Weisfeiler&#21644;Lehman&#21464;&#24471;&#20919;&#28448;&#20102;
&lt;/p&gt;
&lt;p&gt;
Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent. (arXiv:2311.01205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01205
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;Injectivity Bit Flip Attack&#26469;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#20854;&#23545;&#22270;&#32467;&#26500;&#30340;&#35782;&#21035;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#23545;&#20301;&#21453;&#36716;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25915;&#20987;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#27602;&#21270;&#21644;&#35268;&#36991;&#19978;&#65292;&#24573;&#30053;&#20102;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#26435;&#37325;&#30340;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#65292;&#22914;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20301;&#21453;&#36716;&#25915;&#20987;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29420;&#29305;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27880;&#20837;&#29575;&#20301;&#21453;&#36716;&#25915;&#20987;&#65288;Injectivity Bit Flip Attack&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#20301;&#21453;&#36716;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#38024;&#23545;&#37327;&#21270;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#23398;&#20064;&#37051;&#22495;&#32858;&#21512;&#20989;&#25968;&#65292;&#38477;&#20302;&#20102;&#20854;&#21306;&#20998;&#22270;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22833;&#21435;&#20102;Weisfeiler-Lehman&#27979;&#35797;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21033;&#29992;&#29305;&#23450;&#20110;&#26576;&#20123;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25968;&#23398;&#23646;&#24615;&#21487;&#33021;&#20250;&#26174;&#33879;&#22686;&#21152;&#20854;&#23545;&#20301;&#21453;&#36716;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#27880;&#20837;&#29575;&#20301;&#21453;&#36716;&#25915;&#20987;&#21487;&#20197;&#23558;&#21508;&#31181;&#22270;&#23646;&#24615;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26368;&#22823;&#34920;&#36798;&#24615;&#21516;&#26500;&#32593;&#32476;&#38477;&#32423;&#20026;&#38543;&#26426;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior attacks on graph neural networks have mostly focused on graph poisoning and evasion, neglecting the network's weights and biases. Traditional weight-based fault injection attacks, such as bit flip attacks used for convolutional neural networks, do not consider the unique properties of graph neural networks. We propose the Injectivity Bit Flip Attack, the first bit flip attack designed specifically for graph neural networks. Our attack targets the learnable neighborhood aggregation functions in quantized message passing neural networks, degrading their ability to distinguish graph structures and losing the expressivity of the Weisfeiler-Lehman test. Our findings suggest that exploiting mathematical properties specific to certain graph neural network architectures can significantly increase their vulnerability to bit flip attacks. Injectivity Bit Flip Attacks can degrade the maximal expressive Graph Isomorphism Networks trained on various graph property prediction datasets to rando
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#20248;&#21270;&#20840;&#23616;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#24635;&#22870;&#21169;&#65292;&#23454;&#29616;&#21327;&#20316;&#20915;&#31574;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#21463;&#20449;&#24687;&#20849;&#20139;&#19981;&#23436;&#22791;&#30340;&#24433;&#21709;&#65292;&#19988;&#20855;&#26377;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.00201</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Natural Policy Gradient Methods for Multi-task Reinforcement Learning. (arXiv:2311.00201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#20248;&#21270;&#20840;&#23616;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#24635;&#22870;&#21169;&#65292;&#23454;&#29616;&#21327;&#20316;&#20915;&#31574;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#21463;&#20449;&#24687;&#20849;&#20139;&#19981;&#23436;&#22791;&#30340;&#24433;&#21709;&#65292;&#19988;&#20855;&#26377;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#20915;&#31574;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22810;&#20219;&#21153;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#26377;&#33258;&#24049;&#30340;&#31169;&#26377;&#22870;&#21169;&#20989;&#25968;&#23545;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#20849;&#20139;&#29615;&#22659;&#30340;&#30456;&#21516;&#36716;&#31227;&#26680;&#12290;&#38024;&#23545;&#26080;&#38480;&#26102;&#38388;&#27493;&#26631;&#35760;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#20986;&#19968;&#31181;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#65292;&#22312;&#20998;&#25955;&#30340;&#26041;&#24335;&#19979;&#65292;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#25240;&#25187;&#24635;&#22870;&#21169;&#20043;&#21644;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#20165;&#19982;&#20854;&#22312;&#32473;&#23450;&#22270;&#25299;&#25169;&#20013;&#30340;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#22312; softmax &#21442;&#25968;&#21270;&#19979;&#24320;&#23637;&#20102;&#32852;&#37030;&#32431;&#31929;&#21644;&#29109;&#27491;&#21017;&#21270;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#23558;&#26799;&#24230;&#36319;&#36394;&#24212;&#29992;&#20110;&#20840;&#23616; Q &#20989;&#25968;&#65292;&#20197;&#20943;&#36731;&#20449;&#24687;&#20849;&#20139;&#19981;&#23436;&#22791;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#31934;&#30830;&#31574;&#30053;&#35780;&#20272;&#19979;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#65292;&#36825;&#20123;&#20445;&#35777;&#20960;&#20046;&#26159;&#29420;&#31435;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated reinforcement learning (RL) enables collaborative decision making of multiple distributed agents without sharing local data trajectories. In this work, we consider a multi-task setting, in which each agent has its own private reward function corresponding to different tasks, while sharing the same transition kernel of the environment. Focusing on infinite-horizon tabular Markov decision processes, the goal is to learn a globally optimal policy that maximizes the sum of the discounted total rewards of all the agents in a decentralized manner, where each agent only communicates with its neighbors over some prescribed graph topology. We develop federated vanilla and entropy-regularized natural policy gradient (NPG) methods under softmax parameterization, where gradient tracking is applied to the global Q-function to mitigate the impact of imperfect information sharing. We establish non-asymptotic global convergence guarantees under exact policy evaluation, which are nearly indep
&lt;/p&gt;</description></item><item><title>Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.</title><link>http://arxiv.org/abs/2310.17072</link><description>&lt;p&gt;
&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17072
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;MMP&#65289;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#19968;&#31995;&#21015;&#36830;&#32493;&#36712;&#36857;&#27969;&#24418;&#65292;&#27599;&#19968;&#20010;&#36712;&#36857;&#27969;&#24418;&#37117;&#33021;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#30001;&#23545;&#27969;&#24418;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#35299;&#30721;&#20989;&#25968;&#20197;&#21450;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#23494;&#24230;&#32452;&#25104;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#30001;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#25197;&#26354;&#65292;MMP&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;--&#36890;&#36807;&#21464;&#24418;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#30456;&#20284;&#30340;&#36816;&#21160;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#27861;&#30456;&#37051;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;IMMP&#65289;&#65292;&#20854;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20445;&#25345;&#20102;&#27969;&#24418;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#21644;&#20351;&#29992;&#20102;&#19968;&#20010;Riemannian&#24230;&#37327;&#65292;&#29992;&#20110;&#36816;&#21160;&#31354;&#38388;&#65288;&#21363;&#65292;&#21442;&#25968;&#21270;&#26354;&#32447;&#31354;&#38388;&#65289;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;CurveGeom Riemannian&#24230;&#37327;&#12290;&#23545;&#20110;&#24179;&#38754;&#38556;&#30861;&#36991;&#35753;&#36816;&#21160;&#21644;&#25512;&#21160;&#25805;&#32437;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IMMP&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;MMP&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Gabe-YHLee/IMMP&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2310.02304</link><description>&lt;p&gt;
&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65306;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;&#24605;&#32500;&#26641;&#21644;&#31243;&#24207;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#8220;&#33050;&#25163;&#26550;&#8221;&#31243;&#24207;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#31243;&#24207;&#26500;&#24314;&#20102;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;&#33050;&#25163;&#26550;&#31243;&#24207;&#36890;&#24120;&#20351;&#29992;Python&#31561;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#31181;&#23376;&#8220;&#25913;&#36827;&#22120;&#8221;&#24320;&#22987;&#65292;&#36890;&#36807;&#22810;&#27425;&#26597;&#35810;&#35821;&#35328;&#27169;&#22411;&#24182;&#36820;&#22238;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#26681;&#25454;&#32473;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25913;&#36827;&#36755;&#20837;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36816;&#34892;&#36825;&#20010;&#31181;&#23376;&#25913;&#36827;&#22120;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#22312;&#19968;&#31995;&#21015;&#32454;&#20998;&#20219;&#21153;&#20013;&#65292;&#24471;&#21040;&#30340;&#25913;&#36827;&#25913;&#36827;&#22120;&#29983;&#25104;&#30340;&#31243;&#24207;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#31181;&#23376;&#25913;&#36827;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#30340;&#21508;&#31181;&#33258;&#25105;&#25913;&#36827;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#27874;&#26463;&#25628;&#32034;&#12289;&#36951;&#20256;&#31639;&#27861;&#21644;&#27169;&#25311;&#36864;&#28779;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#27809;&#26377;&#25913;&#21464;&#65292;&#36825;&#24182;&#19981;&#26159;&#19968;&#31181;&#22686;&#38271;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#23398;&#38382;&#31572;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.16035</link><description>&lt;p&gt;
MedEdit&#65306;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases. (arXiv:2309.16035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#23398;&#38382;&#31572;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34429;&#28982;&#22312;&#19968;&#33324;&#39046;&#22495;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#38754;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24448;&#24448;&#20316;&#20026;&#8220;&#40657;&#30418;&#8221;&#36816;&#20316;&#65292;&#38590;&#20197;&#20462;&#25913;&#20854;&#34892;&#20026;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#26088;&#22312;&#25913;&#36827;LLM&#30340;&#21709;&#24212;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26816;&#32034;&#31574;&#30053;&#65292;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;LLM&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#12290;&#36890;&#36807;&#23545;MedQA-SMILE&#25968;&#25454;&#38598;&#36827;&#34892;&#21307;&#23398;QA&#30340;&#37325;&#28857;&#30740;&#31350;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#26816;&#32034;&#27169;&#22411;&#21644;&#21521;LLM&#25552;&#20379;&#30340;&#20107;&#23454;&#25968;&#37327;&#23545;&#20854;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#32534;&#36753;&#21518;&#30340;Vicuna&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20174;44.46&#65285;&#25552;&#39640;&#21040;48.54&#65285;&#12290;&#36825;&#39033;&#24037;&#20316;&#20984;&#26174;&#20102;&#27169;&#22411;&#32534;&#36753;&#25913;&#21892;LLM&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20026;&#32531;&#35299;&#40657;&#30418;LLM&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks like medical question answering (QA). Moreover, they tend to function as "black-boxes," making it challenging to modify their behavior. Addressing this, our study delves into model editing utilizing in-context learning, aiming to improve LLM responses without the need for fine-tuning or retraining. Specifically, we propose a comprehensive retrieval strategy to extract medical facts from an external knowledge base, and then we incorporate them into the query prompt for the LLM. Focusing on medical QA using the MedQA-SMILE dataset, we evaluate the impact of different retrieval models and the number of facts provided to the LLM. Notably, our edited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%. This work underscores the potential of model editing to enhance LLM performance, offering a practical approach to mitigate the challenges of black-box LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#24213;&#23618;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#30340;&#25925;&#20107;&#24773;&#33410;&#24067;&#23616;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;&#25903;&#25345;&#25152;&#26399;&#26395;&#21465;&#20107;&#30340;&#28216;&#25103;&#22320;&#22270;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.15242</link><description>&lt;p&gt;
PlotMap&#65306;&#29992;&#20110;&#26500;&#24314;&#28216;&#25103;&#19990;&#30028;&#30340;&#33258;&#21160;&#24067;&#23616;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
PlotMap: Automated Layout Design for Building Game Worlds. (arXiv:2309.15242v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#24213;&#23618;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#30340;&#25925;&#20107;&#24773;&#33410;&#24067;&#23616;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;&#25903;&#25345;&#25152;&#26399;&#26395;&#21465;&#20107;&#30340;&#28216;&#25103;&#22320;&#22270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#19990;&#30028;&#26500;&#24314;&#26159;&#24320;&#21457;&#28216;&#25103;&#30340;&#21465;&#20107;&#21644;&#29289;&#29702;&#19990;&#30028;&#30340;&#36807;&#31243;&#65292;&#22312;&#28216;&#25103;&#20307;&#39564;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22791;&#21463;&#22909;&#35780;&#30340;&#29420;&#31435;&#28216;&#25103;&#21644;AAA&#32423;&#35270;&#39057;&#28216;&#25103;&#34987;&#36190;&#36175;&#20854;&#20248;&#31168;&#30340;&#19990;&#30028;&#26500;&#24314;&#65292;&#20854;&#20013;&#28216;&#25103;&#22320;&#22270;&#19982;&#21465;&#20107;&#32039;&#23494;&#34701;&#21512;&#24182;&#25552;&#21319;&#20102;&#28216;&#25103;&#20307;&#39564;&#65292;&#21560;&#24341;&#20102;&#29609;&#23478;&#24182;&#30041;&#19979;&#20102;&#28145;&#21051;&#30340;&#21360;&#35937;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#25903;&#25345;&#25152;&#26399;&#26395;&#21465;&#20107;&#30340;&#28216;&#25103;&#22320;&#22270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#28385;&#36275;&#21508;&#31181;&#32771;&#34385;&#22240;&#32032;&#30340;&#22797;&#26434;&#32422;&#26463;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#20391;&#37325;&#20110;&#28216;&#25103;&#26426;&#21046;&#25110;&#22320;&#22270;&#22320;&#24418;&#30340;&#32771;&#34385;&#65292;&#32780;&#24573;&#35270;&#20102;&#25903;&#25345;&#25925;&#20107;&#30340;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#35843;&#25972;&#25165;&#33021;&#35774;&#35745;&#20986;&#36866;&#24212;&#29305;&#23450;&#25925;&#20107;&#30340;&#28216;&#25103;&#19990;&#30028;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#28216;&#25103;&#19990;&#30028;&#26500;&#24314;&#27969;&#31243;&#20013;&#24341;&#20837;&#19968;&#20010;&#19982;&#24213;&#23618;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#26080;&#20851;&#30340;&#25925;&#20107;&#24773;&#33410;&#24067;&#23616;&#35774;&#35745;&#39069;&#22806;&#23618;&#38754;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
World-building, the process of developing both the narrative and physical world of a game, plays a vital role in the game's experience. Critically acclaimed independent and AAA video games are praised for strong world building, with game maps that masterfully intertwine with and elevate the narrative, captivating players and leaving a lasting impression. However, designing game maps that support a desired narrative is challenging, as it requires satisfying complex constraints from various considerations. Most existing map generation methods focus on considerations about gameplay mechanics or map topography, while the need to support the story is typically neglected. As a result, extensive manual adjustment is still required to design a game world that facilitates particular stories. In this work, we approach this problem by introducing an extra layer of plot facility layout design that is independent of the underlying map generation method in a world-building pipeline. Concretely, we p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10759</link><description>&lt;p&gt;
&#20026;&#22823;&#22411;&#22270;&#34920;&#31034;&#31616;&#21270;&#21644;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#34920;&#31034;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#20013;&#28041;&#21450;&#20102;&#22823;&#37327;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22522;&#26412;&#32534;&#30721;&#22120;&#31867;&#21035;&#65292;&#30001;&#20110;&#20854;&#20840;&#23616;&#27880;&#24847;&#21147;&#21487;&#20197;&#25429;&#25417;&#21040;&#37051;&#33410;&#28857;&#20043;&#22806;&#30340;&#25152;&#26377;&#23545;&#24433;&#21709;&#65292;&#22240;&#27492;&#22312;&#23567;&#22411;&#22270;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#32487;&#25215;&#20102;Transformer&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#28145;&#23618;&#22810;&#22836;&#27880;&#24847;&#21147;&#26469;&#37319;&#29992;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#20110;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#20063;&#33021;&#22312;&#33410;&#28857;&#25968;&#37327;&#20174;&#21315;&#32423;&#21040;&#21313;&#20159;&#32423;&#30340;&#33539;&#22260;&#20869;&#24102;&#26469;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#40723;&#21169;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20854;&#20013;&#20840;&#23616;&#27880;&#24847;&#21147;&#26159;&#19968;&#20010;&#38459;&#30861;&#21487;&#25193;&#23637;&#24615;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#26696;&#31216;&#20026;&#31616;&#21270;&#22270;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.00416</link><description>&lt;p&gt;
&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controllable Motion Diffusion Model. (arXiv:2306.00416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#21160;&#30011;&#20013;&#65292;&#20026;&#34394;&#25311;&#35282;&#33394;&#29983;&#25104;&#36924;&#30495;&#19988;&#21487;&#25511;&#30340;&#36816;&#21160;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20174;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#38480;&#20110;&#31163;&#32447;&#24212;&#29992;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21516;&#26102;&#29983;&#25104;&#25152;&#26377;&#27493;&#39588;&#30340;&#24207;&#21015;&#32423;&#29983;&#25104;&#12290;&#20026;&#20102;&#33021;&#22815;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20197;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#20026;&#22522;&#30784;&#65292;&#36880;&#27493;&#29983;&#25104;&#36816;&#21160;&#24207;&#21015;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#20351;&#29992;&#26631;&#20934;DDPM&#31639;&#27861;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#20135;&#29983;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#36816;&#21160;&#25511;&#21046;&#19979;&#38271;&#26102;&#38388;&#20869;&#30340;&#39640;&#20445;&#30495;&#24230;&#36816;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating realistic and controllable motions for virtual characters is a challenging task in computer animation, and its implications extend to games, simulations, and virtual reality. Recent studies have drawn inspiration from the success of diffusion models in image generation, demonstrating the potential for addressing this task. However, the majority of these studies have been limited to offline applications that target at sequence-level generation that generates all steps simultaneously. To enable real-time motion synthesis with diffusion models in response to time-varying control signals, we propose the framework of the Controllable Motion Diffusion Model (COMODO). Our framework begins with an auto-regressive motion diffusion model (A-MDM), which generates motion sequences step by step. In this way, simply using the standard DDPM algorithm without any additional complexity, our framework is able to generate high-fidelity motion sequences over extended periods with different type
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25361;&#25112;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#23398;&#20064;&#12289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#21644;&#19981;&#23545;&#40784;&#25968;&#25454;&#23398;&#20064;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01008</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Multimodal Learning: A Survey. (arXiv:2304.01008v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01008
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25361;&#25112;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#23398;&#20064;&#12289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#21644;&#19981;&#23545;&#40784;&#25968;&#25454;&#23398;&#20064;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#29702;&#35299;&#21644;&#20998;&#26512;&#26469;&#33258;&#22810;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#19979;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#20110;&#37197;&#23545;&#25968;&#25454;&#21644;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#27169;&#22411;&#30340;&#25193;&#23637;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#37492;&#20110;&#37326;&#22806;&#26377;&#22823;&#35268;&#27169;&#26410;&#27880;&#37322;&#30340;&#25968;&#25454;&#21487;&#29992;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#20026;&#32531;&#35299;&#27880;&#37322;&#29942;&#39048;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#31574;&#30053;&#12290;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;SSML&#65289;&#24314;&#31435;&#22312;&#36825;&#20004;&#20010;&#26041;&#21521;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20379;&#20102;&#20174;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;SSML&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#38416;&#36848;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#38754;&#20020;&#30340;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#65292;&#65288;2&#65289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19982;&#19981;&#23545;&#40784;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#36825;&#20123;&#25361;&#25112;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#65288;1&#65289;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning, which aims to understand and analyze information from multiple modalities, has achieved substantial progress in the supervised regime in recent years. However, the heavy dependence on data paired with expensive human annotations impedes scaling up models. Meanwhile, given the availability of large-scale unannotated data in the wild, self-supervised learning has become an attractive strategy to alleviate the annotation bottleneck. Building on these two directions, self-supervised multimodal learning (SSML) provides ways to learn from raw multimodal data. In this survey, we provide a comprehensive review of the state-of-the-art in SSML, in which we elucidate three major challenges intrinsic to self-supervised learning with multimodal data: (1) learning representations from multimodal data without labels, (2) fusion of different modalities, and (3) learning with unaligned data. We then detail existing solutions to these challenges. Specifically, we consider (1) object
&lt;/p&gt;</description></item></channel></rss>