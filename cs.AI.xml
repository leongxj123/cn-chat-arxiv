<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#23545;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#30340;&#20219;&#21153;&#33258;&#23450;&#20041;&#30340;&#19968;&#22871;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;YOLOv5&#26159;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#23637;&#29616;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.02877</link><description>&lt;p&gt;
FlightScope: &#21355;&#26143;&#22270;&#20687;&#20013;&#39134;&#34892;&#22120;&#26816;&#27979;&#31639;&#27861;&#30340;&#28145;&#24230;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#30340;&#20219;&#21153;&#33258;&#23450;&#20041;&#30340;&#19968;&#22871;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;YOLOv5&#26159;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#23637;&#29616;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02877v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#36965;&#24863;&#21355;&#26143;&#22270;&#20687;&#20013;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#29289;&#29702;&#23398;&#21644;&#29615;&#22659;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#19981;&#26029;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#22312;&#24120;&#35265;&#30340;&#22522;&#20110;&#22320;&#38754;&#25293;&#25668;&#30340;&#29031;&#29255;&#19978;&#23454;&#26045;&#21644;&#27979;&#35797;&#12290;&#26412;&#25991;&#23545;&#19968;&#22871;&#38024;&#23545;&#22312;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#36825;&#19968;&#20219;&#21153;&#23450;&#21046;&#30340;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#21033;&#29992;&#22823;&#22411;HRPlanesV2&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19982;GDIT&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#39564;&#35777;&#65292;&#35813;&#30740;&#31350;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;YOLO&#29256;&#26412;5&#21644;8&#12289;Faster RCNN&#12289;CenterNet&#12289;RetinaNet&#12289;RTMDet&#21644;DETR&#65292;&#22343;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#12290;&#36825;&#39033;&#20840;&#38754;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;YOLOv5&#20316;&#20026;&#35782;&#21035;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#39134;&#26426;&#36825;&#19968;&#29305;&#23450;&#26696;&#20363;&#30340;&#21331;&#36234;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#30340;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02877v1 Announce Type: cross  Abstract: Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch. This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions. This research
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-3.5&#30340;&#25220;&#34989;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;Faiss&#21644;BERT&#30340;&#39640;&#25928;&#39640;&#20934;&#30830;&#24615;&#30340;&#25220;&#34989;&#35782;&#21035;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#39640;&#27700;&#24179;&#25220;&#34989;&#26816;&#27979;&#30740;&#31350;&#25968;&#25454;&#38598;&#32570;&#22833;&#30340;&#31354;&#30333;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20248;&#24322;</title><link>https://arxiv.org/abs/2404.01582</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#22686;&#24378;&#30340;&#20316;&#19994;&#25220;&#34989;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-3.5&#30340;&#25220;&#34989;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;Faiss&#21644;BERT&#30340;&#39640;&#25928;&#39640;&#20934;&#30830;&#24615;&#30340;&#25220;&#34989;&#35782;&#21035;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#39640;&#27700;&#24179;&#25220;&#34989;&#26816;&#27979;&#30740;&#31350;&#25968;&#25454;&#38598;&#32570;&#22833;&#30340;&#31354;&#30333;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20248;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25220;&#34989;&#26816;&#27979;&#20219;&#21153;&#26159;&#19968;&#39033;&#24120;&#35265;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#32473;&#23450;&#25991;&#26412;&#26159;&#21542;&#21253;&#21547;&#20174;&#20854;&#20182;&#25991;&#26412;&#20013;&#25220;&#34989;&#25110;&#22797;&#21046;&#30340;&#20869;&#23481;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#26816;&#27979;&#39640;&#27700;&#24179;&#30340;&#25220;&#34989;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-3.5&#30340;&#25220;&#34989;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#20135;&#29983;&#20102;32,927&#23545;&#25991;&#26412;&#25220;&#34989;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#25220;&#34989;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Faiss&#21644;BERT&#30340;&#39640;&#25928;&#39640;&#20934;&#30830;&#24615;&#30340;&#25220;&#34989;&#35782;&#21035;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;98.86&#65285;&#12289;98.90&#65285;&#12289;98.86&#65285;&#21644;0.9888&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#28436;&#31034;&#24179;&#21488;&#65292;&#20801;&#35768;&#29992;&#25143;&#19978;&#20256;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01582v1 Announce Type: cross  Abstract: Text plagiarism detection task is a common natural language processing task that aims to detect whether a given text contains plagiarism or copying from other texts. In existing research, detection of high level plagiarism is still a challenge due to the lack of high quality datasets. In this paper, we propose a plagiarized text data generation method based on GPT-3.5, which produces 32,927 pairs of text plagiarism detection datasets covering a wide range of plagiarism methods, bridging the gap in this part of research. Meanwhile, we propose a plagiarism identification method based on Faiss with BERT with high efficiency and high accuracy. Our experiments show that the performance of this model outperforms other models in several metrics, including 98.86\%, 98.90%, 98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively. At the end, we also provide a user-friendly demo platform that allows users to upload a text 
&lt;/p&gt;</description></item><item><title>RLGNet&#25552;&#20986;&#20102;&#37325;&#22797;-&#23616;&#37096;-&#20840;&#23616;&#21382;&#21490;&#32593;&#32476;&#65292;&#21033;&#29992;&#20840;&#23616;&#21382;&#21490;&#32534;&#30721;&#22120;&#21644;&#23616;&#37096;&#21382;&#21490;&#32534;&#30721;&#22120;&#20998;&#21035;&#25429;&#25417;&#21382;&#21490;&#20449;&#24687;&#30340;&#25972;&#20307;&#21644;&#30456;&#20851;&#32454;&#33410;&#65292;&#20197;&#26377;&#25928;&#36827;&#34892;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2404.00586</link><description>&lt;p&gt;
RLGNet&#65306;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#30340;&#37325;&#22797;-&#23616;&#37096;-&#20840;&#23616;&#21382;&#21490;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RLGNet: Repeating-Local-Global History Network for Temporal Knowledge Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00586
&lt;/p&gt;
&lt;p&gt;
RLGNet&#25552;&#20986;&#20102;&#37325;&#22797;-&#23616;&#37096;-&#20840;&#23616;&#21382;&#21490;&#32593;&#32476;&#65292;&#21033;&#29992;&#20840;&#23616;&#21382;&#21490;&#32534;&#30721;&#22120;&#21644;&#23616;&#37096;&#21382;&#21490;&#32534;&#30721;&#22120;&#20998;&#21035;&#25429;&#25417;&#21382;&#21490;&#20449;&#24687;&#30340;&#25972;&#20307;&#21644;&#30456;&#20851;&#32454;&#33410;&#65292;&#20197;&#26377;&#25928;&#36827;&#34892;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00586v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#25512;&#29702;&#22522;&#20110;&#21382;&#21490;&#20449;&#24687;&#20197;&#39044;&#27979;&#26410;&#26469;&#12290;&#22240;&#27492;&#65292;&#35299;&#26512;&#21644;&#25366;&#25496;&#21382;&#21490;&#20449;&#24687;&#23545;&#39044;&#27979;&#26410;&#26469;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#21516;&#26102;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35282;&#24230;&#22788;&#29702;&#21644;&#29702;&#35299;&#21382;&#21490;&#20449;&#24687;&#12290;&#24573;&#35270;&#20840;&#23616;&#35270;&#22270;&#21487;&#33021;&#23548;&#33268;&#24573;&#30053;&#23439;&#35266;&#36235;&#21183;&#21644;&#27169;&#24335;&#65292;&#32780;&#24573;&#35270;&#23616;&#37096;&#35270;&#22270;&#21487;&#33021;&#23548;&#33268;&#36951;&#28431;&#20851;&#38190;&#35814;&#32454;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26041;&#27861;&#19981;&#20851;&#27880;&#20174;&#39640;&#39057;&#37325;&#22797;&#20107;&#20214;&#20013;&#23398;&#20064;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25484;&#25569;&#39057;&#32321;&#21457;&#29983;&#30340;&#21382;&#21490;&#20107;&#20214;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\textbf{R}epetitive-\textbf{L}ocal-\textbf{G}lobal History \textbf{Net}work&#65288;RLGNet&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#20840;&#23616;&#21382;&#21490;&#32534;&#30721;&#22120;&#25429;&#25417;&#21382;&#21490;&#20449;&#24687;&#30340;&#20840;&#38754;&#24615;&#36136;&#12290;&#38543;&#21518;&#65292;&#23616;&#37096;&#21382;&#21490;&#32534;&#30721;&#22120;&#25552;&#20379;&#19982;&#26597;&#35810;&#26102;&#38388;&#25139;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00586v1 Announce Type: new  Abstract: Temporal Knowledge Graph (TKG) reasoning is based on historical information to predict the future. Therefore, parsing and mining historical information is key to predicting the future. Most existing methods fail to concurrently address and comprehend historical information from both global and local perspectives. Neglecting the global view might result in overlooking macroscopic trends and patterns, while ignoring the local view can lead to missing critical detailed information. Additionally, some methods do not focus on learning from high-frequency repeating events, which means they may not fully grasp frequently occurring historical events. To this end, we propose the \textbf{R}epetitive-\textbf{L}ocal-\textbf{G}lobal History \textbf{Net}work(RLGNet). We utilize a global history encoder to capture the overarching nature of historical information. Subsequently, the local history encoder provides information related to the query timestam
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#26426;&#22120;&#27169;&#22411;&#65292;&#25903;&#25345;&#20102;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#36825;&#19968;&#35770;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.17101</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65306;&#19968;&#20010;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
AI Consciousness is Inevitable: A Theoretical Computer Science Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#26426;&#22120;&#27169;&#22411;&#65292;&#25903;&#25345;&#20102;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#36825;&#19968;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#26469;&#23457;&#35270;&#24847;&#35782;&#65292;&#36825;&#26159;&#25968;&#23398;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#30740;&#31350;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#30340;&#35745;&#31639;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#20026;&#24847;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#30340;&#26426;&#22120;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#21463;&#21040;&#20102;&#33406;&#20262;&#183;&#22270;&#28789;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#35745;&#31639;&#27169;&#22411;&#21644;&#20271;&#32435;&#24503;&#183;&#24052;&#23572;&#26031;&#24847;&#35782;&#21095;&#22330;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#23613;&#31649;&#38750;&#24120;&#31616;&#21333;&#65292;&#36825;&#20010;&#27169;&#22411;&#22312;&#39640;&#23618;&#27425;&#19978;&#19982;&#35768;&#22810;&#20851;&#20110;&#20154;&#31867;&#21644;&#21160;&#29289;&#24847;&#35782;&#30340;&#20027;&#35201;&#31185;&#23398;&#29702;&#35770;&#30456;&#19968;&#33268;&#65292;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#26029;&#65306;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17101v1 Announce Type: new  Abstract: We look at consciousness through the lens of Theoretical Computer Science, a branch of mathematics that studies computation under resource limitations. From this perspective, we develop a formal machine model for consciousness. The model is inspired by Alan Turing's simple yet powerful model of computation and Bernard Baars' theater model of consciousness. Though extremely simple, the model aligns at a high level with many of the major scientific theories of human and animal consciousness, supporting our claim that machine consciousness is inevitable.
&lt;/p&gt;</description></item><item><title>RadioGAT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#39057;&#24102;&#23556;&#39057;&#22320;&#22270;&#37325;&#24314;&#20013;&#30340;&#25361;&#25112;&#65292;&#21019;&#26032;&#22320;&#23558;&#27169;&#22411;&#24314;&#27169;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#28040;&#38500;&#20102;&#23545;&#22810;&#21306;&#22495;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.16397</link><description>&lt;p&gt;
RadioGAT&#65306;&#22522;&#20110;&#32852;&#21512;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#22810;&#39057;&#24102;&#26080;&#32447;&#23556;&#39057;&#22320;&#22270;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band Radiomap Reconstruction via Graph Attention Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16397
&lt;/p&gt;
&lt;p&gt;
RadioGAT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#39057;&#24102;&#23556;&#39057;&#22320;&#22270;&#37325;&#24314;&#20013;&#30340;&#25361;&#25112;&#65292;&#21019;&#26032;&#22320;&#23558;&#27169;&#22411;&#24314;&#27169;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#28040;&#38500;&#20102;&#23545;&#22810;&#21306;&#22495;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39057;&#24102;&#23556;&#39057;&#22320;&#22270;&#37325;&#24314;&#65288;MB-RMR&#65289;&#26159;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#29992;&#20110;&#20219;&#21153;&#22914;&#39057;&#35889;&#31649;&#29702;&#21644;&#32593;&#32476;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;MB-RMR&#26041;&#27861;&#22312;&#37096;&#32626;&#20013;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#27169;&#25311;&#25968;&#25454;&#25110;&#23436;&#25972;&#32467;&#26500;&#21270;&#22320;&#38754;&#23454;&#20917;&#65292;&#36825;&#20123;&#25361;&#25112;&#28304;&#33258;&#27169;&#25311;&#25968;&#25454;&#19982;&#23454;&#38469;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#29616;&#23454;&#19990;&#30028;&#27979;&#37327;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;RadioGAT&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;MB-RMR&#65292;&#22312;&#21333;&#20010;&#21306;&#22495;&#20869;&#28040;&#38500;&#20102;&#23545;&#22810;&#21306;&#22495;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;RadioGAT&#21019;&#26032;&#22320;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#31354;&#38388;-&#39057;&#35889;&#30456;&#20851;&#32534;&#30721;&#19982;&#25968;&#25454;&#39537;&#21160;&#30340;&#23556;&#39057;&#22320;&#22270;&#27867;&#21270;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#23545;&#22823;&#37327;&#25968;&#25454;&#28304;&#30340;&#20381;&#36182;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21019;&#26032;&#30340;&#32534;&#30721;&#23558;&#31232;&#30095;&#30340;&#22810;&#39057;&#24102;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#24320;&#22987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16397v1 Announce Type: cross  Abstract: Multi-band radiomap reconstruction (MB-RMR) is a key component in wireless communications for tasks such as spectrum management and network planning. However, traditional machine-learning-based MB-RMR methods, which rely heavily on simulated data or complete structured ground truth, face significant deployment challenges. These challenges stem from the differences between simulated and actual data, as well as the scarcity of real-world measurements. To address these challenges, our study presents RadioGAT, a novel framework based on Graph Attention Network (GAT) tailored for MB-RMR within a single area, eliminating the need for multi-region datasets. RadioGAT innovatively merges model-based spatial-spectral correlation encoding with data-driven radiomap generalization, thus minimizing the reliance on extensive data sources. The framework begins by transforming sparse multi-band data into a graph structure through an innovative encoding
&lt;/p&gt;</description></item><item><title>LLMs&#24050;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#20840;&#38754;&#23637;&#31034;&#20102;LLMs&#22312;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20854;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#25913;&#36827;&#65292;&#25581;&#31034;&#20102;&#20027;&#35201;&#21457;&#23637;&#36235;&#21183;&#21644;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#35752;&#35770;&#20102;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16303</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Biomedical and Health Informatics: A Bibliometric Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16303
&lt;/p&gt;
&lt;p&gt;
LLMs&#24050;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#20840;&#38754;&#23637;&#31034;&#20102;LLMs&#22312;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20854;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#25913;&#36827;&#65292;&#25581;&#31034;&#20102;&#20027;&#35201;&#21457;&#23637;&#36235;&#21183;&#21644;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#35752;&#35770;&#20102;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36805;&#36895;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#65288;BHI&#65289;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20026;&#20998;&#26512;&#25968;&#25454;&#12289;&#27835;&#30103;&#24739;&#32773;&#21644;&#24320;&#23637;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#33258;2022&#24180;&#33267;2023&#24180;&#30340;&#30740;&#31350;&#25991;&#31456;&#21644;&#21512;&#20316;&#32593;&#32476;&#65292;&#20840;&#38754;&#23637;&#31034;LLMs&#22312;BHI&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;&#23427;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#22914;&#20309;&#21487;&#20197;&#25913;&#36827;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#65292;&#22914;&#21307;&#23398;&#35786;&#26029;&#12289;&#24739;&#32773;&#21442;&#19982;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#21307;&#23398;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#30830;&#23450;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#32472;&#21046;&#20102;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#31361;&#20986;&#20102;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#20027;&#35201;&#36827;&#23637;&#12290;&#26368;&#21518;&#65292;&#23427;&#35752;&#35770;&#20102;&#22312;BHI&#20013;&#20351;&#29992;LLMs&#30340;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#38544;&#31169;&#21644;&#21487;&#38752;&#30340;&#21307;&#30103;&#24314;&#35758;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;LLMs&#22914;&#20309;&#36827;&#19968;&#27493;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16303v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This bibliometric review aims to provide a panoramic view of how LLMs have been used in BHI by examining research articles and collaboration networks from 2022 to 2023. It further explores how LLMs can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine. To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field. Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations. Looking ahead, we consider how LLMs could further transform biomedical research as we
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Nested Neural Feature Fields (N2F2) &#23454;&#29616;&#20102;&#23618;&#27425;&#21270;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#29289;&#29702;&#32500;&#24230;&#25110;&#35821;&#20041;&#32500;&#24230;&#31561;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.10997</link><description>&lt;p&gt;
&#23884;&#22871;&#31070;&#32463;&#29305;&#24449;&#22330;&#30340;&#23618;&#27425;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10997
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Nested Neural Feature Fields (N2F2) &#23454;&#29616;&#20102;&#23618;&#27425;&#21270;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#29289;&#29702;&#32500;&#24230;&#25110;&#35821;&#20041;&#32500;&#24230;&#31561;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#29702;&#35299;&#22810;&#23618;&#25277;&#35937;&#30340;&#22797;&#26434;&#22330;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23884;&#22871;&#31070;&#32463;&#29305;&#24449;&#22330; (N2F2)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#30417;&#30563;&#26469;&#23398;&#20064;&#21333;&#20010;&#29305;&#24449;&#22330;&#65292;&#22312;&#21516;&#19968;&#39640;&#32500;&#29305;&#24449;&#20013;&#30340;&#19981;&#21516;&#32500;&#24230;&#32534;&#30721;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#28789;&#27963;&#23450;&#20041;&#23618;&#27425;&#65292;&#21487;&#20197;&#26681;&#25454;&#29289;&#29702;&#32500;&#24230;&#12289;&#35821;&#20041;&#32500;&#24230;&#25110;&#20004;&#32773;&#22343;&#21305;&#37197;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22330;&#26223;&#30340;&#20840;&#38754;&#21644;&#32454;&#33268;&#29702;&#35299;&#12290;&#25105;&#20204;&#21033;&#29992;2D&#31867;&#21035;&#26080;&#20851;&#20998;&#21106;&#27169;&#22411;&#22312;&#22270;&#20687;&#31354;&#38388;&#30340;&#20219;&#24847;&#23610;&#24230;&#25552;&#20379;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#20687;&#32032;&#20998;&#32452;&#65292;&#24182;&#26597;&#35810;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#20026;&#36825;&#20123;&#27573;&#33853;&#20013;&#30340;&#27599;&#20010;&#37096;&#20998;&#33719;&#24471;&#19982;&#35821;&#35328;&#23545;&#40784;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#23618;&#30417;&#30563;&#26041;&#27861;&#23558;&#19981;&#21516;&#30340;&#23884;&#22871;&#29305;&#24449;&#22330;&#32500;&#24230;&#20998;&#37197;&#32473;&#25552;&#21462;C
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10997v1 Announce Type: cross  Abstract: Understanding complex scenes at multiple levels of abstraction remains a formidable challenge in computer vision. To address this, we introduce Nested Neural Feature Fields (N2F2), a novel approach that employs hierarchical supervision to learn a single feature field, wherein different dimensions within the same high-dimensional feature encode scene properties at varying granularities. Our method allows for a flexible definition of hierarchies, tailored to either the physical dimensions or semantics or both, thereby enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D class-agnostic segmentation model to provide semantically meaningful pixel groupings at arbitrary scales in the image space, and query the CLIP vision-encoder to obtain language-aligned embeddings for each of these segments. Our proposed hierarchical supervision method then assigns different nested dimensions of the feature field to distill the C
&lt;/p&gt;</description></item><item><title>&#36825;&#19968;&#21019;&#26032;&#24515;&#29702;&#27835;&#30103;&#27169;&#22411;HealMe&#36890;&#36807;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24605;&#32500;&#65292;&#24182;&#20419;&#36827;&#20102;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.05574</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#27835;&#30103;&#20013;&#36827;&#34892;&#35748;&#30693;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05574
&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#21019;&#26032;&#24515;&#29702;&#27835;&#30103;&#27169;&#22411;HealMe&#36890;&#36807;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24605;&#32500;&#65292;&#24182;&#20419;&#36827;&#20102;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#27835;&#30103;&#20013;&#21487;&#20197;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#29087;&#32451;&#22788;&#29702;&#35748;&#30693;&#37325;&#26500;&#31561;&#20851;&#38190;&#20219;&#21153;&#65292;&#20811;&#26381;&#32670;&#32827;&#12289;&#19981;&#20449;&#20219;&#12289;&#27835;&#30103;&#24072;&#25216;&#33021;&#24046;&#24322;&#21644;&#36164;&#28304;&#31232;&#32570;&#31561;&#25361;&#25112;&#12290;&#22312;&#20808;&#21069;&#30340;&#35748;&#30693;&#37325;&#26500;&#20013;&#65292;&#20027;&#35201;&#23558;&#36127;&#38754;&#24773;&#32490;&#36716;&#21270;&#20026;&#31215;&#26497;&#30340;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#25928;&#26524;&#26377;&#38480;&#65292;&#32463;&#24120;&#19981;&#33021;&#20419;&#36827;&#23458;&#25143;&#33258;&#25105;&#21457;&#29616;&#26367;&#20195;&#35270;&#35282;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24110;&#21161;&#21644;&#36171;&#33021;&#36890;&#36807;&#33258;&#36866;&#24212;&#35821;&#35328;&#22312;&#24515;&#29702;&#22686;&#24378;&#65288;HealMe&#65289;&#27169;&#22411;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#35748;&#30693;&#37325;&#26500;&#30103;&#27861;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24819;&#27861;&#65292;&#24182;&#20419;&#36827;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35270;&#35282;&#12290;HealMe&#19982;&#20256;&#32479;LLM&#26041;&#27861;&#19981;&#21516;&#65292;&#37319;&#29992;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#12290;&#23427;&#36890;&#36807;&#31995;&#32479;&#25351;&#23548;&#23458;&#25143;&#21306;&#20998;&#24773;&#22659;&#21644;&#24863;&#21463;&#65292;&#38598;&#24605;&#24191;&#30410;&#23547;&#25214;&#26367;&#20195;&#35270;&#35282;&#65292;&#24182;&#21046;&#23450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05574v1 Announce Type: cross  Abstract: Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing 
&lt;/p&gt;</description></item><item><title>RAM-EHR&#36890;&#36807;&#22686;&#24378;&#26816;&#32034;&#24182;&#21033;&#29992;&#24635;&#32467;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20020;&#24202;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00815</link><description>&lt;p&gt;
RAM-EHR: &#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19978;&#30340;&#26816;&#32034;&#22686;&#24378;&#19982;&#20020;&#24202;&#39044;&#27979;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00815
&lt;/p&gt;
&lt;p&gt;
RAM-EHR&#36890;&#36807;&#22686;&#24378;&#26816;&#32034;&#24182;&#21033;&#29992;&#24635;&#32467;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20020;&#24202;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RAM-EHR&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#19978;&#20020;&#24202;&#39044;&#27979;&#30340;&#26816;&#32034;&#22686;&#24378;&#65288;Retrieval Augmentation&#65289;&#27969;&#31243;&#12290;RAM-EHR&#39318;&#20808;&#25910;&#38598;&#22810;&#20010;&#30693;&#35782;&#26469;&#28304;&#65292;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#25991;&#26412;&#26684;&#24335;&#65292;&#24182;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#26469;&#33719;&#21462;&#19982;&#21307;&#23398;&#27010;&#24565;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#36825;&#19968;&#31574;&#30053;&#35299;&#20915;&#20102;&#19982;&#22797;&#26434;&#27010;&#24565;&#21517;&#31216;&#30456;&#20851;&#30340;&#22256;&#38590;&#12290;RAM-EHR&#28982;&#21518;&#22686;&#24191;&#20102;&#19982;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#20195;&#30721;&#32852;&#21512;&#35757;&#32451;&#30340;&#26412;&#22320;EHR&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#26469;&#33258;&#24739;&#32773;&#23601;&#35786;&#21644;&#24635;&#32467;&#30693;&#35782;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#22312;&#20004;&#20010;EHR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RAM-EHR&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#30693;&#35782;&#22686;&#24378;&#22522;&#32447;&#25928;&#26524;&#26174;&#33879;&#65288;AUROC&#22686;&#30410;3.4&#65285;&#65292;AUPR&#22686;&#30410;7.2&#65285;&#65289;&#65292;&#24378;&#35843;&#20102;RAM-EHR&#30340;&#24635;&#32467;&#30693;&#35782;&#23545;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#23558;&#21457;&#24067;&#22312;\url{https://github.com/ritaranx/RAM-EHR}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00815v1 Announce Type: cross  Abstract: We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks. The code will be published at \url{https://github.com/ritaranx/RAM-EHR}.
&lt;/p&gt;</description></item><item><title>Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#20986;&#20102;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#35757;&#32451;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14873</link><description>&lt;p&gt;
Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Technical Report on the Checkfor.ai AI-Generated Text Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14873
&lt;/p&gt;
&lt;p&gt;
Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#20986;&#20102;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#35757;&#32451;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Checkfor.ai&#25991;&#26412;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#21306;&#20998;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#30340;&#25991;&#26412;&#21644;&#30001;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#12290;Checkfor.ai&#22312;&#30001;&#21313;&#31181;&#25991;&#26412;&#39046;&#22495;&#65288;&#23398;&#29983;&#20889;&#20316;&#12289;&#21019;&#24847;&#20889;&#20316;&#12289;&#31185;&#23398;&#20889;&#20316;&#12289;&#20070;&#31821;&#12289;&#30334;&#31185;&#20840;&#20070;&#12289;&#26032;&#38395;&#12289;&#30005;&#23376;&#37038;&#20214;&#12289;&#31185;&#23398;&#35770;&#25991;&#12289;&#31616;&#31572;&#38382;&#31572;&#65289;&#21644;8&#20010;&#24320;&#28304;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#34920;&#29616;&#20248;&#20110;&#38646;&#20914;&#20987;&#26041;&#27861;&#22914;DetectGPT&#20197;&#21450;&#20027;&#27969;&#21830;&#19994;AI&#26816;&#27979;&#24037;&#20855;&#65292;&#35823;&#24046;&#29575;&#38477;&#20302;&#20102;9&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31639;&#27861;&#65292;&#21363;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#65292;&#20351;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#35780;&#35770;&#31561;&#39640;&#25968;&#25454;&#39046;&#22495;&#23454;&#29616;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26356;&#20302;&#35823;&#25253;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Checkfor.ai&#19981;&#23545;&#38750;&#27597;&#35821;&#33521;&#35821;&#20154;&#22763;&#20135;&#29983;&#20559;&#35265;&#65292;&#24182;&#25512;&#24191;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#39046;&#22495;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14873v1 Announce Type: cross  Abstract: We present the Checkfor.ai text classifier, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Checkfor.ai outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 9 times lower error rates on a comprehensive benchmark comprised of ten text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q\&amp;A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Checkfor.ai is not biased against nonnative English speakers and generalizes to domains and models unseen during training.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#35774;&#35745;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.14236</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;&#30340;&#33258;&#21160;&#35774;&#35745;&#19982;&#20248;&#21270;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14236
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#35774;&#35745;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;(DFC)&#22797;&#26434;&#19988;&#32791;&#26102;&#65292;&#30005;&#36335;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#30005;&#23376;&#24037;&#31243;&#24072;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35774;&#35745;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#25913;&#36827;DFC&#30340;&#35774;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#24037;&#31243;&#24072;&#35774;&#35745;&#32463;&#39564;&#30340;&#20381;&#36182;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;&#30005;&#36335;&#35774;&#35745;&#30456;&#20851;&#30340;&#20027;&#35266;&#24615;&#21644;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19982;&#20256;&#32479;&#30340;&#24037;&#31243;&#24072;&#39537;&#21160;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35774;&#35745;&#25928;&#29575;&#21644;&#36136;&#37327;&#19978;&#37117;&#26377;&#26126;&#26174;&#25913;&#21892;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#35774;&#35745;&#22797;&#26434;&#25110;&#24555;&#36895;&#21457;&#23637;&#30340;DFC&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14236v1 Announce Type: cross  Abstract: Designing distributed filtering circuits (DFCs) is complex and time-consuming, with the circuit performance relying heavily on the expertise and experience of electronics engineers. However, manual design methods tend to have exceedingly low-efficiency. This study proposes a novel end-to-end automated method for fabricating circuits to improve the design of DFCs. The proposed method harnesses reinforcement learning (RL) algorithms, eliminating the dependence on the design experience of engineers. Thus, it significantly reduces the subjectivity and constraints associated with circuit design. The experimental findings demonstrate clear improvements in both design efficiency and quality when comparing the proposed method with traditional engineer-driven methods. In particular, the proposed method achieves superior performance when designing complex or rapidly evolving DFCs. Furthermore, compared to existing circuit automation design techn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;IT&#31995;&#32479;&#30340;&#26032;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#19968;&#31181;&#24555;&#36895;&#26816;&#27979;&#24322;&#24120;&#26681;&#26412;&#21407;&#22240;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#22312;&#22312;&#32447;&#25968;&#25454;&#20013;&#36935;&#21040;&#26032;&#24322;&#24120;&#26102;&#36827;&#34892;&#23376;&#22270;&#36941;&#21382;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06500</link><description>&lt;p&gt;
&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#23454;&#26102;&#26816;&#27979;&#26681;&#26412;&#21407;&#22240;&#65292;&#24212;&#29992;&#20110;IT&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
On the Fly Detection of Root Causes from Observed Data with Application to IT Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;IT&#31995;&#32479;&#30340;&#26032;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#19968;&#31181;&#24555;&#36895;&#26816;&#27979;&#24322;&#24120;&#26681;&#26412;&#21407;&#22240;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#22312;&#22312;&#32447;&#25968;&#25454;&#20013;&#36935;&#21040;&#26032;&#24322;&#24120;&#26102;&#36827;&#34892;&#23376;&#22270;&#36941;&#21382;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#38408;&#20540;&#30340;IT&#31995;&#32479;&#30340;&#26032;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#26816;&#27979;&#27492;&#31867;&#31995;&#32479;&#20013;&#24322;&#24120;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#24403;&#26681;&#26412;&#21407;&#22240;&#27809;&#26377;&#22240;&#26524;&#20851;&#32852;&#26102;&#65292;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#27491;&#30830;&#30340;&#65307;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#24178;&#39044;&#26469;&#25918;&#26494;&#36825;&#31181;&#20551;&#35774;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21450;&#20854;&#22522;&#20110;&#20195;&#29702;&#30340;&#25193;&#23637;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#22312;&#36935;&#21040;&#22312;&#32447;&#25968;&#25454;&#20013;&#30340;&#26032;&#24322;&#24120;&#26102;&#36827;&#34892;&#23376;&#22270;&#36941;&#21382;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#26469;&#33258;&#26367;&#20195;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#25110;&#30495;&#23454;IT&#30417;&#25511;&#25968;&#25454;&#29983;&#25104;&#30340;&#25968;&#25454;&#26102;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new structural causal model tailored for representing threshold-based IT systems and presents a new algorithm designed to rapidly detect root causes of anomalies in such systems. When root causes are not causally related, the method is proven to be correct; while an extension is proposed based on the intervention of an agent to relax this assumption. Our algorithm and its agent-based extension leverage causal discovery from offline data and engage in subgraph traversal when encountering new anomalies in online data. Our extensive experiments demonstrate the superior performance of our methods, even when applied to data generated from alternative structural causal models or real IT monitoring data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;Large Language Model (LLM)&#20026;&#22522;&#30784;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#65292;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;LLM&#31649;&#36947;&#65292;&#21457;&#29616;GPT-2 + BART&#31649;&#36947;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#12290;&#36825;&#31181;&#30740;&#31350;&#26377;&#21161;&#20110;&#25913;&#21892;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#31038;&#20132;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00260</link><description>&lt;p&gt;
&#20197;LLM&#20026;&#22522;&#30784;&#23454;&#29616;&#38754;&#21521;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#30340;&#21487;&#25193;&#23637;&#26426;&#22120;&#20154;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;Large Language Model (LLM)&#20026;&#22522;&#30784;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#65292;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;LLM&#31649;&#36947;&#65292;&#21457;&#29616;GPT-2 + BART&#31649;&#36947;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#12290;&#36825;&#31181;&#30740;&#31350;&#26377;&#21161;&#20110;&#25913;&#21892;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#31038;&#20132;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;&#36825;&#31181;&#20132;&#27969;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Large Language Model (LLM)&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#31038;&#20132;&#26426;&#22120;&#20154;NAO&#25198;&#28436;&#20102;&#19968;&#20010;&#21050;&#28608;&#22120;(&#21475;&#22836;&#25551;&#36848;&#31038;&#20132;&#24773;&#26223;&#24182;&#25552;&#38382;)&#12289;&#25552;&#31034;&#22120;(&#25552;&#20379;&#19977;&#20010;&#36873;&#25321;&#39033;&#20379;&#36873;&#25321;)&#21644;&#22870;&#21169;&#22120;(&#24403;&#31572;&#26696;&#27491;&#30830;&#26102;&#32473;&#20104;&#31216;&#36190;)&#30340;&#35282;&#33394;&#12290;&#23545;&#20110;&#21050;&#28608;&#22120;&#30340;&#35282;&#33394;&#65292;&#31038;&#20132;&#24773;&#22659;&#12289;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#26159;&#20351;&#29992;&#25105;&#20204;&#30340;LLM&#31649;&#36947;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;GPT-2 + BART&#21644;GPT-2 + GPT-2&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;GPT-2&#22312;&#31649;&#36947;&#20013;&#26159;&#29992;&#20110;&#26080;&#30417;&#30563;&#31038;&#20132;&#24773;&#22659;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;SOCIALIQA&#25968;&#25454;&#38598;&#23545;&#25152;&#26377;LLM&#31649;&#36947;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-2 + BART&#31649;&#36947;&#22312;&#36890;&#36807;&#32467;&#21512;&#21508;&#33258;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;BERTscore&#12290;&#36825;&#31181;&#35266;&#23519;&#32467;&#26524;&#20063;&#19982;&#20799;&#31461;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27700;&#24179;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a social robot capable of verbally interacting with children with Autism Spectrum Disorder (ASD). This communication is meant to teach perspective-taking using text generated using a Large Language Model (LLM) pipeline. The social robot NAO acts as a stimulator (verbally describes a social situation and asks a question), prompter (presents three options to choose from), and reinforcer (praises when the answer is correct). For the role of the stimulator, the social situation, questions, and options are generated using our LLM pipeline. We compare two approaches: GPT-2 + BART and GPT-2 + GPT-2, where the first GPT-2 common between the pipelines is used for unsupervised social situation generation. We use the SOCIALIQA dataset to fine-tune all of our LLM pipelines. We found that the GPT-2 + BART pipeline had a better BERTscore for generating the questions and the options by combining their individual loss functions. This observation was also consistent with the h
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.08977</link><description>&lt;p&gt;
&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Weighted Ensemble Models Are Strong Continual Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08977
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#24471;&#20197;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#22312;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#25968;&#25454;&#26102;&#19981;&#21487;&#29992;&#12290;CL&#26412;&#36136;&#19978;&#26159;&#22312;&#33021;&#22815;&#23398;&#20064;&#26032;&#20219;&#21153;&#65288;&#21363;&#21487;&#22609;&#24615;&#65289;&#21644;&#20445;&#25345;&#20808;&#21069;&#23398;&#20064;&#27010;&#24565;&#30340;&#24615;&#33021;&#65288;&#21363;&#31283;&#23450;&#24615;&#65289;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;&#20808;&#21069;&#21644;&#24403;&#21069;&#20219;&#21153;&#30340;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#38598;&#25104;&#12290;&#36825;&#31181;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25345;&#32493;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoMA&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#22609;&#24615;&#22312;&#24403;&#21069;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#20559;&#31163;&#22826;&#36828;&#30340;&#20808;&#21069;&#26435;&#37325;&#37197;&#32622;&#65292;&#20174;&#32780;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CoMA&#30340;&#25913;&#36827;&#22411;&#21464;&#20307;&#65292;&#21517;&#20026;&#25345;&#32493;&#36153;&#33293;&#23572;&#21152;&#26435;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoFiMA&#65289;&#65292;&#35813;&#27169;&#22411;&#23545;&#27599;&#19968;&#20010;&#21442;&#25968;&#36827;&#34892;&#36873;&#25321;&#24615;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08977v2 Announce Type: replace-cross  Abstract: In this work, we study the problem of continual learning (CL) where the goal is to learn a model on a sequence of tasks, such that the data from the previous tasks becomes unavailable while learning on the current task data. CL is essentially a balancing act between being able to learn on the new task (i.e., plasticity) and maintaining the performance on the previously learned concepts (i.e., stability). Intending to address the stability-plasticity trade-off, we propose to perform weight-ensembling of the model parameters of the previous and current tasks. This weighted-ensembled model, which we call Continual Model Averaging (or CoMA), attains high accuracy on the current task by leveraging plasticity, while not deviating too far from the previous weight configuration, ensuring stability. We also propose an improved variant of CoMA, named Continual Fisher-weighted Model Averaging (or CoFiMA), that selectively weighs each para
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2312.00326</link><description>&lt;p&gt;
Agent-OM&#65306;&#21033;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Agent-OM: Leveraging LLM Agents for Ontology Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#65288;OM&#65289;&#33021;&#22815;&#23454;&#29616;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#65292;&#36890;&#36807;&#23545;&#40784;&#30456;&#20851;&#23454;&#20307;&#26469;&#35299;&#20915;&#20854;&#27010;&#24565;&#24322;&#26500;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;LLM&#35774;&#35745;&#33539;&#24335;&#65292;&#21629;&#21517;&#20026;Agent-OM&#65292;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#26816;&#32034;&#21644;&#21305;&#37197;&#30340;&#21516;&#20307;&#20195;&#29702;&#20197;&#21450;&#19968;&#32452;&#22522;&#20110;&#25552;&#31034;&#30340;&#31616;&#21333;OM&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00326v2 Announce Type: replace  Abstract: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese agents for retrieval and matching, with a set of simple prompt-based OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAE
&lt;/p&gt;</description></item><item><title>RGI-Net&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#39640;&#38454;&#21453;&#23556;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#20256;&#32479;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#25151;&#38388;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2309.01513</link><description>&lt;p&gt;
RGI-Net&#65306;&#22312;&#27809;&#26377;&#19968;&#38454;&#22238;&#22768;&#30340;&#24773;&#20917;&#19979;&#20174;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#25512;&#26029;3D&#25151;&#38388;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01513
&lt;/p&gt;
&lt;p&gt;
RGI-Net&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#39640;&#38454;&#21453;&#23556;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#20256;&#32479;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#25151;&#38388;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25151;&#38388;&#20960;&#20309;&#26159;&#23454;&#29616;&#36924;&#30495;&#30340;3D&#38899;&#39057;&#28210;&#26579;&#30340;&#37325;&#35201;&#20808;&#39564;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#21040;&#36798;&#26102;&#38388;&#65288;TOA&#65289;&#25110;&#21040;&#36798;&#26102;&#38388;&#24046;&#65288;TDOA&#65289;&#20449;&#24687;&#21457;&#23637;&#20102;&#21508;&#31181;&#25151;&#38388;&#20960;&#20309;&#25512;&#26029;&#65288;RGI&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;RGI&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#20123;&#20551;&#35774;&#65292;&#22914;&#20984;&#25151;&#38388;&#24418;&#29366;&#12289;&#24050;&#30693;&#22681;&#22721;&#25968;&#37327;&#21644;&#19968;&#38454;&#21453;&#23556;&#30340;&#21487;&#35265;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;RGI-Net&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#19978;&#36848;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#25151;&#38388;&#20960;&#20309;&#12290;RGI-Net&#23398;&#20064;&#24182;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#65288;RIRs&#65289;&#20013;&#30340;&#39640;&#38454;&#21453;&#23556;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#24418;&#29366;&#20026;&#38750;&#20984;&#24418;&#25110;RIRs&#20013;&#32570;&#23569;&#19968;&#38454;&#21453;&#23556;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#25151;&#38388;&#24418;&#29366;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#20174;&#35013;&#26377;&#22278;&#24418;&#40614;&#20811;&#39118;&#30340;&#32039;&#20945;&#38899;&#39057;&#35774;&#22791;&#27979;&#37327;&#30340;RIRs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01513v2 Announce Type: replace-cross  Abstract: Room geometry is important prior information for implementing realistic 3D audio rendering. For this reason, various room geometry inference (RGI) methods have been developed by utilizing the time of arrival (TOA) or time difference of arrival (TDOA) information in room impulse responses. However, the conventional RGI technique poses several assumptions, such as convex room shapes, the number of walls known in priori, and the visibility of first-order reflections. In this work, we introduce the deep neural network (DNN), RGI-Net, which can estimate room geometries without the aforementioned assumptions. RGI-Net learns and exploits complex relationships between high-order reflections in room impulse responses (RIRs) and, thus, can estimate room shapes even when the shape is non-convex or first-order reflections are missing in the RIRs. The network takes RIRs measured from a compact audio device equipped with a circular microphon
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#21644;&#22343;&#22330;&#21338;&#24328;&#30340;&#32467;&#21512;&#26377;&#26395;&#22312;&#24456;&#22823;&#35268;&#27169;&#19978;&#35299;&#20915;&#28216;&#25103;&#30340;&#22343;&#34913;&#21644;&#31038;&#20250;&#26368;&#20248;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2205.12944</link><description>&lt;p&gt;
&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Learning in Mean Field Games: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.12944
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#22343;&#22330;&#21338;&#24328;&#30340;&#32467;&#21512;&#26377;&#26395;&#22312;&#24456;&#22823;&#35268;&#27169;&#19978;&#35299;&#20915;&#28216;&#25103;&#30340;&#22343;&#34913;&#21644;&#31038;&#20250;&#26368;&#20248;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21512;&#20316;&#21644;&#21512;&#20316;&#28216;&#25103;&#22312;&#25317;&#26377;&#22823;&#37327;&#29609;&#23478;&#26102;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20294;&#38543;&#30528;&#29609;&#23478;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36890;&#24120;&#21464;&#24471;&#38590;&#20197;&#35299;&#20915;&#12290;&#22343;&#22330;&#21338;&#24328;(Mean Field Games, MFGs)&#30001;Lasry&#21644;Lions&#20197;&#21450;Huang&#65292;Caines&#21644;Malham\'e&#24341;&#20837;&#65292;&#20381;&#38752;&#22343;&#22330;&#36817;&#20284;&#20801;&#35768;&#29609;&#23478;&#25968;&#37327;&#22686;&#38271;&#21040;&#26080;&#31351;&#22823;&#12290;&#20256;&#32479;&#35299;&#20915;&#36825;&#20123;&#28216;&#25103;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#35299;&#20915;&#24102;&#26377;&#23545;&#27169;&#22411;&#30340;&#23436;&#20840;&#20102;&#35299;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning, RL)&#20986;&#29616;&#22312;&#35299;&#20915;&#35268;&#27169;&#22797;&#26434;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;RL&#21644;MFGs&#30340;&#32467;&#21512;&#26377;&#26395;&#35299;&#20915;&#22312;&#20154;&#21475;&#35268;&#27169;&#21644;&#29615;&#22659;&#22797;&#26434;&#24615;&#26041;&#38754;&#38750;&#24120;&#24222;&#22823;&#30340;&#28216;&#25103;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#36817;&#36805;&#36895;&#22686;&#38271;&#30340;&#20851;&#20110;RL&#26041;&#27861;&#22312;MFGs&#20013;&#23398;&#20064;&#22343;&#34913;&#21644;&#31038;&#20132;&#26368;&#20248;&#30340;&#25991;&#29486;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;M&#20013;&#26368;&#24120;&#35265;&#30340;&#35774;&#32622;(&#38745;&#24577;&#12289;&#31283;&#24577;&#21644;&#36827;&#21270;&#30340;)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.12944v3 Announce Type: replace-cross  Abstract: Non-cooperative and cooperative games with a very large number of players have many applications but remain generally intractable when the number of players increases. Introduced by Lasry and Lions, and Huang, Caines and Malham\'e, Mean Field Games (MFGs) rely on a mean-field approximation to allow the number of players to grow to infinity. Traditional methods for solving these games generally rely on solving partial or stochastic differential equations with a full knowledge of the model. Recently, Reinforcement Learning (RL) has appeared promising to solve complex problems at scale. The combination of RL and MFGs is promising to solve games at a very large scale both in terms of population size and environment complexity. In this survey, we review the quickly growing recent literature on RL methods to learn equilibria and social optima in MFGs. We first identify the most common settings (static, stationary, and evolutive) of M
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;trie&#30340;&#20869;&#23384;&#39640;&#25928;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#65292;&#22312;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#26377;&#26174;&#33879;&#25913;&#21892;&#65292;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;256GB&#31995;&#32479;&#20869;&#23384;&#19979;&#22823;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2202.06834</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;Tries&#30340;&#20869;&#23384;&#39640;&#25928;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient Sequential Pattern Mining with Hybrid Tries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.06834
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;trie&#30340;&#20869;&#23384;&#39640;&#25928;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#65292;&#22312;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#26377;&#26174;&#33879;&#25913;&#21892;&#65292;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;256GB&#31995;&#32479;&#20869;&#23384;&#19979;&#22823;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#25968;&#25454;&#38598;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#23545;&#20110;&#33021;&#22815;&#22788;&#29702;&#22914;&#27492;&#24222;&#22823;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#25366;&#25496;&#31639;&#27861;&#30340;&#38656;&#27714;&#21464;&#24471;&#26085;&#30410;&#36843;&#20999;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#26041;&#27861;&#29992;&#20110;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#65288;SPM&#65289;&#65292;&#36825;&#26159;&#30693;&#35782;&#21457;&#29616;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20027;&#39064;&#65292;&#38754;&#20020;&#30528;&#38024;&#23545;&#22823;&#25968;&#25454;&#38598;&#30340;&#24050;&#30693;&#20869;&#23384;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;trie&#25968;&#25454;&#32467;&#26500;&#65292;&#21033;&#29992;&#37325;&#22797;&#27169;&#24335;&#32039;&#20945;&#22320;&#23384;&#20648;&#20869;&#23384;&#20013;&#30340;&#25968;&#25454;&#38598;; &#20197;&#21450;&#19968;&#20010;&#30456;&#24212;&#30340;&#25366;&#25496;&#31639;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#20174;&#27492;&#32039;&#20945;&#34920;&#31034;&#20013;&#25552;&#21462;&#27169;&#24335;&#12290;&#23545;&#30495;&#23454;&#27979;&#35797;&#23454;&#20363;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;&#23545;&#20110;&#23567;&#21040;&#20013;&#31561;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#20869;&#23384;&#28040;&#32791;&#24179;&#22343;&#25552;&#39640;&#20102;88&#65285;&#65292;&#35745;&#31639;&#26102;&#38388;&#25552;&#39640;&#20102;41&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#21807;&#19968;&#19968;&#20010;&#22312;&#31995;&#32479;&#20869;&#23384;&#20026;256GB&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#30340;SPM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.06834v2 Announce Type: replace-cross  Abstract: As modern data sets continue to grow exponentially in size, the demand for efficient mining algorithms capable of handling such large data sets becomes increasingly imperative. This paper develops a memory-efficient approach for Sequential Pattern Mining (SPM), a fundamental topic in knowledge discovery that faces a well-known memory bottleneck for large data sets. Our methodology involves a novel hybrid trie data structure that exploits recurring patterns to compactly store the data set in memory; and a corresponding mining algorithm designed to effectively extract patterns from this compact representation. Numerical results on real-life test instances show an average improvement of 88% in memory consumption and 41% in computation time for small to medium-sized data sets compared to the state of the art. Furthermore, our algorithm stands out as the only capable SPM approach for large data sets within 256GB of system memory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.17010</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;StarCoder&#30340;&#25913;&#36827;&#29256;&#26412;WizardCoder&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#24494;&#35843;&#23558;&#20854;&#36866;&#24212;&#20110;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;WizardCoder&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25506;&#31350;&#20102;&#26368;&#20339;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#38024;&#23545;&#36127;&#26679;&#26412;&#36828;&#22810;&#20110;&#27491;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;WizardCoder&#27169;&#22411;&#22312;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#30340;&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#22312;ROC AUC&#21644;F1&#24230;&#37327;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20854;&#35757;&#32451;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#23545;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#27491;&#21017;&#21270;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#19979;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.15196</link><description>&lt;p&gt;
&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#27491;&#21017;&#21270;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Regularized Q-Learning with Linear Function Approximation. (arXiv:2401.15196v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#27491;&#21017;&#21270;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#19979;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#25104;&#21151;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#27491;&#21017;&#21270;&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#25506;&#32034;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#26102;&#65292;&#36825;&#20123;&#31639;&#27861;&#65288;&#22914;&#36719;Q&#23398;&#20064;&#65289;&#30340;&#25910;&#25947;&#24615;&#36136;&#24182;&#19981;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#21333;&#29615;&#36335;&#31639;&#27861;&#65292;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#20445;&#35777;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#25237;&#24433;&#36125;&#23572;&#26364;&#35823;&#24046;&#12290;&#35813;&#31639;&#27861;&#22312;&#20004;&#20010;&#23610;&#24230;&#19978;&#36816;&#34892;&#65306;&#19968;&#20010;&#36739;&#24930;&#30340;&#23610;&#24230;&#29992;&#20110;&#26356;&#26032;&#29366;&#24577;&#21160;&#20316;&#20540;&#30340;&#30446;&#26631;&#32593;&#32476;&#65292;&#19968;&#20010;&#36739;&#24555;&#30340;&#23610;&#24230;&#29992;&#20110;&#22312;&#22522;&#21521;&#37327;&#31354;&#38388;&#20013;&#36924;&#36817;&#36125;&#23572;&#26364;&#22791;&#20221;&#12290;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#23384;&#22312;&#19979;&#65292;&#35813;&#31639;&#27861;&#25910;&#25947;&#20110;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35813;&#31639;&#27861;&#34893;&#29983;&#31574;&#30053;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several successful reinforcement learning algorithms make use of regularization to promote multi-modal policies that exhibit enhanced exploration and robustness. With functional approximation, the convergence properties of some of these algorithms (e.g. soft Q-learning) are not well understood. In this paper, we consider a single-loop algorithm for minimizing the projected Bellman error with finite time convergence guarantees in the case of linear function approximation. The algorithm operates on two scales: a slower scale for updating the target network of the state-action values, and a faster scale for approximating the Bellman backups in the subspace of the span of basis vectors. We show that, under certain assumptions, the proposed algorithm converges to a stationary point in the presence of Markovian noise. In addition, we provide a performance guarantee for the policies derived from the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14521</link><description>&lt;p&gt;
&#20197;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;-&#27010;&#24565;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#31616;&#27905;&#21487;&#35299;&#37322;&#30340;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#37319;&#29992;&#22522;&#20110;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#20316;&#20026;&#22522;&#26412;&#35745;&#31639;&#21333;&#20803;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#21333;&#20010;&#20301;&#32622;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#65288;&#28145;&#24230;&#65289;&#65292;&#32780;&#19981;&#26159;&#23545;&#22823;&#26679;&#26412;&#38598;&#27700;&#21306;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#24191;&#24230;&#12290;&#30446;&#26631;&#26159;&#21457;&#29616;&#19968;&#20010;&#26368;&#23567;&#30340;&#34920;&#31034;&#65288;&#21333;&#20803;&#29366;&#24577;&#25968;&#21644;&#27969;&#37327;&#36335;&#24452;&#25968;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#33021;&#22815;&#35299;&#37322;&#32473;&#23450;&#38598;&#27700;&#21306;&#36755;&#20837;&#29366;&#24577;&#21644;&#36755;&#20986;&#34892;&#20026;&#30340;&#20027;&#35201;&#36807;&#31243;&#65292;&#29305;&#21035;&#24378;&#35843;&#27169;&#25311;&#20840;&#33539;&#22260;&#65288;&#39640;&#12289;&#20013;&#12289;&#20302;&#65289;&#30340;&#27969;&#37327;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#21306;&#22495;&#65292;&#37319;&#29992;&#31867;&#20284;HyMod&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;3&#20010;&#21333;&#20803;&#29366;&#24577;&#21644;2&#20010;&#20027;&#35201;&#27969;&#21160;&#36335;&#24452;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#26679;&#30340;&#34920;&#31034;&#65292;&#20294;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#27700;&#25991;&#22270;&#30340;&#26102;&#38388;&#21644;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the applicability of machine learning technologies to the development of parsimonious, interpretable, catchment-scale hydrologic models using directed-graph architectures based on the mass-conserving perceptron (MCP) as the fundamental computational unit. Here, we focus on architectural complexity (depth) at a single location, rather than universal applicability (breadth) across large samples of catchments. The goal is to discover a minimal representation (numbers of cell-states and flow paths) that represents the dominant processes that can explain the input-state-output behaviors of a given catchment, with particular emphasis given to simulating the full range (high, medium, and low) of flow dynamics. We find that a HyMod-like architecture with three cell-states and two major flow pathways achieves such a representation at our study location, but that the additional incorporation of an input-bypass mechanism significantly improves the timing and shape of the hydrograph
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09786</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#27169;&#22411;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#38271;&#23614;&#35859;&#35789;&#20998;&#24067;&#21644;&#32570;&#22833;&#27880;&#37322;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;SGG&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#35757;&#32451;SGG&#65288;ST-SGG&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#20026;&#20854;&#20998;&#37197;&#20266;&#26631;&#31614;&#20197;&#35757;&#32451;SGG&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#30340;&#33258;&#35757;&#32451;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#35774;&#35745;&#36866;&#29992;&#20110;SGG&#20219;&#21153;&#30340;&#33258;&#35757;&#32451;&#26694;&#26550;&#26356;&#20855;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#22266;&#26377;&#29305;&#24615;&#65292;&#22914;&#35821;&#20041;&#27495;&#20041;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#35859;&#35789;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SGG&#20266;&#26631;&#31614;&#25216;&#26415;&#65292;&#31216;&#20026;&#20855;&#26377;&#21160;&#37327;&#30340;&#31867;&#21035;&#33258;&#36866;&#24212;&#38408;&#20540;&#21270;&#65288;CATM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#24050;&#26377;&#30340;SGG&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;&#65288;GSL&#65289;&#65292;&#20174;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is benefici
&lt;/p&gt;</description></item><item><title>CoSSegGaussians&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#23454;&#29616;&#32039;&#20945;&#21644;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2401.05925</link><description>&lt;p&gt;
CoSSegGaussians&#65306;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians. (arXiv:2401.05925v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05925
&lt;/p&gt;
&lt;p&gt;
CoSSegGaussians&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#23454;&#29616;&#32039;&#20945;&#21644;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65288;CoSSegGaussians&#65289;&#65292;&#35813;&#26041;&#27861;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#36755;&#20837;&#65292;&#20197;&#24555;&#36895;&#30340;&#28210;&#26579;&#36895;&#24230;&#23454;&#29616;&#32039;&#20945;&#30340;3D&#19968;&#33268;&#24615;&#22330;&#26223;&#20998;&#21106;&#12290;&#20808;&#21069;&#22522;&#20110;NeRF&#30340;3D&#20998;&#21106;&#26041;&#27861;&#20381;&#36182;&#20110;&#38544;&#24335;&#25110;&#20307;&#32032;&#31070;&#32463;&#22330;&#34920;&#31034;&#21644;&#20809;&#32447;&#34892;&#36827;&#20307;&#31215;&#28210;&#26579;&#65292;&#36825;&#20123;&#26041;&#27861;&#32791;&#26102;&#36739;&#38271;&#12290;&#26368;&#36817;&#30340;3D&#39640;&#26031;&#22330;&#25237;&#24433;&#26174;&#33879;&#25552;&#39640;&#20102;&#28210;&#26579;&#36895;&#24230;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#20998;&#21106;&#26041;&#27861;&#65288;&#20363;&#22914;&#39640;&#26031;&#20998;&#32452;&#65289;&#22312;&#38646;&#26679;&#26412;&#20998;&#21106;&#20013;&#27809;&#26377;&#25552;&#20379;&#32039;&#20945;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#22312;&#36935;&#21040;&#19981;&#19968;&#33268;&#30340;2D&#26426;&#22120;&#29983;&#25104;&#26631;&#31614;&#26102;&#65292;&#26080;&#27861;&#30452;&#25509;&#20026;&#27599;&#20010;&#39640;&#26031;&#20998;&#37197;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#32039;&#20945;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#27973;&#23618;&#35299;&#30721;&#32593;&#32476;&#23558;&#27599;&#20010;&#39640;&#26031;&#28857;&#30340;&#34701;&#21512;&#31354;&#38388;&#21644;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#26144;&#23556;&#65292;&#36805;&#36895;&#23454;&#29616;&#32039;&#20945;&#19988;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05373</link><description>&lt;p&gt;
&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#28176;&#28176;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#22312;&#22788;&#29702;&#30001;&#22270;&#34920;&#31034;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#38754;&#20020;&#30528;&#39640;&#22797;&#26434;&#24615;&#21644;&#22823;&#20869;&#23384;&#24320;&#38144;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#20108;&#36827;&#21046;&#29305;&#24449;&#32780;&#19981;&#26159;&#36830;&#32493;&#29305;&#24449;&#30340;SNNs&#26469;&#26367;&#20195;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#36825;&#20250;&#24573;&#35270;&#22270;&#32467;&#26500;&#20449;&#24687;&#24182;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#23548;&#33268;&#32454;&#33410;&#30340;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#20248;&#21270;&#21160;&#24577;&#23574;&#23792;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22312;&#26102;&#38388;&#27493;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#36825;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;\method{}&#65289;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#20943;&#36731;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;\method{} &#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#23427;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#21160;&#24577;&#22320;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#20197;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph \underline{N}eural Networks (\method{}). To mitigate the information loss problem, \method{} propagates
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#20026;&#20309;&#20998;&#20139;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#25311;&#29992;&#25143;&#33030;&#24369;&#24615;&#26469;&#25581;&#31034;&#24433;&#21709;&#29992;&#25143;&#20998;&#20139;&#34892;&#20026;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.15772</link><description>&lt;p&gt;
&#29992;&#25143;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20998;&#20139;&#20167;&#24680;&#35328;&#35770;&#30340;&#22240;&#26524;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Causal Understanding of Why Users Share Hate Speech on Social Media. (arXiv:2310.15772v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#20026;&#20309;&#20998;&#20139;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#25311;&#29992;&#25143;&#33030;&#24369;&#24615;&#26469;&#25581;&#31034;&#24433;&#21709;&#29992;&#25143;&#20998;&#20139;&#34892;&#20026;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#23041;&#32961;&#21040;&#20010;&#20154;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#20581;&#24247;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#23548;&#33268;&#29616;&#23454;&#20013;&#30340;&#26292;&#21147;&#20107;&#20214;&#12290;&#20167;&#24680;&#35328;&#35770;&#20256;&#25773;&#32972;&#21518;&#30340;&#37325;&#35201;&#39537;&#21160;&#22240;&#32032;&#26159;&#36716;&#21457;&#65292;&#20294;&#26159;&#20154;&#20204;&#24456;&#23569;&#20102;&#35299;&#20026;&#20160;&#20040;&#29992;&#25143;&#20250;&#36716;&#21457;&#20167;&#24680;&#35328;&#35770;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#12289;&#22240;&#26524;&#20998;&#26512;&#30340;&#29992;&#25143;&#23646;&#24615;&#26694;&#26550;&#65292;&#30740;&#31350;&#29992;&#25143;&#20026;&#20309;&#20998;&#20139;&#20167;&#24680;&#35328;&#35770;&#12290;&#28982;&#32780;&#65292;&#22312;&#20174;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#26102;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#31867;&#25968;&#25454;&#24456;&#21487;&#33021;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#65292;&#24182;&#19988;&#29992;&#25143;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#33030;&#24369;&#24615;&#23384;&#22312;&#28151;&#28102;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19977;&#27493;&#22240;&#26524;&#26694;&#26550;&#65306;&#65288;1&#65289;&#25105;&#20204;&#36890;&#36807;&#36870;&#21521;&#20542;&#21521;&#35780;&#20998;&#26469;&#28040;&#38500;&#35266;&#23519;&#24615;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#20559;&#24046;&#12290;&#65288;2&#65289;&#25105;&#20204;&#20351;&#29992;&#28040;&#38500;&#20559;&#24046;&#30340;&#20542;&#21521;&#35780;&#20998;&#26469;&#27169;&#25311;&#29992;&#25143;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#28508;&#22312;&#33030;&#24369;&#24615;&#20316;&#20026;&#28508;&#22312;&#23884;&#20837;&#12290;&#65288;3&#65289;&#25105;&#20204;&#24314;&#31435;&#20102;&#29992;&#25143;&#23646;&#24615;&#23545;&#29992;&#25143;&#20998;&#20139;&#20167;&#24680;&#35328;&#35770;&#27010;&#29575;&#30340;&#22240;&#26524;&#25928;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech on social media threatens the mental and physical well-being of individuals and is further responsible for real-world violence. An important driver behind the spread of hate speech and thus why hateful posts can go viral are reshares, yet little is known about why users reshare hate speech. In this paper, we present a comprehensive, causal analysis of the user attributes that make users reshare hate speech. However, causal inference from observational social media data is challenging, because such data likely suffer from selection bias, and there is further confounding due to differences in the vulnerability of users to hate speech. We develop a novel, three-step causal framework: (1) We debias the observational social media data by applying inverse propensity scoring. (2) We use the debiased propensity scores to model the latent vulnerability of users to hate speech as a latent embedding. (3) We model the causal effects of user attributes on users' probability of sharing h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;</title><link>http://arxiv.org/abs/2309.16701</link><description>&lt;p&gt;
MVMR: &#22312;&#22810;&#20010;&#21487;&#38752;&#35270;&#39057;&#38598;&#20013;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#28608;&#22686;&#65292;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#23427;&#33268;&#21147;&#20110;&#26816;&#27979;&#19982;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21305;&#37197;&#30340;&#35270;&#39057;&#29255;&#27573;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#27809;&#26377;&#25506;&#32034;&#22312;&#23384;&#22312;&#22810;&#20010;&#27491;&#36127;&#35270;&#39057;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#20013;&#23450;&#20301;&#19968;&#20010;&#26102;&#21051;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#65288;Massive Videos Moment Retrieval&#65289;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#29616;&#26377;&#35270;&#39057;&#23450;&#20301;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30446;&#26631;&#26597;&#35810;&#19982;&#35270;&#39057;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#20174;&#32780;&#23450;&#20041;&#27491;&#36127;&#38598;&#12290;&#38024;&#23545;&#25552;&#20986;&#30340;MVMR&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;STMAE&#65289;&#26469;&#25552;&#39640;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#21452;&#25513;&#34109;&#31574;&#30053;&#22788;&#29702;&#37096;&#20998;&#21487;&#35265;&#30340;MTS&#25968;&#25454;&#65292;&#21253;&#25324;&#31354;&#38388;&#25513;&#34109;&#21644;&#26102;&#38388;&#25513;&#34109;&#12290;</title><link>http://arxiv.org/abs/2309.15169</link><description>&lt;p&gt;
&#25581;&#31034;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Revealing the Power of Spatial-Temporal Masked Autoencoders in Multivariate Time Series Forecasting. (arXiv:2309.15169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;STMAE&#65289;&#26469;&#25552;&#39640;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#21452;&#25513;&#34109;&#31574;&#30053;&#22788;&#29702;&#37096;&#20998;&#21487;&#35265;&#30340;MTS&#25968;&#25454;&#65292;&#21253;&#25324;&#31354;&#38388;&#25513;&#34109;&#21644;&#26102;&#38388;&#25513;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#28041;&#21450;&#22522;&#20110;&#21382;&#21490;&#35266;&#27979;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#24320;&#21457;&#33021;&#22815;&#26126;&#30830;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#30340;&#22797;&#26434;&#31354;&#38388;-&#26102;&#38388;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;STMAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;MTS&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#25552;&#39640;&#31354;&#38388;-&#26102;&#38388;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;STMAE&#21253;&#25324;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#12290;&#32534;&#30721;&#22120;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#25513;&#34109;&#31574;&#30053;&#22788;&#29702;&#37096;&#20998;&#21487;&#35265;&#30340;MTS&#25968;&#25454;&#65292;&#21253;&#25324;&#22522;&#20110;&#20559;&#32622;&#38543;&#26426;&#28216;&#36208;&#30340;&#31354;&#38388;&#25513;&#34109;&#21644;&#22522;&#20110;&#34917;&#19969;&#30340;&#26102;&#38388;&#25513;&#34109;&#12290;&#38543;&#21518;&#65292;&#35299;&#30721;&#22120;&#26088;&#22312;&#37325;&#26500;&#20004;&#20010;&#25513;&#34109;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series (MTS) forecasting involves predicting future time series data based on historical observations. Existing research primarily emphasizes the development of complex spatial-temporal models that capture spatial dependencies and temporal correlations among time series variables explicitly. However, recent advances have been impeded by challenges relating to data scarcity and model robustness. To address these issues, we propose Spatial-Temporal Masked Autoencoders (STMAE), an MTS forecasting framework that leverages masked autoencoders to enhance the performance of spatial-temporal baseline models. STMAE consists of two learning stages. In the pretraining stage, an encoder-decoder architecture is employed. The encoder processes the partially visible MTS data produced by a novel dual-masking strategy, including biased random walk-based spatial masking and patch-based temporal masking. Subsequently, the decoders aim to reconstruct the masked counterparts from both spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#24212;&#29992;&#26694;&#26550;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.10254</link><description>&lt;p&gt;
LLM&#24179;&#21488;&#23433;&#20840;&#65306;&#23558;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#24212;&#29992;&#20110;OpenAI&#30340;ChatGPT&#25554;&#20214;
&lt;/p&gt;
&lt;p&gt;
LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins. (arXiv:2309.10254v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#24212;&#29992;&#26694;&#26550;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22914;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24179;&#21488;&#24320;&#22987;&#25552;&#20379;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#65292;&#20197;&#19982;&#20114;&#32852;&#32593;&#19978;&#30340;&#31532;&#19977;&#26041;&#26381;&#21153;&#36827;&#34892;&#20132;&#20114;&#12290;&#34429;&#28982;&#36825;&#20123;&#25554;&#20214;&#25193;&#23637;&#20102;LLM&#24179;&#21488;&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#26159;&#30001;&#20219;&#24847;&#30340;&#31532;&#19977;&#26041;&#24320;&#21457;&#30340;&#65292;&#22240;&#27492;&#19981;&#33021;&#38544;&#24335;&#20449;&#20219;&#12290;&#25554;&#20214;&#36824;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;LLM&#24179;&#21488;&#21644;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#31946;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20026;LLM&#24179;&#21488;&#35774;&#35745;&#32773;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#19968;&#20010;&#25915;&#20987;&#20998;&#31867;&#27861;&#30340;&#34920;&#36848;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;LLM&#24179;&#21488;&#30456;&#20851;&#26041;&#22914;&#20309;&#21033;&#29992;&#20182;&#20204;&#30340;&#33021;&#21147;&#21644;&#36131;&#20219;&#23545;&#24444;&#27492;&#36827;&#34892;&#25915;&#20987;&#26469;&#24320;&#21457;&#30340;&#12290;&#20316;&#20026;&#25105;&#20204;&#36845;&#20195;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM) platforms, such as ChatGPT, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the poten
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00237</link><description>&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#30340;&#20844;&#24320;&#21487;&#20849;&#20139;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes. (arXiv:2309.00237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00237
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#30340;&#20020;&#24202;&#26696;&#20363;&#25253;&#21578;&#65292;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#65292;&#20197;&#35299;&#20915;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#21512;&#25104;&#35760;&#24405;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#19987;&#38376;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;Asclepius&#12290;&#34429;&#28982;Asclepius&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Asclepius&#19982;&#21253;&#25324;GPT-3.5-turbo&#21644;&#20854;&#20182;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#22312;&#20869;&#30340;&#20960;&#31181;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#23558;Asclepius&#19982;&#20854;&#22312;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21464;&#20307;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21147;&#22320;&#35777;&#26126;&#65292;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#22312;&#26500;&#24314;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#21487;&#20197;&#20316;&#20026;&#21487;&#34892;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#21270;&#23398;&#36807;&#31243;&#30340;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#20013;&#65292;&#23545;&#21333;&#28304;&#21644;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#36827;&#34892;&#36866;&#24212;&#65292;&#20351;&#29992;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#20063;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.11247</link><description>&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#29992;&#20110;&#21270;&#23398;&#36807;&#31243;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes. (arXiv:2308.11247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#21270;&#23398;&#36807;&#31243;&#30340;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#20013;&#65292;&#23545;&#21333;&#28304;&#21644;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#36827;&#34892;&#36866;&#24212;&#65292;&#20351;&#29992;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#20063;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#35786;&#26029;&#26159;&#36807;&#31243;&#30417;&#35270;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#22522;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#39044;&#27979;&#25925;&#38556;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#36825;&#20123;&#21464;&#21270;&#21487;&#33021;&#30001;&#20110;&#30417;&#27979;&#36807;&#31243;&#20013;&#30340;&#21464;&#21270;&#65292;&#22914;&#25805;&#20316;&#27169;&#24335;&#30340;&#25913;&#21464;&#65292;&#23548;&#33268;&#36328;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#22312;&#21270;&#23398;&#24037;&#19994;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#30000;&#32435;&#35199;-&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#32972;&#26223;&#19979;&#65292;&#25552;&#20379;&#20102;&#21333;&#28304;&#21644;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#22312;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24191;&#27867;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#36827;&#34892;&#36866;&#24212;&#65292;&#20351;&#29992;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#20063;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#23545;&#20110;&#21333;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#22522;&#20934;&#27169;&#22411;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fault diagnosis is an essential component in process supervision. Indeed, it determines which kind of fault has occurred, given that it has been previously detected, allowing for appropriate intervention. Automatic fault diagnosis systems use machine learning for predicting the fault type from sensor readings. Nonetheless, these models are sensible to changes in the data distributions, which may be caused by changes in the monitored process, such as changes in the mode of operation. This scenario is known as Cross-Domain Fault Diagnosis (CDFD). We provide an extensive comparison of single and multi-source unsupervised domain adaptation (SSDA and MSDA respectively) algorithms for CDFD. We study these methods in the context of the Tennessee-Eastmann Process, a widely used benchmark in the chemical industry. We show that using multiple domains during training has a positive effect, even when no adaptation is employed. As such, the MSDA baseline improves over the SSDA baseline classificati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#35770;&#65292;&#24212;&#29992;&#20110;&#23454;&#39564;&#29289;&#29702;&#20013;&#30340;&#22810;&#31181;&#25104;&#20687;&#34920;&#36798;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#35770;&#35299;&#20915;&#20102;&#22810;&#35270;&#35282;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08206</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#35770;&#22312;&#23454;&#39564;&#29289;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Explainable Multi-View Deep Networks Methodology for Experimental Physics. (arXiv:2308.08206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#35770;&#65292;&#24212;&#29992;&#20110;&#23454;&#39564;&#29289;&#29702;&#20013;&#30340;&#22810;&#31181;&#25104;&#20687;&#34920;&#36798;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#35770;&#35299;&#20915;&#20102;&#22810;&#35270;&#35282;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23454;&#39564;&#24120;&#28041;&#21450;&#22810;&#31181;&#25104;&#20687;&#34920;&#36798;&#65292;&#22914;X&#23556;&#32447;&#25195;&#25551;&#21644;&#26174;&#24494;&#22270;&#20687;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#36825;&#20123;&#23454;&#39564;&#30340;&#30417;&#30563;&#20998;&#26512;&#20013;&#12290;&#21512;&#24182;&#19981;&#21516;&#30340;&#22270;&#20687;&#34920;&#36798;&#32463;&#24120;&#38656;&#35201;&#27491;&#30830;&#20998;&#26512;&#21644;&#20570;&#20986;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#22810;&#35270;&#35282;&#25968;&#25454;&#24212;&#36816;&#32780;&#29983; - &#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#26679;&#26412;&#30001;&#26469;&#33258;&#19981;&#21516;&#35282;&#24230;&#12289;&#26469;&#28304;&#25110;&#27169;&#24577;&#30340;&#35270;&#22270;&#25551;&#36848;&#12290;&#22810;&#35270;&#35282;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#20110;&#21487;&#38752;&#21644;&#21487;&#20449;&#30340;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22810;&#35270;&#35282;&#27169;&#22411;&#32570;&#20047;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#30001;&#20110;&#20854;&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#19981;&#21516;&#22810;&#35270;&#35282;&#26550;&#26500;&#65292;&#27599;&#20010;&#26550;&#26500;&#37117;&#36866;&#21512;&#35299;&#20915;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#37322;&#22810;&#35270;&#35282;&#27169;&#22411;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical experiments often involve multiple imaging representations, such as X-ray scans and microscopic images. Deep learning models have been widely used for supervised analysis in these experiments. Combining different image representations is frequently required to analyze and make a decision properly. Consequently, multi-view data has emerged - datasets where each sample is described by views from different angles, sources, or modalities. These problems are addressed with the concept of multi-view learning. Understanding the decision-making process of deep learning models is essential for reliable and credible analysis. Hence, many explainability methods have been devised recently. Nonetheless, there is a lack of proper explainability in multi-view models, which are challenging to explain due to their architectures. In this paper, we suggest different multi-view architectures for the vision domain, each suited to another problem, and we also present a methodology for explaining th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;ChatGPT&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;ChatGPT&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.12488</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#36719;&#20214;&#23433;&#20840;&#65306;&#25506;&#32034;ChatGPT&#22312;&#23433;&#20840;&#24212;&#29992;&#20013;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications. (arXiv:2307.12488v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;ChatGPT&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;ChatGPT&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#22810;&#25165;&#22810;&#33402;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;ChatGPT&#22312;&#21508;&#20010;&#39046;&#22495;&#24212;&#23545;&#38382;&#39064;&#30340;&#28508;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#23637;&#31034;&#12290;&#23427;&#33021;&#22815;&#20998;&#26512;&#12289;&#29702;&#35299;&#21644;&#32508;&#21512;&#26469;&#33258;&#22312;&#32447;&#36164;&#28304;&#21644;&#29992;&#25143;&#36755;&#20837;&#30340;&#20449;&#24687;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;ChatGPT&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#23457;&#26597;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;ChatGPT&#22312;&#38754;&#21521;&#23433;&#20840;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#33021;&#21147;&#65292;&#20174;&#25915;&#20987;&#32773;&#21644;&#23433;&#20840;&#20998;&#26512;&#24072;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#35780;&#20272;ChatGPT&#22312;&#20960;&#20010;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22238;&#31572;&#36136;&#37327;&#65292;&#24182;&#26377;&#24847;&#22320;&#24341;&#20837;&#25361;&#25112;&#26469;&#35780;&#20272;&#20854;&#21709;&#24212;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;ChatGPT&#25552;&#20379;&#30340;&#31572;&#26696;&#36136;&#37327;&#30340;&#32771;&#23519;&#65292;&#25105;&#20204;&#23545;&#20854;&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#39046;&#22495;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#26377;&#20102;&#26356;&#28165;&#26224;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, as a versatile large language model, has demonstrated remarkable potential in addressing inquiries across various domains. Its ability to analyze, comprehend, and synthesize information from both online sources and user inputs has garnered significant attention. Previous research has explored ChatGPT's competence in code generation and code reviews. In this paper, we delve into ChatGPT's capabilities in security-oriented program analysis, focusing on perspectives from both attackers and security analysts. We present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess ChatGPT's responses. Through an examination of the quality of answers provided by ChatGPT, we gain a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12169</link><description>&lt;p&gt;
&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#20026;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20219;&#24847;&#21040;&#20219;&#24847;&#32593;&#32476;&#30340;&#20256;&#32479;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21576;&#29616;&#20986;&#19968;&#31181;&#29420;&#29305;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#22312;&#20854;&#20013;&#65292;&#21482;&#26377;&#23567;&#32452;&#30340;GPU&#38656;&#35201;&#39640;&#24102;&#23485;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;GPU&#23567;&#32452;&#20043;&#38388;&#65292;&#36890;&#20449;&#38750;&#24120;&#24494;&#19981;&#36275;&#36947;&#12289;&#31232;&#30095;&#19988;&#22343;&#21248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#32039;&#23494;&#21305;&#37197;LLMs&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#38598;&#32676;&#20998;&#21106;&#20026;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#20219;&#24847;&#21040;&#20219;&#24847;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;HB&#22495;&#12290;&#22312;HB&#22495;&#20043;&#38388;&#65292;&#32593;&#32476;&#21482;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32593;&#32476;&#36830;&#25509;&#31216;&#20026;&#8220;&#20165;&#36712;&#36947;&#36830;&#25509;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;Clos&#32593;&#32476;&#21487;&#20197;&#23558;&#32593;&#32476;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;LLM&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2307.06290</link><description>&lt;p&gt;
&#25351;&#20196;&#25366;&#25496;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#32463;&#21382;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#36171;&#20104;&#27169;&#22411;&#24378;&#22823;&#30340;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#22238;&#24212;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#26102;&#20173;&#28982;&#26080;&#27861;&#29702;&#35299;&#20154;&#31867;&#25351;&#20196;&#12290;&#20026;&#20102;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#25351;&#20196;&#24494;&#35843;&#24050;&#25104;&#20026;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#20173;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#36136;&#37327;&#30340;&#32447;&#24615;&#35268;&#21017;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#26469;&#36827;&#34892;InstructMining&#30340;&#24314;&#27169;&#12290;&#20026;&#20102;&#30740;&#31350;&#25968;&#25454;&#36136;&#37327;&#19982;&#36825;&#20123;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32454;&#33268;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#32553;&#25918;&#23450;&#24459;&#19982;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#22686;&#21152;&#20250;&#24341;&#21457;&#19981;&#21516;&#31038;&#32676;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#35265;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.03201</link><description>&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#19981;&#20855;&#22791;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws Do Not Scale. (arXiv:2307.03201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32553;&#25918;&#23450;&#24459;&#19982;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#22686;&#21152;&#20250;&#24341;&#21457;&#19981;&#21516;&#31038;&#32676;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#35265;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#30340;&#24130;&#24459;&#20851;&#31995;&#65292;&#23427;&#25551;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#27169;&#22411;&#35774;&#35745;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#22914;&#25968;&#25454;&#38598;&#22823;&#23567;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#65288;&#25110;&#27169;&#22411;&#21442;&#25968;&#31561;&#65289;&#30340;&#22686;&#21152;&#65292;&#22522;&#20110;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#23558;&#30456;&#24212;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#21516;&#26102;&#65292;&#36825;&#31181;&#32553;&#25918;&#23450;&#24459;&#20851;&#31995;&#24573;&#35270;&#20102;&#29992;&#20110;&#34913;&#37327;&#24615;&#33021;&#30340;&#25351;&#26631;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#21644;&#26377;&#20105;&#35758;&#30340;&#65292;&#25110;&#32773;&#21487;&#33021;&#19981;&#31526;&#21512;&#19981;&#21516;&#20154;&#32676;&#23545;&#27169;&#22411;&#36755;&#20986;&#36136;&#37327;&#30340;&#24863;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#38543;&#30528;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#22686;&#38271;&#65292;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#30340;&#19981;&#21516;&#31038;&#32676;&#65288;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#65289;&#30340;&#25968;&#37327;&#21487;&#33021;&#20250;&#22686;&#21152;&#65292;&#27599;&#20010;&#31038;&#32676;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20013;&#25152;&#20195;&#34920;&#30340;&#31038;&#32676;&#21487;&#33021;&#23384;&#22312;&#20215;&#20540;&#35266;&#25110;&#20559;&#35265;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has proposed a power law relationship, referred to as ``scaling laws,'' between the performance of artificial intelligence (AI) models and aspects of those models' design (e.g., dataset size). In other words, as the size of a dataset (or model parameters, etc) increases, the performance of a given model trained on that dataset will correspondingly increase. However, while compelling in the aggregate, this scaling law relationship overlooks the ways that metrics used to measure performance may be precarious and contested, or may not correspond with how different groups of people may perceive the quality of models' output. In this paper, we argue that as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or p
&lt;/p&gt;</description></item><item><title>GIO&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#30340;&#31616;&#21333;&#25918;&#26494;&#21644;&#39640;&#25928;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#38750;&#24120;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.11670</link><description>&lt;p&gt;
GIO&#65306;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#36873;&#25321;&#30340;&#26799;&#24230;&#20449;&#24687;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
GIO: Gradient Information Optimization for Training Dataset Selection. (arXiv:2306.11670v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11670
&lt;/p&gt;
&lt;p&gt;
GIO&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#30340;&#31616;&#21333;&#25918;&#26494;&#21644;&#39640;&#25928;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#38750;&#24120;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#65292;&#36890;&#24120;&#26377;&#21033;&#20110;&#22312;&#21487;&#29992;&#35757;&#32451;&#26679;&#26412;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#20026;&#36825;&#20123;&#26679;&#26412;&#20855;&#26377;&#19981;&#21516;&#30340;&#36136;&#37327;&#65292;&#25110;&#32773;&#24076;&#26395;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#20449;&#24687;&#20248;&#21270;&#65288;GIO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#36873;&#25321;&#38382;&#39064;&#65292;&#23427;&#21482;&#38656;&#35201;&#19968;&#23567;&#32452;&#65288;&#26410;&#26631;&#35760;&#30340;&#65289;&#26679;&#26412;&#26469;&#20195;&#34920;&#30446;&#26631;&#20998;&#24067;&#12290;GIO&#20174;&#19968;&#20010;&#23454;&#36341;&#20013;&#38590;&#20197;&#22788;&#29702;&#30340;&#33258;&#28982;&#30340;&#20449;&#24687;&#29702;&#35770;&#30446;&#26631;&#24320;&#22987;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20986;&#36890;&#36807;&#23545;&#30446;&#26631;&#36827;&#34892;&#31616;&#21333;&#30340;&#25918;&#26494;&#21644;&#39640;&#25928;&#30340;&#23454;&#29616;&#65292;&#23427;&#21487;&#20197;&#34987;&#39640;&#24230;&#25193;&#23637;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#25340;&#20889;&#32416;&#27491;&#21644;&#22270;&#20687;&#35782;&#21035;&#31561;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GIO&#22312;&#38750;&#24120;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20135;&#29983;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;GIO&#26412;&#36523;&#30340;&#19981;&#21516;&#34920;&#31034;&#27169;&#22411;&#21644;&#36229;&#21442;&#25968;&#26159;&#31283;&#20581;&#30340;&#12290;GIO&#26159;&#20219;&#21153;&#21644;&#39046;&#22495;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation. In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets. These findings are robust to different representation models and hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be applied out-of-the-box to ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#21270;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#20013;&#21807;&#19968;&#21487;&#29305;&#24449;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#20256;&#36882;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07662</link><description>&lt;p&gt;
&#26102;&#38388;&#21270;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#30340;&#21807;&#19968;&#21487;&#29305;&#24449;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
Temporalising Unique Characterisability and Learnability of Ontology-Mediated Queries. (arXiv:2306.07662v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#21270;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#20013;&#21807;&#19968;&#21487;&#29305;&#24449;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#20256;&#36882;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#31034;&#20363;&#26469;&#30740;&#31350;&#25968;&#25454;&#24211;&#26597;&#35810;&#30340;&#21807;&#19968;&#21487;&#29305;&#24449;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;&#24050;&#32463;&#25193;&#23637;&#21040;&#20102;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33719;&#24471;&#30340;&#32467;&#26524;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#25552;&#21319;&#21040;&#26102;&#38388;&#21270;&#30340;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#38750;&#26102;&#38388;&#21270;&#24773;&#20917;&#19979;&#30456;&#20851;&#26041;&#27861;&#65292;&#28982;&#21518;&#23637;&#31034;&#20102;&#36890;&#29992;&#30340;&#20256;&#36882;&#32467;&#26524;&#65292;&#21487;&#20197;&#30830;&#23450;&#29616;&#26377;&#32467;&#26524;&#22312;&#20309;&#31181;&#26465;&#20214;&#19979;&#21487;&#20197;&#25512;&#24191;&#21040;&#26102;&#38388;&#21270;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the study of the unique characterisability and learnability of database queries by means of examples has been extended to ontology-mediated queries. Here, we study in how far the obtained results can be lifted to temporalised ontology-mediated queries. We provide a systematic introduction to the relevant approaches in the non-temporal case and then show general transfer results pinpointing under which conditions existing results can be lifted to temporalised queries.
&lt;/p&gt;</description></item><item><title>CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.04979</link><description>&lt;p&gt;
CoCo: &#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#30340;&#32806;&#21512;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. (arXiv:2306.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04979
&lt;/p&gt;
&lt;p&gt;
CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#26497;&#22823;&#30340;&#20195;&#20215;&#26469;&#33719;&#24471;&#12290;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25506;&#32034;&#20854;&#20182;&#26631;&#27880;&#22270;&#20197;&#22686;&#24378;&#30446;&#26631;&#22495;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#22914;&#20309;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#21040;&#39046;&#22495;&#36866;&#24212;&#20013;&#20173;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#23545;&#22270;&#25299;&#25169;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#20197;&#21450;&#30456;&#24403;&#22823;&#30340;&#39046;&#22495;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoCo&#65288;Coupled Contrastive Graph Representation Learning&#65289;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20174;&#32806;&#21512;&#23398;&#20064;&#20998;&#25903;&#20013;&#25552;&#21462;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#12290;CoCo&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#25903;&#21644;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#20998;&#25903;&#65292;&#20998;&#21035;&#29992;&#38544;&#24335;&#21644;&#26174;&#24335;&#26041;&#24335;&#25506;&#32034;&#22270;&#25299;&#25169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32806;&#21512;&#20998;&#25903;&#32467;&#21512;&#21040;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \underline{Co}upled \underline{Co}ntrastive Graph Representation Learning (\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-3 Davinci-003&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#29305;&#36136;&#65292;&#20294;&#22312;&#19981;&#21516;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.04308</link><description>&lt;p&gt;
GPT-3&#30340;&#20154;&#26684;&#27979;&#35797;&#65306;&#26102;&#38388;&#21487;&#38752;&#24615;&#26377;&#38480;&#65292;&#20294;&#20984;&#26174;&#20102;&#31038;&#20132;&#28212;&#26395;&#30340;&#20154;&#26684;&#24037;&#20855;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results. (arXiv:2306.04308v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-3 Davinci-003&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#29305;&#36136;&#65292;&#20294;&#22312;&#19981;&#21516;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#32842;&#22825;&#26426;&#22120;&#20154;GPT-3 Davinci-003&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#21450;&#20854;&#20010;&#24615;&#21270;&#36164;&#26009;&#30340;&#20154;&#26684;&#38382;&#21367;&#30340;&#26102;&#38388;&#21487;&#38752;&#24615;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22330;&#21512;&#65292;&#24515;&#29702;&#38382;&#21367;&#34987;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#28982;&#21518;&#23558;&#22238;&#31572;&#19982;&#20154;&#31867;&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#65292;&#26377;&#20123;&#37327;&#34920;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#26377;&#20123;&#21017;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#19968;&#33268;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;Davinci-003&#26174;&#31034;&#20986;&#19968;&#20010;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#23588;&#20854;&#26159;&#22312;&#20146;&#21644;&#21147;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#31572;&#30340;&#22522;&#30784;&#65292;&#26080;&#35770;&#26159;&#30001;&#20027;&#35266;&#33258;&#25105;&#21453;&#24605;&#36824;&#26159;&#39044;&#23450;&#31639;&#27861;&#39537;&#21160;&#65292;&#23578;&#19981;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
To assess the potential applications and limitations of chatbot GPT-3 Davinci-003, this study explored the temporal reliability of personality questionnaires applied to the chatbot and its personality profile. Psychological questionnaires were administered to the chatbot on two separate occasions, followed by a comparison of the responses to human normative data. The findings revealed varying levels of agreement in the chatbot's responses over time, with some scales displaying excellent while others demonstrated poor agreement. Overall, Davinci-003 displayed a socially desirable and pro-social personality profile, particularly in the domain of communion. However, the underlying basis of the chatbot's responses, whether driven by conscious self-reflection or predetermined algorithms, remains uncertain.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.11527</link><description>&lt;p&gt;
InstructIE: &#19968;&#20221;&#22522;&#20110;&#25351;&#20196;&#30340;&#20013;&#25991;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11527
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#31216;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462; (Instruction-based IE)&#65292;&#23427;&#26088;&#22312;&#35201;&#27714;&#31995;&#32479;&#36981;&#24490;&#29305;&#23450;&#30340;&#25351;&#20196;&#25110;&#25351;&#21335;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340; 270,000 &#20010;&#24369;&#30417;&#30563;&#25968;&#25454;&#21644; 1,000 &#20010;&#39640;&#36136;&#37327;&#20247;&#21253;&#27880;&#37322;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;InstructIE&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/zjunlp/DeepKE/tree/main/example/llm &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>SongDriver2&#23454;&#29616;&#20102;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#65292;&#24182;&#25552;&#20986;&#20102;&#26580;&#21644;&#36807;&#28193;&#26426;&#21046;&#65292;&#20351;&#38899;&#20048;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24615;&#21644;&#24179;&#28369;&#36807;&#28193;&#12290;</title><link>http://arxiv.org/abs/2305.08029</link><description>&lt;p&gt;
SongDriver2&#65306;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#19982;&#26580;&#21644;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
SongDriver2: Real-time Emotion-based Music Arrangement with Soft Transition. (arXiv:2305.08029v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08029
&lt;/p&gt;
&lt;p&gt;
SongDriver2&#23454;&#29616;&#20102;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#65292;&#24182;&#25552;&#20986;&#20102;&#26580;&#21644;&#36807;&#28193;&#26426;&#21046;&#65292;&#20351;&#38899;&#20048;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24615;&#21644;&#24179;&#28369;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#26088;&#22312;&#23558;&#32473;&#23450;&#30340;&#38899;&#20048;&#36716;&#21270;&#20026;&#21478;&#19968;&#20010;&#33021;&#22815;&#23454;&#26102;&#24341;&#36215;&#29992;&#25143;&#29305;&#23450;&#24773;&#24863;&#20849;&#40483;&#30340;&#38899;&#20048;&#65292;&#22312;&#38899;&#20048;&#30103;&#27861;&#12289;&#28216;&#25103;&#37197;&#20048;&#21644;&#30005;&#24433;&#37197;&#20048;&#31561;&#21508;&#31181;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#26631;&#24773;&#24863;&#30340;&#32454;&#31890;&#24230;&#21644;&#21487;&#21464;&#24615;&#65292;&#24179;&#34913;&#24773;&#24863;&#23454;&#26102;&#21305;&#37197;&#21644;&#26580;&#21644;&#24773;&#24863;&#36716;&#25442;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23454;&#29616;&#24773;&#24863;&#23454;&#26102;&#21305;&#37197;&#65292;&#32780;&#26580;&#21644;&#36807;&#28193;&#30340;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#24433;&#21709;&#20102;&#38899;&#20048;&#30340;&#25972;&#20307;&#24773;&#24863;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SongDriver2&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#35782;&#21035;&#26368;&#21518;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#38899;&#20048;&#24773;&#32490;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#24403;&#21069;&#26102;&#38388;&#27493;&#30340;&#30446;&#26631;&#36755;&#20837;&#24773;&#32490;&#34701;&#21512;&#12290;&#34701;&#21512;&#30340;&#24773;&#24863;&#38543;&#21518;&#20316;&#20026;SongDriver2&#26681;&#25454;&#36755;&#20837;&#26059;&#24459;&#25968;&#25454;&#29983;&#25104;&#21363;&#23558;&#21040;&#26469;&#30340;&#38899;&#20048;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#35843;&#25972;&#38899;&#20048;&#30456;&#20284;&#24615;&#21644;&#24773;&#24863;&#23454;&#26102;&#21305;&#37197;&#65292;&#20197;&#23454;&#29616;&#20004;&#31181;&#19981;&#21516;&#24773;&#24863;&#20043;&#38388;&#30340;&#36807;&#28193;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36719;&#36807;&#28193;&#26426;&#21046;&#65292;&#23558;&#25554;&#20540;&#21644;&#24179;&#28369;&#28388;&#27874;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;SongDriver2&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24615;&#21644;&#24179;&#28369;&#36807;&#28193;&#30340;&#24773;&#24863;&#38899;&#20048;&#65292;&#36825;&#34920;&#26126;&#20854;&#22312;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time emotion-based music arrangement, which aims to transform a given music piece into another one that evokes specific emotional resonance with the user in real-time, holds significant application value in various scenarios, e.g., music therapy, video game soundtracks, and movie scores. However, balancing emotion real-time fit with soft emotion transition is a challenge due to the fine-grained and mutable nature of the target emotion. Existing studies mainly focus on achieving emotion real-time fit, while the issue of soft transition remains understudied, affecting the overall emotional coherence of the music. In this paper, we propose SongDriver2 to address this balance. Specifically, we first recognize the last timestep's music emotion and then fuse it with the current timestep's target input emotion. The fused emotion then serves as the guidance for SongDriver2 to generate the upcoming music based on the input melody data. To adjust music similarity and emotion real-time fit f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38754;&#21521;&#22522;&#20110;&#36328;&#24230;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#30340;&#25104;&#26412;&#25928;&#30410;&#20247;&#21253;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#26041;&#27861;&#36827;&#34892;&#24037;&#20154;&#36873;&#25321;&#65292;&#24182;&#29992;&#31227;&#20301;&#12289;&#25193;&#23637;&#21644;&#25910;&#32553;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27880;&#37322;&#36136;&#37327;&#21644;&#38477;&#20302;&#25104;&#26412;&#65292;F1&#24471;&#20998;&#30456;&#23545;&#20110;&#20165;&#19987;&#23478;&#30340;&#22522;&#32447;&#25552;&#39640;&#20102;100.04&#65285;&#65292;&#25104;&#26412;&#33410;&#32422;&#39640;&#36798;65.97&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.06683</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#36328;&#24230;&#30340;&#24207;&#21015;&#26631;&#27880;&#30340;&#25104;&#26412;&#25928;&#30410;&#20247;&#21253;&#65306;&#24037;&#20154;&#36873;&#25321;&#21644;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation. (arXiv:2305.06683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38754;&#21521;&#22522;&#20110;&#36328;&#24230;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#30340;&#25104;&#26412;&#25928;&#30410;&#20247;&#21253;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#26041;&#27861;&#36827;&#34892;&#24037;&#20154;&#36873;&#25321;&#65292;&#24182;&#29992;&#31227;&#20301;&#12289;&#25193;&#23637;&#21644;&#25910;&#32553;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27880;&#37322;&#36136;&#37327;&#21644;&#38477;&#20302;&#25104;&#26412;&#65292;F1&#24471;&#20998;&#30456;&#23545;&#20110;&#20165;&#19987;&#23478;&#30340;&#22522;&#32447;&#25552;&#39640;&#20102;100.04&#65285;&#65292;&#25104;&#26412;&#33410;&#32422;&#39640;&#36798;65.97&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20154;&#36873;&#25321;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20110;&#36328;&#24230;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#30340;&#27880;&#37322;&#36136;&#37327;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;&#19982;&#20197;&#21069;&#38024;&#23545;&#31616;&#21333;&#20219;&#21153;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#28041;&#21450;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#20013;&#30340;&#26631;&#31614;&#30456;&#20114;&#20381;&#36182;&#24615;&#22797;&#26434;&#24615;&#12290;&#25152;&#25552;&#35758;&#30340;&#31639;&#27861;&#20351;&#29992;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;CMAB&#65289;&#26041;&#27861;&#36827;&#34892;&#24037;&#20154;&#36873;&#25321;&#12290;&#35299;&#20915;&#20102;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#38459;&#30861;&#20102;&#24037;&#20154;&#36873;&#25321;&#30340;&#31163;&#32447;&#27169;&#25311;&#65292;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#31227;&#20301;&#12289;&#25193;&#23637;&#21644;&#25910;&#32553;&#65288;SES&#65289;&#30340;&#21019;&#26032;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;SES&#26041;&#27861;&#19987;&#38376;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#12290;&#22312;CoNLL 2003 NER&#21644;&#20013;&#25991;OEI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20005;&#26684;&#27979;&#35797;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;F1&#24471;&#20998;&#30456;&#23545;&#20110;&#20165;&#19987;&#23478;&#30340;&#22522;&#32447;&#25552;&#39640;&#20102;100.04&#65285;&#65292;&#25104;&#26412;&#33410;&#32422;&#39640;&#36798;65.97&#65285;&#12290;&#26412;&#25991;&#36824;&#21253;&#25324;&#19968;&#20010;&#29420;&#31435;&#20110;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel worker selection algorithm, enhancing annotation quality and reducing costs in challenging span-based sequence labeling tasks in Natural Language Processing (NLP). Unlike previous studies targeting simpler tasks, this study contends with the complexities of label interdependencies in sequence labeling tasks. The proposed algorithm utilizes a Combinatorial Multi-Armed Bandit (CMAB) approach for worker selection. The challenge of dealing with imbalanced and small-scale datasets, which hinders offline simulation of worker selection, is tackled using an innovative data augmentation method termed shifting, expanding, and shrinking (SES). The SES method is designed specifically for sequence labeling tasks. Rigorous testing on CoNLL 2003 NER and Chinese OEI datasets showcased the algorithm's efficiency, with an increase in F1 score up to 100.04% of the expert-only baseline, alongside cost savings up to 65.97%. The paper also encompasses a dataset-independent test
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.03857</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#65306;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pre-trained AudioLDM for Text to Sound Generation: A Benchmark Study. (arXiv:2303.03857v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the advantages of using pre-trained AudioLDM as the backbone for sound generation, demonstrates the benefits of using pre-trained models for text-to-sound generation in data-scarcity scenarios, and evaluates various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols to provide a basis for future research.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#36817;&#22312;&#25991;&#26412;&#25552;&#31034;&#19979;&#23454;&#29616;&#20102;&#22768;&#38899;&#29983;&#25104;&#30340;&#31361;&#30772;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#34920;&#29616;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#24403;&#21069;&#30340;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#36807;&#24230;&#25311;&#21512;&#65289;&#19978;&#38754;&#20020;&#38382;&#39064;&#65292;&#20174;&#32780;&#26174;&#33879;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65288;&#20363;&#22914;&#35757;&#32451;&#26465;&#20214;&#65289;&#21487;&#33021;&#20250;&#24433;&#21709;AudioLDM&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#36825;&#20123;&#21327;&#35758;&#20801;&#35768;&#22312;&#20849;&#21516;&#22522;&#30784;&#19978;&#20844;&#24179;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have recently achieved breakthroughs in sound generation with text prompts. Despite their promising performance, current text-to-sound generation models face issues on small-scale datasets (e.g., overfitting), significantly limiting their performance. In this paper, we investigate the use of pre-trained AudioLDM, the state-of-the-art model for text-to-audio generation, as the backbone for sound generation. Our study demonstrates the advantages of using pre-trained models for text-to-sound generation, especially in data-scarcity scenarios. In addition, experiments show that different training strategies (e.g., training conditions) may affect the performance of AudioLDM on datasets of different scales. To facilitate future studies, we also evaluate various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols, which allow fair comparisons and benchmarking of these methods on the common ground.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#36125;&#21494;&#26031;&#23545;&#25163;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#19977;&#20154; Kuhn poker &#20013;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26126;&#26174;&#36229;&#36807;&#25152;&#26377;&#30340;&#20195;&#29702;&#21830;&#65292;&#21253;&#25324;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.06027</link><description>&lt;p&gt;
&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#36125;&#21494;&#26031;&#23545;&#25163;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Bayesian Opponent Modeling in Multiplayer Imperfect-Information Games. (arXiv:2212.06027v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#36125;&#21494;&#26031;&#23545;&#25163;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#19977;&#20154; Kuhn poker &#20013;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26126;&#26174;&#36229;&#36807;&#25152;&#26377;&#30340;&#20195;&#29702;&#21830;&#65292;&#21253;&#25324;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#20195;&#29702;&#21830;&#19982;&#22810;&#20010;&#23545;&#31435;&#20195;&#29702;&#21830;&#36827;&#34892;&#25112;&#30053;&#20114;&#21160;&#65292;&#23545;&#25163;&#21487;&#33021;&#37319;&#29992;&#21508;&#31181;&#31574;&#30053;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#24773;&#22659;&#65292;&#35774;&#35745;&#20195;&#29702;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#35745;&#31639;&#25110;&#36924;&#36817;&#30456;&#20851;&#30340;&#21338;&#24328;&#29702;&#35770;&#35299;&#65292;&#22914;&#32435;&#20160;&#22343;&#34913;&#65292;&#28982;&#21518;&#36981;&#24490;&#35268;&#23450;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#31574;&#30053;&#24573;&#30053;&#20102;&#23545;&#25163;&#29609;&#28216;&#25103;&#30340;&#20219;&#20309;&#35266;&#23519;&#65292;&#36825;&#20123;&#35266;&#23519;&#21487;&#33021;&#34920;&#26126;&#21487;&#20197;&#21033;&#29992;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#23545;&#25163;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20132;&#20114;&#25910;&#38598;&#23545;&#25163;&#29609;&#28216;&#25103;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#23545;&#19977;&#20154; Kuhn &#25169;&#20811;&#23637;&#24320;&#20102;&#23545;&#35768;&#22810;&#30495;&#23454;&#23545;&#25163;&#21644;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#30340;&#20195;&#29702;&#21830;&#65292;&#21253;&#25324;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world settings agents engage in strategic interactions with multiple opposing agents who can employ a wide variety of strategies. The standard approach for designing agents for such settings is to compute or approximate a relevant game-theoretic solution concept such as Nash equilibrium and then follow the prescribed strategy. However, such a strategy ignores any observations of opponents' play, which may indicate shortcomings that can be exploited. We present an approach for opponent modeling in multiplayer imperfect-information games where we collect observations of opponents' play through repeated interactions. We run experiments against a wide variety of real opponents and exact Nash equilibrium strategies in three-player Kuhn poker and show that our algorithm significantly outperforms all of the agents, including the exact Nash equilibrium strategies.
&lt;/p&gt;</description></item></channel></rss>