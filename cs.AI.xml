<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01399</link><description>&lt;p&gt;
&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Model to explain Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#20363;&#22914;&#23545;&#35821;&#20041;&#30456;&#20851;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#24577;&#26469;&#23398;&#20064;&#34920;&#31034;&#12290;&#22312;&#20247;&#22810;SSL&#26041;&#27861;&#20013;&#65292;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;SimCLR&#65292;CLIP&#21644;VicREG&#65289;&#22240;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#22312;&#19979;&#28216;&#24615;&#33021;&#19978;&#25509;&#36817;&#26377;&#30417;&#30563;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32972;&#21518;&#30340;&#26426;&#21046;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20102;&#20960;&#31867;&#20855;&#26377;&#37492;&#21035;&#24615;&#30340;&#33258;&#30417;&#30563;&#31639;&#27861;&#65288;&#21253;&#25324;&#23545;&#27604;&#26041;&#27861;&#65289;&#36817;&#20284;&#35825;&#23548;&#20854;&#34920;&#31034;&#20013;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19982;&#20114;&#20449;&#24687;&#21644;&#25237;&#24433;&#22836;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#29983;&#25104;&#24335;&#22320;&#25311;&#21512;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;&#22914;SimVE&#65289;&#65292;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65288;&#20363;&#22914;FashionMNIST&#65292;CIFAR10&#65292;CelebA&#65289;&#65292;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;VAE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows th
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#31243;&#24207;&#21592;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;RealHumanEval&#20316;&#20026;&#34913;&#37327;&#20854;&#24110;&#21161;&#24615;&#30340;&#30028;&#38754;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#23545;&#31243;&#24207;&#21592;&#29983;&#20135;&#21147;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02806</link><description>&lt;p&gt;
RealHumanEval: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#31243;&#24207;&#21592;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02806
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#31243;&#24207;&#21592;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;RealHumanEval&#20316;&#20026;&#34913;&#37327;&#20854;&#24110;&#21161;&#24615;&#30340;&#30028;&#38754;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#23545;&#31243;&#24207;&#21592;&#29983;&#20135;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#20027;&#35201;&#20381;&#36182;&#20110;&#38745;&#24577;&#22522;&#20934;&#65292;&#21253;&#25324;HumanEval&#65288;Chen&#31561;&#65292;2021&#65289;&#65292;&#36825;&#20123;&#22522;&#20934;&#29992;&#20110;&#34913;&#37327;LLMs&#29983;&#25104;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;&#23436;&#25972;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20316;&#31243;&#24207;&#21592;&#21161;&#25163;&#65292;&#25105;&#20204;&#30740;&#31350;&#29616;&#26377;&#22522;&#20934;&#19978;&#30340;&#22686;&#30410;&#26159;&#21542;&#33021;&#36716;&#21270;&#20026;&#20351;&#29992;LLMs&#32534;&#30721;&#26102;&#31243;&#24207;&#21592;&#29983;&#20135;&#21147;&#30340;&#25552;&#21319;&#65292;&#21253;&#25324;&#32534;&#30721;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#12290;&#38500;&#20102;&#38745;&#24577;&#22522;&#20934;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21487;&#33021;&#29992;&#20316;&#24230;&#37327;LLM&#24110;&#21161;&#24615;&#20195;&#29702;&#30340;&#20559;&#22909;&#24230;&#37327;&#30340;&#23454;&#29992;&#24615;&#65292;&#20363;&#22914;&#20195;&#30721;&#25509;&#21463;&#25110;&#22797;&#21046;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RealHumanEval&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;LLMs&#36741;&#21161;&#31243;&#24207;&#21592;&#30340;&#33021;&#21147;&#30340;&#32593;&#32476;&#30028;&#38754;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#23436;&#25104;&#25110;&#32842;&#22825;&#25903;&#25345;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#65288;N = 213&#65289;&#65292;&#20351;&#29992;RealHumanEval&#65292;&#20854;&#20013;&#29992;&#25143;&#19982;&#20845;&#20010;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#21508;&#24322;&#30340;LLMs&#36827;&#34892;&#20132;&#20114;&#12290;&#23613;&#31649;&#38745;&#24577;&#22522;&#20934;&#27809;&#26377;&#21253;&#21547;&#20154;&#20026;&#24178;&#39044;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02806v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding. In addition to static benchmarks, we investigate the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we 
&lt;/p&gt;</description></item><item><title>FineFake &#25968;&#25454;&#38598;&#20026;&#32454;&#31890;&#24230;&#22810;&#39046;&#22495;&#20551;&#26032;&#38395;&#26816;&#27979;&#25552;&#20379;&#20102;&#30693;&#35782;&#22686;&#24378;&#65292;&#21253;&#21547;16,909&#20010;&#25968;&#25454;&#26679;&#26412;&#35206;&#30422;&#20845;&#20010;&#35821;&#20041;&#20027;&#39064;&#21644;&#20843;&#20010;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2404.01336</link><description>&lt;p&gt;
FineFake&#65306;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#39046;&#22495;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#30693;&#35782;&#22686;&#24378;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detecction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01336
&lt;/p&gt;
&lt;p&gt;
FineFake &#25968;&#25454;&#38598;&#20026;&#32454;&#31890;&#24230;&#22810;&#39046;&#22495;&#20551;&#26032;&#38395;&#26816;&#27979;&#25552;&#20379;&#20102;&#30693;&#35782;&#22686;&#24378;&#65292;&#21253;&#21547;16,909&#20010;&#25968;&#25454;&#26679;&#26412;&#35206;&#30422;&#20845;&#20010;&#35821;&#20041;&#20027;&#39064;&#21644;&#20843;&#20010;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#35780;&#20272;&#26032;&#38395;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#36890;&#24120;&#20165;&#20851;&#27880;&#21333;&#19968;&#35821;&#20041;&#20027;&#39064;&#30340;&#26032;&#38395;&#25110;&#26469;&#33258;&#21333;&#19968;&#24179;&#21488;&#30340;&#26032;&#38395;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#30495;&#23454;&#22330;&#26223;&#20013;&#22810;&#39046;&#22495;&#26032;&#38395;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#20102;&#35299;&#19981;&#21516;&#39046;&#22495;&#30340;&#20551;&#26032;&#38395;&#65292;&#22806;&#37096;&#30693;&#35782;&#21644;&#32454;&#31890;&#24230;&#27880;&#37322;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#25552;&#20379;&#31934;&#30830;&#35777;&#25454;&#24182;&#25581;&#31034;&#21046;&#36896;&#20551;&#26032;&#38395;&#30340;&#22810;&#26679;&#28508;&#22312;&#31574;&#30053;&#65292;&#32780;&#36825;&#20063;&#26159;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#25152;&#24573;&#30053;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;FineFake&#30340;&#26032;&#22411;&#22810;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#32454;&#31890;&#24230;&#27880;&#37322;&#12290;FineFake&#28085;&#30422;&#20102;&#26469;&#33258;&#20845;&#20010;&#35821;&#20041;&#20027;&#39064;&#21644;&#20843;&#20010;&#24179;&#21488;&#30340;16,909&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#27599;&#20010;&#26032;&#38395;&#39033;&#30446;&#37117;&#21253;&#21547;&#22810;&#27169;&#24577;&#20869;&#23481;&#12289;&#28508;&#22312;&#31038;&#20132;&#32972;&#26223;&#12289;&#21322;&#33258;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01336v1 Announce Type: cross  Abstract: Existing benchmarks for fake news detection have significantly contributed to the advancement of models in assessing the authenticity of news content. However, these benchmarks typically focus solely on news pertaining to a single semantic topic or originating from a single platform, thereby failing to capture the diversity of multi-domain news in real scenarios. In order to understand fake news across various domains, the external knowledge and fine-grained annotations are indispensable to provide precise evidence and uncover the diverse underlying strategies for fabrication, which are also ignored by existing benchmarks. To address this gap, we introduce a novel multi-domain knowledge-enhanced benchmark with fine-grained annotations, named \textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six semantic topics and eight platforms. Each news item is enriched with multi-modal content, potential social context, semi-man
&lt;/p&gt;</description></item><item><title>CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.19918</link><description>&lt;p&gt;
CtRL-Sim&#65306;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#24212;&#24615;&#21487;&#25511;&#39550;&#39542;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19918
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CtRL-Sim&#65292;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#22686;&#24378;&#30340;Nocturne&#27169;&#25311;&#22120;&#20013;&#30340;&#22238;&#25253;&#26465;&#20214;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#39640;&#25928;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;Nocturne&#27169;&#25311;&#22120;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#65292;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19918v1 Announce Type: cross  Abstract: Evaluating autonomous vehicle stacks (AVs) in simulation typically involves replaying driving logs from real-world recorded traffic. However, agents replayed from offline data do not react to the actions of the AV, and their behaviour cannot be easily controlled to simulate counterfactual scenarios. Existing approaches have attempted to address these shortcomings by proposing methods that rely on heuristics or learned generative models of real-world data but these approaches either lack realism or necessitate costly iterative sampling procedures to control the generated behaviours. In this work, we take an alternative approach and propose CtRL-Sim, a method that leverages return-conditioned offline reinforcement learning within a physics-enhanced Nocturne simulator to efficiently generate reactive and controllable traffic agents. Specifically, we process real-world driving data through the Nocturne simulator to generate a diverse offli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#65292;&#24182;&#19982;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#23545;&#40784;&#65292;&#26469;&#20811;&#26381;&#22235;&#36275;&#21160;&#20316;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14864</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20998;&#20223;&#30495;&#23398;&#20064;&#22235;&#36275;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Quadruped Locomotion Using Differentiable Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#65292;&#24182;&#19982;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#23545;&#40784;&#65292;&#26469;&#20811;&#26381;&#22235;&#36275;&#21160;&#20316;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#37096;&#20998;&#26426;&#22120;&#20154;&#36816;&#21160;&#25511;&#21046;&#30340;&#36827;&#23637;&#37117;&#26159;&#30001;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#24494;&#20998;&#20223;&#30495;&#30340;&#28508;&#21147;&#12290;&#21487;&#24494;&#20998;&#20223;&#30495;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#35745;&#31639;&#20302;&#21464;&#24322;&#19968;&#38454;&#26799;&#24230;&#65292;&#25215;&#35834;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20854;&#22312;&#22235;&#36275;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#38754;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#21487;&#24494;&#20998;&#20223;&#30495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#30001;&#20110;&#25509;&#35302;&#20016;&#23500;&#29615;&#22659;&#65288;&#22914;&#22235;&#36275;&#21160;&#20316;&#65289;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#22797;&#26434;&#20248;&#21270;&#26223;&#35266;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#20851;&#38190;&#24819;&#27861;&#21253;&#25324;&#23558;&#21487;&#33021;&#30001;&#20110;&#25509;&#35302;&#32780;&#20986;&#29616;&#19981;&#36830;&#32493;&#24615;&#30340;&#22797;&#26434;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#31616;&#21270;&#27169;&#22411;&#20135;&#29983;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#19982;&#26356;&#31934;&#30830;&#30340;&#19981;&#21487;&#24494;&#20998;&#27169;&#22411;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14864v1 Announce Type: cross  Abstract: While most recent advancements in legged robot control have been driven by model-free reinforcement learning, we explore the potential of differentiable simulation. Differentiable simulation promises faster convergence and more stable training by computing low-variant first-order gradients using the robot model, but so far, its use for legged robot control has remained limited to simulation. The main challenge with differentiable simulation lies in the complex optimization landscape of robotic tasks due to discontinuities in contact-rich environments, e.g., quadruped locomotion. This work proposes a new, differentiable simulation framework to overcome these challenges. The key idea involves decoupling the complex whole-body simulation, which may exhibit discontinuities due to contact, into two separate continuous domains. Subsequently, we align the robot state resulting from the simplified model with a more precise, non-differentiable 
&lt;/p&gt;</description></item><item><title>Nellie&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#12289;&#26080;&#20559;&#35265;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;2D/3D&#27963;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#25552;&#21462;&#22810;&#26679;&#32454;&#32990;&#20869;&#32467;&#26500;&#29305;&#24449;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#23618;&#20998;&#21106;&#21644;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13214</link><description>&lt;p&gt;
Nellie&#65306;&#33258;&#21160;&#30340;2D/3D&#27963;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#22120;&#23448;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Nellie: Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13214
&lt;/p&gt;
&lt;p&gt;
Nellie&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#12289;&#26080;&#20559;&#35265;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;2D/3D&#27963;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#25552;&#21462;&#22810;&#26679;&#32454;&#32990;&#20869;&#32467;&#26500;&#29305;&#24449;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#23618;&#20998;&#21106;&#21644;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#32454;&#32990;&#22120;&#30340;&#20998;&#26512;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#20294;&#23545;&#20110;&#29702;&#35299;&#29983;&#29289;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Nellie&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#19988;&#26080;&#20559;&#35265;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#25552;&#21462;&#22810;&#26679;&#30340;&#32454;&#32990;&#20869;&#32467;&#26500;&#29305;&#24449;&#12290;Nellie&#33021;&#22815;&#36866;&#24212;&#22270;&#20687;&#30340;&#20803;&#25968;&#25454;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#30340;&#36755;&#20837;&#12290;Nellie&#30340;&#39044;&#22788;&#29702;&#31649;&#36947;&#22312;&#22810;&#20010;&#32454;&#32990;&#20869;&#23610;&#24230;&#19978;&#22686;&#24378;&#20102;&#32467;&#26500;&#23545;&#27604;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20122;&#22120;&#23448;&#21306;&#22495;&#30340;&#24378;&#22823;&#20998;&#23618;&#20998;&#21106;&#12290;&#36890;&#36807;&#21322;&#24452;&#33258;&#36866;&#24212;&#30340;&#27169;&#24335;&#21305;&#37197;&#26041;&#26696;&#29983;&#25104;&#21644;&#36319;&#36394;&#20869;&#37096;&#36816;&#21160;&#25429;&#25417;&#26631;&#35760;&#65292;&#24182;&#29992;&#20316;&#20122;&#20307;&#31215;&#27969;&#25554;&#20540;&#30340;&#25351;&#21335;&#12290;Nellie&#22312;&#22810;&#20010;&#20998;&#23618;&#27700;&#24179;&#25552;&#21462;&#22823;&#37327;&#29305;&#24449;&#65292;&#29992;&#20110;&#28145;&#24230;&#21644;&#21487;&#23450;&#21046;&#30340;&#20998;&#26512;&#12290;Nellie&#20855;&#26377;&#22522;&#20110;Napari&#30340;GUI&#65292;&#23454;&#29616;&#26080;&#20195;&#30721;&#25805;&#20316;&#21644;&#21487;&#35270;&#21270;&#65292;&#21516;&#26102;&#20854;&#27169;&#22359;&#21270;&#30340;&#24320;&#28304;&#20195;&#30721;&#24211;&#25552;&#20379;&#20102;&#32463;&#39564;&#20016;&#23500;&#29992;&#25143;&#30340;&#33258;&#23450;&#20041;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13214v1 Announce Type: cross  Abstract: The analysis of dynamic organelles remains a formidable challenge, though key to understanding biological processes. We introduce Nellie, an automated and unbiased pipeline for segmentation, tracking, and feature extraction of diverse intracellular structures. Nellie adapts to image metadata, eliminating user input. Nellie's preprocessing pipeline enhances structural contrast on multiple intracellular scales allowing for robust hierarchical segmentation of sub-organellar regions. Internal motion capture markers are generated and tracked via a radius-adaptive pattern matching scheme, and used as guides for sub-voxel flow interpolation. Nellie extracts a plethora of features at multiple hierarchical levels for deep and customizable analysis. Nellie features a Napari-based GUI that allows for code-free operation and visualization, while its modular open-source codebase invites customization by experienced users. We demonstrate Nellie's wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18679</link><description>&lt;p&gt;
&#25968;&#25454;&#35299;&#37322;&#22120;&#65306;&#29992;&#20110;&#25968;&#25454;&#31185;&#23398;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Interpreter: An LLM Agent For Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23454;&#26102;&#25968;&#25454;&#35843;&#25972;&#12289;&#20248;&#21270;&#19987;&#19994;&#30693;&#35782;&#20197;&#24212;&#23545;&#21508;&#31181;&#20219;&#21153;&#38388;&#22797;&#26434;&#20381;&#36182;&#24615;&#20197;&#21450;&#31934;&#30830;&#25512;&#29702;&#30340;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#30340;&#25968;&#25454;&#31185;&#23398;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#24378;&#35843;&#19977;&#31181;&#20851;&#38190;&#25216;&#26415;&#20197;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#26696;&#30340;&#20195;&#30721;&#65306;1&#65289;&#20855;&#26377;&#20998;&#23618;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#29992;&#20110;&#23454;&#26102;&#25968;&#25454;&#36866;&#24212;&#24615;&#65307;2&#65289;&#24037;&#20855;&#38598;&#25104;&#21160;&#24577;&#21270;&#65292;&#20197;&#22686;&#24378;&#20195;&#30721;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#29087;&#32451;&#24230;&#65292;&#20016;&#23500;&#24517;&#35201;&#30340;&#19987;&#19994;&#30693;&#35782;&#65307;3&#65289;&#22312;&#21453;&#39304;&#20013;&#35782;&#21035;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#35760;&#24405;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#22312;&#21508;&#31181;&#25968;&#25454;&#31185;&#23398;&#21644;&#29616;&#23454;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#19982;&#24320;&#28304;&#22522;&#32447;&#30456;&#27604;&#65292;&#23427;&#23637;&#29616;&#20102;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18679v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Layer Collapse&#65288;LaCo&#65289;&#30340;&#31616;&#26126;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#32467;&#26500;&#30340;&#21516;&#26102;&#36805;&#36895;&#20943;&#23567;&#23610;&#23544;&#65292;&#24182;&#22312;&#21098;&#26525;&#27604;&#20363;&#36798;&#21040;25-30%&#26102;&#20445;&#25345;&#36229;&#36807;80%&#30340;&#24179;&#22343;&#20219;&#21153;&#24615;&#33021;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11187</link><description>&lt;p&gt;
LaCo&#65306;&#36890;&#36807;&#23618;&#21472;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
LaCo: Large Language Model Pruning via Layer Collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11187
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Layer Collapse&#65288;LaCo&#65289;&#30340;&#31616;&#26126;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#32467;&#26500;&#30340;&#21516;&#26102;&#36805;&#36895;&#20943;&#23567;&#23610;&#23544;&#65292;&#24182;&#22312;&#21098;&#26525;&#27604;&#20363;&#36798;&#21040;25-30%&#26102;&#20445;&#25345;&#36229;&#36807;80%&#30340;&#24179;&#22343;&#20219;&#21153;&#24615;&#33021;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#32463;&#21382;&#30528;&#23610;&#23544;&#25193;&#22823;&#30340;&#26126;&#26174;&#36235;&#21183;&#65292;&#36825;&#32473;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22914;&#27169;&#22411;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#27169;&#22411;&#21098;&#26525;&#21463;&#21040;&#21508;&#31181;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#30828;&#20214;&#25903;&#25345;&#38480;&#21046;&#12289;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#21644;&#23545;&#27169;&#22411;&#20869;&#37096;&#32467;&#26500;&#30340;&#25913;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#31216;&#20026;Layer Collapse&#65288;LaCo&#65289;&#65292;&#20854;&#20013;&#21518;&#32622;&#27169;&#22411;&#23618;&#25240;&#21472;&#21040;&#21069;&#32622;&#23618;&#65292;&#20351;&#27169;&#22411;&#23610;&#23544;&#36805;&#36895;&#20943;&#23567;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#32467;&#26500;&#12290;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21098;&#26525;&#27604;&#20363;&#36798;&#21040;25-30%&#26102;&#65292;&#20445;&#25345;&#20102;&#36229;&#36807;80%&#30340;&#24179;&#22343;&#20219;&#21153;&#24615;&#33021;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#21518;&#35757;&#32451;&#23454;&#39564;&#20197;&#30830;&#35748;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11187v1 Announce Type: cross  Abstract: Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the internal structure of the model. In this paper, we propose a concise layer-wise pruning method called \textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\% at pruning ratios of 25-30\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the proposed pr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04856</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Explaining Learned Reward Functions with Counterfactual Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04856
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#34892;&#20026;&#25110;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#26159;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#26080;&#27861;&#22987;&#32456;&#25552;&#21462;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#25509;&#25910;&#30340;&#22870;&#21169;&#26469;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;CTEs&#21046;&#23450;&#20102;&#20845;&#20010;&#36136;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Monte-Carlo&#30340;&#26032;&#31639;&#27861;&#26469;&#29983;&#25104;&#20248;&#21270;&#36825;&#20123;&#36136;&#37327;&#26631;&#20934;&#30340;CTEs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#20154;&#27169;&#22411;&#26469;&#34913;&#37327;&#29983;&#25104;&#30340;&#35299;&#37322;&#23545;&#20854;&#30340;&#20449;&#24687;&#24615;&#12290;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#22686;&#21152;&#20102;&#20854;&#39044;&#27979;&#19982;&#26410;&#35265;&#36712;&#36857;&#19978;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#23398;&#20250;&#20102;&#20934;&#30830;&#21028;&#26029;&#36712;&#36857;&#20043;&#38388;&#30340;&#22870;&#21169;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#26469;&#33258;&#21160;&#36827;&#34892;&#25968;&#25454;&#25972;&#29702;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02658</link><description>&lt;p&gt;
&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#30340;&#39564;&#35777;&#22120;&#65306;&#20851;&#20110;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#26469;&#33258;&#21160;&#36827;&#34892;&#25968;&#25454;&#25972;&#29702;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#30417;&#30563;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#39564;&#35777;&#22120;&#26469;&#35780;&#20272;&#25512;&#29702;&#22120;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#65292;&#24050;&#32463;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#36991;&#20813;&#22312;&#39564;&#35777;&#22120;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#25968;&#25454;&#25972;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;MiPS&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#65292;&#24182;&#33719;&#24471;&#19968;&#20010;&#20934;&#30830;&#29575;&#65292;&#20854;&#20013;&#20934;&#30830;&#23436;&#25104;&#30340;&#27604;&#20363;&#23450;&#20041;&#20026;&#20934;&#30830;&#29575;&#12290;&#25512;&#29702;&#22120;&#20013;&#30340;&#38169;&#35823;&#20250;&#23548;&#33268;MiPS&#20302;&#20272;&#20013;&#38388;&#27493;&#39588;&#30340;&#20934;&#30830;&#29575;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#65292;&#32780;&#19981;&#26159;&#20302;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65288;GSM8K&#19978;&#30340;&#20934;&#30830;&#29575;+0.67&#65285;&#65292;&#25968;&#23398;&#19978;&#30340;&#20934;&#30830;&#29575;+4.16&#65285;&#65292;MBPP&#19978;&#30340;&#20934;&#30830;&#29575;+0.92&#65285;&#19982;&#36755;&#20986;s&#30456;&#27604;&#12290;&#65289;
&lt;/p&gt;
&lt;p&gt;
Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22312;&#25910;&#38598;&#21040;&#30340;&#36712;&#36857;&#19978;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00658</link><description>&lt;p&gt;
&#36890;&#36807;&#25910;&#38598;&#36712;&#36857;&#21644;&#21512;&#25104;&#22870;&#21169;&#26469;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22312;&#25910;&#38598;&#21040;&#30340;&#36712;&#36857;&#19978;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#36880;&#27493;&#21512;&#29702;&#21270;&#29983;&#25104;&#65292;&#23637;&#31034;&#20102;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#21512;&#29702;&#21270;&#30340;&#21487;&#38752;&#24615;&#21644;&#24544;&#23454;&#24615;&#65292;&#27491;&#22312;&#36827;&#34892;&#22823;&#37327;&#24037;&#20316;&#12290;&#26377;&#20123;&#26041;&#27861;&#23558;&#25512;&#29702;&#24314;&#27169;&#20026;&#35268;&#21010;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#21017;&#19987;&#27880;&#20110;&#27880;&#37322;&#30340;&#36807;&#31243;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35268;&#21010;&#30340;&#25628;&#32034;&#36807;&#31243;&#24448;&#24448;&#30001;&#20110;&#39057;&#32321;&#35780;&#20272;&#20013;&#38388;&#25512;&#29702;&#29366;&#24577;&#21644;&#24191;&#27867;&#30340;&#25506;&#32034;&#31354;&#38388;&#32780;&#23548;&#33268;&#39640;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30417;&#30563;&#25512;&#29702;&#36807;&#31243;&#23545;&#20110;LLM&#35757;&#32451;&#26469;&#35828;&#26159;&#26114;&#36149;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#36712;&#36857;&#30452;&#25509;&#26681;&#25454;&#21512;&#25104;&#30340;&#36807;&#31243;&#22870;&#21169;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. 
&lt;/p&gt;</description></item><item><title>ComplexityNet&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;LLM&#25512;&#29702;&#25928;&#29575;&#65292;&#36890;&#36807;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#36755;&#20986;&#27010;&#29575;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#24182;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#30830;&#23450;&#26041;&#38754;&#21462;&#24471;&#20102;79%&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11511</link><description>&lt;p&gt;
ComplexityNet: &#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11511
&lt;/p&gt;
&lt;p&gt;
ComplexityNet&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;LLM&#25512;&#29702;&#25928;&#29575;&#65292;&#36890;&#36807;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#36755;&#20986;&#27010;&#29575;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#24182;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#30830;&#23450;&#26041;&#38754;&#21462;&#24471;&#20102;79%&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ComplexityNet&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#20219;&#21153;&#22797;&#26434;&#24615;&#32780;&#35774;&#35745;&#30340;&#31616;&#21270;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19981;&#21516;&#33021;&#21147;&#30340;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#26469;&#39044;&#27979;&#20934;&#30830;&#36755;&#20986;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;Mostly Basic Python Problems&#65288;MBPP&#65289;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20102;ComplexityNet&#12290;&#25105;&#20204;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#31532;&#19968;&#32452;&#26631;&#31614;&#26469;&#23450;&#20041;&#20219;&#21153;&#22797;&#26434;&#24615;&#12290;ComplexityNet&#22312;&#30830;&#23450;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;79%&#20934;&#30830;&#29575;&#65292;&#36739;&#21407;&#22987;&#12289;&#38750;&#24494;&#35843;&#27169;&#22411;&#30340;34%&#20934;&#30830;&#29575;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#19982;&#20351;&#29992;&#26368;&#39640;&#22797;&#26434;&#24615;&#27169;&#22411;&#30456;&#27604;&#65292;ComplexityNet&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;86.7%&#30340;&#39640;&#20195;&#30721;&#29983;&#25104;&#20934;&#30830;&#29575;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#24494;&#35843;&#36739;&#23567;&#30340;&#27169;&#22411;&#26469;&#23545;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#26356;&#24179;&#34913;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11511v2 Announce Type: replace-cross  Abstract: We present ComplexityNet, a streamlined language model designed for assessing task complexity. This model predicts the likelihood of accurate output by various language models, each with different capabilities. Our initial application of ComplexityNet involves the Mostly Basic Python Problems (MBPP) dataset. We pioneered the creation of the first set of labels to define task complexity. ComplexityNet achieved a notable 79% accuracy in determining task complexity, a significant improvement over the 34% accuracy of the original, non fine-tuned model. Furthermore, ComplexityNet effectively reduces computational resource usage by 90% compared to using the highest complexity model, while maintaining a high code generation accuracy of 86.7%. This study demonstrates that fine-tuning smaller models to categorize tasks based on their complexity can lead to a more balanced trade-off between accuracy and efficiency in the use of Large Lan
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#38405;&#35835;&#20020;&#24202;&#31508;&#35760;&#65292;&#24739;&#32773;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35299;&#21644;&#33258;&#20449;&#12290;&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;&#21644;&#22686;&#21152;&#19978;&#19979;&#25991;&#65292;&#20351;&#20020;&#24202;&#31508;&#35760;&#26356;&#26131;&#35835;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22686;&#24378;&#23545;&#24739;&#32773;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2401.09637</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#23545;&#24739;&#32773;&#38405;&#35835;&#20020;&#24202;&#31508;&#35760;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study. (arXiv:2401.09637v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09637
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#38405;&#35835;&#20020;&#24202;&#31508;&#35760;&#65292;&#24739;&#32773;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35299;&#21644;&#33258;&#20449;&#12290;&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;&#21644;&#22686;&#21152;&#19978;&#19979;&#25991;&#65292;&#20351;&#20020;&#24202;&#31508;&#35760;&#26356;&#26131;&#35835;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22686;&#24378;&#23545;&#24739;&#32773;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#36890;&#36807;&#38405;&#35835;&#20182;&#20204;&#30340;&#20020;&#24202;&#31508;&#35760;&#33719;&#24471;&#20102;&#35768;&#22810;&#22909;&#22788;&#65292;&#21253;&#25324;&#22686;&#21152;&#23545;&#33258;&#36523;&#20581;&#24247;&#30340;&#25511;&#21046;&#24863;&#21644;&#23545;&#25252;&#29702;&#35745;&#21010;&#30340;&#29702;&#35299;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#31508;&#35760;&#20013;&#22797;&#26434;&#30340;&#21307;&#23398;&#27010;&#24565;&#21644;&#26415;&#35821;&#38459;&#30861;&#20102;&#24739;&#32773;&#30340;&#29702;&#35299;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#28966;&#34385;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#24739;&#32773;&#30340;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31616;&#21270;&#31508;&#35760;&#12289;&#20174;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#22686;&#21152;&#19978;&#19979;&#25991;&#65292;&#20197;&#20351;&#20020;&#24202;&#31508;&#35760;&#26356;&#26131;&#35835;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#24037;&#20855;&#25552;&#31034;&#25913;&#36827;&#30340;GPT-4&#23545;&#30001;&#20083;&#33146;&#30284;&#24184;&#23384;&#32773;&#25424;&#36192;&#30340;&#30495;&#23454;&#20020;&#24202;&#31508;&#35760;&#21644;&#20020;&#24202;&#21307;&#29983;&#29983;&#25104;&#30340;&#21512;&#25104;&#20020;&#24202;&#31508;&#35760;&#36827;&#34892;&#36825;&#20123;&#22686;&#24378;&#20219;&#21153;&#12290;&#20849;&#26377;12&#26465;&#31508;&#35760;&#65292;3868&#20010;&#23383;&#12290;2023&#24180;6&#26376;&#65292;&#25105;&#20204;&#38543;&#26426;&#20998;&#37197;&#20102;200&#21517;&#32654;&#22269;&#22899;&#24615;&#21442;&#19982;&#32773;&#65292;&#24182;&#21521;&#20182;&#20204;&#20998;&#21457;&#20102;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#22686;&#24378;&#30340;&#20020;&#24202;&#31508;&#35760;&#12290;&#21442;&#19982;&#32773;&#22238;&#31572;&#20102;&#26377;&#20851;&#27599;&#20010;&#31508;&#35760;&#30340;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#20182;&#20204;&#23545;&#21518;&#32493;&#34892;&#21160;&#30340;&#29702;&#35299;&#21644;&#33258;&#25105;&#25253;&#21578;&#30340;&#33258;&#20449;&#24515;&#12290;&#25105;&#20204;&#21457;&#29616;&#22686;&#24378;&#23545;&#38405;&#35835;&#29702;&#35299;&#21644;&#33258;&#20449;&#24515;&#21451;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients derive numerous benefits from reading their clinical notes, including an increased sense of control over their health and improved understanding of their care plan. However, complex medical concepts and jargon within clinical notes hinder patient comprehension and may lead to anxiety. We developed a patient-facing tool to make clinical notes more readable, leveraging large language models (LLMs) to simplify, extract information from, and add context to notes. We prompt engineered GPT-4 to perform these augmentation tasks on real clinical notes donated by breast cancer survivors and synthetic notes generated by a clinician, a total of 12 notes with 3868 words. In June 2023, 200 female-identifying US-based participants were randomly assigned three clinical notes with varying levels of augmentations using our tool. Participants answered questions about each note, evaluating their understanding of follow-up actions and self-reported confidence. We found that augmentations were ass
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#32447;&#24615;&#21464;&#25442;&#22120;&#20316;&#20026;transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;transformers&#22312;&#22788;&#29702;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#25512;&#26029;&#25104;&#26412;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15719</link><description>&lt;p&gt;
&#24490;&#29615;&#32447;&#24615;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Linear Transformers. (arXiv:2310.15719v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#32447;&#24615;&#21464;&#25442;&#22120;&#20316;&#20026;transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;transformers&#22312;&#22788;&#29702;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#25512;&#26029;&#25104;&#26412;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
transformer&#26550;&#26500;&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#33021;&#22815;&#25429;&#25417;&#38271;&#36317;&#31163;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20063;&#26159;&#20854;&#22312;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#26102;&#26377;&#25928;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#25104;&#21151;&#65292;transformers&#20173;&#28982;&#26377;&#20004;&#20010;&#37325;&#22823;&#32570;&#28857;&#65292;&#38480;&#21046;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65306;(1)&#20026;&#20102;&#35760;&#20303;&#36807;&#21435;&#30340;&#20449;&#24687;&#65292;&#33258;&#27880;&#24847;&#26426;&#21046;&#38656;&#35201;&#35775;&#38382;&#25972;&#20010;&#21382;&#21490;&#20449;&#24687;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;(2)transformers&#30340;&#25512;&#26029;&#25104;&#26412;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#24490;&#29615;&#26367;&#20195;&#26041;&#26696;&#65292;&#20854;&#20855;&#26377;&#29420;&#31435;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#26029;&#25104;&#26412;&#24182;&#26377;&#25928;&#22320;&#21033;&#29992;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#19978;&#36848;&#35745;&#31639;&#38480;&#21046;&#20960;&#20046;&#20351;&#24471;transformers&#30340;&#24212;&#29992;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35786;&#26029;&#29615;&#22659;&#20013;&#37327;&#21270;&#20102;&#25105;&#20204;&#26550;&#26500;&#20013;&#19981;&#21516;&#37096;&#20998;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-attention mechanism in the transformer architecture is capable of capturing long-range dependencies and it is the main reason behind its effectiveness in processing sequential data. Nevertheless, despite their success, transformers have two significant drawbacks that still limit their broader applicability: (1) In order to remember past information, the self-attention mechanism requires access to the whole history to be provided as context. (2) The inference cost in transformers is expensive. In this paper we introduce recurrent alternatives to the transformer self-attention mechanism that offer a context-independent inference cost, leverage long-range dependencies effectively, and perform well in practice. We evaluate our approaches in reinforcement learning problems where the aforementioned computational limitations make the application of transformers nearly infeasible. We quantify the impact of the different components of our architecture in a diagnostic environment and as
&lt;/p&gt;</description></item><item><title>GraphMaker&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.13833</link><description>&lt;p&gt;
GraphMaker: &#25193;&#25955;&#27169;&#22411;&#33021;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?. (arXiv:2310.13833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13833
&lt;/p&gt;
&lt;p&gt;
GraphMaker&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20855;&#26377;&#33410;&#28857;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#22270;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#21019;&#24314;&#19982;&#30495;&#23454;&#19990;&#30028;&#31034;&#20363;&#31867;&#20284;&#30340;&#21512;&#25104;&#12289;&#23500;&#23646;&#24615;&#22270;&#23545;&#20110;&#20849;&#20139;&#22270;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#21644;&#24320;&#21457;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#24403;&#21407;&#22987;&#25968;&#25454;&#38480;&#21046;&#34987;&#20849;&#20139;&#26102;&#12290;&#20256;&#32479;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#20123;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#27809;&#26377;&#23646;&#24615;&#21644;&#36739;&#23567;&#30340;&#20998;&#23376;&#22270;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#26041;&#38754;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#23646;&#24615;-&#32467;&#26500;&#30456;&#20851;&#24615;&#21644;&#22270;&#30340;&#22823;&#35268;&#27169;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#65306;GraphMaker&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#29983;&#25104;&#36807;&#31243;&#30340;&#32452;&#21512;&#65292;&#21457;&#29616;&#24322;&#27493;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#20869;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale graphs with node attributes are increasingly common in various real-world applications. Creating synthetic, attribute-rich graphs that mirror real-world examples is crucial, especially for sharing graph data for analysis and developing learning models when original data is restricted to be shared. Traditional graph generation methods are limited in their capacity to handle these complex structures. Recent advances in diffusion models have shown potential in generating graph structures without attributes and smaller molecular graphs. However, these models face challenges in generating large attributed graphs due to the complex attribute-structure correlations and the large size of these graphs. This paper introduces a novel diffusion model, GraphMaker, specifically designed for generating large attributed graphs. We explore various combinations of node attribute and graph structure generation processes, finding that an asynchronous approach more effectively captures the intr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#30340;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#22522;&#26412;LLM&#19978;&#21152;&#20837;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#39044;&#27979;&#32676;&#20307;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;GPO&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11523</link><description>&lt;p&gt;
&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Group Preference Optimization: Few-Shot Alignment of Large Language Models. (arXiv:2310.11523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11523
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#30340;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#22522;&#26412;LLM&#19978;&#21152;&#20837;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#39044;&#27979;&#32676;&#20307;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;GPO&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35768;&#22810;&#24212;&#29992;&#65292;&#20174;&#32842;&#22825;&#26426;&#22120;&#20154;&#21040;&#21019;&#24847;&#20889;&#20316;&#65292;&#37117;&#38656;&#35201;&#32454;&#33268;&#20837;&#24494;&#30340;&#20027;&#35266;&#21028;&#26029;&#65292;&#36825;&#20123;&#21028;&#26029;&#22312;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#31639;&#27861;&#22312;&#27599;&#20010;&#32676;&#20307;&#19978;&#23545;&#40784;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#32780;&#35328;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#32676;&#20307;&#29305;&#23450;&#20559;&#22909;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#22312;GPO&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#25193;&#20805;&#22522;&#26412;LLM&#65292;&#29992;&#20110;&#39044;&#27979;&#32676;&#20307;&#23545;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20559;&#22909;&#12290;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#27169;&#22359;&#21442;&#25968;&#21270;&#20026;&#19968;&#20010;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#30340;transformer&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#22810;&#20010;&#32676;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;LLM&#22312;&#19977;&#20010;&#20154;&#31867;&#24847;&#35265;&#36866;&#24212;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;GPO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65292;&#21363;&#36127;&#36131;&#32534;&#30721;&#29305;&#23450;&#30693;&#35782;&#30340;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21448;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.03084</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Discovering Knowledge-Critical Subnetworks in Pretrained Language Models. (arXiv:2310.03084v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65292;&#21363;&#36127;&#36131;&#32534;&#30721;&#29305;&#23450;&#30693;&#35782;&#30340;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21448;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#38544;&#21547;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23450;&#20301;&#36825;&#20123;&#34920;&#31034;&#24182;&#23558;&#20854;&#35299;&#31163;&#20986;&#26469;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21253;&#21547;&#20102;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65306;&#36127;&#36131;&#32534;&#30721;&#27169;&#22411;&#25152;&#35760;&#24518;&#30340;&#29305;&#23450;&#30693;&#35782;&#30340;&#29305;&#23450;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#26469;&#21457;&#29616;&#36825;&#20123;&#23376;&#32593;&#32476;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#26469;&#31934;&#30830;&#22320;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;GPT2&#21464;&#20307;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#39640;&#24230;&#31232;&#30095;&#23376;&#32593;&#32476;&#65288;98%+&#65289;&#65292;&#23427;&#20204;&#20165;&#36127;&#36131;&#29305;&#23450;&#30340;&#20851;&#31995;&#30693;&#35782;&#38598;&#21512;&#12290;&#24403;&#21024;&#38500;&#36825;&#20123;&#23376;&#32593;&#32476;&#26102;&#65292;&#21097;&#20313;&#30340;&#32593;&#32476;&#20173;&#20445;&#25345;&#20102;&#22823;&#37096;&#20998;&#20854;&#21021;&#22987;&#23481;&#37327;&#65288;&#23545;&#35821;&#35328;&#21644;&#20854;&#20182;&#35760;&#24518;&#20851;&#31995;&#30340;&#24314;&#27169;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.07601</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#20449;&#21495;&#21644;&#24369;&#30417;&#30563;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#24230;&#20449;&#21495;&#20195;&#34920;&#20102;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#21592;&#36890;&#24120;&#29992;&#26469;&#35780;&#20272;&#22312;&#32447;&#20869;&#23481;&#30495;&#23454;&#24615;&#30340;&#19968;&#31995;&#21015;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#21487;&#20449;&#24230;&#20449;&#21495;&#25552;&#21462;&#30340;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#35757;&#32451;&#39640;&#20934;&#30830;&#29575;&#30340;&#29305;&#23450;&#20449;&#21495;&#25552;&#21462;&#22120;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#23545;&#25152;&#26377;&#21487;&#20449;&#24230;&#20449;&#21495;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#19968;&#32452;18&#20010;&#21487;&#20449;&#24230;&#20449;&#21495;&#26469;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20197;&#20135;&#29983;&#27599;&#20010;&#20449;&#21495;&#30340;&#24369;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#36825;&#20123;&#28508;&#22312;&#30340;&#22122;&#22768;&#26631;&#31614;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#32467;&#21512;&#20102;&#38646;-shot LLM&#21487;&#20449;&#24230;&#20449;&#21495;&#26631;&#27880;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#65292;&#32780;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#35757;&#32451;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credibility signals represent a wide range of heuristics that are typically used by journalists and fact-checkers to assess the veracity of online content. Automating the task of credibility signal extraction, however, is very challenging as it requires high-accuracy signal-specific extractors to be trained, while there are currently no sufficiently large datasets annotated with all credibility signals. This paper investigates whether large language models (LLMs) can be prompted effectively with a set of 18 credibility signals to produce weak labels for each signal. We then aggregate these potentially noisy labels using weak supervision in order to predict content veracity. We demonstrate that our approach, which combines zero-shot LLM credibility signal labeling and weak supervision, outperforms state-of-the-art classifiers on two misinformation datasets without using any ground-truth labels for training. We also analyse the contribution of the individual credibility signals towards p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;mesa-optimization&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20869;&#37096;&#23398;&#20064;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#39537;&#21160;&#39044;&#27979;&#29983;&#25104;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;&#23398;&#20064;&#30340;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;mesa-optimization&#21487;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2309.05858</link><description>&lt;p&gt;
&#25581;&#31034;Transformer&#20013;&#30340;mesa-optimization&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Uncovering mesa-optimization algorithms in Transformers. (arXiv:2309.05858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;mesa-optimization&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20869;&#37096;&#23398;&#20064;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#39537;&#21160;&#39044;&#27979;&#29983;&#25104;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;&#23398;&#20064;&#30340;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;mesa-optimization&#21487;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#20027;&#23548;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#21331;&#36234;&#24615;&#33021;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20551;&#35774;Transformer&#30340;&#24378;&#22823;&#24615;&#33021;&#28304;&#20110;&#20854;&#26550;&#26500;&#20013;&#23545;mesa-optimization&#30340;&#20559;&#22909;&#65292;&#21363;&#19968;&#20010;&#23398;&#20064;&#36807;&#31243;&#22312;&#27169;&#22411;&#30340;&#21069;&#21521;&#20256;&#36882;&#20013;&#36816;&#34892;&#65292;&#30001;&#20197;&#19979;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65306;&#65288;i&#65289;&#26500;&#24314;&#20869;&#37096;&#23398;&#20064;&#30446;&#26631;&#65292;&#21644;&#65288;ii&#65289;&#36890;&#36807;&#20248;&#21270;&#25214;&#21040;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#22312;&#31616;&#21333;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;Transformer&#36827;&#34892;&#20102;&#36870;&#21521;&#24037;&#31243;&#65292;&#25581;&#31034;&#20102;&#39537;&#21160;&#39044;&#27979;&#29983;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24213;&#23618;mesa-optimization&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#21069;&#21521;&#20256;&#36882;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#31435;&#21363;&#34987;&#37325;&#26032;&#24212;&#29992;&#20110;&#35299;&#20915;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#36825;&#34920;&#26126;mesa-optimization&#21487;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention 
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26045;&#21152;&#28966;&#34385;&#33021;&#24433;&#21709;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;</title><link>http://arxiv.org/abs/2304.11111</link><description>&lt;p&gt;
&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28966;&#34385;&#20250;&#22686;&#21152;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Inducing anxiety in large language models increases exploration and bias. (arXiv:2304.11111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11111
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26045;&#21152;&#28966;&#34385;&#33021;&#24433;&#21709;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#25913;&#21464;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65292;&#24341;&#21457;&#20844;&#20247;&#30340;&#36777;&#35770;&#12290;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#20309;&#26102;&#33021;&#22815;&#27491;&#24120;&#24037;&#20316;&#21644;&#25104;&#21151;&#65292;&#20063;&#20026;&#20160;&#20040;&#20250;&#22833;&#36133;&#21644;&#34892;&#20026;&#22833;&#24120;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#31038;&#20250;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#35745;&#31639;&#31934;&#31070;&#30149;&#23398;&#30340;&#35270;&#35282;&#36716;&#21521;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;Generative Pre-Trained Transformer 3.5&#65292;&#24182;&#23558;&#20854;&#32622;&#20110;&#31934;&#31070;&#30149;&#23398;&#20013;&#24120;&#35265;&#30340;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#23545;&#24120;&#35265;&#30340;&#28966;&#34385;&#38382;&#21367;&#20570;&#20986;&#26377;&#21147;&#30340;&#21453;&#24212;&#65292;&#20135;&#29983;&#27604;&#20154;&#31867;&#20027;&#20307;&#26356;&#39640;&#30340;&#28966;&#34385;&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#24773;&#32490;&#24863;&#24212;&#25552;&#31034;&#21487;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;GPT-3.5&#30340;&#21453;&#24212;&#12290;&#24773;&#24863;&#24863;&#24212;&#19981;&#20165;&#24433;&#21709;GPT-3.5&#22312;&#34913;&#37327;&#25506;&#32034;&#20915;&#31574;-making&#30340;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#36824;&#24433;&#21709;&#20854;&#22312;&#20043;&#21069;&#24314;&#31435;&#30340;&#34913;&#37327;&#31181;&#26063;&#20027;&#20041;&#21644;&#22833;&#33021;&#20027;&#20041;&#31561;&#20559;&#35265;&#30340;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;GPT-3.5&#22312;&#21463;&#21040;&#28966;&#34385;&#35825;&#23548;&#26102;&#21576;&#29616;&#20986;&#26126;&#26174;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#22686;&#21152;&#65292;&#34920;&#26126;&#20854;&#36755;&#20986;&#23481;&#26131;&#21463;&#21040;&#24773;&#24863;&#25805;&#32437;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#20351;&#29992;&#36807;&#31243;&#20013;&#38656;&#35201;&#26356;&#22810;&#30340;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are transforming research on machine learning while galvanizing public debates. Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance. We propose to turn the lens of computational psychiatry, a framework used to computationally describe and modify aberrant behavior, to the outputs produced by these models. We focus on the Generative Pre-Trained Transformer 3.5 and subject it to tasks commonly studied in psychiatry. Our results show that GPT-3.5 responds robustly to a common anxiety questionnaire, producing higher anxiety scores than human subjects. Moreover, GPT-3.5's responses can be predictably changed by using emotion-inducing prompts. Emotion-induction not only influences GPT-3.5's behavior in a cognitive task measuring exploratory decision-making but also influences its behavior in a previously-established task measuring biases such as racism and ableism. Crucially, GPT-3.5 shows a s
&lt;/p&gt;</description></item></channel></rss>