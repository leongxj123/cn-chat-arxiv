<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#38469;&#35774;&#32622;&#65292;&#31216;&#20026;&#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#33258;&#36866;&#24212;&#31243;&#24207;&#35268;&#21010;&#65292;&#20811;&#26381;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#27493;&#39588;&#38271;&#24230;&#21464;&#21270;&#30340;&#27169;&#22411;&#19981;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12289;&#29702;&#35299;&#27493;&#39588;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#30693;&#35782;&#23545;&#20110;&#29983;&#25104;&#21512;&#29702;&#19988;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#20197;&#21450;&#29992;&#27493;&#39588;&#32423;&#26631;&#31614;&#25110;&#24207;&#21015;&#32423;&#26631;&#31614;&#26631;&#27880;&#25351;&#23548;&#35270;&#39057;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.18600</link><description>&lt;p&gt;
RAP&#65306;&#26816;&#32034;&#22686;&#24378;&#22411;&#35268;&#21010;&#22120;&#29992;&#20110;&#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#33258;&#36866;&#24212;&#31243;&#24207;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18600
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#38469;&#35774;&#32622;&#65292;&#31216;&#20026;&#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#33258;&#36866;&#24212;&#31243;&#24207;&#35268;&#21010;&#65292;&#20811;&#26381;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#27493;&#39588;&#38271;&#24230;&#21464;&#21270;&#30340;&#27169;&#22411;&#19981;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12289;&#29702;&#35299;&#27493;&#39588;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#30693;&#35782;&#23545;&#20110;&#29983;&#25104;&#21512;&#29702;&#19988;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#20197;&#21450;&#29992;&#27493;&#39588;&#32423;&#26631;&#31614;&#25110;&#24207;&#21015;&#32423;&#26631;&#31614;&#26631;&#27880;&#25351;&#23548;&#35270;&#39057;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#31243;&#24207;&#35268;&#21010;&#28041;&#21450;&#26681;&#25454;&#21021;&#22987;&#21644;&#30446;&#26631;&#29366;&#24577;&#30340;&#35270;&#35273;&#35266;&#23519;&#29983;&#25104;&#19968;&#31995;&#21015;&#21160;&#20316;&#27493;&#39588;&#12290;&#23613;&#31649;&#36825;&#19968;&#20219;&#21153;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#38656;&#35201;&#35299;&#20915;&#65306;&#65288;1&#65289;&#33258;&#36866;&#24212;&#31243;&#24207;&#65306;&#20808;&#21069;&#30340;&#24037;&#20316;&#23384;&#22312;&#19968;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#21363;&#21160;&#20316;&#27493;&#39588;&#30340;&#25968;&#37327;&#26159;&#24050;&#30693;&#19988;&#22266;&#23450;&#30340;&#65292;&#23548;&#33268;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27493;&#39588;&#38271;&#24230;&#21464;&#21270;&#30340;&#27169;&#22411;&#19981;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#65288;2&#65289;&#26102;&#38388;&#20851;&#31995;&#65306;&#29702;&#35299;&#27493;&#39588;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#30693;&#35782;&#23545;&#20110;&#29983;&#25104;&#21512;&#29702;&#19988;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#65288;3&#65289;&#27880;&#37322;&#25104;&#26412;&#65306;&#29992;&#27493;&#39588;&#32423;&#26631;&#31614;&#65288;&#21363;&#26102;&#38388;&#25139;&#65289;&#25110;&#24207;&#21015;&#32423;&#26631;&#31614;&#65288;&#21363;&#21160;&#20316;&#31867;&#21035;&#65289;&#26631;&#27880;&#25351;&#23548;&#35270;&#39057;&#26159;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#38469;&#35774;&#32622;&#65292;&#31216;&#20026;&#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#33258;&#36866;&#24212;&#31243;&#24207;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18600v1 Announce Type: cross  Abstract: Procedure Planning in instructional videos entails generating a sequence of action steps based on visual observations of the initial and target states. Despite the rapid progress in this task, there remain several critical challenges to be solved: (1) Adaptive procedures: Prior works hold an unrealistic assumption that the number of action steps is known and fixed, leading to non-generalizable models in real-world scenarios where the sequence length varies. (2) Temporal relation: Understanding the step temporal relation knowledge is essential in producing reasonable and executable plans. (3) Annotation cost: Annotating instructional videos with step-level labels (i.e., timestamp) or sequence-level labels (i.e., action category) is demanding and labor-intensive, limiting its generalizability to large-scale datasets.In this work, we propose a new and practical setting, called adaptive procedure planning in instructional videos, where the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.00781</link><description>&lt;p&gt;
ChatDiet&#65306;&#36890;&#36807;LLM&#22686;&#24378;&#26694;&#26550;&#36171;&#33021;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#23545;&#20581;&#24247;&#30340;&#28145;&#36828;&#24433;&#21709;&#20351;&#24471;&#20808;&#36827;&#30340;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#26381;&#21153;&#25104;&#20026;&#24517;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#20010;&#24615;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20114;&#21160;&#24615;&#31561;&#20851;&#38190;&#20803;&#32032;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#20204;&#21333;&#29420;&#30340;&#20351;&#29992;&#26410;&#33021;&#23454;&#29616;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#39537;&#21160;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChatDiet&#38598;&#25104;&#20102;&#20010;&#20154;&#21644;&#20154;&#32676;&#27169;&#22411;&#65292;&#36741;&#20197;&#19968;&#20010;&#21327;&#35843;&#22120;&#65292;&#26080;&#32541;&#26816;&#32034;&#21644;&#22788;&#29702;&#30456;&#20851;&#20449;&#24687;&#12290;&#20854;&#32467;&#26524;&#26159;&#21160;&#24577;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#39135;&#21697;&#25512;&#33616;&#65292;&#26681;&#25454;&#20010;&#20154;&#29992;&#25143;&#21916;&#22909;&#23450;&#21046;&#12290;&#25105;&#20204;&#23545;ChatDiet&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#26696;&#20363;&#30740;&#31350;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#22240;&#26524;&#20010;&#20154;&#27169;&#22411;&#26469;&#20272;&#35745;&#20010;&#20154;&#33829;&#20859;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#21463;&#38480;&#65292;&#26080;&#27861;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;MatSci-LLMs&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.05200</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#20934;&#22791;&#22909;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#26448;&#26009;&#21457;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Ready for Real-World Materials Discovery?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05200
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#21463;&#38480;&#65292;&#26080;&#27861;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;MatSci-LLMs&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24378;&#22823;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#65292;&#21152;&#24555;&#20102;&#26448;&#26009;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#23384;&#22312;&#19981;&#36275;&#65292;&#26080;&#27861;&#25104;&#20026;&#23454;&#29992;&#30340;&#26448;&#26009;&#31185;&#23398;&#24037;&#20855;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#30456;&#20851;&#22833;&#36133;&#26696;&#20363;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#29702;&#35299;&#21644;&#25512;&#29702;&#22797;&#26434;&#12289;&#30456;&#20114;&#20851;&#32852;&#30340;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#29616;&#26377;&#38480;&#21046;&#12290;&#37492;&#20110;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21457;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#29983;&#25104;&#19982;&#27979;&#35797;&#30340;&#26448;&#26009;&#31185;&#23398;LLMs&#65288;MatSci-LLMs&#65289;&#30340;&#26694;&#26550;&#12290;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;MatSci-LLMs&#30340;&#36335;&#24452;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#24314;&#31435;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#31185;&#23398;&#25991;&#29486;&#65292;&#20854;&#20013;&#23384;&#22312;&#21508;&#31181;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction cha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#20027;&#20379;&#24212;&#38142;&#65288;ASC&#65289;&#30340;&#27491;&#24335;&#23450;&#20041;&#12289;&#29305;&#24449;&#21644;&#36741;&#21161;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#27010;&#24565;&#26694;&#26550;&#21644;&#20379;&#24212;&#38142;&#33258;&#27835;&#21442;&#32771;&#27169;&#22411;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#36825;&#20123;&#27010;&#24565;&#30340;&#21021;&#22987;ASC&#23454;&#26045;&#12290;</title><link>http://arxiv.org/abs/2401.14183</link><description>&lt;p&gt;
&#24378;&#35843;&#33258;&#20027;&#20379;&#24212;&#38142;&#65306;&#23450;&#20041;&#12289;&#29305;&#24449;&#12289;&#27010;&#24565;&#26694;&#26550;&#21644;&#33258;&#20027;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Towards Autonomous Supply Chains: Definition, Characteristics, Conceptual Framework, and Autonomy Levels. (arXiv:2401.14183v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#20027;&#20379;&#24212;&#38142;&#65288;ASC&#65289;&#30340;&#27491;&#24335;&#23450;&#20041;&#12289;&#29305;&#24449;&#21644;&#36741;&#21161;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#27010;&#24565;&#26694;&#26550;&#21644;&#20379;&#24212;&#38142;&#33258;&#27835;&#21442;&#32771;&#27169;&#22411;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#36825;&#20123;&#27010;&#24565;&#30340;&#21021;&#22987;ASC&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20840;&#29699;&#24615;&#24178;&#25200;&#65292;&#22914;&#22823;&#27969;&#34892;&#21644;&#22320;&#32536;&#25919;&#27835;&#20914;&#31361;&#65292;&#28145;&#21051;&#26292;&#38706;&#20102;&#20256;&#32479;&#20379;&#24212;&#38142;&#30340;&#33030;&#24369;&#24615;&#65292;&#38656;&#35201;&#25506;&#32034;&#26356;&#26377;&#24377;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#33258;&#20027;&#20379;&#24212;&#38142;&#65288;ASC&#65289;&#20316;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#25552;&#20379;&#20102;&#22312;&#21160;&#33633;&#30340;&#36152;&#26131;&#29615;&#22659;&#20013;&#22686;&#21152;&#21487;&#35265;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#24377;&#24615;&#30340;&#20248;&#21183;&#12290;&#23613;&#31649;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#22810;&#24180;&#26469;&#19968;&#30452;&#22312;&#35752;&#35770;ASC&#65292;&#20294;&#32570;&#20047;&#33391;&#22909;&#24314;&#31435;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;ASC&#30340;&#27491;&#24335;&#23450;&#20041;&#20197;&#21450;&#20854;&#23450;&#20041;&#29305;&#24449;&#21644;&#36741;&#21161;&#27010;&#24565;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MIISI&#27169;&#22411;&#30340;&#20998;&#23618;&#27010;&#24565;&#26694;&#26550;&#12290;&#36890;&#36807;&#19968;&#20010;&#37325;&#28857;&#20851;&#27880;&#32905;&#31867;&#20379;&#24212;&#38142;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;&#36825;&#19968;&#27010;&#24565;&#27169;&#22411;&#30340;&#21021;&#22987;ASC&#23454;&#26045;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#19971;&#32423;&#20379;&#24212;&#38142;&#33258;&#27835;&#21442;&#32771;&#27169;&#22411;&#65292;&#25551;&#32472;&#20102;&#23454;&#29616;&#23436;&#20840;&#20379;&#24212;&#38142;&#33258;&#27835;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent global disruptions, such as the pandemic and geopolitical conflicts, have profoundly exposed vulnerabilities in traditional supply chains, requiring exploration of more resilient alternatives. Autonomous supply chains (ASCs) have emerged as a potential solution, offering increased visibility, flexibility, and resilience in turbulent trade environments. Despite discussions in industry and academia over several years, ASCs lack well-established theoretical foundations. This paper addresses this research gap by presenting a formal definition of ASC along with its defining characteristics and auxiliary concepts. We propose a layered conceptual framework called the MIISI model. An illustrative case study focusing on the meat supply chain demonstrates an initial ASC implementation based on this conceptual model. Additionally, we introduce a seven-level supply chain autonomy reference model, delineating a trajectory towards achieving a full supply chain autonomy. Recognising that this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.03302</link><description>&lt;p&gt;
&#34892;&#21160;&#20013;&#30340;&#29616;&#23454;&#20027;&#20041;&#65306;&#20351;&#29992;YOLOv8&#21644;DeiT&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#35786;&#26029;&#33041;&#32959;&#30244;&#30340;&#24322;&#24120;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31185;&#23398;&#39046;&#22495;&#65292;&#30001;&#20110;&#33041;&#32959;&#30244;&#22312;&#24739;&#32773;&#20013;&#30340;&#32597;&#35265;&#31243;&#24230;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#24322;&#24120;&#24773;&#20917;&#19979;&#26816;&#27979;&#32959;&#30244;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#12290;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#65288;NBML&#65289;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#21253;&#25324;81&#21517;&#24739;&#32773;&#65292;&#20854;&#20013;&#21253;&#25324;30&#20363;&#32959;&#30244;&#30149;&#20363;&#21644;51&#20363;&#27491;&#24120;&#30149;&#20363;&#12290;&#26816;&#27979;&#21644;&#20998;&#31867;&#27969;&#31243;&#34987;&#20998;&#20026;&#20004;&#20010;&#36830;&#32493;&#30340;&#20219;&#21153;&#12290;&#26816;&#27979;&#38454;&#27573;&#21253;&#25324;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#39044;&#22788;&#29702;&#65292;&#20197;&#20462;&#25913;&#22270;&#20687;&#26679;&#26412;&#21644;&#27599;&#20010;&#31867;&#21035;&#30340;&#24739;&#32773;&#25968;&#37327;&#65292;&#20197;&#31526;&#21512;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#20998;&#24067;&#65288;9&#20010;&#27491;&#24120;&#26679;&#26412;&#23545;&#24212;1&#20010;&#32959;&#30244;&#26679;&#26412;&#65289;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#20013;&#38500;&#20102;&#24120;&#35265;&#30340;&#35780;&#20272;&#25351;&#26631;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;... [&#25688;&#35201;&#38271;&#24230;&#24050;&#36798;&#21040;&#19978;&#38480;]
&lt;/p&gt;
&lt;p&gt;
In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#22827;&#22971;&#30340;&#21307;&#38498;/&#23621;&#27665;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#20197;&#21450;&#23545;&#31283;&#23450;B&#21305;&#37197;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#25509;&#36817;&#21487;&#34892;&#30340;&#31283;&#23450;&#21305;&#37197;&#65292;&#24182;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2311.00405</link><description>&lt;p&gt;
&#22827;&#22971;&#21487;&#20197;&#34987;&#35299;&#20915;&#65306;&#26032;&#30340;&#31639;&#27861;&#21644;&#23545;&#24102;&#22827;&#22971;&#30340;&#21307;&#38498;/&#23621;&#27665;&#38382;&#39064;&#30340;&#38590;&#24230;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Couples can be tractable: New algorithms and hardness results for the Hospitals / Residents problem with Couples. (arXiv:2311.00405v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#22827;&#22971;&#30340;&#21307;&#38498;/&#23621;&#27665;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#20197;&#21450;&#23545;&#31283;&#23450;B&#21305;&#37197;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#25509;&#36817;&#21487;&#34892;&#30340;&#31283;&#23450;&#21305;&#37197;&#65292;&#24182;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#22827;&#22971;&#30340;&#21307;&#38498;/&#23621;&#27665;&#38382;&#39064;&#65288;Hospitals / Residents problem with Couples&#65292;&#31616;&#31216;HRC&#65289;&#65292;&#20854;&#20013;&#35299;&#30340;&#19968;&#31181;&#26159;&#31283;&#23450;&#21305;&#37197;&#65292;&#25110;&#32773;&#25253;&#21578;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#24102;&#22827;&#22971;&#30340;HRC&#23454;&#20363;&#20013;&#25214;&#21040;&#19968;&#20010;&#25509;&#36817;&#21487;&#34892;&#30340;&#31283;&#23450;&#21305;&#37197;&#65288;&#36890;&#36807;&#26368;&#22810;&#35843;&#25972;&#21307;&#38498;&#30340;&#23481;&#37327;1&#20010;&#21333;&#20301;&#65289;&#65292;&#20854;&#20013;&#22827;&#22971;&#30340;&#20559;&#22909;&#26159;&#23376;&#21709;&#24212;&#24615;&#30340;&#65288;&#21363;&#65292;&#22914;&#26524;&#19968;&#20010;&#25104;&#21592;&#36716;&#31227;&#21040;&#19968;&#20010;&#26356;&#22909;&#30340;&#21307;&#38498;&#65292;&#37027;&#20040;&#22827;&#22971;&#20063;&#20250;&#21464;&#24471;&#26356;&#22909;&#65289;&#21644;&#23376;&#23436;&#22791;&#24615;&#30340;&#65288;&#21363;&#65292;&#23545;&#20110;&#27599;&#23545;&#20010;&#21035;&#21487;&#25509;&#21463;&#30340;&#21307;&#38498;&#37117;&#26159;&#22827;&#22971;&#19968;&#36215;&#21487;&#25509;&#21463;&#30340;&#65289;&#65292;&#36890;&#36807;&#23558;&#20854;&#35268;&#32422;&#21040;&#31283;&#23450;&#22266;&#23450;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;HRC&#23376;&#21709;&#24212;&#24615;&#12289;&#23376;&#23436;&#22791;&#23454;&#20363;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#35813;&#23454;&#20363;&#26159;&#19968;&#20010;&#21452;&#37325;&#24066;&#22330;&#65292;&#25110;&#32773;&#25152;&#26377;&#22827;&#22971;&#23646;&#20110;&#20960;&#31181;&#21487;&#33021;&#31867;&#22411;&#20043;&#19968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20063;&#24847;&#21619;&#30528;&#31283;&#23450;B&#21305;&#37197;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#65292;&#20854;&#20013;&#24213;&#23618;&#22270;&#26159;&#24102;&#26377;&#29615;&#30340;&#22810;&#37325;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we study the {\sc Hospitals / Residents problem with Couples} ({\sc hrc}), where a solution is a stable matching or a report that none exists. We present a novel polynomial-time algorithm that can find a near-feasible stable matching (adjusting the hospitals' capacities by at most 1) in an {\sc hrc} instance where the couples' preferences are sub-responsive (i.e., if one member switches to a better hospital, than the couple also improves) and sub-complete (i.e., each pair of hospitals that are individually acceptable to both members are jointly acceptable for the couple) by reducing it to an instance of the {\sc Stable Fixtures} problem. We also present a polynomial-time algorithm for {\sc hrc} in a sub-responsive, sub-complete instance that is a Dual Market, or where all couples are one of several possible types. We show that our algorithm also implies the polynomial-time solvability of a stable b-matching problem, where the underlying graph is a multigraph with loops.  
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.03234</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#12290;&#30001;&#20110;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#20854;&#35299;&#20915;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;FCCO&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;FCCO&#30340;&#30740;&#31350;&#20551;&#35774;&#20869;&#22806;&#20989;&#25968;&#37117;&#26159;&#20809;&#28369;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#31181;&#31867;&#30340;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20854;&#20013;&#22806;&#20989;&#25968;&#26159;&#24369;&#20984;&#19988;&#38750;&#36882;&#20943;&#30340;&#65292;&#20869;&#20989;&#25968;&#26159;&#24369;&#20984;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26694;&#26550;WASA&#65292;&#36890;&#36807;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#26377;&#23884;&#20837;&#28304;&#20449;&#24687;&#30340;&#21512;&#25104;&#25991;&#26412;&#27700;&#21360;&#26469;&#35299;&#20915;&#28304;&#24402;&#23646;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00646</link><description>&lt;p&gt;
WASA&#65306;&#22522;&#20110;&#27700;&#21360;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#30340;&#28304;&#24402;&#23646;
&lt;/p&gt;
&lt;p&gt;
WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data. (arXiv:2310.00646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26694;&#26550;WASA&#65292;&#36890;&#36807;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#26377;&#23884;&#20837;&#28304;&#20449;&#24687;&#30340;&#21512;&#25104;&#25991;&#26412;&#27700;&#21360;&#26469;&#35299;&#20915;&#28304;&#24402;&#23646;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#20854;&#21830;&#19994;&#21270;&#30340;&#24040;&#22823;&#28508;&#21147;&#24341;&#21457;&#20102;&#23545;&#20854;&#35757;&#32451;&#25968;&#25454;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#30340;&#20005;&#37325;&#20851;&#27880;&#12290;&#29305;&#21035;&#26159;&#65292;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25991;&#26412;&#21487;&#33021;&#20405;&#29359;&#34987;&#29992;&#20110;&#35757;&#32451;LLM&#30340;&#25968;&#25454;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#33021;&#22815;&#65288;a&#65289;&#36890;&#36807;&#27700;&#21360;&#35782;&#21035;&#20986;&#23545;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25991;&#26412;&#20570;&#20986;&#36129;&#29486;&#30340;&#25968;&#25454;&#25552;&#20379;&#32773;&#65288;&#28304;&#24402;&#23646;&#65289;&#65307;&#20197;&#21450;&#65288;b&#65289;&#39564;&#35777;&#25991;&#26412;&#25968;&#25454;&#26159;&#21542;&#26469;&#33258;&#20110;&#26576;&#20010;&#25968;&#25454;&#25552;&#20379;&#32773;&#23545;LLM&#36827;&#34892;&#20102;&#35757;&#32451;&#65288;&#25968;&#25454;&#26469;&#28304;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#27700;&#21360;&#25216;&#26415;&#21487;&#20197;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#35753;LLM&#29983;&#25104;&#20855;&#26377;&#23884;&#20837;&#28304;&#20449;&#24687;&#30340;&#21512;&#25104;&#25991;&#26412;&#27700;&#21360;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31181;&#27700;&#21360;&#25216;&#26415;&#26694;&#26550;&#30340;&#20851;&#38190;&#29305;&#24615;&#65288;&#20363;&#22914;&#28304;&#24402;&#23646;&#20934;&#30830;&#24615;&#12289;&#25269;&#25239;&#23545;&#25163;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#30340;WAtermarking for Source Attribution&#65288;WASA&#65289;&#26694;&#26550;.
&lt;/p&gt;
&lt;p&gt;
The impressive performances of large language models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the intellectual property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to (a) identify the data provider who contributed to the generation of a synthetic text by an LLM (source attribution) and (b) verify whether the text data from a data provider has been used to train an LLM (data provenance). In this paper, we show that both problems can be solved by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a WAtermarking for Source Attribution (WASA) framework that satisfies
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17012</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35748;&#30693;&#20559;&#24046;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Cognitive Biases in Large Language Models as Evaluators. (arXiv:2309.17012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#38750;&#24120;&#26377;&#25928;&#12290;&#26412;&#30740;&#31350;&#32452;&#35013;&#20102;15&#20010;&#22823;&#23567;&#19981;&#21516;&#30340;LLMs&#65292;&#24182;&#36890;&#36807;&#20854;&#20182;LLMs&#30340;&#20559;&#22909;&#25490;&#21517;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#36755;&#20986;&#21709;&#24212;&#65292;&#20363;&#22914;System Star&#27604;System Square&#26356;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#36755;&#20986;&#20013;&#20845;&#31181;&#19981;&#21516;&#35748;&#30693;&#20559;&#24046;&#30340;&#35748;&#30693;&#20559;&#24046;&#22522;&#20934;&#27979;&#35797;&#65288;CoBBLEr&#65289;&#65292;&#22914;&#33258;&#25105;&#20013;&#24515;&#20559;&#24046;&#65292;&#21363;&#27169;&#22411;&#26356;&#21916;&#27426;&#23558;&#33258;&#24049;&#30340;&#36755;&#20986;&#22312;&#35780;&#20272;&#20013;&#25490;&#21517;&#36739;&#39640;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#26159;&#26377;&#20559;&#35265;&#30340;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;&#22312;&#27599;&#20010;&#35780;&#20272;&#20013;&#37117;&#34920;&#29616;&#20986;&#23545;&#25105;&#20204;&#20559;&#35265;&#22522;&#20934;&#30340;&#24378;&#28872;&#36857;&#35937;&#65288;&#22312;&#25152;&#26377;&#27169;&#22411;&#19978;&#30340;&#24179;&#22343;&#27604;&#36739;&#32422;&#20026;40%&#65289;&#65292;&#36825;&#23545;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#35745;&#31639;&#20102;&#24179;&#22343;&#30340;Rank-Biased O&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased O
&lt;/p&gt;</description></item><item><title>TempFuser&#26159;&#19968;&#31181;&#38271;&#30701;&#26102;&#24207;&#34701;&#21512;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#23398;&#20064;&#31354;&#20013;&#26684;&#26007;&#20013;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#39134;&#34892;&#21160;&#20316;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#27169;&#22411;&#25104;&#21151;&#22320;&#23398;&#20250;&#20102;&#22797;&#26434;&#30340;&#25112;&#26007;&#21160;&#20316;&#65292;&#24182;&#22312;&#38754;&#23545;&#39640;&#32423;&#23545;&#25163;&#26102;&#23637;&#29616;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#25112;&#26415;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.03257</link><description>&lt;p&gt;
TempFuser: &#20351;&#29992;&#38271;&#30701;&#26102;&#24207;&#34701;&#21512;&#36716;&#25442;&#22120;&#23398;&#20064;&#31354;&#20013;&#26684;&#26007;&#20013;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#39134;&#34892;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
TempFuser: Learning Tactical and Agile Flight Maneuvers in Aerial Dogfights using a Long Short-Term Temporal Fusion Transformer. (arXiv:2308.03257v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03257
&lt;/p&gt;
&lt;p&gt;
TempFuser&#26159;&#19968;&#31181;&#38271;&#30701;&#26102;&#24207;&#34701;&#21512;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#23398;&#20064;&#31354;&#20013;&#26684;&#26007;&#20013;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#39134;&#34892;&#21160;&#20316;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#27169;&#22411;&#25104;&#21151;&#22320;&#23398;&#20250;&#20102;&#22797;&#26434;&#30340;&#25112;&#26007;&#21160;&#20316;&#65292;&#24182;&#22312;&#38754;&#23545;&#39640;&#32423;&#23545;&#25163;&#26102;&#23637;&#29616;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#25112;&#26415;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31354;&#20013;&#25112;&#26007;&#20013;&#65292;&#31354;&#25112;&#21160;&#20316;&#23545;&#25112;&#26415;&#26426;&#21160;&#21644;&#25935;&#25463;&#25112;&#26007;&#26426;&#30340;&#31354;&#27668;&#21160;&#21147;&#23398;&#37117;&#25552;&#20986;&#20102;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TempFuser&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#30701;&#26102;&#24207;&#34701;&#21512;&#36716;&#25442;&#22120;&#65292;&#26088;&#22312;&#23398;&#20064;&#31354;&#20013;&#26684;&#26007;&#20013;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#39134;&#34892;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#22522;&#20110;LSTM&#30340;&#36755;&#20837;&#23884;&#20837;&#26469;&#32534;&#30721;&#38271;&#26399;&#31232;&#30095;&#21644;&#30701;&#26399;&#23494;&#38598;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#23884;&#20837;&#36890;&#36807;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#36827;&#34892;&#25972;&#21512;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25429;&#33719;&#20102;&#25112;&#26007;&#26426;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#31471;&#21040;&#31471;&#30340;&#39134;&#34892;&#25351;&#20196;&#65292;&#30830;&#20445;&#21344;&#25454;&#20248;&#21183;&#20301;&#32622;&#24182;&#36229;&#36234;&#23545;&#25163;&#12290;&#32463;&#36807;&#23545;&#39640;&#20445;&#30495;&#39134;&#34892;&#27169;&#25311;&#22120;&#20013;&#22810;&#31181;&#31867;&#22411;&#23545;&#25163;&#39134;&#26426;&#30340;&#24191;&#27867;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#25191;&#34892;&#22797;&#26434;&#30340;&#25112;&#26007;&#21160;&#20316;&#65292;&#19988;&#22987;&#32456;&#34920;&#29616;&#20248;&#20110;&#22810;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38754;&#23545;&#39640;&#32423;&#23545;&#25163;&#26102;&#23637;&#29616;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#25112;&#26415;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In aerial combat, dogfighting poses intricate challenges that demand an understanding of both strategic maneuvers and the aerodynamics of agile fighter aircraft. In this paper, we introduce TempFuser, a novel long short-term temporal fusion transformer designed to learn tactical and agile flight maneuvers in aerial dogfights. Our approach employs two distinct LSTM-based input embeddings to encode long-term sparse and short-term dense state representations. By integrating these embeddings through a transformer encoder, our model captures the tactics and agility of fighter jets, enabling it to generate end-to-end flight commands that secure dominant positions and outmaneuver the opponent. After extensive training against various types of opponent aircraft in a high-fidelity flight simulator, our model successfully learns to perform complex fighter maneuvers, consistently outperforming several baseline models. Notably, our model exhibits human-like strategic maneuvers even when facing adv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22270;&#24418;&#12289;&#36716;&#25442;&#21644;&#22522;&#20110;&#26415;&#35821;&#30340;&#23884;&#20837;&#32467;&#21512;&#36215;&#26469;&#30340;&#32479;&#19968;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#26816;&#32034;&#65292;&#20197;&#35299;&#20915;Etsy&#25628;&#32034;&#20013;&#30340;&#35821;&#20041;&#24046;&#36317;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20998;&#20139;&#20102;&#29305;&#24449;&#24037;&#31243;&#12289;&#30828;&#36127;&#37319;&#26679;&#31574;&#30053;&#21644;&#24212;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26032;&#31574;&#30053;&#65292;&#20197;&#26500;&#24314;&#20855;&#26377;&#24037;&#19994;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#25913;&#21892;&#25972;&#20307;&#25628;&#32034;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2306.04833</link><description>&lt;p&gt;
Etsy&#25628;&#32034;&#20013;&#32479;&#19968;&#23884;&#20837;&#24335;&#20010;&#24615;&#21270;&#26816;&#32034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unified Embedding Based Personalized Retrieval in Etsy Search. (arXiv:2306.04833v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22270;&#24418;&#12289;&#36716;&#25442;&#21644;&#22522;&#20110;&#26415;&#35821;&#30340;&#23884;&#20837;&#32467;&#21512;&#36215;&#26469;&#30340;&#32479;&#19968;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#26816;&#32034;&#65292;&#20197;&#35299;&#20915;Etsy&#25628;&#32034;&#20013;&#30340;&#35821;&#20041;&#24046;&#36317;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20998;&#20139;&#20102;&#29305;&#24449;&#24037;&#31243;&#12289;&#30828;&#36127;&#37319;&#26679;&#31574;&#30053;&#21644;&#24212;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26032;&#31574;&#30053;&#65292;&#20197;&#26500;&#24314;&#20855;&#26377;&#24037;&#19994;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#25913;&#21892;&#25972;&#20307;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#20449;&#24687;&#26816;&#32034;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#23614;&#26597;&#35810;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#35821;&#20041;&#24046;&#36317;&#38382;&#39064;&#30340;&#26222;&#36941;&#26041;&#27861;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#28909;&#38376;&#26597;&#35810;&#36890;&#24120;&#32570;&#20047;&#19978;&#19979;&#25991;&#65292;&#26377;&#24191;&#27867;&#30340;&#24847;&#22270;&#65292;&#29992;&#25143;&#21382;&#21490;&#20114;&#21160;&#30340;&#38468;&#21152;&#19978;&#19979;&#25991;&#26377;&#21161;&#20110;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#35299;&#20915;&#35821;&#20041;&#24046;&#36317;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#35821;&#20041;&#26816;&#32034;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#23398;&#20064;&#19968;&#31181;&#32479;&#19968;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#22270;&#24418;&#12289;&#21464;&#21387;&#22120;&#21644;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#21516;&#26102;&#20998;&#20139;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#20197;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#29305;&#24449;&#24037;&#31243;&#12289;&#30828;&#36127;&#37319;&#26679;&#31574;&#30053;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#24212;&#29992;&#26041;&#38754;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#21253;&#25324;&#29992;&#20110;&#25552;&#39640;&#25628;&#32034;&#30456;&#20851;&#24615;&#21644;&#37096;&#32626;&#27492;&#31867;&#27169;&#22411;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#21644;&#20854;&#20182;&#25216;&#24039;&#12290;&#25105;&#20204;&#30340;&#20010;&#24615;&#21270;&#26816;&#32034;&#27169;&#22411;&#26174;&#30528;&#25552;&#39640;&#20102;&#25972;&#20307;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding-based neural retrieval is a prevalent approach to address the semantic gap problem which often arises in product search on tail queries. In contrast, popular queries typically lack context and have a broad intent where additional context from users historical interaction can be helpful. In this paper, we share our novel approach to address both: the semantic gap problem followed by an end to end trained model for personalized semantic retrieval. We propose learning a unified embedding model incorporating graph, transformer and term-based embeddings end to end and share our design choices for optimal tradeoff between performance and efficiency. We share our learnings in feature engineering, hard negative sampling strategy, and application of transformer model, including a novel pre-training strategy and other tricks for improving search relevance and deploying such a model at industry scale. Our personalized retrieval model significantly improves the overall search experience,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#21644;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.05207</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26469;&#35782;&#21035;&#30315;&#30187;-&#38388;&#38553;-&#25439;&#20260;&#36830;&#32493;&#29366;&#24577;&#19979;&#30340;&#33041;&#30005;&#22270;&#22270;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Machine Learning System to Identify EEG Patterns on the Ictal-Interictal-Injury Continuum. (arXiv:2211.05207v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#21644;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21307;&#23398;&#39046;&#22495;&#65292;&#20154;&#20204;&#21628;&#21505;&#22312;&#29992;&#20110;&#20020;&#24202;&#24037;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#65288;&#30315;&#30187;&#12289;LPD&#12289;GPD&#12289;LRDA&#12289;GRDA&#12289;&#20854;&#20182;&#65289;&#30340;&#23384;&#22312;&#12290;&#27599;&#20010;&#39044;&#27979;&#37117;&#37197;&#26377;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#20511;&#21161;&#20110;&#19987;&#38376;&#30340;&#29992;&#25143;&#30028;&#38754;&#25552;&#20379;&#25903;&#25345;&#12290;&#27492;&#26032;&#22411;&#27169;&#22411;&#26550;&#26500;&#23398;&#20064;&#20102;&#19968;&#32452;&#21407;&#22411;&#31034;&#20363;&#65288;&#8220;&#21407;&#22411;&#8221;&#65289;&#65292;&#24182;&#36890;&#36807;&#23558;&#26032;&#30340;EEG&#29255;&#27573;&#19982;&#36825;&#20123;&#21407;&#22411;&#36827;&#34892;&#27604;&#36739;&#26469;&#20570;&#20986;&#20915;&#31574;&#12290;&#36825;&#20123;&#21407;&#22411;&#21487;&#20197;&#26159;&#21333;&#31867;&#65288;&#20165;&#19982;&#19968;&#20010;&#31867;&#30456;&#20851;&#65289;&#25110;&#21452;&#31867;&#65288;&#19982;&#20004;&#20010;&#31867;&#30456;&#20851;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65306;1&#65289;&#20351;&#29992;&#20840;&#23616;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#65292;&#23558;1275&#32500;cEEG&#28508;&#22312;&#29305;&#24449;&#26144;&#23556;&#21040;&#20108;&#32500;&#31354;&#38388;&#20013;&#65292;&#21487;&#35270;&#21270;&#30315;&#30187;-&#38388;&#38553;-&#25439;&#20260;&#36830;&#32493;&#29366;&#24577;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20854;&#39640;&#32500;&#32467;&#26500;&#12290;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#35299;&#37322;&#26041;&#27861;&#65292;&#20351;&#20154;&#31867;&#19987;&#23478;&#33021;&#22815;&#26597;&#35810;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#25509;&#25910;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#35299;&#37322;&#12290;3&#65289;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#26576;&#20010;&#20915;&#31574;&#30340;&#36755;&#20837;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#65292;&#20801;&#35768;&#35814;&#32454;&#26816;&#26597;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#24615;&#27169;&#22411;&#20998;&#31867;EEG&#22270;&#26696;&#21644;&#25552;&#20379;&#19987;&#23478;&#21451;&#22909;&#30340;&#35299;&#37322;&#30340;&#23454;&#29992;&#24615;&#65292;&#36825;&#20004;&#20010;&#26041;&#38754;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many medical subfields, there is a call for greater interpretability in the machine learning systems used for clinical work. In this paper, we design an interpretable deep learning model to predict the presence of 6 types of brainwave patterns (Seizure, LPD, GPD, LRDA, GRDA, other) commonly encountered in ICU EEG monitoring. Each prediction is accompanied by a high-quality explanation delivered with the assistance of a specialized user interface. This novel model architecture learns a set of prototypical examples (``prototypes'') and makes decisions by comparing a new EEG segment to these prototypes. These prototypes are either single-class (affiliated with only one class) or dual-class (affiliated with two classes).  We present three main ways of interpreting the model: 1) Using global-structure preserving methods, we map the 1275-dimensional cEEG latent features to a 2D space to visualize the ictal-interictal-injury continuum and gain insight into its high-dimensional structure. 2
&lt;/p&gt;</description></item></channel></rss>