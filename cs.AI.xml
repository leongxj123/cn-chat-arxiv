<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;X-Portrait&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#36816;&#21160;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.15931</link><description>&lt;p&gt;
X-Portrait: &#20855;&#26377;&#20998;&#23618;&#21160;&#20316;&#27880;&#24847;&#21147;&#30340;&#34920;&#29616;&#24615;&#32918;&#20687;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15931
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;X-Portrait&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#36816;&#21160;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;X-Portrait&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#22522;&#20110;&#21333;&#20010;&#32918;&#20687;&#20316;&#20026;&#22806;&#35266;&#21442;&#32771;&#65292;&#24182;&#21033;&#29992;&#26469;&#33258;&#39537;&#21160;&#35270;&#39057;&#30340;&#36816;&#21160;&#26469;&#20026;&#20854;&#28155;&#21152;&#21160;&#30011;&#65292;&#25429;&#25417;&#20855;&#26377;&#39640;&#24230;&#21160;&#24577;&#24615;&#21644;&#24494;&#22937;&#38754;&#37096;&#34920;&#24773;&#20197;&#21450;&#24191;&#27867;&#33539;&#22260;&#22836;&#37096;&#36816;&#21160;&#12290;&#22312;&#20854;&#26680;&#24515;&#37096;&#20998;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#20808;&#39564;&#20316;&#20026;&#28210;&#26579;&#39592;&#26550;&#65292;&#21516;&#26102;&#22312;ControlNet&#26694;&#26550;&#20869;&#36890;&#36807;&#26032;&#39062;&#30340;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#12290;&#19982;&#20256;&#32479;&#30340;&#31895;&#31961;&#26174;&#24335;&#25511;&#21046;&#65288;&#22914;&#38754;&#37096;&#26631;&#24535;&#28857;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#36816;&#21160;&#25511;&#21046;&#27169;&#22359;&#23398;&#20250;&#30452;&#25509;&#20174;&#21407;&#22987;&#39537;&#21160;RGB&#36755;&#20837;&#20013;&#35299;&#35835;&#21160;&#24577;&#12290;&#36890;&#36807;&#26377;&#25928;&#22686;&#24378;&#23545;&#30524;&#31070;&#31561;&#23567;&#23610;&#24230;&#32454;&#24494;&#24046;&#24322;&#30340;&#36816;&#21160;&#20851;&#27880;&#30340;&#22522;&#20110;&#34917;&#19969;&#30340;&#23616;&#37096;&#25511;&#21046;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#36816;&#21160;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15931v1 Announce Type: cross  Abstract: We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeba
&lt;/p&gt;</description></item><item><title>AutoRE &#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF&#30340;&#26032;&#39062;&#20851;&#31995;&#25277;&#21462;&#33539;&#24335;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#22312;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;</title><link>https://arxiv.org/abs/2403.14888</link><description>&lt;p&gt;
AutoRE&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
AutoRE: Document-Level Relation Extraction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14888
&lt;/p&gt;
&lt;p&gt;
AutoRE &#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF&#30340;&#26032;&#39062;&#20851;&#31995;&#25277;&#21462;&#33539;&#24335;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#22312;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#24322;&#24120;&#33021;&#21147;&#65292;&#36825;&#28608;&#21169;&#30528;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#23427;&#20204;&#36827;&#34892;&#20449;&#24687;&#25277;&#21462;(IE)&#20219;&#21153;&#65292;&#21253;&#25324;&#20851;&#31995;&#25277;&#21462;(RE)&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;(SentRE)&#20219;&#21153;&#65292;&#36825;&#36890;&#24120;&#28085;&#30422;&#20102;&#21333;&#20010;&#21477;&#23376;&#20869;&#30340;&#19968;&#32452;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26041;&#27861;&#37319;&#29992;&#23558;&#20851;&#31995;&#20316;&#20026;&#20505;&#36873;&#36873;&#25321;&#38598;&#25104;&#21040;&#25552;&#31034;&#27169;&#26495;&#20013;&#30340;&#26041;&#24335;&#65292;&#23548;&#33268;&#22312;&#22788;&#29702;&#20998;&#24067;&#22312;&#32473;&#23450;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#26102;&#25928;&#29575;&#20302;&#19979;&#65292;&#24615;&#33021;&#20122;&#20248;&#65292;&#24182;&#22312;&#22788;&#29702;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;(DocRE)&#20219;&#21153;&#26102;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoRE&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;DocRE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF(Re
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14888v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional abilities in comprehending and generating text, motivating numerous researchers to utilize them for Information Extraction (IE) purposes, including Relation Extraction (RE). Nonetheless, most existing methods are predominantly designed for Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a restricted set of relations and triplet facts within a single sentence. Furthermore, certain approaches resort to treating relations as candidate choices integrated into prompt templates, leading to inefficient processing and suboptimal performance when tackling Document-Level Relation Extraction (DocRE) tasks, which entail handling multiple relations and triplet facts distributed across a given document, posing distinct challenges. To overcome these limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel RE extraction paradigm named RHF (Re
&lt;/p&gt;</description></item><item><title>Particip-AI &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;&#65292;&#24341;&#39046;&#20154;&#24037;&#26234;&#33021;&#30340;&#27665;&#20027;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.14791</link><description>&lt;p&gt;
Particip-AI: &#19968;&#31181;&#27665;&#20027;&#35843;&#26597;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14791
&lt;/p&gt;
&lt;p&gt;
Particip-AI &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;&#65292;&#24341;&#39046;&#20154;&#24037;&#26234;&#33021;&#30340;&#27665;&#20027;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#22914;ChatGPT&#65292;&#20284;&#20046;&#38477;&#20302;&#20102;&#20844;&#20247;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21450;&#21033;&#29992;&#20854;&#21147;&#37327;&#30340;&#38376;&#27099;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;&#21644;&#21457;&#23637;&#20173;&#25484;&#25569;&#22312;&#23569;&#25968;&#20154;&#25163;&#20013;&#65292;&#21457;&#23637;&#36895;&#24230;&#21152;&#24555;&#19988;&#32570;&#20047;&#39118;&#38505;&#35780;&#20272;&#12290;&#20316;&#20026;&#36808;&#21521;&#20154;&#24037;&#26234;&#33021;&#27665;&#20027;&#27835;&#29702;&#21644;&#39118;&#38505;&#35780;&#20272;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Particip-AI&#65292;&#19968;&#20010;&#26694;&#26550;&#29992;&#20110;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#23558;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#21450;&#20854;&#21361;&#23475;&#21644;&#30410;&#22788;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#25910;&#38598;&#20351;&#29992;&#24773;&#20917;&#26356;&#21152;&#32454;&#33268;&#21644;&#35814;&#32454;&#22320;&#30740;&#31350;&#20844;&#20247;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#22312;&#22791;&#36873;&#26041;&#26696;&#19979;&#65288;&#21363;&#24320;&#21457;&#21644;&#19981;&#24320;&#21457;&#19968;&#31181;&#20351;&#29992;&#24773;&#20917;&#65289;&#36827;&#34892;&#39118;&#38505;&#35780;&#20272;&#21576;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#21361;&#23475;&#65292;&#24182;&#36890;&#36807;&#20570;&#20986;&#23545;&#20854;&#21457;&#23637;&#30340;&#32467;&#35770;&#24615;&#36873;&#25321;&#38416;&#26126;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#25351;&#23548;&#27665;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#25215;&#35834;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;295&#20010;&#20154;&#21475;&#22810;&#26679;&#21270;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14791v1 Announce Type: cross  Abstract: General purpose AI, such as ChatGPT, seems to have lowered the barriers for the public to use AI and harness its power. However, the governance and development of AI still remain in the hands of a few, and the pace of development is accelerating without proper assessment of risks. As a first step towards democratic governance and risk assessment of AI, we introduce Particip-AI, a framework to gather current and future AI use cases and their harms and benefits from non-expert public. Our framework allows us to study more nuanced and detailed public opinions on AI through collecting use cases, surfacing diverse harms through risk assessment under alternate scenarios (i.e., developing and not developing a use case), and illuminating tensions over AI development through making a concluding choice on its development. To showcase the promise of our framework towards guiding democratic AI, we gather responses from 295 demographically diverse 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;</title><link>https://arxiv.org/abs/2403.09793</link><description>&lt;p&gt;
&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#65306;&#20855;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#34892;&#21160;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#27491;&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25317;&#25380;&#22330;&#26223;&#65292;&#24182;&#25104;&#20026;&#25105;&#20204;&#31038;&#20250;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#20010;&#20855;&#26377;&#20010;&#20307;&#20154;&#31867;&#32771;&#34385;&#30340;&#31038;&#20250;&#21487;&#25509;&#21463;&#30340;&#23548;&#33322;&#34892;&#20026;&#23545;&#20110;&#21487;&#25193;&#23637;&#30340;&#24212;&#29992;&#21644;&#20154;&#31867;&#25509;&#21463;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#30340;&#23548;&#33322;&#31574;&#30053;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#24314;&#35758;&#26681;&#25454;&#26426;&#22120;&#20154;&#23637;&#31034;&#30340;&#31038;&#20132;&#34892;&#20026;&#23558;&#29616;&#26377;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20998;&#20026;&#20855;&#26377;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#21644;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#32780;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26500;&#24335;&#28304;&#33258;&#31038;&#20250;&#23398;&#23450;&#20041;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09793v1 Announce Type: cross  Abstract: Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep Reinforcement Learning (DRL) approaches are recently used to learn a robot's navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot's exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot's social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, 
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04202</link><description>&lt;p&gt;
&#24322;&#36136;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#20013;&#36947;&#24503;&#34892;&#20026;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#26085;&#30410;&#20851;&#27880;AI&#31995;&#32479;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#30340;&#38382;&#39064;&#31361;&#26174;&#20102;&#22312;&#20154;&#24037;&#20195;&#29702;&#20013;&#23884;&#20837;&#36947;&#24503;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#32463;&#39564;&#23398;&#20064;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#22810;&#20195;&#29702;&#65288;&#31038;&#20250;&#65289;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20135;&#29983;&#22797;&#26434;&#30340;&#32676;&#20307;&#23618;&#38754;&#29616;&#35937;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#27169;&#25311;&#30340;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#26469;&#30740;&#31350;&#29420;&#31435;&#23398;&#20064;&#20195;&#29702;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#23454;&#36341;&#20013;&#20195;&#29702;&#31038;&#20250;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36947;&#24503;&#24322;&#36136;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#65292;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#38754;&#23545;&#21518;&#26524;&#20027;&#20041;&#32773;&#65288;&#21363;&#20851;&#24515;&#38543;&#26102;&#38388;&#26368;&#22823;&#21270;&#26576;&#31181;&#32467;&#26524;&#65289;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#23545;&#25163;&#65288;&#21363;&#19987;&#27880;&#20110;&#31435;&#21363;&#36981;&#23432;&#29305;&#23450;&#35268;&#33539;&#65289; &#12290;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#36947;&#24503;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;LLMs&#23884;&#20837;&#21040;&#22270;&#24418;&#35268;&#21010;&#20013;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#29616;&#25104;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLMs-based&#35268;&#21010;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.00783</link><description>&lt;p&gt;
&#35770;LLMs&#22312;&#35268;&#21010;&#20013;&#30340;&#20316;&#29992;&#65306;&#23558;LLMs&#23884;&#20837;&#35268;&#21010;&#22270;&#20013;
&lt;/p&gt;
&lt;p&gt;
On the Roles of LLMs in Planning: Embedding LLMs into Planning Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00783
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;LLMs&#23884;&#20837;&#21040;&#22270;&#24418;&#35268;&#21010;&#20013;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#29616;&#25104;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLMs-based&#35268;&#21010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#21010;&#21512;&#25104;&#26088;&#22312;&#29983;&#25104;&#19968;&#31995;&#21015;&#21160;&#20316;&#25110;&#31574;&#30053;&#65292;&#23558;&#32473;&#23450;&#30340;&#21021;&#22987;&#29366;&#24577;&#36716;&#31227;&#21040;&#30446;&#26631;&#29366;&#24577;&#65292;&#25552;&#20379;&#30340;&#39046;&#22495;&#27169;&#22411;&#21487;&#20197;&#30001;&#19987;&#23478;&#35774;&#35745;&#25110;&#20174;&#35757;&#32451;&#25968;&#25454;&#25110;&#19982;&#19990;&#30028;&#30340;&#20132;&#20114;&#20013;&#23398;&#20064;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#20852;&#35268;&#21010;&#33021;&#21147;&#30340;&#22768;&#31216;&#25152;&#21560;&#24341;&#65292;&#25552;&#20986;&#20102;&#30740;&#31350;LLMs&#35268;&#21010;&#26377;&#25928;&#24615;&#30340;&#24037;&#20316;&#65292;&#32780;&#19981;&#32771;&#34385;LLMs&#20013;&#29616;&#25104;&#35268;&#21010;&#25216;&#26415;&#30340;&#21033;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;LLMs&#22312;&#29616;&#25104;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#20316;&#29992;&#36827;&#19968;&#27493;&#30740;&#31350;LLMs&#30340;&#35268;&#21010;&#33021;&#21147;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;LLMs&#23884;&#20837;&#21040;&#20247;&#25152;&#21608;&#30693;&#30340;&#35268;&#21010;&#26694;&#26550;&#20043;&#19968;&#65292;&#22522;&#20110;&#22270;&#30340;&#35268;&#21010;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLMs&#30340;&#35268;&#21010;&#26694;&#26550;&#65292;&#20854;&#20013;LLMs&#23884;&#20837;&#21040;&#20004;&#20010;&#32423;&#21035;&#30340;&#35268;&#21010;&#22270;&#20013;&#65292;&#21363;&#30456;&#20114;&#32422;&#26463;&#29983;&#25104;&#32423;&#21035;&#21644;&#32422;&#26463;&#35299;&#20915;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00783v1 Announce Type: new  Abstract: Plan synthesis aims to generate a course of actions or policies to transit given initial states to goal states, provided domain models that could be designed by experts or learnt from training data or interactions with the world. Intrigued by the claims of emergent planning capabilities in large language models (LLMs), works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs. In this paper, we aim to further study the insight of the planning capability of LLMs by investigating the roles of LLMs in off-the-shelf planning frameworks. To do this, we investigate the effectiveness of embedding LLMs into one of the well-known planning frameworks, graph-based planning, proposing a novel LLMs-based planning framework with LLMs embedded in two levels of planning graphs, i.e., mutual constraints generation level and constraints solving level. We emp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#22411;&#32452;&#21512;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12750</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Model Composition for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12750
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#32452;&#21512;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#21457;&#23637;&#26174;&#31034;&#20986;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#26397;&#30528;&#21019;&#24314;&#33021;&#22815;&#29702;&#35299;&#21508;&#31181;&#27169;&#24577;&#36755;&#20837;&#30340;&#22810;&#21151;&#33021;MLLMs&#30340;&#30446;&#26631;&#36808;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#37197;&#23545;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#36825;&#23545;&#36164;&#28304;&#35201;&#27714;&#39640;&#19988;&#38590;&#20197;&#25193;&#23637;&#21040;&#26032;&#30340;&#27169;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29616;&#26377;MLLMs&#30340;&#27169;&#22411;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#27169;&#22411;&#30340;&#26032;&#33539;&#24335;&#65292;&#35813;&#26032;&#27169;&#22411;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#22522;&#26412;&#23454;&#29616;NaiveMC&#36890;&#36807;&#37325;&#29992;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#21512;&#24182;LLM&#21442;&#25968;&#23637;&#31034;&#20102;&#36825;&#19968;&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DAMC&#26469;&#35299;&#20915;&#22312;&#21512;&#24182;&#36807;&#31243;&#20013;&#30340;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MCUB&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;MLLMs&#29702;&#35299;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12750v1 Announce Type: cross  Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to unders
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;COIECD&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20914;&#31361;&#19978;&#19979;&#25991;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#22312;&#38750;&#20914;&#31361;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11893</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#35299;&#30721;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;-&#29109;&#32422;&#26463;&#26469;&#35782;&#21035;&#21644;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11893
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;COIECD&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20914;&#31361;&#19978;&#19979;&#25991;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#22312;&#38750;&#20914;&#31361;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#20869;&#37096;&#21270;&#20102;&#22823;&#37327;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29616;&#23454;&#24212;&#29992;&#38656;&#35201;&#22806;&#37096;&#19978;&#19979;&#25991;&#30693;&#35782;&#26469;&#24110;&#21161;&#27169;&#22411;&#23436;&#25104;&#22522;&#26412;&#20219;&#21153;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#30693;&#35782;&#20914;&#31361;&#30340;&#20851;&#38190;&#22256;&#22659;&#65292;&#21363;&#19978;&#19979;&#25991;&#30693;&#35782;&#19982;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30456;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#30721;&#26041;&#27861;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;&#65292;&#21487;&#33021;&#20250;&#22312;&#27809;&#26377;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#26080;&#24847;&#20013;&#38477;&#20302;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#20449;&#24687;-&#29109;&#32422;&#26463;&#35299;&#30721;&#65288;COIECD&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#20914;&#31361;&#24182;&#35299;&#20915;&#23427;&#20204;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20914;&#31361;&#19978;&#19979;&#25991;&#30340;&#24544;&#23454;&#24230;&#65292;&#21516;&#26102;&#22312;&#38750;&#20914;&#31361;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;COIECD&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#20013;&#23637;&#29616;&#20986;&#36739;&#24378;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;&#25552;&#20379;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11893v1 Announce Type: new  Abstract: Large language models internalize enormous parametric knowledge during pre-training. Concurrently, realistic applications necessitate external contextual knowledge to aid models on the underlying tasks. This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts. In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them. It can improve the model's faithfulness to conflicting context, and simultaneously maintain high performance among non- Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets. Code is available.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24314;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;PDDL&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#35832;&#22914;&#21202;&#32034;&#36719;&#20214;&#21644;&#25935;&#24863;&#25968;&#25454;&#22806;&#27844;&#31561;&#24191;&#27867;&#25915;&#20987;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.10985</link><description>&lt;p&gt;
&#21033;&#29992;AI&#35268;&#21010;&#25216;&#26415;&#26816;&#27979;&#20113;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Leveraging AI Planning For Detecting Cloud Security Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10985
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24314;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;PDDL&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#35832;&#22914;&#21202;&#32034;&#36719;&#20214;&#21644;&#25935;&#24863;&#25968;&#25454;&#22806;&#27844;&#31561;&#24191;&#27867;&#25915;&#20987;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#26381;&#21153;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#19988;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#25968;&#25454;&#23384;&#20648;&#12289;&#22788;&#29702;&#21644;&#21327;&#20316;&#35299;&#20915;&#26041;&#26696;&#12290;&#38543;&#30528;&#23427;&#20204;&#30340;&#26222;&#21450;&#65292;&#19982;&#20854;&#23433;&#20840;&#28431;&#27934;&#30456;&#20851;&#30340;&#25285;&#24551;&#20063;&#22312;&#22686;&#38271;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#27844;&#38706;&#21644;&#21202;&#32034;&#36719;&#20214;&#31561;&#22797;&#26434;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#36798;&#20113;&#31995;&#32479;&#20013;&#19981;&#21516;&#23545;&#35937;&#65288;&#22914;&#29992;&#25143;&#12289;&#25968;&#25454;&#23384;&#20648;&#12289;&#23433;&#20840;&#35282;&#33394;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#24314;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#12290;&#35775;&#38382;&#25511;&#21046;&#35823;&#37197;&#32622;&#36890;&#24120;&#26159;&#20113;&#25915;&#20987;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;PDDL&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#23433;&#20840;&#28431;&#27934;&#65292;&#20363;&#22914;&#21487;&#33021;&#23548;&#33268;&#24191;&#27867;&#25915;&#20987;&#65288;&#22914;&#21202;&#32034;&#36719;&#20214;&#65289;&#21644;&#25935;&#24863;&#25968;&#25454;&#22806;&#27844;&#31561;&#12290;&#35268;&#21010;&#22120;&#21487;&#20197;&#29983;&#25104;&#25915;&#20987;&#20197;&#35782;&#21035;&#20113;&#20013;&#30340;&#27492;&#31867;&#28431;&#27934;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;14&#20010;&#19981;&#21516;&#21830;&#19994;&#32452;&#32455;&#30340;&#30495;&#23454;&#20122;&#39532;&#36874;AWS&#20113;&#37197;&#32622;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10985v1 Announce Type: cross  Abstract: Cloud computing services provide scalable and cost-effective solutions for data storage, processing, and collaboration. Alongside their growing popularity, concerns related to their security vulnerabilities leading to data breaches and sophisticated attacks such as ransomware are growing. To address these, first, we propose a generic framework to express relations between different cloud objects such as users, datastores, security roles, to model access control policies in cloud systems. Access control misconfigurations are often the primary driver for cloud attacks. Second, we develop a PDDL model for detecting security vulnerabilities which can for example lead to widespread attacks such as ransomware, sensitive data exfiltration among others. A planner can then generate attacks to identify such vulnerabilities in the cloud. Finally, we test our approach on 14 real Amazon AWS cloud configurations of different commercial organizations
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;</title><link>https://arxiv.org/abs/2402.10962</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#27979;&#37327;&#21644;&#25511;&#21046;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Measuring and Controlling Persona Drift in Language Model Dialogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#25215;&#25285;&#29305;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#12290;&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#65292;&#23427;&#20204;&#23558;&#26159;&#31283;&#23450;&#30340;&#65292;&#22240;&#27492;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#22312;&#25972;&#20010;&#23545;&#35805;&#36807;&#31243;&#20013;&#32487;&#32493;&#26681;&#25454;&#35268;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#36890;&#36807;&#20004;&#20010;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#33258;&#25105;&#23545;&#35805;&#26469;&#35780;&#20272;&#8220;&#20154;&#35774;&#8221;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#27169;&#22411;&#22914;LLaMA2-chat-70B&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;&#20843;&#36718;&#23545;&#35805;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#12290;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#30001;&#20110;&#38271;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#34928;&#20943;&#65292;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#19982;&#20004;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.08983</link><description>&lt;p&gt;
SafeDecoding: &#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#21161;&#31561;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#20026;&#20102;&#20351;LLM&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#21253;&#25324;&#23433;&#20840;&#24615;&#22312;&#20869;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#36234;&#29425;&#25915;&#20987;&#26088;&#22312;&#24341;&#21457;LLM&#30340;&#38750;&#39044;&#26399;&#21644;&#19981;&#23433;&#20840;&#34892;&#20026;&#65292;&#20173;&#28982;&#26159;LLM&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#23041;&#32961;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;SafeDecoding&#26469;&#38450;&#24481;LLM&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#23433;&#20840;&#24863;&#30693;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#24320;&#21457;SafeDecoding&#26102;&#30340;&#27934;&#23519;&#21147;&#22522;&#20110;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#20195;&#34920;&#26377;&#23475;&#20869;&#23481;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#36229;&#36807;&#20195;&#34920;&#26080;&#23475;&#21709;&#24212;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#20173;&#28982;&#20986;&#29616;&#22312;&#25353;&#27010;&#29575;&#38477;&#24207;&#25490;&#24207;&#30340;&#26631;&#35760;&#20013;&#30340;&#21069;&#20960;&#20010;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#35782;&#21035;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#24182;&#22686;&#24378;&#20854;&#33391;&#24615;&#24433;&#21709;&#21147;&#26469;&#20943;&#36731;&#36234;&#29425;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08983v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#39046;&#22495;&#20013;&#32570;&#20047;&#32467;&#26500;&#21270;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07039</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#21327;&#35843;&#25259;&#38706;&#65306;&#36229;&#36234;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Coordinated Disclosure for AI: Beyond Security Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07039
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#39046;&#22495;&#20013;&#32570;&#20047;&#32467;&#26500;&#21270;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20260;&#23475;&#25253;&#21578;&#22312;&#25259;&#38706;&#25110;&#35299;&#20915;&#31639;&#27861;&#32570;&#38519;&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#31181;&#20020;&#26102;&#24615;&#30340;&#25805;&#20316;&#65292;&#32570;&#20047;&#32467;&#26500;&#21270;&#30340;&#36807;&#31243;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21327;&#35843;&#28431;&#27934;&#25259;&#38706;&#65288;CVD&#65289;&#30340;&#20262;&#29702;&#21644;&#29983;&#24577;&#31995;&#32479;&#22312;&#36719;&#20214;&#23433;&#20840;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#32654;&#22269;&#30340;&#32972;&#26223;&#19979;&#65292;&#20026;&#20102;&#40723;&#21169;&#31177;&#25345;&#21892;&#24847;&#34892;&#20107;&#30340;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#65292;&#24314;&#31435;&#19968;&#20010;&#23433;&#20840;&#38450;&#25252;&#26465;&#27454;&#20197;&#23545;&#25239;&#35745;&#31639;&#26426;&#27450;&#35784;&#21644;&#28389;&#29992;&#27861;&#26696;&#19968;&#30452;&#23384;&#22312;&#38271;&#26399;&#30340;&#27861;&#24459;&#21644;&#25919;&#31574;&#26007;&#20105;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20013;&#30340;&#31639;&#27861;&#32570;&#38519;&#19982;&#20256;&#32479;&#36719;&#20214;&#28431;&#27934;&#23384;&#22312;&#30528;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#19968;&#31181;&#19987;&#38376;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#29305;&#27530;&#22797;&#26434;&#24615;&#30340;&#19987;&#38376;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#30340;&#23454;&#26045;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;ML&#20013;&#30340;&#25259;&#38706;&#21382;&#21490;&#32972;&#26223;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Harm reporting in the field of Artificial Intelligence (AI) currently operates on an ad hoc basis, lacking a structured process for disclosing or addressing algorithmic flaws. In contrast, the Coordinated Vulnerability Disclosure (CVD) ethos and ecosystem play a pivotal role in software security and transparency. Within the U.S. context, there has been a protracted legal and policy struggle to establish a safe harbor from the Computer Fraud and Abuse Act, aiming to foster institutional support for security researchers acting in good faith. Notably, algorithmic flaws in Machine Learning (ML) models present distinct challenges compared to traditional software vulnerabilities, warranting a specialized approach. To address this gap, we propose the implementation of a dedicated Coordinated Flaw Disclosure (CFD) framework tailored to the intricacies of machine learning and artificial intelligence issues. This paper delves into the historical landscape of disclosures in ML, encompassing the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.06782</link><description>&lt;p&gt;
&#19982;&#26356;&#26377;&#35828;&#26381;&#21147;&#30340;LLMs&#36777;&#35770;&#20250;&#23548;&#33268;&#26356;&#30495;&#23454;&#30340;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Debating with More Persuasive LLMs Leads to More Truthful Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#25152;&#38656;&#34892;&#20026;&#19968;&#33268;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#23558;&#36229;&#36807;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#65292;&#20154;&#31867;&#35780;&#20272;&#30340;&#35282;&#33394;&#23558;&#28436;&#21464;&#20026;&#38750;&#19987;&#23478;&#30417;&#30563;&#19987;&#23478;&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#25105;&#20204;&#38382;&#65306;&#26356;&#24369;&#30340;&#27169;&#22411;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#21527;&#65311;&#25105;&#20204;&#22312;&#31867;&#20284;&#30340;&#29615;&#22659;&#20013;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26356;&#24378;&#30340;&#27169;&#22411;&#65288;&#19987;&#23478;&#65289;&#25317;&#26377;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#32780;&#26356;&#24369;&#30340;&#27169;&#22411;&#65288;&#38750;&#19987;&#23478;&#65289;&#32570;&#20047;&#36825;&#20123;&#20449;&#24687;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#26041;&#27861;&#26159;\textit{&#36777;&#35770;}&#65292;&#20854;&#20013;&#20004;&#20010;LLM&#19987;&#23478;&#20998;&#21035;&#25903;&#25345;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#19968;&#20010;&#38750;&#19987;&#23478;&#36873;&#25321;&#31572;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#36777;&#35770; consistently&#24110;&#21161;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#65292;&#20998;&#21035;&#36798;&#21040;76%&#21644;88%&#30340;&#20934;&#30830;&#24615;&#65288;&#26420;&#32032;&#22522;&#20934;&#20998;&#21035;&#20026;48%&#21644;60%&#65289;&#12290;&#27492;&#22806;&#65292;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#20248;&#21270;&#19987;&#23478;&#36777;&#35770;&#32773;&#30340;&#35828;&#26381;&#21147;&#20250;&#25552;&#39640;&#38750;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05140</link><description>&lt;p&gt;
Tag-LLM: &#23558;&#36890;&#29992;&#30340;LLM&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#20877;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#19987;&#38376;&#39046;&#22495;&#20013;&#65292;&#22914;&#29289;&#29702;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26410;&#20805;&#20998;&#28085;&#30422;&#30340;&#39046;&#22495;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36890;&#29992;LLMs&#37325;&#26032;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26377;&#25928;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#34987;&#21442;&#25968;&#21270;&#20026;&#36830;&#32493;&#21521;&#37327;&#24182;&#38468;&#21152;&#21040;LLMs&#30340;&#23884;&#20837;&#23618;&#65292;&#20197;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#26631;&#31614;&#65306;&#39046;&#22495;&#26631;&#31614;&#29992;&#20110;&#38480;&#23450;&#19987;&#19994;&#34920;&#31034;&#65288;&#20363;&#22914;&#21270;&#23398;&#24335;&#65289;&#24182;&#25552;&#20379;&#39046;&#22495;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65307;&#21151;&#33021;&#26631;&#31614;&#29992;&#20110;&#34920;&#31034;&#29305;&#23450;&#30340;&#21151;&#33021;&#65288;&#20363;&#22914;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65289;&#24182;&#21387;&#32553;&#21151;&#33021;&#35299;&#20915;&#25351;&#20196;&#12290;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#39046;&#22495;&#30693;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#36825;&#20123;&#26631;&#31614;&#30340;&#21327;&#35758;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#20197;&#21450;&#26032;&#20852;&#30340;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#36824;&#35752;&#35770;&#20102;LLMs&#30340;&#26368;&#26032;&#33539;&#20363;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05121</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Table Processing: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#20197;&#21450;&#26032;&#20852;&#30340;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#36824;&#35752;&#35770;&#20102;LLMs&#30340;&#26368;&#26032;&#33539;&#20363;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#36890;&#24120;&#26159;&#20108;&#32500;&#32467;&#26500;&#21270;&#30340;&#65292;&#29992;&#20110;&#23384;&#20648;&#22823;&#37327;&#25968;&#25454;&#65292;&#22312;&#25968;&#25454;&#24211;&#26597;&#35810;&#12289;&#30005;&#23376;&#34920;&#26684;&#35745;&#31639;&#21644;&#20174;&#32593;&#32476;&#34920;&#26684;&#29983;&#25104;&#25253;&#21578;&#31561;&#26085;&#24120;&#27963;&#21160;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#36825;&#20123;&#20197;&#34920;&#26684;&#20026;&#20013;&#24515;&#30340;&#20219;&#21153;&#21487;&#20197;&#24102;&#26469;&#37325;&#22823;&#30340;&#20844;&#20247;&#21033;&#30410;&#65292;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#20852;&#36259;&#12290;&#35813;&#35843;&#26597;&#23545;&#34920;&#26684;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27010;&#36848;&#65292;&#19981;&#20165;&#28085;&#30422;&#20256;&#32479;&#39046;&#22495;&#22914;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#65288;Table QA&#65289;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#36824;&#21253;&#25324;&#26368;&#36817;&#24378;&#35843;&#30340;&#26041;&#38754;&#65292;&#22914;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#36229;&#36234;&#20102;&#26089;&#26399;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;LLM&#20351;&#29992;&#20013;&#30340;&#26368;&#26032;&#33539;&#20363;&#12290;&#37325;&#28857;&#26159;LLMs&#39046;&#22495;&#20869;&#30340;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#20960;&#20010;&#25361;&#25112;&#65292;&#28085;&#30422;&#31169;&#26377;&#37096;&#32626;&#12289;&#39640;&#25928;&#25512;&#26029;&#21644; LLMS &#21457;&#23637;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet calculations, and generating reports from web tables. Automating these table-centric tasks with Large Language Models (LLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides an extensive overview of table tasks, encompassing not only the traditional areas like table question answering (Table QA) and fact verification, but also newly emphasized aspects such as table manipulation and advanced table data analysis. Additionally, it goes beyond the early strategies of pre-training and fine-tuning small language models, to include recent paradigms in LLM usage. The focus here is particularly on instruction-tuning, prompting, and agent-based approaches within the realm of LLMs. Finally, we highlight several challenges, ranging from private deployment and efficient inference to the developmen
&lt;/p&gt;</description></item><item><title>SoftMAC&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#21487;&#24494;&#20223;&#30495;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#36719;&#20307;&#12289;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#39044;&#27979;&#30340;&#25509;&#35302;&#27169;&#22411;&#21644;&#31359;&#36879;&#36861;&#36394;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#31359;&#36879;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2312.03297</link><description>&lt;p&gt;
SoftMAC&#65306;&#22522;&#20110;&#39044;&#27979;&#25509;&#35302;&#27169;&#22411;&#21644;&#19982;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#21452;&#21521;&#32806;&#21512;&#30340;&#21487;&#24494;&#36719;&#20307;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03297
&lt;/p&gt;
&lt;p&gt;
SoftMAC&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#21487;&#24494;&#20223;&#30495;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#36719;&#20307;&#12289;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#39044;&#27979;&#30340;&#25509;&#35302;&#27169;&#22411;&#21644;&#31359;&#36879;&#36861;&#36394;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#31359;&#36879;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#29289;&#29702;&#20223;&#30495;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#20915;&#26426;&#22120;&#20154;&#30456;&#20851;&#38382;&#39064;&#30340;&#25928;&#29575;&#12290;&#20026;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#25805;&#32437;&#22330;&#26223;&#20013;&#24212;&#29992;&#21487;&#24494;&#20223;&#30495;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23558;&#21508;&#31181;&#26448;&#26009;&#38598;&#25104;&#21040;&#32479;&#19968;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SoftMAC&#65292;&#19968;&#20010;&#21487;&#24494;&#20223;&#30495;&#26694;&#26550;&#65292;&#23558;&#36719;&#20307;&#19982;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#32806;&#21512;&#22312;&#19968;&#36215;&#12290;SoftMAC&#20351;&#29992;&#22522;&#20110;&#36830;&#32493;&#21147;&#23398;&#30340;&#26448;&#26009;&#28857;&#27861;&#26469;&#27169;&#25311;&#36719;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#27979;&#30340;MPM&#25509;&#35302;&#27169;&#22411;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#31359;&#36879;&#65292;&#32780;&#19981;&#20250;&#24341;&#20837;&#20854;&#20182;&#24322;&#24120;&#29616;&#35937;&#65292;&#22914;&#19981;&#33258;&#28982;&#30340;&#21453;&#24377;&#12290;&#20026;&#20102;&#23558;MPM&#31890;&#23376;&#19982;&#21487;&#21464;&#24418;&#21644;&#38750;&#20307;&#31215;&#34915;&#29289;&#32593;&#26684;&#32806;&#21512;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31359;&#36879;&#36861;&#36394;&#31639;&#27861;&#65292;&#37325;&#24314;&#23616;&#37096;&#21306;&#22495;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03297v2 Announce Type: replace-cross  Abstract: Differentiable physics simulation provides an avenue to tackle previously intractable challenges through gradient-based optimization, thereby greatly improving the efficiency of solving robotics-related problems. To apply differentiable simulation in diverse robotic manipulation scenarios, a key challenge is to integrate various materials in a unified framework. We present SoftMAC, a differentiable simulation framework that couples soft bodies with articulated rigid bodies and clothes. SoftMAC simulates soft bodies with the continuum-mechanics-based Material Point Method (MPM). We provide a novel forecast-based contact model for MPM, which effectively reduces penetration without introducing other artifacts like unnatural rebound. To couple MPM particles with deformable and non-volumetric clothes meshes, we also propose a penetration tracing algorithm that reconstructs the signed distance field in local area. Diverging from prev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20840;&#38754;&#25552;&#39640;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#24615;&#33021;</title><link>https://arxiv.org/abs/2209.00568</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Contrastive Knowledge Co-Distillation for Event Temporal Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00568
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20840;&#38754;&#25552;&#39640;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#65288;ETRE&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20107;&#20214;&#23545;&#20301;&#20110;&#19981;&#21516;&#36317;&#31163;&#30340;&#35805;&#35821;&#20013;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25509;&#36817;&#24615;&#24102;&#12290;&#20851;&#20110;&#20301;&#20110;&#26356;&#36828;&#65288;&#21363;&#8220;&#38271;&#8221;&#65289;&#25110;&#26356;&#36817;&#65288;&#21363;&#8220;&#30701;&#8221;&#65289;&#25509;&#36817;&#24615;&#24102;&#30340;&#20107;&#20214;&#23545;&#30340;&#26102;&#38388;&#39034;&#24207;&#20256;&#36798;&#26041;&#24335;&#19981;&#21516;&#12290;&#30446;&#21069;ETRE&#27169;&#22411;&#24448;&#24448;&#22312;&#20301;&#20110;&#30701;&#25110;&#38271;&#25509;&#36817;&#24615;&#24102;&#30340;&#20107;&#20214;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#19981;&#33021;&#21516;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#25991;&#26412;&#21253;&#21547;&#25152;&#26377;&#31867;&#22411;&#30340;&#26102;&#38388;&#20107;&#20214;&#23545;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#65292;&#36825;&#26159;&#19968;&#31181;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#22810;&#20010;&#20107;&#20214;&#23545;&#25509;&#36817;&#24615;&#24102;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#23545;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MulCo&#25104;&#21151;&#22320;&#25972;&#21512;&#20102;&#36328;&#30701;&#21644;&#38271;&#25509;&#36817;&#24615;&#24102;&#30340;&#19982;&#26102;&#38388;&#25512;&#29702;&#30456;&#20851;&#30340;&#35821;&#35328;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00568v2 Announce Type: replace-cross  Abstract: Event Temporal Relation Extraction (ETRE) is a crucial yet challenging problem. Event pairs are situated within a discourse at different distances, which we refer to as proximity bands. The temporal ordering communicated about event pairs situated at more remote (i.e., ``long'') or less remote (i.e., ``short'') proximity bands is encoded differently. SOTA ETRE models have tended to perform well on events situated at either short or long proximity bands, but not both. Yet, real-world, natural texts contain all types of temporal event-pairs. In this paper, we present MulCo: Multi-Scale Contrastive Knowledge Co-Distillation, a fusion approach that shares knowledge across multiple event pair proximity bands in order to improve performance on all types of temporal datasets. Our experimental results show that MulCo successfully integrates linguistic cues pertaining to temporal reasoning across both short and long proximity bands and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.10805</link><description>&lt;p&gt;
&#23398;&#20064;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#65288;CATE&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;CATE&#21487;&#20197;&#22312;&#20219;&#21153;&#35268;&#21010;&#21644;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#22522;&#20110;CATE&#30340;&#20219;&#21153;&#24418;&#24335;&#65292;&#22914;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#65292;&#20854;&#20013;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#20197;&#35821;&#20041;&#21644;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#36830;&#25509;&#21160;&#20316;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#29992;&#20110;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#12290;&#23613;&#31649;&#20219;&#21153;&#20855;&#26377;&#30452;&#35266;&#24615;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#22256;&#38590;&#37325;&#37325;&#65292;&#20154;&#31867;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#22522;&#30784;&#65292;&#23637;&#31034;&#20102;&#36830;&#25509;&#35270;&#39057;&#29702;&#35299;&#20013;&#21160;&#20316;&#21644;&#25928;&#26524;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09786</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#27169;&#22411;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#38271;&#23614;&#35859;&#35789;&#20998;&#24067;&#21644;&#32570;&#22833;&#27880;&#37322;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;SGG&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#35757;&#32451;SGG&#65288;ST-SGG&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#20026;&#20854;&#20998;&#37197;&#20266;&#26631;&#31614;&#20197;&#35757;&#32451;SGG&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#30340;&#33258;&#35757;&#32451;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#35774;&#35745;&#36866;&#29992;&#20110;SGG&#20219;&#21153;&#30340;&#33258;&#35757;&#32451;&#26694;&#26550;&#26356;&#20855;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#22266;&#26377;&#29305;&#24615;&#65292;&#22914;&#35821;&#20041;&#27495;&#20041;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#35859;&#35789;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SGG&#20266;&#26631;&#31614;&#25216;&#26415;&#65292;&#31216;&#20026;&#20855;&#26377;&#21160;&#37327;&#30340;&#31867;&#21035;&#33258;&#36866;&#24212;&#38408;&#20540;&#21270;&#65288;CATM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#24050;&#26377;&#30340;SGG&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;&#65288;GSL&#65289;&#65292;&#20174;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is benefici
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;COVID-19&#30123;&#33495;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#39539;&#26021;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#21644;&#32463;&#36807;&#31574;&#21010;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#33258;&#21160;&#39539;&#26021;&#34394;&#20551;&#20449;&#24687;&#12290;&#36825;&#20026;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.19834</link><description>&lt;p&gt;
AMIR&#65306;&#22522;&#20110;COVID-19&#30123;&#33495;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#39539;&#26021;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AMIR: Automated MisInformation Rebuttal -- A COVID-19 Vaccination Datasets based Recommendation System. (arXiv:2310.19834v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;COVID-19&#30123;&#33495;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#39539;&#26021;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#21644;&#32463;&#36807;&#31574;&#21010;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#33258;&#21160;&#39539;&#26021;&#34394;&#20551;&#20449;&#24687;&#12290;&#36825;&#20026;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#36817;&#24180;&#26469;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#31038;&#20250;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#22312;COVID-19&#22823;&#27969;&#34892;&#30340;&#32972;&#26223;&#19979;&#65292;&#23427;&#21152;&#21095;&#20102;&#30123;&#33495;&#29369;&#35947;&#19981;&#20915;&#12290;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24403;&#21153;&#20043;&#24613;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#33719;&#21462;&#30340;&#29616;&#26377;&#20449;&#24687;&#65292;&#24182;&#19982;&#26356;&#22810;&#32463;&#36807;&#31574;&#21010;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#65292;&#20197;&#20419;&#36827;&#22823;&#35268;&#27169;&#33258;&#21160;&#39539;&#26021;&#34394;&#20551;&#20449;&#24687;&#12290;&#34429;&#28982;&#36825;&#37324;&#30340;&#24819;&#27861;&#21487;&#20197;&#25512;&#24191;&#24182;&#37325;&#26032;&#24212;&#29992;&#20110;&#20351;&#29992;&#22810;&#31181;&#20449;&#24687;&#26469;&#28304;&#21644;&#28385;&#36275;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20809;&#35889;&#30340;&#36739;&#22823;&#33539;&#22260;&#30340;&#34394;&#20551;&#20449;&#24687;&#32531;&#35299;&#65292;&#20294;&#26412;&#24037;&#20316;&#20316;&#20026;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#21463;&#38480;&#20110;&#20854;&#33539;&#22260;&#65292;&#20165;&#38480;&#20110;&#23545;&#25512;&#25991;&#30340;&#21453;&#39539;&#65292;&#19988;&#20165;&#38480;&#20110;COVID-19&#30456;&#20851;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#23427;&#21033;&#29992;&#20102;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;FaCov&#65288;&#32463;&#20107;&#23454;&#26680;&#26597;&#30340;&#25991;&#31456;&#65289;&#21644;misleading&#65288;&#31038;&#20132;&#23186;&#20307;&#25512;&#25991;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation has emerged as a major societal threat in recent years in general; specifically in the context of the COVID-19 pandemic, it has wrecked havoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable solutions for combating misinformation are the need of the hour. This work explored how existing information obtained from social media and augmented with more curated fact checked data repositories can be harnessed to facilitate automated rebuttal of misinformation at scale. While the ideas herein can be generalized and reapplied in the broader context of misinformation mitigation using a multitude of information sources and catering to the spectrum of social media platforms, this work serves as a proof of concept, and as such, it is confined in its scope to only rebuttal of tweets, and in the specific context of misinformation regarding COVID-19. It leverages two publicly available datasets, viz. FaCov (fact-checked articles) and misleading (social media Twitt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;2D&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;3D&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#24687;&#36827;&#34892;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#36890;&#36807;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#65292;&#26377;&#25928;&#35299;&#20915;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#22270;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07986</link><description>&lt;p&gt;
&#35266;&#28857;&#25991;&#26412;&#20498;&#32622;&#65306;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#37322;&#25918;&#26032;&#39062;&#30340;&#35270;&#22270;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Viewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained 2D Diffusion Models. (arXiv:2309.07986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;2D&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;3D&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#24687;&#36827;&#34892;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#36890;&#36807;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#65292;&#26377;&#25928;&#35299;&#20915;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#22270;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#23545;&#35937;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#20165;&#36890;&#36807;2D&#30417;&#30563;&#26469;&#34920;&#31034;&#19990;&#30028;&#30340;&#30495;&#23454;3D&#32467;&#26500;&#65311;&#25105;&#20204;&#35777;&#26126;&#65292;&#26159;&#30340;&#65292;3D&#30693;&#35782;&#34987;&#32534;&#30721;&#22312;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#65292;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#31070;&#32463;&#26144;&#23556;&#22120;&#65292;&#29992;&#20110;&#33719;&#21462;&#30456;&#26426;&#35270;&#28857;&#21442;&#25968;&#24182;&#39044;&#27979;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#21521;&#37327;&#65307;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#28508;&#22312;&#21521;&#37327;&#26469;&#35843;&#25972;&#25193;&#25955;&#29983;&#25104;&#36807;&#31243;&#65292;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#30456;&#26426;&#35270;&#28857;&#30340;&#22270;&#20687;&#12290;ViewNeTI&#33258;&#28982;&#35299;&#20915;&#20102;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#65288;NVS&#65289;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#34987;&#20923;&#32467;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#24456;&#23569;&#30340;&#36755;&#20837;&#35270;&#22270;&#26469;&#35299;&#20915;NVS&#38382;&#39064;&#65307;&#25105;&#20204;&#29978;&#33267;&#21487;&#20197;&#36827;&#34892;&#21333;&#35270;&#22270;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#21333;&#35270;&#22270;NVS&#39044;&#27979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models understand spatial relationship between objects, but do they represent the true 3D structure of the world from only 2D supervision? We demonstrate that yes, 3D knowledge is encoded in 2D image diffusion models like Stable Diffusion, and we show that this structure can be exploited for 3D vision tasks. Our method, Viewpoint Neural Textual Inversion (ViewNeTI), controls the 3D viewpoint of objects in generated images from frozen diffusion models. We train a small neural mapper to take camera viewpoint parameters and predict text encoder latents; the latents then condition the diffusion generation process to produce images with the desired camera viewpoint.  ViewNeTI naturally addresses Novel View Synthesis (NVS). By leveraging the frozen diffusion model as a prior, we can solve NVS with very few input views; we can even do single-view novel view synthesis. Our single-view NVS predictions have good semantic details and photorealism compared to prior methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28857;&#20113;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;Point-MA2E&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#29992;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#25439;&#22351;&#28857;&#20113;&#21040;&#36824;&#21407;&#28857;&#20113;&#30340;&#37325;&#24314;&#65292;&#25193;&#23637;&#20102;&#30446;&#21069;&#25513;&#33180;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2211.06841</link><description>&lt;p&gt;
Point-MA2E:&#33258;&#30417;&#30563;&#28857;&#20113;&#23398;&#20064;&#30340;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Point-MA2E: Masked and Affine Transformed AutoEncoder for Self-supervised Point Cloud Learning. (arXiv:2211.06841v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28857;&#20113;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;Point-MA2E&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#29992;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#25439;&#22351;&#28857;&#20113;&#21040;&#36824;&#21407;&#28857;&#20113;&#30340;&#37325;&#24314;&#65292;&#25193;&#23637;&#20102;&#30446;&#21069;&#25513;&#33180;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#28857;&#20113;&#23398;&#20064;&#20013;&#65292;&#25513;&#33180;&#24314;&#27169;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#20174;&#20854;&#25513;&#33180;&#23545;&#24212;&#37096;&#20998;&#37325;&#24314;&#23436;&#25972;&#28857;&#20113;&#12290;&#32771;&#34385;&#21040;&#25513;&#33180;&#21482;&#20250;&#25439;&#22351;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#28857;&#65292;&#26412;&#25991;&#25512;&#24191;&#20223;&#23556;&#21464;&#25442;&#31574;&#30053;&#65292;&#36890;&#36807;&#29305;&#23450;&#35268;&#21017;&#30772;&#22351;&#25152;&#26377;&#36755;&#20837;&#28857;&#65292;&#20197;&#34917;&#20805;&#27969;&#34892;&#30340;&#25513;&#33180;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#28857;&#20113;&#23398;&#20064;&#30340;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#65288;Point-MA2E&#65289;&#12290;&#22312;&#27492;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#28857;&#20113;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#21644;&#25513;&#33180;&#65292;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20174;&#20854;&#25439;&#22351;&#29256;&#26412;&#20013;&#37325;&#24314;&#21407;&#22987;&#28857;&#20113;&#12290;&#25506;&#32034;&#20102;&#21508;&#31181;&#28857;&#20113;&#32534;&#30721;&#22120;&#12290;&#23545;&#20110;&#38750;Transformer&#32534;&#30721;&#22120;&#65292;&#25353;&#29031;&#24120;&#35265;&#20570;&#27861;&#30452;&#25509;&#37325;&#24314;&#26410;&#25439;&#22351;&#30340;&#28857;&#20113;&#12290;&#23545;&#20110;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23558;&#37325;&#24314;&#23436;&#25972;&#28857;&#20113;&#20998;&#35299;&#20026;&#35814;&#32454;&#30340;&#23616;&#37096;&#34917;&#19969;&#21644;&#31895;&#30053;&#30340;&#20840;&#23616;&#24418;&#29366;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked modeling has demonstrated its effectiveness in self-supervised point cloud learning by reconstructing the complete point cloud from its masked counterpart. Considering that masking only corrupts partial points of the input, in this paper, we promote the affine transformation, which corrupts all input points with certain rules, to complement the popular masking strategy, leading to the Masked and Affine transformed AutoEncoder for point cloud learning (Point-MA2E). Generally, we corrupt the point cloud with affine transformation and masking as input and learn an encoder-decoder model to reconstruct the original point cloud from its corrupted version. Various point cloud encoders are explored in this study. For non-Transformer encoders, we follow the common practice to reconstruct the uncorrupted point cloud directly. For Transformer-based encoders, we decompose the reconstruction of the complete point cloud into the reconstructions of detailed local patches and rough global shape
&lt;/p&gt;</description></item></channel></rss>