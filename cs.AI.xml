<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#20915;&#31574;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"K&#32423;&#25512;&#29702;"&#30340;&#26032;&#39062;&#25512;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35797;&#39564;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#25512;&#29702;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23481;&#26131;&#20986;&#38169;&#65292;&#32780;"K&#32423;&#25512;&#29702;"&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01521</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;K&#32423;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
K-Level Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#20915;&#31574;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"K&#32423;&#25512;&#29702;"&#30340;&#26032;&#39062;&#25512;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35797;&#39564;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#25512;&#29702;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23481;&#26131;&#20986;&#38169;&#65292;&#32780;"K&#32423;&#25512;&#29702;"&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#20854;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#21160;&#24577;&#12289;&#20132;&#20114;&#21644;&#31454;&#20105;&#22330;&#26223;&#65288;&#22914;&#21830;&#19994;&#25112;&#30053;&#21644;&#32929;&#31080;&#24066;&#22330;&#20998;&#26512;&#65289;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#27491;&#24335;&#25506;&#32034;LLMs&#22312;&#24555;&#36895;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#20915;&#31574;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#35797;&#39564;&#65292;&#20197;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#20013;&#21160;&#24577;&#20915;&#31574;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#20123;&#25361;&#25112;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#65292;&#21487;&#20197;&#23545;LLMs&#30340;&#21160;&#24577;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#28165;&#26224;&#12289;&#21487;&#25511;&#21644;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25512;&#29702;&#26041;&#27861;&#22312;&#38656;&#35201;k&#32423;&#24605;&#32771;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#23481;&#26131;&#20986;&#38169; - &#36825;&#26159;&#20043;&#21069;&#30740;&#31350;&#20013;&#26410;&#35299;&#20915;&#30340;&#20851;&#38190;&#27010;&#24565;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLMs&#25512;&#29702;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#8220;K&#32423;&#25512;&#29702;&#8221;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#23545;&#25163;&#30340;&#35270;&#35282;&#65292;&#20174;&#36882;&#24402;&#35282;&#24230;&#36816;&#29992;&#22522;&#20110;k&#32423;&#24605;&#32771;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have demonstrated their proficiency in complex reasoning tasks, their performance in dynamic, interactive, and competitive scenarios - such as business strategy and stock market analysis - remains underexplored. To bridge this gap, we formally explore the dynamic reasoning capabilities of LLMs for decision-making in rapidly evolving environments. We introduce two game theory-based pilot challenges that mirror the complexities of real-world dynamic decision-making. These challenges are well-defined, enabling clear, controllable, and precise evaluation of LLMs' dynamic reasoning abilities. Through extensive experiments, we find that existing reasoning methods tend to falter in dynamic settings that require k-level thinking - a key concept not tackled by previous works. To address this, we propose a novel reasoning approach for LLMs, named "K-Level Reasoning". This approach adopts the perspective of rivals to recursively employ k-level thinking based on 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PROMPT-SAW&#27169;&#22411;&#65292;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#26469;&#23454;&#29616;&#25991;&#26412;&#25552;&#31034;&#30340;&#21387;&#32553;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00489</link><description>&lt;p&gt;
PROMPT-SAW&#65306;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#36827;&#34892;&#25991;&#26412;&#25552;&#31034;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PROMPT-SAW&#27169;&#22411;&#65292;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#26469;&#23454;&#29616;&#25991;&#26412;&#25552;&#31034;&#30340;&#21387;&#32553;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22810;&#31181;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#25552;&#31034;&#26159;LLM&#25512;&#29702;&#20013;&#30340;&#22522;&#26412;&#24037;&#20855;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36229;&#38271;&#25552;&#31034;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25104;&#26412;&#12290;&#29616;&#26377;&#30340;&#21387;&#32553;&#38271;&#25552;&#31034;&#30340;&#23581;&#35797;&#23548;&#33268;&#21387;&#32553;&#25552;&#31034;&#22312;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#23545;&#25552;&#31034;&#25928;&#29992;&#20135;&#29983;&#26377;&#23475;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PROMPT-SAW&#65306;&#36890;&#36807;&#20851;&#31995;&#24863;&#30693;&#22270;&#36827;&#34892;&#25552;&#31034;&#21387;&#32553;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#24863;&#30693;&#25552;&#31034;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;PROMPT-SAW&#20351;&#29992;&#25552;&#31034;&#30340;&#25991;&#26412;&#20449;&#24687;&#26500;&#24314;&#22270;&#24418;&#65292;&#22312;&#22270;&#24418;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#20803;&#32032;&#65292;&#20174;&#32780;&#24471;&#20986;&#21387;&#32553;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;GSM8K-AUG&#65292;&#21363;&#29616;&#26377;GSM8k&#22522;&#20934;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#29992;&#20110;&#20219;&#21153;&#19981;&#21487;&#30693;&#25552;&#31034;&#65292;&#20197;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00489v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platf
&lt;/p&gt;</description></item><item><title>CoLLEGe&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28789;&#27963;&#30340;&#26032;&#27010;&#24565;&#23884;&#20837;&#65292;&#29992;&#20110;&#29616;&#20195;&#21270;&#23569;&#26679;&#26412;&#27010;&#24565;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.15362</link><description>&lt;p&gt;
CoLLEGe: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#23884;&#20837;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CoLLEGe: Concept Embedding Generation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15362
&lt;/p&gt;
&lt;p&gt;
CoLLEGe&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28789;&#27963;&#30340;&#26032;&#27010;&#24565;&#23884;&#20837;&#65292;&#29992;&#20110;&#29616;&#20195;&#21270;&#23569;&#26679;&#26412;&#27010;&#24565;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24555;&#36895;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#36890;&#24120;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#24494;&#35843;&#36807;&#31243;&#25165;&#33021;&#23398;&#20064;&#24471;&#26356;&#31283;&#20581;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CoLLEGe&#65288;Concept Learning with Language Embedding Generation&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29616;&#20195;&#21270;&#30340;&#23569;&#26679;&#26412;&#27010;&#24565;&#23398;&#20064;&#12290;CoLLEGe&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#21477;&#23376;&#25110;&#23450;&#20041;&#29983;&#25104;&#26032;&#27010;&#24565;&#30340;&#28789;&#27963;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20803;&#23398;&#20064;&#30446;&#26631;&#21482;&#26159;&#20419;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#38543;&#21518;&#30340;&#21477;&#23376;&#20013;&#36827;&#34892;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#65292;&#20351;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15362v1 Announce Type: cross  Abstract: Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept lear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;Stackelberg Mean Field Game&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#26080;&#27169;&#22411;Stackelberg&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12093</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#35266;&#22522;&#30784;&#30340;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#23398;&#20064;&#65306;&#19968;&#31181;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#22343;&#22330;&#21338;&#24328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;Stackelberg Mean Field Game&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#26080;&#27169;&#22411;Stackelberg&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#22312;&#20419;&#36827;&#32463;&#27982;&#22686;&#38271;&#21644;&#31038;&#20250;&#31283;&#23450;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#22522;&#20110;Stackelberg Mean Field Game&#65288;SMFG&#65289;&#27169;&#22411;&#65292;&#23558;&#26368;&#20248;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#38382;&#39064;&#24314;&#27169;&#65292;&#20854;&#20013;&#25919;&#24220;&#20316;&#20026;&#25919;&#31574;&#21046;&#23450;&#30340;&#39046;&#23548;&#32773;&#65292;&#22823;&#35268;&#27169;&#23478;&#24237;&#21160;&#24577;&#21709;&#24212;&#20026;&#36861;&#38543;&#32773;&#12290;&#36825;&#31181;&#24314;&#27169;&#26041;&#27861;&#25429;&#25417;&#20102;&#25919;&#24220;&#21644;&#22823;&#35268;&#27169;&#23478;&#24237;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#21160;&#24577;&#21338;&#24328;&#65292;&#24182;&#21487;&#20197;&#35299;&#37322;&#22320;&#35780;&#20272;&#22522;&#20110;&#24494;&#35266;&#22522;&#30784;&#30340;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#25928;&#26524;&#65292;&#36825;&#26159;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;SMFG&#30340;&#26041;&#27861;&#65292;&#23558;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;Stackelberg&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#65288;SMFRL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#29420;&#31435;&#20110;&#20808;&#21069;&#30340;&#29615;&#22659;&#30693;&#35782;&#21644;&#36716;&#21464;&#36816;&#34892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;SMFG&#26041;&#27861;&#22312;&#32463;&#27982;&#25919;&#31574;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12093v1 Announce Type: cross  Abstract: Effective macroeconomic policies play a crucial role in promoting economic growth and social stability. This paper models the optimal macroeconomic policy problem based on the \textit{Stackelberg Mean Field Game} (SMFG), where the government acts as the leader in policy-making, and large-scale households dynamically respond as followers. This modeling method captures the asymmetric dynamic game between the government and large-scale households, and interpretably evaluates the effects of macroeconomic policies based on microfoundations, which is difficult for existing methods to achieve. We also propose a solution for SMFGs, incorporating pre-training on real data and a model-free \textit{Stackelberg mean-field reinforcement learning }(SMFRL) algorithm, which operates independently of prior environmental knowledge and transitions. Our experimental results showcase the superiority of the SMFG method over other economic policies in terms 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#8220;&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;&#8221;&#26469;&#35299;&#20915;&#21463;&#26410;&#30693;&#25805;&#32437;&#24433;&#21709;&#30340;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#33021;&#20165;&#30693;&#36947;&#19968;&#37096;&#20998;&#21463;&#24433;&#21709;&#26679;&#26412;&#12290;&#21457;&#29616;&#32416;&#27491;&#28040;&#38500;&#38382;&#39064;&#19982;&#20256;&#32479;&#20197;&#38544;&#31169;&#20026;&#23548;&#21521;&#30340;&#28040;&#38500;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.14015</link><description>&lt;p&gt;
&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Corrective Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#8220;&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;&#8221;&#26469;&#35299;&#20915;&#21463;&#26410;&#30693;&#25805;&#32437;&#24433;&#21709;&#30340;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#33021;&#20165;&#30693;&#36947;&#19968;&#37096;&#20998;&#21463;&#24433;&#21709;&#26679;&#26412;&#12290;&#21457;&#29616;&#32416;&#27491;&#28040;&#38500;&#38382;&#39064;&#19982;&#20256;&#32479;&#20197;&#38544;&#31169;&#20026;&#23548;&#21521;&#30340;&#28040;&#38500;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#38754;&#20020;&#25968;&#25454;&#23436;&#25972;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;&#20174;&#20114;&#32852;&#32593;&#20013;&#33719;&#21462;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#26524;&#27169;&#22411;&#24320;&#21457;&#32773;&#21457;&#29616;&#26576;&#20123;&#25968;&#25454;&#34987;&#31713;&#25913;&#25110;&#38169;&#35823;&#65292;&#20182;&#20204;&#21487;&#20197;&#37319;&#21462;&#20160;&#20040;&#25514;&#26045;&#12290;&#36825;&#20123;&#34987;&#31713;&#25913;&#30340;&#25968;&#25454;&#20250;&#23548;&#33268;&#19981;&#21033;&#24433;&#21709;&#65292;&#22914;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#26679;&#26412;&#30340;&#25915;&#20987;&#12289;&#31995;&#32479;&#24615;&#20559;&#35265;&#65292;&#20197;&#21450;&#22312;&#26576;&#20123;&#36755;&#20837;&#39046;&#22495;&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#12290;&#36890;&#24120;&#65292;&#24182;&#38750;&#25152;&#26377;&#34987;&#31713;&#25913;&#30340;&#35757;&#32451;&#26679;&#26412;&#37117;&#26159;&#24050;&#30693;&#30340;&#65292;&#32780;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#20195;&#34920;&#24615;&#30340;&#21463;&#24433;&#21709;&#25968;&#25454;&#34987;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14015v1 Announce Type: cross  Abstract: Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize "Corrective Machine Unlearning" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22522;&#30784;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#38024;&#23545;&#24615;&#24494;&#35843;&#65292;&#29978;&#33267;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#21319;&#20102;&#30456;&#20851;&#27861;&#24459;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.09693</link><description>&lt;p&gt;
BLT: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22788;&#29702;&#22522;&#30784;&#27861;&#24459;&#25991;&#26412;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
BLT: Can Large Language Models Handle Basic Legal Text?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09693
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22522;&#30784;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#38024;&#23545;&#24615;&#24494;&#35843;&#65292;&#29978;&#33267;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#21319;&#20102;&#30456;&#20851;&#27861;&#24459;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#20687;GPT-4&#12289;Claude&#21644;{PaLM 2}&#36825;&#26679;&#30340;&#26368;&#22909;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#22312;&#22788;&#29702;&#22522;&#30784;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#24459;&#24072;&#21644;&#27861;&#24459;&#21161;&#29702;&#26399;&#26395;LLM&#38646;-shot&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#27604;&#22914;&#26597;&#25214;&#35777;&#35789;&#25991;&#20214;&#30340;&#26576;&#19968;&#34892;&#25110;&#21512;&#21516;&#30340;&#26576;&#20010;&#23376;&#37096;&#20998;&#30340;&#25991;&#26412;&#12290;LLM&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#24046;&#21170;&#34920;&#29616;&#23545;&#23427;&#20204;&#22312;&#27861;&#24459;&#23454;&#36341;&#20013;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36825;&#20123;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#29978;&#33267;&#20351;&#19968;&#20010;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#25509;&#36817;&#23436;&#32654;&#65292;&#24182;&#19988;&#36824;&#25552;&#21319;&#20102;&#30456;&#20851;&#27861;&#24459;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#35768;&#22810;&#39046;&#22495;&#25152;&#38656;&#30340;&#31616;&#21333;&#34892;&#20026;&#22312;&#22522;&#30784;LLM&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#65292;&#38500;&#38750;&#26377;&#39046;&#22495;&#19987;&#23478;&#30340;&#39069;&#22806;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09693v2 Announce Type: replace-cross  Abstract: We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM 2} currently perform poorly at basic legal text handling. We introduce a benchmark consisting of tasks that lawyers and paralegals would expect LLMs to handle zero-shot, such as looking up the text at a line of a witness deposition or at a subsection of a contract. LLMs' poor performance on this benchmark casts into doubt their reliability as-is for legal practice. However, fine-tuning for these tasks brings even a smaller model to near-perfect performance on our test set and also raises performance on a related legal task. These results suggest that many simple behaviors needed for a domain may not be present in foundational LLMs, without additional engagement from subject matter experts.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#34892;&#20154;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#37325;&#35201;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2308.02935</link><description>&lt;p&gt;
&#25581;&#31034;&#30450;&#28857;&#65306;&#23545;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#20844;&#24179;&#24615;&#30340;&#20851;&#38190;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Blind Spots: A Critical Examination of Fairness in Autonomous Driving Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.02935
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#34892;&#20154;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#37325;&#35201;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24050;&#32463;&#25193;&#23637;&#20102;&#26234;&#33021;&#36710;&#36742;&#29289;&#32852;&#32593;&#30340;&#33539;&#22260;&#65292;&#24182;&#25104;&#20026;Web&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20844;&#24179;&#24615;&#23545;&#20110;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#39640;&#36136;&#37327;&#26159;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#20013;&#30340;&#34892;&#20154;&#26816;&#27979;&#22120;&#30340;&#32972;&#26223;&#19979;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#34892;&#20154;&#26816;&#27979;&#22120;&#20844;&#24179;&#24615;&#30340;&#32508;&#21512;&#35780;&#20272;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#20986;&#29616;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20843;&#31181;&#34987;&#24191;&#27867;&#25506;&#32034;&#30340;DL&#34892;&#20154;&#26816;&#27979;&#22120;&#22312;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#20043;&#38388;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#24443;&#24213;&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#20026;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#27880;&#37322;&#65292;&#20849;&#28041;&#21450;8,311&#24352;&#22270;&#20687;&#65292;16,070&#20010;&#24615;&#21035;&#26631;&#31614;&#65292;20,115&#20010;&#24180;&#40836;&#26631;&#31614;&#21644;3,513&#20010;&#32932;&#33394;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#37325;&#35201;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.02935v2 Announce Type: replace-cross  Abstract: Autonomous driving systems have extended the spectrum of Web of Things for intelligent vehicles and have become an important component of the Web ecosystem. Similar to traditional Web-based applications, fairness is an essential aspect for ensuring the high quality of autonomous driving systems, particularly in the context of pedestrian detectors within them. However, there is an absence in the literature of a comprehensive assessment of the fairness of current Deep Learning (DL)-based pedestrian detectors. To fill the gap, we evaluate eight widely-explored DL-based pedestrian detectors across demographic groups on large-scale real-world datasets. To enable a thorough fairness evaluation, we provide extensive annotations for the datasets, resulting in 8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin tone labels. Our findings reveal significant fairness issues related to age. The undetected proportions f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#29615;&#22659;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#23646;&#24615;&#32487;&#25215;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#38750;&#24179;&#20961;&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#33021;&#21147;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.06640</link><description>&lt;p&gt;
&#23454;&#39564;&#29615;&#22659;&#33021;&#22815;&#20419;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#31283;&#20581;&#30340;&#35821;&#20041;&#23646;&#24615;&#25512;&#26029;&#20013;&#30340;&#34920;&#29616;&#65292;&#20294;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently. (arXiv:2401.06640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#29615;&#22659;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#23646;&#24615;&#32487;&#25215;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#38750;&#24179;&#20961;&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#33021;&#21147;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26080;&#20154;&#30417;&#30563;&#35780;&#20272;&#20984;&#26174;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#25191;&#34892;&#24847;&#20041;&#25552;&#21462;&#26041;&#38754;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22312;&#24341;&#20837;&#23454;&#39564;&#29615;&#22659;&#65288;&#22914;&#19978;&#19979;&#25991;&#31034;&#20363;&#21644;&#25351;&#23548;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;LMs&#30340;&#34920;&#29616;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#12290;&#37027;&#20040;&#36825;&#26159;&#21542;&#36866;&#29992;&#20110;&#20808;&#21069;&#30740;&#31350;&#30340;&#24847;&#20041;&#25935;&#24863;&#20219;&#21153;&#21602;&#65311;&#25105;&#20204;&#22312;&#25511;&#21046;&#19978;&#19979;&#25991;&#31034;&#20363;&#21644;&#25351;&#23548;&#20869;&#23481;&#30340;&#21069;&#25552;&#19979;&#65292;&#23545;&#23454;&#39564;&#29615;&#22659;&#23545;&#20110;&#25552;&#39640;LMs&#22312;&#25191;&#34892;&#23646;&#24615;&#32487;&#25215;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#20219;&#21153;&#26159;&#39044;&#20808;&#34920;&#26126;LMs&#26080;&#27861;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23454;&#39564;&#29615;&#22659;&#30830;&#23454;&#21487;&#20197;&#23548;&#33268;LMs&#22312;&#23646;&#24615;&#32487;&#25215;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#26159;&#19981;&#19968;&#33268;&#30340;&#65306;&#36890;&#36807;&#23545;&#20219;&#21153;&#36827;&#34892;&#26368;&#23567;&#25913;&#20889;&#65292;&#21457;&#29616;&#19968;&#20123;LMs&#20174;&#36755;&#20837;&#20013;&#25429;&#25417;&#21040;&#27973;&#23618;&#30340;&#38750;&#35821;&#20041;&#24335;&#21551;&#21457;&#24335;&#20449;&#24687;&#65292;&#36825;&#34920;&#26126;&#35745;&#31639;&#26426;&#30340;&#34892;&#20026;&#20855;&#26377;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent zero-shot evaluations have highlighted important limitations in the abilities of language models (LMs) to perform meaning extraction. However, it is now well known that LMs can demonstrate radical improvements in the presence of experimental contexts such as in-context examples and instructions. How well does this translate to previously studied meaning-sensitive tasks? We present a case-study on the extent to which experimental contexts can improve LMs' robustness in performing property inheritance -- predicting semantic properties of novel concepts, a task that they have been previously shown to fail on. Upon carefully controlling the nature of the in-context examples and the instructions, our work reveals that they can indeed lead to non-trivial property inheritance behavior in LMs. However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computati
&lt;/p&gt;</description></item><item><title>&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#20102;&#26679;&#26412;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.05442</link><description>&lt;p&gt;
&#21151;&#33021;&#22270;&#27169;&#22411;&#65306;&#32467;&#26500;&#23454;&#29616;&#31163;&#32447;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05442
&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#20102;&#26679;&#26412;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#20026;&#20102;&#35299;&#20915;&#39044;&#27979;&#38382;&#39064;&#32780;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#32463;&#24120;&#24076;&#26395;&#23558;&#23427;&#20204;&#29992;&#20110;&#20248;&#21270;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#32452;&#34507;&#30333;&#36136;&#21450;&#20854;&#23545;&#24212;&#30340;&#33639;&#20809;&#27700;&#24179;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#20026;&#20855;&#26377;&#26368;&#39640;&#33639;&#20809;&#30340;&#26032;&#34507;&#30333;&#36136;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#65288;DDO&#65289;&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#36229;&#20986;&#20102;&#26631;&#20934;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#25104;&#21151;&#39044;&#27979;&#22312;&#35757;&#32451;&#38598;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#20248;&#20110;&#26368;&#20339;&#35774;&#35745;&#30340;&#26032;&#35774;&#35745;&#30340;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#29978;&#33267;&#19981;&#28165;&#26970;&#29616;&#26377;&#26041;&#27861;&#20160;&#20040;&#26102;&#20505;&#29978;&#33267;&#33021;&#27604;&#31616;&#21333;&#22320;&#36873;&#25321;&#25968;&#25454;&#38598;&#20013;&#26368;&#20339;&#35774;&#35745;&#30340;&#26420;&#32032;&#26041;&#27861;&#25191;&#34892;&#24471;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#24182;&#20174;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#20998;&#35299;&#23454;&#29616;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24341;&#21147;&#27874;&#27169;&#26495;&#24211;&#20013;&#21253;&#21547;&#39640;&#27425;&#35856;&#27874;&#27169;&#24335;&#65292;&#21033;&#29992;&#24341;&#21147;&#27874;&#27169;&#24335;&#20043;&#38388;&#30340;&#33258;&#28982;&#20851;&#32852;&#65292;&#21487;&#20197;&#22823;&#24133;&#24230;&#20943;&#23569;&#21305;&#37197;&#28388;&#27874;&#30340;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#25628;&#32034;&#24341;&#21147;&#27874;&#20107;&#20214;&#30340;&#28789;&#25935;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.15233</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#26041;&#27861;&#29992;&#20110;&#24102;&#26377;&#26356;&#39640;&#27425;&#35856;&#27874;&#30340;&#24341;&#21147;&#27874;&#27169;&#26495;&#24211;&#65306;&#36890;&#36807;&#21313;&#20493;&#20943;&#23569;&#21305;&#37197;&#28388;&#27874;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
A new approach to template banks of gravitational waves with higher harmonics: reducing matched-filtering cost by over an order of magnitude. (arXiv:2310.15233v1 [gr-qc])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24341;&#21147;&#27874;&#27169;&#26495;&#24211;&#20013;&#21253;&#21547;&#39640;&#27425;&#35856;&#27874;&#27169;&#24335;&#65292;&#21033;&#29992;&#24341;&#21147;&#27874;&#27169;&#24335;&#20043;&#38388;&#30340;&#33258;&#28982;&#20851;&#32852;&#65292;&#21487;&#20197;&#22823;&#24133;&#24230;&#20943;&#23569;&#21305;&#37197;&#28388;&#27874;&#30340;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#25628;&#32034;&#24341;&#21147;&#27874;&#20107;&#20214;&#30340;&#28789;&#25935;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#21147;&#27874;&#20107;&#20214;&#30340;&#25628;&#32034;&#20351;&#29992;&#20449;&#21495;&#27169;&#22411;&#25110;&#27169;&#26495;&#12290;&#30446;&#21069;&#22312;LIGO-Virgo-Kagra (LVK)&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#27169;&#26495;&#20165;&#27169;&#25311;&#20102;&#20449;&#21495;&#30340;&#20027;&#23548;&#22235;&#26497;&#27169;&#24335;$(\ell,m)=(2,2)$&#65292;&#24573;&#30053;&#20102;&#27425;&#35201;&#30340;&#39640;&#38454;&#27169;&#24335;(HM)&#20363;&#22914;$(\ell,m)=(3,3)$&#65292;$(4,4)$&#65292;&#36825;&#20123;&#27169;&#24335;&#26159;&#30001;&#24191;&#20041;&#30456;&#23545;&#35770;&#39044;&#27979;&#30340;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25628;&#32034;&#21487;&#33021;&#20250;&#22312;&#21442;&#25968;&#31354;&#38388;&#30340;&#19968;&#20123;&#26377;&#36259;&#21306;&#22495;&#65292;&#22914;&#39640;&#36136;&#37327;&#21644;&#38750;&#23545;&#31216;&#36136;&#37327;&#27604;&#30340;&#31995;&#32479;&#20013;&#22833;&#21435;&#23545;&#40657;&#27934;&#21512;&#24182;&#30340;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#31574;&#30053;&#65292;&#23558;HM&#21253;&#21547;&#22312;&#27169;&#26495;&#24211;&#20013;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#24335;&#20043;&#38388;&#30340;&#33258;&#28982;&#20851;&#32852;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#29275;&#39039;&#38468;&#21152;&#20844;&#24335;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#27169;&#25311;&#19982;&#32473;&#23450;$(2,2)$&#27874;&#24418;&#30456;&#23545;&#24212;&#30340;&#33258;&#26059;&#23545;&#40784;&#30340;$(3,3)$&#65292;$(4,4)$&#27874;&#24418;&#12290;&#21487;&#20197;&#23545;&#27599;&#20010;&#27169;&#24335;&#30340;&#25968;&#25454;&#36827;&#34892;&#21333;&#29420;&#28388;&#27874;&#65292;&#24471;&#21040;&#20449;&#22122;&#27604;(SNR)&#30340;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#65292;&#28982;&#21518;&#21487;&#20197;&#20197;&#30456;&#23545;&#24265;&#20215;&#30340;&#26041;&#24335;&#23558;&#20854;&#32452;&#21512;&#36215;&#26469;&#36827;&#34892;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searches for gravitational wave events use models, or templates, for the signals of interest. The templates used in current searches in the LIGO-Virgo-Kagra (LVK) data model the dominant quadrupole mode $(\ell,m)=(2,2)$ of the signals, and omit sub-dominant higher-order modes (HM) such as $(\ell,m)=(3,3)$, $(4,4)$, which are predicted by general relativity. Hence, these searches could lose sensitivity to black hole mergers in interesting parts of parameter space, such as systems with high-masses and asymmetric mass ratios. We develop a new strategy to include HM in template banks that exploits the natural connection between the modes. We use a combination of post-Newtonian formulae and machine learning tools to model aligned-spin $(3,3)$, $(4,4)$ waveforms corresponding to a given $(2,2)$ waveform. Each of these modes can be individually filtered against the data to yield separate timeseries of signal-to-noise ratios (SNR), which can be combined in a relatively inexpensive way to margi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#20013;&#20844;&#24179;&#22522;&#20934;&#27979;&#35797;&#21644;&#25216;&#26415;&#26041;&#27861;&#36873;&#25321;&#30340;&#20105;&#35758;&#65292;&#24182;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.06119</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36827;&#23637;&#65306;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#21644;&#24322;&#36136;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis. (arXiv:2310.06119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06119
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#20013;&#20844;&#24179;&#22522;&#20934;&#27979;&#35797;&#21644;&#25216;&#26415;&#26041;&#27861;&#36873;&#25321;&#30340;&#20105;&#35758;&#65292;&#24182;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#24191;&#27867;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#31995;&#32479;&#20013;&#65292;&#22914;&#20132;&#36890;&#21644;&#33021;&#28304;&#31995;&#32479;&#65292;&#23545;&#20110;&#29702;&#35299;&#21644;&#24433;&#21709;&#36825;&#20123;&#31995;&#32479;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;MTS&#20013;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#21644;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#33719;&#24471;&#20102;&#24456;&#22823;&#30340;&#27969;&#34892;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#21644;&#26102;&#31354;&#39044;&#27979;&#65288;STF&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#30340;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#21644;&#25216;&#26415;&#26041;&#27861;&#30340;&#36873;&#25321;&#22312;&#30456;&#20851;&#24037;&#20316;&#20013;&#19968;&#30452;&#23384;&#22312;&#20105;&#35758;&#12290;&#36825;&#20123;&#20105;&#35758;&#26174;&#33879;&#38459;&#30861;&#20102;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#36827;&#23637;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#20105;&#35758;&#65292;&#20197;&#25552;&#20379;&#23545;&#21462;&#24471;&#30340;&#36827;&#23637;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;&#20026;&#20102;&#35299;&#20915;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BasicTS&#65292;&#19968;&#20010;&#26088;&#22312;&#20844;&#24179;&#27604;&#36739;MTS&#39044;&#27979;&#30340;&#22522;&#20934;&#12290;BasicTS&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35757;&#32451;&#27969;&#31243;&#21644;&#21512;&#29702;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#33021;&#22815;&#23545;30&#22810;&#31181;&#27969;&#34892;&#30340;MTS&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20844;&#27491;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series (MTS) widely exists in real-word complex systems, such as traffic and energy systems, making their forecasting crucial for understanding and influencing these systems. Recently, deep learning-based approaches have gained much popularity for effectively modeling temporal and spatial dependencies in MTS, specifically in Long-term Time Series Forecasting (LTSF) and Spatial-Temporal Forecasting (STF). However, the fair benchmarking issue and the choice of technical approaches have been hotly debated in related work. Such controversies significantly hinder our understanding of progress in this field. Thus, this paper aims to address these controversies to present insights into advancements achieved. To resolve benchmarking issues, we introduce BasicTS, a benchmark designed for fair comparisons in MTS forecasting. BasicTS establishes a unified training pipeline and reasonable evaluation settings, enabling an unbiased evaluation of over 30 popular MTS forecasting mode
&lt;/p&gt;</description></item><item><title>GOOSE&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#40517;&#30340;&#34892;&#20026;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#24037;&#31243;&#25361;&#25112;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10420</link><description>&lt;p&gt;
GOOSE&#31639;&#27861;: &#19968;&#20010;&#24378;&#22823;&#30340;&#20248;&#21270;&#24037;&#20855;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24037;&#31243;&#25361;&#25112;&#21450;&#26356;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
GOOSE Algorithm: A Powerful Optimization Tool for Real-World Engineering Challenges and Beyond. (arXiv:2307.10420v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10420
&lt;/p&gt;
&lt;p&gt;
GOOSE&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#40517;&#30340;&#34892;&#20026;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#24037;&#31243;&#25361;&#25112;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;GOOSE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#40517;&#22312;&#20241;&#24687;&#21644;&#35269;&#39135;&#26102;&#30340;&#34892;&#20026;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#40517;&#38752;&#30528;&#19968;&#21482;&#33151;&#20445;&#25345;&#24179;&#34913;&#65292;&#20197;&#23432;&#25252;&#21644;&#20445;&#25252;&#32676;&#20307;&#20013;&#30340;&#20854;&#20182;&#20010;&#20307;&#12290;GOOSE&#31639;&#27861;&#22312;19&#20010;&#30693;&#21517;&#30340;&#22522;&#20934;&#27979;&#35797;&#20989;&#25968;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#19982;&#36951;&#20256;&#31639;&#27861;(GA)&#12289;&#31890;&#23376;&#32676;&#20248;&#21270;(PSO)&#12289;&#34619;&#34579;&#31639;&#27861;(DA)&#21644;&#36866;&#24212;&#24615;&#20381;&#36182;&#20248;&#21270;&#22120;(FDO)&#30340;&#27604;&#36739;&#30740;&#31350;&#26469;&#39564;&#35777;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#22312;10&#20010;&#29616;&#20195;&#22522;&#20934;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#34619;&#34579;&#31639;&#27861;&#12289;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;(WOA)&#21644;&#40144;&#40060;&#32676;&#31639;&#27861;(SSA)&#31561;&#19977;&#20010;&#26368;&#36817;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;GOOSE&#31639;&#27861;&#36824;&#22312;5&#20010;&#32463;&#20856;&#22522;&#20934;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23558;&#25152;&#24471;&#32467;&#26524;&#19982;&#36866;&#24212;&#24615;&#20381;&#36182;&#20248;&#21270;&#22120;(FDO)&#12289;FOX&#20248;&#21270;&#22120;&#12289;&#34678;&#20248;&#21270;&#31639;&#27861;(BOA)&#12289;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#12289;&#20154;&#24037;&#34562;&#32676;&#31639;&#27861;&#21644;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#31561;&#20845;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes the GOOSE algorithm as a novel metaheuristic algorithm based on the goose's behavior during rest and foraging. The goose stands on one leg and keeps his balance to guard and protect other individuals in the flock. The GOOSE algorithm is benchmarked on 19 well-known benchmark test functions, and the results are verified by a comparative study with genetic algorithm (GA), particle swarm optimization (PSO), dragonfly algorithm (DA), and fitness dependent optimizer (FDO). In addition, the proposed algorithm is tested on 10 modern benchmark functions, and the gained results are compared with three recent algorithms, such as the dragonfly algorithm, whale optimization algorithm (WOA), and salp swarm algorithm (SSA). Moreover, the GOOSE algorithm is tested on 5 classical benchmark functions, and the obtained results are evaluated with six algorithms, such as fitness dependent optimizer (FDO), FOX optimizer, butterfly optimization algorithm (BOA), whale optimization algorit
&lt;/p&gt;</description></item><item><title>HyperDreamBooth&#26159;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#20013;&#24555;&#36895;&#29983;&#25104;&#20010;&#24615;&#21270;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#24182;&#21516;&#26102;&#20445;&#30041;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.06949</link><description>&lt;p&gt;
HyperDreamBooth&#65306;&#29992;&#20110;&#24555;&#36895;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#36229;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. (arXiv:2307.06949v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06949
&lt;/p&gt;
&lt;p&gt;
HyperDreamBooth&#26159;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#20013;&#24555;&#36895;&#29983;&#25104;&#20010;&#24615;&#21270;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#24182;&#21516;&#26102;&#20445;&#30041;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#20010;&#20307;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;&#28982;&#32780;&#65292;&#20010;&#24615;&#21270;&#36807;&#31243;&#22312;&#26102;&#38388;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#27599;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#26102;&#38388;&#25237;&#20837;&#65292;&#20026;&#27599;&#20010;&#20027;&#39064;&#23384;&#20648;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#20250;&#23545;&#23384;&#20648;&#23481;&#37327;&#25552;&#20986;&#35201;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperDreamBooth-&#19968;&#31181;&#33021;&#22815;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#26377;&#25928;&#29983;&#25104;&#19968;&#32452;&#20010;&#24615;&#21270;&#26435;&#37325;&#30340;&#36229;&#32593;&#32476;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#26435;&#37325;&#32452;&#21512;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#24182;&#25645;&#37197;&#24555;&#36895;&#24494;&#35843;&#65292;HyperDreamBooth&#33021;&#22815;&#20197;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#29983;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20027;&#39064;&#32454;&#33410;&#21516;&#26102;&#20063;&#20445;&#25345;&#27169;&#22411;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#32422;50&#20493;&#20307;&#29616;&#20102;&#38754;&#37096;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#21457;&#29616;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#30340;&#21478;&#19968;&#20010;&#35282;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04955</link><description>&lt;p&gt;
&#35770;&#31070;&#32463;&#32593;&#32476;&#23545;&#38477;&#35299;&#22810;&#36793;&#24418;&#30340;&#24863;&#30693;&#23384;&#22312;&#30340;&#22522;&#26412;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Degraded Polygons Raise Fundamental Questions of Neural Network Perception. (arXiv:2306.04955v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#21457;&#29616;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#30340;&#21478;&#19968;&#20010;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24448;&#24448;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#19981;&#19968;&#33268;&#30340;&#34892;&#20026;&#65306;&#20174;&#23545;&#25239;&#25915;&#20987;&#21040;&#22270;&#20687;&#25439;&#22351;&#65292;&#28145;&#24230;&#23398;&#20064;&#35270;&#35273;&#27169;&#22411;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#28982;&#32780;&#20154;&#31867;&#21364;&#33021;&#22815;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24674;&#22797;&#21463;&#25439;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#22312;&#20154;&#31867;&#35270;&#35273;&#30340;&#8220;&#35782;&#21035;&#32452;&#20214;&#8221;&#29702;&#35770;&#20013;&#39318;&#27425;&#24341;&#20837;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#33258;&#21160;&#21270;&#24418;&#29366;&#21487;&#24674;&#22797;&#24615;&#27979;&#35797;&#65292;&#24555;&#36895;&#29983;&#25104;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23558;&#21382;&#21490;&#19978;&#25163;&#21160;&#21019;&#24314;&#22270;&#20687;&#21487;&#24674;&#22797;&#24615;&#23454;&#39564;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#29616;&#20195;&#21270;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#22810;&#36793;&#24418;&#30340;&#33021;&#21147;&#20197;&#21450;&#20854;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well-known that modern computer vision systems often exhibit behaviors misaligned with those of humans: from adversarial attacks to image corruptions, deep learning vision models suffer in a variety of settings that humans capably handle. In light of these phenomena, here we introduce another, orthogonal perspective studying the human-machine vision gap. We revisit the task of recovering images under degradation, first introduced over 30 years ago in the Recognition-by-Components theory of human vision. Specifically, we study the performance and behavior of neural networks on the seemingly simple task of classifying regular polygons at varying orders of degradation along their perimeters. To this end, we implement the Automated Shape Recoverability Test for rapidly generating large-scale datasets of perimeter-degraded regular polygons, modernizing the historically manual creation of image recoverability experiments. We then investigate the capacity of neural networks to recognize
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item></channel></rss>