<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>AI&#31185;&#23398;&#23478;&#20195;&#29702;&#32467;&#21512;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36171;&#33021;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#65292;&#23545;&#22810;&#20010;&#39046;&#22495;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02831</link><description>&lt;p&gt;
&#21033;&#29992;AI&#20195;&#29702;&#36171;&#33021;&#29983;&#29289;&#21307;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Empowering Biomedical Discovery with AI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02831
&lt;/p&gt;
&lt;p&gt;
AI&#31185;&#23398;&#23478;&#20195;&#29702;&#32467;&#21512;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36171;&#33021;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#65292;&#23545;&#22810;&#20010;&#39046;&#22495;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#24819;&#8220;AI&#31185;&#23398;&#23478;&#8221;&#20316;&#20026;&#31995;&#32479;&#65292;&#33021;&#22815;&#36827;&#34892;&#24576;&#30097;&#24615;&#23398;&#20064;&#21644;&#25512;&#29702;&#65292;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#19982;&#23454;&#39564;&#24179;&#21488;&#38598;&#25104;&#30340;&#21327;&#20316;&#20195;&#29702;&#65292;&#36171;&#33021;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#12290;&#29983;&#29289;&#21307;&#23398;AI&#20195;&#29702;&#19981;&#26159;&#35201;&#21076;&#38500;&#20154;&#31867;&#22312;&#21457;&#29616;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65292;&#32780;&#26159;&#32467;&#21512;&#20154;&#31867;&#30340;&#21019;&#36896;&#21147;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;&#21033;&#29992;AI&#20998;&#26512;&#22823;&#22411;&#25968;&#25454;&#38598;&#12289;&#23548;&#33322;&#20551;&#35774;&#31354;&#38388;&#65292;&#24182;&#25191;&#34892;&#37325;&#22797;&#24615;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#20195;&#29702;&#25797;&#38271;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#33258;&#25105;&#35780;&#20272;&#21644;&#35268;&#21010;&#21457;&#29616;&#24037;&#20316;&#27969;&#31243;&#12290;&#23427;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#32467;&#26500;&#21270;&#35760;&#24518;&#65292;&#20197;&#20415;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#25972;&#21512;&#31185;&#23398;&#30693;&#35782;&#12289;&#29983;&#29289;&#21407;&#29702;&#21644;&#29702;&#35770;&#12290;AI&#20195;&#29702;&#21487;&#20197;&#24433;&#21709;&#20174;&#28151;&#21512;&#32454;&#32990;&#27169;&#25311;&#12289;&#21487;&#32534;&#31243;&#25511;&#21046;&#34920;&#22411;&#21040;&#32454;&#32990;&#30005;&#36335;&#35774;&#35745;&#20197;&#21450;&#26032;&#30103;&#27861;&#24320;&#21457;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02831v1 Announce Type: new  Abstract: We envision 'AI scientists' as systems capable of skeptical learning and reasoning that empower biomedical research through collaborative agents that integrate machine learning tools with experimental platforms. Rather than taking humans out of the discovery process, biomedical AI agents combine human creativity and expertise with AI's ability to analyze large datasets, navigate hypothesis spaces, and execute repetitive tasks. AI agents are proficient in a variety of tasks, including self-assessment and planning of discovery workflows. These agents use large language models and generative models to feature structured memory for continual learning and use machine learning tools to incorporate scientific knowledge, biological principles, and theories. AI agents can impact areas ranging from hybrid cell simulation, programmable control of phenotypes, and the design of cellular circuits to the development of new therapies.
&lt;/p&gt;</description></item><item><title>&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#39044;&#31639;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#36873;&#25321;&#25490;&#21517;&#27425;&#20110;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.00725</link><description>&lt;p&gt;
&#36234;&#22823;&#36234;&#22909;&#21527;&#65311;&#36890;&#36807;&#39044;&#31639;&#37325;&#26032;&#20998;&#37197;&#25913;&#36827;LLM&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
The Larger the Better? Improved LLM Code-Generation via Budget Reallocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00725
&lt;/p&gt;
&lt;p&gt;
&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#39044;&#31639;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#36873;&#25321;&#25490;&#21517;&#27425;&#20110;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27604;&#36739;&#23567;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20063;&#38656;&#35201;&#26356;&#22810;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#23601;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#20004;&#20010;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;&#39044;&#31639;&#19979;&#36816;&#34892;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#65288;&#20363;&#22914;&#65292;&#35745;&#31639;&#36164;&#28304;&#65292;&#36816;&#34892;&#26102;&#38388;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#22823;&#23567;&#30340;&#20195;&#30721;&#29983;&#25104;LLMs&#65292;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#20363;&#22914;&#36816;&#34892;&#19968;&#20010;70B&#27169;&#22411;&#19968;&#27425;&#19982;&#20174;13B&#27169;&#22411;&#29983;&#25104;&#20116;&#20010;&#36755;&#20986;&#24182;&#36873;&#25321;&#19968;&#20010;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26631;&#20934;&#21333;&#20803;&#27979;&#35797;&#35774;&#32622;&#20013;&#65292;&#21453;&#22797;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#26368;&#39640;&#21487;&#36798;15%&#30340;&#22686;&#30410;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#36739;&#23567;&#27169;&#22411;&#20013;&#22522;&#20110;&#25490;&#21517;&#30340;&#20505;&#36873;&#36873;&#25321;&#34920;&#29616;&#19981;&#21450;&#26469;&#33258;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;&#36739;&#23567;&#27169;&#22411;&#32780;&#38750;&#36739;&#22823;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00725v1 Announce Type: cross  Abstract: It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the imp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ReMamber&#65292;&#19968;&#31181;&#25972;&#21512;&#20102;Mamba&#21644;&#22810;&#27169;&#24577;Mamba Twister&#22359;&#30340;&#26032;&#22411;RIS&#26550;&#26500;&#65292;&#36890;&#36807;&#20854;&#29420;&#29305;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#25197;&#26354;&#26426;&#21046;&#23454;&#29616;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;</title><link>https://arxiv.org/abs/2403.17839</link><description>&lt;p&gt;
ReMamber&#65306;&#20351;&#29992;Mamba Twister&#23454;&#29616;&#24341;&#29992;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ReMamber: Referring Image Segmentation with Mamba Twister
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ReMamber&#65292;&#19968;&#31181;&#25972;&#21512;&#20102;Mamba&#21644;&#22810;&#27169;&#24577;Mamba Twister&#22359;&#30340;&#26032;&#22411;RIS&#26550;&#26500;&#65292;&#36890;&#36807;&#20854;&#29420;&#29305;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#25197;&#26354;&#26426;&#21046;&#23454;&#29616;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#29992;&#22270;&#20687;&#20998;&#21106;&#65288;RIS&#65289;&#21033;&#29992;&#21464;&#25442;&#22120;&#22312;&#35299;&#37322;&#22797;&#26434;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20108;&#27425;&#35745;&#31639;&#25104;&#26412;&#20351;&#20854;&#22312;&#25429;&#25417;&#36828;&#31243;&#35270;&#35273;-&#35821;&#35328;&#20381;&#36182;&#24615;&#26041;&#38754;&#28040;&#32791;&#36164;&#28304;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;Mamba&#36890;&#36807;&#39640;&#25928;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#22312;&#22788;&#29702;&#26041;&#38754;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23558;Mamba&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#20132;&#20114;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#22240;&#20026;&#36890;&#36947;&#20132;&#20114;&#19981;&#36275;&#65292;&#26080;&#27861;&#26377;&#25928;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReMamber&#65292;&#36825;&#26159;&#19968;&#31181;&#25972;&#21512;&#20102;Mamba&#21644;&#22810;&#27169;&#24577;Mamba Twister&#22359;&#24378;&#22823;&#21151;&#33021;&#30340;&#26032;&#22411;RIS&#26550;&#26500;&#12290;Mamba Twister&#36890;&#36807;&#20854;&#29420;&#29305;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#25197;&#26354;&#26426;&#21046;&#26126;&#30830;&#24314;&#27169;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#20854;&#29420;&#29305;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#25197;&#26354;&#26426;&#21046;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;ReMamber&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20854;&#20182;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17839v1 Announce Type: cross  Abstract: Referring Image Segmentation (RIS) leveraging transformers has achieved great success on the interpretation of complex visual-language tasks. However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies. Fortunately, Mamba addresses this with efficient linear complexity in processing. However, directly applying Mamba to multi-modal interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of multi-modal data. In this paper, we propose ReMamber, a novel RIS architecture that integrates the power of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly models image-text interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism. We achieve the state-of-the-art on three challenging benchmarks. Moreover, we conduct thorough analyses of ReMamber and discuss other
&lt;/p&gt;</description></item><item><title>AutoRE &#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF&#30340;&#26032;&#39062;&#20851;&#31995;&#25277;&#21462;&#33539;&#24335;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#22312;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;</title><link>https://arxiv.org/abs/2403.14888</link><description>&lt;p&gt;
AutoRE&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
AutoRE: Document-Level Relation Extraction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14888
&lt;/p&gt;
&lt;p&gt;
AutoRE &#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF&#30340;&#26032;&#39062;&#20851;&#31995;&#25277;&#21462;&#33539;&#24335;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#22312;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#24322;&#24120;&#33021;&#21147;&#65292;&#36825;&#28608;&#21169;&#30528;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#23427;&#20204;&#36827;&#34892;&#20449;&#24687;&#25277;&#21462;(IE)&#20219;&#21153;&#65292;&#21253;&#25324;&#20851;&#31995;&#25277;&#21462;(RE)&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;(SentRE)&#20219;&#21153;&#65292;&#36825;&#36890;&#24120;&#28085;&#30422;&#20102;&#21333;&#20010;&#21477;&#23376;&#20869;&#30340;&#19968;&#32452;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26041;&#27861;&#37319;&#29992;&#23558;&#20851;&#31995;&#20316;&#20026;&#20505;&#36873;&#36873;&#25321;&#38598;&#25104;&#21040;&#25552;&#31034;&#27169;&#26495;&#20013;&#30340;&#26041;&#24335;&#65292;&#23548;&#33268;&#22312;&#22788;&#29702;&#20998;&#24067;&#22312;&#32473;&#23450;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#26102;&#25928;&#29575;&#20302;&#19979;&#65292;&#24615;&#33021;&#20122;&#20248;&#65292;&#24182;&#22312;&#22788;&#29702;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;(DocRE)&#20219;&#21153;&#26102;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoRE&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;DocRE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF(Re
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14888v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional abilities in comprehending and generating text, motivating numerous researchers to utilize them for Information Extraction (IE) purposes, including Relation Extraction (RE). Nonetheless, most existing methods are predominantly designed for Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a restricted set of relations and triplet facts within a single sentence. Furthermore, certain approaches resort to treating relations as candidate choices integrated into prompt templates, leading to inefficient processing and suboptimal performance when tackling Document-Level Relation Extraction (DocRE) tasks, which entail handling multiple relations and triplet facts distributed across a given document, posing distinct challenges. To overcome these limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel RE extraction paradigm named RHF (Re
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#22836;&#30340;&#25805;&#20316;&#65292;&#25581;&#31034;&#20102;&#32467;&#21512;&#20102;&#21477;&#27861;&#20381;&#36182;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#35821;&#20041;&#24863;&#24212;&#22836;&#30340;&#20986;&#29616;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13055</link><description>&lt;p&gt;
&#35782;&#21035;&#35821;&#20041;&#24863;&#24212;&#22836;&#20197;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Identifying Semantic Induction Heads to Understand In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#22836;&#30340;&#25805;&#20316;&#65292;&#25581;&#31034;&#20102;&#32467;&#21512;&#20102;&#21477;&#27861;&#20381;&#36182;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#35821;&#20041;&#24863;&#24212;&#22836;&#30340;&#20986;&#29616;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#25512;&#29702;&#36923;&#36753;&#30340;&#19981;&#36879;&#26126;&#24615;&#24341;&#21457;&#20102;&#23545;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#65292;&#25105;&#20204;&#23545;&#27880;&#24847;&#21147;&#22836;&#30340;&#25805;&#20316;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#26159;&#21542;&#32534;&#30721;&#20102;&#33258;&#28982;&#35821;&#35328;&#20013;&#23384;&#22312;&#30340;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#65306;&#20174;&#21477;&#23376;&#20013;&#35299;&#26512;&#30340;&#21477;&#27861;&#20381;&#36182;&#21644;&#30693;&#35782;&#22270;&#20013;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#27880;&#24847;&#21147;&#22836;&#34920;&#29616;&#20986;&#19968;&#31181;&#27169;&#24335;&#65292;&#21363;&#24403;&#20851;&#27880;&#22836;&#26631;&#35760;&#26102;&#65292;&#23427;&#20204;&#20250;&#22238;&#24518;&#36215;&#23614;&#26631;&#35760;&#65292;&#24182;&#22686;&#21152;&#36825;&#20123;&#23614;&#26631;&#35760;&#30340;&#36755;&#20986;&#36923;&#36753;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#35821;&#20041;&#24863;&#24212;&#22836;&#30340;&#21046;&#23450;&#19982;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#20986;&#29616;&#23384;&#22312;&#23494;&#20999;&#20851;&#32852;&#12290;&#35821;&#20041;&#27880;&#24847;&#21147;&#22836;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#25105;&#20204;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13055v1 Announce Type: cross  Abstract: Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our
&lt;/p&gt;</description></item><item><title>HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;</title><link>https://arxiv.org/abs/2402.12656</link><description>&lt;p&gt;
HyperMoE: &#36890;&#36807;&#19987;&#23478;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#23454;&#29616;&#26356;&#22909;&#30340;&#19987;&#23478;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12656
&lt;/p&gt;
&lt;p&gt;
HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;(MoE)&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#23558;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#36335;&#30001;&#21040;&#29305;&#23450;&#30340;&#19987;&#23478;&#23376;&#38598;&#36827;&#34892;&#22788;&#29702;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#19987;&#23478;&#30693;&#35782;&#30340;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#38754;&#20020;&#25361;&#25112;&#65306;&#36890;&#36807;&#22686;&#21152;&#23545;&#19987;&#23478;&#30693;&#35782;&#30340;&#20351;&#29992;&#26469;&#22686;&#24378;&#24615;&#33021;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#31232;&#30095;&#24230;&#20943;&#23569;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#30683;&#30462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperMoE&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;Hypernetworks&#20043;&#19978;&#30340;&#26032;&#39062;MoE&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;MoE&#30340;&#35745;&#31639;&#36807;&#31243;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#27010;&#24565;&#36827;&#34892;&#20102;&#38598;&#25104;&#12290;&#22522;&#20110;&#26410;&#36873;&#25321;&#19987;&#23478;&#20449;&#24687;&#29983;&#25104;&#30340;&#29305;&#23450;&#27169;&#22359;&#20316;&#20026;&#34917;&#20805;&#20449;&#24687;&#65292;&#20801;&#35768;&#26410;&#34987;&#36873;&#20013;&#30340;&#19987;&#23478;&#30340;&#30693;&#35782;&#22312;&#20445;&#25345;&#36873;&#25321;&#31232;&#30095;&#24615;&#30340;&#21516;&#26102;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12656v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;</title><link>https://arxiv.org/abs/2402.10885</link><description>&lt;p&gt;
&#22522;&#20110;3D&#22330;&#26223;&#34920;&#31034;&#30340;3D&#25193;&#25955;&#22120;Actor&#65306;&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
3D Diffuser Actor: Policy Diffusion with 3D Scene Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10885
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25193;&#25955;&#31574;&#30053;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25193;&#25955;&#31574;&#30053;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#22522;&#20110;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#32463;&#34920;&#29616;&#20986;&#20248;&#20110;&#30830;&#23450;&#24615;&#21644;&#20854;&#20182;&#22522;&#20110;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;3D&#26426;&#22120;&#20154;&#31574;&#30053;&#20351;&#29992;&#20174;&#21333;&#20010;&#25110;&#22810;&#20010;&#25668;&#20687;&#22836;&#35270;&#35282;&#33719;&#21462;&#30340;&#24863;&#24212;&#28145;&#24230;&#32858;&#21512;&#30340;3D&#22330;&#26223;&#29305;&#24449;&#34920;&#31034;&#12290;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#27604;&#20854;2D&#23545;&#24212;&#29289;&#22312;&#25668;&#20687;&#26426;&#35270;&#35282;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#32479;&#19968;&#20102;&#36825;&#20004;&#26465;&#32447;&#36335;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#22120;Actor&#65292;&#36825;&#26159;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#23427;&#22312;&#32473;&#23450;&#35821;&#35328;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#26500;&#24314;&#35270;&#35273;&#22330;&#26223;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#22312;&#20854;&#19978;&#36827;&#34892;&#26465;&#20214;&#36845;&#20195;&#21435;&#22122;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#12290;&#22312;&#27599;&#20010;&#21435;&#22122;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#20272;&#35745;&#34920;&#31034;&#20026;3D&#22330;&#26223;&#20196;&#29260;&#65292;&#24182;&#39044;&#27979;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10885v1 Announce Type: cross  Abstract: We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#30340;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.00773</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#30340;&#23376;&#25277;&#26679;&#38598;&#21512;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures. (arXiv:2401.00773v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00773
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#30340;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#28151;&#21512;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#32479;&#35745;&#21407;&#29702;&#19978;&#26377;&#30452;&#35266;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#20316;&#20026;&#20256;&#32479;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#22312;&#32858;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#26126;&#26174;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#24191;&#27867;&#37319;&#29992;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#21463;&#21040;&#19982;&#26500;&#24314;&#26816;&#27979;&#22120;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#24322;&#24120;&#20540;&#30340;&#25935;&#24863;&#24615;&#26377;&#20851;&#30340;&#25361;&#25112;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#38598;&#21512;&#30340;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#65292;&#19981;&#20165;&#30830;&#20445;&#20102;&#39640;&#25928;&#35745;&#31639;&#65292;&#36824;&#22686;&#24378;&#20102;&#32467;&#26524;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic mixture models are acknowledged as a valuable tool for unsupervised outlier detection owing to their interpretability and intuitive grounding in statistical principles. Within this framework, Dirichlet process mixture models emerge as a compelling alternative to conventional finite mixture models for both clustering and outlier detection tasks. However, despite their evident advantages, the widespread adoption of Dirichlet process mixture models in unsupervised outlier detection has been hampered by challenges related to computational inefficiency and sensitivity to outliers during the construction of detectors. To tackle these challenges, we propose a novel outlier detection method based on ensembles of Dirichlet process Gaussian mixtures. The proposed method is a fully unsupervised algorithm that capitalizes on random subspace and subsampling ensembles, not only ensuring efficient computation but also enhancing the robustness of the resulting outlier detector. Moreover,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;</title><link>http://arxiv.org/abs/2309.08499</link><description>&lt;p&gt;
P-ROCKET: &#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification. (arXiv:2309.08499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;ROCKET&#21644;MINIROCKET&#22240;&#20854;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;ROCKET&#21644;MINIROCKET&#21033;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#38543;&#26426;&#19968;&#32500;&#21367;&#31215;&#26680;&#65292;&#21487;&#20197;&#24555;&#36895;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#39640;&#25928;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20840;&#38754;&#25429;&#25417;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#26469;&#35828;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36827;&#21270;&#31639;&#27861;S-ROCKET&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21098;&#26525;&#20887;&#20313;&#30340;&#21367;&#31215;&#26680;&#12290;&#28982;&#32780;&#65292;&#36827;&#21270;&#31639;&#27861;&#26412;&#36523;&#30340;&#29305;&#24615;&#23548;&#33268;&#22312;S-ROCKET&#20013;&#35780;&#20272;&#21367;&#31215;&#26680;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20013;&#65292;&#19982;&#30452;&#25509;&#35780;&#20272;&#20855;&#26377;&#38750;&#26174;&#33879;&#24046;&#24322;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;S-ROCKET&#19981;&#21516;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#36890;&#36807;&#28040;&#38500;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#36830;&#25509;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, two time series classification models, ROCKET and MINIROCKET, have attracted much attention for their low training cost and state-of-the-art accuracy. Utilizing random 1-D convolutional kernels without training, ROCKET and MINIROCKET can rapidly extract features from time series data, allowing for the efficient fitting of linear classifiers. However, to comprehensively capture useful features, a large number of random kernels are required, which is incompatible for resource-constrained devices. Therefore, a heuristic evolutionary algorithm named S-ROCKET is devised to recognize and prune redundant kernels. Nevertheless, the inherent nature of evolutionary algorithms renders the evaluation of kernels within S-ROCKET an unacceptable time-consuming process. In this paper, diverging from S-ROCKET, which directly evaluates random kernels with nonsignificant differences, we remove kernels from a feature selection perspective by eliminating associating connections in the sequ
&lt;/p&gt;</description></item><item><title>Belebele&#26159;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23567;&#22411;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2308.16884</link><description>&lt;p&gt;
Belebele&#22522;&#20934;&#25968;&#25454;&#38598;&#65306;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#24182;&#34892;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants. (arXiv:2308.16884v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16884
&lt;/p&gt;
&lt;p&gt;
Belebele&#26159;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23567;&#22411;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Belebele&#65292;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#22522;&#20934;&#30340;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#65292;&#20351;&#24471;&#21487;&#20197;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#22522;&#20110;Flores-200&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#20010;&#30701;&#31687;&#25991;&#31456;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#20010;&#22810;&#36873;&#31572;&#26696;&#12290;&#38382;&#39064;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#65292;&#20197;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#27700;&#24179;&#30340;&#27169;&#22411;&#12290;&#21333;&#29420;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#24050;&#32463;&#36275;&#22815;&#22256;&#38590;&#65292;&#21487;&#20197;&#25361;&#25112;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#30001;&#20110;&#23436;&#20840;&#24182;&#34892;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#30452;&#25509;&#27604;&#36739;&#25152;&#26377;&#35821;&#35328;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#20294;&#23567;&#22411;MLMs&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much small
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#22240;&#26524;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#19979;&#30340;&#20849;&#21516;&#21407;&#22240;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#26469;&#35782;&#21035;&#20849;&#21516;&#21407;&#22240;C&#65292;&#19982;&#26368;&#22823;&#29109;&#21407;&#21017;&#23494;&#20999;&#30456;&#20851;&#12290;&#23545;&#20110;&#20004;&#20010;&#20108;&#20803;&#23545;&#31216;&#21464;&#37327;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#31867;&#20284;&#20110;&#20108;&#38454;&#30456;&#21464;&#30340;&#26465;&#20214;&#27010;&#29575;&#38750;&#35299;&#26512;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.17557</link><description>&lt;p&gt;
&#26368;&#21487;&#33021;&#30340;&#20849;&#21516;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
The most likely common cause. (arXiv:2306.17557v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17557
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22240;&#26524;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#19979;&#30340;&#20849;&#21516;&#21407;&#22240;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#26469;&#35782;&#21035;&#20849;&#21516;&#21407;&#22240;C&#65292;&#19982;&#26368;&#22823;&#29109;&#21407;&#21017;&#23494;&#20999;&#30456;&#20851;&#12290;&#23545;&#20110;&#20004;&#20010;&#20108;&#20803;&#23545;&#31216;&#21464;&#37327;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#31867;&#20284;&#20110;&#20108;&#38454;&#30456;&#21464;&#30340;&#26465;&#20214;&#27010;&#29575;&#38750;&#35299;&#26512;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;A&#21644;B&#30340;&#20849;&#21516;&#21407;&#22240;&#21407;&#21017;&#22312;&#22240;&#26524;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24403;&#23427;&#20204;&#30340;&#20849;&#21516;&#21407;&#22240;C&#34987;&#35748;&#20026;&#24050;&#32463;&#23384;&#22312;&#65292;&#20294;&#21482;&#35266;&#27979;&#21040;&#20102;A&#21644;B&#30340;&#32852;&#21512;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;C&#19981;&#33021;&#34987;&#21807;&#19968;&#30830;&#23450;&#65288;&#28508;&#22312;&#28151;&#26434;&#22240;&#23376;&#38382;&#39064;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#19988;&#20801;&#35768;&#35782;&#21035;&#19982;&#20849;&#21516;&#21407;&#22240;&#21407;&#21017;&#19968;&#33268;&#30340;C&#12290;&#23427;&#19982;&#26368;&#22823;&#29109;&#21407;&#21017;&#23494;&#20999;&#30456;&#20851;&#12290;&#23545;&#20004;&#20010;&#20108;&#20803;&#23545;&#31216;&#21464;&#37327;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26465;&#20214;&#27010;&#29575;&#30340;&#38750;&#35299;&#26512;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#20108;&#38454;&#30456;&#21464;&#12290;&#36825;&#21457;&#29983;&#22312;&#35266;&#23519;&#21040;&#30340;&#27010;&#29575;&#20998;&#24067;&#20174;&#30456;&#20851;&#21040;&#21453;&#30456;&#20851;&#30340;&#36807;&#28193;&#26399;&#38388;&#12290;&#35752;&#35770;&#20102;&#24191;&#20041;&#20284;&#28982;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#65288;&#22914;&#39044;&#27979;&#20284;&#28982;&#21644;&#26368;&#23567;&#20849;&#21516;&#21407;&#22240;&#29109;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The common cause principle for two random variables $A$ and $B$ is examined in the case of causal insufficiency, when their common cause $C$ is known to exist, but only the joint probability of $A$ and $B$ is observed. As a result, $C$ cannot be uniquely identified (the latent confounder problem). We show that the generalized maximum likelihood method can be applied to this situation and allows identification of $C$ that is consistent with the common cause principle. It closely relates to the maximum entropy principle. Investigation of the two binary symmetric variables reveals a non-analytic behavior of conditional probabilities reminiscent of a second-order phase transition. This occurs during the transition from correlation to anti-correlation in the observed probability distribution. The relation between the generalized likelihood approach and alternative methods, such as predictive likelihood and the minimum common cause entropy, is discussed. The consideration of the common cause
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23618;&#27425;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#23398;&#26415;&#20986;&#29256;&#29289;&#36890;&#36807;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#30740;&#31350;&#27963;&#21160;&#22312;&#19981;&#21516;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20801;&#35768;&#36328;&#23398;&#31185;&#21644;&#36328;&#39046;&#22495;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2302.00390</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;"Web of Science"&#20013;&#23545;&#30740;&#31350;&#39046;&#22495;&#36827;&#34892;&#23618;&#27425;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Classification of Research Fields in the "Web of Science" Using Deep Learning. (arXiv:2302.00390v2 [cs.DL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23618;&#27425;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#23398;&#26415;&#20986;&#29256;&#29289;&#36890;&#36807;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#30740;&#31350;&#27963;&#21160;&#22312;&#19981;&#21516;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20801;&#35768;&#36328;&#23398;&#31185;&#21644;&#36328;&#39046;&#22495;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#25277;&#35937;&#23558;&#23398;&#26415;&#20986;&#29256;&#29289;&#33258;&#21160;&#20998;&#31867;&#21040;&#19977;&#32423;&#23618;&#27425;&#26631;&#31614;&#38598;&#65288;&#23398;&#31185;&#12289;&#39046;&#22495;&#12289;&#23376;&#39046;&#22495;&#65289;&#20013;&#65292;&#20197;&#22810;&#31867;&#21035;&#35774;&#32622;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25991;&#31456;&#30340;&#30693;&#35782;&#29983;&#20135;&#21644;&#24341;&#29992;&#30340;&#24433;&#21709;&#65292;&#23545;&#30740;&#31350;&#27963;&#21160;&#22312;&#25152;&#36848;&#23618;&#27425;&#32467;&#26500;&#20013;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#31867;&#65292;&#24182;&#20801;&#35768;&#36825;&#20123;&#27963;&#21160;&#34987;&#24402;&#20026;&#22810;&#20010;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#31995;&#32479;&#22312;Microsoft Academic Graph (&#29256;&#26412;2018-05-17)&#30340;160 million&#20221;&#25688;&#35201;&#29255;&#27573;&#20013;&#21306;&#20998;&#20102;44&#20010;&#23398;&#31185;&#12289;718&#20010;&#39046;&#22495;&#21644;1,485&#20010;&#23376;&#39046;&#22495;&#12290;&#25105;&#20204;&#20197;&#27169;&#22359;&#21270;&#21644;&#20998;&#24067;&#24335;&#26041;&#24335;&#36827;&#34892;&#25209;&#37327;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#21644;&#20801;&#35768;&#36328;&#23398;&#31185;&#21644;&#36328;&#39046;&#22495;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#24635;&#20849;&#65292;&#25105;&#20204;&#22312;&#25152;&#26377;&#32771;&#34385;&#30340;&#27169;&#22411;&#65288;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#21464;&#24418;&#22120;&#65289;&#20013;&#36827;&#34892;&#20102;3,140&#27425;&#23454;&#39564;&#12290;&#20998;&#31867;&#20934;&#30830;&#29575;&gt; 90&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a hierarchical classification system that automatically categorizes a scholarly publication using its abstract into a three-tier hierarchical label set (discipline, field, subfield) in a multi-class setting. This system enables a holistic categorization of research activities in the mentioned hierarchy in terms of knowledge production through articles and impact through citations, permitting those activities to fall into multiple categories. The classification system distinguishes 44 disciplines, 718 fields and 1,485 subfields among 160 million abstract snippets in Microsoft Academic Graph (version 2018-05-17). We used batch training in a modularized and distributed fashion to address and allow for interdisciplinary and interfield classifications in single-label and multi-label settings. In total, we have conducted 3,140 experiments in all considered models (Convolutional Neural Networks, Recurrent Neural Networks, Transformers). The classification accuracy is &gt; 90%
&lt;/p&gt;</description></item></channel></rss>