<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.01476</link><description>&lt;p&gt;
TraveLER&#65306;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;&#30340;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TraveLER: A Multi-LMM Agent Framework for Video Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01476
&lt;/p&gt;
&lt;p&gt;
TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#20197;&#24103;&#20026;&#21333;&#20301;&#36827;&#34892;&#22788;&#29702;&#12290;&#34429;&#28982;&#22522;&#20110;&#22270;&#20687;&#30340;&#35270;&#39057;&#26041;&#27861;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#30340;&#23616;&#38480;&#26159;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20102;&#22914;&#20309;&#36873;&#25321;&#20851;&#38190;&#26102;&#38388;&#25139;&#65292;&#24182;&#19988;&#26080;&#27861;&#22312;&#30830;&#23450;&#38169;&#35823;&#26102;&#38388;&#25139;&#26102;&#36827;&#34892;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#25552;&#21462;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#32454;&#33410;&#65292;&#32780;&#26159;&#25552;&#20379;&#24103;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#23427;&#27839;&#30528;&#35270;&#39057;&#36827;&#34892;&#31227;&#21160;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#30340;&#26041;&#24335;&#36845;&#20195;&#22320;&#20174;&#20851;&#38190;&#24103;&#25910;&#38598;&#30456;&#20851;&#20449;&#24687;&#65292;&#30452;&#21040;&#33719;&#24471;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TraveLER&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#21046;&#23450;&#8220;&#36941;&#21382;&#8221;&#35270;&#39057;&#35745;&#21010;&#30340;&#27169;&#22411;&#65292;&#35810;&#38382;&#20851;&#20110;&#21333;&#20010;&#24103;&#30340;&#38382;&#39064;&#20197;&#8220;&#23450;&#20301;&#8221;&#24182;&#23384;&#20648;&#20851;&#38190;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01476v1 Announce Type: cross  Abstract: Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to "Traverse" through the video, ask questions about individual frames to "Locate" and store key info
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;MindArm&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#22823;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#20551;&#32930;&#36816;&#21160;&#65292;&#24110;&#21161;&#24739;&#32773;&#25191;&#34892;&#21508;&#31181;&#27963;&#21160;</title><link>https://arxiv.org/abs/2403.19992</link><description>&lt;p&gt;
MindArm: &#26426;&#26800;&#26234;&#33021;&#38750;&#20405;&#20837;&#24335;&#31070;&#32463;&#39537;&#21160;&#20551;&#32930;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MindArm: Mechanized Intelligent Non-Invasive Neuro-Driven Prosthetic Arm System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;MindArm&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#22823;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#20551;&#32930;&#36816;&#21160;&#65292;&#24110;&#21161;&#24739;&#32773;&#25191;&#34892;&#21508;&#31181;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#27531;&#30142;&#25110;&#38590;&#20197;&#31227;&#21160;&#25163;&#33218;&#30340;&#20154;&#65288;&#31616;&#31216;&#8220;&#24739;&#32773;&#8221;&#65289;&#22312;&#26377;&#25928;&#35299;&#20915;&#29983;&#29702;&#38480;&#21046;&#26041;&#38754;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#20004;&#20010;&#21407;&#22240;&#65306;&#19968;&#26159;&#20687;&#20197;&#24605;&#32500;&#25511;&#21046;&#20026;&#20027;&#30340;&#20551;&#32930;&#35774;&#22791;&#36890;&#24120;&#38750;&#24120;&#26114;&#36149;&#24182;&#38656;&#35201;&#26114;&#36149;&#30340;&#32500;&#25252;&#65307;&#20108;&#26159;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#26114;&#36149;&#30340;&#20405;&#20837;&#24615;&#33041;&#37096;&#25163;&#26415;&#65292;&#36825;&#31181;&#25163;&#26415;&#39118;&#38505;&#39640;&#65292;&#26114;&#36149;&#19988;&#32500;&#25252;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#24182;&#19981;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#36130;&#21153;&#32972;&#26223;&#30340;&#25152;&#26377;&#24739;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;MindArm&#65292;&#21363;&#19968;&#31181;&#26426;&#26800;&#26234;&#33021;&#38750;&#20405;&#20837;&#24335;&#31070;&#32463;&#39537;&#21160;&#20551;&#32930;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;MindArm&#31995;&#32479;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24341;&#25806;&#23558;&#22823;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#39044;&#26399;&#30340;&#20551;&#32930;&#36816;&#21160;&#65292;&#20174;&#32780;&#24110;&#21161;&#24739;&#32773;&#23454;&#26045;&#35768;&#22810;&#27963;&#21160;&#65292;&#23613;&#31649;&#20182;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19992v1 Announce Type: new  Abstract: Currently, people with disability or difficulty to move their arms (referred to as "patients") have very limited technological solutions to efficiently address their physiological limitations. It is mainly due to two reasons: (1) the non-invasive solutions like mind-controlled prosthetic devices are typically very costly and require expensive maintenance; and (2) other solutions require costly invasive brain surgery, which is high risk to perform, expensive, and difficult to maintain. Therefore, current technological solutions are not accessible for all patients with different financial backgrounds. Toward this, we propose a low-cost technological solution called MindArm, a mechanized intelligent non-invasive neuro-driven prosthetic arm system. Our MindArm system employs a deep neural network (DNN) engine to translate brain signals into the intended prosthetic arm motion, thereby helping patients to perform many activities despite their 
&lt;/p&gt;</description></item><item><title>UADA3D&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#22788;&#29702;&#31232;&#30095;LiDAR&#25968;&#25454;&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.17633</link><description>&lt;p&gt;
UADA3D&#65306;&#38754;&#21521;&#31232;&#30095;LiDAR&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#30340;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17633
&lt;/p&gt;
&lt;p&gt;
UADA3D&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#22788;&#29702;&#31232;&#30095;LiDAR&#25968;&#25454;&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#22312;&#22522;&#20110;LiDAR&#30340;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36866;&#24212;&#24050;&#24314;&#31435;&#30340;&#39640;&#23494;&#24230;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#26356;&#31232;&#30095;&#30340;&#28857;&#20113;&#65292;&#25429;&#25417;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#22330;&#26223;&#65306;&#19981;&#20165;&#26469;&#33258;&#36947;&#36335;&#19978;&#30340;&#36710;&#36742;&#65292;&#36824;&#26469;&#33258;&#20154;&#34892;&#36947;&#19978;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#65292;&#36973;&#36935;&#30528;&#26126;&#26174;&#19981;&#21516;&#30340;&#29615;&#22659;&#26465;&#20214;&#21644;&#20256;&#24863;&#22120;&#37197;&#32622;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;3D&#29289;&#20307;&#26816;&#27979;&#65288;UADA3D&#65289;&#12290;UADA3D&#19981;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#25110;&#24072;&#29983;&#26550;&#26500;&#12290;&#30456;&#21453;&#65292;&#23427;&#20351;&#29992;&#23545;&#25239;&#26041;&#27861;&#30452;&#25509;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#36866;&#24212;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#22343;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65292;&#24456;&#24555;&#23558;&#20250;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17633v1 Announce Type: cross  Abstract: In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains. Our code is open-source and will be available soon.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#22522;&#30784;&#21644;&#25351;&#20196;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#21477;&#23376;&#21487;&#20449;&#24230;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#20998;&#25968;&#26159;&#26368;&#21487;&#38752;&#30340;&#21477;&#23376;&#21487;&#20449;&#24230;&#25351;&#26631;&#65292;&#20294;&#20173;&#20302;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.14859</link><description>&lt;p&gt;
&#22312;&#22522;&#30784;&#27169;&#22411;&#21644;&#25351;&#20196;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27604;&#36739;&#21487;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14859
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22522;&#30784;&#21644;&#25351;&#20196;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#21477;&#23376;&#21487;&#20449;&#24230;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#20998;&#25968;&#26159;&#26368;&#21487;&#38752;&#30340;&#21477;&#23376;&#21487;&#20449;&#24230;&#25351;&#26631;&#65292;&#20294;&#20173;&#20302;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#30340;LLM&#21487;&#20197;&#21709;&#24212;&#26126;&#30830;&#21046;&#23450;&#20026;&#25552;&#31034;&#30340;&#26597;&#35810;&#65292;&#36825;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#19982;&#20154;&#31867;&#29992;&#25143;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#33021;&#22815;&#21033;&#29992;LLM&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#30340;&#38544;&#24335;&#30693;&#35782;&#12290;&#26412;&#25991;&#23545;&#35780;&#20272;LLM&#20013;&#35821;&#20041;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#65288;a&#65289;&#26126;&#30830;&#25552;&#31034;&#21644;&#65288;b&#65289;&#30452;&#25509;&#35835;&#21462;&#27169;&#22411;&#20998;&#37197;&#32473;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#30340;&#38544;&#24335;&#20272;&#35745;&#65292;&#22312;&#33521;&#35821;&#21477;&#23376;&#21487;&#20449;&#24230;&#20219;&#21153;&#20013;&#27604;&#36739;&#20102;&#22522;&#30784;&#21644;&#25351;&#20196;&#35843;&#20248;LLM&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;1&#34920;&#26126;&#65292;&#36328;&#27169;&#22411;&#26550;&#26500;&#21644;&#21487;&#20449;&#24230;&#25968;&#25454;&#38598;&#65292;&#65288;i&#65289;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#20998;&#25968;&#26159;&#21477;&#23376;&#21487;&#20449;&#24230;&#26368;&#21487;&#38752;&#30340;&#25351;&#26631;&#65292;&#38646;&#29031;&#23556;&#25552;&#31034;&#20135;&#29983;&#19981;&#19968;&#33268;&#19988;&#36890;&#24120;&#25928;&#26524;&#19981;&#20339;&#30340;&#32467;&#26524;&#65307;&#65288;ii&#65289;&#22522;&#20110;LL&#30340;&#24615;&#33021;&#20173;&#20302;&#20110;&#20154;&#31867;&#34920;&#29616;&#65307;&#65288;iii&#65289;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14859v1 Announce Type: cross  Abstract: Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood ($\textit{LL}$) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) $\textit{LL}$-based performance is still inferior to human performance; (iii) instruction-tuned models hav
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10853</link><description>&lt;p&gt;
&#21482;&#35828;&#21517;&#31216;&#65306;&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#23454;&#29616;&#20165;&#21033;&#29992;&#31867;&#21035;&#21517;&#31216;&#36827;&#34892;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Just Say the Name: Online Continual Learning with Category Names Only via Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25104;&#26412;&#36807;&#39640;&#65292;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#21463;&#21040;&#22823;&#35268;&#27169;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#24314;&#35758;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#35832;&#22914;&#25968;&#25454;&#19981;&#24179;&#34913;&#12289;&#20351;&#29992;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36830;&#32493;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550; - &#20165;&#20351;&#29992;&#21517;&#31216;&#30340;&#29983;&#25104;&#24335;&#36830;&#32493;&#23398;&#20064;&#65288;G-NoCL&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;G-NoCL&#20351;&#29992;&#19968;&#32452;&#29983;&#25104;&#22120;G&#20197;&#21450;&#23398;&#20064;&#32773;&#12290;&#24403;&#36935;&#21040;&#26032;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#31867;&#21035;&#65289;&#26102;&#65292;G-NoCL&#37319;&#29992;&#26032;&#39062;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;DIverSity and COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#20174;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#26368;&#20248;&#25277;&#26679;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DISCOBER&#22312;G-NoCL&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#28085;&#30422;&#20102;In-Distributi&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10853v1 Announce Type: cross  Abstract: In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distributi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550; Diffusion-TS&#65292;&#32467;&#21512;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#21644;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#38750;&#22122;&#22768;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26088;&#22312;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01742</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25193;&#25955;&#29992;&#20110;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diffusion-TS: Interpretable Diffusion for General Time Series Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01742
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550; Diffusion-TS&#65292;&#32467;&#21512;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#21644;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#38750;&#22122;&#22768;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26088;&#22312;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models (DDPMs)&#27491;&#36880;&#28176;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#65292;&#26368;&#36817;&#24050;&#22312;&#38899;&#39057;&#21512;&#25104;&#12289;&#26102;&#38388;&#24207;&#21015;&#22635;&#34917;&#21644;&#39044;&#27979;&#31561;&#39046;&#22495;&#21462;&#24471;&#31361;&#30772;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-TS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#20854;&#20013;&#20998;&#35299;&#25216;&#26415;&#25351;&#23548;Diffusion-TS&#25429;&#33719;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#20041;&#21547;&#20041;&#65292;&#32780;&#21464;&#21387;&#22120;&#20174;&#22024;&#26434;&#30340;&#27169;&#22411;&#36755;&#20837;&#20013;&#25366;&#25496;&#35814;&#32454;&#30340;&#24207;&#21015;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#19981;&#26159;&#22312;&#27599;&#20010;&#25193;&#25955;&#27493;&#39588;&#20013;&#37325;&#24314;&#22122;&#22768;&#65292;&#24182;&#32467;&#21512;&#20102;&#22522;&#20110;Fourier&#30340;&#25439;&#22833;&#39033;&#12290;&#39044;&#26399;Diffusion-TS&#21487;&#20197;&#29983;&#25104;&#26082;&#20855;&#26377;&#35299;&#37322;&#24615;&#21448;&#30495;&#23454;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36824;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;Diffusion
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01742v1 Announce Type: cross  Abstract: Denoising diffusion probabilistic models (DDPMs) are becoming the leading paradigm for generative models. It has recently shown breakthroughs in audio synthesis, time series imputation and forecasting. In this paper, we propose Diffusion-TS, a novel diffusion-based framework that generates multivariate time series samples of high quality by using an encoder-decoder transformer with disentangled temporal representations, in which the decomposition technique guides Diffusion-TS to capture the semantic meaning of time series while transformers mine detailed sequential information from the noisy model input. Different from existing diffusion-based approaches, we train the model to directly reconstruct the sample instead of the noise in each diffusion step, combining a Fourier-based loss term. Diffusion-TS is expected to generate time series satisfying both interpretablity and realness. In addition, it is shown that the proposed Diffusion-T
&lt;/p&gt;</description></item><item><title>&#36127;&#37319;&#26679;&#26041;&#27861;&#23545;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#21508;&#31181;&#36127;&#37319;&#26679;&#26041;&#27861;&#21450;&#20854;&#23545;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#25104;&#21151;&#30340;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.19195</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#36127;&#37319;&#26679;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Negative Sampling in Knowledge Graph Representation Learning: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19195
&lt;/p&gt;
&lt;p&gt;
&#36127;&#37319;&#26679;&#26041;&#27861;&#23545;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#21508;&#31181;&#36127;&#37319;&#26679;&#26041;&#27861;&#21450;&#20854;&#23545;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#25104;&#21151;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65288;KGRL&#65289;&#25110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#22312;&#30693;&#35782;&#26500;&#24314;&#21644;&#20449;&#24687;&#25506;&#32034;&#30340;AI&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#32534;&#30721;&#20026;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#12290;&#22312;KGE&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#27491;&#36127;&#26679;&#26412;&#23545;&#20110;&#21306;&#20998;&#30446;&#30340;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20174;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#20013;&#33719;&#21462;&#36127;&#26679;&#26412;&#38754;&#20020;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#26377;&#25928;&#29983;&#25104;&#25216;&#26415;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#20123;&#36127;&#26679;&#26412;&#30340;&#36136;&#37327;&#23545;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#20934;&#30830;&#24615;&#26377;&#30528;&#24456;&#22823;&#24433;&#21709;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#29983;&#25104;&#25104;&#20026;KGRL&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#26412;&#20840;&#38754;&#35843;&#30740;&#35770;&#25991;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#21508;&#31181;&#36127;&#37319;&#26679;&#65288;NS&#65289;&#26041;&#27861;&#21450;&#20854;&#23545;KGRL&#25104;&#21151;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;NS&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#27010;&#36848;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19195v1 Announce Type: new  Abstract: Knowledge graph representation learning (KGRL) or knowledge graph embedding (KGE) plays a crucial role in AI applications for knowledge construction and information exploration. These models aim to encode entities and relations present in a knowledge graph into a lower-dimensional vector space. During the training process of KGE models, using positive and negative samples becomes essential for discrimination purposes. However, obtaining negative samples directly from existing knowledge graphs poses a challenge, emphasizing the need for effective generation techniques. The quality of these negative samples greatly impacts the accuracy of the learned embeddings, making their generation a critical aspect of KGRL. This comprehensive survey paper systematically reviews various negative sampling (NS) methods and their contributions to the success of KGRL. Their respective advantages and disadvantages are outlined by categorizing existing NS me
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#19978;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18012</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#32422;&#26463;&#25277;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18012
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#19978;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#22312;&#20998;&#26512;&#23458;&#35266;&#20989;&#25968;&#25110;&#32422;&#26463;&#19981;&#21487;&#29992;&#26102;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#26410;&#30693;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#20294;&#26377;&#38480;&#30740;&#31350;&#20851;&#27880;&#20102;&#32422;&#26463;&#26465;&#20214;&#26410;&#26126;&#30830;&#32473;&#20986;&#30340;&#24773;&#20917;&#12290;&#24573;&#30053;&#36825;&#20123;&#32422;&#26463;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#36341;&#20013;&#19981;&#29616;&#23454;&#30340;&#34394;&#20551;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#26410;&#30693;&#32422;&#26463;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#20102;&#23558;&#20248;&#21270;&#36807;&#31243;&#38480;&#21046;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#36890;&#36807;&#23458;&#35266;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#30340;&#25277;&#26679;&#38382;&#39064;&#12290;&#20026;&#20102;&#22686;&#24378;&#25277;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#39044;&#28909;&#65292;&#28982;&#21518;&#26159;Langevin&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18012v1 Announce Type: cross  Abstract: Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable. While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly. Overlooking these constraints can lead to spurious solutions that are unrealistic in practice. To deal with such unknown constraints, we propose to perform optimization within the data manifold using diffusion models. To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the diffusion model. To enhance sampling efficiency, we propose a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dyna
&lt;/p&gt;</description></item><item><title>Ansible Lightspeed&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;&#65292;&#19987;&#27880;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;Ansible&#20195;&#30721;&#65292;&#20026;IT&#33258;&#21160;&#21270;&#39046;&#22495;&#24102;&#26469;&#20102;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.17442</link><description>&lt;p&gt;
Ansible Lightspeed: &#19968;&#31181;&#29992;&#20110;IT&#33258;&#21160;&#21270;&#30340;&#20195;&#30721;&#29983;&#25104;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Ansible Lightspeed: A Code Generation Service for IT Automation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17442
&lt;/p&gt;
&lt;p&gt;
Ansible Lightspeed&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;&#65292;&#19987;&#27880;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;Ansible&#20195;&#30721;&#65292;&#20026;IT&#33258;&#21160;&#21270;&#39046;&#22495;&#24102;&#26469;&#20102;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#19990;&#20351;&#24471;&#21019;&#24314;&#21487;&#25552;&#39640;&#24320;&#21457;&#32773;&#29983;&#20135;&#21147;&#30340;&#24037;&#20855;&#25104;&#20026;&#21487;&#33021;&#65292;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDEs&#65289;&#24120;&#34987;&#29992;&#20316;&#19982;LLMs&#20132;&#20114;&#30340;&#25509;&#21475;&#12290;&#24050;&#21457;&#24067;&#35768;&#22810;&#36825;&#31867;&#24037;&#20855;&#65292;&#20294;&#20960;&#20046;&#20840;&#37096;&#37117;&#19987;&#27880;&#20110;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#65292;&#24456;&#23569;&#20851;&#27880;&#23545;IT&#33258;&#21160;&#21270;&#33267;&#20851;&#37325;&#35201;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#12290;Ansible&#26159;&#19968;&#31181;&#22522;&#20110;YAML&#30340;IT&#33258;&#21160;&#21270;&#29305;&#23450;&#35821;&#35328;&#12290;Red Hat Ansible Lightspeed&#19982;IBM Watson Code Assistant&#21512;&#20316;&#30340;Ansible Lightspeed&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26381;&#21153;&#65292;&#19987;&#38376;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;Ansible&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17442v1 Announce Type: cross  Abstract: The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for IT automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Red Hat Ansible Lightspeed with IBM Watson Code Assistant, further referred to as Ansible Lightspeed, is an LLM-based service designed explicitly for natural language to Ansible code generation.   In this paper, we describe the design and implementation of the Ansible Lightspeed service and analyze feedback from thousands of real users. We examine diverse performance indicators, clas
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.13764</link><description>&lt;p&gt;
CriticBench: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#35770;&#23478;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Evaluating Large Language Models as Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13764
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102; CriticBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22235;&#20010;&#20851;&#38190;&#35780;&#35770;&#33021;&#21147;&#32500;&#24230;&#65288;&#21453;&#39304;&#12289;&#27604;&#36739;&#12289;&#25913;&#36827;&#21644;&#20803;&#21453;&#39304;&#65289;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;CriticBench&#21253;&#21547;&#20061;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#36136;&#37327;&#32454;&#31890;&#24230;&#27700;&#24179;&#19978;&#35780;&#35770;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#25581;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#26377;&#36259;&#30340;&#20851;&#31995;&#12290;CriticBench&#30340;&#25968;&#25454;&#38598;&#12289;&#36164;&#28304;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#23558;&#22312;https://github.com/gmftbyGMFTBY/Cri&#19978;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13764v1 Announce Type: cross  Abstract: Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/Cri
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#25972;&#20010;&#20998;&#24067;&#22312;&#22238;&#24402;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#26469;&#33258;&#20110;&#20248;&#21270;&#30340;&#25913;&#36827;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.13425</link><description>&lt;p&gt;
&#22312;&#22238;&#24402;&#20013;&#25506;&#35752;&#30452;&#26041;&#22270;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Investigating the Histogram Loss in Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13425
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25972;&#20010;&#20998;&#24067;&#22312;&#22238;&#24402;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#26469;&#33258;&#20110;&#20248;&#21270;&#30340;&#25913;&#36827;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#26159;&#65292;&#22312;&#22238;&#24402;&#20013;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#25972;&#20010;&#20998;&#24067;&#65292;&#21363;&#20351;&#21482;&#38656;&#35201;&#22343;&#20540;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290; &#36825;&#31181;&#39069;&#22806;&#30340;&#24314;&#27169;&#36890;&#24120;&#20250;&#24102;&#26469;&#24615;&#33021;&#22686;&#30410;&#65292;&#20294;&#32972;&#21518;&#30340;&#21407;&#22240;&#23578;&#19981;&#23436;&#20840;&#28165;&#26970;&#12290; &#26412;&#25991;&#30740;&#31350;&#20102;&#22238;&#24402;&#20013;&#30340;&#19968;&#31181;&#26368;&#26032;&#26041;&#27861;&#65292;&#21363;&#30452;&#26041;&#22270;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#20998;&#24067;&#21644;&#28789;&#27963;&#30452;&#26041;&#22270;&#39044;&#27979;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#26469;&#23398;&#20064;&#30446;&#26631;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290; &#25105;&#20204;&#35774;&#35745;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20026;&#20160;&#20040;&#20197;&#21450;&#20309;&#26102;&#20250;&#20986;&#29616;&#24615;&#33021;&#22686;&#30410;&#65292;&#20197;&#21450;&#25439;&#22833;&#30340;&#19981;&#21516;&#32452;&#20214;&#22914;&#20309;&#20026;&#27492;&#20570;&#20986;&#36129;&#29486;&#12290; &#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#23398;&#20064;&#20998;&#24067;&#30340;&#22909;&#22788;&#26469;&#33258;&#20110;&#20248;&#21270;&#30340;&#25913;&#36827;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#26041;&#22270;&#25439;&#22833;&#22312;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13425v1 Announce Type: cross  Abstract: It is becoming increasingly common in regression to train neural networks that model the entire distribution even if only the mean is required for prediction. This additional modeling often comes with performance gain and the reasons behind the improvement are not fully known. This paper investigates a recent approach to regression, the Histogram Loss, which involves learning the conditional distribution of the target variable by minimizing the cross-entropy between a target distribution and a flexible histogram prediction. We design theoretical and empirical analyses to determine why and when this performance gain appears, and how different components of the loss contribute to it. Our results suggest that the benefits of learning distributions in this setup come from improvements in optimization rather than learning a better representation. We then demonstrate the viability of the Histogram Loss in common deep learning applications wi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20110;ICD-9&#20195;&#30721;&#30340;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10940</link><description>&lt;p&gt;
&#20020;&#24202;&#31243;&#24207;&#20195;&#30721;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10940
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20110;ICD-9&#20195;&#30721;&#30340;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#26088;&#22312;&#36890;&#36807;&#23558;&#31995;&#32479;&#29983;&#25104;&#30340;&#24314;&#35758;&#19982;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#32467;&#21512;&#26469;&#22686;&#24378;&#20020;&#24202;&#21307;&#29983;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#22522;&#20110;&#25163;&#26415;ICD-9&#20195;&#30721;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26469;&#37327;&#21270;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19981;&#20165;&#23637;&#31034;&#20102;&#31243;&#24207;&#20195;&#30721;&#19982;&#23454;&#38469;&#21307;&#30103;&#32467;&#26524;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10940v1 Announce Type: new  Abstract: A Clinical Decision Support System (CDSS) is designed to enhance clinician decision-making by combining system-generated recommendations with medical expertise. Given the high costs, intensive labor, and time-sensitive nature of medical treatments, there is a pressing need for efficient decision support, especially in complex emergency scenarios. In these scenarios, where information can be limited, an advanced CDSS framework that leverages AI (artificial intelligence) models to effectively reduce diagnostic uncertainty has utility. Such an AI-enabled CDSS framework with quantified uncertainty promises to be practical and beneficial in the demanding context of real-world medical care. In this study, we introduce the concept of Medical Entropy, quantifying uncertainties in patient outcomes predicted by neural machine translation based on the ICD-9 code of procedures. Our experimental results not only show strong correlations between proce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;aespa&#65292;&#23427;&#22312;&#20445;&#25345;&#23436;&#25972;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08958</link><description>&lt;p&gt;
&#36808;&#21521;&#36229;&#22823;&#35268;&#27169;Transformer&#30340;&#19979;&#19968;&#32423;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;aespa&#65292;&#23427;&#22312;&#20445;&#25345;&#23436;&#25972;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#25104;&#20026;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#30005;&#35270;&#31561;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#36229;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#26696;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#36825;&#21487;&#33021;&#25104;&#20026;&#23454;&#38469;&#24773;&#20917;&#20013;&#39057;&#32321;&#27169;&#22411;&#26356;&#26032;&#21644;&#22810;&#31181;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#29942;&#39048;&#12290;&#20316;&#20026;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#27425;&#24615;PTQ&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26377;&#20123;&#21463;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#32771;&#34385;&#21040;Transformer&#20013;&#27880;&#24847;&#21147;&#27169;&#22359;&#20869;&#37096;&#23618;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PTQ&#31639;&#27861;&#65292;&#23427;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#21483;&#20570;aespa&#65292;&#36890;&#36807;&#22312;&#25928;&#29575;&#19978;&#36827;&#34892;&#36880;&#23618;&#37327;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#36328;&#23618;&#20381;&#36182;&#20197;&#20445;&#30041;&#27880;&#24847;&#21147;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08958v1 Announce Type: cross Abstract: With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost-effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers. In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency. The key idea of the proposed algorithm called aespa is to perform quantization layer-wise for efficiency while considering cross-layer dependency to preserve the attention score. Through extensive exp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.08644</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#26029;&#39640;&#25928;LLMs&#30340;&#20018;&#32852;Transformer
&lt;/p&gt;
&lt;p&gt;
Tandem Transformers for Inference Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08644
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs )&#20855;&#26377;&#33258;&#22238;&#24402;&#30340;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#35789;&#20803;&#26159;&#25353;&#39034;&#24207;&#29983;&#25104;&#30340;&#12290;&#23613;&#31649;&#26377;&#20123;&#39044;&#27979;&#21644;&#24182;&#34892;&#35299;&#30721;&#25216;&#26415;&#35797;&#22270;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#38480;&#21046;&#65306;&#35201;&#20040;&#20381;&#36182;&#26356;&#31934;&#31616;&#20294;&#20934;&#30830;&#24230;&#36739;&#20302;&#30340;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#65292;&#35201;&#20040;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22522;&#30784;LLM&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#21363;&#20018;&#32852;Transformer&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#26550;&#26500;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;(1)&#19968;&#20010;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;(2)&#19968;&#20010;&#20197;&#22359;&#27169;&#24335;&#36816;&#34892;&#30340;&#22823;&#27169;&#22411;(&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#35789;&#20803;)&#12290;&#36890;&#36807;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#22823;&#24133;&#25552;&#21319;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;PaLM2&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;PaLM2-Bison&#21644;PaLM2-Gecko&#30340;&#20018;&#32852;&#30456;&#36739;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;3.3%&#65292;&#19982;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#30456;&#27604;&#65292;&#25552;&#20379;&#20102;1.16&#20493;&#30340;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08151</link><description>&lt;p&gt;
&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#29992;&#20110;sigmoid&#20998;&#31867;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26799;&#24230;&#27969;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#21464;&#25442;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#28857;&#32423;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#65288;LOO&#65289;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#12290;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#20363;&#22914;&#35745;&#31639;&#19982;AIC&#31867;&#20284;&#30340;LOO&#25110;&#35745;&#31639;LOO ROC / PRC&#26354;&#32447;&#20197;&#21450;&#27966;&#29983;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#22914;AUROC&#21644;AUPRC&#12290;&#36890;&#36807;&#21464;&#20998;&#27861;&#21644;&#26799;&#24230;&#27969;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20004;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#21333;&#27493;&#21464;&#25442;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#23558;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#38752;&#36817;&#30446;&#26631;LOO&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;&#36825;&#26679;&#65292;&#21464;&#25442;&#31283;&#23450;&#20102;&#37325;&#35201;&#24615;&#26435;&#37325;&#12290;&#22240;&#20026;&#21464;&#25442;&#28041;&#21450;&#21040;&#20284;&#28982;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#25152;&#20197;&#32467;&#26524;&#30340;&#33945;&#29305;&#21345;&#32599;&#31215;&#20998;&#20381;&#36182;&#20110;&#27169;&#22411;Hessian&#30340;Jacobian&#34892;&#21015;&#24335;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#36825;&#20123;Jacobian&#34892;&#21015;&#24335;&#30340;&#38381;&#21512;&#31934;&#30830;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a set of gradient-flow-guided adaptive importance sampling (IS) transformations to stabilize Monte-Carlo approximations of point-wise leave one out cross-validated (LOO) predictions for Bayesian classification models. One can leverage this methodology for assessing model generalizability by for instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves and derived metrics like the AUROC and AUPRC. By the calculus of variations and gradient flow, we derive two simple nonlinear single-step transformations that utilize gradient information to shift a model's pre-trained full-data posterior closer to the target LOO posterior predictive distributions. In doing so, the transformations stabilize importance weights. Because the transformations involve the gradient of the likelihood function, the resulting Monte Carlo integral depends on Jacobian determinants with respect to the model Hessian. We derive closed-form exact formulae for these Jacobian determinants in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#26465;&#20214;&#27491;&#23450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#12290;&#36825;&#20026;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#24102;&#26469;&#20102;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.06955</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Training dynamics in Physics-Informed Neural Networks with feature mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#26465;&#20214;&#27491;&#23450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#12290;&#36825;&#20026;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#24102;&#26469;&#20102;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26631;&#24535;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#21464;&#20307;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26469;&#33258;&#26356;&#24191;&#27867;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30740;&#31350;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#32463;&#39564;&#24615;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#26469;&#30740;&#31350;&#24102;&#26377;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;PINNs&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#26465;&#20214;&#27491;&#23450;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#20316;&#20026;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#38598;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#23454;&#29616;&#65292;&#24182;&#21463;&#30410;&#20110;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine learning approach for solving Partial Differential Equations (PDEs). Although its variants have achieved significant progress, the empirical success of utilising feature mapping from the wider Implicit Neural Representations studies has been substantially neglected. We investigate the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel, which sheds light on the convergence and generalisation of the model. We also show the inadequacy of commonly used Fourier-based feature mapping in some scenarios and propose the conditional positive definite Radial Basis Function as a better alternative. The empirical results reveal the efficacy of our method in diverse forward and inverse problem sets. This simple technique can be easily implemented in coordinate input networks and benefits the broad PINNs research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06900</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#35782;&#21035;&#27602;&#24615;&#21527;&#65311;&#32467;&#26500;&#21270;&#27602;&#24615;&#35843;&#26597;&#26694;&#26550;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#36981;&#23432;&#31038;&#20250;&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#23384;&#22312;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#27602;&#24615;&#24230;&#37327;&#20381;&#36182;&#20110;&#22312;&#29305;&#23450;&#27602;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#30721;&#22120;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#22806;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#25152;&#20551;&#23450;&#30340;&#27602;&#24615;&#23450;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#33258;&#21160;&#40065;&#26834;&#24230;&#37327;&#65292;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#22238;&#24212;&#26159;&#21542;&#20855;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#27602;&#24615;&#22240;&#32032;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#24230;&#37327;&#25351;&#26631;LLMs As ToxiciTy Evaluators&#65288;LATTE&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;F1&#24471;&#20998;&#27604;&#29616;&#26377;&#25216;&#26415;&#25351;&#26631;&#25552;&#39640;&#20102;12&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19978;&#28216;&#27602;&#24615;&#23545;&#24230;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24222;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;270M&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#26174;&#24335;&#25628;&#32034;&#65292;&#21462;&#24471;&#20102;&#22823;&#24072;&#32423;&#27700;&#24179;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#30340;&#25104;&#21151;&#12290;&#27169;&#22411;&#22312;Lichess&#38378;&#30005;&#25112;&#35780;&#20998;&#19978;&#36798;&#21040;&#20102;2895&#65292;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22269;&#38469;&#35937;&#26827;&#35868;&#39064;&#65292;&#20248;&#20110;AlphaZero&#21644;GPT-3.5-turbo-instruct&#12290;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23545;&#20110;&#23454;&#29616;&#24378;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#25928;&#26524;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.04494</link><description>&lt;p&gt;
&#19981;&#38656;&#25628;&#32034;&#21363;&#21487;&#23454;&#29616;&#22823;&#24072;&#32423;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;
&lt;/p&gt;
&lt;p&gt;
Grandmaster-Level Chess Without Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24222;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;270M&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#26174;&#24335;&#25628;&#32034;&#65292;&#21462;&#24471;&#20102;&#22823;&#24072;&#32423;&#27700;&#24179;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#30340;&#25104;&#21151;&#12290;&#27169;&#22411;&#22312;Lichess&#38378;&#30005;&#25112;&#35780;&#20998;&#19978;&#36798;&#21040;&#20102;2895&#65292;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22269;&#38469;&#35937;&#26827;&#35868;&#39064;&#65292;&#20248;&#20110;AlphaZero&#21644;GPT-3.5-turbo-instruct&#12290;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23545;&#20110;&#23454;&#29616;&#24378;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#25928;&#26524;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#31361;&#30772;&#24615;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#35268;&#27169;&#21270;&#65292;&#21363;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22823;&#35268;&#27169;&#26550;&#26500;&#21644;&#31354;&#21069;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22269;&#38469;&#35937;&#26827;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479;&#30340;&#20381;&#36182;&#22797;&#26434;&#21551;&#21457;&#24335;&#31639;&#27861;&#12289;&#26174;&#24335;&#25628;&#32034;&#25110;&#20108;&#32773;&#32467;&#21512;&#30340;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;1000&#19975;&#23616;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#20102;&#19968;&#20010;&#25317;&#26377;2.7&#20159;&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#29992;&#24378;&#22823;&#30340;Stockfish 16&#24341;&#25806;&#25552;&#20379;&#30340;&#21160;&#20316;&#20540;&#26469;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#26827;&#23616;&#65292;&#20135;&#29983;&#22823;&#32422;150&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#26368;&#22823;&#30340;&#27169;&#22411;&#22312;Lichess&#38378;&#30005;&#25112;Elo&#19978;&#36798;&#21040;&#20102;2895&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22269;&#38469;&#35937;&#26827;&#35868;&#39064;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#29305;&#23450;&#39046;&#22495;&#30340;&#35843;&#25972;&#25110;&#26174;&#24335;&#25628;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;AlphaZero&#30340;&#31574;&#30053;&#21644;&#20215;&#20540;&#32593;&#32476;&#65288;&#26080;MCTS&#65289;&#20197;&#21450;GPT-3.5-turbo-instruct&#12290;&#23545;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#31995;&#32479;&#30740;&#31350;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#21462;&#24471;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35782;&#21035;&#26377;&#27602;&#12289;&#20882;&#29359;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;&#20869;&#23481;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#25913;&#36827;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#20102;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#24037;&#20316;&#20013;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2311.07879</link><description>&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#24182;&#19981;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#65306;&#24357;&#21512;&#25903;&#25345;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35782;&#21035;&#26377;&#27602;&#12289;&#20882;&#29359;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;&#20869;&#23481;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#25913;&#36827;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#20102;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#24037;&#20316;&#20013;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35782;&#21035;&#26377;&#27602;&#12289;&#20882;&#29359;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;&#20869;&#23481;&#26041;&#38754;&#21462;&#24471;&#20102;&#38271;&#36275;&#30340;&#36827;&#23637;&#65292;&#26088;&#22312;&#20943;&#36731;&#31649;&#29702;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#20219;&#21153;&#30340;&#25913;&#36827;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#20102;&#31649;&#29702;&#21592;&#22312;&#24037;&#20316;&#20013;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#36807;&#21435;&#30740;&#31350;&#21162;&#21147;&#33268;&#21147;&#20110;&#20026;&#20869;&#23481;&#31649;&#29702;&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#33258;&#21160;&#21270;&#25903;&#25345;&#19982;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#30340;&#38656;&#27714;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#23588;&#20854;&#26159;&#22312;&#35782;&#21035;&#36829;&#21453;&#21508;&#31181;&#31649;&#29702;&#35268;&#21017;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;Hugging Face&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#25581;&#31034;&#28085;&#30422;&#19977;&#20010;&#31034;&#33539;&#35770;&#22363;&#30340;&#21508;&#31181;&#31649;&#29702;&#35268;&#21017;&#21644;&#25351;&#21335;&#30340;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#26631;&#35760;&#26576;&#20010;&#29305;&#23450;&#35770;&#22363;&#30340;&#24179;&#21488;&#35268;&#21017;&#36829;&#35268;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29992;&#25143;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07879v2 Announce Type: replace-cross  Abstract: Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators' needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey stud
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;</title><link>http://arxiv.org/abs/2401.13796</link><description>&lt;p&gt;
&#19981;&#35201;&#25353;&#25353;&#38062;&#65281;&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning. (arXiv:2401.13796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20026;&#22810;&#20010;&#39046;&#22495;&#25552;&#20379;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ML&#24037;&#20855;&#30340;&#26085;&#30410;&#21487;&#33719;&#24471;&#24615;&#65292;&#35768;&#22810;&#20174;&#19994;&#32773;&#32570;&#20047;&#28145;&#20837;&#30340;ML&#19987;&#19994;&#30693;&#35782;&#65292;&#37319;&#29992;&#20102;&#8220;&#25353;&#25353;&#38062;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#32780;&#24573;&#35270;&#20102;&#24213;&#23618;&#31639;&#27861;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;&#23427;&#24341;&#21457;&#20102;&#23545;&#32467;&#26524;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#24615;&#33021;&#35780;&#20272;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;ML&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#27844;&#38706;&#65292;&#20854;&#20013;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#30340;&#20048;&#35266;&#24615;&#33021;&#20272;&#35745;&#12290;&#35780;&#20272;&#24615;&#33021;&#19982;&#23454;&#38469;&#22312;&#26032;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#24046;&#24322;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#26412;&#25991;&#29305;&#21035;&#23558;ML&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#20998;&#20026;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#23545;&#38382;&#39064;&#27714;&#35299;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31616;&#27905;&#24615;&#19981;&#20165;&#38477;&#20302;&#20102;&#22238;&#31572;&#38271;&#24230;&#65292;&#19988;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#12290;&#28982;&#32780;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.05618</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#27714;&#35299;&#20013;&#65292;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models. (arXiv:2401.05618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#23545;&#38382;&#39064;&#27714;&#35299;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31616;&#27905;&#24615;&#19981;&#20165;&#38477;&#20302;&#20102;&#22238;&#31572;&#38271;&#24230;&#65292;&#19988;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#12290;&#28982;&#32780;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;(CCoT)&#25552;&#31034;&#12290;&#25105;&#20204;&#23558;&#26631;&#20934;&#30340;CoT&#21644;CCoT&#25552;&#31034;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#20102;&#35299;&#31616;&#27905;&#24615;&#23545;&#22238;&#31572;&#38271;&#24230;&#21644;&#27491;&#30830;&#31572;&#26696;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;(MCQA)&#22522;&#20934;&#30340;&#35780;&#20272;&#12290;CCoT&#23558;GPT-3.5&#21644;GPT-4&#30340;&#24179;&#22343;&#22238;&#31572;&#38271;&#24230;&#20998;&#21035;&#20943;&#23569;&#20102;48.70&#65285;&#65292;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#65292;&#24102;&#26377;CCoT&#30340;GPT-3.5&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;27.69&#65285;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;CCoT&#23548;&#33268;&#27599;&#20010;&#26631;&#35760;&#30340;&#25104;&#26412;&#24179;&#22343;&#38477;&#20302;&#20102;22.67&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20351;&#29992;CoT&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#26469;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;LLM&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#20026;&#30740;&#31350;LLM&#20013;&#36880;&#27493;&#25512;&#29702;&#30340;&#24418;&#25104;&#34892;&#20026;&#30340;AI&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#29992;&#20110;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#65292;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#21644;&#19968;&#31181;&#26032;&#30340;&#21452;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#24182;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#23545;&#21508;&#31181;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11477</link><description>&lt;p&gt;
Robust-MBFD&#65306;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#21644;&#19968;&#31181;&#26032;&#30340;&#21452;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#30340;&#31283;&#20581;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults Detection Using Multiple Deep Learning Training Strategies and A Novel Double Loss Function. (arXiv:2310.11477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#29992;&#20110;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#65292;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#21644;&#19968;&#31181;&#26032;&#30340;&#21452;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#24182;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#23545;&#21508;&#31181;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#65288;MBFD&#65289;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25391;&#21160;&#20449;&#21495;&#35782;&#21035;&#30005;&#26426;&#36724;&#25215;&#30340;&#25925;&#38556;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;MBFD&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MBFD&#31995;&#32479;&#65292;&#20998;&#21035;&#25506;&#32034;&#20102;&#30417;&#30563;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#36825;&#19977;&#31181;&#35757;&#32451;&#31574;&#30053;&#12290;&#23545;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#24182;&#25214;&#20986;&#20102;&#36866;&#29992;&#20110;MBFD&#20219;&#21153;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#32654;&#22269;&#26426;&#26800;&#25925;&#38556;&#39044;&#38450;&#25216;&#26415;&#21327;&#20250;&#65288;MFPT&#65289;&#12289;&#20975;&#26031;&#35199;&#20648;&#22823;&#23398;&#36724;&#25215;&#20013;&#24515;&#65288;CWRU&#65289;&#21644;&#24085;&#24503;&#21338;&#24681;&#22823;&#23398;&#30340;&#30005;&#26426;&#39537;&#21160;&#31995;&#32479;&#36724;&#25215;&#25439;&#20260;&#29366;&#24577;&#30417;&#27979;&#31561;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive analysis of motor bearing fault detection (MBFD), which involves the task of identifying faults in a motor bearing based on its vibration. To this end, we first propose and evaluate various machine learning based systems for the MBFD task. Furthermore, we propose three deep learning based systems for the MBFD task, each of which explores one of the following training strategies: supervised learning, semi-supervised learning, and unsupervised learning. The proposed machine learning based systems and deep learning based systems are evaluated, compared, and then they are used to identify the best model for the MBFD task. We conducted extensive experiments on various benchmark datasets of motor bearing faults, including those from the American Society for Mechanical Failure Prevention Technology (MFPT), Case Western Reserve University Bearing Center (CWRU), and the Condition Monitoring of Bearing Damage in Electromechanical Drive Systems from Paderborn U
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00339</link><description>&lt;p&gt;
FedLPA: &#20351;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation. (arXiv:2310.00339v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#22320;&#32858;&#21512;&#21040;&#26381;&#21153;&#22120;&#19978;&#30340;&#20840;&#23616;&#27169;&#22411;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#38544;&#31169;&#38382;&#39064;&#20943;&#23569;&#12289;&#28508;&#22312;&#25915;&#20987;&#20943;&#24369;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#30340;&#25512;&#21160;&#65292;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#65288;&#21363;&#23558;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#38388;&#30340;&#36890;&#20449;&#38480;&#21046;&#20026;&#19968;&#36718;&#65289;&#22312;&#30740;&#31350;&#32773;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#21333;&#27425;&#32858;&#21512;&#30340;&#24615;&#33021;&#23481;&#26131;&#21463;&#21040;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#27425;&#32858;&#21512;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#65288;FedLPA&#65289;&#12290;FedLPA&#33021;&#22815;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#65292;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#25110;&#26292;&#38706;&#20219;&#20309;&#26426;&#23494;&#30340;&#26412;&#22320;&#20449;&#24687;&#65292;&#27604;&#22914;&#26631;&#31614;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing the overhead of communication, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with Layer-wise Posterior Aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any confidential local information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#65288;GAUNet&#65289;&#65292;&#29992;&#20110;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#21487;&#20197;&#19982;ASU-DNN&#30456;&#23218;&#32654;&#65292;&#24182;&#19988;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16970</link><description>&lt;p&gt;
&#20855;&#26377;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#30340;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discrete-Choice Model with Generalized Additive Utility Network. (arXiv:2309.16970v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#65288;GAUNet&#65289;&#65292;&#29992;&#20110;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#21487;&#20197;&#19982;ASU-DNN&#30456;&#23218;&#32654;&#65292;&#24182;&#19988;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#26159;&#20998;&#26512;&#20915;&#31574;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#20225;&#19994;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20351;&#29992;&#32447;&#24615;&#25928;&#29992;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#36923;&#36753;&#27169;&#22411;&#65288;MNLs&#65289;&#22240;&#20854;&#26131;&#20110;&#20351;&#29992;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20855;&#26377;&#31070;&#32463;&#32593;&#32476;&#65288;&#20363;&#22914;ASU-DNN&#65289;&#30340;MNLs&#65292;&#24182;&#19988;&#22312;&#34892;&#20026;&#36873;&#25321;&#30340;&#39044;&#27979;&#31934;&#24230;&#19978;&#27604;&#20256;&#32479;MNLs&#26356;&#39640;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30001;&#20110;&#22797;&#26434;&#32467;&#26500;&#32780;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#31216;&#20026;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#65288;GAUNet&#65289;&#65292;&#29992;&#20110;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#19996;&#20140;&#25910;&#38598;&#30340;&#20986;&#34892;&#35843;&#26597;&#25968;&#25454;&#35780;&#20272;&#20102;&#20855;&#26377;GAUNet&#30340;MNL&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;ASU-DNN&#30456;&#24403;&#65292;&#24182;&#19988;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete-choice models are a powerful framework for analyzing decision-making behavior to provide valuable insights for policymakers and businesses. Multinomial logit models (MNLs) with linear utility functions have been used in practice because they are ease to use and interpretable. Recently, MNLs with neural networks (e.g., ASU-DNN) have been developed, and they have achieved higher prediction accuracy in behavior choice than classical MNLs. However, these models lack interpretability owing to complex structures. We developed utility functions with a novel neural-network architecture based on generalized additive models, named generalized additive utility network ( GAUNet), for discrete-choice models. We evaluated the performance of the MNL with GAUNet using the trip survey data collected in Tokyo. Our models were comparable to ASU-DNN in accuracy and exhibited improved interpretability compared to previous models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36741;&#21161;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#25991;&#31038;&#31185;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;16&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#20998;&#26512;&#12289;&#25991;&#26412;&#25366;&#25496;&#12289;&#31038;&#20132;&#32593;&#32476;&#25512;&#26029;&#31561;&#12290;</title><link>http://arxiv.org/abs/2309.14379</link><description>&lt;p&gt;
&#26426;&#22120;&#36741;&#21161;&#30340;&#28151;&#21512;&#26041;&#27861;&#65306;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#20154;&#25991;&#31038;&#31185;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence. (arXiv:2309.14379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36741;&#21161;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#25991;&#31038;&#31185;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;16&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#20998;&#26512;&#12289;&#25991;&#26412;&#25366;&#25496;&#12289;&#31038;&#20132;&#32593;&#32476;&#25512;&#26029;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19981;&#26029;&#36827;&#21270;&#20026;&#20154;&#25991;&#31038;&#31185;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#33021;&#22815;&#22312;&#20197;&#21069;&#36890;&#24120;&#30001;&#20154;&#21147;&#23436;&#25104;&#30340;&#23450;&#24615;&#20998;&#26512;&#20219;&#21153;&#20013;&#23454;&#29616;&#35268;&#27169;&#21270;&#12289;&#33258;&#21160;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#20197;&#21033;&#29992;&#23450;&#24615;&#20998;&#26512;&#19987;&#19994;&#30693;&#35782;&#12289;&#26426;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20005;&#35880;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#27880;&#37325;&#36879;&#26126;&#24230;&#21644;&#21487;&#22797;&#21046;&#24615;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;16&#20010;&#26426;&#22120;&#36741;&#21161;&#30340;&#26696;&#20363;&#30740;&#31350;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#12290;&#20219;&#21153;&#21253;&#25324;&#35821;&#35328;&#21644;&#35805;&#35821;&#20998;&#26512;&#12289;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#12289;&#37319;&#35775;&#20998;&#26512;&#12289;&#21382;&#21490;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#21644;&#25991;&#26412;&#25366;&#25496;&#12289;&#25919;&#27835;&#31435;&#22330;&#26816;&#27979;&#12289;&#25991;&#26412;&#21644;&#24605;&#24819;&#37325;&#22797;&#20351;&#29992;&#12289;&#25991;&#23398;&#21644;&#30005;&#24433;&#20013;&#30340;&#25991;&#31867;&#26500;&#25104;&#12289;&#31038;&#20132;&#32593;&#32476;&#25512;&#26029;&#12289;&#33258;&#21160;&#35789;&#20856;&#32534;&#32386;&#12289;&#20803;&#25968;&#25454;&#34917;&#20805;&#21644;&#22810;&#27169;&#24577;&#35270;&#35273;&#25991;&#21270;&#20998;&#26512;&#12290;&#19982;&#29616;&#26377;LLM&#24212;&#29992;&#25991;&#29486;&#20013;&#23545;&#33521;&#25991;&#30340;&#20851;&#27880;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#28085;&#30422;&#22810;&#31181;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing capacities of large language models (LLMs) present an unprecedented opportunity to scale up data analytics in the humanities and social sciences, augmenting and automating qualitative analytic tasks previously typically allocated to human labor. This contribution proposes a systematic mixed methods framework to harness qualitative analytic expertise, machine scalability, and rigorous quantification, with attention to transparency and replicability. 16 machine-assisted case studies are showcased as proof of concept. Tasks include linguistic and discourse analysis, lexical semantic change detection, interview analysis, historical event cause inference and text mining, detection of political stance, text and idea reuse, genre composition in literature and film; social network inference, automated lexicography, missing metadata augmentation, and multimodal visual cultural analytics. In contrast to the focus on English in the emerging LLM applicability literature, many exampl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#65292;&#36890;&#36807;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30701;DNA&#24207;&#21015;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11087</link><description>&lt;p&gt;
Embed-Search-Align: &#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;DNA&#24207;&#21015;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Embed-Search-Align: DNA Sequence Alignment using Transformer Models. (arXiv:2309.11087v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11087
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#65292;&#36890;&#36807;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30701;DNA&#24207;&#21015;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNA&#24207;&#21015;&#23545;&#40784;&#28041;&#21450;&#23558;&#30701;DNA&#35835;&#21462;&#20998;&#37197;&#21040;&#24191;&#27867;&#30340;&#21442;&#32771;&#22522;&#22240;&#32452;&#19978;&#30340;&#26368;&#21487;&#33021;&#20301;&#32622;&#12290;&#36825;&#20010;&#36807;&#31243;&#23545;&#20110;&#21508;&#31181;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#21464;&#24322;&#35843;&#29992;&#12289;&#36716;&#24405;&#32452;&#23398;&#21644;&#34920;&#35266;&#22522;&#22240;&#32452;&#23398;&#12290;&#20256;&#32479;&#26041;&#27861;&#32463;&#36807;&#25968;&#21313;&#24180;&#30340;&#25913;&#36827;&#65292;&#20197;&#20004;&#20010;&#27493;&#39588;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65306;&#20808;&#36827;&#34892;&#22522;&#22240;&#32452;&#32034;&#24341;&#65292;&#28982;&#21518;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#20197;&#30830;&#23450;&#32473;&#23450;&#35835;&#21462;&#30340;&#21487;&#33021;&#20301;&#32622;&#12290;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#23558;&#25991;&#26412;&#32534;&#30721;&#20026;&#23884;&#20837;&#21521;&#37327;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21162;&#21147;&#25506;&#32034;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#30456;&#21516;&#30340;Transformer&#26550;&#26500;&#20026;DNA&#24207;&#21015;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#24050;&#32463;&#22312;&#28041;&#21450;&#20998;&#31867;&#30701;DNA&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26089;&#26399;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#26816;&#27979;&#32534;&#30721;&#21644;&#38750;&#32534;&#30721;&#21306;&#22495;&#20197;&#21450;&#35782;&#21035;&#22686;&#24378;&#23376;&#21644;&#21551;&#21160;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#24207;&#21015;&#23545;&#40784;&#20219;&#21153;&#65292;&#23545;&#40784;&#20219;&#21153;&#30340;&#20851;&#38190;&#26159;&#22312;&#20445;&#25345;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;&#21516;&#26102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23545;&#24212;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNA sequence alignment involves assigning short DNA reads to the most probable locations on an extensive reference genome. This process is crucial for various genomic analyses, including variant calling, transcriptomics, and epigenomics. Conventional methods, refined over decades, tackle this challenge in two steps: genome indexing followed by efficient search to locate likely positions for given reads. Building on the success of Large Language Models (LLM) in encoding text into embeddings, where the distance metric captures semantic similarity, recent efforts have explored whether the same Transformer architecture can produce numerical representations for DNA sequences. Such models have shown early promise in tasks involving classification of short DNA sequences, such as the detection of coding vs non-coding regions, as well as the identification of enhancer and promoter sequences. Performance at sequence classification tasks does not, however, translate to sequence alignment, where i
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#30001;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#32534;&#35793;&#30340;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#21333;&#20301;&#32763;&#36716;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#35774;&#35745;&#20102;&#33258;&#21160;&#25628;&#32034;&#24037;&#20855;&#20197;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#20301;&#65292;&#24182;&#30830;&#23450;&#20102;&#23454;&#38469;&#25915;&#20987;&#21521;&#37327;&#65292;&#25581;&#31034;&#20102;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#25915;&#20987;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.06223</link><description>&lt;p&gt;
&#25581;&#31034;&#23545;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#21333;&#20301;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Unveiling Signle-Bit-Flip Attacks on DNN Executables. (arXiv:2309.06223v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06223
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30001;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#32534;&#35793;&#30340;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#21333;&#20301;&#32763;&#36716;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#35774;&#35745;&#20102;&#33258;&#21160;&#25628;&#32034;&#24037;&#20855;&#20197;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#20301;&#65292;&#24182;&#30830;&#23450;&#20102;&#23454;&#38469;&#25915;&#20987;&#21521;&#37327;&#65292;&#25581;&#31034;&#20102;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#25915;&#20987;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20301;&#32763;&#36716;&#25915;&#20987;(BFA)&#21487;&#20197;&#36890;&#36807;DRAM Rowhammer&#21033;&#29992;&#26469;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#20027;&#35201;&#38024;&#23545;&#39640;&#32423;DNN&#26694;&#26550;&#65288;&#22914;PyTorch&#65289;&#20013;&#30340;&#27169;&#22411;&#26435;&#37325;&#25991;&#20214;&#36827;&#34892;&#20301;&#32763;&#36716;&#12290;&#28982;&#32780;&#65292;DNN&#32463;&#24120;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#32534;&#35793;&#25104;&#20302;&#32423;&#21487;&#25191;&#34892;&#25991;&#20214;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20302;&#32423;&#30828;&#20214;&#21407;&#35821;&#12290;&#32534;&#35793;&#21518;&#30340;&#20195;&#30721;&#36890;&#24120;&#36895;&#24230;&#24456;&#24555;&#65292;&#24182;&#19988;&#19982;&#39640;&#32423;DNN&#26694;&#26550;&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#30340;&#25191;&#34892;&#33539;&#24335;&#12290;&#26412;&#25991;&#38024;&#23545;&#30001;DL&#32534;&#35793;&#22120;&#32534;&#35793;&#30340;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;BFA&#25915;&#20987;&#38754;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#25628;&#32034;&#24037;&#20855;&#65292;&#29992;&#20110;&#35782;&#21035;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#20013;&#30340;&#26131;&#21463;&#25915;&#20987;&#20301;&#65292;&#24182;&#30830;&#23450;&#21033;&#29992;BFAs&#25915;&#20987;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#20013;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#23454;&#38469;&#25915;&#20987;&#21521;&#37327;&#65288;&#32780;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#23545;&#25915;&#20987;&#27169;&#22411;&#26435;&#37325;&#20570;&#20986;&#20102;&#24378;&#20551;&#35774;&#65289;&#12290;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#20284;&#20046;&#27604;&#39640;&#32423;DNN&#20013;&#30340;&#27169;&#22411;&#26356;&#21152;&#8220;&#19981;&#36879;&#26126;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that bit-flip attacks (BFAs) can manipulate deep neural networks (DNNs) via DRAM Rowhammer exploitations. Existing attacks are primarily launched over high-level DNN frameworks like PyTorch and flip bits in model weight files. Nevertheless, DNNs are frequently compiled into low-level executables by deep learning (DL) compilers to fully leverage low-level hardware primitives. The compiled code is usually high-speed and manifests dramatically distinct execution paradigms from high-level DNN frameworks.  In this paper, we launch the first systematic study on the attack surface of BFA specifically for DNN executables compiled by DL compilers. We design an automated search tool to identify vulnerable bits in DNN executables and identify practical attack vectors that exploit the model structure in DNN executables with BFAs (whereas prior works make likely strong assumptions to attack model weights). DNN executables appear more "opaque" than models in high-level DNN 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HIntS&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#22522;&#20110;Granger&#22240;&#26524;&#24615;&#25429;&#25417;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#38190;&#20107;&#20214;&#65292;&#21457;&#29616;&#21644;&#35757;&#32451;&#19968;&#31995;&#21015;&#25805;&#20316;&#22240;&#32032;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#32032;&#30340;&#25216;&#33021;&#65292;&#20854;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#25512;&#21160;&#20219;&#21153;&#19978;&#26377;2-3&#20493;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#26377;&#25928;&#30340;&#22788;&#29702;&#20102;&#22797;&#26434;&#38382;&#39064;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.09509</link><description>&lt;p&gt;
Granger&#22240;&#26524;&#30340;&#20998;&#23618;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Granger-Causal Hierarchical Skill Discovery. (arXiv:2306.09509v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HIntS&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#22522;&#20110;Granger&#22240;&#26524;&#24615;&#25429;&#25417;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#38190;&#20107;&#20214;&#65292;&#21457;&#29616;&#21644;&#35757;&#32451;&#19968;&#31995;&#21015;&#25805;&#20316;&#22240;&#32032;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#32032;&#30340;&#25216;&#33021;&#65292;&#20854;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#25512;&#21160;&#20219;&#21153;&#19978;&#26377;2-3&#20493;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#26377;&#25928;&#30340;&#22788;&#29702;&#20102;&#22797;&#26434;&#38382;&#39064;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#30340;&#31574;&#30053;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#20250;&#36973;&#21463;&#20302;&#26679;&#26412;&#25928;&#29575;&#21644;&#26377;&#38480;&#36716;&#31227;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HIntS&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#23398;&#20064;&#24471;&#21040;&#30340;&#20132;&#20114;&#26816;&#27979;&#22120;&#26469;&#21457;&#29616;&#21644;&#35757;&#32451;&#19968;&#31995;&#21015;&#25216;&#33021;&#65292;&#36825;&#20123;&#25216;&#33021;&#25805;&#20316;&#22240;&#32032;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#32032;&#12290;&#21463;Granger&#22240;&#26524;&#24615;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#26080;&#30417;&#30563;&#26816;&#27979;&#22120;&#25429;&#25417;&#21040;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#38190;&#20107;&#20214;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#23398;&#20064;&#26377;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#33021;&#36716;&#31227;&#21040;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#26159;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25152;&#38754;&#20020;&#30340;&#22256;&#22659;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24102;&#26377;&#38556;&#30861;&#29289;&#30340;&#26426;&#22120;&#20154;&#25512;&#21160;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;HIntS - &#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#65292;&#20854;&#20182;RL&#21644;HRL&#26041;&#27861;&#37117;&#34920;&#29616;&#19981;&#20339;&#12290;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#19981;&#20165;&#23637;&#31034;&#20102;&#20351;&#29992;Breakout&#30340;&#21464;&#20307;&#30340;&#36716;&#31227;&#65292;&#32780;&#19988;&#19982;&#21487;&#27604;&#36739;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30456;&#27604;&#65292;&#36824;&#34920;&#29616;&#20986;2-3&#20493;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;HIntS&#19968;&#36215;&#35777;&#26126;&#20102;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has shown promising results learning policies for complex tasks, but can often suffer from low sample efficiency and limited transfer. We introduce the Hierarchy of Interaction Skills (HIntS) algorithm, which uses learned interaction detectors to discover and train a hierarchy of skills that manipulate factors in factored environments. Inspired by Granger causality, these unsupervised detectors capture key events between factors to sample efficiently learn useful skills and transfer those skills to other related tasks -- tasks where many reinforcement learning techniques struggle. We evaluate HIntS on a robotic pushing task with obstacles -- a challenging domain where other RL and HRL methods fall short. The learned skills not only demonstrate transfer using variants of Breakout, a common RL benchmark, but also show 2-3x improvement in both sample efficiency and final performance compared to comparable RL baselines. Together, HIntS demonstrates a proof of co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;AI&#31574;&#23637;&#21644;&#35266;&#20247;&#20114;&#21160;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#26500;&#24819;&#20102;&#36203;&#23572;&#36763;&#22522;&#24066;&#33402;&#26415;&#21452;&#24180;&#23637;&#65292;&#20351;&#29992;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#23558;&#23460;&#20869;&#33402;&#26415;&#21697;&#25918;&#32622;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;360&#33402;&#26415;&#20840;&#26223;&#22270;&#65292;&#20197;&#21019;&#36896;&#20986;&#33402;&#26415;&#21697;&#19982;&#22478;&#24066;&#31354;&#38388;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2306.03753</link><description>&lt;p&gt;
AI&#33402;&#26415;&#31574;&#23637;&#65306;&#37325;&#26032;&#26500;&#24819;&#36203;&#23572;&#36763;&#22522;&#24066;&#33402;&#26415;&#21452;&#24180;&#23637;
&lt;/p&gt;
&lt;p&gt;
AI Art Curation: Re-imagining the city of Helsinki in occasion of its Biennial. (arXiv:2306.03753v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;AI&#31574;&#23637;&#21644;&#35266;&#20247;&#20114;&#21160;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#26500;&#24819;&#20102;&#36203;&#23572;&#36763;&#22522;&#24066;&#33402;&#26415;&#21452;&#24180;&#23637;&#65292;&#20351;&#29992;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#23558;&#23460;&#20869;&#33402;&#26415;&#21697;&#25918;&#32622;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;360&#33402;&#26415;&#20840;&#26223;&#22270;&#65292;&#20197;&#21019;&#36896;&#20986;&#33402;&#26415;&#21697;&#19982;&#22478;&#24066;&#31354;&#38388;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#31574;&#23637;&#23454;&#36341;&#30340;&#29305;&#28857;&#26159;&#20197;&#30693;&#35782;&#30340;&#26041;&#24335;&#23637;&#31034;&#33402;&#26415;&#25910;&#34255;&#21697;&#12290;&#26426;&#22120;&#36807;&#31243;&#30340;&#29305;&#28857;&#26159;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#21644;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#12290;&#26412;&#25991;&#35774;&#24819;&#20102;AI&#31574;&#23637;&#21644;&#35266;&#20247;&#20114;&#21160;&#65292;&#20197;&#25506;&#32034;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#31574;&#23637;&#30028;&#30340;&#24433;&#21709;&#12290;&#35813;&#39033;&#30446;&#26159;&#20026;2023&#24180;&#36203;&#23572;&#36763;&#22522;&#33402;&#26415;&#21452;&#24180;&#23637;&#30340;&#22330;&#21512;&#32780;&#24320;&#21457;&#30340;&#65292;&#39064;&#20026;&#8220;&#21487;&#33021;&#20986;&#29616;&#26032;&#30340;&#26041;&#21521;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#36203;&#23572;&#36763;&#22522;&#33402;&#26415;&#21338;&#29289;&#39302;&#65288;HAM&#65289;&#30340;&#34255;&#21697;&#65292;&#36890;&#36807;&#26426;&#22120;&#24863;&#30693;&#30340;&#35270;&#35282;&#37325;&#26032;&#26500;&#24819;&#20102;&#36203;&#23572;&#36763;&#22522;&#24066;&#12290;&#25105;&#20204;&#20351;&#29992;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#23637;&#31034;&#23460;&#20869;&#33402;&#26415;&#21697;&#65292;&#26681;&#25454;&#30456;&#20284;&#24615;&#35780;&#20998;&#20998;&#37197;&#34394;&#26500;&#30340;&#22352;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#30340;360&#33402;&#26415;&#20840;&#26223;&#22270;&#26469;&#25913;&#21464;&#27599;&#20214;&#33402;&#26415;&#21697;&#22312;&#22478;&#24066;&#20013;&#30340;&#25152;&#22788;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#27599;&#20214;&#33402;&#26415;&#21697;&#20301;&#32622;&#30340;360&#20840;&#26223;&#22270;&#30340;&#28145;&#24230;&#20540;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#33402;&#26415;&#21697;&#25552;&#31034;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#36825;&#20010;&#39033;&#30446;&#30340;&#32467;&#26524;&#23601;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Art curatorial practice is characterized by the presentation of an art collection in a knowledgeable way. Machine processes are characterized by their capacity to manage and analyze large amounts of data. This paper envisages AI curation and audience interaction to explore the implications of contemporary machine learning models for the curatorial world. This project was developed for the occasion of the 2023 Helsinki Art Biennial, entitled New Directions May Emerge. We use the Helsinki Art Museum (HAM) collection to re-imagine the city of Helsinki through the lens of machine perception. We use visual-textual models to place indoor artworks in public spaces, assigning fictional coordinates based on similarity scores. We transform the space that each artwork inhabits in the city by generating synthetic 360 art panoramas. We guide the generation estimating depth values from 360 panoramas at each artwork location, and machine-generated prompts of the artworks. The result of this project i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#31232;&#30095;&#38647;&#36798;&#26816;&#27979;&#26144;&#23556;&#21040;&#26497;&#22352;&#26631;&#27979;&#37327;&#32593;&#26684;&#65292;&#24182;&#29983;&#25104;&#21160;&#24577;&#32593;&#26684;&#22320;&#22270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20960;&#20309;ISM&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20026;&#20174;&#26377;&#38480;&#35270;&#22330;&#30340;&#38647;&#36798;&#20013;&#23398;&#20064;&#26497;&#22352;&#26631;&#26041;&#26696;&#30340;&#21333;&#24103;&#27979;&#37327;&#32593;&#26684;&#30340;&#31532;&#19968;&#20010;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12409</link><description>&lt;p&gt;
&#21160;&#24577;&#21344;&#25454;&#32593;&#26684;&#22320;&#22270;&#30340;&#28145;&#24230;&#38647;&#36798;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Radar Inverse Sensor Models for Dynamic Occupancy Grid Maps. (arXiv:2305.12409v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#31232;&#30095;&#38647;&#36798;&#26816;&#27979;&#26144;&#23556;&#21040;&#26497;&#22352;&#26631;&#27979;&#37327;&#32593;&#26684;&#65292;&#24182;&#29983;&#25104;&#21160;&#24577;&#32593;&#26684;&#22320;&#22270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20960;&#20309;ISM&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20026;&#20174;&#26377;&#38480;&#35270;&#22330;&#30340;&#38647;&#36798;&#20013;&#23398;&#20064;&#26497;&#22352;&#26631;&#26041;&#26696;&#30340;&#21333;&#24103;&#27979;&#37327;&#32593;&#26684;&#30340;&#31532;&#19968;&#20010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#26159;&#22522;&#20110;&#20256;&#24863;&#22120;&#36755;&#20837;&#23545;&#36710;&#36742;&#29615;&#22659;&#36827;&#34892;&#24314;&#27169;&#12290;&#30001;&#20110;&#20854;&#20247;&#25152;&#21608;&#30693;&#30340;&#20248;&#21183;&#65292;&#38647;&#36798;&#25104;&#20026;&#25512;&#26029;&#22260;&#32469;&#36710;&#36742;&#30340;&#32593;&#26684;&#21333;&#20803;&#21344;&#29992;&#29366;&#24577;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#38647;&#36798;&#26816;&#27979;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;&#65288;ISM&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#31232;&#30095;&#38647;&#36798;&#26816;&#27979;&#21040;&#26497;&#22352;&#26631;&#27979;&#37327;&#32593;&#26684;&#30340;&#26144;&#23556;&#12290;&#25913;&#36827;&#30340;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#27979;&#37327;&#30340;&#32593;&#26684;&#29992;&#20316;&#21442;&#32771;&#12290;&#23398;&#20064;&#21040;&#30340;&#38647;&#36798;&#27979;&#37327;&#32593;&#26684;&#19982;&#38647;&#36798;&#22810;&#26222;&#21202;&#36895;&#24230;&#27979;&#37327;&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#29992;&#20110;&#29983;&#25104;&#21160;&#24577;&#32593;&#26684;&#22320;&#22270;&#65288;DGM&#65289;&#12290;&#22312;&#23454;&#38469;&#30340;&#39640;&#36895;&#20844;&#36335;&#24773;&#26223;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20960;&#20309;ISM&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#20174;&#26377;&#38480;&#35270;&#22330;&#65288;FOV&#65289;&#30340;&#38647;&#36798;&#20013;&#23398;&#20064;&#26497;&#22352;&#26631;&#26041;&#26696;&#30340;&#21333;&#24103;&#27979;&#37327;&#32593;&#26684;&#30340;&#26041;&#27861;&#12290;&#23398;&#20064;&#26694;&#26550;&#20351;&#23398;&#20064;&#21040;&#30340;ISM&#21487;&#20197;&#30452;&#25509;&#23884;&#20837;&#21040;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#29366;&#24577;&#20272;&#35745;&#26041;&#26696;&#20013;&#65292;&#20197;&#25552;&#39640;&#29615;&#22659;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To implement autonomous driving, one essential step is to model the vehicle environment based on the sensor inputs. Radars, with their well-known advantages, became a popular option to infer the occupancy state of grid cells surrounding the vehicle. To tackle data sparsity and noise of radar detections, we propose a deep learning-based Inverse Sensor Model (ISM) to learn the mapping from sparse radar detections to polar measurement grids. Improved lidar-based measurement grids are used as reference. The learned radar measurement grids, combined with radar Doppler velocity measurements, are further used to generate a Dynamic Grid Map (DGM). Experiments in real-world highway scenarios show that our approach outperforms the hand-crafted geometric ISMs. In comparison to state-of-the-art deep learning methods, our approach is the first one to learn a single-frame measurement grid in the polar scheme from radars with a limited Field Of View (FOV). The learning framework makes the learned ISM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#20998;&#24067;&#22806;&#26816;&#27979;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21487;&#20197;&#20998;&#35299;&#25104;&#20219;&#21153;&#20869;&#39044;&#27979;&#21644;&#20219;&#21153; ID &#39044;&#27979;&#65292;&#24182;&#19988;&#20219;&#21153; ID &#39044;&#27979;&#19982;&#20998;&#24067;&#22806;&#26816;&#27979;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2304.10038</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#25345;&#32493;&#23398;&#20064;&#65306;&#32479;&#19968;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Open-World Continual Learning: Unifying Novelty Detection and Continual Learning. (arXiv:2304.10038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#20998;&#24067;&#22806;&#26816;&#27979;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21487;&#20197;&#20998;&#35299;&#25104;&#20219;&#21153;&#20869;&#39044;&#27979;&#21644;&#20219;&#21153; ID &#39044;&#27979;&#65292;&#24182;&#19988;&#20219;&#21153; ID &#39044;&#27979;&#19982;&#20998;&#24067;&#22806;&#26816;&#27979;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528; AI agent &#22312;&#26410;&#30693;&#25110;&#26032;&#22855;&#30340;&#30495;&#23454;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#23427;&#20204;&#38656;&#35201;&#20855;&#22791; (1) &#35748;&#35782;&#24050;&#32463;&#23398;&#20064;&#36807;&#30340;&#29289;&#20307;&#21644;&#26816;&#27979;&#21040;&#20043;&#21069;&#26410;&#35265;&#25110;&#23398;&#20064;&#30340;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450; (2) &#22686;&#37327;&#22320;&#23398;&#20064;&#26032;&#29289;&#21697;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#26377;&#30693;&#35782;&#21644;&#26356;&#24378;&#22823;&#12290; (1) &#31216;&#20026;&#26032;&#39062;&#24615;&#26816;&#27979;&#25110;&#20998;&#24067;&#22806; (OOD) &#26816;&#27979;&#65292;&#32780; (2) &#31216;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064; (CIL)&#65292;&#26159;&#25345;&#32493;&#23398;&#20064; (CL) &#30340;&#19968;&#31181;&#35774;&#32622;&#12290;&#22312;&#29616;&#26377;&#30340;&#30740;&#31350;&#20013;&#65292;OOD &#26816;&#27979;&#21644; CIL &#34987;&#35270;&#20026;&#20004;&#20010;&#23436;&#20840;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102; OOD &#26816;&#27979;&#23454;&#38469;&#19978;&#23545;&#20110; CIL &#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034; CIL &#21487;&#20197;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65306;&#20219;&#21153;&#20869;&#39044;&#27979; (WP) &#21644;&#20219;&#21153; ID &#39044;&#27979;(TP)&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102; TP &#19982; OOD &#26816;&#27979;&#30456;&#20851;&#12290;&#20851;&#38190;&#30340;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#26080;&#35770; WP &#21644; OOD &#26816;&#27979;&#65288;&#25110; TP&#65289;&#26159;&#21542;&#30001; CIL &#31639;&#27861;&#26174;&#24335;&#25110;&#38544;&#24335;&#22320;&#23450;&#20041;&#65292;&#22909;&#30340; WP &#21644;&#33391;&#22909;&#30340; OOD &#26816;&#27979;&#25110; TP &#24635;&#26159;&#23384;&#22312;&#23884;&#20837;&#22312;&#20219;&#20309; CIL &#31639;&#27861;&#20013;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI agents are increasingly used in the real open world with unknowns or novelties, they need the ability to (1) recognize objects that (i) they have learned and (ii) detect items that they have not seen or learned before, and (2) learn the new items incrementally to become more and more knowledgeable and powerful. (1) is called novelty detection or out-of-distribution (OOD) detection and (2) is called class incremental learning (CIL), which is a setting of continual learning (CL). In existing research, OOD detection and CIL are regarded as two completely different problems. This paper theoretically proves that OOD detection actually is necessary for CIL. We first show that CIL can be decomposed into two sub-problems: within-task prediction (WP) and task-id prediction (TP). We then prove that TP is correlated with OOD detection. The key theoretical result is that regardless of whether WP and OOD detection (or TP) are defined explicitly or implicitly by a CIL algorithm, good WP and go
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;Robustmix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;Imagenet-C&#21644;Stylized Imagenet&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#22312;&#36991;&#20813;&#35745;&#31639;&#24320;&#38144;&#21644;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#30340;&#21516;&#26102;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2304.02847</link><description>&lt;p&gt;
Robustmix&#65306;&#36890;&#36807;&#27491;&#21017;&#21270;&#28145;&#24230;&#32593;&#32476;&#30340;&#39057;&#29575;&#20559;&#24046;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustmix: Improving Robustness by Regularizing the Frequency Bias of Deep Nets. (arXiv:2304.02847v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;Robustmix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;Imagenet-C&#21644;Stylized Imagenet&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#22312;&#36991;&#20813;&#35745;&#31639;&#24320;&#38144;&#21644;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#30340;&#21516;&#26102;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#22312;&#19968;&#31995;&#21015;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23545;&#20154;&#31867;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#30340;&#25200;&#21160;&#20173;&#28982;&#24456;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Robustmix&#30340;Mixup&#26032;&#25193;&#23637;&#65292;&#35813;&#25193;&#23637;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#22522;&#20110;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#27491;&#21017;&#21270;&#25913;&#21892;&#20102;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;Imagenet-C&#21644;Stylized Imagenet&#12290;&#23427;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#36827;&#19968;&#27493;&#34917;&#20805;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20351;&#29992;EfficientNet-B8&#27169;&#22411;&#21644;RandAugment&#36798;&#21040;&#20102;44.8&#30340;&#26368;&#26032;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#30456;&#27604;&#22522;&#32447;&#38477;&#20302;&#20102;16&#20010;mCE&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep networks have achieved impressive results on a range of well-curated benchmark datasets. Surprisingly, their performance remains sensitive to perturbations that have little effect on human performance. In this work, we propose a novel extension of Mixup called Robustmix that regularizes networks to classify based on lower-frequency spatial features. We show that this type of regularization improves robustness on a range of benchmarks such as Imagenet-C and Stylized Imagenet. It adds little computational overhead and, furthermore, does not require a priori knowledge of a large set of image transformations. We find that this approach further complements recent advances in model architecture and data augmentation, attaining a state-of-the-art mCE of 44.8 with an EfficientNet-B8 model and RandAugment, which is a reduction of 16 mCE compared to the baseline.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#20559;&#24207;&#30340;&#32447;&#24615;&#25193;&#23637;&#25968;&#37327;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#30001;&#21253;&#21547;&#20851;&#31995;&#30830;&#23450;&#30340;&#22270;&#24418;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#20559;&#24207;&#65292;&#25214;&#21040;&#20102;&#36335;&#24452;&#12289;&#29615;&#12289;&#26143;&#24418;&#22270;&#12289;&#21452;&#26143;&#24418;&#22270;&#21644;&#23436;&#20840;&#22270;&#30340;&#26500;&#36896;&#24207;&#21015;&#25968;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20844;&#24335;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#32467;&#26500;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.13186</link><description>&lt;p&gt;
&#26500;&#36896;&#25968;&#65306;&#22914;&#20309;&#24314;&#31435;&#19968;&#20010;&#22270;&#24418;&#65311;
&lt;/p&gt;
&lt;p&gt;
Construction numbers: How to build a graph?. (arXiv:2302.13186v2 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13186
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#20559;&#24207;&#30340;&#32447;&#24615;&#25193;&#23637;&#25968;&#37327;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#30001;&#21253;&#21547;&#20851;&#31995;&#30830;&#23450;&#30340;&#22270;&#24418;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#20559;&#24207;&#65292;&#25214;&#21040;&#20102;&#36335;&#24452;&#12289;&#29615;&#12289;&#26143;&#24418;&#22270;&#12289;&#21452;&#26143;&#24418;&#22270;&#21644;&#23436;&#20840;&#22270;&#30340;&#26500;&#36896;&#24207;&#21015;&#25968;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20844;&#24335;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#32467;&#26500;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;50&#24180;&#21069;&#65292;&#26031;&#22374;&#21033;&#32771;&#34385;&#20102;&#35745;&#31639;&#20559;&#24207;&#30340;&#32447;&#24615;&#25193;&#23637;&#25968;&#37327;&#38382;&#39064;&#12290;&#23545;&#20110;&#30001;&#21253;&#21547;&#20851;&#31995;&#30830;&#23450;&#30340;&#22270;&#24418;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#20559;&#24207;&#65292;&#25105;&#20204;&#31216;&#36825;&#26679;&#30340;&#32447;&#24615;&#25193;&#23637;&#20026;&#22270;&#24418;&#30340;&#8220;&#26500;&#36896;&#24207;&#21015;&#8221;&#65292;&#22240;&#20026;&#27599;&#20010;&#36793;&#37117;&#36981;&#24490;&#20854;&#20004;&#20010;&#31471;&#28857;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#36335;&#24452;&#12289;&#29615;&#12289;&#26143;&#24418;&#22270;&#12289;&#21452;&#26143;&#24418;&#22270;&#21644;&#23436;&#20840;&#22270;&#30340;&#27492;&#31867;&#24207;&#21015;&#25968;&#37327;&#12290;&#23545;&#20110;&#36335;&#24452;&#65292;&#25105;&#20204;&#35748;&#21516;&#26031;&#22374;&#21033;&#30340;&#24819;&#27861;&#65288;&#20999;&#32447;&#25968;&#65289;&#65292;&#24182;&#24471;&#21040;&#20102;&#20854;&#20182;&#31867;&#22411;&#30340;&#20844;&#24335;&#12290;&#27492;&#22806;&#36824;&#30740;&#31350;&#20102;&#32467;&#26500;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counting the number of linear extensions of a partial order was considered by Stanley about 50 years ago. For the partial order on the vertices and edges of a graph determined by inclusion, we call such linear extensions {\it construction sequences} for the graph as each edge follows both of its endpoints. The number of such sequences for paths, cycles, stars, double-stars, and complete graphs is found. For paths, we agree with Stanley (the Tangent numbers) and get formulas for the other classes. Structure and applications are also studied.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#38024;&#23545;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20171;&#32461;&#20102;&#28041;&#21450;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20197;&#21450;&#25551;&#36848;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#21644;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#35758;&#24615;&#30340;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2211.09172</link><description>&lt;p&gt;
&#25991;&#23383;&#23545;&#35805;&#20013;&#30340;&#28145;&#24230;&#24773;&#24863;&#35782;&#21035;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#38024;&#23545;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20171;&#32461;&#20102;&#28041;&#21450;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20197;&#21450;&#25551;&#36848;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#21644;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#35758;&#24615;&#30340;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#24180;&#26469;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#26032;&#30340;&#24212;&#29992;&#21644;&#23454;&#26045;&#22330;&#26223;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#21033;&#29992;&#23545;&#35805;&#35821;&#22659;&#12289;&#35828;&#35805;&#20154;&#21644;&#24773;&#24863;&#21160;&#24577;&#24314;&#27169;&#65292;&#35299;&#37322;&#24120;&#35782;&#34920;&#36798;&#12289;&#38750;&#27491;&#24335;&#35821;&#35328;&#21644;&#35773;&#21050;&#65292;&#24212;&#23545;&#23454;&#26102;&#24773;&#24863;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#35782;&#21035;&#24773;&#24863;&#21407;&#22240;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#31181;&#20998;&#31867;&#27861;&#65292;&#22810;&#35821;&#35328;&#24773;&#24863;&#35782;&#21035;&#20197;&#21450;&#35299;&#37322;&#24615;&#12290;&#26412;&#35843;&#30740;&#39318;&#20808;&#20171;&#32461;&#20102;&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#19982;&#27492;&#20219;&#21153;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#28982;&#21518;&#65292;&#23427;&#20171;&#32461;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#22810;&#31181;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#24773;&#24863;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#23427;&#25551;&#36848;&#20102;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#65292;&#24182;&#35299;&#37322;&#20102;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#26368;&#21518;&#65292;&#23427;&#25552;&#20379;&#20102;&#23545;&#20110;&#26356;&#22909;&#30340;&#26694;&#26550;&#30340;&#24314;&#35758;&#24615;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#22788;&#29702;&#20027;&#35266;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Emotion Recognition in Conversations (ERC) has seen a tremendous advancement in the last few years, new applications and implementation scenarios present novel challenges and opportunities. These range from leveraging the conversational context, speaker and emotion dynamics modelling, to interpreting common sense expressions, informal language and sarcasm, addressing challenges of real time ERC, recognizing emotion causes, different taxonomies across datasets, multilingual ERC to interpretability. This survey starts by introducing ERC, elaborating on the challenges and opportunities pertaining to this task. It proceeds with a description of the emotion taxonomies and a variety of ERC benchmark datasets employing such taxonomies. This is followed by descriptions of the most prominent works in ERC with explanations of the Deep Learning architectures employed. Then, it provides advisable ERC practices towards better frameworks, elaborating on methods to deal with subjectivity in ann
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#26631;&#24230;&#22312;&#32447;&#23398;&#20064;&#30340;&#31561;&#35843;&#33410;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20855;&#26377;&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#12289;&#38543;&#26102;&#38543;&#22320;&#21644;&#26080;&#26631;&#24230;&#30340;&#29305;&#28857;&#65292;&#24182;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#36951;&#25022;&#30340;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22312;&#32447;&#26657;&#27491;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.14586</link><description>&lt;p&gt;
&#20351;&#29992;&#20110;&#26080;&#26631;&#24230;&#22312;&#32447;&#23398;&#20064;&#30340;&#31561;&#35843;&#33410;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Isotuning With Applications To Scale-Free Online Learning. (arXiv:2112.14586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14586
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#26631;&#24230;&#22312;&#32447;&#23398;&#20064;&#30340;&#31561;&#35843;&#33410;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20855;&#26377;&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#12289;&#38543;&#26102;&#38543;&#22320;&#21644;&#26080;&#26631;&#24230;&#30340;&#29305;&#28857;&#65292;&#24182;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#36951;&#25022;&#30340;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22312;&#32447;&#26657;&#27491;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25193;&#23637;&#21644;&#32467;&#21512;&#20102;&#25991;&#29486;&#20013;&#30340;&#20960;&#31181;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#12289;&#38543;&#26102;&#38543;&#22320;&#21644;&#26080;&#26631;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#26080;&#26631;&#24230;&#30340;&#36951;&#25022;&#30028;&#38480;&#24517;&#39035;&#19982;&#26368;&#22823;&#25439;&#22833;&#25104;&#32447;&#24615;&#20851;&#31995;&#65292;&#19981;&#35770;&#26159;&#23545;&#20110;&#22823;&#25439;&#22833;&#36824;&#26159;&#23545;&#20110;&#38750;&#24120;&#23567;&#30340;&#25439;&#22833;&#12290;&#33258;&#36866;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#34920;&#26126;&#31639;&#27861;&#21487;&#20197;&#21033;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#24182;&#21487;&#33021;&#20855;&#26377;&#24120;&#25968;&#36951;&#25022;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#23613;&#21487;&#33021;&#23569;&#20381;&#36182;&#21442;&#25968;&#30340;&#24555;&#36895;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#24212;&#35813;&#26159;&#38543;&#26102;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#19981;&#20381;&#36182;&#20110;&#26102;&#38388;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#21644;&#20027;&#35201;&#24037;&#20855;&#26159;&#31561;&#35843;&#33410;&#25216;&#26415;&#65292;&#23427;&#26159;&#24179;&#34913;&#36951;&#25022;&#26435;&#34913;&#30340;&#24605;&#24819;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#24037;&#20855;&#26469;&#36731;&#26494;&#35774;&#35745;&#21644;&#20998;&#26512;&#36825;&#26679;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#36951;&#25022;&#30340;&#36895;&#29575;&#65288;&#26080;&#35770;&#26159;&#24120;&#25968;&#12289;$O(\log T)$&#12289;$O(\sqrt{T})$&#31561;&#65289;&#65292;&#24182;&#19988;&#22312;&#21516;&#26679;&#30340;&#35266;&#23519;&#37327;&#19978;&#27604;&#22312;&#20107;&#21518;&#36873;&#25321;&#30340;&#26368;&#20248;&#23398;&#20064;&#36895;&#24230;&#39640;&#20986;2&#20493;&#12290;&#31532;&#20108;&#20010;&#24037;&#20855;&#26159;&#22312;&#32447;&#26657;&#27491;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;...
&lt;/p&gt;
&lt;p&gt;
We extend and combine several tools of the literature to design fast, adaptive, anytime and scale-free online learning algorithms. Scale-free regret bounds must scale linearly with the maximum loss, both toward large losses and toward very small losses. Adaptive regret bounds demonstrate that an algorithm can take advantage of easy data and potentially have constant regret. We seek to develop fast algorithms that depend on as few parameters as possible, in particular they should be anytime and thus not depend on the time horizon. Our first and main tool, isotuning, is a generalization of the idea of balancing the trade-off of the regret. We develop a set of tools to design and analyze such learning rates easily and show that they adapts automatically to the rate of the regret (whether constant, $O(\log T)$, $O(\sqrt{T})$, etc.) within a factor 2 of the optimal learning rate in hindsight for the same observed quantities. The second tool is an online correction, which allows us to obtain
&lt;/p&gt;</description></item></channel></rss>