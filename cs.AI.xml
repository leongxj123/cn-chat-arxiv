<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01586</link><description>&lt;p&gt;
TrustAgent: &#36890;&#36807;&#20195;&#29702;&#26500;&#25104;&#23454;&#29616;&#23433;&#20840;&#21487;&#20449;&#36182;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#21487;&#20449;&#24230;&#20173;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#30001;&#20110;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#65292;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#23545;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#32500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#31181;&#31574;&#30053;&#65306;&#39044;&#20808;&#35268;&#21010;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#20043;&#21069;&#21521;&#27169;&#22411;&#27880;&#20837;&#23433;&#20840;&#30693;&#35782;&#65307;&#35268;&#21010;&#36807;&#31243;&#20013;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#26102;&#22686;&#24378;&#23433;&#20840;&#24615;&#65307;&#35745;&#21010;&#21518;&#26816;&#26597;&#31574;&#30053;&#65292;&#36890;&#36807;&#35745;&#21010;&#21518;&#26816;&#26597;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#26377;&#25928;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20854;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.01365</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM
&lt;/p&gt;
&lt;p&gt;
Prompt-prompted Mixture of Experts for Efficient LLM Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;transformer&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#23454;&#29992;&#24615;&#65292;&#23427;&#20204;&#24050;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#19968;&#20123;&#26041;&#27861;&#65292;&#22914;&#20462;&#21098;&#25110;&#26500;&#24314;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#65292;&#26088;&#22312;&#21033;&#29992;transformer&#21069;&#39304;&#65288;FF&#65289;&#22359;&#20013;&#30340;&#31232;&#30095;&#24615;&#65292;&#20197;&#25552;&#39640;&#36895;&#24230;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#21644;&#19981;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#25110;&#20165;&#38480;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GRIFFIN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;MoE&#65292;&#23427;&#22312;&#24207;&#21015;&#32423;&#21035;&#20026;&#19981;&#21516;&#38750;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#22823;&#37327;LLMs&#36873;&#25321;&#29420;&#29305;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;&#36825;&#26159;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#20851;&#38190;&#35266;&#23519;&#21040;&#65292;&#35768;&#22810;&#32463;&#36807;&#35757;&#32451;&#30340;LLMs&#22312;&#24207;&#21015;&#20013;&#33258;&#28982;&#20135;&#29983;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;FF&#28608;&#27963;&#27169;&#24335;&#65292;&#36825;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01365v1 Announce Type: cross  Abstract: With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16950</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#25104;&#23545;&#20559;&#22909;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16950
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#22312;&#35780;&#20272;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35780;&#20272;&#20013;&#20173;&#23384;&#22312;&#20559;&#35265;&#65292;&#24120;&#24120;&#38590;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#35780;&#20272;&#19968;&#33268;&#30340;&#36830;&#36143;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;LLM&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#29616;&#26377;&#26088;&#22312;&#20943;&#36731;&#20559;&#35265;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#26377;&#25928;&#23558;LLM&#35780;&#20272;&#22120;&#23545;&#40784;&#12290;&#21463;&#21040;RLHF&#20013;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#20351;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;Pairwise-preference Search&#65288;PAIRS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;LLMs&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#24182;&#26377;&#25928;&#23545;&#20505;&#36873;&#25991;&#26412;&#36827;&#34892;&#25490;&#24207;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;PAIRS&#22312;&#20195;&#34920;&#24615;&#35780;&#20272;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#27604;&#30452;&#25509;&#25171;&#20998;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#20041;&#21521;&#37327;&#30340;&#33041;&#25509;&#22320;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.15176</link><description>&lt;p&gt;
&#35821;&#20041;&#21521;&#37327;&#30340;&#33041;&#25509;&#22320;&#25913;&#21892;&#20102;&#31070;&#32463;&#35299;&#30721;&#35270;&#35273;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
Brain-grounding of semantic vectors improves neural decoding of visual stimuli
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#20041;&#21521;&#37327;&#30340;&#33041;&#25509;&#22320;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#20934;&#30830;&#20840;&#38754;&#30340;&#31639;&#27861;&#26469;&#35299;&#30721;&#22823;&#33041;&#20869;&#23481;&#26159;&#31070;&#32463;&#31185;&#23398;&#21644;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#22823;&#33041;&#27963;&#21160;&#27169;&#24335;&#26144;&#23556;&#21040;&#19968;&#20010;&#35821;&#20041;&#21521;&#37327;&#34920;&#31034;&#30340;&#31070;&#32463;&#35299;&#30721;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#20041;&#21521;&#37327;&#30340;&#33041;&#25509;&#22320;&#65292;&#23427;&#23545;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15176v1 Announce Type: cross  Abstract: Developing algorithms for accurate and comprehensive neural decoding of mental contents is one of the long-cherished goals in the field of neuroscience and brain-machine interfaces. Previous studies have demonstrated the feasibility of neural decoding by training machine learning models to map brain activity patterns into a semantic vector representation of stimuli. These vectors, hereafter referred as pretrained feature vectors, are usually derived from semantic spaces based solely on image and/or text features and therefore they might have a totally different characteristics than how visual stimuli is represented in the human brain, resulting in limiting the capability of brain decoders to learn this mapping. To address this issue, we propose a representation learning framework, termed brain-grounding of semantic vectors, which fine-tunes pretrained feature vectors to better align with the neural representation of visual stimuli in t
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>Ansible Lightspeed&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;&#65292;&#19987;&#27880;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;Ansible&#20195;&#30721;&#65292;&#20026;IT&#33258;&#21160;&#21270;&#39046;&#22495;&#24102;&#26469;&#20102;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.17442</link><description>&lt;p&gt;
Ansible Lightspeed: &#19968;&#31181;&#29992;&#20110;IT&#33258;&#21160;&#21270;&#30340;&#20195;&#30721;&#29983;&#25104;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Ansible Lightspeed: A Code Generation Service for IT Automation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17442
&lt;/p&gt;
&lt;p&gt;
Ansible Lightspeed&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;&#65292;&#19987;&#27880;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;Ansible&#20195;&#30721;&#65292;&#20026;IT&#33258;&#21160;&#21270;&#39046;&#22495;&#24102;&#26469;&#20102;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#19990;&#20351;&#24471;&#21019;&#24314;&#21487;&#25552;&#39640;&#24320;&#21457;&#32773;&#29983;&#20135;&#21147;&#30340;&#24037;&#20855;&#25104;&#20026;&#21487;&#33021;&#65292;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDEs&#65289;&#24120;&#34987;&#29992;&#20316;&#19982;LLMs&#20132;&#20114;&#30340;&#25509;&#21475;&#12290;&#24050;&#21457;&#24067;&#35768;&#22810;&#36825;&#31867;&#24037;&#20855;&#65292;&#20294;&#20960;&#20046;&#20840;&#37096;&#37117;&#19987;&#27880;&#20110;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#65292;&#24456;&#23569;&#20851;&#27880;&#23545;IT&#33258;&#21160;&#21270;&#33267;&#20851;&#37325;&#35201;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#12290;Ansible&#26159;&#19968;&#31181;&#22522;&#20110;YAML&#30340;IT&#33258;&#21160;&#21270;&#29305;&#23450;&#35821;&#35328;&#12290;Red Hat Ansible Lightspeed&#19982;IBM Watson Code Assistant&#21512;&#20316;&#30340;Ansible Lightspeed&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26381;&#21153;&#65292;&#19987;&#38376;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;Ansible&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17442v1 Announce Type: cross  Abstract: The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for IT automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Red Hat Ansible Lightspeed with IBM Watson Code Assistant, further referred to as Ansible Lightspeed, is an LLM-based service designed explicitly for natural language to Ansible code generation.   In this paper, we describe the design and implementation of the Ansible Lightspeed service and analyze feedback from thousands of real users. We examine diverse performance indicators, clas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31639;&#27861;&#20934;&#30830;&#24615;&#20316;&#20026;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#37096;&#32626;&#22312;&#32447;RL&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#24378;&#35843;&#20102;&#23545;&#21442;&#19982;&#32773;&#20445;&#25252;&#21644;&#25968;&#25454;&#31185;&#23398;&#25928;&#29992;&#30340;&#20445;&#30041;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#36827;&#34892;&#39044;&#37096;&#32626;&#35268;&#21010;&#21644;&#23454;&#26102;&#30417;&#27979;&#20197;&#30830;&#20445;&#31639;&#27861;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17003</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#30417;&#27979;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17003
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31639;&#27861;&#20934;&#30830;&#24615;&#20316;&#20026;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#37096;&#32626;&#22312;&#32447;RL&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#24378;&#35843;&#20102;&#23545;&#21442;&#19982;&#32773;&#20445;&#25252;&#21644;&#25968;&#25454;&#31185;&#23398;&#25928;&#29992;&#30340;&#20445;&#30041;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#36827;&#34892;&#39044;&#37096;&#32626;&#35268;&#21010;&#21644;&#23454;&#26102;&#30417;&#27979;&#20197;&#30830;&#20445;&#31639;&#27861;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#20026;&#20010;&#24615;&#21270;&#20020;&#24202;&#35797;&#39564;&#20013;&#21442;&#19982;&#32773;&#30340;&#27835;&#30103;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#21307;&#30103;&#39046;&#22495;&#37096;&#32626;&#22312;&#32447;&#33258;&#20027;&#31639;&#27861;&#20351;&#24471;&#36136;&#37327;&#25511;&#21046;&#21644;&#25968;&#25454;&#36136;&#37327;&#29305;&#21035;&#38590;&#20197;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20316;&#20026;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#37096;&#32626;&#22312;&#32447;RL&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#30340;&#31639;&#27861;&#20934;&#30830;&#24615;&#12290;&#23427;&#24378;&#35843;&#20102;&#31639;&#27861;&#23545;&#65288;1&#65289;&#20445;&#25252;&#21442;&#19982;&#32773;&#21644;&#65288;2&#65289;&#20445;&#30041;&#25968;&#25454;&#22312;&#35797;&#39564;&#21518;&#20998;&#26512;&#20013;&#30340;&#31185;&#23398;&#25928;&#29992;&#30340;&#36131;&#20219;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#37096;&#32626;&#21069;&#35268;&#21010;&#21644;&#23454;&#26102;&#30417;&#27979;&#30340;&#26694;&#26550;&#65292;&#20197;&#21327;&#21161;&#31639;&#27861;&#24320;&#21457;&#32773;&#21644;&#20020;&#24202;&#30740;&#31350;&#20154;&#21592;&#30830;&#20445;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#26694;&#26550;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26469;&#33258;Oralytics&#20020;&#24202;&#35797;&#39564;&#30340;&#30495;&#23454;&#26696;&#20363;&#12290;&#33258;2023&#24180;&#26149;&#23395;&#20197;&#26469;&#65292;&#36825;&#39033;&#35797;&#39564;&#25104;&#21151;&#22320;&#37096;&#32626;&#20102;&#19968;&#31181;&#33258;&#20027;&#30340;&#22312;&#32447;RL&#31639;&#27861;&#26469;&#36827;&#34892;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17003v1 Announce Type: cross  Abstract: Online reinforcement learning (RL) algorithms offer great potential for personalizing treatment for participants in clinical trials. However, deploying an online, autonomous algorithm in the high-stakes healthcare setting makes quality control and data quality especially difficult to achieve. This paper proposes algorithm fidelity as a critical requirement for deploying online RL algorithms in clinical trials. It emphasizes the responsibility of the algorithm to (1) safeguard participants and (2) preserve the scientific utility of the data for post-trial analyses. We also present a framework for pre-deployment planning and real-time monitoring to help algorithm developers and clinical researchers ensure algorithm fidelity. To illustrate our framework's practical application, we present real-world examples from the Oralytics clinical trial. Since Spring 2023, this trial successfully deployed an autonomous, online RL algorithm to persona
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#20043;&#38388;&#24046;&#24322;&#65292;&#33021;&#21160;&#24577;&#27979;&#37327;&#28040;&#24687;&#37325;&#35201;&#24615;&#24182;&#25429;&#25417;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#30340;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.12954</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#30340;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Conditional Logical Message Passing Transformer for Complex Query Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12954
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#20043;&#38388;&#24046;&#24322;&#65292;&#33021;&#21160;&#24577;&#27979;&#37327;&#28040;&#24687;&#37325;&#35201;&#24615;&#24182;&#25429;&#25417;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#30340;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#19978;&#30340;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#65288;CQA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;KGs&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#27169;&#22411;&#26469;&#36890;&#36807;&#25191;&#34892;&#22810;&#36339;&#36923;&#36753;&#25512;&#29702;&#26469;&#35299;&#20915;CQA&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#19981;&#33021;&#21516;&#26102;&#22312;&#19968;&#36339;&#21644;&#22810;&#36339;&#26597;&#35810;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#30340;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#12290;&#34429;&#28982;&#22312;&#19968;&#36339;&#21644;&#22810;&#36339;&#26597;&#35810;&#19978;&#37117;&#26377;&#25928;&#65292;&#20294;&#23427;&#24573;&#30053;&#20102;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#33410;&#28857;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#22312;&#33410;&#28857;&#23884;&#20837;&#26356;&#26032;&#38454;&#27573;&#65292;&#35813;&#26426;&#21046;&#19981;&#33021;&#21160;&#24577;&#34913;&#37327;&#19981;&#21516;&#28040;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#23427;&#33021;&#21542;&#25429;&#25417;&#19982;&#33410;&#28857;&#21644;&#25509;&#25910;&#28040;&#24687;&#30456;&#20851;&#30340;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;&#65288;CLMPT&#65289;&#65292;&#32771;&#34385;&#20102;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#19988;&#20855;&#26377;&#21160;&#24577;&#27979;&#37327;&#19981;&#21516;&#28040;&#24687;&#37325;&#35201;&#24615;&#20197;&#21450;&#25429;&#25417;&#19982;&#33410;&#28857;&#21644;&#25509;&#25910;&#28040;&#24687;&#30456;&#20851;&#30340;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12954v1 Announce Type: cross  Abstract: Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task. Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning. However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously. Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph. In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear. In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiTimelyGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#31070;&#32463;&#21151;&#33021;&#39044;&#27979;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.09558</link><description>&lt;p&gt;
&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#21452;&#21521;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Generative Pre-training for Improving Time Series Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09558
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiTimelyGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#31070;&#32463;&#21151;&#33021;&#39044;&#27979;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20197;&#29992;&#20110;&#21028;&#21035;&#20219;&#21153;&#19968;&#30452;&#26159;&#19968;&#39033;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#35201;&#20040;&#26159;&#21333;&#21521;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#35201;&#20040;&#26159;&#38543;&#26426;&#23631;&#34109;&#26631;&#35760;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#21452;&#21521;&#21450;&#26102;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65288;BiTimelyGPT&#65289;&#65292;&#23427;&#36890;&#36807;&#20132;&#26367;&#30340;Transformer&#23618;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#19979;&#19968;&#20010;&#26631;&#35760;&#21644;&#19978;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#20445;&#30041;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#21407;&#22987;&#20998;&#24067;&#21644;&#25968;&#25454;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#20840;&#31209;&#21069;&#21521;&#21644;&#21518;&#21521;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290; &#20351;&#29992;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#65292;BiTimelyGPT&#22312;&#39044;&#27979;&#31070;&#32463;&#21151;&#33021;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#27880;&#24847;&#21147;&#28909;&#22270;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#30340;BiTimelyGPT&#33021;&#22815;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#21028;&#21035;&#24615;&#30340;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09558v1 Announce Type: new  Abstract: Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09391</link><description>&lt;p&gt;
LlaSMol:&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#25512;&#36827;&#21270;&#23398;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#22312;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#29616;&#26377;&#24037;&#20316;&#34920;&#26126;&#23427;&#20204;&#22312;&#21270;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20196;&#20154;&#22833;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;LLM&#22312;&#19968;&#31995;&#21015;&#21270;&#23398;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GPT-4&#65292;&#24182;&#25509;&#36817;SoTA&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#26159;&#19968;&#20010;&#21517;&#20026;SMolInstruct&#30340;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;&#20102;14&#20010;&#32463;&#36807;&#31934;&#24515;&#25361;&#36873;&#30340;&#21270;&#23398;&#20219;&#21153;&#21644;&#36229;&#36807;&#19977;&#30334;&#19975;&#20010;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#21270;&#23398;LLM&#22880;&#23450;&#20102;&#22362;&#23454;&#22522;&#30784;&#12290;&#22522;&#20110;SMolInstruct&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#24320;&#28304;LLM&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20854;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Mistral ser&#26159;&#26368;&#20339;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09391v1 Announce Type: new Abstract: Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral ser
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05346</link><description>&lt;p&gt;
KIX: &#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIX: A Metacognitive Generalization Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05346
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#20854;&#20182;&#21160;&#29289;&#33021;&#22815;&#28789;&#27963;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21644;&#24212;&#29992;&#38271;&#26399;&#31215;&#32047;&#30340;&#39640;&#32423;&#30693;&#35782;&#26469;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;&#65292;&#36825;&#34920;&#29616;&#20102;&#19968;&#31181;&#27867;&#21270;&#26234;&#33021;&#34892;&#20026;&#12290;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26356;&#22810;&#22320;&#26159;&#19987;&#23478;&#65292;&#32570;&#20047;&#36825;&#31181;&#36890;&#29992;&#34892;&#20026;&#12290;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#38656;&#35201;&#29702;&#35299;&#21644;&#21033;&#29992;&#20851;&#38190;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;Knowledge-Interaction-eXecution (KIX)&#65292;&#24182;&#19988;&#35748;&#20026;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#26469;&#21033;&#29992;&#31867;&#22411;&#31354;&#38388;&#21487;&#20197;&#20419;&#36827;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#26159;&#23558;&#30693;&#35782;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#26377;&#26395;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#30340;&#25512;&#24191;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time. But artificial agents are more of a specialist, lacking such generalist behaviors. Artificial agents will require understanding and exploiting critical structured knowledge representations. We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization. It is a natural way of integrating knowledge into reinforcement learning and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#23558;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#34892;&#20026;&#25209;&#35780;&#32773;&#29992;&#20110;&#25429;&#25417;&#19981;&#33391;&#20195;&#29702;&#34892;&#20026;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#24573;&#35270;&#20219;&#21153;&#32422;&#26463;&#21644;&#29992;&#25143;&#20559;&#22909;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04210</link><description>&lt;p&gt;
&#8220;&#20219;&#21153;&#25104;&#21151;&#8221;&#36828;&#36828;&#19981;&#22815;&#65306;&#25506;&#31350;&#23558;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#34892;&#20026;&#25209;&#35780;&#32773;&#20197;&#25429;&#25417;&#19981;&#33391;&#20195;&#29702;&#34892;&#20026;&#30340;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
"Task Success" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#23558;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#34892;&#20026;&#25209;&#35780;&#32773;&#29992;&#20110;&#25429;&#25417;&#19981;&#33391;&#20195;&#29702;&#34892;&#20026;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#24573;&#35270;&#20219;&#21153;&#32422;&#26463;&#21644;&#29992;&#25143;&#20559;&#22909;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#34987;&#35777;&#26126;&#23545;&#20110;&#25277;&#26679;&#26377;&#24847;&#20041;&#30340;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#24456;&#26377;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20219;&#21153;&#32422;&#26463;&#21644;&#29992;&#25143;&#20559;&#22909;&#12290;&#24403;&#27169;&#22411;&#19982;&#22806;&#37096;&#39564;&#35777;&#32773;&#32467;&#21512;&#65292;&#24182;&#26681;&#25454;&#39564;&#35777;&#21453;&#39304;&#36880;&#27493;&#25110;&#36880;&#28176;&#24471;&#20986;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#23427;&#20204;&#30340;&#23436;&#20840;&#33021;&#21147;&#26356;&#22909;&#22320;&#34987;&#21033;&#29992;&#12290;&#22312;&#20855;&#36523;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#32972;&#26223;&#19979;&#65292;&#39564;&#35777;&#36890;&#24120;&#20165;&#28041;&#21450;&#35780;&#20272;&#25351;&#20196;&#20013;&#25351;&#23450;&#30340;&#30446;&#26631;&#26465;&#20214;&#26159;&#21542;&#24050;&#28385;&#36275;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;&#36825;&#20123;&#20195;&#29702;&#32773;&#26080;&#32541;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#24517;&#39035;&#32771;&#34385;&#21040;&#26356;&#24191;&#27867;&#30340;&#32422;&#26463;&#21644;&#20559;&#22909;&#65292;&#36229;&#36234;&#20165;&#20219;&#21153;&#25104;&#21151;&#65288;&#20363;&#22914;&#65292;&#26426;&#22120;&#20154;&#24212;&#35813;&#35880;&#24910;&#22320;&#25235;&#20303;&#38754;&#21253;&#65292;&#20197;&#36991;&#20813;&#26126;&#26174;&#30340;&#21464;&#24418;&#65289;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26080;&#38480;&#33539;&#22260;&#65292;&#26500;&#24314;&#31867;&#20284;&#20110;&#29992;&#20110;&#26174;&#24335;&#30693;&#35782;&#20219;&#21153;&#65288;&#22914;&#22260;&#26827;&#21644;&#23450;&#29702;&#35777;&#26126;&#65289;&#30340;&#33050;&#26412;&#21270;&#39564;&#35777;&#22120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#27809;&#26377;&#21487;&#38752;&#30340;&#39564;&#35777;&#32773;&#21487;&#29992;&#26102;&#65292;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#20195;&#29702;&#30340;&#34892;&#20026;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is avail
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#36739;&#20302;&#30340;&#36951;&#25022;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03110</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Latent Auto-Regressive Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#36739;&#20302;&#30340;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#38750;&#24179;&#31283;&#22870;&#21169;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#30340;&#20844;&#24335;&#65292;&#20854;&#20013;&#33218;&#30340;&#24179;&#22343;&#22870;&#21169;&#38543;&#26102;&#38388;&#21464;&#21270;&#26159;&#30001;&#19968;&#20123;&#26410;&#30693;&#30340;&#28508;&#22312;&#33258;&#22238;&#24402;(AR)&#29366;&#24577;&#30340;&#39034;&#24207;k&#20915;&#23450;&#30340;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#30340;&#29615;&#22659;&#31216;&#20026;&#28508;&#22312;AR&#36172;&#21338;&#26426;&#12290;&#28508;&#22312;AR&#36172;&#21338;&#26426;&#30340;&#19981;&#21516;&#24418;&#24335;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#37117;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#34892;&#20026;&#20581;&#24247;&#25110;&#25945;&#32946;&#31561;&#26032;&#20852;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#36825;&#37324;&#32570;&#20047;&#23545;&#29615;&#22659;&#30340;&#26426;&#21046;&#24314;&#27169;&#12290;&#22914;&#26524;AR&#39034;&#24207;k&#24050;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#34920;&#29616;&#20986;O(k&#8730;T)&#30340;&#36951;&#25022;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;k&#34987;&#38169;&#35823;&#22320;&#20272;&#35745;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20063;&#32988;&#36807;&#26631;&#20934;&#30340;UCB&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the stochastic multi-armed bandit problem with non-stationary rewards. We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$. We call this new environment the latent AR bandit. Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment. If the AR order $k$ is known, we propose an algorithm that achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACE&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#65292;&#24182;&#24341;&#20837;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#26234;&#33021;&#20307;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02097</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#39062;&#24615;&#20849;&#20139;&#35299;&#20915;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACE&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#65292;&#24182;&#24341;&#20837;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#26234;&#33021;&#20307;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#21327;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#26159;&#20840;&#23616;&#29366;&#24577;&#30340;&#26032;&#39062;&#24615;&#19981;&#21487;&#29992;&#65292;&#32780;&#23616;&#37096;&#35266;&#23519;&#30340;&#26032;&#39062;&#24615;&#23384;&#22312;&#20559;&#24046;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#26234;&#33021;&#20307;&#22914;&#20309;&#21327;&#35843;&#22320;&#36827;&#34892;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;MACE&#12290;&#36890;&#36807;&#20165;&#20256;&#25773;&#23616;&#37096;&#26032;&#39062;&#24615;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#32771;&#34385;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#32047;&#35745;&#26032;&#39062;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#20869;&#22312;&#22238;&#25253;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#25506;&#32034;&#20135;&#29983;&#26356;&#22823;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#19977;&#31181;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration in decentralized cooperative multi-agent reinforcement learning faces two challenges. One is that the novelty of global states is unavailable, while the novelty of local observations is biased. The other is how agents can explore in a coordinated way. To address these challenges, we propose MACE, a simple yet effective multi-agent coordinated exploration method. By communicating only local novelty, agents can take into account other agents' local novelty to approximate the global novelty. Further, we newly introduce weighted mutual information to measure the influence of one agent's action on other agents' accumulated novelty. We convert it as an intrinsic reward in hindsight to encourage agents to exert more influence on other agents' exploration and boost coordinated exploration. Empirically, we show that MACE achieves superior performance in three multi-agent environments with sparse rewards.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00798</link><description>&lt;p&gt;
&#27491;&#24335;-LLM&#65306;&#23558;&#24418;&#24335;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#38598;&#25104;&#20110;&#21487;&#25511;&#30340;LLM&#26234;&#33021;&#20307;&#20013;
&lt;/p&gt;
&lt;p&gt;
Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#22810;&#27493;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#20869;&#23481;&#29983;&#25104;&#36807;&#31243;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#65292;&#24403;&#21069;&#30340;LLM&#26234;&#33021;&#20307;&#32463;&#24120;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#36825;&#25439;&#23475;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#24615;&#33021;&#24182;&#30772;&#22351;&#20102;&#29992;&#25143;&#23545;LLM&#26234;&#33021;&#20307;&#30340;&#20449;&#20219;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;LLM&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#31934;&#30830;&#24615;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#23558;&#20182;&#20204;&#23545;&#35745;&#21010;&#36807;&#31243;&#30340;&#35201;&#27714;&#25110;&#32422;&#26463;&#34920;&#36798;&#20026;&#33258;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#22312;&#33258;&#21160;&#26426;&#30340;&#30417;&#30563;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#22534;&#26632;&#30340;LLM&#35745;&#21010;&#29983;&#25104;&#36807;&#31243;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#35745;&#21010;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#20351;&#35745;&#21010;&#36807;&#31243;&#21487;&#25511;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#20219;&#21153;&#21644;&#23454;&#38469;&#30340;&#30495;&#23454;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19988;obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.
&lt;/p&gt;
&lt;p&gt;
Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;&#26032;&#30340;&#35299;&#37322;&#25209;&#35780;&#20219;&#21153;&#12289;&#21019;&#24314;&#20154;&#24037;&#39564;&#35777;&#36807;&#30340;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#24320;&#28304;&#33258;&#21160;&#25209;&#35780;&#27169;&#22411;&#65292;&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#26377;&#21161;&#20110;&#25581;&#31034;&#23398;&#29983;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2311.09613</link><description>&lt;p&gt;
&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#65306;&#36890;&#36807;&#35299;&#37322;&#25209;&#35780;&#35780;&#20272;LLM
&lt;/p&gt;
&lt;p&gt;
Digital Socrates: Evaluating LLMs through Explanation Critiques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09613
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#26032;&#30340;&#35299;&#37322;&#25209;&#35780;&#20219;&#21153;&#12289;&#21019;&#24314;&#20154;&#24037;&#39564;&#35777;&#36807;&#30340;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#24320;&#28304;&#33258;&#21160;&#25209;&#35780;&#27169;&#22411;&#65292;&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#26377;&#21161;&#20110;&#25581;&#31034;&#23398;&#29983;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;LLMs&#21487;&#20197;&#25552;&#20379;&#26377;&#29702;&#26377;&#25454;&#30340;&#35299;&#37322;&#20197;&#21450;&#31572;&#26696;&#65292;&#20294;&#36825;&#20123;&#35299;&#37322;&#30340;&#24615;&#36136;&#21644;&#36136;&#37327;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23450;&#20041;&#19968;&#31181;&#35814;&#32454;&#30340;&#26041;&#24335;&#26469;&#34920;&#24449;&#29616;&#20195;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#21019;&#24314;&#19968;&#20010;&#32454;&#33268;&#19988;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#35780;&#20272;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#34920;&#24449;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#26114;&#36149;&#30340;API&#35843;&#29992;&#25110;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#65306;(a)&#23450;&#20041;&#35299;&#37322;&#25209;&#35780;&#30340;&#26032;&#20219;&#21153;&#8212;&#8212;&#35782;&#21035;&#21644;&#20998;&#31867;&#35299;&#37322;&#20013;&#30340;&#20219;&#20309;&#20027;&#35201;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#24314;&#35758;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65307;(b)&#20026;&#27492;&#20219;&#21153;&#21019;&#24314;&#19968;&#20010;&#35268;&#27169;&#21487;&#35266;&#19988;&#32463;&#36807;&#20154;&#24037;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65307;(c)&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#24320;&#28304;&#30340;&#33258;&#21160;&#25209;&#35780;&#27169;&#22411;&#65288;&#31216;&#20026;&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#65289;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#22914;&#20309;&#26377;&#21161;&#20110;&#36890;&#36807;&#26816;&#26597;&#20854;&#29702;&#30001;&#26469;&#25581;&#31034;&#26377;&#20851;&#23398;&#29983;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09613v2 Announce Type: replace-cross  Abstract: While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26041;&#24335;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#25512;&#24191;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#24615;&#36136;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.16808</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#26469;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Encoding Temporal Statistical-space Priors via Augmented Representation. (arXiv:2401.16808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16808
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26041;&#24335;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#25512;&#24191;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#24615;&#36136;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20173;&#28982;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#26102;&#38388;&#32500;&#24230;&#19982;&#35768;&#22810;&#39046;&#22495;&#23494;&#20999;&#30456;&#20851;&#12290;&#23613;&#31649;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#39640;&#22122;&#22768;&#20449;&#21495;&#27604;&#12289;&#38750;&#27491;&#24577;&#24615;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#25968;&#25454;&#32570;&#20047;&#20173;&#28982;&#26159;&#25361;&#25112;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#22686;&#24378;&#25216;&#26415;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22686;&#24378;&#34920;&#31034;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#20316;&#20026;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#36827;&#34892;&#32534;&#30721;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;&#32479;&#35745;&#31354;&#38388;&#22686;&#24378;&#34920;&#31034;&#65288;SSAR&#65289;&#12290;&#22522;&#20110;&#39640;&#32500;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#21551;&#21457;&#20102;&#25105;&#20204;&#30340;&#34920;&#31034;&#22686;&#24378;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#20010;&#19979;&#28216;&#26102;&#38388;&#23398;&#20064;&#31639;&#27861;&#30340;&#32463;&#39564;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#26816;&#26597;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20987;&#36133;&#20102;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#30340;&#24615;&#36136;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling time series data remains a pervasive issue as the temporal dimension is inherent to numerous domains. Despite significant strides in time series forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and lack of data continue challenging practitioners. In response, we leverage a simple representation augmentation technique to overcome these challenges. Our augmented representation acts as a statistical-space prior encoded at each time step. In response, we name our method Statistical-space Augmented Representation (SSAR). The underlying high-dimensional data-generating process inspires our representation augmentation. We rigorously examine the empirical generalization performance on two data sets with two downstream temporal learning algorithms. Our approach significantly beats all five up-to-date baselines. Moreover, the highly modular nature of our approach can easily be applied to various settings. Lastly, fully-fledged theoretical perspectives are availa
&lt;/p&gt;</description></item><item><title>FedRSU&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#23427;&#36890;&#36807;&#25972;&#21512;RSU&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#24490;&#29615;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12862</link><description>&lt;p&gt;
FedRSU: &#22522;&#20110;RSU&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#22330;&#26223;&#27969;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units. (arXiv:2401.12862v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12862
&lt;/p&gt;
&lt;p&gt;
FedRSU&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#23427;&#36890;&#36807;&#25972;&#21512;RSU&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#24490;&#29615;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RSU&#65288;&#36335;&#36793;&#21333;&#20803;&#65289;&#36890;&#36807;&#36710;&#32852;&#32593;&#65288;V2X&#65289;&#36890;&#20449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#30446;&#21069;&#65292;&#21333;&#20010;RSU&#30340;&#20351;&#29992;&#20027;&#35201;&#38598;&#20013;&#22312;&#23454;&#26102;&#25512;&#26029;&#21644;V2X&#21327;&#20316;&#19978;&#65292;&#32780;&#24573;&#35270;&#20102;RSU&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#28508;&#22312;&#20215;&#20540;&#12290;&#25972;&#21512;&#22823;&#37327;&#26469;&#33258;&#22810;&#20010;RSU&#30340;&#25968;&#25454;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20016;&#23500;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#21644;&#20256;&#36755;&#24222;&#22823;&#25968;&#25454;&#37327;&#30340;&#22256;&#38590;&#26159;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#38544;&#34255;&#20215;&#20540;&#30340;&#20004;&#20010;&#19981;&#21487;&#36991;&#20813;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FedRSU&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#30417;&#30563;&#22330;&#26223;&#27969;&#20272;&#35745;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;FedRSU&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#23545;&#20110;&#27599;&#20010;RSU&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#21518;&#32493;&#26410;&#26469;&#30340;&#22810;&#27169;&#24577;&#35266;&#27979;&#23545;&#27599;&#20010;&#26102;&#38388;&#25139;&#30340;&#28857;&#30340;&#22330;&#26223;&#27969;&#39044;&#27979;&#36827;&#34892;&#30417;&#30563;&#12290;FedRSU&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#32852;&#37030;
&lt;/p&gt;
&lt;p&gt;
Roadside unit (RSU) can significantly improve the safety and robustness of autonomous vehicles through Vehicle-to-Everything (V2X) communication. Currently, the usage of a single RSU mainly focuses on real-time inference and V2X collaboration, while neglecting the potential value of the high-quality data collected by RSU sensors. Integrating the vast amounts of data from numerous RSUs can provide a rich source of data for model training. However, the absence of ground truth annotations and the difficulty of transmitting enormous volumes of data are two inevitable barriers to fully exploiting this hidden value. In this paper, we introduce FedRSU, an innovative federated learning framework for self-supervised scene flow estimation. In FedRSU, we present a recurrent self-supervision training paradigm, where for each RSU, the scene flow prediction of points at every timestamp can be supervised by its subsequent future multi-modality observation. Another key component of FedRSU is federated
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#20174;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36807;&#28193;&#21040;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#24182;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33021;&#21147;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2311.01043</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models for Autonomous Driving. (arXiv:2311.01043v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01043
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#20174;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36807;&#28193;&#21040;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#24182;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33021;&#21147;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#20316;&#20026;&#25913;&#21464;&#20132;&#36890;&#21644;&#22478;&#24066;&#27969;&#21160;&#24615;&#30340;&#20652;&#21270;&#21058;&#65292;&#27491;&#36235;&#21521;&#20110;&#20174;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36716;&#21521;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#27169;&#22359;&#21270;&#31995;&#32479;&#21463;&#21040;&#32423;&#32852;&#27169;&#22359;&#20013;&#30340;&#32047;&#31215;&#35823;&#24046;&#21644;&#19981;&#28789;&#27963;&#30340;&#39044;&#35774;&#35268;&#21017;&#30340;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#36890;&#36807;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#26377;&#28508;&#21147;&#36991;&#20813;&#38169;&#35823;&#32047;&#31215;&#65292;&#23613;&#31649;&#30001;&#20110;&#20854;&#40657;&#30418;&#24615;&#36136;&#65292;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#20351;&#24471;&#20915;&#31574;&#30340;&#39564;&#35777;&#21644;&#21487;&#36861;&#28335;&#24615;&#21464;&#24471;&#22797;&#26434;&#12290;&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#29702;&#35299;&#32972;&#26223;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#31561;&#33021;&#21147;&#12290;&#33258;&#28982;&#32780;&#28982;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#36171;&#20104;&#33258;&#21160;&#39550;&#39542;&#20197;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;LLM&#19982;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#32467;&#21512;&#65292;&#21487;&#33021;&#25171;&#24320;&#23545;&#24320;&#25918;&#19990;&#30028;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22823;&#38376;&#65292;&#36825;&#26159;&#24403;&#21069;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#25152;&#32570;&#20047;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their ``black box" nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper,
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#24847;&#26102;&#38388;&#25552;&#28860;&#26041;&#27861;&#65292;&#32467;&#21512;&#24658;&#23450;&#26102;&#38388;&#21160;&#20316;&#35268;&#21010;&#22120;&#65288;CTMP&#65289;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#25913;&#21892;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#20154;&#31995;&#32479;&#25317;&#26377;&#30340;&#22810;&#20313;&#35745;&#21010;&#26102;&#38388;&#26469;&#23436;&#21892;&#36816;&#21160;&#35268;&#21010;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00837</link><description>&lt;p&gt;
&#20219;&#24847;&#26102;&#38388;&#25552;&#28860;&#30340;&#24658;&#23450;&#26102;&#38388;&#36816;&#21160;&#35268;&#21010;&#21644;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Constant-time Motion Planning with Anytime Refinement for Manipulation. (arXiv:2311.00837v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00837
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#24847;&#26102;&#38388;&#25552;&#28860;&#26041;&#27861;&#65292;&#32467;&#21512;&#24658;&#23450;&#26102;&#38388;&#21160;&#20316;&#35268;&#21010;&#22120;&#65288;CTMP&#65289;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#25913;&#21892;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#20154;&#31995;&#32479;&#25317;&#26377;&#30340;&#22810;&#20313;&#35745;&#21010;&#26102;&#38388;&#26469;&#23436;&#21892;&#36816;&#21160;&#35268;&#21010;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#20027;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20854;&#33258;&#20027;&#24615;&#30340;&#20449;&#20219;&#26377;&#38480;&#65292;&#23558;&#20854;&#38480;&#21046;&#20026;&#21018;&#24615;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#31995;&#32479;&#12290;&#25805;&#20316;&#22120;&#30340;&#22797;&#26434;&#37197;&#32622;&#31354;&#38388;&#65292;&#20197;&#21450;&#36991;&#38556;&#21644;&#32422;&#26463;&#28385;&#36275;&#30340;&#25361;&#25112;&#32463;&#24120;&#20351;&#24471;&#36816;&#21160;&#35268;&#21010;&#25104;&#20026;&#23454;&#29616;&#21487;&#38752;&#21644;&#36866;&#24212;&#24615;&#33258;&#20027;&#24615;&#30340;&#29942;&#39048;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#19968;&#31867;&#24658;&#23450;&#26102;&#38388;&#36816;&#21160;&#35268;&#21010;&#22120;&#65288;CTMP&#65289;&#12290;&#36825;&#20123;&#35268;&#21010;&#22120;&#21033;&#29992;&#39044;&#22788;&#29702;&#38454;&#27573;&#35745;&#31639;&#25968;&#25454;&#32467;&#26500;&#65292;&#33021;&#22815;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#26102;&#38388;&#38480;&#21046;&#20869;&#29983;&#25104;&#36816;&#21160;&#35268;&#21010;&#65292;&#34429;&#28982;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20248;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#35768;&#22810;&#26102;&#38388;&#20851;&#38190;&#30340;&#20219;&#21153;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#31995;&#32479;&#36890;&#24120;&#26377;&#27604;CTMP&#25152;&#38656;&#30340;&#22312;&#32447;&#37096;&#20998;&#26356;&#22810;&#30340;&#35745;&#21010;&#26102;&#38388;&#65292;&#36825;&#20123;&#26102;&#38388;&#21487;&#20197;&#29992;&#26469;&#25913;&#21892;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;CTMP&#20013;&#19982;&#20219;&#24847;&#26102;&#38388;&#25552;&#28860;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulators are essential for future autonomous systems, yet limited trust in their autonomy has confined them to rigid, task-specific systems. The intricate configuration space of manipulators, coupled with the challenges of obstacle avoidance and constraint satisfaction, often makes motion planning the bottleneck for achieving reliable and adaptable autonomy. Recently, a class of constant-time motion planners (CTMP) was introduced. These planners employ a preprocessing phase to compute data structures that enable online planning provably guarantee the ability to generate motion plans, potentially sub-optimal, within a user defined time bound. This framework has been demonstrated to be effective in a number of time-critical tasks. However, robotic systems often have more time allotted for planning than the online portion of CTMP requires, time that can be used to improve the solution. To this end, we propose an anytime refinement approach that works in combination with CTMP a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#22791;&#20102;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.07665</link><description>&lt;p&gt;
&#28145;&#24230;&#22238;&#28335;&#23545;&#22240;&#26524;&#19968;&#33268;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deep Backtracking Counterfactuals for Causally Compliant Explanations. (arXiv:2310.07665v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#22791;&#20102;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#21487;&#20197;&#36890;&#36807;&#22238;&#31572;&#22312;&#25913;&#21464;&#24773;&#20917;&#19979;&#20250;&#35266;&#23519;&#21040;&#20160;&#20040;&#26469;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#26465;&#20214;&#26159;&#26681;&#25454;&#23454;&#38469;&#35266;&#23519;&#12290;&#34429;&#28982;&#32463;&#20856;&#30340;&#20171;&#20837;&#24335;&#35299;&#37322;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22238;&#28335;&#21407;&#21017;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20445;&#25345;&#25152;&#26377;&#22240;&#26524;&#23450;&#24459;&#23436;&#25972;&#24615;&#30340;&#26367;&#20195;&#21746;&#23398;&#65292;&#20294;&#20854;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#30001;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#32452;&#25104;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#32467;&#26500;&#20998;&#37197;&#26045;&#21152;&#20102;&#26465;&#20214;&#65292;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#19968;&#20010;&#21487;&#34892;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#19982;&#21453;&#20107;&#23454;&#35299;&#37322;&#39046;&#22495;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20195;&#34920;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#36981;&#23432;&#22240;&#26524;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactuals can offer valuable insights by answering what would have been observed under altered circumstances, conditional on a factual observation. Whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative the backtracking principle has emerged as an alternative philosophy where all causal laws are kept intact. In the present work, we introduce a practical method for computing backtracking counterfactuals in structural causal models that consist of deep generative components. To this end, we impose conditions on the structural assignments that enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of a causal model. Our formulation also facilitates a comparison with methods in the field of counterfactual explanations. Compared to these, our method represents a versatile, modular and causally compliant alternative. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17012</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35748;&#30693;&#20559;&#24046;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Cognitive Biases in Large Language Models as Evaluators. (arXiv:2309.17012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#38750;&#24120;&#26377;&#25928;&#12290;&#26412;&#30740;&#31350;&#32452;&#35013;&#20102;15&#20010;&#22823;&#23567;&#19981;&#21516;&#30340;LLMs&#65292;&#24182;&#36890;&#36807;&#20854;&#20182;LLMs&#30340;&#20559;&#22909;&#25490;&#21517;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#36755;&#20986;&#21709;&#24212;&#65292;&#20363;&#22914;System Star&#27604;System Square&#26356;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#36755;&#20986;&#20013;&#20845;&#31181;&#19981;&#21516;&#35748;&#30693;&#20559;&#24046;&#30340;&#35748;&#30693;&#20559;&#24046;&#22522;&#20934;&#27979;&#35797;&#65288;CoBBLEr&#65289;&#65292;&#22914;&#33258;&#25105;&#20013;&#24515;&#20559;&#24046;&#65292;&#21363;&#27169;&#22411;&#26356;&#21916;&#27426;&#23558;&#33258;&#24049;&#30340;&#36755;&#20986;&#22312;&#35780;&#20272;&#20013;&#25490;&#21517;&#36739;&#39640;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#26159;&#26377;&#20559;&#35265;&#30340;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;&#22312;&#27599;&#20010;&#35780;&#20272;&#20013;&#37117;&#34920;&#29616;&#20986;&#23545;&#25105;&#20204;&#20559;&#35265;&#22522;&#20934;&#30340;&#24378;&#28872;&#36857;&#35937;&#65288;&#22312;&#25152;&#26377;&#27169;&#22411;&#19978;&#30340;&#24179;&#22343;&#27604;&#36739;&#32422;&#20026;40%&#65289;&#65292;&#36825;&#23545;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#35745;&#31639;&#20102;&#24179;&#22343;&#30340;Rank-Biased O&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased O
&lt;/p&gt;</description></item><item><title>CoFiI2P&#26159;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#21644;&#29305;&#24449;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.14660</link><description>&lt;p&gt;
CoFiI2P: &#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#30340;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration. (arXiv:2309.14660v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14660
&lt;/p&gt;
&lt;p&gt;
CoFiI2P&#26159;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#21644;&#29305;&#24449;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21040;&#28857;&#20113;&#65288;I2P&#65289;&#27880;&#20876;&#26159;&#26426;&#22120;&#20154;&#23548;&#33322;&#21644;&#31227;&#21160;&#24314;&#22270;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;I2P&#27880;&#20876;&#26041;&#27861;&#22312;&#28857;&#21040;&#20687;&#32032;&#32423;&#21035;&#19978;&#20272;&#35745;&#23545;&#24212;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#26469;&#33258;&#20840;&#23616;&#32422;&#26463;&#30340;&#39640;&#32423;&#24341;&#23548;&#30340;I2P&#21305;&#37197;&#23481;&#26131;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;I2P&#27880;&#20876;&#32593;&#32476;CoFiI2P&#65292;&#36890;&#36807;&#31895;&#21040;&#31934;&#30340;&#26041;&#24335;&#25552;&#21462;&#23545;&#24212;&#20851;&#31995;&#65292;&#20197;&#24471;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#39318;&#20808;&#65292;&#23558;&#22270;&#20687;&#21644;&#28857;&#20113;&#36755;&#20837;&#21040;&#19968;&#20010;&#20849;&#20139;&#32534;&#30721;-&#35299;&#30721;&#32593;&#32476;&#20013;&#36827;&#34892;&#23618;&#27425;&#21270;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31895;&#21040;&#31934;&#30340;&#21305;&#37197;&#27169;&#22359;&#65292;&#21033;&#29992;&#29305;&#24449;&#24314;&#31435;&#31283;&#20581;&#30340;&#29305;&#24449;&#23545;&#24212;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#31895;&#21305;&#37197;&#22359;&#20013;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;I2P&#21464;&#25442;&#27169;&#22359;&#65292;&#20174;&#22270;&#20687;&#21644;&#28857;&#20113;&#20013;&#25429;&#25417;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#36890;&#36807;&#21028;&#21035;&#25551;&#36848;&#23376;&#65292;&#23436;&#25104;&#31895;-&#32454;&#29305;&#24449;&#21305;&#37197;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#32454;&#21270;&#21305;&#37197;&#27169;&#22359;&#36827;&#19968;&#27493;&#25552;&#21319;&#23545;&#24212;&#20851;&#31995;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-to-point cloud (I2P) registration is a fundamental task in the fields of robot navigation and mobile mapping. Existing I2P registration works estimate correspondences at the point-to-pixel level, neglecting the global alignment. However, I2P matching without high-level guidance from global constraints may converge to the local optimum easily. To solve the problem, this paper proposes CoFiI2P, a novel I2P registration network that extracts correspondences in a coarse-to-fine manner for the global optimal solution. First, the image and point cloud are fed into a Siamese encoder-decoder network for hierarchical feature extraction. Then, a coarse-to-fine matching module is designed to exploit features and establish resilient feature correspondences. Specifically, in the coarse matching block, a novel I2P transformer module is employed to capture the homogeneous and heterogeneous global information from image and point cloud. With the discriminate descriptors, coarse super-point-to-su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03720</link><description>&lt;p&gt;
&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35268;&#21010;&#22825;&#28982;&#27668;&#20379;&#24212;&#21644;&#28040;&#36153;&#20197;&#21450;&#20248;&#21270;&#33719;&#24471;&#22825;&#28982;&#27668;&#25104;&#26412;&#26041;&#38754;&#65292;&#32771;&#34385;&#23395;&#33410;&#24615;&#21644;&#36235;&#21183;&#24615;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27493; ahead &#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#38598;&#25104;&#20102;&#21464;&#28857;&#26816;&#27979;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;Hoeffding&#26641;&#39044;&#27979;&#22120;&#20316;&#20026;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21098;&#35009;&#30340;&#31934;&#30830;&#32447;&#24615;&#26102;&#38388;&#65288;PELT&#65289;&#31639;&#27861;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#12290;&#21464;&#28857;&#26816;&#27979;&#38598;&#25104;&#20351;&#24471;&#36873;&#25321;&#19981;&#21516;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;LEGO&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#20351;&#29992;LiDAR&#21333;&#29420;&#36827;&#34892;&#36319;&#36394;&#30340;LEGO&#26041;&#27861;&#22312;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09908</link><description>&lt;p&gt;
LEGO: &#23545;&#20110;&#22522;&#20110;&#28857;&#20113;&#30340;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds. (arXiv:2308.09908v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;LEGO&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#20351;&#29992;LiDAR&#21333;&#29420;&#36827;&#34892;&#36319;&#36394;&#30340;LEGO&#26041;&#27861;&#22312;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#36319;&#36394;-&#26816;&#27979;&#26041;&#27861;&#65292;&#25968;&#25454;&#20851;&#32852;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#65288;LEGO&#65289;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;LEGO&#36319;&#36394;&#22120;&#38598;&#25104;&#20102;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21046;&#23450;&#20851;&#32852;&#35780;&#20998;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#39640;&#25928;&#30340;&#30446;&#26631;&#21305;&#37197;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#29366;&#24577;&#26356;&#26032;&#36807;&#31243;&#65292;&#26412;&#25991;&#36824;&#28155;&#21152;&#20102;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#23558;&#23545;&#35937;&#29366;&#24577;&#30340;&#26102;&#38388;&#36830;&#36143;&#24615;&#32435;&#20837;&#36319;&#36394;&#20013;&#65292;&#30830;&#20445;&#19968;&#33268;&#30340;&#36319;&#36394;&#12290;&#19982;&#20854;&#20182;&#22312;&#32447;&#36319;&#36394;&#26041;&#27861;&#65288;&#21253;&#25324;&#22522;&#20110;LiDAR&#21644;&#22522;&#20110;LiDAR-&#30456;&#26426;&#34701;&#21512;&#30340;&#26041;&#27861;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20165;&#21033;&#29992;LiDAR&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#25552;&#20132;&#32467;&#26524;&#33267;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#25490;&#34892;&#27036;&#26102;&#65292;LEGO&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online multi-object tracking (MOT) plays a pivotal role in autonomous systems. The state-of-the-art approaches usually employ a tracking-by-detection method, and data association plays a critical role. This paper proposes a learning and graph-optimized (LEGO) modular tracker to improve data association performance in the existing literature. The proposed LEGO tracker integrates graph optimization and self-attention mechanisms, which efficiently formulate the association score map, facilitating the accurate and efficient matching of objects across time frames. To further enhance the state update process, the Kalman filter is added to ensure consistent tracking by incorporating temporal coherence in the object states. Our proposed method utilizing LiDAR alone has shown exceptional performance compared to other online tracking approaches, including LiDAR-based and LiDAR-camera fusion-based methods. LEGO ranked 1st at the time of submitting results to KITTI object tracking evaluation ranki
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;(SNNs)&#20013;&#30340;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;SNNs&#22312;&#33410;&#33021;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17670</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23398;&#20064;&#38388;&#36317;&#30340;&#33192;&#32960;&#21367;&#31215;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings. (arXiv:2306.17670v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;(SNNs)&#20013;&#30340;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;SNNs&#22312;&#33410;&#33021;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;(SNNs)&#26159;&#26500;&#24314;&#33410;&#33021;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22914;&#35821;&#38899;&#35782;&#21035;&#31561;&#26102;&#38388;&#20219;&#21153;&#12290;&#22312;SNNs&#20013;&#65292;&#24310;&#36831;&#25351;&#30340;&#26159;&#20174;&#19968;&#20010;&#31070;&#32463;&#20803;&#21040;&#21478;&#19968;&#20010;&#31070;&#32463;&#20803;&#20256;&#25773;&#38656;&#35201;&#30340;&#26102;&#38388;&#12290;&#36825;&#20123;&#24310;&#36831;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#24433;&#21709;&#33033;&#20914;&#21040;&#36798;&#26102;&#38388;&#65292;&#24050;&#30693;&#23574;&#23792;&#31070;&#32463;&#20803;&#23545;&#20110;&#37325;&#21472;&#30340;&#36755;&#20837;&#33033;&#20914;&#26377;&#26356;&#24378;&#30340;&#21709;&#24212;&#12290;&#26356;&#27491;&#24335;&#22320;&#35828;&#65292;&#29702;&#35770;&#19978;&#24050;&#32463;&#35777;&#26126;&#21487;&#22609;&#24615;&#24310;&#36831;&#26497;&#22823;&#22686;&#21152;&#20102;SNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#36825;&#20123;&#24310;&#36831;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#22312;&#28145;&#24230;&#21069;&#39304;SNNs&#20013;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#27169;&#25311;&#36830;&#32493;&#23618;&#20043;&#38388;&#30340;&#24310;&#36831;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#27839;&#26102;&#38388;&#36724;&#30340;&#19968;&#32500;&#21367;&#31215;&#12290;&#21367;&#31215;&#26680;&#20165;&#21253;&#21547;&#23569;&#25968;&#38750;&#38646;&#26435;&#37325; - &#27599;&#20010;&#31361;&#35302;&#19968;&#20010; - &#23427;&#20204;&#30340;&#20301;&#32622;&#23545;&#24212;&#20110;&#24310;&#36831;&#12290;&#36825;&#20123;&#20301;&#32622;&#19982;&#26435;&#37325;&#19968;&#36215;&#34987;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In SNNs, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter because they influence the spike arrival times, and it is well-known that spiking neurons respond more strongly to coincident input spikes. More formally, it has been shown theoretically that plastic delays greatly increase the expressivity in SNNs. Yet, efficient algorithms to learn these delays have been lacking. Here, we propose a new discrete-time algorithm that addresses this issue in deep feedforward SNNs using backpropagation, in an offline manner. To simulate delays between consecutive layers, we use 1D convolutions across time. The kernels contain only a few non-zero weights - one per synapse - whose positions correspond to the delays. These positions are learned together with the wei
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#24863;&#22120;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20013;&#35821;&#20041;&#25216;&#26415;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#21457;&#29616;&#27492;&#31867;&#31995;&#32479;&#24517;&#39035;&#20811;&#26381;&#30340;&#20851;&#38190;&#25361;&#25112;&#20026;&#20114;&#25805;&#20316;&#24615;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#24773;&#22659;&#26816;&#27979;&#12289;&#24773;&#22659;&#39044;&#27979;&#12289;&#20915;&#31574;&#25903;&#25345;&#21644;&#30693;&#35782;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.04335</link><description>&lt;p&gt;
&#20256;&#24863;&#22120;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#35821;&#20041;&#25216;&#26415;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#26144;&#23556;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semantic Technologies in Sensor-Based Personal Health Monitoring Systems: A Systematic Mapping Study. (arXiv:2306.04335v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04335
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#24863;&#22120;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20013;&#35821;&#20041;&#25216;&#26415;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#21457;&#29616;&#27492;&#31867;&#31995;&#32479;&#24517;&#39035;&#20811;&#26381;&#30340;&#20851;&#38190;&#25361;&#25112;&#20026;&#20114;&#25805;&#20316;&#24615;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#24773;&#22659;&#26816;&#27979;&#12289;&#24773;&#22659;&#39044;&#27979;&#12289;&#20915;&#31574;&#25903;&#25345;&#21644;&#30693;&#35782;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20110;&#30142;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#12289;&#39044;&#38450;&#21644;&#39044;&#27979;&#36234;&#26469;&#36234;&#37325;&#35270;&#12290;&#27492;&#22806;&#65292;&#20256;&#24863;&#22120;&#25216;&#26415;&#21644;&#29289;&#32852;&#32593;&#25216;&#26415;&#30340;&#19981;&#26029;&#36827;&#27493;&#20063;&#25512;&#21160;&#20102;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#35821;&#20041;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#24322;&#26500;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20114;&#25805;&#20316;&#24615;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#34920;&#31034;&#19987;&#23478;&#20581;&#24247;&#30693;&#35782;&#20197;&#25903;&#25345;&#20915;&#31574;&#25152;&#38656;&#30340;&#22797;&#26434;&#25512;&#29702;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#24863;&#22120;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20013;&#35821;&#20041;&#25216;&#26415;&#30340;&#24212;&#29992;&#29616;&#29366;&#12290;&#20351;&#29992;&#31995;&#32479;&#26041;&#27861;&#23545;40&#20010;&#20195;&#34920;&#35813;&#39046;&#22495;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#31995;&#32479;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#36890;&#36807;&#36825;&#39033;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#27492;&#31867;&#31995;&#32479;&#24517;&#39035;&#20811;&#26381;&#30340;&#20845;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#20114;&#25805;&#20316;&#24615;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#24773;&#22659;&#26816;&#27979;&#12289;&#24773;&#22659;&#39044;&#27979;&#12289;&#20915;&#31574;&#25903;&#25345;&#21644;&#30693;&#35782;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an increased focus on early detection, prevention, and prediction of diseases. This, together with advances in sensor technology and the Internet of Things, has led to accelerated efforts in the development of personal health monitoring systems. Semantic technologies have emerged as an effective way to not only deal with the issue of interoperability associated with heterogeneous health sensor data, but also to represent expert health knowledge to support complex reasoning required for decision-making. This study evaluates the state of the art in the use of semantic technologies in sensor-based personal health monitoring systems. Using a systematic approach, a total of 40 systems representing the state of the art in the field are analysed. Through this analysis, six key challenges that such systems must overcome for optimal and effective health monitoring are identified: interoperability, context awareness, situation detection, situation prediction, deci
&lt;/p&gt;</description></item><item><title>BiomedGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#22312;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;16&#20010;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#21253;&#25324;&#36229;&#36807;&#20102;OpenAI&#30340;GPT-4V&#21644;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#21516;&#26102;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.17100</link><description>&lt;p&gt;
BiomedGPT&#65306;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#32479;&#19968;&#19988;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks. (arXiv:2305.17100v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17100
&lt;/p&gt;
&lt;p&gt;
BiomedGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#22312;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;16&#20010;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#21253;&#25324;&#36229;&#36807;&#20102;OpenAI&#30340;GPT-4V&#21644;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#21516;&#26102;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#21644;&#32500;&#25252;&#20013;&#19981;&#22815;&#28789;&#27963;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#32467;&#21512;&#29616;&#20195;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#20026;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26377;&#28508;&#21147;&#35299;&#37322;&#19981;&#21516;&#30340;&#21307;&#30103;&#27169;&#24577;&#65292;&#24182;&#20135;&#29983;&#22914;&#33258;&#30001;&#25991;&#26412;&#25253;&#21578;&#25110;&#30142;&#30149;&#35786;&#26029;&#31561;&#34920;&#36798;&#24615;&#36755;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BiomedGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#22810;&#26679;&#21270;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#24320;&#28304;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;BiomedGPT&#22312;26&#20010;&#25968;&#25454;&#38598;&#30340;&#20116;&#20010;&#20020;&#24202;&#37325;&#35201;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;16&#20010;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25918;&#23556;&#23398;&#20154;&#21592;&#35780;&#20272;&#20013;&#65292;&#23427;&#36229;&#36234;&#20102;OpenAI&#30340;GPT-4 with vision&#65288;GPT-4V&#65289;&#65292;&#24182;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#36229;&#36807;&#20102;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#27492;&#22806;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional task- and modality-specific artificial intelligence (AI) models are inflexible in real-world deployment and maintenance for biomedicine. At the same time, the growing availability of biomedical data, coupled with the advancements in modern multi-modal multi-task AI techniques, has paved the way for the emergence of generalist biomedical AI solutions. These solutions hold the potential to interpret different medical modalities and produce expressive outputs such as free-text reports or disease diagnosis. Here, we propose BiomedGPT, the first open-source and generalist visual language AI for diverse biomedical tasks. BiomedGPT achieved 16 state-of-the-art results across five clinically significant tasks on 26 datasets. Notably, it outperformed OpenAI's GPT-4 with vision (GPT-4V) in radiology human evaluation and surpassed Google's Med-PaLM M (12B) in breast cancer diagnosis and medical visual question answering. Moreover, BiomedGPT facilitates zero-shot transfer learning, gr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2305.13840</link><description>&lt;p&gt;
Control-A-Video: &#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models. (arXiv:2305.13840v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#65288;T2V&#65289;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;Video-ControlNet&#12290;&#35813;&#27169;&#22411;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;&#26377;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#21487;&#35757;&#32451;&#30340;&#26102;&#38388;&#23618;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#36328;&#24103;&#24314;&#27169;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#24103;&#26465;&#20214;&#31574;&#30053;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#22312;&#33258;&#22238;&#24402;&#26041;&#24335;&#19979;&#29983;&#25104;&#36716;&#25442;&#33258;&#22270;&#20687;&#39046;&#22495;&#20197;&#21450;&#20219;&#24847;&#38271;&#24230;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;Video-ControlNet&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#20174;&#36755;&#20837;&#35270;&#39057;&#20013;&#24341;&#20837;&#36816;&#21160;&#20808;&#39564;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26550;&#26500;&#21644;&#31574;&#30053;&#65292;Video-ControlNet&#21487;&#20197;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#65292;&#29983;&#25104;&#20855;&#26377;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#20248;&#36136;&#19968;&#33268;&#35270;&#39057;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a controllable text-to-video (T2V) diffusion model, named Video-ControlNet, that generates videos conditioned on a sequence of control signals, such as edge or depth maps. Video-ControlNet is built on a pre-trained conditional text-to-image (T2I) diffusion model by incorporating a spatial-temporal self-attention mechanism and trainable temporal layers for efficient cross-frame modeling. A first-frame conditioning strategy is proposed to facilitate the model to generate videos transferred from the image domain as well as arbitrary-length videos in an auto-regressive manner. Moreover, Video-ControlNet employs a novel residual-based noise initialization strategy to introduce motion prior from an input video, producing more coherent videos. With the proposed architecture and strategies, Video-ControlNet can achieve resource-efficient convergence and generate superior quality and consistent videos with fine-grained control. Extensive experiments demonstrate its success i
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;SparseFit&#65292;&#19968;&#31181;&#23569;&#26679;&#26412;&#21050;&#28608;&#30340;&#31232;&#30095;&#24494;&#35843;&#31574;&#30053;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21487;&#29992;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.13235</link><description>&lt;p&gt;
SPARSEFIT&#65306;&#23569;&#26679;&#26412;&#21050;&#28608;&#30340;&#31232;&#30095;&#24494;&#35843;&#65292;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations. (arXiv:2305.13235v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13235
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;SparseFit&#65292;&#19968;&#31181;&#23569;&#26679;&#26412;&#21050;&#28608;&#30340;&#31232;&#30095;&#24494;&#35843;&#31574;&#30053;&#65292;&#29992;&#20110;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21487;&#29992;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#30340;&#20915;&#31574;&#23545;&#20110;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#22312;&#37096;&#32626;&#26102;&#30340;&#21487;&#20449;&#24230;&#24456;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#35777;&#26126;&#27169;&#22411;&#30340;&#39044;&#27979;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20316;&#20026;&#30495;&#23454;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26082;&#26114;&#36149;&#21448;&#21487;&#33021;&#23545;&#20110;&#26576;&#20123;&#24212;&#29992;&#31243;&#24207;&#26469;&#35828;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#20351;&#27169;&#22411;&#22312;&#21482;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21487;&#29992;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#21050;&#28608;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#20351;&#24471;&#24494;&#35843;&#21313;&#20998;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SparseFit&#65292;&#19968;&#31181;&#31232;&#30095;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#31574;&#30053;&#65292;&#21033;&#29992;&#31163;&#25955;&#21050;&#28608;&#26469;&#32852;&#21512;&#29983;&#25104;&#39044;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;T5&#27169;&#22411;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;SparseFit&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining the decisions of neural models is crucial for ensuring their trustworthiness at deployment time. Using Natural Language Explanations (NLEs) to justify a model's predictions has recently gained increasing interest. However, this approach usually demands large datasets of human-written NLEs for the ground-truth answers, which are expensive and potentially infeasible for some applications. For models to generate high-quality NLEs when only a few NLEs are available, the fine-tuning of Pre-trained Language Models (PLMs) in conjunction with prompt-based learning recently emerged. However, PLMs typically have billions of parameters, making fine-tuning expensive. We propose SparseFit, a sparse few-shot fine-tuning strategy that leverages discrete prompts to jointly generate predictions and NLEs. We experiment with SparseFit on the T5 model and four datasets and compare it against state-of-the-art parameter-efficient fine-tuning techniques. We perform automatic and human evaluations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05215</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20808;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#39044;&#35757;&#32451;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21450;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#26368;&#36817;&#65292;&#36965;&#24863;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#22522;&#30784;&#27169;&#22411;&#22312;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#21442;&#25968;&#65288;&#21253;&#25324;86M&#12289;605.26M&#12289;1.3B&#21644;2.4B&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#30830;&#23450;&#21442;&#25968;&#22686;&#21152;&#26159;&#21542;&#20250;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10&#20159;&#20010;&#36965;&#24863;&#22270;&#20687;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21521;&#30740;&#31350;&#31038;&#21306;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
&lt;/p&gt;</description></item><item><title>SE-MoE&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#30340;&#35757;&#32451;&#26041;&#24335;&#21644;&#19981;&#21516;&#20248;&#21270;&#25514;&#26045;&#65292;&#20197;&#25552;&#39640;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#36127;&#36733;&#24179;&#34913;&#12289;&#36890;&#20449;/&#35745;&#31639;&#25928;&#29575;&#21644;&#20869;&#23384;&#38480;&#21046;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.10034</link><description>&lt;p&gt;
SE-MoE: &#19968;&#31181;&#21487;&#25193;&#23637;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#28151;&#21512;&#19987;&#23478;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System. (arXiv:2205.10034v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10034
&lt;/p&gt;
&lt;p&gt;
SE-MoE&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#30340;&#35757;&#32451;&#26041;&#24335;&#21644;&#19981;&#21516;&#20248;&#21270;&#25514;&#26045;&#65292;&#20197;&#25552;&#39640;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#36127;&#36733;&#24179;&#34913;&#12289;&#36890;&#20449;/&#35745;&#31639;&#25928;&#29575;&#21644;&#20869;&#23384;&#38480;&#21046;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#22810;&#26679;&#24615;&#22686;&#21152;&#65292;&#23558;&#27169;&#22411;&#20998;&#24067;&#24335;&#35757;&#32451;&#22312;&#24322;&#26500;&#35745;&#31639;&#31995;&#32479;&#19978;&#20197;&#26041;&#20415;&#29983;&#25104;&#22823;&#27169;&#22411;&#25104;&#20026;&#20102;&#19968;&#31181;&#36235;&#21183;&#12290;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#21033;&#29992;&#20998;&#27835;&#31574;&#30053;&#36890;&#36807;&#38376;&#25511;&#21644;&#24182;&#34892;&#22788;&#29702;&#26041;&#24335;&#26469;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#24182;&#25511;&#21046;&#24635;&#20307;&#27169;&#22411;/&#25968;&#25454;&#22823;&#23567;&#12290;&#34429;&#28982; DeepSpeed &#22312;&#36827;&#34892;&#22823;&#35268;&#27169; MoE &#35757;&#32451;&#19978;&#36827;&#34892;&#20102;&#23581;&#35797;&#65292;&#20294;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#20173;&#26377;&#25552;&#21319;&#30340;&#31354;&#38388;&#65292;&#21253;&#25324;&#36127;&#36733;&#24179;&#34913;&#12289;&#36890;&#20449;/&#35745;&#31639;&#25928;&#29575;&#21644;&#20869;&#23384;&#38480;&#21046;&#31561;&#26041;&#38754;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102; SE-MoE&#65292;&#20351;&#29992;&#24377;&#24615; MoE &#35757;&#32451;&#12289;&#22522;&#20110;&#23618;&#27425;&#23384;&#20648;&#30340; 2D &#39044;&#21462;&#21644;&#34701;&#21512;&#36890;&#20449;&#31561;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#19981;&#21516;&#30340;&#31867;&#22411;&#20013;&#33719;&#24471;&#39640;&#25928;&#30340;&#24182;&#34892;&#22788;&#29702;&#12290;&#38024;&#23545;&#21333;&#33410;&#28857;&#30340;&#21487;&#25193;&#23637;&#25512;&#29702;&#65292;&#29305;&#21035;&#26159;&#24403;&#27169;&#22411;&#22823;&#23567;&#22823;&#20110; GPU &#20869;&#23384;&#26102;&#65292;SE-MoE &#23558; CPU-GPU &#23384;&#20648;&#32852;&#21512;&#24418;&#25104;&#19968;&#20010;&#26356;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing diversity of ML infrastructures nowadays, distributed training over heterogeneous computing systems is desired to facilitate the production of big models. Mixture-of-Experts (MoE) models have been proposed to lower the cost of training subject to the overall size of models/data through gating and parallelism in a divide-and-conquer fashion. While DeepSpeed has made efforts in carrying out large-scale MoE training over heterogeneous infrastructures, the efficiency of training and inference could be further improved from several system aspects, including load balancing, communication/computation efficiency, and memory footprint limits. In this work, we present SE-MoE that proposes Elastic MoE training with 2D prefetch and Fusion communication over Hierarchical storage, so as to enjoy efficient parallelisms in various types. For scalable inference in a single node, especially when the model size is larger than GPU memory, SE-MoE forms the CPU-GPU memory jointly into a 
&lt;/p&gt;</description></item></channel></rss>