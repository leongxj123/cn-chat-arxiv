<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.18717</link><description>&lt;p&gt;
&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning for Deep Causal Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18717
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#22238;&#31572;&#8220;&#22914;&#26524;$y$&#21464;&#20026;$z$&#65292;$x$&#20250;&#22914;&#20309;&#21464;&#21270;&#65311;&#8221;&#36825;&#31867;&#38382;&#39064;&#30340;&#27169;&#22411;&#23545;&#20110;&#25512;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#33021;&#22815;&#35299;&#20915;&#36825;&#31867;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#30446;&#21069;&#35201;&#27714;&#25152;&#26377;&#30456;&#20851;&#21464;&#37327;&#22343;&#24050;&#34987;&#35266;&#23519;&#21040;&#65292;&#24182;&#19988;&#30456;&#24212;&#30340;&#26631;&#31614;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#29992;&#12290;&#25105;&#20204;&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18717v1 Announce Type: cross  Abstract: Developing models that can answer questions of the form "How would $x$ change if $y$ had been $z$?" is fundamental for advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#22686;&#24378;&#24314;&#27169;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#65292;&#20174;&#32780;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#21644;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.18241</link><description>&lt;p&gt;
NeuSDFusion: &#19968;&#31181;&#31354;&#38388;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;3D&#24418;&#29366;&#30340;&#23436;&#25104;&#12289;&#37325;&#24314;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18241
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#22686;&#24378;&#24314;&#27169;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#65292;&#20174;&#32780;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#21644;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24418;&#29366;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#26465;&#20214;&#21644;&#32422;&#26463;&#30340;&#21019;&#26032;&#24615;3D&#20869;&#23481;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;3D&#24418;&#29366;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23616;&#37096;&#32452;&#20214;&#65292;&#23558;&#27599;&#20010;&#20803;&#32032;&#23396;&#31435;&#22788;&#29702;&#32780;&#19981;&#32771;&#34385;&#31354;&#38388;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;3D&#25968;&#25454;&#34920;&#31034;&#21644;&#24418;&#29366;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#22810;&#26679;&#24615;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#21270;&#19988;&#31526;&#21512;&#25351;&#23450;&#32422;&#26463;&#30340;3D&#24418;&#29366;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#26469;&#22686;&#24378;3D&#24418;&#29366;&#24314;&#27169;&#12290;&#20026;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#24182;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#31181;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#65292;&#30452;&#25509;&#20351;&#29992;&#27491;&#20132;&#30340;2D&#24179;&#38754;&#23398;&#20064;3D&#24418;&#29366;&#30340;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20256;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18241v1 Announce Type: cross  Abstract: 3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a tra
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;SAT&#32534;&#30721;&#30340;&#20559;&#24207;&#27169;&#22411;&#29992;&#20110;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#23545;&#24102;&#23485;&#30528;&#33394;&#38382;&#39064;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.15961</link><description>&lt;p&gt;
SAT&#32534;&#30721;&#30340;&#20559;&#24207;&#27169;&#22411;&#29992;&#20110;&#22270;&#30528;&#33394;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SAT Encoding of Partial Ordering Models for Graph Coloring Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15961
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;SAT&#32534;&#30721;&#30340;&#20559;&#24207;&#27169;&#22411;&#29992;&#20110;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#23545;&#24102;&#23485;&#30528;&#33394;&#38382;&#39064;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20559;&#24207;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#27169;&#22411;&#30340;&#22270;&#30528;&#33394;&#38382;&#39064;&#65288;GCP&#65289;&#21644;&#24102;&#23485;&#30528;&#33394;&#38382;&#39064;&#65288;BCP&#65289;&#30340;&#26032;SAT&#32534;&#30721;&#12290; GCP&#35201;&#27714;&#32473;&#23450;&#22270;&#30340;&#39030;&#28857;&#20998;&#37197;&#26368;&#23569;&#25968;&#37327;&#30340;&#39068;&#33394;&#65292;&#20197;&#20415;&#27599;&#20004;&#20010;&#30456;&#37051;&#30340;&#39030;&#28857;&#24471;&#21040;&#19981;&#21516;&#30340;&#39068;&#33394;&#12290; BCP&#26159;&#19968;&#20010;&#27867;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#26465;&#36793;&#37117;&#26377;&#19968;&#20010;&#26435;&#37325;&#65292;&#35201;&#27714;&#20998;&#37197;&#30340;&#39068;&#33394;&#20043;&#38388;&#26377;&#26368;&#23567;&#30340;&#8220;&#36317;&#31163;&#8221;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#20351;&#29992;&#30340;&#8220;&#26368;&#22823;&#8221;&#39068;&#33394;&#12290; &#23545;&#20110;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;GCP&#65292;&#25105;&#20204;&#22312;DIMACS&#22522;&#20934;&#38598;&#19978;&#23454;&#39564;&#27604;&#36739;&#20102;&#25105;&#20204;&#26032;&#30340;SAT&#32534;&#30721;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290; &#25105;&#20204;&#30340;&#35780;&#20272;&#35777;&#23454;&#65292;&#36825;&#31181;SAT&#32534;&#30721;&#23545;&#20110;&#31232;&#30095;&#22270;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#29978;&#33267;&#22312;&#19968;&#20123;DIMACS&#31034;&#20363;&#19978;&#32988;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290; &#23545;&#20110;BCP&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22522;&#20110;&#20559;&#24207;&#30340;SAT&#21644;ILP&#20844;&#24335;&#30340;&#22823;&#23567;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#23567;&#20110;&#32463;&#20856;&#30340;&#35299;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15961v1 Announce Type: new  Abstract: In this paper, we suggest new SAT encodings of the partial-ordering based ILP model for the graph coloring problem (GCP) and the bandwidth coloring problem (BCP). The GCP asks for the minimum number of colors that can be assigned to the vertices of a given graph such that each two adjacent vertices get different colors. The BCP is a generalization, where each edge has a weight that enforces a minimal "distance" between the assigned colors, and the goal is to minimize the "largest" color used. For the widely studied GCP, we experimentally compare our new SAT encoding to the state-of-the-art approaches on the DIMACS benchmark set. Our evaluation confirms that this SAT encoding is effective for sparse graphs and even outperforms the state-of-the-art on some DIMACS instances. For the BCP, our theoretical analysis shows that the partial-ordering based SAT and ILP formulations have an asymptotically smaller size than that of the classical assi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.15371</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#32034;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models explore in-context?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15371
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36827;&#34892;&#25506;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#25105;&#20204;&#20851;&#27880;&#29616;&#26377;LLMs&#30340;&#21407;&#29983;&#24615;&#33021;&#65292;&#27809;&#26377;&#36827;&#34892;&#35757;&#32451;&#24178;&#39044;&#12290;&#25105;&#20204;&#23558;LLMs&#37096;&#32626;&#20026;&#31616;&#21333;&#22810;&#33218;&#32769;&#34382;&#26426;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#24182;&#23436;&#20840;&#22312;&#19978;&#19979;&#25991;&#20013;&#25351;&#23450;&#29615;&#22659;&#25551;&#36848;&#21644;&#20132;&#20114;&#21382;&#21490;&#65292;&#21363;&#22312;LLM&#25552;&#31034;&#20869;&#37096;&#36827;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#23545;GPT-3.5&#12289;GPT-4&#21644;Llama2&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24182;&#27809;&#26377;&#31283;&#20581;&#22320;&#36827;&#34892;&#25506;&#32034;&#65306;i&#65289;&#22312;&#25105;&#20204;&#30340;&#25152;&#26377;&#23454;&#39564;&#20013;&#65292;&#21482;&#26377;&#19968;&#20010;&#37197;&#32622;&#23548;&#33268;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#65306;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#22806;&#37096;&#24635;&#32467;&#30340;&#20132;&#20114;&#21382;&#21490;&#30340;GPT-4&#65292;&#36825;&#20123;&#34987;&#21576;&#29616;&#20026;&#20805;&#20998;&#32479;&#35745;&#30340;&#24773;&#20917;&#65307;ii&#65289;&#25152;&#26377;&#20854;&#20182;&#37197;&#32622;&#37117;&#27809;&#26377;&#20135;&#29983;&#31283;&#20581;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#21253;&#25324;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#20854;&#20182;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15371v1 Announce Type: cross  Abstract: We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;OTSeg&#20013;&#30340;Multi-Prompts Sinkhorn Attention&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14183</link><description>&lt;p&gt;
OTSeg&#65306;&#22810;&#25552;&#31034;Sinkhorn&#27880;&#24847;&#21147;&#29992;&#20110;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14183
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;OTSeg&#20013;&#30340;Multi-Prompts Sinkhorn Attention&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#26368;&#26032;&#25104;&#21151;&#35777;&#26126;&#20102;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#30693;&#35782;&#36716;&#31227;&#21040;&#20687;&#32032;&#32423;&#20998;&#31867;&#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;CLIP&#30693;&#35782;&#26469;&#32039;&#23494;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#21644;&#20687;&#32032;&#23884;&#20837;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OTSeg&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#31639;&#27861;&#30340;&#22810;&#25552;&#31034;Sinkhorn&#65288;MPS&#65289;&#65292;&#36825;&#20351;&#24471;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#22270;&#20687;&#20687;&#32032;&#20869;&#30340;&#21508;&#31181;&#35821;&#20041;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;Sinkformers&#22312;&#21333;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MPS&#30340;&#25193;&#23637;&#65292;&#31216;&#20026;&#22810;&#25552;&#31034;Sinkhorn&#27880;&#24847;&#21147;&#65288;MPSA&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#21462;&#20195;&#20102;Transformer&#26694;&#26550;&#20013;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14183v1 Announce Type: cross  Abstract: The recent success of CLIP has demonstrated promising results in zero-shot semantic segmentation by transferring muiltimodal knowledge to pixel-level classification. However, leveraging pre-trained CLIP knowledge to closely align text embeddings with pixel embeddings still has limitations in existing approaches. To address this issue, we propose OTSeg, a novel multimodal attention mechanism aimed at enhancing the potential of multiple text prompts for matching associated pixel embeddings. We first propose Multi-Prompts Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads multiple text prompts to selectively focus on various semantic features within image pixels. Moreover, inspired by the success of Sinkformers in unimodal settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn Attention (MPSA), which effectively replaces cross-attention mechanisms within Transformer framework in multimodal settin
&lt;/p&gt;</description></item><item><title>EnvGen&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#33258;&#36866;&#24212;&#21019;&#24314;&#35757;&#32451;&#29615;&#22659;&#65292;&#24110;&#21161;&#23567;&#22411;&#20855;&#36523;&#20307;RL&#20195;&#29702;&#22312;&#24369;&#28857;&#26041;&#38754;&#23398;&#20064;&#26377;&#29992;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12014</link><description>&lt;p&gt;
EnvGen: &#36890;&#36807;LLMs&#29983;&#25104;&#21644;&#35843;&#25972;&#29615;&#22659;&#20197;&#35757;&#32451;&#20855;&#36523;&#20307;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12014
&lt;/p&gt;
&lt;p&gt;
EnvGen&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#33258;&#36866;&#24212;&#21019;&#24314;&#35757;&#32451;&#29615;&#22659;&#65292;&#24110;&#21161;&#23567;&#22411;&#20855;&#36523;&#20307;RL&#20195;&#29702;&#22312;&#24369;&#28857;&#26041;&#38754;&#23398;&#20064;&#26377;&#29992;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20851;&#36890;&#36807;&#20114;&#21160;&#36827;&#34892;&#20855;&#36523;&#20307;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#30452;&#25509;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#65292;&#20197;&#30830;&#23450;&#29615;&#22659;&#20013;&#30340;&#19979;&#19968;&#27493;&#12290;LLM&#20195;&#29702;&#30001;&#20110;&#20854;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27604;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20197;&#24448;&#36739;&#23567;&#30340;&#20195;&#29702;&#34920;&#29616;&#26356;&#24378;&#65307;&#20294;&#39057;&#32321;&#35843;&#29992;LLMs&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;EnvGen&#65292;&#19968;&#20010;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#31034;&#19968;&#20010;LLM&#29983;&#25104;&#35757;&#32451;&#29615;&#22659;&#65292;&#20351;&#20195;&#29702;&#21487;&#20197;&#24555;&#36895;&#24182;&#34892;&#23398;&#20064;&#19981;&#21516;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#33719;&#24471;&#20219;&#21153;&#25551;&#36848;&#21644;&#27169;&#25311;&#22120;&#30446;&#26631;&#65292;&#28982;&#21518;&#34987;&#35201;&#27714;&#29983;&#25104;&#19968;&#32452;&#29615;&#22659;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12014v1 Announce Type: cross  Abstract: Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g
&lt;/p&gt;</description></item><item><title>&#20998;&#21449;&#27880;&#24847;&#21147;&#26159;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#21333;&#19978;&#19979;&#25991;&#25209;&#37327;&#25277;&#26679;&#29615;&#22659;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20998;&#25104;&#20004;&#20010;&#29420;&#31435;&#30340;&#25805;&#20316;&#26469;&#20943;&#23569;&#20887;&#20313;&#20869;&#23384;IO&#25104;&#26412;&#65292;&#25552;&#39640;&#25928;&#29575;&#24182;&#38477;&#20302;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2403.08845</link><description>&lt;p&gt;
&#21333;&#19978;&#19979;&#25991;&#22823;&#25209;&#37327;&#25277;&#26679;&#30340;&#20998;&#21449;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Bifurcated Attention for Single-Context Large-Batch Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08845
&lt;/p&gt;
&lt;p&gt;
&#20998;&#21449;&#27880;&#24847;&#21147;&#26159;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#21333;&#19978;&#19979;&#25991;&#25209;&#37327;&#25277;&#26679;&#29615;&#22659;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20998;&#25104;&#20004;&#20010;&#29420;&#31435;&#30340;&#25805;&#20316;&#26469;&#20943;&#23569;&#20887;&#20313;&#20869;&#23384;IO&#25104;&#26412;&#65292;&#25552;&#39640;&#25928;&#29575;&#24182;&#38477;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#21449;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21333;&#19978;&#19979;&#25991;&#25209;&#37327;&#25277;&#26679;&#29615;&#22659;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20943;&#23569;&#20887;&#20313;&#30340;&#20869;&#23384;IO&#25104;&#26412;&#65292;&#36825;&#26159;&#39640;&#25209;&#37327;&#22823;&#23567;&#21644;&#38271;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#24310;&#36831;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#20998;&#21449;&#27880;&#24847;&#21147;&#36890;&#36807;&#22312;&#22686;&#37327;&#35299;&#30721;&#26399;&#38388;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#21010;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;GEMM&#25805;&#20316;&#65292;&#20998;&#21035;&#19987;&#27880;&#20110;&#26469;&#33258;&#39044;&#22635;&#20805;&#30340;KV&#32531;&#23384;&#20197;&#21450;&#35299;&#30721;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#30830;&#20445;&#20102;&#31934;&#30830;&#30340;&#35745;&#31639;&#65292;&#24182;&#32500;&#25345;&#24120;&#35268;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35745;&#31639;&#36127;&#36733;&#65288;FLOPs&#65289;&#65292;&#20294;&#20943;&#23569;&#20102;&#20869;&#23384;IO&#12290;&#20998;&#21449;&#27880;&#24847;&#21147;&#36824;&#19982;&#20943;&#23569;KV&#32531;&#23384;&#20869;&#23384;IO&#24050;&#30693;&#30340;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#26426;&#21046;&#20860;&#23481;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#26356;&#39640;&#30340;&#25209;&#37327;&#22823;&#23567;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#30001;&#27492;&#24102;&#26469;&#30340;&#25928;&#29575;&#23548;&#33268;&#26356;&#20302;&#30340;&#24310;&#36831;&#65292;&#25913;&#21892;&#20102;&#23454;&#26102;&#24212;&#29992;&#30340;&#36866;&#29992;&#24615;&#65292;&#20363;&#22914;&#23454;&#29616;&#22823;&#35268;&#27169;&#24182;&#34892;&#30340;&#31572;&#26696;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08845v1 Announce Type: cross  Abstract: In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts. This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths. Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process. This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO. Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length. The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation 
&lt;/p&gt;</description></item><item><title>&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#19981;&#23384;&#22312;&#30340;&#35780;&#20272;&#21644;&#22686;&#24378;&#29616;&#26377;&#35780;&#20272;&#30340;&#39057;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#24037;&#20316;&#20154;&#21592;&#24037;&#20316;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.06322</link><description>&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20013;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#26816;&#26597;&#25506;&#35270;&#21644;&#27963;&#21160;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Leveraging Computer Vision in the Intensive Care Unit (ICU) for Examining Visitation and Mobility
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06322
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#19981;&#23384;&#22312;&#30340;&#35780;&#20272;&#21644;&#22686;&#24378;&#29616;&#26377;&#35780;&#20272;&#30340;&#39057;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#24037;&#20316;&#20154;&#21592;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23494;&#20999;&#30417;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#24739;&#32773;&#30340;&#37325;&#35201;&#24615;&#65292;&#30001;&#20110;&#21307;&#25252;&#20154;&#21592;&#38754;&#20020;&#30340;&#26102;&#38388;&#38480;&#21046;&#65292;&#35768;&#22810;&#26041;&#38754;&#20173;&#28982;&#21463;&#21040;&#38480;&#21046;&#35780;&#20272;&#12290;&#36807;&#24230;&#30340;&#25506;&#35270;&#21487;&#33021;&#22312;&#20241;&#24687;&#26102;&#38388;&#21152;&#21095;&#24490;&#29615;&#33410;&#24459;&#32010;&#20081;&#21644;&#35893;&#22916;&#30340;&#39118;&#38505;&#65292;&#20294;&#22312;ICU&#20013;&#24182;&#26410;&#34987;&#25429;&#25417;&#12290;&#21516;&#26679;&#65292;&#27963;&#21160;&#33021;&#21147;&#21487;&#20197;&#26159;ICU&#24739;&#32773;&#24247;&#22797;&#25110;&#24694;&#21270;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#20294;&#21482;&#34987;&#38646;&#26143;&#22320;&#25429;&#25417;&#25110;&#26681;&#26412;&#19981;&#34987;&#25429;&#25417;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#25214;&#21040;&#20102;&#24212;&#29992;&#65292;&#20943;&#36731;&#20102;&#20154;&#21147;&#36127;&#25285;&#12290;&#22312;ICU&#20013;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#20063;&#26377;&#21487;&#33021;&#23454;&#29616;&#19981;&#23384;&#22312;&#30340;&#35780;&#20272;&#25110;&#22686;&#24378;&#29616;&#26377;&#35780;&#20272;&#30340;&#39057;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#24037;&#20316;&#20154;&#21592;&#30340;&#24037;&#20316;&#37327;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#25104;&#20687;&#30340;&#26368;&#26032;&#38750;&#20405;&#20837;&#24335;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06322v1 Announce Type: cross  Abstract: Despite the importance of closely monitoring patients in the Intensive Care Unit (ICU), many aspects are still assessed in a limited manner due to the time constraints imposed on healthcare providers. For example, although excessive visitations during rest hours can potentially exacerbate the risk of circadian rhythm disruption and delirium, it is not captured in the ICU. Likewise, while mobility can be an important indicator of recovery or deterioration in ICU patients, it is only captured sporadically or not captured at all. In the past few years, the computer vision field has found application in many domains by reducing the human burden. Using computer vision systems in the ICU can also potentially enable non-existing assessments or enhance the frequency and accuracy of existing assessments while reducing the staff workload. In this study, we leverage a state-of-the-art noninvasive computer vision system based on depth imaging to c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03881</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Latent Dataset Distillation with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#25968;&#25454;&#38598;&#24102;&#26469;&#23384;&#20648;&#25361;&#25112;&#65292;&#24182;&#19988;&#21253;&#21547;&#19968;&#20123;&#38750;&#24433;&#21709;&#21147;&#26679;&#26412;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#34987;&#24573;&#30053;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#26368;&#32456;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#20986;&#29616;&#20102;&#23558;&#25968;&#25454;&#38598;&#20449;&#24687;&#33976;&#39311;&#25104;&#19968;&#32452;&#21387;&#32553;&#26679;&#26412;&#65288;&#21512;&#25104;&#26679;&#26412;&#65289;&#65292;&#21363;&#33976;&#39311;&#25968;&#25454;&#38598;&#30340;&#27010;&#24565;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#36873;&#25321;&#29992;&#20110;&#36830;&#25509;&#21407;&#22987;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26550;&#26500;&#65288;&#36890;&#24120;&#26159;ConvNet&#65289;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#19982;&#33976;&#39311;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#21017;&#26368;&#32456;&#20934;&#30830;&#24615;&#20250;&#38477;&#20302;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20363;&#22914;128x128&#21450;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03881v1 Announce Type: cross  Abstract: The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both chal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#37325;&#20889;&#31995;&#32479;&#21551;&#21457;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31639;&#27861;&#20219;&#21153;&#65292;&#36890;&#36807;Selector&#12289;Solver&#21644;Combiner&#19977;&#20010;&#19987;&#38376;&#27169;&#22359;&#23454;&#29616;&#31639;&#27861;&#20219;&#21153;&#30340;&#31616;&#21270;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#22806;&#25512;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.17407</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#37325;&#20889;&#31995;&#32479;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Neural Rewriting System to Solve Algorithmic Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17407
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#37325;&#20889;&#31995;&#32479;&#21551;&#21457;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31639;&#27861;&#20219;&#21153;&#65292;&#36890;&#36807;Selector&#12289;Solver&#21644;Combiner&#19977;&#20010;&#19987;&#38376;&#27169;&#22359;&#23454;&#29616;&#31639;&#27861;&#20219;&#21153;&#30340;&#31616;&#21270;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20173;&#28982;&#38590;&#20197;&#23398;&#20064;&#38656;&#35201;&#31995;&#32479;&#24212;&#29992;&#32452;&#21512;&#35268;&#21017;&#26469;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#38382;&#39064;&#23454;&#20363;&#30340;&#31639;&#27861;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21019;&#26041;&#27861;&#26469;&#23398;&#20064;&#21463;&#37325;&#20889;&#31995;&#32479;&#21551;&#21457;&#30340;&#31639;&#27861;&#20219;&#21153;&#65292;&#37325;&#20889;&#31995;&#32479;&#26159;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#32463;&#20856;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#37325;&#20889;&#31995;&#32479;&#21487;&#20197;&#34987;&#23454;&#29616;&#20026;&#19968;&#20010;&#30001;&#19987;&#38376;&#27169;&#22359;&#32452;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#65306;&#36873;&#25321;&#22120;&#35782;&#21035;&#35201;&#22788;&#29702;&#30340;&#30446;&#26631;&#23376;&#34920;&#36798;&#24335;&#65292;&#27714;&#35299;&#22120;&#36890;&#36807;&#35745;&#31639;&#30456;&#24212;&#30340;&#32467;&#26524;&#31616;&#21270;&#23376;&#34920;&#36798;&#24335;&#65292;&#32452;&#21512;&#22120;&#36890;&#36807;&#29992;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#26367;&#25442;&#23376;&#34920;&#36798;&#24335;&#29983;&#25104;&#21407;&#22987;&#34920;&#36798;&#24335;&#30340;&#26032;&#29256;&#26412;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#28041;&#21450;&#31616;&#21270;&#28041;&#21450;&#21015;&#34920;&#12289;&#31639;&#26415;&#21644;&#20195;&#25968;&#34920;&#36798;&#24335;&#30340;&#31526;&#21495;&#20844;&#24335;&#30340;&#31639;&#27861;&#20219;&#21153;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#25152;&#25552;&#26550;&#26500;&#30340;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17407v1 Announce Type: cross  Abstract: Modern neural network architectures still struggle to learn algorithmic procedures that require to systematically apply compositional rules to solve out-of-distribution problem instances. In this work, we propose an original approach to learn algorithmic tasks inspired by rewriting systems, a classic framework in symbolic artificial intelligence. We show that a rewriting system can be implemented as a neural architecture composed by specialized modules: the Selector identifies the target sub-expression to process, the Solver simplifies the sub-expression by computing the corresponding result, and the Combiner produces a new version of the original expression by replacing the sub-expression with the solution provided. We evaluate our model on three types of algorithmic tasks that require simplifying symbolic formulas involving lists, arithmetic, and algebraic expressions. We test the extrapolation capabilities of the proposed architectu
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#20010;&#22686;&#24378;&#20854;&#23454;&#29992;&#24615;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#26696;&#24573;&#35270;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SafeCoder&#65292;&#36890;&#36807;&#23433;&#20840;&#24494;&#35843;&#21644;&#26631;&#20934;&#25351;&#20196;&#35843;&#20248;&#30456;&#32467;&#21512;&#65292;&#26469;&#20248;&#21270;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09497</link><description>&lt;p&gt;
&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Secure Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09497
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#20010;&#22686;&#24378;&#20854;&#23454;&#29992;&#24615;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#26696;&#24573;&#35270;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SafeCoder&#65292;&#36890;&#36807;&#23433;&#20840;&#24494;&#35843;&#21644;&#26631;&#20934;&#25351;&#20196;&#35843;&#20248;&#30456;&#32467;&#21512;&#65292;&#26469;&#20248;&#21270;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;(LMs)&#22312;&#26085;&#24120;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#65292;&#23588;&#20854;&#22312;&#32534;&#31243;&#20013;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#36807;&#31243;&#65292;&#36890;&#36807;&#35757;&#32451;LMs&#36981;&#24490;&#29992;&#25143;&#25351;&#20196;&#21644;&#20154;&#31867;&#20559;&#22909;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#24378;&#20102;LMs&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#26696;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#25351;&#20196;&#35843;&#20248;&#30340;LMs&#20063;&#32463;&#24120;&#20135;&#29983;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#65292;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SafeCoder&#26469;&#22635;&#34917;&#36825;&#20010;&#24046;&#36317;&#12290;SafeCoder&#20351;&#29992;&#19968;&#20010;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23433;&#20840;&#20026;&#20013;&#24515;&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#25910;&#38598;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;&#23433;&#20840;&#24494;&#35843;&#19982;&#26631;&#20934;&#30340;&#25351;&#20196;&#35843;&#20248;&#30456;&#32467;&#21512;&#65292;&#20197;&#20415;&#21516;&#26102;&#20248;&#21270;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;SafeCoder&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09497v1 Announce Type: cross  Abstract: Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.09303</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#30340;&#21363;&#26102;&#27010;&#25324;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28382;&#21518;&#27010;&#25324;&#8212;&#8212;&#34920;&#31034;&#20998;&#27495;&#30340;&#35777;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09303
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#23545;&#27604;&#20102;&#20154;&#31867;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#35768;&#22810;&#34892;&#20026;&#27604;&#36739;&#12290;&#36890;&#24120;&#65292;&#27604;&#36739;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#23398;&#20064;&#36807;&#31243;&#30340;&#26368;&#32456;&#32467;&#26524;&#65292;&#36890;&#36807;&#27979;&#37327;&#21644;&#27604;&#36739;&#30446;&#26631;&#31867;&#21035;&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#22914;&#20309;&#24418;&#25104;&#21363;&#20854;&#36807;&#31243;&#8212;&#8212;&#21363;&#22312;&#33719;&#21462;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#21464;&#21270;&#21644;&#20013;&#38388;&#38454;&#27573;&#8212;&#8212;&#24448;&#24448;&#23569;&#26377;&#30452;&#25509;&#21644;&#23454;&#35777;&#30340;&#27604;&#36739;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#21644;&#19981;&#21516;&#32463;&#20856;&#19982;&#26368;&#26032;&#25216;&#26415;&#30340;DNNs&#20013;&#21487;&#36716;&#31227;&#34920;&#31034;&#26159;&#22914;&#20309;&#34987;&#33719;&#21462;&#30340;&#30340;&#35814;&#32454;&#35843;&#26597;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#38480;&#30340;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#65292;&#35813;&#29615;&#22659;&#20013;&#25105;&#20204;&#23545;&#40784;&#20102;&#23398;&#20064;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#22914;&#36215;&#22987;&#28857;&#12289;&#36755;&#20837;&#27169;&#24335;&#12289;&#21487;&#29992;&#36755;&#20837;&#25968;&#25454;&#20197;&#21450;&#25552;&#20379;&#30340;&#21453;&#39304;&#12290;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#25105;&#20204;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09303v1 Announce Type: cross Abstract: Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06963</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;
&lt;/p&gt;
&lt;p&gt;
Tree Ensembles for Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#19978;&#20449;&#24515;&#30028;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#65292;&#25972;&#21512;&#21040;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#27969;&#34892;&#30340;&#26641;&#38598;&#25104;&#26041;&#27861;XGBoost&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#24403;&#24212;&#29992;&#20110;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#36947;&#36335;&#32593;&#32476;&#23548;&#33322;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#26102;&#65292;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for contextual multi-armed bandits based on tree ensembles. Our framework integrates two widely used bandit methods, Upper Confidence Bound and Thompson Sampling, for both standard and combinatorial settings. We demonstrate the effectiveness of our framework via several experimental studies, employing XGBoost, a popular tree ensemble method. Compared to state-of-the-art methods based on neural networks, our methods exhibit superior performance in terms of both regret minimization and computational runtime, when applied to benchmark datasets and the real-world application of navigation over road networks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#39564;&#35777;&#20102;GPT-4&#29983;&#25104;&#30340;&#21465;&#36848;&#22312;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#21465;&#36848;&#33021;&#22815;&#36275;&#22815;&#20256;&#36798;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#12290;</title><link>https://arxiv.org/abs/2402.05435</link><description>&lt;p&gt;
GPT-4&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#29983;&#25104;&#29983;&#27963;&#20107;&#20214;&#30340;&#21465;&#36848;&#65306;&#19968;&#39033;&#39564;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#39564;&#35777;&#20102;GPT-4&#29983;&#25104;&#30340;&#21465;&#36848;&#22312;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#21465;&#36848;&#33021;&#22815;&#36275;&#22815;&#20256;&#36798;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21508;&#31181;&#21465;&#36848;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20419;&#36827;&#20102;&#23545;&#20854;&#22312;&#21465;&#36848;&#24418;&#24335;&#20013;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#25928;&#26524;&#30340;&#31995;&#32479;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#38646;-shot&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#20351;&#29992;OpenAI&#30340;GPT-4&#29983;&#25104;&#20102;24,000&#20010;&#21465;&#36848;&#12290;&#20174;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#25163;&#21160;&#20998;&#31867;&#20102;2,880&#20010;&#21465;&#36848;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#20256;&#36798;&#20986;&#29983;&#12289;&#27515;&#20129;&#12289;&#25307;&#32856;&#21644;&#35299;&#38599;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;87.43%&#30340;&#21465;&#36848;&#36275;&#22815;&#20256;&#36798;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#20026;&#20102;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#65292;&#25105;&#20204;&#23545;&#20998;&#31867;&#25968;&#25454;&#38598;&#35757;&#32451;&#21644;&#39564;&#35777;&#20102;&#20061;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#23545;&#21097;&#20313;21,120&#20010;&#21465;&#36848;&#30340;&#20998;&#31867;&#39044;&#27979;&#20998;&#26512;&#12290;&#25152;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23558;&#26377;&#25928;&#30340;&#21465;&#36848;&#20998;&#31867;&#20026;&#26377;&#25928;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21516;&#26102;&#23558;&#26080;&#25928;&#30340;&#21465;&#36848;&#20998;&#31867;&#20026;&#26080;&#25928;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#20165;&#25512;&#36827;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36824;&#25552;&#20379;&#20102;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21465;&#36848;&#30340;&#26377;&#30410;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) play a pivotal role in generating vast arrays of narratives, facilitating a systematic exploration of their effectiveness for communicating life events in narrative form. In this study, we employ a zero-shot structured narrative prompt to generate 24,000 narratives using OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and evaluate their validity in conveying birth, death, hiring, and firing events. Remarkably, 87.43% of the narratives sufficiently convey the intention of the structured prompt. To automate the identification of valid and invalid narratives, we train and validate nine Machine Learning models on the classified datasets. Leveraging these models, we extend our analysis to predict the classifications of the remaining 21,120 narratives. All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid. Our findings not only advance th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#29615;&#22659;&#20013;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#21161;&#25163;&#21644;&#22522;&#20110;&#24847;&#22270;&#30340;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#21161;&#25163;&#22312;&#29992;&#25143;&#20307;&#39564;&#12289;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#21487;&#29992;&#24615;&#21644;&#20010;&#20154;&#25928;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04955</link><description>&lt;p&gt;
&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#29615;&#22659;&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65306;&#27604;&#36739;&#24847;&#22270;&#21644;&#22522;&#20110;LLM&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04955
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#29615;&#22659;&#20013;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#21161;&#25163;&#21644;&#22522;&#20110;&#24847;&#22270;&#30340;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#21161;&#25163;&#22312;&#29992;&#25143;&#20307;&#39564;&#12289;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#21487;&#29992;&#24615;&#21644;&#20010;&#20154;&#25928;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#21161;&#25163;&#26159;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#20026;&#20154;&#31867;&#24037;&#20316;&#32773;&#25552;&#20379;&#19978;&#19979;&#25991;&#24863;&#30693;&#25903;&#25345;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#20256;&#32479;&#19978;&#65292;&#35748;&#30693;&#21161;&#25163;&#20197;&#29305;&#23450;&#26041;&#24335;&#22238;&#24212;&#39044;&#23450;&#20041;&#30340;&#29992;&#25143;&#24847;&#22270;&#21644;&#23545;&#35805;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21018;&#24615;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#22810;&#26679;&#24615;&#22788;&#29702;&#19981;&#22909;&#12290;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#22914;GPT-4&#12289;Llama2&#21644;Gemini&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21487;&#33021;&#20351;&#35748;&#30693;&#21161;&#25163;&#33021;&#22815;&#20197;&#26356;&#28789;&#27963;&#12289;&#26356;&#20687;&#20154;&#31867;&#30340;&#26041;&#24335;&#36827;&#34892;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39069;&#22806;&#30340;&#33258;&#30001;&#24230;&#21487;&#33021;&#20250;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#21518;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#29615;&#22659;&#20013;&#12290;&#20026;&#20102;&#35780;&#20272;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#20351;&#29992;LLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#21161;&#25163;&#21644;&#22522;&#20110;&#24847;&#22270;&#30340;&#31995;&#32479;&#22312;&#20132;&#20114;&#25928;&#29575;&#12289;&#29992;&#25143;&#20307;&#39564;&#12289;&#24037;&#20316;&#37327;&#21644;&#21487;&#29992;&#24615;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#21161;&#25163;&#22312;&#29992;&#25143;&#20307;&#39564;&#12289;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#21487;&#29992;&#24615;&#21644;&#20010;&#20154;&#25928;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive assistants (CA) are chatbots that provide context-aware support to human workers in knowledge-intensive tasks. Traditionally, cognitive assistants respond in specific ways to predefined user intents and conversation patterns. However, this rigidness does not handle the diversity of natural language well. Recent advances in natural language processing (NLP), powering large language models (LLM) such as GPT-4, Llama2, and Gemini, could enable CAs to converse in a more flexible, human-like manner. However, the additional degrees of freedom may have unforeseen consequences, especially in knowledge-intensive contexts where accuracy is crucial. As a preliminary step to assessing the potential of using LLMs in these contexts, we conducted a user study comparing an LLM-based CA to an intent-based system regarding interaction efficiency, user experience, workload, and usability. This revealed that LLM-based CAs exhibited better user experience, task completion rate, usability, and per
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;AI&#27169;&#22411;&#30340;&#30830;&#20999;&#31639;&#27861;&#65292;&#26426;&#21046;&#35299;&#37322;&#24615;&#65288;MI&#65289;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24494;&#19981;&#36275;&#36947;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#32780;&#24573;&#35270;&#20102;&#38544;&#34255;&#22312;&#32593;&#32476;&#20869;&#37096;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#30028;&#26397;&#30528;&#26032;&#30340;&#26694;&#26550;&#21162;&#21147;&#65292;&#30740;&#31350;&#36825;&#20123;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03855</link><description>&lt;p&gt;
&#12298;&#23450;&#20301;&#35770;&#25991;&#65306;&#25506;&#32034;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#26032;&#26694;&#26550;&#12299;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Toward New Frameworks for Studying Model Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03855
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;AI&#27169;&#22411;&#30340;&#30830;&#20999;&#31639;&#27861;&#65292;&#26426;&#21046;&#35299;&#37322;&#24615;&#65288;MI&#65289;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24494;&#19981;&#36275;&#36947;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#32780;&#24573;&#35270;&#20102;&#38544;&#34255;&#22312;&#32593;&#32476;&#20869;&#37096;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#30028;&#26397;&#30528;&#26032;&#30340;&#26694;&#26550;&#21162;&#21147;&#65292;&#30740;&#31350;&#36825;&#20123;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#21046;&#35299;&#37322;&#24615;&#65288;MI&#65289;&#26088;&#22312;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;AI&#27169;&#22411;&#23398;&#20064;&#30340;&#30830;&#20999;&#31639;&#27861;&#26469;&#29702;&#35299;&#27169;&#22411;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;MI&#30740;&#31350;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#37117;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#21644;&#31526;&#21495;&#23545;&#40784;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33021;&#21147;&#24182;&#19981;&#37027;&#20040;&#24494;&#19981;&#36275;&#36947;&#65292;&#36825;&#20026;&#30740;&#31350;&#32593;&#32476;&#20869;&#37096;&#30340;&#38544;&#34255;&#34920;&#31034;&#20316;&#20026;&#20998;&#26512;&#21333;&#20301;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25991;&#29486;&#22238;&#39038;&#65292;&#23545;&#29305;&#24449;&#21644;&#34892;&#20026;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#30340;&#34920;&#31034;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#21644;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#22522;&#26412;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#35752;&#35770;&#21644;&#25506;&#32034;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30740;&#31350;&#34920;&#31034;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#24182;&#19988;&#30446;&#21069;MI&#20013;&#24314;&#31435;&#30340;&#26041;&#27861;&#19981;&#36275;&#20197;&#29702;&#35299;&#34920;&#31034;&#65292;&#22240;&#27492;&#25512;&#21160;&#30740;&#31350;&#30028;&#26397;&#30528;&#30740;&#31350;&#34920;&#31034;&#30340;&#26032;&#26694;&#26550;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations. With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22686;&#24378;Transformer RNNs&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#26368;&#23567;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.02625</link><description>&lt;p&gt;
&#29992;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#22686;&#24378;Transformer RNNs
&lt;/p&gt;
&lt;p&gt;
Enhancing Transformer RNNs with Multiple Temporal Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02625
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22686;&#24378;Transformer RNNs&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#26368;&#23567;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26550;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20854;&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#32500;&#25252;&#20808;&#21069;&#36935;&#21040;&#30340;&#25991;&#26412;&#30340;&#22810;&#26679;&#26102;&#38388;&#35270;&#22270;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#32435;&#20837;&#20102;Receptance Weighted Key Value&#65288;RWKV&#65289;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;&#35813;&#26550;&#26500;&#22312;&#21333;&#20010;&#38544;&#34255;&#29366;&#24577;&#20013;&#20445;&#30041;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#30340;&#22266;&#26377;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#26368;&#23569;&#65288;&#20165;&#20026;&#26368;&#21021;&#21442;&#25968;&#25968;&#37327;&#30340;0.04%&#65289;&#65292;&#20063;&#23454;&#29616;&#20102;&#27492;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#25152;&#38656;&#30340;&#39069;&#22806;&#21442;&#25968;&#32463;&#36807;&#24494;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#23436;&#20840;&#39044;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#22312;&#25552;&#31034;&#25512;&#26029;&#36807;&#31243;&#20013;&#20445;&#25345;&#20102;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data. This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context. To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state. Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\%$ of the original number of parameters. Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training. The resulting model maintains linear computational complexity during prompt inference, en
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22825;&#27668;&#39044;&#27979;&#30340;&#20892;&#19994;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#22269;&#20892;&#19994;&#38754;&#20020;&#30340;&#22825;&#27668;&#19981;&#21033;&#22240;&#32032;&#23545;&#31918;&#39135;&#29983;&#20135;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#30408;&#21033;&#12289;&#21487;&#25345;&#32493;&#21644;&#20892;&#27665;&#21451;&#22909;&#30340;&#20892;&#19994;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2401.11410</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20892;&#19994;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#22810;&#21464;&#37327;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Agricultural Recommendation System based on Deep Learning: A Multivariate Weather Forecasting Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11410
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22825;&#27668;&#39044;&#27979;&#30340;&#20892;&#19994;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#22269;&#20892;&#19994;&#38754;&#20020;&#30340;&#22825;&#27668;&#19981;&#21033;&#22240;&#32032;&#23545;&#31918;&#39135;&#29983;&#20135;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#30408;&#21033;&#12289;&#21487;&#25345;&#32493;&#21644;&#20892;&#27665;&#21451;&#22909;&#30340;&#20892;&#19994;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#22269;&#20027;&#35201;&#26159;&#19968;&#20010;&#20892;&#19994;&#22269;&#23478;&#65292;&#20892;&#19994;&#37096;&#38376;&#23545;&#20110;&#21152;&#24555;&#32463;&#27982;&#22686;&#38271;&#21644;&#20445;&#38556;&#20154;&#27665;&#31918;&#39135;&#23433;&#20840;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#23391;&#21152;&#25289;&#22269;&#21171;&#21160;&#23494;&#38598;&#22411;&#20892;&#19994;&#21462;&#24471;&#20102;&#31918;&#39135;&#20135;&#37327;&#31283;&#27493;&#22686;&#38271;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#19981;&#21033;&#22825;&#27668;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#22914;&#26292;&#38632;&#12289;&#20302;&#28201;&#21644;&#24178;&#26097;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22240;&#32032;&#20005;&#37325;&#24433;&#21709;&#20102;&#31918;&#39135;&#29983;&#20135;&#65292;&#20351;&#24471;&#22269;&#23478;&#30340;&#31918;&#39135;&#23433;&#20840;&#21463;&#21040;&#23041;&#32961;&#12290;&#20026;&#20102;&#23454;&#29616;&#30408;&#21033;&#12289;&#21487;&#25345;&#32493;&#19988;&#20892;&#27665;&#21451;&#22909;&#30340;&#20892;&#19994;&#23454;&#36341;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20316;&#29289;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11410v2 Announce Type: replace-cross  Abstract: Bangladesh is predominantly an agricultural country, where the agrarian sector plays an essential role in accelerating economic growth and enabling the food security of the people. The performance of this sector has an overwhelming impact on the primary macroeconomic objectives like food security, employment generation, poverty alleviation, human resources development, and other economic and social forces. Although Bangladesh's labor-intensive agriculture has achieved steady increases in food grain production, it often suffered from unfavorable weather conditions such as heavy rainfall, low temperature, and drought. Consequently, these factors hinder the production of food substantially, putting the country's overall food security in danger. In order to have a profitable, sustainable, and farmer-friendly agricultural practice, this paper proposes a context-based crop recommendation system powered by a weather forecast model. Wi
&lt;/p&gt;</description></item><item><title>DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.10158</link><description>&lt;p&gt;
DISTINQT: &#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#38544;&#31169;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks. (arXiv:2401.10158v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10158
&lt;/p&gt;
&lt;p&gt;
DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#21644;6G&#20197;&#21518;&#30340;&#32593;&#32476;&#23558;&#25903;&#25345;&#20381;&#36182;&#19968;&#23450;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#26032;&#30340;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29992;&#20363;&#21644;&#24212;&#29992;&#31243;&#24207;&#12290;&#21450;&#26102;&#39044;&#27979;QoS&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65288;&#22914;&#36710;&#36742;&#36890;&#20449;&#65289;&#23588;&#20026;&#37325;&#35201;&#12290;&#23613;&#31649;&#30452;&#21040;&#26368;&#36817;&#65292;QoS&#39044;&#27979;&#19968;&#30452;&#30001;&#38598;&#20013;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35299;&#20915;&#26041;&#26696;&#23436;&#25104;&#65292;&#20294;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#20123;&#38544;&#31169;&#12289;&#35745;&#31639;&#21644;&#36816;&#33829;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#26367;&#20195;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65288;&#22914;&#20998;&#21106;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#23558;&#22797;&#26434;&#24230;&#36739;&#20302;&#30340;AI&#20219;&#21153;&#20998;&#24067;&#22312;&#33410;&#28857;&#20043;&#38388;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#30340;&#24322;&#26500;&#24615;&#65292;&#24403;&#28041;&#21450;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#26102;&#65292;&#20250;&#20986;&#29616;&#26032;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISTINQT&#30340;&#38754;&#21521;QoS&#39044;&#27979;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have been surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a privacy-aware distributed learning framework for QoS prediction. Our framework supports mult
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-Net&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#20132;&#26367;&#21453;&#21521;&#20256;&#25773;&#26426;&#21046;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#23548;&#25968;&#20449;&#24687;&#21160;&#24577;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#22686;&#24378;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01772</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#35745;&#31639;&#33539;&#24335;&#65306;&#21487;&#23398;&#20064;&#31070;&#32463;&#20803;&#21644;&#21487;&#36866;&#24212;&#32467;&#26500;&#30340;X-Net
&lt;/p&gt;
&lt;p&gt;
A Novel Paradigm for Neural Computation: X-Net with Learnable Neurons and Adaptable Structure. (arXiv:2401.01772v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-Net&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#20132;&#26367;&#21453;&#21521;&#20256;&#25773;&#26426;&#21046;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#23548;&#25968;&#20449;&#24687;&#21160;&#24577;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#22686;&#24378;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#24050;&#32463;&#28183;&#36879;&#21040;&#21508;&#20010;&#23398;&#31185;&#39046;&#22495;&#65292;&#20174;&#29983;&#29289;&#20449;&#24687;&#23398;&#21040;&#37329;&#34701;&#20998;&#26512;&#65292;&#22312;&#24403;&#20195;&#31185;&#23398;&#30740;&#31350;&#20013;&#24050;&#32463;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#32593;&#32476;&#32467;&#26500;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#22266;&#26377;&#38480;&#21046;&#23548;&#33268;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;X-Net&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#35774;&#35745;&#30340;&#20132;&#26367;&#21453;&#21521;&#20256;&#25773;&#26426;&#21046;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#23548;&#25968;&#20449;&#24687;&#21160;&#24577;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#22686;&#24378;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks (ANNs) have permeated various disciplinary domains, ranging from bioinformatics to financial analytics, where their application has become an indispensable facet of contemporary scientific research endeavors. However, the inherent limitations of traditional neural networks arise due to their relatively fixed network structures and activation functions. 1, The type of activation function is single and relatively fixed, which leads to poor "unit representation ability" of the network, and it is often used to solve simple problems with very complex networks; 2, the network structure is not adaptive, it is easy to cause network structure redundant or insufficient. To address the aforementioned issues, this study proposes a novel neural network called X-Net. By utilizing our designed Alternating Backpropagation mechanism, X-Net dynamically selects appropriate activation functions based on derivative information during training to enhance the network's representati
&lt;/p&gt;</description></item><item><title>&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;&#35270;&#35273;&#32447;&#32034;&#24182;&#19982;&#24739;&#32773;&#20020;&#24202;&#29366;&#24577;&#20851;&#32852;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#23458;&#35266;&#21644;&#35814;&#32454;&#30340;&#30417;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00565</link><description>&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#26816;&#27979;&#35270;&#35273;&#32447;&#32034;&#19982;&#24739;&#32773;&#20020;&#24202;&#29366;&#24577;&#30340;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Detecting Visual Cues in the Intensive Care Unit and Association with Patient Clinical Status. (arXiv:2311.00565v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00565
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;&#35270;&#35273;&#32447;&#32034;&#24182;&#19982;&#24739;&#32773;&#20020;&#24202;&#29366;&#24577;&#20851;&#32852;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#23458;&#35266;&#21644;&#35814;&#32454;&#30340;&#30417;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#20026;&#24739;&#26377;&#29983;&#21629;&#23041;&#32961;&#30340;&#24739;&#32773;&#25552;&#20379;&#23494;&#20999;&#30417;&#25252;&#21644;&#36830;&#32493;&#25252;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#32773;&#30340;&#24037;&#20316;&#36127;&#33655;&#65292;ICU&#20013;&#30340;&#36830;&#32493;&#24739;&#32773;&#35780;&#20272;&#20173;&#28982;&#26377;&#38480;&#12290;&#29616;&#26377;&#30340;ICU&#24739;&#32773;&#35780;&#20272;&#65292;&#22914;&#30140;&#30171;&#25110;&#27963;&#21160;&#33021;&#21147;&#35780;&#20272;&#65292;&#22823;&#22810;&#26159;&#38646;&#25955;&#21644;&#25163;&#21160;&#23454;&#26045;&#30340;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#20154;&#20026;&#38169;&#35823;&#30340;&#28508;&#22312;&#21487;&#33021;&#24615;&#12290;&#24320;&#21457;&#33021;&#22815;&#22686;&#24378;ICU&#20013;&#20154;&#31867;&#35780;&#20272;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24037;&#20855;&#21487;&#20197;&#26377;&#21033;&#20110;&#25552;&#20379;&#26356;&#23458;&#35266;&#21644;&#35814;&#32454;&#30340;&#30417;&#27979;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#25429;&#25417;&#19982;&#30140;&#30171;&#25110;&#19981;&#23433;&#30456;&#20851;&#30340;&#24739;&#32773;&#38754;&#37096;&#32447;&#32034;&#30340;&#21464;&#21270;&#21487;&#20197;&#24110;&#21161;&#35843;&#25972;&#19982;&#30140;&#30171;&#30456;&#20851;&#30340;&#33647;&#29289;&#25110;&#26816;&#27979;&#21487;&#33021;&#24341;&#36215;&#19981;&#23433;&#30340;&#24773;&#20917;&#65292;&#22914;&#35893;&#22916;&#12290;&#27492;&#22806;&#65292;&#22312;&#19981;&#33391;&#20020;&#24202;&#20107;&#20214;&#21457;&#29983;&#26399;&#38388;&#25110;&#20043;&#21069;&#65292;&#35270;&#35273;&#32447;&#32034;&#30340;&#24494;&#22937;&#21464;&#21270;&#19982;&#39640;&#20998;&#36776;&#29575;&#29983;&#29702;&#20449;&#21495;&#30456;&#32467;&#21512;&#21487;&#33021;&#26377;&#21161;&#20110;&#36830;&#32493;&#24739;&#32773;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intensive Care Units (ICU) provide close supervision and continuous care to patients with life-threatening conditions. However, continuous patient assessment in the ICU is still limited due to time constraints and the workload on healthcare providers. Existing patient assessments in the ICU such as pain or mobility assessment are mostly sporadic and administered manually, thus introducing the potential for human errors. Developing Artificial intelligence (AI) tools that can augment human assessments in the ICU can be beneficial for providing more objective and granular monitoring capabilities. For example, capturing the variations in a patient's facial cues related to pain or agitation can help in adjusting pain-related medications or detecting agitation-inducing conditions such as delirium. Additionally, subtle changes in visual cues during or prior to adverse clinical events could potentially aid in continuous patient monitoring when combined with high-resolution physiological signal
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05227</link><description>&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#38761;&#21629;&#31185;&#23398;&#33539;&#24335;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#36807;&#31243;&#30340;&#27700;&#25991;&#23398;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology. (arXiv:2310.05227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05227
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#23545;&#20110;&#35299;&#20915;&#27700;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#31185;&#23398;&#21644;&#31038;&#20250;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#30340;&#21160;&#24577;&#24433;&#21709;&#19979;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#28982;&#32780;&#27700;&#25991;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#29420;&#31435;&#30340;&#33539;&#24335;&#23384;&#22312;&#26126;&#26174;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20197;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#36825;&#31181;&#35748;&#30693;&#38556;&#30861;&#65292;&#24182;&#38761;&#26032;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#35770;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#31038;&#21306;&#65288;PaML&#65289;&#65292;&#23558;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#25110;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#27169;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20174;&#29289;&#29702;&#25968;&#25454;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#20449;&#24687;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#24863;&#30693;&#28151;&#21512;&#23398;&#20064;&#22235;&#20010;&#26041;&#38754;&#20998;&#26512;&#20102;&#36825;&#20123;PaML&#26041;&#27861;&#12290;PaML&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20551;&#35774;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypothe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#36830;&#32493;&#25552;&#31034;&#20256;&#36882;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28304;&#25552;&#31034;&#32534;&#30721;&#21040;&#30456;&#23545;&#31354;&#38388;&#20013;&#65292;&#24182;&#25628;&#32034;&#30456;&#24212;&#30340;&#30446;&#26631;&#25552;&#31034;&#65292;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20102;&#20219;&#21153;&#35821;&#20041;&#30340;&#27867;&#21270;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01691</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#36830;&#32493;&#25552;&#31034;&#20256;&#36882;&#65306;&#22312;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#27867;&#21270;&#20219;&#21153;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models. (arXiv:2310.01691v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01691
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#36830;&#32493;&#25552;&#31034;&#20256;&#36882;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28304;&#25552;&#31034;&#32534;&#30721;&#21040;&#30456;&#23545;&#31354;&#38388;&#20013;&#65292;&#24182;&#25628;&#32034;&#30456;&#24212;&#30340;&#30446;&#26631;&#25552;&#31034;&#65292;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20102;&#20219;&#21153;&#35821;&#20041;&#30340;&#27867;&#21270;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#36890;&#36807;&#35843;&#25972;&#25552;&#31034;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#65292;&#29305;&#21035;&#26159;&#36830;&#32493;&#25552;&#31034;&#65292;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#36830;&#32493;&#25552;&#31034;&#20256;&#36882;&#26041;&#27861;&#65292;&#20854;&#20013;&#28304;&#25552;&#31034;&#34987;&#32534;&#30721;&#21040;&#30456;&#23545;&#31354;&#38388;&#20013;&#65292;&#24182;&#25628;&#32034;&#30456;&#24212;&#30340;&#30446;&#26631;&#25552;&#31034;&#20197;&#23558;&#20854;&#20256;&#36882;&#21040;&#30446;&#26631;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#36830;&#32493;&#25552;&#31034;&#20013;&#30340;&#8220;&#20219;&#21153;&#35821;&#20041;&#8221;&#21487;&#20197;&#22312;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#23558;&#26469;&#33258;&#22810;&#20010;&#28304;&#27169;&#22411;&#30340;&#8220;&#20219;&#21153;&#35821;&#20041;&#8221;&#32467;&#21512;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20256;&#36882;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning in natural language processing (NLP) has become an increasingly popular method for adapting large language models to specific tasks. However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge. In this work, we propose a zero-shot continuous prompt transfer method, where source prompts are encoded into relative space and the corresponding target prompts are searched for transferring to target models. Experimental results confirm the effectiveness of our method, showing that 'task semantics' in continuous prompts can be generalized across various language models. Moreover, we find that combining 'task semantics' from multiple source models can further enhance the generalizability of transfer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#32676;&#20307;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#26469;&#20943;&#36731;&#20840;&#23616;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24182;&#20445;&#25345;&#38544;&#31169;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07085</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#35774;&#22791;&#19978;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32676;&#20307;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Group Bias in Federated Learning for Heterogeneous Devices. (arXiv:2309.07085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#32676;&#20307;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#26469;&#20943;&#36731;&#20840;&#23616;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24182;&#20445;&#25345;&#38544;&#31169;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#27491;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#23853;&#38706;&#22836;&#35282;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36793;&#32536;&#37096;&#32626;&#26159;&#24322;&#26500;&#30340;&#65292;&#21363;&#23427;&#20204;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#29615;&#22659;&#22312;&#37096;&#32626;&#20013;&#21508;&#19981;&#30456;&#21516;&#12290;&#36825;&#31181;&#36793;&#32536;&#24322;&#26500;&#36829;&#21453;&#20102;&#26412;&#22320;&#25968;&#25454;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#29420;&#31435;&#19988;&#20998;&#24067;&#30456;&#21516; (IID) &#30340;&#29305;&#24615;&#65292;&#20135;&#29983;&#20102;&#26377;&#20559;&#35265;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#21363;&#23545;&#29305;&#23450;&#31038;&#21306;&#25110;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#21644;&#27495;&#35270;&#12290;&#29616;&#26377;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#21482;&#20851;&#27880;&#38750;IID&#25968;&#25454;&#20013;&#30001;&#26631;&#31614;&#24322;&#26500;&#24341;&#36215;&#30340;&#20559;&#35265;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#30001;&#29305;&#24449;&#24322;&#26500;&#23548;&#33268;&#30340;&#39046;&#22495;&#21464;&#21270;&#65292;&#20063;&#27809;&#26377;&#35299;&#20915;&#20840;&#23616;&#32676;&#20307;&#20844;&#24179;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#19981;&#22686;&#21152;&#36164;&#28304;&#21033;&#29992;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#32676;&#20307;&#20559;&#35265;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;&#24179;&#22343;&#26465;&#20214;&#27010;&#29575;&#26469;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients and produces biased global models i.e. models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity and do not address global group-fairness property.  Our work proposes a group-fair FL framework that minimizes group-bias while preserving privacy and without resource utilization overhead. Our main idea is to leverage average conditional probabilities to compute a cross-domain group \textit{importance we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#27979;&#35797;&#29983;&#25104;&#22120;&#65288;GenBo&#65289;&#65292;&#23427;&#36890;&#36807;&#22312;&#26080;&#25925;&#38556;&#29615;&#22659;&#20013;&#21464;&#24322;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39550;&#39542;&#26465;&#20214;&#26469;&#29983;&#25104;&#36793;&#30028;&#29366;&#24577;&#23545;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27979;&#35797;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10590</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#27979;&#35797;&#19982;&#25913;&#36827;&#30340;&#36793;&#30028;&#29366;&#24577;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Boundary State Generation for Testing and Improvement of Autonomous Driving Systems. (arXiv:2307.10590v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10590
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#27979;&#35797;&#29983;&#25104;&#22120;&#65288;GenBo&#65289;&#65292;&#23427;&#36890;&#36807;&#22312;&#26080;&#25925;&#38556;&#29615;&#22659;&#20013;&#21464;&#24322;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39550;&#39542;&#26465;&#20214;&#26469;&#29983;&#25104;&#36793;&#30028;&#29366;&#24577;&#23545;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27979;&#35797;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#20256;&#24863;&#22120;&#25216;&#26415;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65288;ADS&#65289;&#20855;&#26377;&#20102;&#36234;&#26469;&#36234;&#39640;&#30340;&#33258;&#20027;&#24615;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;ADS&#27979;&#35797;&#26041;&#27861;&#20462;&#25913;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#30340;&#21487;&#25511;&#23646;&#24615;&#65292;&#30452;&#21040;ADS&#20986;&#29616;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;1&#65289;&#23545;&#27169;&#25311;&#29615;&#22659;&#30340;&#20462;&#25913;&#21487;&#33021;&#19981;&#23481;&#26131;&#36716;&#31227;&#21040;&#23454;&#38469;&#27979;&#35797;&#29615;&#22659;&#65288;&#20363;&#22914;&#25913;&#21464;&#36947;&#36335;&#24418;&#29366;&#65289;&#65307;&#65288;2&#65289;&#21363;&#20351;ADS&#22312;&#26576;&#20123;&#29615;&#22659;&#20013;&#25104;&#21151;&#65292;&#36825;&#20123;&#29615;&#22659;&#23454;&#20363;&#20063;&#20250;&#34987;&#20002;&#24323;&#65292;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#21253;&#21547;ADS&#21487;&#33021;&#20986;&#29616;&#38382;&#39064;&#30340;&#28508;&#22312;&#39550;&#39542;&#26465;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ADS&#27979;&#35797;&#29983;&#25104;&#22120;&#8212;&#8212;GenBo&#65288;GENerator of BOundary state pairs&#65289;&#12290;GenBo&#22312;&#19968;&#20010;&#26080;&#25925;&#38556;&#29615;&#22659;&#23454;&#20363;&#20013;&#21464;&#24322;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39550;&#39542;&#26465;&#20214;&#65288;&#20301;&#32622;&#65292;&#36895;&#24230;&#21644;&#26041;&#21521;&#65289;&#65292;&#24182;&#26377;&#25928;&#22320;&#29983;&#25104;&#21487;&#36793;&#30028;&#21270;&#30340;&#29366;&#24577;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Deep Neural Networks (DNNs) and sensor technologies are enabling autonomous driving systems (ADSs) with an ever-increasing level of autonomy. However, assessing their dependability remains a critical concern. State-of-the-art ADS testing approaches modify the controllable attributes of a simulated driving environment until the ADS misbehaves. Such approaches have two main drawbacks: (1) modifications to the simulated environment might not be easily transferable to the in-field test setting (e.g., changing the road shape); (2) environment instances in which the ADS is successful are discarded, despite the possibility that they could contain hidden driving conditions in which the ADS may misbehave.  In this paper, we present GenBo (GENerator of BOundary state pairs), a novel test generator for ADS testing. GenBo mutates the driving conditions of the ego vehicle (position, velocity and orientation), collected in a failure-free environment instance, and efficiently gener
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.05284</link><description>&lt;p&gt;
&#20851;&#20110;&#38656;&#35201;&#25551;&#36848;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65306;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets. (arXiv:2307.05284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#20998;&#24067;&#20559;&#31227;&#38656;&#35201;&#19981;&#21516;&#30340;&#31639;&#27861;&#21644;&#25805;&#20316;&#24178;&#39044;&#12290;&#26041;&#27861;&#30740;&#31350;&#24517;&#39035;&#20197;&#20854;&#25152;&#28041;&#21450;&#30340;&#20855;&#20307;&#20559;&#31227;&#20026;&#22522;&#30784;&#12290;&#23613;&#31649;&#26032;&#20852;&#30340;&#22522;&#20934;&#25968;&#25454;&#20026;&#23454;&#35777;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#38544;&#21547;&#22320;&#20851;&#27880;&#21327;&#21464;&#37327;&#20559;&#31227;&#65292;&#24182;&#19988;&#23454;&#35777;&#21457;&#29616;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20559;&#31227;&#31867;&#22411;&#65292;&#20363;&#22914;&#65292;&#24403;$Y|X$&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#20043;&#21069;&#20851;&#20110;&#31639;&#27861;&#24615;&#33021;&#30340;&#35266;&#23519;&#21487;&#33021;&#26080;&#25928;&#12290;&#25105;&#20204;&#23545;5&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;86,000&#20010;&#27169;&#22411;&#37197;&#32622;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#19968;&#31181;&#31934;&#32454;&#30340;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;WhyShift&#65292;&#19968;&#20010;&#30001;&#31574;&#21010;&#30340;&#30495;&#23454;&#19990;&#30028;&#20559;&#31227;&#27979;&#35797;&#24179;&#21488;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#23545;&#25105;&#20204;&#22522;&#20934;&#24615;&#33021;&#30340;&#20559;&#31227;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#30001;&#20110;$Y|X$-&#20559;&#31227;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#24456;&#24120;&#35265;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21463;&#21040;&#26368;&#22823;$Y|X$-&#20559;&#31227;&#24433;&#21709;&#30340;&#21327;&#21464;&#37327;&#21306;&#22495;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithm
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25216;&#33021;&#31579;&#36873;&#19982;&#20248;&#21270;&#30340;Skill-Critic&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#19979;&#24378;&#21270;&#23398;&#20064;&#20013;&#20302;&#23618;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08388</link><description>&lt;p&gt;
Skill-Critic: &#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#25216;&#33021;&#30340;&#31579;&#36873;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Skill-Critic: Refining Learned Skills for Reinforcement Learning. (arXiv:2306.08388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08388
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#31579;&#36873;&#19982;&#20248;&#21270;&#30340;Skill-Critic&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#19979;&#24378;&#21270;&#23398;&#20064;&#20013;&#20302;&#23618;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#26102;&#38388;&#25277;&#35937;&#23558;&#19968;&#20010;&#31574;&#30053;&#20998;&#20026;&#22810;&#20010;&#23618;&#27425;&#65292;&#21152;&#24555;&#38271;&#26399;&#20915;&#31574;&#30340;&#36895;&#24230;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#29615;&#22659;&#20013;&#65292;&#25216;&#33021;&#21363;&#21407;&#22987;&#21160;&#20316;&#30340;&#24207;&#21015;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#26377;&#26395;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#25216;&#33021;&#30340;&#28508;&#22312;&#31354;&#38388;&#21644;&#31574;&#30053;&#26159;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#65292;&#20294;&#30001;&#20110;&#28436;&#31034;&#35206;&#30422;&#33539;&#22260;&#20302;&#25110;&#20998;&#24067;&#36716;&#31227;&#65292;&#25152;&#24471;&#21040;&#30340;&#20302;&#23618;&#31574;&#30053;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Fine-tuning&#20302;&#23618;&#31574;&#30053;&#19982;&#39640;&#23618;&#25216;&#33021;&#36873;&#25321;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;Skill-Critic&#31639;&#27861;&#20248;&#21270;&#20102;&#20302;&#23618;&#21644;&#39640;&#23618;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20174;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#21021;&#22987;&#21270;&#21644;&#35268;&#33539;&#21270;&#65292;&#20197;&#24341;&#23548;&#32852;&#21512;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#31232;&#30095;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Gran Turismo Sport&#20013;&#26032;&#30340;&#31232;&#30095;&#22870;&#21169;&#33258;&#20027;&#36187;&#36710;&#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Skill-Critic&#30340;&#20302;&#23618;&#31574;&#30053;Fine-tuning&#21644;&#28436;&#31034;&#24341;&#23548;&#31574;&#30053;&#21021;&#22987;&#21270;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning (RL) can accelerate long-horizon decision-making by temporally abstracting a policy into multiple levels. Promising results in sparse reward environments have been seen with skills, i.e. sequences of primitive actions. Typically, a skill latent space and policy are discovered from offline data, but the resulting low-level policy can be unreliable due to low-coverage demonstrations or distribution shifts. As a solution, we propose fine-tuning the low-level policy in conjunction with high-level skill selection. Our Skill-Critic algorithm optimizes both the low and high-level policies; these policies are also initialized and regularized by the latent space learned from offline demonstrations to guide the joint policy optimization. We validate our approach in multiple sparse RL environments, including a new sparse reward autonomous racing task in Gran Turismo Sport. The experiments show that Skill-Critic's low-level policy fine-tuning and demonstration-g
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#22312;&#22522;&#20110;&#22768;&#38899;&#30340;&#21333;&#35789;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#33258;&#21457;&#36830;&#25509;&#20004;&#20010;&#25110;&#19977;&#20010;&#21333;&#35789;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20250;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#25105;&#20204;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#24335;&#21644;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#37117;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.01626</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#30340;&#22522;&#30784;&#35821;&#27861;&#65306;&#33258;&#21457;&#32852;&#25509;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks. (arXiv:2305.01626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01626
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#22312;&#22522;&#20110;&#22768;&#38899;&#30340;&#21333;&#35789;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#33258;&#21457;&#36830;&#25509;&#20004;&#20010;&#25110;&#19977;&#20010;&#21333;&#35789;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20250;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#25105;&#20204;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#24335;&#21644;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#37117;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#30340;&#35745;&#31639;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#25991;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#26368;&#26222;&#36941;&#21644;&#22522;&#26412;&#30340;&#35821;&#27861;&#29305;&#24615;&#20043;&#19968;&#8212;&#8212;&#32852;&#25509;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21457;&#32852;&#25509;&#29616;&#35937;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#20010;&#21035;&#21333;&#35789;&#30340;&#22768;&#23398;&#35760;&#24405;&#19978;&#35757;&#32451;&#26102;&#65292;&#24320;&#22987;&#20135;&#29983;&#36755;&#20986;&#65292;&#36825;&#20123;&#36755;&#20986;&#23558;&#20004;&#20010;&#29978;&#33267;&#19977;&#20010;&#21333;&#35789;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#32780;&#19981;&#20250;&#25509;&#35302;&#21040;&#20855;&#26377;&#22810;&#20010;&#21333;&#35789;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#20004;&#20010;&#21333;&#35789;&#30340;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29615;&#22659;&#19979;&#35757;&#32451;&#30340;&#21407;&#22987;&#35821;&#38899;CNN&#20197;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#23427;&#19981;&#20165;&#23545;&#25105;&#20204;&#29702;&#35299;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#23398;&#20064;&#26041;&#24335;&#26377;&#24433;&#21709;&#65292;&#36824;&#23545;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational models of syntax are predominantly text-based. Here we propose that basic syntax can be modeled directly from raw speech in a fully unsupervised way. We focus on one of the most ubiquitous and basic properties of syntax -- concatenation. We introduce spontaneous concatenation: a phenomenon where convolutional neural networks (CNNs) trained on acoustic recordings of individual words start generating outputs with two or even three words concatenated without ever accessing data with multiple words in the input. Additionally, networks trained on two words learn to embed words into novel unobserved word combinations. To our knowledge, this is a previously unreported property of CNNs trained on raw speech in the Generative Adversarial Network setting and has implications both for our understanding of how these architectures learn as well as for modeling syntax and its evolution from raw acoustic inputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;AI&#22686;&#24378;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#31995;&#32479;&#65292;&#36890;&#36807;&#26222;&#36866;&#24863;&#30693;&#21644;&#25968;&#25454;&#22788;&#29702;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#30340;&#35270;&#35273;&#30417;&#27979;&#21644;&#35780;&#20272;&#65292;&#25552;&#39640;&#25252;&#29702;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06252</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#65306;&#36890;&#36807;&#26222;&#36866;&#24863;&#30693;&#38761;&#21629;&#24615;&#22320;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with Pervasive Sensing. (arXiv:2303.06252v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;AI&#22686;&#24378;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#31995;&#32479;&#65292;&#36890;&#36807;&#26222;&#36866;&#24863;&#30693;&#21644;&#25968;&#25454;&#22788;&#29702;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#30340;&#35270;&#35273;&#30417;&#27979;&#21644;&#35780;&#20272;&#65292;&#25552;&#39640;&#25252;&#29702;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an AI-enhanced ICU system that improves patient visual monitoring and assessment, and ultimately enhances the quality of care, through pervasive sensing and data processing.
&lt;/p&gt;
&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#26159;&#19968;&#20010;&#19987;&#38376;&#30340;&#21307;&#38498;&#31354;&#38388;&#65292;&#29992;&#20110;&#25509;&#21463;&#21361;&#37325;&#30149;&#20154;&#30340;&#23494;&#38598;&#25252;&#29702;&#21644;&#30417;&#27979;&#12290;&#20840;&#38754;&#30340;&#30417;&#27979;&#23545;&#20110;&#35780;&#20272;&#24739;&#32773;&#30340;&#30149;&#24773;&#65292;&#29305;&#21035;&#26159;&#30149;&#24773;&#30340;&#20005;&#37325;&#31243;&#24230;&#20197;&#21450;&#26368;&#32456;&#30340;&#25252;&#29702;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#21307;&#25252;&#20154;&#21592;&#30340;&#24037;&#20316;&#37327;&#65292;ICU&#20013;&#30340;&#24739;&#32773;&#30417;&#27979;&#33539;&#22260;&#21463;&#21040;&#38480;&#21046;&#12290;&#30446;&#21069;&#65292;&#21253;&#25324;&#38754;&#37096;&#34920;&#24773;&#12289;&#23039;&#21183;&#21644;&#27963;&#21160;&#33021;&#21147;&#31561;&#32454;&#33410;&#30340;&#35270;&#35273;&#35780;&#20272;&#20165;&#20598;&#23572;&#34987;&#25429;&#25417;&#21040;&#65292;&#25110;&#32773;&#26681;&#26412;&#27809;&#26377;&#34987;&#25429;&#25417;&#21040;&#12290;&#36825;&#20123;&#25163;&#21160;&#35266;&#23519;&#26159;&#20027;&#35266;&#30340;&#65292;&#23481;&#26131;&#20986;&#29616;&#25991;&#26723;&#38169;&#35823;&#65292;&#24182;&#32473;&#25252;&#29702;&#20154;&#21592;&#24102;&#26469;&#39069;&#22806;&#30340;&#24037;&#20316;&#37327;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#28508;&#21147;&#22686;&#24378;&#24739;&#32773;&#30340;&#35270;&#35273;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;&#36825;&#26679;&#30340;&#31995;&#32479;&#38656;&#35201;&#24378;&#22823;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26222;&#36866;&#24863;&#30693;&#21644;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intensive care unit (ICU) is a specialized hospital space where critically ill patients receive intensive care and monitoring. Comprehensive monitoring is imperative in assessing patients conditions, in particular acuity, and ultimately the quality of care. However, the extent of patient monitoring in the ICU is limited due to time constraints and the workload on healthcare providers. Currently, visual assessments for acuity, including fine details such as facial expressions, posture, and mobility, are sporadically captured, or not captured at all. These manual observations are subjective to the individual, prone to documentation errors, and overburden care providers with the additional workload. Artificial Intelligence (AI) enabled systems has the potential to augment the patient visual monitoring and assessment due to their exceptional learning capabilities. Such systems require robust annotated data to train. To this end, we have developed pervasive sensing and data processing s
&lt;/p&gt;</description></item><item><title>SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;</title><link>http://arxiv.org/abs/2301.11329</link><description>&lt;p&gt;
SynthMorph&#23454;&#29616;&#30340;&#32771;&#34385;&#35299;&#21078;&#32467;&#26500;&#21644;&#26080;&#20851;&#37319;&#38598;&#26041;&#27861;&#30340;&#32852;&#21512;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Anatomy-aware and acquisition-agnostic joint registration with SynthMorph. (arXiv:2301.11329v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11329
&lt;/p&gt;
&lt;p&gt;
SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#23556;&#22270;&#20687;&#37197;&#20934;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#22522;&#30707;&#12290;&#34429;&#28982;&#20256;&#32479;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#20248;&#31168;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#20026;&#27599;&#19968;&#23545;&#22270;&#20687;&#36827;&#34892;&#32791;&#26102;&#30340;&#20248;&#21270;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23558;&#22270;&#20687;&#23545;&#26144;&#23556;&#21040;&#36755;&#20986;&#21464;&#25442;&#30340;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35780;&#20272;&#36825;&#20010;&#20989;&#25968;&#26159;&#24555;&#36895;&#30340;&#65292;&#20294;&#25429;&#25417;&#22823;&#30340;&#21464;&#25442;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#19988;&#22914;&#26524;&#27979;&#35797;&#22270;&#20687;&#30340;&#29305;&#24449;&#20174;&#35757;&#32451;&#39046;&#22495;&#21464;&#21270;&#65292;&#22914;&#20998;&#36776;&#29575;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#20986;&#29616;&#22256;&#38590;&#12290;&#22823;&#22810;&#25968;&#20223;&#23556;&#26041;&#27861;&#26159;&#23545;&#35299;&#21078;&#32467;&#26500;&#26080;&#30693;&#30340;&#65292;&#24847;&#21619;&#30528;&#22914;&#26524;&#31639;&#27861;&#32771;&#34385;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#32467;&#26500;&#65292;&#37197;&#20934;&#20250;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#36890;&#36807;SynthMorph&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#23427;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#36827;&#34892;&#25805;&#20316;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#30340;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#30340;&#22270;&#20687;&#26469;&#35757;&#32451;&#32593;&#32476;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#22810;&#26679;&#21270;&#37319;&#38598;&#35268;&#33539;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20248;&#21270;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#32771;&#34385;&#19981;&#21516;&#30340;&#35299;&#21078;&#29305;&#24449;&#21644;&#23398;&#20064;&#25269;&#21046;&#37319;&#38598;&#29305;&#23450;&#38480;&#21046;&#30340;&#21464;&#25442;&#12290;&#36890;&#36807;&#36825;&#20123;&#21019;&#26032;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affine image registration is a cornerstone of medical-image analysis. While classical algorithms can achieve excellent accuracy, they solve a time-consuming optimization for every image pair. Deep-learning (DL) methods learn a function that maps an image pair to an output transform. Evaluating the function is fast, but capturing large transforms can be challenging, and networks tend to struggle if a test-image characteristic shifts from the training domain, such as resolution. Most affine methods are agnostic to anatomy, meaning the registration will be inaccurate if algorithms consider all structures in the image.  We address these shortcomings with SynthMorph, an easy-to-use DL tool for joint affine-deformable registration of any brain image without preprocessing, right off the MRI scanner. First, we leverage a strategy to train networks with wildly varying images synthesized from label maps, yielding robust performance across acquisition specifics unseen at training. Second, we opti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#20013;&#30340;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#30456;&#20851;&#25552;&#31034;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#33719;&#21462;&#29305;&#23450;&#22270;&#20687;&#29305;&#24449;&#24182;&#24341;&#20837;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.03919</link><description>&lt;p&gt;
CLIP-PAE&#65306;&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#20197;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#29992;&#20110;&#21487;&#20998;&#31163;&#12289;&#21487;&#35299;&#37322;&#12289;&#21487;&#25511;&#30340;&#25991;&#26412;&#25351;&#23548;&#33080;&#37096;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation. (arXiv:2210.03919v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#20013;&#30340;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#30456;&#20851;&#25552;&#31034;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#33719;&#21462;&#29305;&#23450;&#22270;&#20687;&#29305;&#24449;&#24182;&#24341;&#20837;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#21040;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#36825;&#25171;&#24320;&#20102;&#19968;&#20010;&#22823;&#38376;&#65292;&#21363;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#25991;&#23383;&#35828;&#26126;&#26469;&#25805;&#20316;&#36755;&#20837;&#22270;&#20687;&#30340;&#20016;&#23500;&#25991;&#23398;&#36164;&#26009;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32852;&#21512;&#31354;&#38388;&#20013;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#23558;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#36890;&#24120;&#20250;&#23548;&#33268;&#32467;&#26524;&#22270;&#20687;&#20013;&#20986;&#29616;&#24847;&#22806;&#30340;&#20266;&#24433;&#12290;&#23545;&#20110;&#25805;&#32437;&#26469;&#35828;&#65292;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#20063;&#24456;&#38590;&#20445;&#35777;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23450;&#20041;&#30001;&#30456;&#20851;&#25552;&#31034;&#23637;&#24320;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#25429;&#33719;&#29305;&#23450;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#33539;&#20363;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#35745;&#31639;&#21644;&#36866;&#24212;&#65292;&#24182;&#24179;&#31283;&#22320;&#34701;&#20837;&#21040;&#20219;&#20309;&#22522;&#20110;CLIP&#30340;&#22270;&#20687;&#25805;&#20316;&#31639;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges images and text by embedding them into a joint latent space. This opens the door to ample literature that aims to manipulate an input image by providing a textual explanation. However, due to the discrepancy between image and text embeddings in the joint space, using text embeddings as the optimization target often introduces undesired artifacts in the resulting images. Disentanglement, interpretability, and controllability are also hard to guarantee for manipulation. To alleviate these problems, we propose to define corpus subspaces spanned by relevant prompts to capture specific image characteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as an optimization target to improve the performance of text-guided image manipulation. Our method is a simple and general paradigm that can be easily computed and adapted, and smoothly incorporated into any CLIP-based image manipulation algorithm. To demo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21457;&#32946;Braitenberg Vehicles&#20195;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21457;&#32946;&#21551;&#21457;&#30340;&#23398;&#20064;&#20195;&#29702;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#33258;&#20027;&#24615;&#30340;&#20855;&#36523;&#32463;&#39564;&#21644;&#24418;&#24577;&#21457;&#29983;&#22686;&#38271;&#27169;&#25311;&#65292;&#24182;&#32771;&#34385;&#20102;&#21457;&#32946;&#36712;&#36857;&#22312;&#31070;&#32463;&#31995;&#32479;&#24418;&#24577;&#29983;&#25104;&#12289;&#21457;&#32946;&#23398;&#20064;&#21644;&#21487;&#22609;&#24615;&#31561;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2103.05753</link><description>&lt;p&gt;
&#25345;&#32493;&#21457;&#23637;&#30340;&#31070;&#32463;&#20223;&#30495;&#65306;&#22522;&#20110;&#20855;&#36523;&#35745;&#31639;&#20195;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continual Developmental Neurosimulation Using Embodied Computational Agents. (arXiv:2103.05753v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.05753
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21457;&#32946;Braitenberg Vehicles&#20195;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21457;&#32946;&#21551;&#21457;&#30340;&#23398;&#20064;&#20195;&#29702;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#33258;&#20027;&#24615;&#30340;&#20855;&#36523;&#32463;&#39564;&#21644;&#24418;&#24577;&#21457;&#29983;&#22686;&#38271;&#27169;&#25311;&#65292;&#24182;&#32771;&#34385;&#20102;&#21457;&#32946;&#36712;&#36857;&#22312;&#31070;&#32463;&#31995;&#32479;&#24418;&#24577;&#29983;&#25104;&#12289;&#21457;&#32946;&#23398;&#20064;&#21644;&#21487;&#22609;&#24615;&#31561;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#21457;&#32946;&#29983;&#29289;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#35745;&#31639;&#24314;&#27169;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#24456;&#22810;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#22522;&#20110;Braitenberg Vehicles&#35774;&#35745;&#24320;&#21457;&#21463;&#21457;&#32946;&#21551;&#21457;&#30340;&#23398;&#20064;&#20195;&#29702;&#12290;&#21033;&#29992;&#36825;&#20123;&#20195;&#29702;&#20307;&#29616;&#20102;&#35745;&#31639;&#33258;&#20027;&#24615;&#30340;&#20855;&#36523;&#29305;&#24615;&#65292;&#19981;&#26029;&#38752;&#36817;&#23545;&#20855;&#36523;&#32463;&#39564;&#21644;&#24418;&#24577;&#21457;&#29983;&#22686;&#38271;&#20316;&#20026;&#35748;&#30693;&#21457;&#23637;&#33021;&#21147;&#32452;&#25104;&#37096;&#20998;&#30340;&#24314;&#27169;&#12290;&#25105;&#20204;&#32771;&#34385;&#29983;&#29289;&#21644;&#35748;&#30693;&#21457;&#23637;&#23545;&#25104;&#24180;&#34920;&#22411;&#29983;&#25104;&#21644;&#21487;&#29992;&#21457;&#23637;&#36335;&#24452;&#30340;&#24433;&#21709;&#12290;&#25345;&#32493;&#21457;&#23637;&#31070;&#32463;&#20223;&#30495;&#20351;&#25105;&#20204;&#33021;&#22815;&#32771;&#34385;&#21457;&#32946;&#36712;&#36857;&#22312;&#36830;&#25509;&#31070;&#32463;&#31995;&#32479;&#24418;&#24577;&#21457;&#29983;&#12289;&#21457;&#32946;&#23398;&#20064;&#21644;&#21487;&#22609;&#24615;&#31561;&#30456;&#20851;&#29616;&#35937;&#20013;&#25152;&#36215;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#19982;&#25345;&#32493;&#23398;&#20064;&#32039;&#23494;&#30456;&#20851;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#21457;&#32946;&#20855;&#36523;&#32039;&#23494;&#38598;&#25104;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#21457;&#32946;Braitenberg Vehicles (dBVs)&#30340;&#20195;&#29702;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is much to learn through synthesis of Developmental Biology, Cognitive Science and Computational Modeling. Our path forward is to present a design for developmentally-inspired learning agents based on Braitenberg Vehicles. Using these agents to exemplify the embodied nature of computational autonomy, we move closer to modeling embodied experience and morphogenetic growth as components of cognitive developmental capacity. We consider biological and cognitive development which influence the generation of adult phenotypes and the contingency of available developmental pathways. Continual developmental neurosimulation allows us to consider the role of developmental trajectories in bridging the related phenomena of nervous system morphogenesis, developmental learning, and plasticity. Being closely tied to continual learning, our approach is tightly integrated with developmental embodiment, and can be implemented using a type of agent called developmental Braitenberg Vehicles (dBVs). T
&lt;/p&gt;</description></item></channel></rss>