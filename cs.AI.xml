<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#20123;&#26080;&#20559;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20381;&#36182;&#31639;&#27861;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#19988;&#36825;&#31181;&#29305;&#23450;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#20219;&#20309;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.14200</link><description>&lt;p&gt;
&#25163;&#26415;&#21592;&#21435;&#20559;&#35265;&#65306;&#31070;&#22855;&#30340;&#26435;&#37325;&#21450;&#22914;&#20309;&#25214;&#21040;&#23427;&#20204;
&lt;/p&gt;
&lt;p&gt;
Debiasing surgeon: fantastic weights and how to find them
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14200
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#20123;&#26080;&#20559;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20381;&#36182;&#31639;&#27861;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#19988;&#36825;&#31181;&#29305;&#23450;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#20219;&#20309;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#19968;&#20010;&#26085;&#30410;&#20851;&#27880;&#30340;&#29616;&#35937;&#26159;&#31639;&#27861;&#20559;&#35265;&#30340;&#20986;&#29616;&#65292;&#23427;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21435;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#26356;&#25110;&#22810;&#25110;&#23569;&#22797;&#26434;&#30340;&#26041;&#27861;&#26469;&#38459;&#27490;&#36825;&#20123;&#27169;&#22411;&#22823;&#35268;&#27169;&#22320;&#20351;&#29992;&#36825;&#20123;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#36825;&#31181;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#30495;&#30340;&#26377;&#24517;&#35201;&#21527;&#65311;&#19968;&#20010;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#21542;&#24050;&#32463;&#21253;&#21547;&#20102;&#19968;&#20123;&#21487;&#20197;&#29420;&#31435;&#20351;&#29992;&#30340;&#8220;&#26080;&#20559;&#23376;&#32593;&#32476;&#8221;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20986;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#32780;&#19981;&#20381;&#36182;&#20110;&#31639;&#27861;&#20559;&#35265;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#23376;&#32593;&#32476;&#36890;&#24120;&#23384;&#22312;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#19968;&#20010;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#20986;&#26469;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#31181;&#29305;&#23450;&#30340;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#35265;&#65292;&#34920;&#26126;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#21487;&#33021;&#36890;&#36807;&#26550;&#26500;&#19978;&#30340;&#23545;&#31574;&#26469;&#35299;&#20915;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14200v1 Announce Type: cross  Abstract: Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic biases that can lead to unfair models, emerges. Several debiasing approaches have been proposed in the realm of deep learning, employing more or less sophisticated approaches to discourage these models from massively employing these biases. However, a question emerges: is this extra complexity really necessary? Is a vanilla-trained model already embodying some ``unbiased sub-networks'' that can be used in isolation and propose a solution without relying on the algorithmic biases? In this work, we show that such a sub-network typically exists, and can be extracted from a vanilla-trained model without requiring additional training. We further validate that such specific architecture is incapable of learning a specific bias, suggesting that there are possible architectural countermeasures to the problem of biases in deep neural networks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#25512;&#29702;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;V-HOI Multi-LLMs Collaborated Reasoning&#65288;V-HOI MLCR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24403;&#21069;V-HOI&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10107</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;LLM&#21512;&#20316;&#25512;&#29702;&#25552;&#21319;&#20154;&#31867;&#20013;&#24515;&#21160;&#24577;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10107
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#25512;&#29702;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;V-HOI Multi-LLMs Collaborated Reasoning&#65288;V-HOI MLCR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24403;&#21069;V-HOI&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#30340;&#21160;&#24577;&#22330;&#26223;&#29702;&#35299;&#22312;&#22686;&#24378;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#33021;&#21147;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20854;&#20013;&#35270;&#39057;&#20154;-&#29289;&#20132;&#20114;&#65288;V-HOI&#65289;&#26816;&#27979;&#26159;&#35821;&#20041;&#22330;&#26223;&#29702;&#35299;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#20840;&#38754;&#29702;&#35299;&#35270;&#39057;&#20013;&#30340;HOI&#20851;&#31995;&#65292;&#20197;&#20351;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#34892;&#20026;&#20915;&#31574;&#21463;&#30410;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;V-HOI&#26816;&#27979;&#27169;&#22411;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#20687;&#20154;&#31867;&#19968;&#26679;&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#65292;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;HOI&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;V-HOI&#22810;LLM&#21327;&#21516;&#25512;&#29702;&#65288;V-HOI MLCR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#30001;&#19968;&#31995;&#21015;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#29616;&#25104;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#20419;&#36827;&#24403;&#21069;V-HOI&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10107v1 Announce Type: cross  Abstract: Human-centered dynamic scene understanding plays a pivotal role in enhancing the capability of robotic and autonomous systems, in which Video-based Human-Object Interaction (V-HOI) detection is a crucial task in semantic scene understanding, aimed at comprehensively understanding HOI relationships within a video to benefit the behavioral decisions of mobile robots and autonomous driving systems. Although previous V-HOI detection models have made significant strides in accurate detection on specific datasets, they still lack the general reasoning ability like human beings to effectively induce HOI relationships. In this study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a novel framework consisting of a series of plug-and-play modules that could facilitate the performance of current V-HOI detection models by leveraging the strong reasoning ability of different off-the-shelf pre-trained large language models (LLMs). 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shortcuts-fused Selective Rationalization (SSR)&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#26469;&#25552;&#21319;&#29702;&#24615;&#21270;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#32531;&#35299;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#26469;&#32452;&#25104;&#29702;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#34917;&#20805;&#24050;&#27880;&#37322;&#29702;&#24615;&#21270;&#25968;&#37327;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07955</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#35299;&#37322;&#65306;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#21457;&#29616;&#26469;&#22686;&#24378;&#29702;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shortcuts-fused Selective Rationalization (SSR)&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#26469;&#25552;&#21319;&#29702;&#24615;&#21270;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#32531;&#35299;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#26469;&#32452;&#25104;&#29702;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#34917;&#20805;&#24050;&#27880;&#37322;&#29702;&#24615;&#21270;&#25968;&#37327;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#31070;&#32463;&#32593;&#32476;&#30340;&#26174;&#33879;&#25104;&#21151;&#24341;&#21457;&#20102;&#26377;&#36873;&#25321;&#24615;&#30340;&#29702;&#24615;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shortcuts-fused Selective Rationalization (SSR)&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#26469;&#25552;&#21319;&#29702;&#24615;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SSR&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#24555;&#25463;&#26041;&#24335;&#21457;&#29616;&#26041;&#27861;&#26469;&#26816;&#27979;&#20960;&#20010;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#24341;&#20837;&#35782;&#21035;&#20986;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#32531;&#35299;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#26469;&#32452;&#25104;&#29702;&#24615;&#21270;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#24357;&#34917;&#24050;&#27880;&#37322;&#29702;&#24615;&#21270;&#25968;&#37327;&#30340;&#24046;&#36317;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#28165;&#26970;&#22320;&#39564;&#35777;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07955v1 Announce Type: cross  Abstract: The remarkable success in neural networks provokes the selective rationalization. It explains the prediction results by identifying a small subset of the inputs sufficient to support them. Since existing methods still suffer from adopting the shortcuts in data to compose rationales and limited large-scale annotated rationales by human, in this paper, we propose a Shortcuts-fused Selective Rationalization (SSR) method, which boosts the rationalization by discovering and exploiting potential shortcuts. Specifically, SSR first designs a shortcuts discovery approach to detect several potential shortcuts. Then, by introducing the identified shortcuts, we propose two strategies to mitigate the problem of utilizing shortcuts to compose rationales. Finally, we develop two data augmentations methods to close the gap in the number of annotated rationales. Extensive experimental results on real-world datasets clearly validate the effectiveness of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22330;&#26223;&#22270;&#39044;&#27979;&#65288;SGA&#65289;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;SceneSayer&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;ODE&#21644;&#31070;&#32463;SDE&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#23545;&#35937;-centric&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#23454;&#29616;&#23545;&#35937;&#20043;&#38388;&#26410;&#26469;&#20851;&#31995;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.04899</link><description>&lt;p&gt;
&#26397;&#21521;&#22330;&#26223;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Scene Graph Anticipation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04899
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22330;&#26223;&#22270;&#39044;&#27979;&#65288;SGA&#65289;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;SceneSayer&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;ODE&#21644;&#31070;&#32463;SDE&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#23545;&#35937;-centric&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#23454;&#29616;&#23545;&#35937;&#20043;&#38388;&#26410;&#26469;&#20851;&#31995;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22330;&#26223;&#22270;&#36890;&#36807;&#23558;&#22330;&#26223;&#20998;&#35299;&#20026;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#20004;&#20004;&#26102;&#38388;&#20851;&#31995;&#26469;&#34920;&#31034;&#35270;&#39057;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#38271;&#26399;&#39044;&#27979;&#23545;&#35937;&#20043;&#38388;&#31934;&#32454;&#31890;&#24230;&#30340;&#20004;&#20004;&#20851;&#31995;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22330;&#26223;&#22270;&#39044;&#27979;&#65288;SGA&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#26368;&#20808;&#36827;&#30340;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#29992;&#20316;&#22522;&#32447;&#65292;&#20197;&#39044;&#27979;&#23545;&#35937;&#20043;&#38388;&#26410;&#26469;&#30340;&#20004;&#20004;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SceneSayer&#12290;&#22312;SceneSayer&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#38754;&#21521;&#23545;&#35937;&#30340;&#20851;&#31995;&#34920;&#31034;&#26469;&#25512;&#26029;&#35266;&#23519;&#21040;&#30340;&#35270;&#39057;&#24103;&#24182;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#20851;&#31995;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#37319;&#29992;&#36830;&#32493;&#26102;&#38388;&#35270;&#35282;&#65292;&#24182;&#20998;&#21035;&#20351;&#29992;&#31070;&#32463;ODE&#21644;&#31070;&#32463;SDE&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#23545;&#35937;&#30456;&#20114;&#20316;&#29992;&#30340;&#28508;&#22312;&#21160;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25512;&#26029;&#26410;&#26469;&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04899v1 Announce Type: cross  Abstract: Spatio-temporal scene graphs represent interactions in a video by decomposing scenes into individual objects and their pair-wise temporal relationships. Long-term anticipation of the fine-grained pair-wise relationships between objects is a challenging problem. To this end, we introduce the task of Scene Graph Anticipation (SGA). We adapt state-of-the-art scene graph generation methods as baselines to anticipate future pair-wise relationships between objects and propose a novel approach SceneSayer. In SceneSayer, we leverage object-centric representations of relationships to reason about the observed video frames and model the evolution of relationships between objects. We take a continuous time perspective and model the latent dynamics of the evolution of object interactions using concepts of NeuralODE and NeuralSDE, respectively. We infer representations of future relationships by solving an Ordinary Differential Equation and a Stoch
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.02504</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02504
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20195;&#34920;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#12290;&#35813;&#33539;&#24335;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21306;&#21035;&#20110;&#20247;&#65292;&#23637;&#31034;&#20102;&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#20063;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#29575;&#12290;&#36825;&#31181;&#25928;&#29575;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#29305;&#21035;&#26377;&#30410;&#65292;&#22240;&#20026;&#27880;&#37322;&#26679;&#26412;&#30340;&#25968;&#37327;&#36890;&#24120;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#25945;&#31243;&#20840;&#38754;&#20171;&#32461;&#20102;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#23454;&#38469;&#24212;&#29992;&#30340;&#26696;&#20363;&#32451;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#33539;&#24335;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#31867;&#21035;&#20998;&#31867;&#21644;&#22238;&#24402;&#12290;&#24378;&#35843;&#20854;&#39640;&#25928;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#65292;&#35813;&#25945;&#31243;&#26088;&#22312;&#40723;&#21169;&#26356;&#24191;&#27867;&#22320;&#37319;&#32435;&#36825;&#31181;&#33539;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#30340;&#24320;&#25918;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02504v1 Announce Type: cross  Abstract: The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BiasBuster&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#21644;&#27979;&#35797;&#22810;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;LLMs&#33258;&#36523;&#26469;&#28040;&#38500;&#20854;&#25552;&#31034;&#20013;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00811</link><description>&lt;p&gt;
LLM&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Cognitive Bias in High-Stakes Decision-Making with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00811
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BiasBuster&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#21644;&#27979;&#35797;&#22810;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;LLMs&#33258;&#36523;&#26469;&#28040;&#38500;&#20854;&#25552;&#31034;&#20013;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25903;&#25345;&#26085;&#30410;&#25193;&#22823;&#30340;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#20154;&#31867;(&#21019;&#36896;&#30340;)&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;LLMs&#21487;&#33021;&#20250;&#32487;&#25215;&#38024;&#23545;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20063;&#21487;&#33021;&#21463;&#21040;&#35748;&#30693;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#22952;&#30861;&#21033;&#29992;LLM&#21327;&#21161;&#20570;&#20986;&#20844;&#24179;&#21644;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;BiasBuster&#65292;&#19968;&#20010;&#26088;&#22312;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#12290;&#21463;&#24515;&#29702;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#20808;&#21069;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#35748;&#30693;&#20559;&#35265;(&#20363;&#22914;&#65292;&#25552;&#31034;&#35825;&#23548;&#12289;&#39034;&#24207;&#12289;&#22266;&#26377;)&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#26469;&#28040;&#38500;&#23427;&#20204;&#33258;&#24049;&#30340;&#25552;&#31034;&#20013;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#39046;&#22495;&#35748;&#30693;&#20559;&#35265;&#23384;&#22312;&#21644;&#24433;&#21709;&#30340;&#20840;&#38754;&#22270;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00811v1 Announce Type: new  Abstract: Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. However, given their training on human (created) data, LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive sciences, we develop a dataset containing 16,800 prompts to evaluate different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, amidst proposing a novel method using LLMs to debias their own prompts. Our analysis provides a comprehensive picture on the presence and effects of cognitive bias across diffe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#21160;&#24335;KBQA&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30693;&#35782;&#24211;&#20114;&#21160;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#65292;&#24182;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.15131</link><description>&lt;p&gt;
&#20114;&#21160;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36718;&#20132;&#20114;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#21160;&#24335;KBQA&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30693;&#35782;&#24211;&#20114;&#21160;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#65292;&#24182;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#30340;&#39046;&#22495;&#12290;KBQA&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#23558;&#22797;&#26434;&#38382;&#39064;&#35299;&#26512;&#20026;&#21487;&#25191;&#34892;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#65288;SP&#65289;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#21160;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#26223;&#19979;&#20805;&#20998;&#21033;&#29992;LLMs&#23558;&#38382;&#39064;&#35299;&#26512;&#20026;&#36923;&#36753;&#24418;&#24335;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20114;&#21160;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;Interactive-KBQA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#19982;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#30452;&#25509;&#20114;&#21160;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#12290;&#23545;&#20110;&#27599;&#31181;&#22797;&#26434;&#38382;&#39064;&#31867;&#21035;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;LLMs&#23436;&#25104;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15131v1 Announce Type: cross  Abstract: This study explores the realm of knowledge-base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. Yet, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;LoT&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#21319;&#20027;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LoT&#33021;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.02769</link><description>&lt;p&gt;
&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;: &#26131;&#20110;&#27169;&#20223;&#30340;&#21487;&#25512;&#24191;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02769
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;LoT&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#21319;&#20027;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LoT&#33021;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21363;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;Learning from Teaching&#65292;&#31616;&#31216;LoT&#65289;&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#12290;&#21463;&#21040;&#20154;&#31867;&#25429;&#25417;&#31616;&#26126;&#25277;&#35937;&#27169;&#24335;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#25512;&#24191;&#30340;&#20851;&#31995;&#26356;&#23481;&#26131;&#25945;&#25480;&#12290;LoT&#36890;&#36807;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#20010;&#27010;&#24565;&#65292;&#36890;&#36807;&#25552;&#20379;&#21453;&#39304;&#26469;&#35757;&#32451;&#20027;&#27169;&#22411;&#21644;&#25913;&#36827;&#20027;&#27169;&#22411;&#65292;&#20197;&#25429;&#25417;&#26356;&#22810;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24341;&#20837;LoT&#30456;&#27604;&#20165;&#22312;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#22909;&#22788;&#12290;&#36825;&#34920;&#26126;&#20102;LoT&#22312;&#35782;&#21035;&#21487;&#25512;&#24191;&#20449;&#24687;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data. It suggests the effectiveness of LoT in identifying generalizable information without falling into th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#20102;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#22870;&#21169;&#27169;&#22411;&#30340;&#36873;&#25321;&#20197;&#21450;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#30340;&#33258;&#28982;&#21464;&#25442;&#36873;&#25321;&#65292;&#35813;&#21464;&#25442;&#24378;&#35843;&#25913;&#21892;&#34920;&#29616;&#19981;&#20339;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#27424;&#25311;&#21512;&#21644;&#22870;&#21169;&#27450;&#39575;&#12290;</title><link>https://arxiv.org/abs/2402.00742</link><description>&lt;p&gt;
&#25913;&#21464;&#21644;&#32452;&#21512;&#22870;&#21169;&#20197;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transforming and Combining Rewards for Aligning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#20102;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#22870;&#21169;&#27169;&#22411;&#30340;&#36873;&#25321;&#20197;&#21450;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#30340;&#33258;&#28982;&#21464;&#25442;&#36873;&#25321;&#65292;&#35813;&#21464;&#25442;&#24378;&#35843;&#25913;&#21892;&#34920;&#29616;&#19981;&#20339;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#27424;&#25311;&#21512;&#21644;&#22870;&#21169;&#27450;&#39575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#39318;&#20808;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#22870;&#21169;&#27169;&#22411;&#26469;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#20004;&#20010;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22870;&#21169;&#27169;&#22411;&#30340;&#20219;&#20309;&#21333;&#35843;&#21464;&#25442;&#37117;&#20445;&#25345;&#20559;&#22909;&#25490;&#21517;&#65307;&#26159;&#21542;&#26377;&#19968;&#31181;&#27604;&#20854;&#20182;&#36873;&#25321;&#8220;&#26356;&#22909;&#8221;&#30340;&#36873;&#25321;&#65311;&#20854;&#27425;&#65292;&#25105;&#20204;&#32463;&#24120;&#24076;&#26395;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#20010;&#29305;&#24615;&#23545;&#40784;&#65306;&#25105;&#20204;&#22914;&#20309;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#65311;&#36890;&#36807;&#23545;&#40784;&#36807;&#31243;&#30340;&#27010;&#29575;&#35299;&#37322;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20174;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#23398;&#20064;&#30340;&#22870;&#21169;&#65288;&#24120;&#35265;&#24773;&#20917;&#65289;&#30340;&#33258;&#28982;&#21464;&#25442;&#36873;&#25321;&#12290;&#36825;&#20010;&#27966;&#29983;&#30340;&#21464;&#25442;&#20855;&#26377;&#20004;&#20010;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#24378;&#35843;&#25913;&#36827;&#34920;&#29616;&#19981;&#20339;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#24050;&#32463;&#24471;&#20998;&#33391;&#22909;&#30340;&#36755;&#20986;&#12290;&#36825;&#26082;&#20943;&#36731;&#20102;&#27424;&#25311;&#21512;&#65288;&#20854;&#20013;&#19968;&#20123;&#25552;&#31034;&#27809;&#26377;&#24471;&#21040;&#25913;&#36827;&#65289;&#65292;&#21448;&#20943;&#23569;&#20102;&#22870;&#21169;&#27450;&#39575;&#65288;&#27169;&#22411;&#23398;&#20064;&#21033;&#29992;&#38169;&#35823;&#25351;&#23450;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12492</link><description>&lt;p&gt;
&#27604;&#36739;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35821;&#35328;&#24314;&#27169;&#65306;&#27169;&#25311;&#32676;&#20307;&#12289;&#20010;&#20307;&#29305;&#28857;&#36824;&#26159;&#20004;&#32773;&#20860;&#39038;&#65311;
&lt;/p&gt;
&lt;p&gt;
Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#23558;&#20154;&#30340;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#65288;&#22914;45&#23681;&#20197;&#19978;&#30340;&#20154;&#32676;&#65289;&#36824;&#26159;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#26356;&#26377;&#25928;&#30340;&#38382;&#39064;&#23578;&#26410;&#30830;&#23450;&#12290;&#32676;&#20307;&#23646;&#24615;&#22312;&#25216;&#26415;&#19978;&#26356;&#23481;&#26131;&#23454;&#29616;&#65292;&#20294;&#26159;&#36807;&#20110;&#31895;&#31961;&#65306;&#24182;&#38750;&#25152;&#26377;45&#23681;&#20197;&#19978;&#30340;&#20154;&#37117;&#20197;&#30456;&#21516;&#30340;&#26041;&#24335;&#20070;&#20889;&#12290;&#30456;&#21453;&#65292;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#33021;&#22815;&#25429;&#25417;&#27599;&#20010;&#20154;&#36523;&#20221;&#30340;&#22797;&#26434;&#24615;&#65292;&#20801;&#35768;&#26356;&#20010;&#24615;&#21270;&#30340;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#27169;&#25311;&#26080;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#24182;&#19988;&#38656;&#35201;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36890;&#36807;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#23558;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#30340;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#65288;&#22914;&#24180;&#40836;&#20272;&#35745;&#25110;&#20010;&#24615;&#35780;&#20272;&#65289;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#31435;&#22330;&#21644;&#20027;&#39064;&#26816;&#27979;&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#34394;&#25311;&#29616;&#23454;&#22836;&#20687;&#27880;&#20876;&#21644;&#38754;&#37096;&#21160;&#30011;&#38382;&#39064;&#65292;&#21457;&#29616;&#22836;&#20687;&#21644;&#22836;&#26174;&#30456;&#26426;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#26159;&#20027;&#35201;&#38590;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.11002</link><description>&lt;p&gt;
&#24555;&#36895;&#27880;&#20876;&#36924;&#30495;&#30340;&#34394;&#25311;&#29616;&#23454;&#22836;&#20687;&#29992;&#20110;&#38754;&#37096;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Fast Registration of Photorealistic Avatars for VR Facial Animation. (arXiv:2401.11002v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#34394;&#25311;&#29616;&#23454;&#22836;&#20687;&#27880;&#20876;&#21644;&#38754;&#37096;&#21160;&#30011;&#38382;&#39064;&#65292;&#21457;&#29616;&#22836;&#20687;&#21644;&#22836;&#26174;&#30456;&#26426;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#26159;&#20027;&#35201;&#38590;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#22312;&#31038;&#20132;&#20114;&#21160;&#26041;&#38754;&#25317;&#26377;&#26356;&#20855;&#27785;&#28024;&#24863;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26159;&#33021;&#22815;&#22312;&#20329;&#25140;VR&#22836;&#26174;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#27169;&#25311;&#19968;&#20010;&#36924;&#30495;&#30340;&#22836;&#20687;&#12290;&#34429;&#28982;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#21487;&#20197;&#23454;&#29616;&#23545;&#29305;&#23450;&#20010;&#20154;&#22836;&#20687;&#36827;&#34892;&#39640;&#36136;&#37327;&#27880;&#20876;&#65292;&#24182;&#36827;&#34892;&#21160;&#30011;&#29983;&#25104;&#65292;&#20294;&#36890;&#29992;&#23454;&#26102;&#27169;&#22411;&#30340;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#22312;&#32447;&#27880;&#20876;&#20063;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#20542;&#26012;&#30340;&#25668;&#20687;&#26426;&#35270;&#35282;&#21644;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#34920;&#26126;&#22836;&#20687;&#19982;&#22836;&#26174;&#30456;&#26426;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#26159;&#22256;&#38590;&#30340;&#20027;&#35201;&#28304;&#27849;&#20043;&#19968;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#22312;&#39046;&#22495;&#19968;&#33268;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#22312;&#24341;&#20837;&#39046;&#22495;&#24046;&#36317;&#21518;&#24615;&#33021;&#19979;&#38477;&#12290;&#22522;&#20110;&#27492;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35774;&#35745;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#37096;&#20998;&#65306;1&#65289;&#19968;&#20010;&#36845;&#20195;&#32454;&#21270;&#27169;&#22359;&#65292;&#25509;&#25910;&#39046;&#22495;&#20869;&#36755;&#20837;&#65292;&#21644;2&#65289;&#19968;&#20010;&#36890;&#29992;&#30340;&#22836;&#20687;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Virtual Reality (VR) bares promise of social interactions that can feel more immersive than other media. Key to this is the ability to accurately animate a photorealistic avatar of one's likeness while wearing a VR headset. Although high quality registration of person-specific avatars to headset-mounted camera (HMC) images is possible in an offline setting, the performance of generic realtime models are significantly degraded. Online registration is also challenging due to oblique camera views and differences in modality. In this work, we first show that the domain gap between the avatar and headset-camera images is one of the primary sources of difficulty, where a transformer-based architecture achieves high accuracy on domain-consistent data, but degrades when the domain-gap is re-introduced. Building on this finding, we develop a system design that decouples the problem into two parts: 1) an iterative refinement module that takes in-domain inputs, and 2) a generic avatar-guided imag
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.04131</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#23450;&#20301;&#36328;&#20219;&#21153;&#24207;&#21015;&#32487;&#32493;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04131
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;Transformer&#27169;&#22411;&#36824;&#21407;&#20026;&#21487;&#35835;&#30340;&#30005;&#36335;&#34920;&#31034;&#65292;&#29992;&#20110;&#23454;&#29616;&#31639;&#27861;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#26469;&#25193;&#23637;&#36825;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#23383;&#12289;&#25968;&#23383;&#35789;&#21644;&#26376;&#20221;&#30340;&#36882;&#22686;&#24207;&#21015;&#12290;&#36890;&#36807;&#24212;&#29992;&#30005;&#36335;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36127;&#36131;&#26816;&#27979;&#24207;&#21015;&#25104;&#21592;&#21644;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#25104;&#21592;&#30340;&#20851;&#38190;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35821;&#20041;&#30456;&#20851;&#24207;&#21015;&#20381;&#36182;&#20110;&#20855;&#26377;&#31867;&#20284;&#20316;&#29992;&#30340;&#20849;&#20139;&#30005;&#36335;&#23376;&#22270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35760;&#24405;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#32534;&#36753;&#36807;&#31243;&#12290;&#36825;&#31181;&#23545;Transformer&#30340;&#26426;&#26800;&#29702;&#35299;&#26159;&#26500;&#24314;&#26356;&#20581;&#22766;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#26356;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#23545;PAEs&#29420;&#29305;&#29305;&#24449;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2311.01473</link><description>&lt;p&gt;
&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples in the Physical World: A Survey. (arXiv:2311.01473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#23545;PAEs&#29420;&#29305;&#29305;&#24449;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23545;&#23545;&#25239;&#26679;&#26412;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#33030;&#24369;&#24615;&#12290;&#38500;&#20102;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#25915;&#20987;&#22806;&#65292;&#23545;&#25239;&#26679;&#26412;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23545;&#29289;&#29702;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#30740;&#31350;&#32570;&#20047;&#23545;&#20854;&#29420;&#29305;&#29305;&#24449;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#23548;&#33268;&#20854;&#37325;&#35201;&#24615;&#21644;&#29702;&#35299;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#35757;&#32451;&#12289;&#21046;&#36896;&#21644;&#37325;&#37319;&#26679;&#36807;&#31243;&#20013;&#20840;&#38754;&#32771;&#23519;PAEs&#30340;&#29305;&#28857;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#36890;&#36807;&#20998;&#26512;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#30830;&#23450;&#21046;&#36896;&#21644;&#37325;&#37319;&#26679;&#26159;PAEs&#20013;&#29420;&#29305;&#23646;&#24615;&#21644;&#29305;&#27530;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#65292;&#25105;&#20204;&#22522;&#20110;&#20854;&#29305;&#23450;&#29305;&#24449;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;PAEs&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#29289;&#29702;&#23545;&#25239;&#19990;&#30028;&#30740;&#31350;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated high vulnerability to adversarial examples. Besides the attacks in the digital world, the practical implications of adversarial examples in the physical world present significant challenges and safety concerns. However, current research on physical adversarial examples (PAEs) lacks a comprehensive understanding of their unique characteristics, leading to limited significance and understanding. In this paper, we address this gap by thoroughly examining the characteristics of PAEs within a practical workflow encompassing training, manufacturing, and re-sampling processes. By analyzing the links between physical adversarial attacks, we identify manufacturing and re-sampling as the primary sources of distinct attributes and particularities in PAEs. Leveraging this knowledge, we develop a comprehensive analysis and classification framework for PAEs based on their specific characteristics, covering over 100 studies on physical-world adversarial e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#26159;&#29616;&#26377;&#31639;&#27861;&#30340;&#25968;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.08419</link><description>&lt;p&gt;
&#22312;&#20108;&#21313;&#20010;&#26597;&#35810;&#20013;&#30772;&#35299;&#40657;&#30418;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Black Box Large Language Models in Twenty Queries. (arXiv:2310.08419v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08419
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#26159;&#29616;&#26377;&#31639;&#27861;&#30340;&#25968;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#30772;&#35299;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#36843;&#20351;LLMs&#36229;&#36234;&#20854;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#36825;&#20123;&#28431;&#27934;&#23545;&#20110;&#29702;&#35299;&#22266;&#26377;&#24369;&#28857;&#24182;&#38450;&#27490;&#26410;&#26469;&#30340;&#19981;&#24403;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Automatic Iterative Refinement&#65288;PAIR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20165;&#36890;&#36807;&#23545;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#31639;&#27861;&#29983;&#25104;&#35821;&#20041;&#30772;&#35299;&#12290;PAIR&#21463;&#21040;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#30340;&#21551;&#21457;&#65292;&#20351;&#29992;&#25915;&#20987;&#32773;LLM&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#30446;&#26631;LLM&#30340;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#25915;&#20987;&#32773;LLM&#36890;&#36807;&#36845;&#20195;&#26597;&#35810;&#30446;&#26631;LLM&#26469;&#26356;&#26032;&#21644;&#25913;&#36827;&#20505;&#36873;&#30772;&#35299;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#36825;&#27604;&#29616;&#26377;&#31639;&#27861;&#39640;&#25928;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;PAIR&#36824;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#30772;&#35299;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbrea
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#22312;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;40.4&#30340;&#25513;&#30721;AP$_r$&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00161</link><description>&lt;p&gt;
&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detection-Oriented Image-Text Pretraining for Open-Vocabulary Detection. (arXiv:2310.00161v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#22312;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;40.4&#30340;&#25513;&#30721;AP$_r$&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#26032;&#30340;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#29992;&#26816;&#27979;&#22120;&#26550;&#26500;&#26367;&#20195;&#24120;&#29992;&#30340;&#20998;&#31867;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#26816;&#27979;&#22120;&#22836;&#37096;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#65292;&#26356;&#22909;&#22320;&#28385;&#36275;&#26816;&#27979;&#30340;&#21306;&#22495;&#32423;&#35782;&#21035;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#26631;&#20934;&#30340;&#23545;&#27604;&#25439;&#22833;&#32780;&#19981;&#20351;&#29992;&#20266;&#26631;&#31614;&#65292;&#26159;&#23545;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31383;&#21475;&#27880;&#24847;&#21147;&#30340;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#26356;&#21152;&#40065;&#26834;&#12289;&#24179;&#31227;&#19981;&#21464;&#65292;&#24182;&#19988;&#19981;&#21463;&#31383;&#21475;&#27169;&#24335;&#30340;&#20559;&#24046;&#24433;&#21709;&#12290;&#22312;&#27969;&#34892;&#30340;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#24120;&#35265;&#30340;ViT-L&#20027;&#24178;&#32593;&#32476;&#21462;&#24471;&#20102;40.4&#30340;&#25513;&#30721;AP$_r$&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new open-vocabulary detection approach based on detection-oriented image-text pretraining to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we replace the commonly used classification architecture with the detector architecture, which better serves the region-level recognition needs of detection by enabling the detector heads to learn from noisy image-text pairs. Using only standard contrastive loss and no pseudo-labeling, our approach is a simple yet effective extension of the contrastive learning method to learn emergent object-semantic cues. In addition, we propose a shifted-window learning approach upon window attention to make the backbone representation more robust, translation-invariant, and less biased by the window pattern. On the popular LVIS open-vocabulary detection benchmark, our approach sets a new state of the art of 40.4 mask AP$_r$ using the common ViT-L backbone, significantly outperforming t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#29615;&#22659;&#30340;&#31639;&#27861; XRM&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#24182;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#29615;&#22659;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16748</link><description>&lt;p&gt;
&#29992;XRM&#21457;&#29616;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Discovering environments with XRM. (arXiv:2309.16748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#29615;&#22659;&#30340;&#31639;&#27861; XRM&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#24182;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#29615;&#22659;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#38656;&#35201;&#29615;&#22659;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27880;&#37322;&#30340;&#33719;&#21462;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#21463;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#26399;&#26395;&#21644;&#24863;&#30693;&#20559;&#24046;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#23454;&#29616;&#24212;&#29992;&#39046;&#22495;&#20840;&#38754;&#27867;&#21270;&#30340;&#40065;&#26834;&#24615;AI&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#24320;&#21457;&#19968;&#31181;&#31639;&#27861;&#26469;&#33258;&#21160;&#21457;&#29616;&#24341;&#21457;&#24191;&#27867;&#27867;&#21270;&#30340;&#29615;&#22659;&#12290;&#30446;&#21069;&#30340;&#25552;&#26696;&#26681;&#25454;&#35757;&#32451;&#35823;&#24046;&#23558;&#31034;&#20363;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#31867;&#65292;&#20294;&#23384;&#22312;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#28155;&#21152;&#20102;&#36229;&#21442;&#25968;&#21644;&#26089;&#20572;&#31574;&#30053;&#65292;&#32780;&#36825;&#20123;&#21442;&#25968;&#26159;&#26080;&#27861;&#22312;&#27809;&#26377;&#20154;&#31867;&#27880;&#37322;&#29615;&#22659;&#30340;&#39564;&#35777;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#30340;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#27491;&#26159;&#35201;&#21457;&#29616;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Cross-Risk-Minimization (XRM) &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;XRM &#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#20010;&#38543;&#26426;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#25152;&#20570;&#30340;&#33258;&#20449;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;XRM &#25552;&#20379;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful out-of-distribution generalization requires environment annotations. Unfortunately, these are resource-intensive to obtain, and their relevance to model performance is limited by the expectations and perceptual biases of human annotators. Therefore, to enable robust AI systems across applications, we must develop algorithms to automatically discover environments inducing broad generalization. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods add hyper-parameters and early-stopping criteria that are impossible to tune without a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#21333;&#20803;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#39118;&#26684;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#38899;&#33394;&#20445;&#30041;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20043;&#21069;&#26410;&#35265;&#30340;&#35821;&#35328;&#19978;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#36328;&#35821;&#35328;&#39118;&#26684;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2309.07566</link><description>&lt;p&gt;
&#22522;&#20110;&#31163;&#25955;&#21333;&#20803;&#30340;&#39118;&#26684;&#36716;&#25442;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer. (arXiv:2309.07566v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#21333;&#20803;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#39118;&#26684;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#38899;&#33394;&#20445;&#30041;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20043;&#21069;&#26410;&#35265;&#30340;&#35821;&#35328;&#19978;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#36328;&#35821;&#35328;&#39118;&#26684;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65288;S2ST&#65289;&#36890;&#36807;&#31163;&#25955;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#26080;&#27861;&#20445;&#30041;&#28304;&#35821;&#38899;&#30340;&#35828;&#35805;&#20154;&#38899;&#33394;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#39640;&#36136;&#37327;&#35828;&#35805;&#20154;&#24179;&#34892;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#23545;&#20110;&#23398;&#20064;&#28304;&#35821;&#38899;&#21644;&#30446;&#26631;&#35821;&#38899;&#20043;&#38388;&#30340;&#39118;&#26684;&#36716;&#25442;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#31163;&#25955;&#21333;&#20803;&#30340;&#22768;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#39118;&#26684;&#36716;&#25442;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#30340;S2ST&#26694;&#26550;&#12290;&#22768;&#23398;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#33258;&#30417;&#30563;&#19978;&#19979;&#25991;&#23398;&#20064;&#33719;&#24471;&#20102;&#39118;&#26684;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#20381;&#36182;&#20110;&#20219;&#20309;&#35828;&#35805;&#20154;&#24179;&#34892;&#25968;&#25454;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#28304;&#35821;&#35328;&#19978;&#23454;&#29616;&#38646;-shot&#36328;&#35821;&#35328;&#39118;&#26684;&#36716;&#25442;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#32763;&#35793;&#35821;&#38899;&#20855;&#26377;&#39640;&#24230;&#30340;&#20445;&#30495;&#24230;&#21644;&#39118;&#26684;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct speech-to-speech translation (S2ST) with discrete self-supervised representations has achieved remarkable accuracy, but is unable to preserve the speaker timbre of the source speech during translation. Meanwhile, the scarcity of high-quality speaker-parallel data poses a challenge for learning style transfer between source and target speech. We propose an S2ST framework with an acoustic language model based on discrete units from a self-supervised model and a neural codec for style transfer. The acoustic language model leverages self-supervised in-context learning, acquiring the ability for style transfer without relying on any speaker-parallel data, thereby overcoming the issue of data scarcity. By using extensive training data, our model achieves zero-shot cross-lingual style transfer on previously unseen source languages. Experiments show that our model generates translated speeches with high fidelity and style similarity. Audio samples are available at this http URL .
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07865</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25512;&#25991;/&#29992;&#25143;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22788;&#29702;&#25991;&#26412;&#22823;&#25968;&#25454;&#22320;&#29702;&#26631;&#35760;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#20272;&#35745;&#22352;&#26631;&#23545;&#65288;&#32463;&#24230;&#65292;&#32428;&#24230;&#65289;&#21644;&#20108;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#33539;&#22260;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#12290;&#24615;&#33021;&#25351;&#26631;&#34920;&#26126;&#65292;&#23545;&#20110;&#22312;&#25512;&#25991;&#20869;&#23481;&#21644;&#20803;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;30&#20844;&#37324;&#65292;&#32654;&#22269;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;15&#20844;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
&lt;/p&gt;</description></item></channel></rss>