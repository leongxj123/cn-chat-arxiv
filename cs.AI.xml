<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20351;&#29992;&#20559;&#24207;&#26102;&#24207;&#30446;&#26631;&#65292;&#23558;&#37096;&#20998;&#26377;&#24207;&#20559;&#22909;&#26144;&#23556;&#21040;MDP&#31574;&#30053;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#24207;&#29702;&#35770;&#23454;&#29616;&#26368;&#20248;&#31574;&#30053;&#30340;&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.18212</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#22522;&#20110;&#20559;&#24207;&#26102;&#24207;&#30446;&#26631;&#30340;&#39318;&#36873;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Preference-Based Planning in Stochastic Environments: From Partially-Ordered Temporal Goals to Most Preferred Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18212
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20559;&#24207;&#26102;&#24207;&#30446;&#26631;&#65292;&#23558;&#37096;&#20998;&#26377;&#24207;&#20559;&#22909;&#26144;&#23556;&#21040;MDP&#31574;&#30053;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#24207;&#29702;&#35770;&#23454;&#29616;&#26368;&#20248;&#31574;&#30053;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#24182;&#38750;&#24635;&#26159;&#36890;&#36807;&#23436;&#20840;&#30340;&#32447;&#24615;&#39034;&#24207;&#26469;&#34920;&#31034;&#65306;&#20351;&#29992;&#37096;&#20998;&#26377;&#24207;&#20559;&#22909;&#26469;&#34920;&#36798;&#19981;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#26159;&#33258;&#28982;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#38543;&#26426;&#31995;&#32479;&#20013;&#20570;&#20915;&#31574;&#21644;&#27010;&#29575;&#35268;&#21010;&#65292;&#36825;&#20123;&#31995;&#32479;&#34987;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#65292;&#32473;&#23450;&#19968;&#32452;&#26377;&#24207;&#20559;&#22909;&#30340;&#26102;&#38388;&#24310;&#20280;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#26102;&#38388;&#24310;&#20280;&#30446;&#26631;&#37117;&#26159;&#20351;&#29992;&#32447;&#24615;&#26102;&#24207;&#36923;&#36753;&#26377;&#38480;&#36712;&#36857;&#65288;LTL$_f$&#65289;&#20013;&#30340;&#20844;&#24335;&#26469;&#34920;&#31034;&#30340;&#12290;&#20026;&#20102;&#26681;&#25454;&#37096;&#20998;&#26377;&#24207;&#20559;&#22909;&#36827;&#34892;&#35268;&#21010;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24207;&#29702;&#35770;&#26469;&#23558;&#23545;&#26102;&#38388;&#30446;&#26631;&#30340;&#20559;&#22909;&#26144;&#23556;&#21040;&#23545;MDP&#31574;&#30053;&#30340;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#38543;&#26426;&#39034;&#24207;&#19979;&#30340;&#19968;&#20010;&#26368;&#20248;&#36873;&#31574;&#30053;&#23558;&#23548;&#33268;MDP&#20013;&#26377;&#38480;&#36335;&#24452;&#19978;&#30340;&#19968;&#20010;&#38543;&#26426;&#38750;&#25903;&#37197;&#27010;&#29575;&#20998;&#24067;&#12290;&#20026;&#20102;&#21512;&#25104;&#19968;&#20010;&#26368;&#20248;&#36873;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31243;&#24207;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18212v1 Announce Type: cross  Abstract: Human preferences are not always represented via complete linear orders: It is natural to employ partially-ordered preferences for expressing incomparable outcomes. In this work, we consider decision-making and probabilistic planning in stochastic systems modeled as Markov decision processes (MDPs), given a partially ordered preference over a set of temporally extended goals. Specifically, each temporally extended goal is expressed using a formula in Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially ordered preference, we introduce order theory to map a preference over temporal goals to a preference over policies for the MDP. Accordingly, a most preferred policy under a stochastic ordering induces a stochastic nondominated probability distribution over the finite paths in the MDP. To synthesize a most preferred policy, our technical approach includes two key steps. In the first step, we develop a procedure to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.03777</link><description>&lt;p&gt;
ENOT&#65306;&#26399;&#26395;&#22238;&#24402;&#29992;&#20110;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#30340;&#24555;&#36895;&#21644;&#20934;&#30830;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03777
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#20849;&#36717;&#21183;&#27491;&#21017;&#21270;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#12290;&#29616;&#26377;NOT&#27714;&#35299;&#22120;&#30340;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#25214;&#21040;&#20849;&#36717;&#31639;&#23376;&#65288;&#21363;c-transform&#65289;&#30340;&#25509;&#36817;&#31934;&#30830;&#36817;&#20284;&#30340;&#36807;&#31243;&#65292;&#36825;&#35201;&#20040;&#36890;&#36807;&#20248;&#21270;&#26368;&#23567;-&#26368;&#22823;&#30446;&#26631;&#65292;&#35201;&#20040;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#23545;&#21021;&#22987;&#36817;&#20284;&#39044;&#27979;&#30340;&#31934;&#32454;&#35843;&#25972;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#12289;&#22312;&#26399;&#26395;&#22238;&#24402;&#24418;&#24335;&#19978;&#24378;&#21046;&#36866;&#24212;&#24615;&#26465;&#20214;&#20110;&#23398;&#20064;&#23545;&#20598;&#21183;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#21270;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#21487;&#33021;&#20849;&#36717;&#21183;&#20998;&#24067;&#30340;&#19978;&#38480;&#20272;&#35745;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#65292;&#28040;&#38500;&#20102;&#23545;&#39069;&#22806;&#24191;&#27867;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03777v1 Announce Type: cross  Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10403</link><description>&lt;p&gt;
&#20174;&#20998;&#27573;&#19977;&#32447;&#24615;&#32593;&#32476;&#20013;&#23548;&#20986;&#22810;&#38754;&#20307;&#22797;&#21512;&#20307;
&lt;/p&gt;
&lt;p&gt;
Polyhedral Complex Derivation from Piecewise Trilinear Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35270;&#21270;&#30340;&#36827;&#23637;&#25581;&#31034;&#20102;&#23427;&#20204;&#32467;&#26500;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#36830;&#32493;&#20998;&#27573;&#20223;&#23556;&#65288;CPWA&#65289;&#20989;&#25968;&#20013;&#25552;&#21462;&#32593;&#26684;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#34920;&#38754;&#34920;&#31034;&#23398;&#20064;&#30340;&#21457;&#23637;&#21253;&#25324;&#38750;&#32447;&#24615;&#20301;&#32622;&#32534;&#30721;&#65292;&#35299;&#20915;&#20102;&#35832;&#22914;&#35889;&#20559;&#24046;&#20043;&#31867;&#30340;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#36825;&#22312;&#24212;&#29992;&#22522;&#20110;CPWA&#20989;&#25968;&#30340;&#32593;&#26684;&#25552;&#21462;&#25216;&#26415;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#30340;&#32593;&#26684;&#25552;&#21462;&#65292;&#23637;&#31034;&#20102;&#22312;&#22855;&#25343;&#23572;&#32422;&#26463;&#19979;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#19977;&#32447;&#24615;&#21306;&#22495;&#20869;&#30340;&#24179;&#38754;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36817;&#20284;&#19977;&#20010;&#39640;&#32500;&#26354;&#38754;&#20043;&#38388;&#30340;&#20132;&#28857;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#27721;&#26126;&#36317;&#31163;&#21644;&#25928;&#29575;&#20197;&#21450;&#35282;&#36317;&#31163;&#26469;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#27491;&#30830;&#24615;&#21644;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#26816;&#26597;&#20102;t&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10403v1 Announce Type: cross  Abstract: Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24773;&#24863;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#30340;&#21453;&#39304;&#65292;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.07640</link><description>&lt;p&gt;
&#21512;&#25104;&#23545;&#22810;&#27169;&#24577;&#25991;&#26412;&#21644;&#22270;&#29255;&#25968;&#25454;&#30340;&#24773;&#24863;&#25511;&#21046;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24773;&#24863;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#30340;&#21453;&#39304;&#65292;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#22810;&#27169;&#24577;&#36755;&#20837;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#29255;&#65289;&#30340;&#24773;&#24863;&#25511;&#21046;&#21453;&#39304;&#33021;&#22815;&#24357;&#34917;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#24046;&#36317;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#20855;&#26377;&#21516;&#29702;&#24515;&#12289;&#20934;&#30830;&#24615;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#21307;&#30103;&#12289;&#33829;&#38144;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#26377;&#30528;&#28145;&#36828;&#30340;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21487;&#25511;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#65288;CMFeed&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#25511;&#21046;&#24615;&#27169;&#22359;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#21644;&#35270;&#35273;&#36755;&#20837;&#12290;&#23427;&#20351;&#29992;Transformer&#21644;Faster R-CNN&#32593;&#32476;&#25552;&#21462;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#21453;&#39304;&#12290;CMFeed&#25968;&#25454;&#38598;&#21253;&#21547;&#22270;&#29255;&#12289;&#25991;&#26412;&#12289;&#23545;&#24086;&#23376;&#30340;&#21453;&#24212;&#12289;&#24102;&#26377;&#30456;&#20851;&#24615;&#35780;&#20998;&#30340;&#20154;&#31867;&#35780;&#35770;&#20197;&#21450;&#23545;&#35780;&#35770;&#30340;&#21453;&#24212;&#12290;&#23545;&#24086;&#23376;&#21644;&#35780;&#35770;&#30340;&#21453;&#24212;&#34987;&#29992;&#26469;&#35757;&#32451;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#20135;&#29983;&#20855;&#26377;&#29305;&#23450;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#24773;&#24863;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generate sentiment-controlled feedback in response to multimodal inputs, comprising both text and images, addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses. This capability has profound applications in healthcare, marketing, and education. To this end, we construct a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system. The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs. It extracts textual and visual features using a transformer and Faster R-CNN networks and combines them to generate feedback. The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments. The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative)
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#20013;&#35299;&#20915;&#22270;&#20687;&#23383;&#24149;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MOCHa&#26469;&#32531;&#35299;&#24187;&#35273;</title><link>https://arxiv.org/abs/2312.03631</link><description>&lt;p&gt;
&#32531;&#35299;&#24320;&#25918;&#35789;&#27719;&#25551;&#36848;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Open-Vocabulary Caption Hallucinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#20013;&#35299;&#20915;&#22270;&#20687;&#23383;&#24149;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MOCHa&#26469;&#32531;&#35299;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#26465;&#20214;&#30340;&#25991;&#26412;&#29983;&#25104;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#22270;&#20687;&#23383;&#24149;&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#19982;&#32473;&#23450;&#22270;&#20687;&#26080;&#27861;&#25512;&#26029;&#30340;&#34394;&#20551;&#32454;&#33410;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#22270;&#20687;&#23383;&#24149;&#20013;&#22823;&#22810;&#20351;&#29992;&#23553;&#38381;&#35789;&#27719;&#23545;&#35937;&#21015;&#34920;&#26469;&#32531;&#35299;&#25110;&#35780;&#20272;&#24187;&#35273;&#65292;&#24573;&#30053;&#20102;&#23454;&#36341;&#20013;&#21457;&#29983;&#30340;&#22823;&#22810;&#25968;&#24187;&#35273;&#31867;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#20013;&#22270;&#20687;&#23383;&#24149;&#20013;&#30340;&#24187;&#35273;&#65292;&#21253;&#25324;&#37327;&#21270;&#23427;&#20204;&#30340;&#23384;&#22312;&#24182;&#20248;&#21270;&#20197;&#20943;&#36731;&#36825;&#31181;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;OpenCHAIR&#22522;&#20934;&#21033;&#29992;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#26469;&#35780;&#20272;&#24320;&#25918;&#35789;&#27719;&#25551;&#36848;&#24187;&#35273;&#65292;&#22312;&#22810;&#26679;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#27969;&#34892;&#30340;CHAIR&#22522;&#20934;&#12290;&#20026;&#20102;&#22312;&#24207;&#21015;&#32423;&#21035;&#19978;&#32531;&#35299;&#24320;&#25918;&#35789;&#27719;&#30340;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MOCHa&#65292;&#19968;&#31181;&#21033;&#29992;&#36827;&#23637;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03631v2 Announce Type: replace-cross  Abstract: While recent years have seen rapid progress in image-conditioned text generation, image captioning still suffers from the fundamental issue of hallucinations, namely, the generation of spurious details that cannot be inferred from the given image. Existing methods largely use closed-vocabulary object lists to mitigate or evaluate hallucinations in image captioning, ignoring most types of hallucinations that occur in practice. To this end, we propose a framework for addressing hallucinations in image captioning in the open-vocabulary setting, including quantifying their presence and optimizing to mitigate such hallucinations. Our OpenCHAIR benchmark leverages generative foundation models to evaluate open-vocabulary caption hallucinations, surpassing the popular CHAIR benchmark in both diversity and accuracy. To mitigate open-vocabulary hallucinations at the sequence level, we propose MOCHa, an approach harnessing advancements in
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36335;&#24452;&#30340;KGC&#35299;&#37322;&#22120;Power-Link&#36890;&#36807;&#24341;&#20837;&#22270;&#21152;&#26435;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#25512;&#21160;&#20102;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.02290</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Path-based Explanation for Knowledge Graph Completion. (arXiv:2401.02290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02290
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#30340;KGC&#35299;&#37322;&#22120;Power-Link&#36890;&#36807;&#24341;&#20837;&#22270;&#21152;&#26435;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#25512;&#21160;&#20102;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20132;&#20114;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#35299;&#37322;&#21364;&#27809;&#26377;&#24471;&#21040;&#24517;&#35201;&#30340;&#20851;&#27880;&#12290;&#23545;&#22522;&#20110;GNN&#30340;KGC&#27169;&#22411;&#32467;&#26524;&#36827;&#34892;&#36866;&#24403;&#35299;&#37322;&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#65292;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;KGC&#35299;&#37322;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23454;&#20363;/&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#32780;&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#65292;&#36335;&#24452;&#21487;&#20197;&#25552;&#20379;&#26356;&#21451;&#22909;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#23545;&#29983;&#25104;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Power-Link&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25506;&#32034;&#22522;&#20110;&#36335;&#24452;&#30340;KGC&#35299;&#37322;&#22120;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21152;&#26435;&#25216;&#26415;&#65292;&#20351;&#24471;&#21487;&#20197;&#20197;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#26696;&#29983;&#25104;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#30340;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years. However, the explanation of the predicted facts has not caught the necessary attention. Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models. Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations. Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored. To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models. We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme. We further introduce three new metrics for 
&lt;/p&gt;</description></item><item><title>FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01483</link><description>&lt;p&gt;
FedSN&#65306;&#19968;&#20010;&#36866;&#29992;&#20110;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01483
&lt;/p&gt;
&lt;p&gt;
FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#24050;&#32463;&#30001;&#21830;&#19994;&#20844;&#21496;&#25104;&#21151;&#22320;&#21457;&#23556;&#21644;&#37096;&#32626;&#21040;&#22826;&#31354;&#20013;&#65292;&#22914;SpaceX&#12290;&#30001;&#20110;LEO&#21355;&#26143;&#37197;&#22791;&#20102;&#22810;&#27169;&#20256;&#24863;&#22120;&#65292;&#23427;&#20204;&#19981;&#20165;&#29992;&#20110;&#36890;&#20449;&#65292;&#36824;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#31354;&#38388;&#35843;&#21046;&#35782;&#21035;&#12289;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19982;LEO&#21355;&#26143;&#30340;&#26377;&#38480;&#25509;&#35302;&#26102;&#38388;&#65288;&#20363;&#22914;5&#20998;&#38047;&#65289;&#65292;&#22320;&#38754;&#31449;&#65288;GS&#65289;&#21487;&#33021;&#26080;&#27861;&#19979;&#36733;&#22914;&#27492;&#22823;&#37327;&#30340;&#21407;&#22987;&#24863;&#27979;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35201;&#22312;LEO&#21355;&#26143;&#19978;&#20351;&#29992;FL&#65292;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;i&#65289;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#65292;ii&#65289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#65292;&#20197;&#21450;iii&#65289;&#27169;&#22411;&#38472;&#26087;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSN&#30340;&#36890;&#29992;FL&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#19968;
&lt;/p&gt;
&lt;p&gt;
Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.17329</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#37096;&#30142;&#30149;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#26159;&#23548;&#33268;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#27835;&#24840;&#32954;&#37096;&#30142;&#30149;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#32954;&#37096;&#31995;&#32479;&#20869;&#30340;&#35768;&#22810;&#22797;&#26434;&#30340;3D&#26641;&#29366;&#32467;&#26500;&#65292;&#22914;&#27668;&#36947;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#22534;&#26632;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23494;&#38598;&#20307;&#32032;&#32593;&#26684;&#30340;&#26631;&#20934;CNN&#26041;&#27861;&#20195;&#20215;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;&#26641;&#39592;&#26550;&#30340;&#22270;&#36830;&#36890;&#24615;&#65292;&#24182;&#32467;&#21512;&#20102;&#38544;&#24335;&#34920;&#38754;&#34920;&#31034;&#12290;&#23427;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#29992;&#30340;&#34920;&#38754;&#12290;&#30001;&#20110;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#25968;&#25454;&#31232;&#32570;&#65292;&#25105;&#20204;&#36824;&#25972;&#29702;&#20102;&#19968;&#22871;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulmonary diseases rank prominently among the principal causes of death worldwide. Curing them will require, among other things, a better understanding of the many complex 3D tree-shaped structures within the pulmonary system, such as airways, arteries, and veins. In theory, they can be modeled using high-resolution image stacks. Unfortunately, standard CNN approaches operating on dense voxel grids are prohibitively expensive. To remedy this, we introduce a point-based approach that preserves graph connectivity of tree skeleton and incorporates an implicit surface representation. It delivers SOTA accuracy at a low computational cost and the resulting models have usable surfaces. Due to the scarcity of publicly accessible data, we have also curated an extensive dataset to evaluate our approach and will make it public.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#35270;&#39057;&#25968;&#25454;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#35821;&#35328;&#32534;&#30721;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.05822</link><description>&lt;p&gt;
&#32534;&#30721;-&#23384;&#20648;-&#26816;&#32034;&#65306;&#36890;&#36807;&#35821;&#35328;&#32534;&#30721;&#30340;&#33258;&#25105;&#20013;&#24515;&#24863;&#30693;&#22686;&#24378;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception. (arXiv:2308.05822v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#35270;&#39057;&#25968;&#25454;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#35821;&#35328;&#32534;&#30721;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20381;&#36182;&#20110;&#33258;&#24049;&#30340;&#35760;&#24518;&#26469;&#32534;&#30721;&#12289;&#23384;&#20648;&#21644;&#26816;&#32034;&#25105;&#20204;&#30340;&#32463;&#21382;&#12290;&#28982;&#32780;&#65292;&#35760;&#24518;&#38388;&#38548;&#26377;&#26102;&#20250;&#21457;&#29983;&#12290;&#23454;&#29616;&#35760;&#24518;&#22686;&#24378;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#22836;&#25140;&#24335;&#26174;&#31034;&#35774;&#22791;&#26469;&#25429;&#25417;&#21644;&#20445;&#30041;&#33258;&#25105;&#20013;&#24515;&#30340;&#35270;&#39057;&#65292;&#36825;&#31181;&#20570;&#27861;&#36890;&#24120;&#34987;&#31216;&#20026;&#29983;&#27963;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#25216;&#26415;&#32570;&#20047;&#39640;&#25928;&#32534;&#30721;&#21644;&#23384;&#20648;&#22914;&#27492;&#22823;&#37327;&#30340;&#35270;&#39057;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20174;&#24222;&#22823;&#30340;&#35270;&#39057;&#23384;&#26723;&#20013;&#26816;&#32034;&#29305;&#23450;&#20449;&#24687;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#22797;&#26434;&#20102;&#24555;&#36895;&#35775;&#38382;&#25152;&#38656;&#20869;&#23481;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as life logging. However, a significant challenge arises from the sheer volume of video data generated through life logging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation system that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Add
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#21363;&#26102;&#30340;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#30340;&#27169;&#24577;&#24046;&#36317;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.01661</link><description>&lt;p&gt;
SIA-FTP: &#19968;&#31181;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SIA-FTP: A Spoken Instruction Aware Flight Trajectory Prediction Framework. (arXiv:2305.01661v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#21363;&#26102;&#30340;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#30340;&#27169;&#24577;&#24046;&#36317;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#38899;&#36890;&#35759;&#36827;&#34892;&#22320;&#31354;&#21327;&#21830;&#26159;&#30830;&#20445;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#65288;ATC&#65289;&#25805;&#20316;&#23433;&#20840;&#21644;&#25928;&#29575;&#30340;&#37325;&#35201;&#21069;&#25552;&#12290;&#20294;&#26159;&#65292;&#38543;&#30528;&#20132;&#36890;&#27969;&#37327;&#30340;&#22686;&#21152;&#65292;&#30001;&#20110;&#20154;&#20026;&#22240;&#32032;&#23548;&#33268;&#30340;&#38169;&#35823;&#25351;&#20196;&#32473;ATC&#23433;&#20840;&#24102;&#26469;&#20102;&#24040;&#22823;&#23041;&#32961;&#12290;&#29616;&#26377;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#65288;FTP&#65289;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21382;&#21490;&#36712;&#36857;&#30340;&#39134;&#34892;&#29366;&#24577;&#65292;&#22312;&#23454;&#26102;&#26426;&#21160;&#25351;&#20196;&#30340;&#39044;&#27979;&#19978;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#24310;&#36831;&#65292;&#36825;&#19981;&#21033;&#20110;&#20914;&#31361;&#26816;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIA-FTP&#30340;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;FTP&#26694;&#26550;&#65292;&#36890;&#36807;&#21253;&#21547;&#21363;&#26102;&#30340;&#35821;&#38899;&#25351;&#20196;&#26469;&#25903;&#25345;&#39640;&#26426;&#21160;FTP&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#27169;&#24577;&#24046;&#36317;&#24182;&#26368;&#23567;&#21270;&#25968;&#25454;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#27880;&#24847;&#26426;&#21046;&#26469;&#34701;&#21512;&#35821;&#38899;&#25351;&#20196;&#23884;&#20837;&#21644;&#39134;&#34892;&#36712;&#36857;&#34920;&#31034;&#12290;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;SIA-FTP&#65292;&#19982;&#29616;&#26377;&#30340;FTP&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ground-air negotiation via speech communication is a vital prerequisite for ensuring safety and efficiency in air traffic control (ATC) operations. However, with the increase in traffic flow, incorrect instructions caused by human factors bring a great threat to ATC safety. Existing flight trajectory prediction (FTP) approaches primarily rely on the flight status of historical trajectory, leading to significant delays in the prediction of real-time maneuvering instruction, which is not conducive to conflict detection. A major reason is that spoken instructions and flight trajectories are presented in different modalities in the current air traffic control (ATC) system, bringing great challenges to considering the maneuvering instruction in the FTP tasks. In this paper, a spoken instruction-aware FTP framework, called SIA-FTP, is innovatively proposed to support high-maneuvering FTP tasks by incorporating instant spoken instruction. To address the modality gap and minimize the data requ
&lt;/p&gt;</description></item></channel></rss>