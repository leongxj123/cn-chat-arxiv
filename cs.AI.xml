<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#30340;&#31574;&#30053;&#65292;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#25968;&#23383;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.01536</link><description>&lt;p&gt;
&#25918;&#32622;&#38170;&#28857;&#65306;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#32473;&#25968;&#23383;&#35821;&#20041;&#19978;&#30340;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Laying Anchors: Semantically Priming Numerals in Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01536
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#30340;&#31574;&#30053;&#65292;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#25968;&#23383;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22823;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31649;&#32447;&#20013;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#28982;&#32780;&#36825;&#20123;&#27169;&#22411;&#26410;&#33021;&#27491;&#30830;&#32534;&#30721;&#25968;&#23383;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#25968;&#23383;&#29702;&#35299;&#30340;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20219;&#20309;&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#26469;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#20123;&#25968;&#23383;&#26631;&#35760;&#30340;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;&#25968;&#20540;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#30340;&#20248;&#36234;&#24615;&#65292;&#23545;&#39046;&#22495;&#20869;&#65288;&#24050;&#35265;&#65289;&#21644;&#39046;&#22495;&#22806;&#65288;&#26410;&#35265;&#65289;&#30340;&#25968;&#23383;&#37117;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23454;&#35777;&#35780;&#20272;&#25193;&#23637;&#21040;&#20174;1&#21040;10&#20159;&#30340;&#25968;&#23383;&#33539;&#22260;&#65292;&#27604;&#20197;&#24448;&#30456;&#21516;&#31867;&#22411;&#30740;&#31350;&#30340;&#33539;&#22260;&#24191;&#24471;&#22810;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#23398;&#24471;&#30340;&#23884;&#20837;&#21521;&#25968;&#23398;&#19978;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01536v1 Announce Type: cross  Abstract: Off-the-shelf pre-trained language models have become the de facto standard in NLP pipelines for a multitude of downstream tasks. However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension. We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens. We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical grounding of our learned embeddings.
&lt;/p&gt;</description></item><item><title>HARMamba&#21033;&#29992;&#26356;&#36731;&#37327;&#32423;&#30340;&#36873;&#25321;&#24615;SSM&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36164;&#28304;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.20183</link><description>&lt;p&gt;
HARMamba: &#22522;&#20110;&#21452;&#21521;&#36873;&#25321;&#24615;SSM&#30340;&#39640;&#25928;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on Bidirectional Selective SSM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20183
&lt;/p&gt;
&lt;p&gt;
HARMamba&#21033;&#29992;&#26356;&#36731;&#37327;&#32423;&#30340;&#36873;&#25321;&#24615;SSM&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36164;&#28304;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#27963;&#21160;&#24863;&#30693;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#30828;&#20214;&#24863;&#30693;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;Mamba&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#20986;&#29616;&#12290;HARMamba&#24341;&#20837;&#20102;&#26356;&#36731;&#37327;&#32423;&#30340;&#36873;&#25321;&#24615;SSM&#20316;&#20026;&#27963;&#21160;&#35782;&#21035;&#30340;&#22522;&#26412;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#31995;&#32479;&#35745;&#31639;&#36127;&#36733;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20183v1 Announce Type: cross  Abstract: Wearable sensor human activity recognition (HAR) is a crucial area of research in activity sensing. While transformer-based temporal deep learning models have been extensively studied and implemented, their large number of parameters present significant challenges in terms of system computing load and memory usage, rendering them unsuitable for real-time mobile activity recognition applications. Recently, an efficient hardware-aware state space model (SSM) called Mamba has emerged as a promising alternative. Mamba demonstrates strong potential in long sequence modeling, boasts a simpler network architecture, and offers an efficient hardware-aware design. Leveraging SSM for activity recognition represents an appealing avenue for exploration. In this study, we introduce HARMamba, which employs a more lightweight selective SSM as the foundational model architecture for activity recognition. The goal is to address the computational resourc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;MANGO&#22522;&#20934;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#22238;&#31572;&#28041;&#21450;&#26144;&#23556;&#21644;&#23548;&#33322;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.19913</link><description>&lt;p&gt;
MANGO&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;MANGO&#22522;&#20934;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#22238;&#31572;&#28041;&#21450;&#26144;&#23556;&#21644;&#23548;&#33322;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MANGO&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#23427;&#20204;&#25191;&#34892;&#22522;&#20110;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;&#19968;&#22871;&#25991;&#26412;&#28216;&#25103;&#30340;53&#20010;&#36855;&#23467;&#65306;&#27599;&#20010;&#36855;&#23467;&#37117;&#19982;&#19968;&#20010;&#28216;&#35272;&#35828;&#26126;&#37197;&#23545;&#65292;&#20854;&#20013;&#21253;&#21547;&#27599;&#20010;&#20301;&#32622;&#30340;&#35775;&#38382;&#20294;&#19981;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#36335;&#24452;&#12290;&#20219;&#21153;&#26159;&#38382;&#31572;&#65306;&#23545;&#20110;&#27599;&#20010;&#36855;&#23467;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35835;&#21462;&#28216;&#35272;&#35828;&#26126;&#24182;&#22238;&#31572;&#25968;&#30334;&#20010;&#26144;&#23556;&#21644;&#23548;&#33322;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;&#20320;&#24212;&#35813;&#20174;&#25151;&#23376;&#35199;&#37096;&#22914;&#20309;&#21435;&#38401;&#27004;&#65311;&#8221;&#21644;&#8220;&#22914;&#26524;&#25105;&#20204;&#20174;&#22320;&#19979;&#23460;&#21521;&#21271;&#21644;&#19996;&#36208;&#65292;&#25105;&#20204;&#20250;&#22312;&#21738;&#37324;&#65311;&#8221;&#12290;&#23613;&#31649;&#36825;&#20123;&#38382;&#39064;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#65292;&#20294;&#20107;&#23454;&#35777;&#26126;&#65292;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#29978;&#33267;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#23558;&#26377;&#21033;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19913v1 Announce Type: cross  Abstract: Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks. In this paper, we propose MANGO, a benchmark to evaluate their capabilities to perform text-based mapping and navigation. Our benchmark includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as "How should you go to Attic from West of House?" and "Where are we if we go north and east from Cellar?". Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit large languag
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#38382;&#32773;&#36890;&#36807;&#35810;&#38382;&#35282;&#33394;&#25198;&#28436;&#32773;&#26469;&#24341;&#20986;&#20559;&#22909;&#65292;&#20174;&#32780;&#36845;&#20195;&#24494;&#35843;&#20197;&#22686;&#21152;&#20219;&#21153;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.19154</link><description>&lt;p&gt;
STaR-GATE: &#25945;&#25480;&#35821;&#35328;&#27169;&#22411;&#35810;&#38382;&#28548;&#28165;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
STaR-GATE: Teaching Language Models to Ask Clarifying Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19154
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#38382;&#32773;&#36890;&#36807;&#35810;&#38382;&#35282;&#33394;&#25198;&#28436;&#32773;&#26469;&#24341;&#20986;&#20559;&#22909;&#65292;&#20174;&#32780;&#36845;&#20195;&#24494;&#35843;&#20197;&#22686;&#21152;&#20219;&#21153;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#26102;&#65292;&#29992;&#25143;&#36890;&#24120;&#20250;&#36951;&#28431;&#37325;&#35201;&#30340;&#32454;&#33410;&#12290;&#34429;&#28982;&#25552;&#38382;&#21487;&#20197;&#35299;&#20915;&#36825;&#31181;&#27495;&#20041;&#65292;&#20294;&#27169;&#22411;&#24448;&#24448;&#24456;&#38590;&#25552;&#20986;&#22909;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22870;&#21169;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;STaR-GATE&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;25,500&#20010;&#29420;&#29305;&#20154;&#29289;-&#20219;&#21153;&#25552;&#31034;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#27169;&#25311;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;--&#25552;&#38382;&#32773;--&#19982;&#19968;&#20010;&#20854;&#20559;&#22909;&#26410;&#30693;&#30340;&#35282;&#33394;&#25198;&#28436;&#32773;&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#36890;&#36807;&#25552;&#38382;&#65292;&#25552;&#38382;&#32773;&#20174;&#35282;&#33394;&#25198;&#28436;&#32773;&#37027;&#37324;&#24341;&#20986;&#20559;&#22909;&#12290;&#25552;&#38382;&#32773;&#22312;&#37027;&#20123;&#22686;&#21152;&#39640;&#36136;&#37327;&#21709;&#24212;&#27010;&#29575;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#36845;&#20195;&#24494;&#35843;&#65292;&#36825;&#20123;&#38382;&#39064;&#26159;&#30001;&#20855;&#26377;&#23545;&#35282;&#33394;&#25198;&#28436;&#32773;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35328;&#32773;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19154v1 Announce Type: cross  Abstract: When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity \citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions. We explore a language model's ability to self-improve \citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful questions -- a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model -- the \texttt{Questioner} -- and a \texttt{Roleplayer} whose preferences are unknown to the \texttt{Questioner}. By asking questions, the \texttt{Questioner} elicits preferences from the \texttt{Roleplayer}. The \texttt{Questioner} is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an \texttt{Oracle} with access to the \texttt{Ro
&lt;/p&gt;</description></item><item><title>Duwak&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#21452;&#37325;&#31192;&#23494;&#27169;&#24335;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27700;&#21360;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.13000</link><description>&lt;p&gt;
Duwak: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21452;&#37325;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Duwak: Dual Watermarks in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13000
&lt;/p&gt;
&lt;p&gt;
Duwak&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#21452;&#37325;&#31192;&#23494;&#27169;&#24335;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27700;&#21360;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26085;&#30410;&#20351;&#29992;&#65292;&#23457;&#35745;&#23427;&#20204;&#30340;&#29992;&#36884;&#12289;&#31649;&#29702;&#23427;&#20204;&#30340;&#24212;&#29992;&#24182;&#20943;&#36731;&#20854;&#28508;&#22312;&#21361;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Duwak&#65292;&#36890;&#36807;&#22312;&#20196;&#29260;&#27010;&#29575;&#20998;&#24067;&#21644;&#25277;&#26679;&#26041;&#26696;&#20013;&#23884;&#20837;&#21452;&#37325;&#31192;&#23494;&#27169;&#24335;&#65292;&#20174;&#26681;&#26412;&#19978;&#25552;&#39640;&#20102;&#27700;&#21360;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13000v1 Announce Type: cross  Abstract: As large language models (LLM) are increasingly used for text generation tasks, it is critical to audit their usages, govern their applications, and mitigate their potential harms. Existing watermark techniques are shown effective in embedding single human-imperceptible and machine-detectable patterns without significantly affecting generated text quality and semantics. However, the efficiency in detecting watermarks, i.e., the minimum number of tokens required to assert detection with significance and robustness against post-editing, is still debatable. In this paper, we propose, Duwak, to fundamentally enhance the efficiency and quality of watermarking by embedding dual secret patterns in both token probability distribution and sampling schemes. To mitigate expression degradation caused by biasing toward certain tokens, we design a contrastive search to watermark the sampling scheme, which minimizes the token repetition and enhances 
&lt;/p&gt;</description></item><item><title>P2LHAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#21516;&#26102;&#23454;&#29616;&#20154;&#31867;&#27963;&#21160;&#30340;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;</title><link>https://arxiv.org/abs/2403.08214</link><description>&lt;p&gt;
P2LHAP&#65306;&#22522;&#20110;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12289;&#20998;&#21106;&#21644;&#39044;&#27979;&#30340;Patch-to-Label Seq2Seq Transformer
&lt;/p&gt;
&lt;p&gt;
P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08214
&lt;/p&gt;
&lt;p&gt;
P2LHAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#21516;&#26102;&#23454;&#29616;&#20154;&#31867;&#27963;&#21160;&#30340;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#21516;&#26102;&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;&#20154;&#31867;&#27963;&#21160;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#36741;&#21161;&#29983;&#27963;&#31561;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#65292;&#32780;&#36825;&#20123;&#39046;&#22495;&#23545;&#20110;&#23454;&#26102;&#29702;&#35299;&#27491;&#22312;&#36827;&#34892;&#21644;&#21363;&#23558;&#21457;&#29983;&#30340;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;P2LHAP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#35299;&#20915;&#36825;&#19977;&#20010;&#20219;&#21153;&#12290;P2LHAP&#23558;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#21010;&#20998;&#20026;&#19968;&#31995;&#21015;&#8220;&#34917;&#19969;&#8221;&#65292;&#20316;&#20026;&#36755;&#20837;&#26631;&#35760;&#65292;&#24182;&#36755;&#20986;&#19968;&#31995;&#21015;&#21253;&#25324;&#39044;&#27979;&#30340;&#26410;&#26469;&#27963;&#21160;&#22312;&#20869;&#30340;&#34917;&#19969;&#32423;&#27963;&#21160;&#26631;&#31614;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21608;&#22260;&#34917;&#19969;&#26631;&#31614;&#30340;&#29420;&#29305;&#24179;&#28369;&#25216;&#26415;&#65292;&#21487;&#20934;&#30830;&#35782;&#21035;&#27963;&#21160;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;P2LHAP&#36890;&#36807;&#20256;&#24863;&#22120;&#20449;&#21495;&#36890;&#36947;&#29420;&#31435;&#30340;Transformer&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23398;&#20064;&#34917;&#19969;&#32423;&#34920;&#31034;&#12290;&#25152;&#26377;&#36890;&#36947;&#22312;&#25152;&#26377;&#24207;&#21015;&#19978;&#20849;&#20139;&#23884;&#20837;&#21644;Transformer&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08214v1 Announce Type: cross  Abstract: Traditional deep learning methods struggle to simultaneously segment, recognize, and forecast human activities from sensor data. This limits their usefulness in many fields such as healthcare and assisted living, where real-time understanding of ongoing and upcoming activities is crucial. This paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles all three tasks in a efficient single-task model. P2LHAP divides sensor data streams into a sequence of "patches", served as input tokens, and outputs a sequence of patch-level activity labels including the predicted future activities. A unique smoothing technique based on surrounding patch labels, is proposed to identify activity boundaries accurately. Additionally, P2LHAP learns patch-level representation by sensor signal channel-independent Transformer encoders and decoders. All channels share embedding and Transformer weights across all sequences. Evaluated on thre
&lt;/p&gt;</description></item><item><title>Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.05530</link><description>&lt;p&gt;
Gemini 1.5&#65306;&#35299;&#38145;&#36328;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05530
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20221;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gemini&#23478;&#26063;&#30340;&#26368;&#26032;&#27169;&#22411;Gemini 1.5 Pro&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22238;&#24518;&#21644;&#25512;&#29702;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#20013;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#20010;&#38271;&#25991;&#26723;&#21644;&#20960;&#23567;&#26102;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#12290;Gemini 1.5 Pro&#22312;&#21508;&#31181;&#24418;&#24335;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#24191;&#27867;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;Gemini 1.0 Ultra&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30456;&#21305;&#25932;&#29978;&#33267;&#36229;&#36807;&#12290;&#22312;&#30740;&#31350;Gemini 1.5 Pro&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#26497;&#38480;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33267;&#23569;10M&#26631;&#35760;&#30340;&#33539;&#22260;&#20869;&#32487;&#32493;&#25913;&#36827;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#20960;&#20046;&#23436;&#32654;&#22320;&#36798;&#21040;&#20102;&#36229;&#36807;99%&#30340;&#26816;&#32034;&#29575;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#27169;&#22411;&#22914;Claude 2.1&#65288;200k&#65289;&#21644;GPT-4 Turbo&#65288;128k&#65289;&#30340;&#19990;&#20195;&#24615;&#39134;&#36291;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05530v1 Announce Type: cross  Abstract: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00816</link><description>&lt;p&gt;
CFRet-DVQA&#65306;&#31895;&#21040;&#31934;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#29992;&#20110;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#65288;DVQA&#65289;&#26159;&#19968;&#20010;&#28041;&#21450;&#26681;&#25454;&#22270;&#20687;&#20869;&#23481;&#22238;&#31572;&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#24037;&#20316;&#20165;&#38480;&#20110;&#23450;&#20301;&#21333;&#39029;&#20869;&#30340;&#20449;&#24687;&#65292;&#19981;&#25903;&#25345;&#36328;&#39029;&#38754;&#38382;&#31572;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#23545;&#27169;&#22411;&#36755;&#20837;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#37096;&#20998;&#34987;&#25130;&#26029;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#23398;&#65292;&#31216;&#20026;CFRet-DVQA&#65292;&#37325;&#28857;&#25918;&#22312;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#19978;&#65292;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#19982;&#25152;&#25552;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#29255;&#27573;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#25991;&#26723;&#26631;&#31614;&#30340;&#39118;&#26684;&#30456;&#31526;&#12290;&#23454;&#39564;&#28436;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00816v1 Announce Type: cross  Abstract: Document Visual Question Answering (DVQA) is a task that involves responding to queries based on the content of images. Existing work is limited to locating information within a single page and does not facilitate cross-page question-and-answer interaction. Furthermore, the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer. In this study, we introduce a simple but effective methodology called CFRet-DVQA, which focuses on retrieval and efficient tuning to address this critical issue effectively. For that, we initially retrieve multiple segments from the document that correlate with the question at hand. Subsequently, we leverage the advanced reasoning abilities of the large language model (LLM), further augmenting its performance through instruction tuning. This approach enables the generation of answers that align with the style of the document labels. The experiments demo
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#26032;&#21160;&#20316;&#30340;&#33021;&#21147;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#23398;&#20064;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#34892;&#20026;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#31574;&#30053;&#25913;&#36827;&#21160;&#20316;&#65292;&#22686;&#24378;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15809</link><description>&lt;p&gt;
&#36890;&#36807;&#34892;&#20026;&#23398;&#20064;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Empowering Large Language Model Agents through Action Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15809
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26032;&#21160;&#20316;&#30340;&#33021;&#21147;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#23398;&#20064;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#34892;&#20026;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#31574;&#30053;&#25913;&#36827;&#21160;&#20316;&#65292;&#22686;&#24378;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#36817;&#26469;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#20174;&#35797;&#38169;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#36825;&#26159;&#26234;&#33021;&#34892;&#20026;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#26032;&#21160;&#20316;&#30340;&#33021;&#21147;&#23545;&#20110;LLM&#20195;&#29702;&#30340;&#23398;&#20064;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20154;&#31867;&#33258;&#28982;&#22320;&#25193;&#23637;&#20182;&#20204;&#30340;&#21160;&#20316;&#31354;&#38388;&#24182;&#36890;&#36807;&#32463;&#39564;&#23398;&#20064;&#21457;&#23637;&#25216;&#33021;&#65292;&#20294;LLM&#20195;&#29702;&#36890;&#24120;&#22312;&#22266;&#23450;&#30340;&#21160;&#20316;&#31354;&#38388;&#20869;&#25805;&#20316;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#25104;&#38271;&#28508;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#24320;&#25918;&#24335;&#34892;&#20026;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LearnAct&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#36845;&#20195;&#23398;&#20064;&#31574;&#30053;&#26469;&#21019;&#24314;&#21644;&#25913;&#36827;Python&#20989;&#25968;&#24418;&#24335;&#30340;&#21160;&#20316;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;LLM&#26681;&#25454;&#22312;&#22833;&#36133;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#35782;&#21035;&#20986;&#30340;&#38169;&#35823;&#65292;&#20462;&#35746;&#21644;&#26356;&#26032;&#24403;&#21069;&#21487;&#29992;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#22686;&#24378;&#21160;&#20316;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15809v1 Announce Type: new  Abstract: Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations acr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12062</link><description>&lt;p&gt;
&#22240;&#26524;&#24179;&#31561;&#20445;&#25252;&#19982;&#31639;&#27861;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Equal Protection as Algorithmic Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21746;&#23398;&#30340;&#25991;&#29486;&#24418;&#25104;&#20102;&#19981;&#21516;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#26631;&#20934;&#12290;&#20854;&#20013;&#26368;&#21463;&#20105;&#35758;&#30340;&#20998;&#31867;&#24179;&#31561;&#35201;&#27714;&#65292;&#39044;&#27979;&#31639;&#27861;&#30340;&#38169;&#35823;&#20998;&#31867;&#22312;&#34987;&#20445;&#25252;&#29305;&#24449;&#25152;&#25351;&#31034;&#30340;&#32676;&#20307;&#20013;&#20197;&#30456;&#31561;&#39057;&#29575;&#21457;&#29983;&#12290;&#23613;&#31649;&#20998;&#31867;&#24179;&#31561;&#20855;&#26377;&#30452;&#35266;&#21560;&#24341;&#21147;&#65292;&#20294;&#24050;&#21463;&#21040;&#25915;&#20987;&#12290;&#25105;&#20204;&#36716;&#21521;&#19968;&#20010;&#30456;&#20851;&#21407;&#21017;&#65292;&#21363;&#24179;&#31561;&#20445;&#25252;&#65292;&#35813;&#21407;&#21017;&#26368;&#21021;&#26159;&#22312;&#21009;&#20107;&#21496;&#27861;&#39046;&#22495;&#21457;&#23637;&#36215;&#26469;&#30340;&#12290;&#24179;&#31561;&#20445;&#25252;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#65288;&#23558;&#22312;&#35268;&#23450;&#30340;&#24847;&#20041;&#19978;&#20855;&#20307;&#35828;&#26126;&#65289;&#36827;&#34892;&#22343;&#31561;&#21270;&#65292;&#32780;&#19981;&#26159;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#27604;&#29575;&#22343;&#31561;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#31561;&#20445;&#25252;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20998;&#31867;&#24179;&#31561;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12062v1 Announce Type: cross  Abstract: Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09611</link><description>&lt;p&gt;
&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Privacy-Aware Sign Language Translation at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#32763;&#35793;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#25968;&#25454;&#31232;&#32570;&#12290;&#30446;&#21069;&#22312;&#32593;&#32476;&#19978;&#21487;&#29992;&#30340;&#22823;&#37096;&#20998;&#25163;&#35821;&#25968;&#25454;&#30001;&#20110;&#32570;&#20047;&#23545;&#40784;&#30340;&#23383;&#24149;&#32780;&#26080;&#27861;&#29992;&#20110;&#35757;&#32451;&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#38598;&#26469;&#25193;&#23637;&#25163;&#35821;&#32763;&#35793;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#20854;&#20013;&#21253;&#21547;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#65292;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;&#25163;&#35821;&#32763;&#35793;&#25216;&#26415;&#24212;&#35813;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SSVP-SLT&#65292;&#23427;&#21033;&#29992;&#21311;&#21517;&#21644;&#26410;&#27880;&#37322;&#30340;&#35270;&#39057;&#36827;&#34892;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#21033;&#29992;&#32463;&#36807;&#31579;&#36873;&#30340;&#24179;&#34892;&#25968;&#25454;&#38598;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#25163;&#35821;&#32763;&#35793;&#24494;&#35843;&#12290; SSVP-SLT&#22312;How2Sign&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24494;&#35843;&#21644;&#38646;&#27425;gloss-free&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#65292;&#27604;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;3&#20010;BLEU-4&#12290;&#36890;&#36807;&#21463;&#25511;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#35821;&#35328;&#21644;&#25163;&#35821;&#35789;&#27719;&#19978;&#37117;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09611v1 Announce Type: new  Abstract: A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#20108;&#36827;&#21046;&#20998;&#24067;&#65292;&#21487;&#35745;&#31639;&#27010;&#29575;&#30005;&#36335;&#27169;&#22411;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#26159;&#31561;&#20215;&#30340;&#65292;&#21363;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#30340;&#30005;&#36335;&#37117;&#21487;&#20197;&#36716;&#21270;&#20026;&#20219;&#20309;&#20854;&#20182;&#27169;&#22411;&#30340;&#30005;&#36335;&#65292;&#21482;&#38656;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#22686;&#21152;&#22823;&#23567;&#12290;&#23427;&#20204;&#26159;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#25512;&#26029;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09085</link><description>&lt;p&gt;
&#21487;&#35745;&#31639;&#27010;&#29575;&#30005;&#36335;&#30340;&#22810;&#39033;&#24335;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Polynomial Semantics of Tractable Probabilistic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#20108;&#36827;&#21046;&#20998;&#24067;&#65292;&#21487;&#35745;&#31639;&#27010;&#29575;&#30005;&#36335;&#27169;&#22411;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#26159;&#31561;&#20215;&#30340;&#65292;&#21363;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#30340;&#30005;&#36335;&#37117;&#21487;&#20197;&#36716;&#21270;&#20026;&#20219;&#20309;&#20854;&#20182;&#27169;&#22411;&#30340;&#30005;&#36335;&#65292;&#21482;&#38656;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#22686;&#21152;&#22823;&#23567;&#12290;&#23427;&#20204;&#26159;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#25512;&#26029;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#30005;&#36335;&#35745;&#31639;&#20195;&#34920;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#32447;&#24615;&#22810;&#39033;&#24335;&#12290;&#23427;&#20204;&#26159;&#21487;&#35745;&#31639;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25903;&#25345;&#39640;&#25928;&#30340;&#36793;&#38469;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#32771;&#34385;&#20102;&#22810;&#31181;&#22810;&#39033;&#24335;&#35821;&#20041;&#65288;&#20363;&#22914;&#65292;&#32593;&#32476;&#22810;&#39033;&#24335;&#12289;&#20284;&#28982;&#22810;&#39033;&#24335;&#12289;&#29983;&#25104;&#20989;&#25968;&#12289;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#29305;&#24449;&#22810;&#39033;&#24335;&#65289;&#12290;&#36825;&#20123;&#20998;&#24067;&#30340;&#22810;&#39033;&#24335;&#32534;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;&#22823;&#37096;&#20998;&#26159;&#26410;&#30693;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20108;&#36827;&#21046;&#20998;&#24067;&#65292;&#36825;&#20123;&#27010;&#29575;&#30005;&#36335;&#27169;&#22411;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#26159;&#31561;&#20215;&#30340;&#65292;&#21363;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#30340;&#30005;&#36335;&#37117;&#21487;&#20197;&#36716;&#21270;&#20026;&#20219;&#20309;&#20854;&#20182;&#27169;&#22411;&#30340;&#30005;&#36335;&#65292;&#21482;&#38656;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#22686;&#21152;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#30456;&#21516;&#31867;&#21035;&#30340;&#20998;&#24067;&#19978;&#37117;&#26159;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#25512;&#26029;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21517;&#20026;&#27010;&#29575;&#29983;&#25104;&#30005;&#36335;&#30340;&#22810;&#39033;&#24335;&#35821;&#20041;&#30340;&#33258;&#28982;&#25512;&#24191;&#65292;&#20197;&#36866;&#29992;&#20110;&#20998;&#31867;&#38543;&#26426;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09085v1 Announce Type: new Abstract: Probabilistic circuits compute multilinear polynomials that represent probability distributions. They are tractable models that support efficient marginal inference. However, various polynomial semantics have been considered in the literature (e.g., network polynomials, likelihood polynomials, generating functions, Fourier transforms, and characteristic polynomials). The relationships between these polynomial encodings of distributions is largely unknown. In this paper, we prove that for binary distributions, each of these probabilistic circuit models is equivalent in the sense that any circuit for one of them can be transformed into a circuit for any of the others with only a polynomial increase in size. They are therefore all tractable for marginal inference on the same class of distributions. Finally, we explore the natural extension of one such polynomial semantics, called probabilistic generating circuits, to categorical random varia
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07812</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#24605;&#32500;&#36807;&#31243;&#20316;&#20026;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Thought Process as Sequential Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07812
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#36741;&#21161;&#20154;&#31867;&#24182;&#23637;&#29616;&#20986;"&#26234;&#33021;&#30340;&#28779;&#33457;"&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20960;&#20010;&#24320;&#25918;&#25361;&#25112;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65306;&#22914;&#23545;&#38544;&#31169;&#30340;&#20851;&#27880;&#12289;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273;&#12289;&#38590;&#20197;&#22788;&#29702;&#38271;&#25991;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;(RATP)&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#33719;&#21462;&#22806;&#37096;&#30693;&#35782;&#65292;RATP&#23558;LLM&#30340;&#24605;&#32771;&#29983;&#25104;&#36807;&#31243;&#23450;&#24335;&#20026;&#22810;&#27493;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#36825;&#31181;&#24605;&#32771;&#36807;&#31243;&#65292;RATP&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#22312;&#22788;&#29702;&#20855;&#26377;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;LLM&#35757;&#32451;&#26041;&#27861;&#21463;&#21040;&#20262;&#29702;&#21644;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;RATP&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated their strong ability to assist people and show "sparks of intelligence". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20462;&#25913;&#22522;&#20110;&#36712;&#36857;&#25104;&#26412;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#27169;&#20223;&#22909;&#30340;&#36712;&#36857;&#21644;&#36991;&#20813;&#22351;&#30340;&#36712;&#36857;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2312.10385</link><description>&lt;p&gt;
&#27169;&#20223;&#22909;&#30340;&#24182;&#36991;&#20813;&#22351;&#30340;&#65306;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#22686;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10385
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20462;&#25913;&#22522;&#20110;&#36712;&#36857;&#25104;&#26412;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#27169;&#20223;&#22909;&#30340;&#36712;&#36857;&#21644;&#36991;&#20813;&#22351;&#30340;&#36712;&#36857;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#25191;&#34892;&#23433;&#20840;&#21160;&#20316;&#30340;&#27969;&#34892;&#26694;&#26550;&#26159;&#32422;&#26463;RL&#65292;&#20854;&#20013;&#21033;&#29992;&#22522;&#20110;&#36712;&#36857;&#30340;&#25104;&#26412;&#32422;&#26463;&#65288;&#25110;&#20854;&#20182;&#25104;&#26412;&#24230;&#37327;&#65289;&#26469;&#25191;&#34892;&#23433;&#20840;&#25805;&#20316;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#22312;&#26368;&#22823;&#21270;&#26399;&#26395;&#22870;&#21169;&#30340;&#21516;&#26102;&#25191;&#34892;&#36825;&#20123;&#32422;&#26463;&#12290;&#26368;&#36817;&#35299;&#20915;&#32422;&#26463;RL&#30340;&#26041;&#27861;&#23558;&#22522;&#20110;&#36712;&#36857;&#30340;&#25104;&#26412;&#32422;&#26463;&#36716;&#25442;&#20026;&#19968;&#20010;&#26367;&#20195;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;RL&#26041;&#27861;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#26469;&#35299;&#20915;&#12290;&#36825;&#31867;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#22312;&#27599;&#20010;&#29366;&#24577;&#19978;&#23545;&#25104;&#26412;&#32422;&#26463;&#36827;&#34892;&#36807;&#24230;&#25110;&#19981;&#36275;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#19981;&#20462;&#25913;&#22522;&#20110;&#36712;&#36857;&#30340;&#25104;&#26412;&#32422;&#26463;&#65292;&#32780;&#26159;&#27169;&#20223;&#8220;&#22909;&#8221;&#36712;&#36857;&#24182;&#36991;&#20813;&#20174;&#36880;&#27493;&#25913;&#36827;&#30340;&#31574;&#30053;&#29983;&#25104;&#30340;&#8220;&#22351;&#8221;&#36712;&#36857;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;oracle&#65292;&#21033;&#29992;&#22870;&#21169;&#38408;&#20540;&#65288;&#38543;&#23398;&#20064;&#21464;&#21270;&#65289;&#21644;&#25972;&#20307;&#25104;&#26412;&#32422;&#26463;&#26469;&#23558;&#36712;&#36857;&#26631;&#35760;&#20026;&#8220;&#22909;&#8221;&#25110;&#8220;&#22351;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10385v3 Announce Type: replace-cross  Abstract: A popular framework for enforcing safe actions in Reinforcement Learning (RL) is Constrained RL, where trajectory based constraints on expected cost (or other cost measures) are employed to enforce safety and more importantly these constraints are enforced while maximizing expected reward. Most recent approaches for solving Constrained RL convert the trajectory based cost constraint into a surrogate problem that can be solved using minor modifications to RL methods. A key drawback with such approaches is an over or underestimation of the cost constraint at each state. Therefore, we provide an approach that does not modify the trajectory based cost constraint and instead imitates ``good'' trajectories and avoids ``bad'' trajectories generated from incrementally improving policies. We employ an oracle that utilizes a reward threshold (which is varied with learning) and the overall cost constraint to label trajectories as ``good''
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21551;&#21457;&#24335;&#39537;&#21160;&#30340;&#31867;&#27604;&#38142;&#25509;&#20419;&#36827;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21551;&#21457;&#24335;&#65292;&#24182;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22788;&#29702;&#26032;&#24773;&#20917;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.06555</link><description>&lt;p&gt;
&#21551;&#21457;&#39537;&#21160;&#30340;&#31867;&#27604;&#38142;&#25509;&#20419;&#36827;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Heuristic-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06555
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21551;&#21457;&#24335;&#39537;&#21160;&#30340;&#31867;&#27604;&#38142;&#25509;&#20419;&#36827;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21551;&#21457;&#24335;&#65292;&#24182;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22788;&#29702;&#26032;&#24773;&#20917;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#20943;&#36731;&#36825;&#19968;&#20219;&#21153;&#23545;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21551;&#21457;&#39537;&#21160;&#30340;&#31867;&#27604;&#38142;&#25509;&#65288;HD-LoA&#65289;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#31034;&#20363;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20026;EAE&#37327;&#36523;&#23450;&#21046;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#24182;&#39564;&#35777;&#20102;LLMs&#36890;&#36807;ICL&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21551;&#21457;&#24335;&#12290;&#22522;&#20110;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;&#21551;&#21457;&#24335;&#39537;&#21160;&#31034;&#33539;&#26500;&#24314;&#26041;&#27861;&#65292;&#23558;&#26434;&#20081;&#30340;&#31034;&#20363;&#36873;&#25321;&#36807;&#31243;&#36716;&#21270;&#20026;&#24378;&#35843;&#20219;&#21153;&#21551;&#21457;&#24335;&#30340;&#26377;&#26465;&#19981;&#32010;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#21463;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31867;&#27604;&#38142;&#25509;&#25552;&#31034;&#65292;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#23558;&#26032;&#24773;&#20917;&#31867;&#27604;&#20110;&#24050;&#30693;&#24773;&#20917;&#26469;&#22788;&#29702;&#26032;&#24773;&#20917;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#22312;&#26377;&#38480;ICL&#31034;&#20363;&#20197;&#22806;&#30340;&#26410;&#35265;&#31867;&#21035;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06555v2 Announce Type: replace-cross  Abstract: In this study, we investigate in-context learning (ICL) in document-level event argument extraction (EAE) to alleviate the dependency on large-scale labeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting to address the challenge of example selection and to develop a prompting strategy tailored for EAE. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations via ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a methodical method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL exa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.12731</link><description>&lt;p&gt;
SHAP&#35780;&#20998;&#22312;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Distributional Uncertainty of the SHAP score in Explainable Machine Learning. (arXiv:2401.12731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#23646;&#20998;&#25968;&#21453;&#26144;&#20102;&#36755;&#20837;&#23454;&#20307;&#20013;&#30340;&#29305;&#24449;&#20540;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#35780;&#20998;&#20043;&#19968;&#26159;SHAP&#35780;&#20998;&#65292;&#23427;&#26159;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;&#35813;&#35780;&#20998;&#30340;&#23450;&#20041;&#20381;&#36182;&#20110;&#23454;&#20307;&#32676;&#20307;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#36890;&#24120;&#19981;&#30693;&#36947;&#31934;&#30830;&#30340;&#20998;&#24067;&#65292;&#22240;&#27492;&#38656;&#35201;&#20027;&#35266;&#22320;&#36827;&#34892;&#20998;&#37197;&#25110;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#35780;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#30693;&#36947;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#30340;SHAP&#35780;&#20998;&#25512;&#29702;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21253;&#21547;&#28508;&#22312;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#32780;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#25104;&#20026;&#22312;&#35813;&#21306;&#22495;&#19978;&#23450;&#20041;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25214;&#21040;&#35813;&#20989;&#25968;&#30340;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attribution scores reflect how important the feature values in an input entity are for the output of a machine learning model. One of the most popular attribution scores is the SHAP score, which is an instantiation of the general Shapley value used in coalition game theory. The definition of this score relies on a probability distribution on the entity population. Since the exact distribution is generally unknown, it needs to be assigned subjectively or be estimated from data, which may lead to misleading feature scores. In this paper, we propose a principled framework for reasoning on SHAP scores under unknown entity population distributions. In our framework, we consider an uncertainty region that contains the potential distributions, and the SHAP score of a feature becomes a function defined over this region. We study the basic problems of finding maxima and minima of this function, which allows us to determine tight ranges for the SHAP scores of all features. In particular, we pinp
&lt;/p&gt;</description></item><item><title>COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04353</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04353
&lt;/p&gt;
&lt;p&gt;
COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#19982;&#22806;&#37096;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25511;&#21046;&#20219;&#21153;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#20219;&#21153;&#19978;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#25968;&#23383;&#29366;&#24577;&#30340;&#20195;&#29702;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25104;&#21151;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11984</link><description>&lt;p&gt;
&#34920;&#31034;&#25277;&#35937;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#28608;&#21169;&#65306;&#22522;&#20110;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Abstractions as Incentives for Reinforcement Learning Agents: A Robotic Grasping Case Study. (arXiv:2309.11984v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#20219;&#21153;&#19978;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#25968;&#23383;&#29366;&#24577;&#30340;&#20195;&#29702;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25104;&#21151;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#19968;&#20010;&#36866;&#24403;&#30340;&#29615;&#22659;&#34920;&#31034;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20915;&#31574;&#36807;&#31243;&#24182;&#19981;&#24635;&#26159;&#31616;&#21333;&#30340;&#12290;&#29366;&#24577;&#34920;&#31034;&#24212;&#35813;&#36275;&#22815;&#21253;&#23481;&#65292;&#20197;&#20415;&#35753;&#20195;&#29702;&#33021;&#22815;&#20449;&#24687;&#22320;&#20915;&#23450;&#20854;&#34892;&#21160;&#65292;&#24182;&#19988;&#36275;&#22815;&#32039;&#20945;&#65292;&#20197;&#25552;&#39640;&#31574;&#30053;&#35757;&#32451;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#20195;&#29702;&#22312;&#29305;&#23450;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#23545;&#31216;&#21644;&#24179;&#38754;&#29289;&#20307;&#25235;&#21462;&#65289;&#19978;&#35299;&#20915;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20174;&#20855;&#26377;&#23436;&#25972;&#31995;&#32479;&#30693;&#35782;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#24320;&#22987;&#65292;&#36890;&#36807;&#25163;&#24037;&#25968;&#23383;&#34920;&#31034;&#21040;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#36880;&#28176;&#20943;&#23569;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#30340;&#24341;&#20837;&#37327;&#65292;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#29366;&#24577;&#34920;&#31034;&#25277;&#35937;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27599;&#31181;&#34920;&#31034;&#23545;&#20195;&#29702;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35299;&#20915;&#20219;&#21153;&#20197;&#21450;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#21487;&#36716;&#31227;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25968;&#23383;&#29366;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Choosing an appropriate representation of the environment for the underlying decision-making process of the \gls{RL} agent is not always straightforward. The state representation should be inclusive enough to allow the agent to informatively decide on its actions and compact enough to increase sample efficiency for policy training. Given this outlook, this work examines the effect of various state representations in incentivizing the agent to solve a specific robotic task: antipodal and planar object grasping. A continuum of state representation abstractions is defined, starting from a model-based approach with complete system knowledge, through hand-crafted numerical, to image-based representations with decreasing level of induced task-specific knowledge. We examine the effects of each representation in the ability of the agent to solve the task in simulation and the transferability of the learned policy to the real robot. The results show that RL agents using numerical states can per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#20197;&#27491;&#30830;&#24615;&#30340;&#35282;&#24230;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#19982;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#36923;&#36753;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.10092</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#65306;&#30693;&#36947;&#20309;&#26102;&#20570;&#20160;&#20040;&#21644;&#20309;&#26102;&#23547;&#27714;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help. (arXiv:2309.10092v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#20197;&#27491;&#30830;&#24615;&#30340;&#35282;&#24230;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#19982;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#36923;&#36753;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#20219;&#21153;&#26159;&#20197;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#34920;&#36798;&#24182;&#20197;&#26102;&#38388;&#21644;&#36923;&#36753;&#39034;&#24207;&#23436;&#25104;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#12290;&#20026;&#20102;&#27491;&#24335;&#23450;&#20041;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;NL&#30340;&#21407;&#23376;&#35859;&#35789;&#22312;LTL&#19978;&#23450;&#20041;&#20102;&#27169;&#22411;&#12290;&#36825;&#19982;&#30456;&#20851;&#30340;&#35268;&#21010;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21407;&#23376;&#35859;&#35789;&#19978;&#23450;&#20041;&#20102;&#25429;&#25417;&#25152;&#38656;&#20302;&#32423;&#31995;&#32479;&#37197;&#32622;&#30340;LTL&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#26426;&#22120;&#20154;&#35745;&#21010;&#65292;&#28385;&#36275;&#22522;&#20110;NL&#30340;&#21407;&#23376;&#21629;&#39064;&#23450;&#20041;&#30340;LTL&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#19968;&#20010;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#22312;&#20110;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#19982;&#36825;&#20123;LTL&#32534;&#30721;&#30340;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HERACLEs&#65292;&#19968;&#20010;&#20998;&#23618;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#22120;&#65292;&#23427;&#20381;&#36182;&#20110;&#29616;&#26377;&#24037;&#20855;&#30340;&#26032;&#22411;&#25972;&#21512;&#65292;&#21253;&#25324;&#65288;i&#65289;&#33258;&#21160;&#26426;&#29702;&#35770;&#65292;&#20197;&#30830;&#23450;&#26426;&#22120;&#20154;&#24212;&#35813;&#23436;&#25104;&#30340;NL&#25351;&#23450;&#30340;&#23376;&#20219;&#21153;&#20197;&#25512;&#36827;&#20219;&#21153;&#36827;&#23637;&#65307;
&lt;/p&gt;
&lt;p&gt;
This paper addresses a new motion planning problem for mobile robots tasked with accomplishing multiple high-level sub-tasks, expressed using natural language (NL), in a temporal and logical order. To formally define such missions, we leverage LTL defined over NL-based atomic predicates modeling the considered NL-based sub-tasks. This is contrast to related planning approaches that define LTL tasks over atomic predicates capturing desired low-level system configurations. Our goal is to design robot plans that satisfy LTL tasks defined over NL-based atomic propositions. A novel technical challenge arising in this setup lies in reasoning about correctness of a robot plan with respect to such LTL-encoded tasks. To address this problem, we propose HERACLEs, a hierarchical conformal natural language planner, that relies on a novel integration of existing tools that include (i) automata theory to determine the NL-specified sub-task the robot should accomplish next to make mission progress; (
&lt;/p&gt;</description></item><item><title>CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12539</link><description>&lt;p&gt;
CALM: &#19968;&#31181;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12539
&lt;/p&gt;
&lt;p&gt;
CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#37327;&#21270;&#21644;&#27604;&#36739;&#23427;&#20204;&#22312;&#31038;&#20250;&#21644;&#20154;&#21475;&#23398;&#20559;&#35265;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#28508;&#22312;&#30340;&#21361;&#23475;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#20559;&#35265;&#27979;&#37327;&#25968;&#25454;&#38598;&#23545;&#20110;&#20154;&#24037;&#35774;&#35745;&#27169;&#26495;&#30340;&#25200;&#21160;&#25935;&#24863;&#65292;&#22240;&#27492;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#65288;CALM&#65289;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#37327;&#21270;LMs&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#26032;&#38395;&#25991;&#31456;&#65289;&#30340;16&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#36807;&#28388;&#20986;224&#20010;&#27169;&#26495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;78,400&#20010;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#27169;&#26495;&#38271;&#24230;&#30340;&#21464;&#24322;&#31243;&#24230;&#31561;&#25351;&#26631;&#65292;&#27604;&#36739;CALM&#19982;&#20808;&#21069;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#27979;&#35797;&#20854;&#23545;&#32454;&#24494;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30456;&#23545;&#20110;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#22240;&#27492;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;20&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24110;&#21161;&#29992;&#25143;&#21028;&#26029;&#26631;&#39064;&#20934;&#30830;&#24615;&#21644;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#26041;&#38754;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#35823;&#23548;&#29992;&#25143;&#23545;&#30495;&#23454;&#26631;&#39064;&#30340;&#20449;&#20208;&#65292;&#24182;&#22686;&#21152;&#23545;&#26410;&#30830;&#23450;&#34394;&#20551;&#26631;&#39064;&#30340;&#20449;&#20208;&#12290;</title><link>http://arxiv.org/abs/2308.10800</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#26080;&#25928;&#19988;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#24615;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is ineffective and potentially harmful for fact checking. (arXiv:2308.10800v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10800
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24110;&#21161;&#29992;&#25143;&#21028;&#26029;&#26631;&#39064;&#20934;&#30830;&#24615;&#21644;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#26041;&#38754;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#35823;&#23548;&#29992;&#25143;&#23545;&#30495;&#23454;&#26631;&#39064;&#30340;&#20449;&#20208;&#65292;&#24182;&#22686;&#21152;&#23545;&#26410;&#30830;&#23450;&#34394;&#20551;&#26631;&#39064;&#30340;&#20449;&#20208;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#26159;&#23545;&#25239;&#38169;&#35823;&#20449;&#24687;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#20294;&#26159;&#23427;&#22312;&#35268;&#27169;&#19978;&#30340;&#23454;&#26045;&#21463;&#21040;&#20102;&#32593;&#32476;&#19978;&#20449;&#24687;&#36807;&#20110;&#24222;&#22823;&#30340;&#38459;&#30861;&#12290;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#20204;&#22312;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#30340;&#20107;&#23454;&#26680;&#26597;&#20449;&#24687;&#26102;&#30340;&#20316;&#29992;&#26426;&#21046;&#24182;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#39044;&#20808;&#30331;&#35760;&#30340;&#38543;&#26426;&#23545;&#29031;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19968;&#27454;&#28909;&#38376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#20107;&#23454;&#26680;&#26597;&#23545;&#25919;&#27835;&#26032;&#38395;&#20449;&#20208;&#21644;&#20998;&#20139;&#24847;&#22270;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#35813;&#20154;&#24037;&#26234;&#33021;&#22312;&#25581;&#31359;&#34394;&#20551;&#26631;&#39064;&#26041;&#38754;&#34920;&#29616;&#24471;&#30456;&#24403;&#19981;&#38169;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#24182;&#27809;&#26377;&#23545;&#21442;&#19982;&#32773;&#35782;&#21035;&#26631;&#39064;&#20934;&#30830;&#24615;&#25110;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#30340;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#20107;&#23454;&#26680;&#26597;&#22120;&#20855;&#26377;&#21361;&#23475;&#24615;&#65306;&#23558;&#19968;&#20123;&#30495;&#23454;&#26631;&#39064;&#35823;&#26631;&#20026;&#34394;&#20551;&#20250;&#38477;&#20302;&#23545;&#20854;&#30340;&#20449;&#20208;&#65292;&#32780;&#23545;&#20854;&#26410;&#30830;&#23450;&#30340;&#34394;&#20551;&#26631;&#39064;&#21017;&#20250;&#22686;&#21152;&#23545;&#20854;&#30340;&#20449;&#20208;&#12290;&#22312;&#31215;&#26497;&#26041;&#38754;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#27491;&#30830;&#26631;&#23450;&#26631;&#39064;&#30340;&#20998;&#20139;&#24847;&#24895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. Recent artificial intelligence (AI) language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. Here we investigate the impact of fact checks generated by a popular AI model on belief in, and sharing intent of, political news in a preregistered randomized control experiment. Although the AI performs reasonably well in debunking false headlines, we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. However, the AI fact-checker is harmful in specific cases: it decreases beliefs in true headlines that it mislabels as false and increases beliefs for false headlines that it is unsure about. On the positive side, the AI increases sharing intents for correctly labeled t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19981;&#21516;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20276;&#38543;&#20914;&#31361;&#39044;&#27979;&#30340;&#35299;&#37322;&#26469;&#20943;&#23569;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#12290;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#35774;&#32622;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#20174;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07636</link><description>&lt;p&gt;
&#19981;&#21516;&#35299;&#37322;: &#21033;&#29992;&#20998;&#27495;&#20943;&#23569;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance. (arXiv:2307.07636v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07636
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19981;&#21516;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20276;&#38543;&#20914;&#31361;&#39044;&#27979;&#30340;&#35299;&#37322;&#26469;&#20943;&#23569;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#12290;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#35774;&#32622;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#20174;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21487;&#35299;&#37322;&#24615;&#26159;&#26085;&#30410;&#22797;&#26434;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#19968;&#20010;&#21487;&#21462;&#29305;&#24449;&#65292;&#20294;&#29616;&#20195;&#35299;&#37322;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#19981;&#19968;&#33268;&#21644;&#30683;&#30462;&#30340;&#12290;&#35299;&#37322;&#30340;&#35821;&#20041;&#24182;&#19981;&#24635;&#26159;&#23436;&#20840;&#29702;&#35299;&#30340; - &#35299;&#37322;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;"&#35299;&#37322;"&#19968;&#20010;&#20915;&#31574;&#65292;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21482;&#26159;&#25903;&#25345;&#19968;&#20010;&#20915;&#31574;&#65311;&#25105;&#20204;&#33021;&#21542;&#24110;&#21161;&#20154;&#20204;&#20174;&#20276;&#38543;&#27491;&#30830;&#39044;&#27979;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#27934;&#23519;&#21147;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#20381;&#36182;&#35299;&#37322;&#25152;&#25552;&#20513;&#30340;&#38169;&#35823;&#39044;&#27979;&#65311;&#22312;&#36825;&#20010;&#35282;&#24230;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#35299;&#37322;&#27010;&#24565;: &#20276;&#38543;&#20914;&#31361;&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#20102;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#35774;&#32622;&#19979;&#19981;&#21516;&#35299;&#37322;&#30340;&#20248;&#21183;&#65292;&#20854;&#20013;&#20855;&#26377;&#30456;&#20284;&#24615;&#33021;&#30340;&#22810;&#20010;&#27169;&#22411;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#19981;&#21516;&#30340;&#35299;&#37322;&#21487;&#20197;&#36890;&#36807;&#35843;&#29992;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#23454;&#29616;&#12290;&#36890;&#36807;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#35299;&#37322;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
While explainability is a desirable characteristic of increasingly complex black-box models, modern explanation methods have been shown to be inconsistent and contradictory. The semantics of explanations is not always fully understood - to what extent do explanations "explain" a decision and to what extent do they merely advocate for a decision? Can we help humans gain insights from explanations accompanying correct predictions and not over-rely on incorrect predictions advocated for by explanations? With this perspective in mind, we introduce the notion of dissenting explanations: conflicting predictions with accompanying explanations. We first explore the advantage of dissenting explanations in the setting of model multiplicity, where multiple models with similar performance may have different predictions. In such cases, providing dissenting explanations could be done by invoking the explanations of disagreeing models. Through a pilot study, we demonstrate that dissenting explanation
&lt;/p&gt;</description></item><item><title>Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07176</link><description>&lt;p&gt;
Safe DreamerV3&#65306;&#24102;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3: Safe Reinforcement Learning with World Models. (arXiv:2307.07176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07176
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#36824;&#27809;&#26377;&#23454;&#29616;, &#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#26410;&#33021;&#28385;&#36275;&#36825;&#20123;&#31995;&#32479;&#30340;&#22522;&#26412;&#23433;&#20840;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#26469;&#22686;&#24378;&#23433;&#20840;&#24615;&#65292;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#65292;&#21253;&#25324;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#36827;&#34892;&#20840;&#38754;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#35757;&#32451;&#65292;&#20063;&#26080;&#27861;&#23454;&#29616;&#38646;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Safe DreamerV3&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#22312;SafeRL&#20013;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#65292;&#26159;&#31532;&#19968;&#20010;&#22312;Safety-Gymnasium&#22522;&#20934;&#20013;&#23454;&#29616;&#36817;&#20046;&#38646;&#25104;&#26412;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#31449;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://sites.google.com/view/safedreamerv3&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread application of Reinforcement Learning (RL) in real-world situations is yet to come to fruition, largely as a result of its failure to satisfy the essential safety demands of such systems. Existing safe reinforcement learning (SafeRL) methods, employing cost functions to enhance safety, fail to achieve zero-cost in complex scenarios, including vision-only tasks, even with comprehensive data sampling and training. To address this, we introduce Safe DreamerV3, a novel algorithm that integrates both Lagrangian-based and planning-based methods within a world model. Our methodology represents a significant advancement in SafeRL as the first algorithm to achieve nearly zero-cost in both low-dimensional and vision-only tasks within the Safety-Gymnasium benchmark. Our project website can be found in: https://sites.google.com/view/safedreamerv3.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06865</link><description>&lt;p&gt;
&#25552;&#31034;&#19981;&#24212;&#34987;&#35270;&#20026;&#31192;&#23494;&#65306;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#30340;&#25104;&#21151;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success. (arXiv:2307.06865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#36890;&#24120;&#36890;&#36807;&#25552;&#31034;&#25216;&#26415;&#26469;&#25511;&#21046;&#65292;&#20854;&#20013;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#26597;&#35810;&#20197;&#26088;&#22312;&#25351;&#23548;&#27169;&#22411;&#22312;&#35813;&#26597;&#35810;&#19978;&#30340;&#34892;&#20026;&#30340;&#25552;&#31034;&#20316;&#20026;&#21069;&#32512;&#12290;&#20844;&#21496;&#29992;&#20110;&#25351;&#23548;&#20854;&#27169;&#22411;&#30340;&#25552;&#31034;&#36890;&#24120;&#34987;&#35270;&#20026;&#31192;&#23494;&#65292;&#38544;&#34255;&#22312;&#26597;&#35810;&#30340;&#29992;&#25143;&#20043;&#22806;&#12290;&#23427;&#20204;&#29978;&#33267;&#34987;&#35270;&#20026;&#21487;&#20197;&#20080;&#21334;&#30340;&#21830;&#21697;&#12290;&#28982;&#32780;&#65292;&#26377;&#32463;&#39564;&#24615;&#30340;&#35777;&#25454;&#26174;&#31034;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#29992;&#25143;&#20173;&#28982;&#21487;&#20197;&#25552;&#21462;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#28304;&#21644;&#22810;&#20010;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#23454;&#38469;&#19978;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#39033;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#24230;&#37327;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#23427;&#20204;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#35770;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
&lt;/p&gt;</description></item><item><title>DR-HAI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20114;&#21160;&#35843;&#21644;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#24322;&#65292;&#20026;&#20419;&#36827;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.14694</link><description>&lt;p&gt;
DR-HAI: &#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#20013;&#22522;&#20110;&#35770;&#35777;&#30340;&#36777;&#35777;&#35843;&#21644;
&lt;/p&gt;
&lt;p&gt;
DR-HAI: Argumentation-based Dialectical Reconciliation in Human-AI Interactions. (arXiv:2306.14694v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14694
&lt;/p&gt;
&lt;p&gt;
DR-HAI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20114;&#21160;&#35843;&#21644;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#24322;&#65292;&#20026;&#20419;&#36827;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DR-HAI&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25193;&#23637;&#20154;&#31867;&#24863;&#30693;&#35268;&#21010;&#20013;&#24120;&#29992;&#30340;&#27169;&#22411;&#35843;&#21644;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#35770;&#35777;&#30340;&#23545;&#35805;&#33539;&#24335;&#65292;DR-HAI&#33021;&#22815;&#36827;&#34892;&#20114;&#21160;&#35843;&#21644;&#65292;&#35299;&#20915;&#35299;&#37322;&#32773;&#21644;&#34987;&#35299;&#37322;&#32773;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#24322;&#12290;&#25105;&#20204;&#23545;DR-HAI&#30340;&#25805;&#20316;&#35821;&#20041;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#25551;&#36848;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#23545;&#20854;&#25928;&#26524;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DR-HAI&#20026;&#20419;&#36827;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#28508;&#21147;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DR-HAI -- a novel argumentation-based framework designed to extend model reconciliation approaches, commonly used in human-aware planning, for enhanced human-AI interaction. By adopting an argumentation-based dialogue paradigm, DR-HAI enables interactive reconciliation to address knowledge discrepancies between an explainer and an explainee. We formally describe the operational semantics of DR-HAI, provide theoretical guarantees, and empirically evaluate its efficacy. Our findings suggest that DR-HAI offers a promising direction for fostering effective human-AI interactions.
&lt;/p&gt;</description></item></channel></rss>