<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;SAMMO&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32534;&#35793;&#26102;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02319</link><description>&lt;p&gt;
Prompt&#20316;&#20026;&#31243;&#24207;&#65306;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39640;&#25928;&#32534;&#35793;&#26102;Prompt&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SAMMO&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32534;&#35793;&#26102;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29616;&#22312;&#33021;&#22788;&#29702;&#26356;&#38271;&#26356;&#22797;&#26434;&#30340;&#36755;&#20837;&#65292;&#36825;&#20419;&#36827;&#20102;&#26356;&#22797;&#26434;&#25552;&#31034;&#30340;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25552;&#31034;&#36890;&#24120;&#38656;&#35201;&#19968;&#20123;&#35843;&#25972;&#20197;&#25552;&#39640;&#37096;&#32626;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#38543;&#30528;&#25552;&#31034;&#22797;&#26434;&#24230;&#21644;LLM&#24378;&#24230;&#30340;&#22686;&#21152;&#65292;&#35768;&#22810;&#25552;&#31034;&#20248;&#21270;&#25216;&#26415;&#24050;&#19981;&#20877;&#36275;&#22815;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SAMMO&#65292;&#19968;&#20010;&#29992;&#20110;&#20803;&#25552;&#31034;&#31243;&#24207;&#30340;{\em &#32534;&#35793;&#26102;}&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#25552;&#31034;&#34920;&#31034;&#20026;&#32467;&#26500;&#21270;&#23545;&#35937;&#65292;&#20801;&#35768;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#25628;&#32034;&#19968;&#32452;&#20016;&#23500;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;SAMMO&#25512;&#24191;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#25351;&#20196;&#35843;&#25972;&#12289;RAG&#31649;&#32447;&#35843;&#25972;&#21644;&#25552;&#31034;&#21387;&#32553;&#26041;&#38754;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#25918;&#25152;&#26377;&#20195;&#30721;&#20379;&#22823;&#23478;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02319v1 Announce Type: cross  Abstract: Large language models (LLMs) can now handle longer and more complex inputs, which facilitate the use of more elaborate prompts. However, prompts often require some tuning to improve performance for deployment. Recent work has proposed automatic prompt optimization methods, but as prompt complexity and LLM strength increase, many prompt optimization techniques are no longer sufficient and a new approach is needed to optimize {\em meta prompt programs}. To address this, we introduce SAMMO, a framework for {\em compile-time} optimizations of metaprompt programs, which represent prompts as structured objects that allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs.   We make all code available open-sou
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#27714;&#35299;&#22120; Q4RPD&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#21253;&#35065;&#25237;&#36882;&#36335;&#24452;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#38382;&#39064;&#31616;&#21270;&#21644;&#25216;&#26415;&#25463;&#24452;&#65292;&#38024;&#23545;&#21253;&#35065;&#37325;&#37327;&#21644;&#23610;&#23544;&#30340;&#30495;&#23454;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.15114</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#36864;&#28779;&#22120;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#21253;&#35065;&#25237;&#36882;&#36335;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving a Real-World Package Delivery Routing Problem Using Quantum Annealers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#27714;&#35299;&#22120; Q4RPD&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#21253;&#35065;&#25237;&#36882;&#36335;&#24452;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#38382;&#39064;&#31616;&#21270;&#21644;&#25216;&#26415;&#25463;&#24452;&#65292;&#38024;&#23545;&#21253;&#35065;&#37325;&#37327;&#21644;&#23610;&#23544;&#30340;&#30495;&#23454;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#20851;&#20110;&#37327;&#23376;&#35745;&#31639;&#21644;&#36335;&#24452;&#38382;&#39064;&#20043;&#38388;&#30340;&#30740;&#31350;&#38750;&#24120;&#22810;&#20135;&#12290;&#22823;&#37096;&#20998;&#20316;&#21697;&#22260;&#32469;&#30528;&#32463;&#20856;&#38382;&#39064;&#65292;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#25110;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#12290;&#23613;&#31649;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#19981;&#21487;&#21542;&#35748;&#30340;&#26159;&#23427;&#20204;&#20197;&#23398;&#26415;&#20026;&#23548;&#21521;&#30340;&#29305;&#28857;&#26080;&#27861;&#28385;&#36275;&#29616;&#23454;&#19990;&#30028;&#30340;&#35201;&#27714;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#38382;&#39064;&#31616;&#21270;&#25110;&#25216;&#26415;&#25463;&#24452;&#12290;&#30456;&#21453;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#27714;&#35299;&#22120;&#65292;&#21629;&#21517;&#20026;Q4RPD&#65292;&#32771;&#34385;&#21040;&#19968;&#31995;&#21015;&#30495;&#23454;&#32422;&#26463;&#26465;&#20214;&#65292;&#22914;&#24322;&#26500;&#36710;&#38431;&#12289;&#20248;&#20808;&#25237;&#36882;&#21644;&#26681;&#25454;&#21253;&#35065;&#37325;&#37327;&#21644;&#23610;&#23544;&#30830;&#23450;&#30340;&#23481;&#37327;&#12290;Q4RPD&#37319;&#29992;&#20102;D-Wave&#30340;Leap&#21463;&#38480;&#20108;&#27425;&#27169;&#22411;&#28151;&#21512;&#27714;&#35299;&#22120;&#12290;&#20026;&#20102;&#35777;&#26126;Q4RPD&#30340;&#24212;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#39564;&#32452;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15114v1 Announce Type: cross  Abstract: Research focused on the conjunction between quantum computing and routing problems has been very prolific in recent years. Most of the works revolve around classical problems such as the Traveling Salesman Problem or the Vehicle Routing Problem. Even though working on these problems is valuable, it is also undeniable that their academic-oriented nature falls short of real-world requirements. The main objective of this research is to present a solving method for realistic instances, avoiding problem relaxations or technical shortcuts. Instead, a quantum-classical hybrid solver has been developed, coined Q4RPD, that considers a set of real constraints such as a heterogeneous fleet of vehicles, priority deliveries, and capacities characterized by two values: weight and dimensions of the packages. Q4RPD resorts to the Leap Constrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the application of Q4RPD, an experimentation compo
&lt;/p&gt;</description></item><item><title>LlamaFactory&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.13372</link><description>&lt;p&gt;
LlamaFactory&#65306;100&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13372
&lt;/p&gt;
&lt;p&gt;
LlamaFactory&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#24494;&#35843;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#27169;&#22411;&#19978;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#38750;&#24179;&#20961;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LlamaFactory&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19968;&#22871;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#20869;&#32622;&#30340;Web UI LlamaBoard &#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#65292;&#26080;&#38656;&#32534;&#30721;&#12290;&#25105;&#20204;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#24050;&#21457;&#24067;&#22312; https://github.com/hiyouga/LLaMA-Factory&#65292;&#24182;&#24050;&#33719;&#24471;&#36229;&#36807;13,000&#39063;&#26143;&#21644;1,600&#20010;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13372v1 Announce Type: new  Abstract: Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#65288;CoAT&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#35757;&#32451;&#22330;&#26223;&#65292;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#21033;&#29992;&#31867;&#27604;&#25512;&#29702;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;CoAT&#65292;&#39044;&#35757;&#32451;&#30340;transformers&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28436;&#31034;&#20013;&#30340;&#26032;&#28508;&#22312;&#27010;&#24565;&#65292;&#20351;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#20989;&#25968;&#21464;&#25442;&#26356;&#21152; robust&#12290;</title><link>https://arxiv.org/abs/2403.09703</link><description>&lt;p&gt;
&#27010;&#24565;&#24863;&#30693;&#25968;&#25454;&#26500;&#24314;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Concept-aware Data Construction Improves In-context Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#65288;CoAT&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#35757;&#32451;&#22330;&#26223;&#65292;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#21033;&#29992;&#31867;&#27604;&#25512;&#29702;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;CoAT&#65292;&#39044;&#35757;&#32451;&#30340;transformers&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28436;&#31034;&#20013;&#30340;&#26032;&#28508;&#22312;&#27010;&#24565;&#65292;&#20351;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#20989;&#25968;&#21464;&#25442;&#26356;&#21152; robust&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#34920;&#29616;&#20026;LMs&#33021;&#22815;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25191;&#34892;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#26377;&#20851;&#31574;&#21010;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#30340;&#24037;&#20316;&#20551;&#23450;ICL&#26159;&#30001;&#20110;&#24040;&#22823;&#30340;&#36807;&#21442;&#25968;&#21270;&#25110;&#22810;&#20219;&#21153;&#35757;&#32451;&#35268;&#27169;&#23548;&#33268;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#24037;&#20316;&#23558;ICL&#33021;&#21147;&#24402;&#22240;&#20110;&#27010;&#24565;&#30456;&#20851;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22312;&#23567;&#35268;&#27169;&#12289;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#20102;&#21151;&#33021;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09703v1 Announce Type: cross  Abstract: Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs' ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges from a vast over-parametrization or the scale of multi-task training. However, recent theoretical work attributes the ICL ability to concept-dependent training data and creates functional in-context learners even in small-scale, synthetic settings.   In this work, we practically explore this newly identified axis of ICL quality. We propose Concept-aware Training (CoAT), a framework for constructing training scenarios that make it beneficial for the LM to learn to utilize the analogical reasoning concepts from demonstrations. We find that by using CoAT, pre-trained transformers can learn to better utilise new latent concepts from demonstrations and that such ability makes ICL more robust to the functio
&lt;/p&gt;</description></item><item><title>Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03640</link><description>&lt;p&gt;
Apollo&#65306;&#36731;&#37327;&#32423;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65306;&#35753;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#26222;&#24800;60&#20159;&#20154;
&lt;/p&gt;
&lt;p&gt;
Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03640
&lt;/p&gt;
&lt;p&gt;
Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#29699;&#21307;&#23398;&#30693;&#35782;&#30340;&#24222;&#22823;&#23384;&#20648;&#24211;&#20027;&#35201;&#26159;&#20197;&#33521;&#35821;&#20026;&#20027;&#65292;&#20294;&#22312;&#20256;&#36882;&#37327;&#36523;&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#26041;&#38754;&#65292;&#26412;&#22320;&#35821;&#35328;&#23545;&#20110;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#23558;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#20154;&#32676;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#28085;&#30422;&#20840;&#29699;61&#20159;&#20154;&#21475;&#30340;&#20845;&#31181;&#26368;&#24120;&#29992;&#35821;&#35328;&#30340;&#21307;&#23398;LLMs&#12290;&#36825;&#19968;&#21162;&#21147;&#26368;&#32456;&#20419;&#25104;&#20102;ApolloCorpora&#22810;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#38598;&#21644;XMedBench&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#22312;&#22810;&#35821;&#35328;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21457;&#24067;&#30340;Apollo&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#30456;&#23545;&#36739;&#23567;&#23610;&#23544;&#65288;&#21363;0.5B&#12289;1.8B&#12289;2B&#12289;6B&#21644;7B&#65289;&#19978;&#21462;&#24471;&#20102;&#19982;&#21516;&#31561;&#22823;&#23567;&#27169;&#22411;&#26368;&#20339;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;Apollo-7B&#26159;&#36804;&#20170;&#20026;&#27490;&#36798;&#21040;70B&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#36739;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03640v1 Announce Type: cross  Abstract: Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30690;&#37327;&#37327;&#21270;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;VQ-BeT&#65289;&#30340;&#22810;&#21151;&#33021;&#34892;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#36830;&#32493;&#21160;&#20316;&#36827;&#34892;&#26631;&#35760;&#21270;&#22788;&#29702;&#22810;&#27169;&#24577;&#21160;&#20316;&#39044;&#27979;&#12289;&#26465;&#20214;&#29983;&#25104;&#21644;&#37096;&#20998;&#35266;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.03181</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#21160;&#20316;&#30340;&#34892;&#20026;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Behavior Generation with Latent Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03181
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30690;&#37327;&#37327;&#21270;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;VQ-BeT&#65289;&#30340;&#22810;&#21151;&#33021;&#34892;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#36830;&#32493;&#21160;&#20316;&#36827;&#34892;&#26631;&#35760;&#21270;&#22788;&#29702;&#22810;&#27169;&#24577;&#21160;&#20316;&#39044;&#27979;&#12289;&#26465;&#20214;&#29983;&#25104;&#21644;&#37096;&#20998;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22797;&#26434;&#34892;&#20026;&#30340;&#29983;&#25104;&#24314;&#27169;&#19968;&#30452;&#26159;&#20915;&#31574;&#21046;&#23450;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#19982;&#35821;&#35328;&#25110;&#22270;&#20687;&#29983;&#25104;&#19981;&#21516;&#65292;&#20915;&#31574;&#21046;&#23450;&#38656;&#35201;&#24314;&#27169;&#21160;&#20316; - &#36830;&#32493;&#20540;&#21521;&#37327;&#65292;&#20854;&#22312;&#20998;&#24067;&#19978;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#21487;&#33021;&#26469;&#33258;&#26410;&#32463;&#31579;&#36873;&#30340;&#26469;&#28304;&#65292;&#22312;&#39034;&#24207;&#39044;&#27979;&#20013;&#29983;&#25104;&#35823;&#24046;&#21487;&#33021;&#20250;&#30456;&#20114;&#32047;&#31215;&#12290;&#26368;&#36817;&#19968;&#31867;&#31216;&#20026;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;BeT&#65289;&#30340;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;k-means&#32858;&#31867;&#23545;&#21160;&#20316;&#36827;&#34892;&#31163;&#25955;&#21270;&#20197;&#25429;&#25417;&#19981;&#21516;&#27169;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;k-means&#22312;&#22788;&#29702;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#25110;&#38271;&#24207;&#21015;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#32570;&#20047;&#26799;&#24230;&#20449;&#24687;&#65292;&#22240;&#27492;BeT&#22312;&#24314;&#27169;&#38271;&#36317;&#31163;&#21160;&#20316;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30690;&#37327;&#37327;&#21270;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;VQ-BeT&#65289;&#30340;&#22810;&#21151;&#33021;&#34892;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#21160;&#20316;&#39044;&#27979;&#12289;&#26465;&#20214;&#29983;&#25104;&#21644;&#37096;&#20998;&#35266;&#23519;&#12290;VQ-BeT&#36890;&#36807;&#23545;&#36830;&#32493;&#21160;&#20316;&#36827;&#34892;&#26631;&#35760;&#21270;&#26469;&#22686;&#24378;BeT
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03181v1 Announce Type: cross  Abstract: Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making. Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.02990</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#30340;&#25968;&#25454;&#22686;&#24378;&#65306;&#25968;&#25454;&#35270;&#35282;&#12289;&#23398;&#20064;&#33539;&#24335;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02990
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#35757;&#32451;&#26679;&#26412;&#22810;&#26679;&#21270;&#32780;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21450;&#20854;&#20182;&#39046;&#22495;&#20013;&#23427;&#20204;&#25552;&#20379;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#20174;&#25968;&#25454;&#35270;&#35282;&#21644;&#23398;&#20064;&#35270;&#35282;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20174;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#31561;&#12290;&#26412;&#35843;&#26597;&#31361;&#26174;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#26088;&#22312;&#20316;&#20026;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02990v1 Announce Type: cross  Abstract: In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a 
&lt;/p&gt;</description></item><item><title>FinAgent&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#20195;&#29702;&#65292;&#36890;&#36807;&#24037;&#20855;&#22686;&#24378;&#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#65292;&#20855;&#26377;&#29420;&#29305;&#30340;&#21452;&#37325;&#21453;&#23556;&#27169;&#22359;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#24182;&#24555;&#36895;&#36866;&#24212;&#24066;&#22330;&#21160;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.18485</link><description>&lt;p&gt;
FinAgent: &#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#20195;&#29702;&#65306;&#24037;&#20855;&#22686;&#24378;&#12289;&#22810;&#26679;&#21270;&#21644;&#36890;&#29992;
&lt;/p&gt;
&lt;p&gt;
FinAgent: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18485
&lt;/p&gt;
&lt;p&gt;
FinAgent&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#20195;&#29702;&#65292;&#36890;&#36807;&#24037;&#20855;&#22686;&#24378;&#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#65292;&#20855;&#26377;&#29420;&#29305;&#30340;&#21452;&#37325;&#21453;&#23556;&#27169;&#22359;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#24182;&#24555;&#36895;&#36866;&#24212;&#24066;&#22330;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20132;&#26131;&#26159;&#24066;&#22330;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21463;&#21040;&#26032;&#38395;&#12289;&#20215;&#26684;&#21644;K&#32447;&#22270;&#31561;&#22810;&#27169;&#24577;&#20449;&#24687;&#26500;&#25104;&#30340;&#20449;&#24687;&#26223;&#35266;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#37327;&#21270;&#20132;&#26131;&#21644;&#19981;&#21516;&#36164;&#20135;&#30340;&#39640;&#39057;&#20132;&#26131;&#31561;&#22810;&#26679;&#21270;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#20808;&#36827;AI&#25216;&#26415;&#22312;&#37329;&#34701;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#37329;&#34701;&#20132;&#26131;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#19981;&#36275;&#21644;&#36328;&#19981;&#21516;&#20219;&#21153;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FinAgent&#65292;&#19968;&#20010;&#20855;&#26377;&#24037;&#20855;&#22686;&#24378;&#21151;&#33021;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#20195;&#29702;&#65292;&#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#12290;FinAgent&#30340;&#24066;&#22330;&#26234;&#33021;&#27169;&#22359;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;-&#25968;&#20540;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;-&#20197;&#20934;&#30830;&#20998;&#26512;&#37329;&#34701;&#24066;&#22330;&#12290;&#20854;&#29420;&#29305;&#30340;&#21452;&#37325;&#21453;&#23556;&#27169;&#22359;&#19981;&#20165;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#24066;&#22330;&#21160;&#24577;&#65292;&#36824;&#34701;&#21512;&#20102;&#22810;&#26679;&#21270;&#30340;&#35760;&#24518;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18485v1 Announce Type: cross  Abstract: Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retri
&lt;/p&gt;</description></item><item><title>XAI&#39046;&#22495;&#34987;&#21010;&#20998;&#20026;&#34013;&#33394;XAI&#21644;&#32418;&#33394;XAI&#20004;&#31181;&#35299;&#37322;&#25991;&#21270;&#65292;&#25351;&#20986;&#20102;&#32418;&#33394;XAI&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#30740;&#31350;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13914</link><description>&lt;p&gt;
&#19981;&#26159;&#20026;&#20102;&#36777;&#35299;&#32780;&#26159;&#20026;&#20102;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explain to Question not to Justify
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13914
&lt;/p&gt;
&lt;p&gt;
XAI&#39046;&#22495;&#34987;&#21010;&#20998;&#20026;&#34013;&#33394;XAI&#21644;&#32418;&#33394;XAI&#20004;&#31181;&#35299;&#37322;&#25991;&#21270;&#65292;&#25351;&#20986;&#20102;&#32418;&#33394;XAI&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#30740;&#31350;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#19968;&#20010;&#24180;&#36731;&#20294;&#38750;&#24120;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35813;&#39046;&#22495;&#30446;&#21069;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#19981;&#21516;&#21644;&#19981;&#20860;&#23481;&#30446;&#26631;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;XAI&#39046;&#22495;&#20869;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#21508;&#31181;&#32447;&#32034;&#20998;&#20026;&#20004;&#31181;&#20114;&#34917;&#30340;&#25991;&#21270;&#65292;&#21363;&#20154;&#31867;/&#20215;&#20540;&#21462;&#21521;&#35299;&#37322;&#65288;&#34013;&#33394;XAI&#65289;&#21644;&#27169;&#22411;/&#39564;&#35777;&#21462;&#21521;&#35299;&#37322;&#65288;&#32418;&#33394;XAI&#65289;&#12290;&#25105;&#20204;&#36824;&#35748;&#20026;&#65292;&#32418;&#33394;XAI&#39046;&#22495;&#30446;&#21069;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#38544;&#34255;&#30528;&#24040;&#22823;&#30340;&#26426;&#36935;&#21644;&#37325;&#35201;&#30740;&#31350;&#30340;&#28508;&#21147;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#26377;&#21069;&#36884;&#30340;&#25361;&#25112;&#26469;&#24635;&#32467;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13914v1 Announce Type: new  Abstract: Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#21160;&#24577;&#35268;&#21010;&#39046;&#22495;&#20013;&#27169;&#25311;&#24037;&#20855;&#20351;&#29992;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#35813;&#39046;&#22495;&#32771;&#34385;&#21040;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;</title><link>https://arxiv.org/abs/2402.11658</link><description>&lt;p&gt;
&#20998;&#23618;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dynamic planning in hierarchical active inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11658
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#21160;&#24577;&#35268;&#21010;&#39046;&#22495;&#20013;&#27169;&#25311;&#24037;&#20855;&#20351;&#29992;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#35813;&#39046;&#22495;&#32771;&#34385;&#21040;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#20154;&#31867;&#22823;&#33041;&#25512;&#26029;&#21644;&#26045;&#21152;&#19982;&#35748;&#30693;&#20915;&#31574;&#30456;&#20851;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#33539;&#24335;&#65292;&#20027;&#21160;&#25512;&#26029;&#65292;&#20026;&#29983;&#29289;&#26377;&#26426;&#20307;&#36866;&#24212;&#24102;&#26469;&#20102;&#22522;&#26412;&#35265;&#35299;&#65292;&#19981;&#26029;&#21162;&#21147;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#20197;&#23558;&#33258;&#24049;&#38480;&#21046;&#22312;&#19982;&#29983;&#21629;&#20860;&#23481;&#30340;&#29366;&#24577;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#20154;&#31867;&#21644;&#21160;&#29289;&#34892;&#20026;&#21487;&#20197;&#35299;&#37322;&#20026;&#20027;&#21160;&#25512;&#26029;&#36807;&#31243;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#31163;&#25955;&#20915;&#31574;&#36824;&#26159;&#36830;&#32493;&#36816;&#21160;&#25511;&#21046;&#65292;&#37117;&#28608;&#21457;&#20102;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#32570;&#20047;&#23545;&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#35268;&#21010;&#34892;&#21160;&#30340;&#20840;&#38754;&#23637;&#26395;&#12290;&#25105;&#20204;&#35774;&#23450;&#20102;&#23545;&#24037;&#20855;&#20351;&#29992;&#36827;&#34892;&#24314;&#27169;&#30340;&#30446;&#26631;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#20027;&#39064;&#65292;&#29282;&#35760;&#20004;&#20010;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20851;&#38190;&#26041;&#38754;&#65306;&#29702;&#35299;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11658v1 Announce Type: new  Abstract: By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions. A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states. Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence. Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments. Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#26694;&#26550;&#65288;LogicCheckGPT&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.11622</link><description>&lt;p&gt;
&#36923;&#36753;&#38381;&#29615;&#65306;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11622
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#26694;&#26550;&#65288;LogicCheckGPT&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#24187;&#35273;&#19968;&#30452;&#26159;&#38459;&#30861;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#26356;&#24191;&#27867;&#24212;&#29992;&#30340;&#36719;&#32907;&#12290;&#23545;&#35937;&#24187;&#35273;&#26159;&#25351;LVLMs&#22312;&#22270;&#20687;&#20013;&#22768;&#31216;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#25351;&#23548;&#35843;&#25972;&#21644;&#22522;&#20110;&#22806;&#37096;&#27169;&#22411;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#22806;&#37096;&#27169;&#22411;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26410;&#28145;&#20837;&#25506;&#35752;&#30340;&#39046;&#22495;&#65292;&#21363;&#21033;&#29992;LVLM&#26412;&#36523;&#26469;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#26679;&#30340;&#30452;&#35273;&#65292;&#21363;LVLM&#20542;&#21521;&#20110;&#23545;&#23384;&#22312;&#30340;&#23545;&#35937;&#20570;&#20986;&#36923;&#36753;&#19968;&#33268;&#30340;&#21453;&#24212;&#65292;&#20294;&#23545;&#24187;&#35273;&#23545;&#35937;&#20570;&#20986;&#19981;&#19968;&#33268;&#30340;&#21453;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#23545;&#35937;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#36731;&#26694;&#26550;&#65292;&#21363;LogicCheckGPT&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36923;&#36753;&#19968;&#33268;&#24615;&#25506;&#27979;&#26469;&#25552;&#20986;&#20855;&#26377;&#36923;&#36753;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11622v1 Announce Type: cross  Abstract: Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical corr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#20010;&#24615;&#21270;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#30340;&#38376;&#27099;&#65292;&#20174;&#32780;&#23454;&#29616;&#28216;&#25103;&#20869;&#23481;&#19982;&#29609;&#23478;&#20559;&#22909;&#30340;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.10133</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#25512;&#29702;: &#26080;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#20010;&#24615;&#21270;&#20869;&#23481;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#20010;&#24615;&#21270;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#30340;&#38376;&#27099;&#65292;&#20174;&#32780;&#23454;&#29616;&#28216;&#25103;&#20869;&#23481;&#19982;&#29609;&#23478;&#20559;&#22909;&#30340;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Procedural content generation&#65288;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#65289;&#20351;&#29992;&#31639;&#27861;&#25216;&#26415;&#20197;&#26356;&#20302;&#30340;&#29983;&#20135;&#25104;&#26412;&#21019;&#24314;&#22823;&#37327;&#26032;&#20869;&#23481;&#12290;&#22312;&#36739;&#26032;&#30340;&#26041;&#27861;&#20013;&#65292;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#25910;&#38598;&#22823;&#37327;&#26114;&#36149;&#30340;&#25968;&#25454;&#65292;&#24182;&#24320;&#21457;&#21644;&#35757;&#32451;&#30456;&#23545;&#22797;&#26434;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#26082;&#32791;&#26102;&#21448;&#26114;&#36149;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#25506;&#32034;&#33021;&#21542;&#36890;&#36807;&#26356;&#23454;&#29992;&#21644;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38477;&#20302;&#20010;&#24615;&#21270;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#30340;&#38376;&#27099;&#12290;&#23558;&#28216;&#25103;&#20869;&#23481;&#19982;&#29609;&#23478;&#20559;&#22909;&#36827;&#34892;&#21305;&#37197;&#26082;&#20351;&#29609;&#23478;&#26356;&#20139;&#21463;&#28216;&#25103;&#65292;&#20063;&#20351;&#24320;&#21457;&#32773;&#26356;&#20381;&#36182;&#29609;&#23478;&#22312;&#28216;&#25103;&#24471;&#21040;&#28385;&#36275;&#20043;&#21518;&#20877;&#36827;&#34892;&#21464;&#29616;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10133v1 Announce Type: new  Abstract: Procedural content generation uses algorithmic techniques to create large amounts of new content for games at much lower production costs. In newer approaches, procedural content generation utilizes machine learning. However, these methods usually require expensive collection of large amounts of data, as well as the development and training of fairly complex learning models, which can be both extremely time-consuming and expensive. The core of our research is to explore whether we can lower the barrier to the use of personalized procedural content generation through a more practical and generalizable approach with large language models. Matching game content with player preferences benefits both players, who enjoy the game more, and developers, who increasingly depend on players enjoying the game before being able to monetize it. Therefore, this paper presents a novel approach to achieving personalization by using large language models t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08114</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Preference Learning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#24494;&#35843;&#25216;&#26415;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23545;&#20110;&#23545;&#40784;&#36825;&#20123;&#27169;&#22411;&#26469;&#35828;&#65292;&#26368;&#20851;&#38190;&#30340;&#32771;&#34385;&#26159;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#20154;&#21147;&#36164;&#28304;&#65292;&#25110;&#32773;&#22312;LLM&#26412;&#36523;&#34987;&#29992;&#20316;oracle&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#27169;&#22411;&#36164;&#28304;&#12290;&#20174;&#20154;&#31867;&#25110;AI&#20559;&#22909;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF / RLAIF&#65289;&#26159;&#36825;&#31181;&#25216;&#26415;&#26368;&#31361;&#20986;&#30340;&#20363;&#23376;&#65292;&#20294;&#23427;&#24448;&#24448;&#22797;&#26434;&#19988;&#19981;&#31283;&#23450;&#12290;&#26368;&#36817;&#65292;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#26356;&#31616;&#21333;&#21644;&#26356;&#31283;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;DPO&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#29109;&#21644;DPO&#20248;&#21270;&#30340;&#38544;&#24335;&#20559;&#22909;&#27169;&#22411;&#30340;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#23454;&#29992;&#37319;&#38598;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26367;&#20195;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25972;&#21512;&#20197;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#21457;&#29616;&#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#28151;&#21512;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.04376</link><description>&lt;p&gt;
&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#26367;&#20195;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#30340;&#25193;&#23637;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for learning with real and surrogate data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26367;&#20195;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25972;&#21512;&#20197;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#21457;&#29616;&#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#28151;&#21512;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#25104;&#26412;&#26114;&#36149;&#25110;&#19981;&#20999;&#23454;&#38469;&#30340;&#33539;&#22260;&#20869;, &#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#30456;&#21453;&#22320;, &#21487;&#20197;&#23558;&#26469;&#33258;&#30446;&#26631;&#20998;&#24067;&#30340;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19982;&#26469;&#33258;&#20844;&#20849;&#25968;&#25454;&#38598;&#12289;&#19981;&#21516;&#24773;&#20917;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#25110;&#30001;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#30340;&#25968;&#25454;&#30456;&#32467;&#21512;, &#20316;&#20026;&#26367;&#20195;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#26696;&#26469;&#23558;&#26367;&#20195;&#25968;&#25454;&#25972;&#21512;&#21040;&#35757;&#32451;&#20013;, &#24182;&#20351;&#29992;&#29702;&#35770;&#27169;&#22411;&#21644;&#23454;&#35777;&#30740;&#31350;&#25506;&#32034;&#20854;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;(i) &#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21407;&#22987;&#20998;&#24067;&#30340;&#27979;&#35797;&#35823;&#24046;&#65307;(ii) &#20026;&#20102;&#33719;&#24471;&#36825;&#31181;&#25928;&#30410;, &#20351;&#29992;&#26368;&#20248;&#21152;&#26435;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38750;&#24120;&#20851;&#38190;&#65307;(iii) &#22312;&#28151;&#21512;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#26367;&#20195;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#21487;&#20197;&#24456;&#22909;&#22320;&#29992;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#12290;&#36825;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting large quantities of high-quality data is often prohibitively expensive or impractical, and a crucial bottleneck in machine learning. One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources like public datasets, data collected under different circumstances, or synthesized by generative models. Blurring distinctions, we refer to such data as `surrogate data'.   We define a simple scheme for integrating surrogate data into training and use both theoretical models and empirical studies to explore its behavior. Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution; $(ii)$ In order to reap this benefit, it is crucial to use optimally weighted empirical risk minimization; $(iii)$ The test error of models trained on mixtures of real and surrogate data is well described by a scaling law. This can be used to predict the optimal weighting and the gai
&lt;/p&gt;</description></item><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31216;&#30772;&#32570;&#38598;&#26469;&#30772;&#22351;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02681</link><description>&lt;p&gt;
&#31561;&#21464;&#23545;&#31216;&#30772;&#32570;&#38598;
&lt;/p&gt;
&lt;p&gt;
Equivariant Symmetry Breaking Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02681
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31216;&#30772;&#32570;&#38598;&#26469;&#30772;&#22351;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#28041;&#21450;&#28508;&#22312;&#23545;&#31216;&#24615;&#30340;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36890;&#36807;&#35774;&#35745;&#65292;ENN&#22312;&#32473;&#23450;&#26356;&#39640;&#23545;&#31216;&#24615;&#36755;&#20837;&#26102;&#26080;&#27861;&#20135;&#29983;&#36739;&#20302;&#23545;&#31216;&#24615;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29289;&#29702;&#31995;&#32479;&#20013;&#20250;&#21457;&#29983;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#19968;&#20010;&#21021;&#22987;&#39640;&#24230;&#23545;&#31216;&#30340;&#29366;&#24577;&#33719;&#24471;&#19968;&#20010;&#36739;&#19981;&#23545;&#31216;&#30340;&#31283;&#23450;&#29366;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#22914;&#20309;&#31995;&#32479;&#22320;&#22312;ENN&#20013;&#30772;&#22351;&#23545;&#31216;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#26032;&#22411;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#31216;&#30772;&#32570;&#38598;&#65288;SBS&#65289;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#19981;&#26159;&#37325;&#26032;&#35774;&#35745;&#29616;&#26377;&#30340;&#32593;&#32476;&#65292;&#32780;&#26159;&#35774;&#35745;&#20102;&#19968;&#32452;&#23545;&#31216;&#30772;&#32570;&#23545;&#35937;&#65292;&#26681;&#25454;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#23545;&#31216;&#24615;&#23558;&#20854;&#36755;&#20837;&#21040;&#25105;&#20204;&#30340;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#38598;&#21512;&#19978;&#23450;&#20041;&#31561;&#21464;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#23427;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#32422;&#26463;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;... (the abstract is incomplete and cut off)
&lt;/p&gt;
&lt;p&gt;
Equivariant neural networks (ENNs) have been shown to be extremely effective in applications involving underlying symmetries. By construction ENNs cannot produce lower symmetry outputs given a higher symmetry input. However, spontaneous symmetry breaking occurs in many physical systems and we may obtain a less symmetric stable state from an initial highly symmetric one. Hence, it is imperative that we understand how to systematically break symmetry in ENNs. In this work, we propose a novel symmetry breaking framework that is fully equivariant. We emphasize that our approach is general and applicable to equivariance under any group. To achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather than redesign existing networks, we design sets of symmetry breaking objects which we feed into our network based on the symmetry of our inputs and outputs. We show there is a natural way to define equivariance on these sets, which gives an additional constraint. Minimizing the si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#21327;&#20316;&#35013;&#37197;&#22330;&#26223;&#20013;&#65292;Cobot&#30340;&#29983;&#20135;&#33410;&#22863;&#23545;&#21442;&#19982;&#32773;&#30340;&#32463;&#39564;&#24615;&#23450;&#20301;&#25511;&#21046;&#21644;&#24773;&#32490;&#29366;&#24577;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#22312;&#24773;&#32490;&#29366;&#24577;&#21644;&#23450;&#20301;&#25511;&#21046;&#26041;&#38754;&#26410;&#21457;&#29616;&#24046;&#24322;&#65292;&#20294;&#32771;&#34385;&#21040;&#20854;&#20182;&#24515;&#29702;&#21464;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#38656;&#35201;&#32771;&#34385;&#20010;&#20307;&#30340;&#24515;&#29702;&#29305;&#24449;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#20114;&#21160;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.00808</link><description>&lt;p&gt;
&#22312;&#21327;&#20316;&#35013;&#37197;&#22330;&#26223;&#20013;&#65292;&#25506;&#32034;Cobot&#30340;&#29983;&#20135;&#33410;&#22863;&#12289;&#25511;&#21046;&#23450;&#20301;&#21644;&#24773;&#32490;&#29366;&#24577;&#20043;&#38388;&#30340;&#21160;&#24577;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Dynamics between Cobot's Production Rhythm, Locus of Control and Emotional State in a Collaborative Assembly Scenario
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#21327;&#20316;&#35013;&#37197;&#22330;&#26223;&#20013;&#65292;Cobot&#30340;&#29983;&#20135;&#33410;&#22863;&#23545;&#21442;&#19982;&#32773;&#30340;&#32463;&#39564;&#24615;&#23450;&#20301;&#25511;&#21046;&#21644;&#24773;&#32490;&#29366;&#24577;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#22312;&#24773;&#32490;&#29366;&#24577;&#21644;&#23450;&#20301;&#25511;&#21046;&#26041;&#38754;&#26410;&#21457;&#29616;&#24046;&#24322;&#65292;&#20294;&#32771;&#34385;&#21040;&#20854;&#20182;&#24515;&#29702;&#21464;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#38656;&#35201;&#32771;&#34385;&#20010;&#20307;&#30340;&#24515;&#29702;&#29305;&#24449;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#20114;&#21160;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#65292;&#21327;&#20316;&#26426;&#22120;&#20154;&#65288;cobots&#65289;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#19988;&#23545;&#35780;&#20272;&#21644;&#27979;&#37327;cobots&#30340;&#26576;&#20123;&#29305;&#24449;&#23545;&#20154;&#30340;&#24433;&#21709;&#20135;&#29983;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#12290;&#22312;&#26412;&#27425;&#35797;&#28857;&#30740;&#31350;&#20013;&#65292;&#30740;&#31350;&#20102;&#19968;&#20010;cobots&#30340;&#29983;&#20135;&#33410;&#22863;&#65288;C1-&#24930;&#12289;C2-&#24555;&#12289;C3-&#26681;&#25454;&#21442;&#19982;&#32773;&#30340;&#27493;&#35843;&#35843;&#25972;&#65289;&#23545;31&#21517;&#21442;&#19982;&#32773;&#30340;&#32463;&#39564;&#24615;&#23450;&#20301;&#25511;&#21046;&#65288;ELoC&#65289;&#21644;&#24773;&#32490;&#29366;&#24577;&#30340;&#24433;&#21709;&#12290;&#36824;&#32771;&#34385;&#20102;&#25805;&#20316;&#20154;&#21592;&#30340;&#32489;&#25928;&#12289;&#22522;&#26412;&#20869;&#37096;&#23450;&#20301;&#25511;&#21046;&#31243;&#24230;&#21644;&#23545;&#26426;&#22120;&#20154;&#30340;&#24577;&#24230;&#12290;&#20851;&#20110;&#24773;&#32490;&#29366;&#24577;&#21644;ELoC&#65292;&#22312;&#19977;&#31181;&#26465;&#20214;&#19979;&#27809;&#26377;&#21457;&#29616;&#24046;&#24322;&#65292;&#20294;&#32771;&#34385;&#21040;&#20854;&#20182;&#24515;&#29702;&#21464;&#37327;&#65292;&#20986;&#29616;&#20102;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#32467;&#26524;&#20284;&#20046;&#34920;&#26126;&#38656;&#35201;&#32771;&#34385;&#20010;&#20307;&#30340;&#24515;&#29702;&#29305;&#24449;&#65292;&#20197;&#25552;&#20379;&#19981;&#21516;&#21644;&#26368;&#20339;&#30340;&#20114;&#21160;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industrial scenarios, there is widespread use of collaborative robots (cobots), and growing interest is directed at evaluating and measuring the impact of some characteristics of the cobot on the human factor. In the present pilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 - Adapted to the participant's pace) of a cobot has on the Experiential Locus of Control (ELoC) and the emotional state of 31 participants has been examined. The operators' performance, the degree of basic internal Locus of Control, and the attitude towards the robots were also considered. No difference was found regarding the emotional state and the ELoC in the three conditions, but considering the other psychological variables, a more complex situation emerges. Overall, results seem to indicate a need to consider the person's psychological characteristics to offer a differentiated and optimal interaction experience.
&lt;/p&gt;</description></item><item><title>UINav&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#36866;&#21512;&#31227;&#21160;&#35774;&#22791;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#65292;&#25104;&#21151;&#29575;&#39640;&#65292;&#35757;&#32451;&#25968;&#25454;&#23569;&#12290;</title><link>https://arxiv.org/abs/2312.10170</link><description>&lt;p&gt;
UINav&#65306;&#19968;&#31181;&#35757;&#32451;&#35774;&#22791;&#31471;&#33258;&#21160;&#21270;&#20195;&#29702;&#30340;&#23454;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UINav: A Practical Approach to Train On-Device Automation Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10170
&lt;/p&gt;
&lt;p&gt;
UINav&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#36866;&#21512;&#31227;&#21160;&#35774;&#22791;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#65292;&#25104;&#21151;&#29575;&#39640;&#65292;&#35757;&#32451;&#25968;&#25454;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#33258;&#20027;&#39537;&#21160;&#24212;&#29992;&#31243;&#24207;&#29992;&#25143;&#30028;&#38754;&#20197;&#23436;&#25104;&#29992;&#25143;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#24403;&#29992;&#25143;&#22788;&#20110;&#24773;&#22659;&#24615;&#25110;&#27704;&#20037;&#24615;&#21463;&#25439;&#26102;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#30410;&#22788;&#12290;&#20043;&#21069;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#19981;&#33021;&#20135;&#29983;&#20855;&#26377;&#26222;&#36941;&#36866;&#29992;&#24615;&#30340;&#27169;&#22411;&#65292;&#32780;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#20165;&#22312;&#31616;&#21333;&#30340;&#25163;&#24037;&#21046;&#20316;&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#38752;&#24037;&#20316;&#65292;&#25110;&#32773;&#20250;&#20135;&#29983;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UINav&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#36866;&#21512;&#31227;&#21160;&#35774;&#22791;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#28436;&#31034;&#25968;&#37327;&#19981;&#22810;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25104;&#21151;&#29575;&#12290;&#20026;&#20102;&#20943;&#23569;&#28436;&#31034;&#30340;&#24037;&#20316;&#37327;&#65292;UINav&#20351;&#29992;&#20102;&#19968;&#20010;&#35009;&#21028;&#27169;&#22411;&#65292;&#22312;&#20195;&#29702;&#22833;&#36133;&#30340;&#20219;&#21153;&#19978;&#20026;&#29992;&#25143;&#25552;&#20379;&#21363;&#26102;&#21453;&#39304;&#65292;&#24182;&#33258;&#21160;&#22686;&#21152;&#20154;&#31867;&#28436;&#31034;&#20197;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#20165;&#38656;10&#27425;&#28436;&#31034;&#65292;UINav&#23601;&#21487;&#20197;&#23454;&#29616;70%&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#26377;&#36275;&#22815;&#22810;&#27425;&#28436;&#31034;&#26102;&#65292;&#23427;&#21487;&#20197;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10170v2 Announce Type: replace-cross  Abstract: Automation systems that can autonomously drive application user interfaces to complete user tasks are of great benefit, especially when users are situationally or permanently impaired. Prior automation systems do not produce generalizable models while AI-based automation agents work reliably only in simple, hand-crafted applications or incur high computation costs. We propose UINav, a demonstration-based approach to train automation agents that fit mobile devices, yet achieving high success rates with modest numbers of demonstrations. To reduce the demonstration overhead, UINav uses a referee model that provides users with immediate feedback on tasks where the agent fails, and automatically augments human demonstrations to increase diversity in training data. Our evaluation shows that with only 10 demonstrations UINav can achieve 70% accuracy, and that with enough demonstrations it can surpass 90% accuracy.
&lt;/p&gt;</description></item><item><title>AutoMix&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#25321;&#26356;&#22823;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#33258;&#25105;&#39564;&#35777;&#21644;&#20803;&#39564;&#35777;&#22120;&#25552;&#39640;&#20102;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#30340;&#20248;&#21270;&#65292;&#23454;&#39564;&#35777;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26368;&#22810;86%.</title><link>https://arxiv.org/abs/2310.12963</link><description>&lt;p&gt;
AutoMix: &#33258;&#21160;&#28151;&#21512;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AutoMix: Automatically Mixing Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.12963
&lt;/p&gt;
&lt;p&gt;
AutoMix&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#25321;&#26356;&#22823;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#33258;&#25105;&#39564;&#35777;&#21644;&#20803;&#39564;&#35777;&#22120;&#25552;&#39640;&#20102;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#30340;&#20248;&#21270;&#65292;&#23454;&#39564;&#35777;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26368;&#22810;86%.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29616;&#22312;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#20113;API&#25552;&#20379;&#21830;&#33719;&#24471;&#12290;&#34429;&#28982;&#36825;&#31181;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#36873;&#25321;&#65292;&#20294;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#36873;&#39033;&#20197;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoMix&#65292;&#19968;&#31181;&#26681;&#25454;&#36739;&#23567;LM&#30340;&#36755;&#20986;&#30340;&#36817;&#20284;&#27491;&#30830;&#24615;&#26469;&#31574;&#30053;&#24615;&#22320;&#23558;&#26597;&#35810;&#36335;&#30001;&#21040;&#26356;&#22823;LM&#30340;&#26041;&#27861;&#12290;AutoMix&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#23569;&#37327;&#26679;&#26412;&#30340;&#33258;&#25105;&#39564;&#35777;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20272;&#35745;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#32780;&#26080;&#38656;&#35757;&#32451;&#12290;&#37492;&#20110;&#39564;&#35777;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#65292;&#25105;&#20204;&#22312;AutoMix&#20013;&#20351;&#29992;&#20102;&#20803;&#39564;&#35777;&#22120;&#26469;&#25552;&#39640;&#36825;&#20123;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;LLAMA2-13B&#21644;GPT-4&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;AutoMix&#36229;&#36234;&#20102;&#24050;&#24314;&#31435;&#30340;&#22522;&#32447;&#65292;&#27599;&#21333;&#20301;&#25104;&#26412;&#30340;&#22686;&#37327;&#25928;&#30410;&#25552;&#39640;&#20102;&#26368;&#22810;86%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.c&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.12963v3 Announce Type: replace  Abstract: Large language models (LLMs) are now available from cloud API providers in various sizes and configurations. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta-verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13B and GPT-4, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 86%. Our code and data are available at https://github.c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#20013;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#30340;&#24212;&#29992;&#12290;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#20316;&#20026;&#22810;&#21151;&#33021;&#12289;&#29983;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#27169;&#25311;&#32467;&#26524;&#24182;&#25351;&#23548;&#23454;&#38469;&#27835;&#30103;&#65292;&#20174;&#32780;&#25552;&#39640;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13699</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative AI-Driven Human Digital Twin in IoT-Healthcare: A Comprehensive Survey. (arXiv:2401.13699v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#20013;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#30340;&#24212;&#29992;&#12290;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#20316;&#20026;&#22810;&#21151;&#33021;&#12289;&#29983;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#27169;&#25311;&#32467;&#26524;&#24182;&#25351;&#23548;&#23454;&#38469;&#27835;&#30103;&#65292;&#20174;&#32780;&#25552;&#39640;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20154;&#31867;&#29983;&#27963;&#30340;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#20581;&#24247;&#25252;&#29702;&#26041;&#38754;&#21560;&#24341;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#21516;&#26102;&#65292;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#20840;&#38754;&#22320;&#25551;&#36848;&#20010;&#20307;&#20154;&#20307;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#22797;&#21046;&#65292;&#24182;&#23454;&#26102;&#21453;&#26144;&#20854;&#29289;&#29702;&#29366;&#20917;&#12290;&#33258;&#28982;&#22320;&#65292;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#34987;&#35774;&#24819;&#20026;&#36890;&#36807;&#20805;&#24403;&#22810;&#21151;&#33021;&#12289;&#29983;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#27979;&#35797;&#24179;&#21488;&#26469;&#22686;&#24378;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#30340;&#33021;&#21147;&#65292;&#27169;&#25311;&#32467;&#26524;&#24182;&#25351;&#23548;&#23454;&#38469;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#25104;&#21151;&#24314;&#31435;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#38656;&#35201;&#39640;&#20445;&#30495;&#24230;&#30340;&#34394;&#25311;&#24314;&#27169;&#21644;&#24378;&#22823;&#30340;&#20449;&#24687;&#20132;&#20114;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#31232;&#32570;&#12289;&#20559;&#20506;&#21644;&#22122;&#22768;&#25968;&#25454;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26368;&#36817;&#27969;&#34892;&#30340;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#30340;&#25216;&#26415;&#21487;&#33021;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21033;&#29992;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#33258;&#21160;&#29983;&#25104;&#12289;&#25805;&#20316;&#21644;&#20462;&#25913;&#28176;&#21464;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of things (IoT) can significantly enhance the quality of human life, specifically in healthcare, attracting extensive attentions to IoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as an innovative paradigm that can comprehensively characterize the replication of the individual human body in the digital world and reflect its physical status in real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the application of healthcare monitoring by acting as a versatile and vivid human digital testbed, simulating the outcomes and guiding the practical treatments. However, successfully establishing HDT requires high-fidelity virtual modeling and strong information interactions but possibly with scarce, biased and noisy data. Fortunately, a recent popular technology called generative artificial intelligence (GAI) may be a promising solution because it can leverage advanced AI algorithms to automatically create, manipulate, and modify val
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#23398;&#38382;&#31572;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.16035</link><description>&lt;p&gt;
MedEdit&#65306;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases. (arXiv:2309.16035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#23398;&#38382;&#31572;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34429;&#28982;&#22312;&#19968;&#33324;&#39046;&#22495;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#38754;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24448;&#24448;&#20316;&#20026;&#8220;&#40657;&#30418;&#8221;&#36816;&#20316;&#65292;&#38590;&#20197;&#20462;&#25913;&#20854;&#34892;&#20026;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#26088;&#22312;&#25913;&#36827;LLM&#30340;&#21709;&#24212;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26816;&#32034;&#31574;&#30053;&#65292;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;LLM&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#12290;&#36890;&#36807;&#23545;MedQA-SMILE&#25968;&#25454;&#38598;&#36827;&#34892;&#21307;&#23398;QA&#30340;&#37325;&#28857;&#30740;&#31350;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#26816;&#32034;&#27169;&#22411;&#21644;&#21521;LLM&#25552;&#20379;&#30340;&#20107;&#23454;&#25968;&#37327;&#23545;&#20854;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#32534;&#36753;&#21518;&#30340;Vicuna&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20174;44.46&#65285;&#25552;&#39640;&#21040;48.54&#65285;&#12290;&#36825;&#39033;&#24037;&#20316;&#20984;&#26174;&#20102;&#27169;&#22411;&#32534;&#36753;&#25913;&#21892;LLM&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20026;&#32531;&#35299;&#40657;&#30418;LLM&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks like medical question answering (QA). Moreover, they tend to function as "black-boxes," making it challenging to modify their behavior. Addressing this, our study delves into model editing utilizing in-context learning, aiming to improve LLM responses without the need for fine-tuning or retraining. Specifically, we propose a comprehensive retrieval strategy to extract medical facts from an external knowledge base, and then we incorporate them into the query prompt for the LLM. Focusing on medical QA using the MedQA-SMILE dataset, we evaluate the impact of different retrieval models and the number of facts provided to the LLM. Notably, our edited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%. This work underscores the potential of model editing to enhance LLM performance, offering a practical approach to mitigate the challenges of black-box LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#20840;&#23616;&#25628;&#32034;&#30340;&#31639;&#27861;&#65288;UMCTS&#65289;&#29992;&#20110;&#26689;&#26550;&#32467;&#26500;&#23610;&#23544;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#26356;&#26032;&#36807;&#31243;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#20197;&#21450;&#20351;&#29992;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#26469;&#33719;&#24471;&#21512;&#36866;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.06045</link><description>&lt;p&gt;
&#22522;&#20110;&#21551;&#21457;&#24335;&#20840;&#23616;&#25628;&#32034;&#30340;&#26689;&#26550;&#32467;&#26500;&#23610;&#23544;&#20248;&#21270;&#38382;&#39064;&#30340;&#25913;&#36827;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;UMCTS&#65289;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Update Monte Carlo tree search (UMCTS) algorithm for heuristic global search of sizing optimization problems for truss structures. (arXiv:2309.06045v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#20840;&#23616;&#25628;&#32034;&#30340;&#31639;&#27861;&#65288;UMCTS&#65289;&#29992;&#20110;&#26689;&#26550;&#32467;&#26500;&#23610;&#23544;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#26356;&#26032;&#36807;&#31243;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#20197;&#21450;&#20351;&#29992;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#26469;&#33719;&#24471;&#21512;&#36866;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26689;&#26550;&#32467;&#26500;&#23610;&#23544;&#20248;&#21270;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36866;&#29992;&#20110;&#22788;&#29702;&#26080;&#26799;&#24230;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#20248;&#21270;&#31639;&#27861;&#8212;&#8212;&#26356;&#26032;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;UMCTS&#65289;&#65292;&#29992;&#20110;&#33719;&#24471;&#21512;&#36866;&#30340;&#26689;&#26550;&#32467;&#26500;&#35774;&#35745;&#12290;UMCTS&#26159;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#23558;&#26032;&#39062;&#30340;&#26356;&#26032;&#36807;&#31243;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#19982;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#30456;&#32467;&#21512;&#12290;&#26356;&#26032;&#36807;&#31243;&#24847;&#21619;&#30528;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#27599;&#20010;&#26500;&#20214;&#30340;&#26368;&#20339;&#25130;&#38754;&#31215;&#36890;&#36807;&#25628;&#32034;&#26641;&#30830;&#23450;&#65292;&#20854;&#21021;&#22987;&#29366;&#24577;&#26159;&#19978;&#19968;&#36718;&#30340;&#26368;&#32456;&#29366;&#24577;&#12290;&#22312;UMCTS&#31639;&#27861;&#20013;&#65292;&#24341;&#20837;&#20102;&#21152;&#36895;&#36873;&#25321;&#25104;&#21592;&#38754;&#31215;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#21152;&#36895;&#22120;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27599;&#20010;&#29366;&#24577;&#65292;&#24179;&#22343;&#22870;&#21169;&#34987;&#26368;&#20339;&#22870;&#21169;&#30340;&#27169;&#25311;&#36807;&#31243;&#20013;&#25910;&#38598;&#26469;&#30340;&#22870;&#21169;&#26367;&#20195;&#65292;&#30830;&#23450;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26356;&#26032;&#36807;&#31243;&#21644;MCTS&#65292;&#20197;&#21450;&#20351;&#29992;UCB&#26469;&#20248;&#21270;&#26689;&#26550;&#32467;&#26500;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sizing optimization of truss structures is a complex computational problem, and the reinforcement learning (RL) is suitable for dealing with multimodal problems without gradient computations. In this paper, a new efficient optimization algorithm called update Monte Carlo tree search (UMCTS) is developed to obtain the appropriate design for truss structures. UMCTS is an RL-based method that combines the novel update process and Monte Carlo tree search (MCTS) with the upper confidence bound (UCB). Update process means that in each round, the optimal cross-sectional area of each member is determined by search tree, and its initial state is the final state in the previous round. In the UMCTS algorithm, an accelerator for the number of selections for member area and iteration number is introduced to reduce the computation time. Moreover, for each state, the average reward is replaced by the best reward collected on the simulation process to determine the optimal solution. The proposed optim
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.10635</link><description>&lt;p&gt;
SciBench: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#23637;&#22312;&#35768;&#22810;&#25968;&#23398;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#22823;&#22810;&#21482;&#21253;&#21547;&#21021;&#39640;&#20013;&#31185;&#30446;&#30340;&#38382;&#39064;&#65292;&#20165;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#22871;&#20214;SciBench&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#27979;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#25152;&#38656;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;SciBench&#21253;&#21547;&#20004;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#24320;&#25918;&#38598;&#65292;&#21253;&#25324;&#20174;&#25968;&#23398;&#12289;&#21270;&#23398;&#21644;&#29289;&#29702;&#25945;&#31185;&#20070;&#20013;&#25688;&#24405;&#30340;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#65292;&#20197;&#21450;&#19968;&#20010;&#23553;&#38381;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25968;&#23398;&#26412;&#31185;&#32771;&#35797;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of deli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.02766</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#32593;&#32476;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Networked Communication for Decentralised Agents in Mean-Field Games. (arXiv:2306.02766v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#32593;&#32476;&#36890;&#20449;&#24341;&#20837;&#22343;&#22330;&#21338;&#24328;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;oracle&#30340;&#24773;&#20917;&#19979;&#65292;N&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#27839;&#30528;&#32463;&#36807;&#30340;&#32463;&#39564;&#31995;&#32479;&#30340;&#21333;&#19968;&#38750;&#21608;&#26399;&#28436;&#21270;&#36335;&#24452;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#21482;&#26377;&#19968;&#20123;&#20851;&#20110;&#32593;&#32476;&#32467;&#26500;&#30340;&#21512;&#29702;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26679;&#26412;&#20445;&#35777;&#65292;&#22312;&#38598;&#20013;&#23398;&#20064;&#21644;&#29420;&#31435;&#23398;&#20064;&#24773;&#20917;&#20043;&#38388;&#26377;&#30028;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#30340;&#26679;&#26412;&#20445;&#35777;&#23454;&#38469;&#19978;&#24182;&#19981;&#20250;&#23548;&#33268;&#23454;&#38469;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#24403;&#29702;&#35770;&#21442;&#25968;&#26410;&#34987;&#35266;&#23519;&#21040;&#65288;&#23548;&#33268;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#19981;&#20934;&#30830;&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#36890;&#20449;&#26041;&#26696;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#19968;&#20010;&#19981;&#21487;&#21462;&#30340;&#38598;&#20013;&#24335;&#25511;&#21046;&#22120;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#36827;&#34892;&#20102;&#20960;&#31181;&#23454;&#38469;&#30340;&#25913;&#36827;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#23427;&#20204;&#30340;&#31532;&#19968;&#20010;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic evolution path of the empirical system. We prove that our architecture, with only a few reasonable assumptions about network structure, has sample guarantees bounded between those of the centralised- and independent-learning cases. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. Accordingly, we show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case, without relying on the undesirable assumption of a centralised controller. We contribute several further practical enhancements to all three theoretical algorithms, allowing us to showcase their first empirical demonstrations. Our expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#39564;&#35777;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#39564;&#35777;&#21644;&#20462;&#22797;&#36719;&#20214;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;ESBMC-AI&#20570;&#20986;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.14752</link><description>&lt;p&gt;
&#36208;&#21521;&#36719;&#20214;&#33258;&#24840;&#65306;&#32467;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#39564;&#35777;&#35299;&#20915;&#36719;&#20214;&#23433;&#20840;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification. (arXiv:2305.14752v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#39564;&#35777;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#39564;&#35777;&#21644;&#20462;&#22797;&#36719;&#20214;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;ESBMC-AI&#20570;&#20986;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#20351;&#24471;&#36719;&#20214;&#28431;&#27934;&#21487;&#20197;&#24471;&#21040;&#39564;&#35777;&#21644;&#33258;&#21160;&#20462;&#22797;&#12290;&#39318;&#20808;&#21033;&#29992;&#26377;&#38480;&#27169;&#22411;&#26816;&#26597;&#65288;BMC&#65289;&#23450;&#20301;&#36719;&#20214;&#28431;&#27934;&#21644;&#27966;&#29983;&#21453;&#20363;&#12290;&#28982;&#21518;&#65292;&#23558;&#21453;&#20363;&#21644;&#28304;&#20195;&#30721;&#25552;&#20379;&#32473;&#22823;&#35821;&#35328;&#27169;&#22411;&#24341;&#25806;&#36827;&#34892;&#20195;&#30721;&#35843;&#35797;&#21644;&#29983;&#25104;&#65292;&#20174;&#32780;&#25214;&#21040;&#28431;&#27934;&#30340;&#26681;&#26412;&#21407;&#22240;&#24182;&#20462;&#22797;&#20195;&#30721;&#12290;&#26368;&#21518;&#65292;&#21017;&#20351;&#29992;BMC&#39564;&#35777;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20462;&#27491;&#29256;&#26412;&#30340;&#20195;&#30721;&#12290; &#20316;&#20026;&#27010;&#24565;&#35777;&#26126;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ESBMC-AI&#65292;&#23427;&#22522;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;SMT&#30340;&#19978;&#19979;&#25991;&#26377;&#30028;&#27169;&#22411;&#26816;&#26597;&#22120;&#65288;ESBMC&#65289;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;gpt-3.5-turbo&#26469;&#26816;&#27979;&#21644;&#20462;&#22797;C&#31243;&#24207;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a novel solution that combines the capabilities of Large Language Models (LLMs) with Formal Verification strategies to verify and automatically repair software vulnerabilities. Initially, we employ Bounded Model Checking (BMC) to locate the software vulnerability and derive a counterexample. The counterexample provides evidence that the system behaves incorrectly or contains a vulnerability. The counterexample that has been detected, along with the source code, are provided to the LLM engine. Our approach involves establishing a specialized prompt language for conducting code debugging and generation to understand the vulnerability's root cause and repair the code. Finally, we use BMC to verify the corrected version of the code generated by the LLM. As a proof of concept, we create ESBMC-AI based on the Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained Transformer model, specifically gpt-3.5-turbo, to detect and fix errors in C program
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#36890;&#36807;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;MBO&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#31283;&#20581;&#22320;&#23547;&#25214;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.13650</link><description>&lt;p&gt;
&#38754;&#21521;&#19981;&#22343;&#34913;&#25968;&#25454;&#30340;&#40065;&#26834;&#22522;&#20110;&#27169;&#22411;&#30340;&#35774;&#35745;&#30340;&#23646;&#24615;&#24341;&#23548;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Property-Guided Generative Modelling for Robust Model-Based Design with Imbalanced Data. (arXiv:2305.13650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#36890;&#36807;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;MBO&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#31283;&#20581;&#22320;&#23547;&#25214;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#25506;&#32034;&#20855;&#26377;&#26497;&#24230;&#31232;&#30095;&#30340;&#26377;&#24847;&#20041;&#21306;&#22495;&#30340;&#39640;&#32500;&#34507;&#30333;&#36136;&#24207;&#21015;&#31354;&#38388;&#12290;&#36825;&#23548;&#33268;&#20102;&#27169;&#22411;&#20248;&#21270;&#65288;MBO&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#20351;&#29992;&#30001;&#24207;&#21015;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#24341;&#23548;&#30340;&#26377;&#25928;&#25628;&#32034;&#27169;&#22411;&#26469;&#36741;&#21161;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#19981;&#24179;&#34913;&#24615;&#20351;&#24471;&#29616;&#26377;&#30340;MBO&#26041;&#27861;&#24456;&#38590;&#25110;&#26681;&#26412;&#26080;&#27861;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#20854;&#28508;&#22312;&#31354;&#38388;&#30001;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#65292;&#20351;&#24471;&#25353;&#29031;&#36825;&#20123;&#23646;&#24615;&#20540;&#20248;&#20808;&#32771;&#34385;&#26679;&#26412;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#21644;&#21322;&#21512;&#25104;&#34507;&#30333;&#36136;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MBO&#19982;PGVAE&#31283;&#20581;&#22320;&#21457;&#29616;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#65292;&#23613;&#31649;&#25968;&#25454;&#38598;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36830;&#32493;&#35774;&#35745;&#31354;&#38388;&#30340;&#26222;&#36866;&#24615;&#21450;&#20854;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of designing protein sequences with desired properties is challenging, as it requires to explore a high-dimensional protein sequence space with extremely sparse meaningful regions. This has led to the development of model-based optimization (MBO) techniques that aid in the design, by using effective search models guided by the properties over the sequence space. However, the intrinsic imbalanced nature of experimentally derived datasets causes existing MBO approaches to struggle or outright fail. We propose a property-guided variational auto-encoder (PGVAE) whose latent space is explicitly structured by the property values such that samples are prioritized according to these properties. Through extensive benchmarking on real and semi-synthetic protein datasets, we demonstrate that MBO with PGVAE robustly finds sequences with improved properties despite significant dataset imbalances. We further showcase the generality of our approach to continuous design spaces, and its rob
&lt;/p&gt;</description></item><item><title>DLRover&#26159;&#19968;&#20010;&#33258;&#21160;&#37197;&#32622;&#21021;&#22987;&#36164;&#28304;&#24182;&#23454;&#26102;&#35843;&#25972;&#36164;&#28304;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36164;&#28304;&#20849;&#20139;&#21644;&#25163;&#21160;&#37197;&#32622;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01468</link><description>&lt;p&gt;
DLRover&#65306;&#19968;&#31181;&#20855;&#26377;&#33258;&#21160;&#20316;&#19994;&#36164;&#28304;&#25512;&#33616;&#30340;&#24377;&#24615;&#28145;&#24230;&#35757;&#32451;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
DLRover: An Elastic Deep Training Extension with Auto Job Resource Recommendation. (arXiv:2304.01468v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01468
&lt;/p&gt;
&lt;p&gt;
DLRover&#26159;&#19968;&#20010;&#33258;&#21160;&#37197;&#32622;&#21021;&#22987;&#36164;&#28304;&#24182;&#23454;&#26102;&#35843;&#25972;&#36164;&#28304;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36164;&#28304;&#20849;&#20139;&#21644;&#25163;&#21160;&#37197;&#32622;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#20113;&#24179;&#21488;&#19978;&#36827;&#34892;&#36164;&#28304;&#20849;&#20139;&#21487;&#20197;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#65292;&#22240;&#27492;&#20113;&#20173;&#28982;&#26159;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#35757;&#32451;&#20316;&#19994;&#30340;&#27969;&#34892;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#27492;&#31867;&#20849;&#20139;&#20063;&#20026;DL&#35757;&#32451;&#20316;&#19994;&#24102;&#26469;&#20102;&#22810;&#37325;&#25361;&#25112;&#65292;&#20363;&#22914;&#39640;&#20248;&#20808;&#32423;&#20316;&#19994;&#21487;&#33021;&#20250;&#24433;&#21709;&#12289;&#29978;&#33267;&#20013;&#26029;&#20302;&#20248;&#20808;&#32423;&#20316;&#19994;&#12290;&#21516;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;DL&#35757;&#32451;&#31995;&#32479;&#35201;&#27714;&#29992;&#25143;&#22312;&#20316;&#19994;&#25552;&#20132;&#20043;&#21069;&#25163;&#21160;&#37197;&#32622;&#20316;&#19994;&#30340;&#36164;&#28304;&#65288;&#21363;&#20998;&#37197;&#32473;&#27599;&#20010;&#33410;&#28857;&#30340;&#33410;&#28857;&#25968;&#21644;CPU&#12289;&#20869;&#23384;&#31561;&#36164;&#28304;&#65289;&#65292;&#24182;&#19988;&#19981;&#33021;&#22312;&#36816;&#34892;&#26102;&#35843;&#25972;&#20316;&#19994;&#30340;&#36164;&#28304;&#12290;&#20316;&#19994;&#30340;&#36164;&#28304;&#37197;&#32622;&#20250;&#28145;&#21051;&#24433;&#21709;&#35813;&#20316;&#19994;&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;&#35757;&#32451;&#21534;&#21520;&#37327;&#12289;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#23436;&#25104;&#29575;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#20250;&#23548;&#33268;&#20316;&#19994;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#29992;&#25143;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26080;&#27861;&#25552;&#20379;&#26368;&#20339;&#30340;&#36164;&#28304;&#37197;&#32622;&#12290;DLRover&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;DL&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#37197;&#32622;DL&#20316;&#19994;&#30340;&#21021;&#22987;&#36164;&#28304;&#24182;&#21160;&#24577;&#35843;&#25972;&#20316;&#19994;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cloud is still a popular platform for distributed deep learning (DL) training jobs since resource sharing in the cloud can improve resource utilization and reduce overall costs. However, such sharing also brings multiple challenges for DL training jobs, e.g., high-priority jobs could impact, even interrupt, low-priority jobs. Meanwhile, most existing distributed DL training systems require users to configure the resources (i.e., the number of nodes and resources like CPU and memory allocated to each node) of jobs manually before job submission and can not adjust the job's resources during the runtime. The resource configuration of a job deeply affect this job's performance (e.g., training throughput, resource utilization, and completion rate). However, this usually leads to poor performance of jobs since users fail to provide optimal resource configuration in most cases. \system~is a distributed DL framework can auto-configure a DL job's initial resources and dynamically tune the j
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#30340;&#35821;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;AI&#30456;&#32467;&#21512;&#25104;&#20026;&#32508;&#21512;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#23558;&#31526;&#21495;&#30693;&#35782;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#26469;&#35299;&#20915;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.12050</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#30340;&#35821;&#20041;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Semantic Framework for Neural-Symbolic Computing. (arXiv:2212.12050v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12050
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#30340;&#35821;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;AI&#30456;&#32467;&#21512;&#25104;&#20026;&#32508;&#21512;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#23558;&#31526;&#21495;&#30693;&#35782;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#26469;&#35299;&#20915;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#31995;&#32479;&#65292;&#23545;&#20110;&#19968;&#31995;&#21015;AI&#38382;&#39064;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20004;&#32773;&#22343;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#26234;&#33021;&#25152;&#38656;&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#12290;&#20154;&#20204;&#35748;&#20026;&#36825;&#26159;&#27599;&#31181;&#26041;&#27861;&#20869;&#22312;&#24369;&#28857;&#25152;&#33268;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#20123;&#24369;&#28857;&#20284;&#20046;&#26159;&#20114;&#34917;&#30340;&#65292;&#31526;&#21495;&#31995;&#32479;&#25797;&#38271;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#22788;&#29702;&#30340;&#20107;&#29289;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#31070;&#32463;&#31526;&#21495;AI&#39046;&#22495;&#35797;&#22270;&#21033;&#29992;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;AI&#30456;&#32467;&#21512;&#25104;&#20026;&#32508;&#21512;&#31995;&#32479;&#12290;&#36890;&#24120;&#36825;&#26159;&#36890;&#36807;&#23558;&#31526;&#21495;&#30693;&#35782;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20294;&#27809;&#26377;&#20844;&#20849;&#30340;&#32534;&#30721;&#23450;&#20041;&#21487;&#20379;&#27604;&#36739;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#31526;&#21495;AI&#30340;&#35821;&#20041;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#28982;&#21518;&#35777;&#26126;&#23427;&#36275;&#20197;&#35299;&#37322;&#22823;&#37327;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#22522;&#20110;&#31526;&#21495;&#31995;&#32479;&#26893;&#26681;&#20110;&#20854;&#39046;&#22495;&#30340;&#31070;&#32463;&#34920;&#24449;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#35299;&#37322;&#21508;&#31181;&#31526;&#21495;&#31995;&#32479;&#22312;&#31070;&#32463;&#34920;&#24449;&#20013;&#30340;&#23454;&#29616;&#26041;&#24335;&#65292;&#21253;&#25324;&#20351;&#29992;&#23398;&#20064;&#30340;&#31070;&#32463;&#34920;&#24449;&#21644;&#20351;&#29992;&#22266;&#23450;&#31070;&#32463;&#34920;&#24449;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two approaches to AI, neural networks and symbolic systems, have been proven very successful for an array of AI problems. However, neither has been able to achieve the general reasoning ability required for human-like intelligence. It has been argued that this is due to inherent weaknesses in each approach. Luckily, these weaknesses appear to be complementary, with symbolic systems being adept at the kinds of things neural networks have trouble with and vice-versa. The field of neural-symbolic AI attempts to exploit this asymmetry by combining neural networks and symbolic AI into integrated systems. Often this has been done by encoding symbolic knowledge into neural networks. Unfortunately, although many different methods for this have been proposed, there is no common definition of an encoding to compare them. We seek to rectify this problem by introducing a semantic framework for neural-symbolic AI, which is then shown to be general enough to account for a large family of neural-symb
&lt;/p&gt;</description></item></channel></rss>