<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24320;&#21019;&#20102;&#26032;&#30340;&#26041;&#27861;&#35770;&#33539;&#24335;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;&#35774;&#35745;&#36807;&#31243;&#30340;&#21019;&#26032;&#28508;&#21147;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#24191;&#27867;&#24212;&#29992;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;2D&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#27169;&#22411;&#65292;&#24182;&#23457;&#35270;&#20854;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.01335</link><description>&lt;p&gt;
&#24314;&#31569;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Architectural Design: A Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01335
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24320;&#21019;&#20102;&#26032;&#30340;&#26041;&#27861;&#35770;&#33539;&#24335;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;&#35774;&#35745;&#36807;&#31243;&#30340;&#21019;&#26032;&#28508;&#21147;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#24191;&#27867;&#24212;&#29992;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;2D&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#27169;&#22411;&#65292;&#24182;&#23457;&#35270;&#20854;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01335v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24320;&#21019;&#20102;&#26032;&#30340;&#26041;&#27861;&#35770;&#33539;&#24335;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;&#35774;&#35745;&#36807;&#31243;&#30340;&#21019;&#26032;&#28508;&#21147;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#19968;&#36235;&#21183;&#21463;&#30410;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#25991;&#31456;&#20840;&#38754;&#22238;&#39038;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#31361;&#20986;&#20102;&#22312;&#29983;&#25104;2D&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#27169;&#22411;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23457;&#26597;&#26469;&#33258;2020&#24180;&#30340;&#26368;&#26032;&#25991;&#29486;&#65292;&#26412;&#25991;&#23457;&#35270;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#24314;&#31569;&#35774;&#35745;&#30340;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#65292;&#20174;&#29983;&#25104;&#21021;&#22987;&#24314;&#31569;3D&#24418;&#24335;&#21040;&#29983;&#25104;&#26368;&#32456;&#24314;&#31569;&#22270;&#20687;&#12290;&#30740;&#31350;&#22686;&#38271;&#30340;&#26126;&#26174;&#36235;&#21183;&#34920;&#26126;&#24314;&#31569;&#35774;&#35745;&#39046;&#22495;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#20542;&#21521;&#19981;&#26029;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01335v1 Announce Type: cross  Abstract: Generative Artificial Intelligence (AI) has pioneered new methodological paradigms in architectural design, significantly expanding the innovative potential and efficiency of the design process. This paper explores the extensive applications of generative AI technologies in architectural design, a trend that has benefited from the rapid development of deep generative models. This article provides a comprehensive review of the basic principles of generative AI and large-scale models and highlights the applications in the generation of 2D images, videos, and 3D models. In addition, by reviewing the latest literature from 2020, this paper scrutinizes the impact of generative AI technologies at different stages of architectural design, from generating initial architectural 3D forms to producing final architectural imagery. The marked trend of research growth indicates an increasing inclination within the architectural design community towa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#37327;&#21270;&#20132;&#26131;&#31574;&#30053;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#19981;&#20165;&#22312;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#21644;&#24314;&#27169;&#25968;&#25454;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#30340;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2404.00424</link><description>&lt;p&gt;
&#20174;&#27880;&#24847;&#21147;&#21040;&#21033;&#28070;&#65306;&#22522;&#20110;Transformer&#30340;&#37327;&#21270;&#20132;&#26131;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
From attention to profit: quantitative trading strategy based on transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#37327;&#21270;&#20132;&#26131;&#31574;&#30053;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#19981;&#20165;&#22312;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#21644;&#24314;&#27169;&#25968;&#25454;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#37327;&#21270;&#20132;&#26131;&#23454;&#36341;&#20013;&#65292;&#24212;&#23545;&#22797;&#26434;&#21160;&#24577;&#30340;&#37329;&#34701;&#24066;&#22330;&#19968;&#30452;&#26159;&#20010;&#25345;&#20037;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#20805;&#20998;&#25429;&#25417;&#21508;&#31181;&#24066;&#22330;&#21464;&#37327;&#65292;&#32463;&#24120;&#24573;&#35270;&#38271;&#26399;&#20449;&#24687;&#24182;&#19988;&#26080;&#27861;&#25429;&#25417;&#21487;&#33021;&#24102;&#26469;&#21033;&#28070;&#30340;&#22522;&#26412;&#20449;&#21495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#25913;&#36827;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35813;&#27169;&#22411;&#30340;&#26032;&#22411;&#22240;&#23376;&#12290;&#36890;&#36807;&#20174;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19981;&#20165;&#21457;&#25381;&#20102;&#20854;&#21407;&#26377;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#25429;&#25417;&#21644;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20851;&#31995;&#30340;&#20248;&#21183;&#65292;&#32780;&#19988;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#25968;&#20540;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#24182;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#30340;&#22238;&#25253;&#12290;&#35813;&#30740;&#31350;&#25910;&#38598;&#20102;2010&#24180;&#33267;2019&#24180;&#20013;&#22269;&#36164;&#26412;&#24066;&#22330;4,601&#21482;&#32929;&#31080;&#30340;5,000,000&#22810;&#26465;&#28378;&#21160;&#25968;&#25454;&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#32929;&#31080;&#34920;&#29616;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00424v1 Announce Type: cross  Abstract: In traditional quantitative trading practice, navigating the complicated and dynamic financial market presents a persistent challenge. Former machine learning approaches have struggled to fully capture various market variables, often ignore long-term information and fail to catch up with essential signals that may lead the profit. This paper introduces an enhanced transformer architecture and designs a novel factor based on the model. By transfer learning from sentiment analysis, the proposed model not only exploits its original inherent advantages in capturing long-range dependencies and modelling complex data relationships but is also able to solve tasks with numerical inputs and accurately forecast future returns over a period. This work collects more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market from 2010 to 2019. The results of this study demonstrated the model's superior performance in predicting stock
&lt;/p&gt;</description></item><item><title>&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#36890;&#36807;&#32771;&#34385;&#20195;&#29702;&#19982;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#22823;&#30340;&#29366;&#24577;&#21344;&#29992;&#24230;&#20559;&#24046;&#26469;&#36991;&#20813;&#28508;&#22312;&#30340;&#28798;&#38590;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.03185</link><description>&lt;p&gt;
&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Preventing Reward Hacking with Occupancy Measure Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03185
&lt;/p&gt;
&lt;p&gt;
&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#36890;&#36807;&#32771;&#34385;&#20195;&#29702;&#19982;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#22823;&#30340;&#29366;&#24577;&#21344;&#29992;&#24230;&#20559;&#24046;&#26469;&#36991;&#20813;&#28508;&#22312;&#30340;&#28798;&#38590;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#29702;&#26681;&#25454;&#19968;&#20010;&#8220;&#20195;&#29702;&#8221;&#22870;&#21169;&#20989;&#25968;&#65288;&#21487;&#33021;&#26159;&#25163;&#21160;&#25351;&#23450;&#25110;&#23398;&#20064;&#30340;&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30456;&#23545;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#22870;&#21169;&#21364;&#34920;&#29616;&#31967;&#31957;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#22870;&#21169;&#27450;&#39575;&#12290;&#30001;&#20110;&#30830;&#20445;&#20195;&#29702;&#21644;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#33391;&#22909;&#23545;&#40784;&#26497;&#20026;&#22256;&#38590;&#65292;&#39044;&#38450;&#22870;&#21169;&#27450;&#39575;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20445;&#23432;&#22320;&#20248;&#21270;&#20195;&#29702;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#20110;&#36890;&#36807;&#24809;&#32602;&#20182;&#20204;&#30340;&#34892;&#20026;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#24378;&#21046;&#35753;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#34920;&#29616;&#31867;&#20284;&#20110;&#8220;&#23433;&#20840;&#8221;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#34892;&#20026;&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#65292;&#22240;&#20026;&#22312;&#21333;&#20010;&#29366;&#24577;&#19979;&#34892;&#20026;&#20998;&#24067;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#65292;&#32780;&#36739;&#22823;&#30340;&#21464;&#21270;&#21487;&#33021;&#24182;&#19981;&#20195;&#34920;&#20219;&#20309;&#21361;&#38505;&#27963;&#21160;&#12290;&#25105;&#20204;&#30340;&#35265;&#35299;&#26159;&#65292;&#24403;&#22870;&#21169;&#27450;&#39575;&#26102;&#65292;&#20195;&#29702;&#35775;&#38382;&#30340;&#29366;&#24577;&#19982;&#23433;&#20840;&#31574;&#30053;&#36798;&#21040;&#30340;&#29366;&#24577;&#25130;&#28982;&#19981;&#21516;&#65292;&#23548;&#33268;&#29366;&#24577;&#21344;&#29992;&#24230;&#30340;&#24040;&#22823;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03185v1 Announce Type: cross  Abstract: Reward hacking occurs when an agent performs very well with respect to a "proxy" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward. Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively. Prior work has particularly focused on enforcing the learned policy to behave similarly to a "safe" policy by penalizing the KL divergence between their action distributions (AD). However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity. Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;Transformer&#20013;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#19979;&#28608;&#27963;&#29305;&#23450;token&#39044;&#27979;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#38416;&#26126;&#22312;LLMs&#20013;&#27880;&#24847;&#21147;&#22914;&#20309;&#20419;&#25104;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#21270;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.15055</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#35299;&#37322;&#19978;&#19979;&#25991;&#26597;&#25214;&#65306;&#25506;&#31350;&#27880;&#24847;&#21147;-MLP&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;Transformer&#20013;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#19979;&#28608;&#27963;&#29305;&#23450;token&#39044;&#27979;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#38416;&#26126;&#22312;LLMs&#20013;&#27880;&#24847;&#21147;&#22914;&#20309;&#20419;&#25104;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#21270;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#21644;Multilayer Perceptron&#20013;&#19987;&#38376;&#39044;&#27979;&#29305;&#23450;token&#30340;"next-token"&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#20419;&#20351;&#20687;GPT-4&#36825;&#26679;&#30340;LLM&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#20869;&#37096;&#65292;&#25105;&#20204;&#21487;&#20197;&#38416;&#26126;&#28608;&#27963;&#26576;&#20123;next-token&#31070;&#32463;&#20803;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30830;&#23450;&#20102;&#35782;&#21035;&#19982;&#39044;&#27979;&#29305;&#23450;token&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#30340;attention heads&#65292;&#36890;&#36807;&#27531;&#24046;&#36830;&#25509;&#28608;&#27963;&#30456;&#20851;&#32852;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#36739;&#26089;&#30340;&#23618;&#20013;&#22987;&#32456;&#28608;&#27963;&#30456;&#21516;next-token&#31070;&#32463;&#20803;&#30340;attention heads&#12290;&#25506;&#32034;&#36825;&#20123;&#19981;&#21516;&#30340;&#28608;&#27963;&#27169;&#24335;&#25581;&#31034;&#20102;&#20026;&#19981;&#21516;&#35821;&#35328;&#19978;&#19979;&#25991;&#19987;&#38376;&#21270;&#30340;&#22836;&#19982;&#29983;&#25104;&#26576;&#20123;tokens&#30456;&#20851;&#32852;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#35299;&#37322;&#21644;&#25506;&#27979;&#23396;&#31435;&#30340;&#32452;&#20214;&#65292;&#20197;&#38416;&#26126;&#27880;&#24847;&#21147;&#22914;&#20309;&#20351;LLMs&#20013;&#30340;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#22788;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15055v1 Announce Type: cross  Abstract: In this paper, we investigate the interplay between attention heads and specialized "next-token" neurons in the Multilayer Perceptron that predict specific tokens. By prompting an LLM like GPT-4 to explain these model internals, we can elucidate attention mechanisms that activate certain next-token neurons. Our analysis identifies attention heads that recognize contexts relevant to predicting a particular token, activating the associated neuron through the residual connection. We focus specifically on heads in earlier layers consistently activating the same next-token neuron across similar prompts. Exploring these differential activation patterns reveals that heads that specialize for distinct linguistic contexts are tied to generating certain tokens. Overall, our method combines neural explanations and probing isolated components to illuminate how attention enables context-dependent, specialized processing in LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31361;&#20986;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#35760;&#24405;&#21644;&#20808;&#36827;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#38761;&#26032;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.14622</link><description>&lt;p&gt;
&#20174;&#20851;&#38190;&#35789;&#21040;&#32467;&#26500;&#21270;&#25688;&#35201;: &#31934;&#31616;&#23398;&#26415;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31361;&#20986;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#35760;&#24405;&#21644;&#20808;&#36827;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#38761;&#26032;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#30701;&#25991;&#24378;&#35843;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#26085;&#30410;&#37325;&#35201;&#65292;&#25351;&#20986;&#20256;&#32479;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25628;&#32034;&#24341;&#25806;&#30001;&#20110;&#20986;&#29256;&#29289;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#32780;&#25928;&#29575;&#20302;&#19979;&#12290;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#32467;&#26500;&#21270;&#35760;&#24405;&#65292;&#25903;&#25345;&#20808;&#36827;&#30340;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#20202;&#34920;&#26495;&#65292;&#20197;&#24443;&#24213;&#25913;&#21464;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#65292;&#21462;&#20195;&#20256;&#32479;&#30340;&#25991;&#26412;&#23494;&#38598;&#22411;&#26041;&#27861;&#12290;&#36825;&#19968;&#24895;&#26223;&#36890;&#36807;&#19968;&#20010;&#20197;&#8220;&#20256;&#26579;&#30149;&#30340;&#32321;&#27542;&#25968;&#20272;&#35745;&#8221;&#30740;&#31350;&#20027;&#39064;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#39564;&#35777;&#24471;&#20197;&#20307;&#29616;&#65292;&#20351;&#29992;&#32463;&#36807;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33258;&#21160;&#21019;&#24314;&#32467;&#26500;&#21270;&#35760;&#24405;&#20197;&#22635;&#20805;&#19968;&#20010;&#36229;&#36234;&#20851;&#38190;&#35789;&#30340;&#21518;&#31471;&#25968;&#25454;&#24211;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#19979;&#19968;&#20195;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#22312;https://orkg.org/usecases/r0-estimates &#19978;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14622v1 Announce Type: cross  Abstract: This short paper highlights the growing importance of information retrieval (IR) engines in the scientific community, addressing the inefficiency of traditional keyword-based search engines due to the rising volume of publications. The proposed solution involves structured records, underpinning advanced information technology (IT) tools, including visualization dashboards, to revolutionize how researchers access and filter articles, replacing the traditional text-heavy approach. This vision is exemplified through a proof of concept centered on the ``reproductive number estimate of infectious diseases'' research theme, using a fine-tuned large language model (LLM) to automate the creation of structured records to populate a backend database that now goes beyond keywords. The result is a next-generation IR method accessible at https://orkg.org/usecases/r0-estimates.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#25361;&#25112;&#21450;&#23545;&#31574;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.12617</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#65306;&#25361;&#25112;&#19982;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Generative AI Security: Challenges and Countermeasures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12617
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#25361;&#25112;&#21450;&#23545;&#31574;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12617v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#34892;&#19994;&#30340;&#19981;&#26029;&#25193;&#23637;&#24341;&#21457;&#20102;&#20154;&#20204;&#30340;&#20852;&#22859;&#21644;&#22686;&#21152;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#29420;&#29305;&#23433;&#20840;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#31649;&#29702;&#36825;&#20123;&#39118;&#38505;&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12617v1 Announce Type: cross  Abstract: Generative AI's expanding footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Altruistic Gradient Adjustment (AgA)&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#35843;&#25972;&#26469;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#65292;&#21152;&#36895;&#25910;&#25947;&#21040;&#26399;&#26395;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2402.12416</link><description>&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20013;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Aligning Individual and Collective Objectives in Multi-Agent Cooperation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12416
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Altruistic Gradient Adjustment (AgA)&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#35843;&#25972;&#26469;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#65292;&#21152;&#36895;&#25910;&#25947;&#21040;&#26399;&#26395;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#39046;&#22495;&#65292;&#38754;&#20020;&#30528;&#28151;&#21512;&#21160;&#26426;&#21512;&#20316;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#30683;&#30462;&#12290;&#24403;&#21069;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#22870;&#21169;&#25110;&#24341;&#20837;&#39069;&#22806;&#26426;&#21046;&#26469;&#20419;&#36827;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#30528;&#25163;&#21160;&#35774;&#35745;&#25104;&#26412;&#21644;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#30340;&#25910;&#25947;&#31243;&#24207;&#35299;&#20915;&#26041;&#26696;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23558;&#28151;&#21512;&#21160;&#26426;&#21338;&#24328;&#24314;&#27169;&#20026;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#20197;&#30740;&#31350;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Altruistic Gradient Adjustment (AgA)&#30340;&#26032;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#35843;&#25972;&#26469;&#26032;&#39062;&#22320;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#35777;&#26126;&#65292;AgA&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#23545;&#40784;&#26435;&#37325;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#21040;&#26399;&#26395;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12416v1 Announce Type: cross  Abstract: In the field of multi-agent learning, the challenge of mixed-motive cooperation is pronounced, given the inherent contradictions between individual and collective goals. Current research in this domain primarily focuses on incorporating domain knowledge into rewards or introducing additional mechanisms to foster cooperation. However, many of these methods suffer from the drawbacks of manual design costs and the lack of a theoretical grounding convergence procedure to the solution. To address this gap, we approach the mixed-motive game by modeling it as a differentiable game to study learning dynamics. We introduce a novel optimization method named Altruistic Gradient Adjustment (AgA) that employs gradient adjustments to novelly align individual and collective objectives. Furthermore, we provide theoretical proof that the selection of an appropriate alignment weight in AgA can accelerate convergence towards the desired solutions while e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#20102;&#36234;&#29425;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26377;&#23475;&#38382;&#39064;&#30340;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;59.42%&#12290;</title><link>https://arxiv.org/abs/2402.10601</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#35789;&#26367;&#25442;&#23494;&#30721;&#26469;&#36234;&#29425;&#19987;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Proprietary Large Language Models using Word Substitution Cipher
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#20102;&#36234;&#29425;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26377;&#23475;&#38382;&#39064;&#30340;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;59.42%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36981;&#24490;&#36947;&#24503;&#21644;&#20262;&#29702;&#20934;&#21017;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21517;&#20026;Jailbreak&#30340;&#21019;&#24847;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25552;&#31034;&#21487;&#20197;&#32469;&#36807;&#23545;&#40784;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36234;&#29425;&#25552;&#31034;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#65288;&#20027;&#35201;&#26159;&#33521;&#35821;&#65289;&#20013;&#30340;&#26377;&#23475;&#38382;&#39064;&#65292;&#21487;&#20197;&#34987;LLMs&#33258;&#36523;&#26816;&#27979;&#21040;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#26368;&#20808;&#36827;&#30340;LLM&#65292;GPT-4&#19978;&#36827;&#34892;&#20102;&#19968;&#20010;&#35797;&#28857;&#30740;&#31350;&#65292;&#35299;&#30721;&#20102;&#20351;&#29992;&#21508;&#31181;&#23494;&#30721;&#25216;&#26415;&#21152;&#23494;&#30340;&#20960;&#20010;&#23433;&#20840;&#21477;&#23376;&#65292;&#21457;&#29616;&#31616;&#21333;&#30340;&#21333;&#35789;&#26367;&#25442;&#23494;&#30721;&#21487;&#20197;&#34987;&#26368;&#26377;&#25928;&#22320;&#35299;&#30721;&#12290;&#21463;&#27492;&#32467;&#26524;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#32534;&#30721;&#25216;&#26415;&#26469;&#32534;&#20889;&#36234;&#29425;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#19981;&#23433;&#20840;&#21333;&#35789;&#26144;&#23556;&#21040;&#23433;&#20840;&#21333;&#35789;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26144;&#23556;&#30340;&#21333;&#35789;&#25552;&#20986;&#19981;&#23433;&#20840;&#38382;&#39064;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;&#39640;&#36798;59.42%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10601v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process. However, most jailbreaking prompts contain harmful questions in the natural language (mainly English), which can be detected by the LLM themselves. In this paper, we present jailbreaking prompts encoded using cryptographic techniques. We first present a pilot study on the state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively. Motivated by this result, we use this encoding technique for writing jailbreaking prompts. We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words. Experimental results show an attack success rate (up to 59.42%) of our proposed jailbrea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#35299;&#20915;&#20102;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#26080;&#27861;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10192</link><description>&lt;p&gt;
&#20511;&#37492;&#22810;&#20307;&#29289;&#29702;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#35299;&#20915;&#20102;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#26080;&#27861;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#19981;&#36879;&#26126;&#30340;&#12289;&#31867;&#20284;&#20110;&#31070;&#35861;&#33324;&#30340;&#29305;&#24615;&#65292;&#20351;&#24471;&#35299;&#37322;&#21644;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#20102;&#34987;&#31216;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#35813;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#65292;&#23558;&#24605;&#32500;&#36807;&#31243;&#24314;&#27169;&#20026;&#19968;&#20010;&#22312;&#20855;&#26377;&#27010;&#24565;&#38468;&#21152;&#30340;&#39030;&#28857;&#30340;&#22270;&#19978;&#30340;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;&#34429;&#28982;&#36825;&#31181;&#25551;&#36848;&#20855;&#26377;&#21508;&#31181;&#22909;&#22788;&#65292;&#21253;&#25324;&#37327;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#19981;&#33021;&#33258;&#28982;&#22320;&#29992;&#26469;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#30340;&#25512;&#24191;&#65292;&#23427;&#23558;&#24605;&#32500;&#36807;&#31243;&#35270;&#20026;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10192v1 Announce Type: cross  Abstract: With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life. However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions. This problem led to the development of the field known as eXplainable Artificial Intelligence (XAI). One method in this field known as Projective Simulation (PS) models a chain-of-thought as a random walk of a particle on a graph with vertices that have concepts attached to them. While this description has various benefits, including the possibility of quantization, it cannot be naturally used to model thoughts that combine several concepts simultaneously. To overcome this limitation, we introduce Multi-Excitation Projective Simulation (mePS), a generalization that considers a chain-of-thought to be a random walk of several particles on a hypergraph. A definition for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12289;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#12290;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#20063;&#34987;&#35752;&#35770;&#21040;&#12290;</title><link>https://arxiv.org/abs/2402.05741</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Real-World Robot Applications of Foundation Models: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12289;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#12290;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#20063;&#34987;&#35752;&#35770;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#31561;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20026;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#24577;&#30340;&#28789;&#27963;&#24212;&#29992;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;&#23427;&#20204;&#30340;&#24433;&#21709;&#28085;&#30422;&#20102;&#21253;&#25324;&#21307;&#30103;&#12289;&#25945;&#32946;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#24635;&#32467;&#28085;&#30422;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#30340;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#27010;&#29575;&#39564;&#35777;AI&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#24378;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#21333;&#35843;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04892</link><description>&lt;p&gt;
&#36890;&#36807;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#27010;&#29575;&#39564;&#35777;AI&#31995;&#32479;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#27010;&#29575;&#39564;&#35777;AI&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#24378;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#21333;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#24418;&#24335;&#39564;&#35777;&#65288;PFV) AI&#31995;&#32479;&#36824;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#23545;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#27169;&#22411;&#21644;/&#25110;&#23646;&#24615;&#65292;&#26041;&#27861;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#31639;&#27861;&#32780;&#24050;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#65288;WMI&#65289;&#30340;AI&#31995;&#32479;PFV&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#21487;&#20197;&#38750;&#24120;&#36890;&#29992;&#22320;&#23450;&#20041;&#38382;&#39064;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#31181;&#32422;&#31616;&#21487;&#20197;&#22312;&#19981;&#20570;&#36807;&#24378;&#30340;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#35768;&#22810;&#26377;&#36259;&#30340;&#23646;&#24615;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#21333;&#35843;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;WMI&#27714;&#35299;&#22120;&#35299;&#20915;&#22810;&#20010;&#39564;&#35777;&#20219;&#21153;&#26469;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#65292;&#28982;&#21518;&#35752;&#35770;&#19982;&#36825;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#30456;&#20851;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The probabilistic formal verification (PFV) of AI systems is in its infancy. So far, approaches have been limited to ad-hoc algorithms for specific classes of models and/or properties.   We propose a unifying framework for the PFV of AI systems based onWeighted Model Integration (WMI), which allows to frame the problem in very general terms.   Crucially, this reduction enables the verification of many properties of interest, like fairness, robustness or monotonicity, over a wide range of machine learning models, without making strong distributional assumptions.   We support the generality of the approach by solving multiple verification tasks with a single, off-the-shelf WMI solver, then discuss the scalability challenges and research directions related to this promising framework.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#20013;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#20174;&#26377;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;LFMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UIM&#26694;&#26550;&#65292;&#26088;&#22312;&#29702;&#35299;LFMs&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#24182;&#35299;&#37322;&#20854;&#20013;&#30340;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.01909</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Catastrophic Inheritance of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#20013;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#20174;&#26377;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;LFMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UIM&#26694;&#26550;&#65292;&#26088;&#22312;&#29702;&#35299;LFMs&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#24182;&#35299;&#37322;&#20854;&#20013;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#22768;&#31216;&#20855;&#26377;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#20154;&#20204;&#23545;&#23427;&#20204;&#22312;&#26426;&#22120;&#23398;&#20064;&#20197;&#21450;&#20854;&#20182;&#21508;&#20010;&#23398;&#31185;&#20013;&#30340;&#31070;&#31192;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#28508;&#21147;&#25552;&#20986;&#20102;&#26497;&#22823;&#20851;&#20999;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#21363;LFMs&#20013;&#26681;&#28145;&#33922;&#22266;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#20174;&#26377;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;LFMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#21253;&#25324;&#21463;&#25439;&#12289;&#38271;&#23614;&#12289;&#26377;&#22122;&#38899;&#12289;&#36229;&#20986;&#20998;&#24067;&#31561;&#26679;&#26412;&#12290;&#36825;&#31181;&#32487;&#25215;&#21487;&#33021;&#23545;&#19979;&#28216;&#24212;&#29992;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#65292;&#22914;&#20559;&#35265;&#12289;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12289;&#24615;&#33021;&#19979;&#38477;&#12289;&#23433;&#20840;&#28431;&#27934;&#12289;&#38544;&#31169;&#27844;&#38706;&#21644;&#20215;&#20540;&#35823;&#24046;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#38382;&#39064;&#32972;&#21518;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;UIM&#26694;&#26550;&#65292;&#26469;&#29702;&#35299;LFMs&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#21253;&#25324;&#26469;&#33258;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#36866;&#24212;&#30340;&#32487;&#25215;&#20869;&#23481;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models (LFMs) are claiming incredible performances. Yet great concerns have been raised about their mythic and uninterpreted potentials not only in machine learning, but also in various other disciplines. In this position paper, we propose to identify a neglected issue deeply rooted in LFMs: Catastrophic Inheritance, describing the weaknesses and limitations inherited from biased large-scale pre-training data to behaviors of LFMs on the downstream tasks, including samples that are corrupted, long-tailed, noisy, out-of-distributed, to name a few. Such inheritance can potentially cause catastrophes to downstream applications, such as bias, lack of generalization, deteriorated performance, security vulnerability, privacy leakage, and value misalignment. We discuss the challenges behind this issue and propose UIM, a framework to Understand the catastrophic inheritance of LFMs from both pre-training and downstream adaptation, Interpret the implications of catastrophic inher
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#35270;&#20026;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#32479;&#19968;&#26816;&#32034;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RetriNet&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#31934;&#30830;&#26816;&#32034;&#20027;&#39064;&#23646;&#24615;&#24182;&#36807;&#28388;&#26080;&#20851;&#20449;&#24687;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#22312;&#20154;&#33080;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.02521</link><description>&lt;p&gt;
&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#26816;&#32034;&#26465;&#20214;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Retrieving Conditions from Reference Images for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#35270;&#20026;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#32479;&#19968;&#26816;&#32034;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RetriNet&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#31934;&#30830;&#26816;&#32034;&#20027;&#39064;&#23646;&#24615;&#24182;&#36807;&#28388;&#26080;&#20851;&#20449;&#24687;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#22312;&#20154;&#33080;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#24320;&#21457;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25216;&#26415;&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21508;&#31181;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#24341;&#36215;&#20102;&#21508;&#31181;&#24212;&#29992;&#30340;&#30456;&#24403;&#22823;&#20852;&#36259;&#12290;&#19968;&#20010;&#26222;&#36941;&#30340;&#22330;&#26223;&#26159;&#22522;&#20110;&#21442;&#32771;&#22270;&#20687;&#20013;&#30340;&#19968;&#20010;&#20027;&#39064;&#29983;&#25104;&#26032;&#30340;&#22270;&#20687;&#12290;&#36825;&#20010;&#20027;&#39064;&#21487;&#20197;&#26159;&#39118;&#26684;&#21270;&#22836;&#20687;&#30340;&#38754;&#37096;&#36523;&#20221;&#65292;&#34394;&#25311;&#35797;&#31359;&#30340;&#36523;&#20307;&#21644;&#26381;&#35013;&#31561;&#12290;&#28385;&#36275;&#36825;&#19968;&#35201;&#27714;&#27491;&#22312;&#28436;&#21464;&#25104;&#19968;&#38376;&#31216;&#20026;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#35270;&#20026;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#32479;&#19968;&#26816;&#32034;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RetriNet&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#31934;&#30830;&#22320;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#26816;&#32034;&#20027;&#39064;&#23646;&#24615;&#24182;&#36807;&#28388;&#25481;&#26080;&#20851;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;RetriNet&#22312;&#20154;&#33080;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02521v2 Announce Type: replace-cross  Abstract: Newly developed diffusion-based techniques have showcased phenomenal abilities in producing a wide range of high-quality images, sparking considerable interest in various applications. A prevalent scenario is to generate new images based on a subject from reference images. This subject could be face identity for styled avatars, body and clothing for virtual try-on and so on. Satisfying this requirement is evolving into a field called Subject-Driven Generation. In this paper, we consider Subject-Driven Generation as a unified retrieval problem with diffusion models. We introduce a novel diffusion model architecture, named RetriNet, designed to address and solve these problems by retrieving subject attributes from reference images precisely, and filter out irrelevant information. RetriNet demonstrates impressive performance when compared to existing state-of-the-art approaches in face generation. We further propose a research and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.10107</link><description>&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs. (arXiv:2310.10107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#20110;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#65292;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#30340;&#23398;&#20064;&#30001;&#20110;&#35266;&#23519;&#25968;&#25454;&#38590;&#20197;&#35299;&#35835;&#32780;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#30693;&#36716;&#31227;&#21644;&#35266;&#27979;&#27169;&#22411;&#30340;POMDPs&#20013;&#30340;&#24207;&#21015;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#21518;&#39564;&#37319;&#26679;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PSRL&#65289;&#22312;POMDPs&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#35777;&#26126;&#20854;&#36125;&#21494;&#26031;&#36951;&#25022;&#38543;&#30528;&#24207;&#21015;&#30340;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#32780;&#32553;&#23567;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36951;&#25022;&#38543;&#30528;&#26102;&#38388;&#38271;&#24230;$H$&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#19979;&#30028;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;POMDP&#26159;&#27424;&#23436;&#22791;&#19988;&#24369;&#21487;&#35782;&#21035;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#30456;&#27604;&#20110;arXiv:2204.08967&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#25913;&#36827;&#20102;&#36951;&#25022;&#30028;&#32422;$\Omega(H^2\sqrt{SA})$&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to Markov Decision Processes (MDPs), learning in Partially Observable Markov Decision Processes (POMDPs) can be significantly harder due to the difficulty of interpreting observations. In this paper, we consider episodic learning problems in POMDPs with unknown transition and observation models. We consider the Posterior Sampling-based Reinforcement Learning (PSRL) algorithm for POMDPs and show that its Bayesian regret scales as the square root of the number of episodes. In general, the regret scales exponentially with the horizon length $H$, and we show that this is inevitable by providing a lower bound. However, under the condition that the POMDP is undercomplete and weakly revealing, we establish a polynomial Bayesian regret bound that improves the regret bound by a factor of $\Omega(H^2\sqrt{SA})$ over the recent result by arXiv:2204.08967.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2307.07840</link><description>&lt;p&gt;
RegExplainer: &#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task. (arXiv:2307.07840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22238;&#24402;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#37322;&#25216;&#26415;&#22823;&#22810;&#38480;&#20110;&#29702;&#35299;&#20998;&#31867;&#20219;&#21153;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23547;&#27714;&#35299;&#37322;&#26469;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65288;XAIG-R&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#20998;&#24067;&#20559;&#31227;&#21644;&#36830;&#32493;&#26377;&#24207;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#24212;&#23545;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;&#20026;&#20102;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#21363;&#26631;&#39064;&#22870;&#21169;&#21644;SAM&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.19599</link><description>&lt;p&gt;
&#32454;&#31890;&#24230;&#35821;&#20041;&#22870;&#21169;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards. (arXiv:2305.19599v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#21363;&#26631;&#39064;&#22870;&#21169;&#21644;SAM&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#22312;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#19979;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32454;&#31890;&#24230;&#35821;&#20041;&#25351;&#23548;&#65292;&#20197;&#25104;&#21151;&#35786;&#26029;&#24418;&#24577;&#24046;&#24322;&#20026;&#27490;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#25991;&#26412;&#27010;&#24565;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#20934;&#30830;&#24418;&#24577;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26032;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#22870;&#21169;--&#26631;&#39064;&#22870;&#21169;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#20309;&#20107;&#29289;&#65288;SAM&#65289;&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-image diffusion models have achieved remarkable success in generating high-quality, realistic images from given text prompts. However, previous methods fail to perform accurate modality alignment between text concepts and generated images due to the lack of fine-level semantic guidance that successfully diagnoses the modality discrepancy. In this paper, we propose FineRewards to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the Semantic Segment Anything (SAM) reward. From the global semantic view, the caption reward generates a corresponding detailed caption that depicts all important contents in the synthetic image via a BLIP-2 model and then calculates the reward score by measuring the similarity between the generated caption and the given prompt. From the local semantic view, the SAM reward segments the generated images into local parts with categ
&lt;/p&gt;</description></item><item><title>GPT-SW3&#26159;&#38754;&#21521;&#21271;&#27431;&#35821;&#35328;&#30340;&#31532;&#19968;&#20010;&#26412;&#22320;&#21270;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20854;&#24320;&#21457;&#36807;&#31243;&#65292;&#21487;&#20316;&#20026;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#38754;&#21521;&#36739;&#23567;&#35821;&#35328;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#21335;&#21644;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2305.12987</link><description>&lt;p&gt;
GPT-SW3&#65306;&#19968;&#31181;&#38754;&#21521;&#21271;&#27431;&#35821;&#35328;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GPT-SW3: An Autoregressive Language Model for the Nordic Languages. (arXiv:2305.12987v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12987
&lt;/p&gt;
&lt;p&gt;
GPT-SW3&#26159;&#38754;&#21521;&#21271;&#27431;&#35821;&#35328;&#30340;&#31532;&#19968;&#20010;&#26412;&#22320;&#21270;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20854;&#24320;&#21457;&#36807;&#31243;&#65292;&#21487;&#20316;&#20026;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#38754;&#21521;&#36739;&#23567;&#35821;&#35328;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#21335;&#21644;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#24320;&#21457;&#38754;&#21521;&#21271;&#27431;&#35821;&#35328;&#30340;&#31532;&#19968;&#20010;&#26412;&#22320;&#21270;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;GPT-SW3&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#24320;&#21457;&#36807;&#31243;&#30340;&#25152;&#26377;&#37096;&#20998;&#65292;&#20174;&#25968;&#25454;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#35757;&#32451;&#37197;&#32622;&#21644;&#25351;&#20196;&#24494;&#35843;&#65292;&#21040;&#35780;&#20272;&#21644;&#21457;&#24067;&#31574;&#30053;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#20316;&#20026;&#25351;&#21335;&#21644;&#21442;&#32771;&#65292;&#24110;&#21161;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#38754;&#21521;&#36739;&#23567;&#35821;&#35328;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper details the process of developing the first native large generative language model for the Nordic languages, GPT-SW3. We cover all parts of the development process, from data collection and processing, training configuration and instruction finetuning, to evaluation and considerations for release strategies. We hope that this paper can serve as a guide and reference for other researchers that undertake the development of large generative models for smaller languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#20102;&#21487;&#38752;&#24615;&#24179;&#34913;&#26041;&#27861;&#26469;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01486</link><description>&lt;p&gt;
ARBEx&#65306;&#29992;&#20110;&#40065;&#26834;&#24615;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#30340;&#20851;&#27880;&#29305;&#24449;&#25552;&#21462;&#19982;&#21487;&#38752;&#24615;&#24179;&#34913;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning. (arXiv:2305.01486v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#20102;&#21487;&#38752;&#24615;&#24179;&#34913;&#26041;&#27861;&#26469;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#26159;&#30001;Vision Transformer&#39537;&#21160;&#30340;&#26032;&#22411;&#20851;&#27880;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#24102;&#26377;&#21487;&#38752;&#24615;&#24179;&#34913;&#65292;&#20197;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#36739;&#24046;&#31867;&#20998;&#24067;&#12289;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#31934;&#21270;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#31383;&#21475;&#30340;&#20132;&#21449;&#20851;&#27880;ViT&#26469;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#65292;&#21152;&#19978;&#26631;&#31614;&#20998;&#24067;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#36890;&#36807;&#21487;&#38752;&#24615;&#24179;&#34913;&#20248;&#21270;&#23545;&#24369;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#39640;&#26631;&#31614;&#39044;&#27979;&#38887;&#24615;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#30830;&#20445;&#27491;&#30830;&#30340;&#26631;&#31614;&#20998;&#31867;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38170;&#25439;&#22833;&#65292;&#40723;&#21169;&#38170;&#28857;&#20043;&#38388;&#30340;&#22823;&#38388;&#38548;&#12290;&#21478;&#22806;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#20063;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#23545;&#20110;&#25552;&#21319;&#22312;FEL&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;ARBEx&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a framework ARBEx, a novel attentive feature extraction framework driven by Vision Transformer with reliability balancing to cope against poor class distributions, bias, and uncertainty in the facial expression learning (FEL) task. We reinforce several data pre-processing and refinement methods along with a window-based cross-attention ViT to squeeze the best of the data. We also employ learnable anchor points in the embedding space with label distributions and multi-head self-attention mechanism to optimize performance against weak predictions with reliability balancing, which is a strategy that leverages anchor points, attention scores, and confidence values to enhance the resilience of label predictions. To ensure correct label classification and improve the models' discriminative power, we introduce anchor loss, which encourages large margins between anchor points. Additionally, the multi-head self-attention mechanism, which is also trainable, plays an i
&lt;/p&gt;</description></item><item><title>TargetCall&#36890;&#36807;&#39044;&#22522;&#35843;&#36807;&#28388;&#65292;&#28040;&#38500;&#20102;basecalling&#20013;&#30340;&#28010;&#36153;&#35745;&#31639;&#65292;&#25552;&#39640;&#20102;&#22522;&#22240;&#32452;&#20998;&#26512;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.04953</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#22522;&#35843;&#36807;&#28388;&#28040;&#38500;basecalling&#20013;&#30340;&#28010;&#36153;&#35745;&#31639;&#30340;TargetCall
&lt;/p&gt;
&lt;p&gt;
TargetCall: Eliminating the Wasted Computation in Basecalling via Pre-Basecalling Filtering. (arXiv:2212.04953v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04953
&lt;/p&gt;
&lt;p&gt;
TargetCall&#36890;&#36807;&#39044;&#22522;&#35843;&#36807;&#28388;&#65292;&#28040;&#38500;&#20102;basecalling&#20013;&#30340;&#28010;&#36153;&#35745;&#31639;&#65292;&#25552;&#39640;&#20102;&#22522;&#22240;&#32452;&#20998;&#26512;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Basecalling&#26159;&#32435;&#31859;&#23380;&#27979;&#24207;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#23427;&#23558;&#32435;&#31859;&#23380;&#27979;&#24207;&#20202;&#30340;&#21407;&#22987;&#20449;&#21495;&#36716;&#25442;&#20026;&#26680;&#37240;&#24207;&#21015;&#65292;&#21363;reads&#12290;&#26368;&#20808;&#36827;&#30340;basecallers&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#39640;&#24230;&#30340;basecalling&#20934;&#30830;&#24615;&#12290;&#36825;&#20351;&#24471;basecalling&#22312;&#35745;&#31639;&#19978;&#25928;&#29575;&#20302;&#19979;&#19988;&#20869;&#23384;&#28040;&#32791;&#22823;&#65292;&#25104;&#20026;&#25972;&#20010;&#22522;&#22240;&#32452;&#20998;&#26512;&#27969;&#31243;&#30340;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#26469;&#35828;&#65292;&#22823;&#22810;&#25968;reads&#19982;&#24863;&#20852;&#36259;&#30340;&#21442;&#32771;&#22522;&#22240;&#32452;&#19981;&#21305;&#37197;&#65288;&#21363;&#30446;&#26631;&#21442;&#32771;&#22522;&#22240;&#32452;&#65289;&#65292;&#22240;&#27492;&#20250;&#22312;&#21518;&#32493;&#30340;&#22522;&#22240;&#32452;&#27969;&#31243;&#27493;&#39588;&#20013;&#34987;&#20002;&#24323;&#65292;&#28010;&#36153;&#20102;basecalling&#30340;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TargetCall&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#28040;&#38500;basecalling&#20013;&#28010;&#36153;&#35745;&#31639;&#30340;&#39044;&#22522;&#35843;&#36807;&#28388;&#22120;&#12290;TargetCall&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;basecalling&#20043;&#21069;&#20002;&#24323;&#19981;&#20250;&#19982;&#30446;&#26631;&#21442;&#32771;&#22522;&#22240;&#32452;&#21305;&#37197;&#30340;reads&#65288;&#21363;&#38750;&#30446;&#26631;reads&#65289;&#12290;TargetCall&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;1&#65289;LightCall&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#31070;&#32463;&#32593;&#32476;basecaller&#65292;&#20135;&#29983;&#22122;&#22768;reads&#65307;
&lt;/p&gt;
&lt;p&gt;
Basecalling is an essential step in nanopore sequencing analysis where the raw signals of nanopore sequencers are converted into nucleotide sequences, i.e., reads. State-of-the-art basecallers employ complex deep learning models to achieve high basecalling accuracy. This makes basecalling computationally-inefficient and memory-hungry; bottlenecking the entire genome analysis pipeline. However, for many applications, the majority of reads do no match the reference genome of interest (i.e., target reference) and thus are discarded in later steps in the genomics pipeline, wasting the basecalling computation. To overcome this issue, we propose TargetCall, the first pre-basecalling filter to eliminate the wasted computation in basecalling. TargetCall's key idea is to discard reads that will not match the target reference (i.e., off-target reads) prior to basecalling. TargetCall consists of two main components: (1) LightCall, a lightweight neural network basecaller that produces noisy reads;
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#20803;&#20114;&#25554;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;Few-Task Meta-Learning&#38382;&#39064;&#20013;&#20219;&#21153;&#25968;&#37327;&#23569;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#21516;&#26102;&#35813;&#26041;&#27861;&#23545;&#39046;&#22495;&#19981;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2205.09990</link><description>&lt;p&gt;
&#38024;&#23545;Few-Task Meta-Learning&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#20803;&#20114;&#25554;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Set-based Meta-Interpolation for Few-Task Meta-Learning. (arXiv:2205.09990v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#20803;&#20114;&#25554;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;Few-Task Meta-Learning&#38382;&#39064;&#20013;&#20219;&#21153;&#25968;&#37327;&#23569;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#21516;&#26102;&#35813;&#26041;&#27861;&#23545;&#39046;&#22495;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#22312;&#32473;&#23450;&#23569;&#37327;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20803;&#35757;&#32451;&#20219;&#21153;&#30340;&#25968;&#37327;&#20173;&#28982;&#38656;&#35201;&#24456;&#22823;&#65292;&#25165;&#33021;&#22312;&#20803;&#27979;&#35797;&#26399;&#38388;&#36827;&#34892;&#27867;&#21270;&#65292;&#36825;&#23545;&#20110;&#21482;&#26377;&#23569;&#37327;&#20219;&#21153;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#29942;&#39048;&#65292;&#21407;&#22240;&#21253;&#25324;&#26500;&#24314;&#20219;&#21153;&#30340;&#22256;&#38590;&#21644;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#20803;&#20114;&#25554;&#26041;&#27861;Meta-Interpolation&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning approaches enable machine learning systems to adapt to new tasks given few examples by leveraging knowledge from related tasks. However, a large number of meta-training tasks are still required for generalization to unseen tasks during meta-testing, which introduces a critical bottleneck for real-world problems that come with only few tasks, due to various reasons including the difficulty and cost of constructing tasks. Recently, several task augmentation methods have been proposed to tackle this issue using domain-specific knowledge to design augmentation techniques to densify the meta-training task distribution. However, such reliance on domain-specific knowledge renders these methods inapplicable to other domains. While Manifold Mixup based task augmentation methods are domain-agnostic, we empirically find them ineffective on non-image domains. To tackle these limitations, we propose a novel domain-agnostic task augmentation method, Meta-Interpolation, which utilizes e
&lt;/p&gt;</description></item></channel></rss>