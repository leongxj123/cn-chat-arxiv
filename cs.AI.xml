<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>IsoBench&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#19981;&#21516;&#21516;&#26500;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26356;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01266</link><description>&lt;p&gt;
IsoBench&#65306;&#22522;&#20110;&#21516;&#26500;&#34920;&#31034;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01266
&lt;/p&gt;
&lt;p&gt;
IsoBench&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#19981;&#21516;&#21516;&#26500;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26356;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#20165;&#25991;&#26412;&#25110;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#21516;&#26102;&#25552;&#31034;&#26102;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#26159;&#21542;&#20250;&#26681;&#25454;&#36755;&#20837;&#26041;&#24335;&#32780;&#25913;&#21464;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\textbf{IsoBench}$&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#30340;&#38382;&#39064;: &#25968;&#23398;&#12289;&#31185;&#23398;&#12289;&#31639;&#27861;&#21644;&#28216;&#25103;&#12290;&#27599;&#20010;&#31034;&#20363;&#21576;&#29616;&#20102;&#22810;&#20010;&#36755;&#20837;&#30340;&#21516;&#26500;&#34920;&#31034;&#65292;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#25968;&#23398;&#23637;&#31034;&#12290;IsoBench&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#21453;&#39304;&#65292;&#20197;&#35786;&#26029;&#30001;&#34920;&#31034;&#24418;&#24335;&#36896;&#25104;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#30456;&#21516;&#38382;&#39064;&#19978;&#65292;&#27169;&#22411;&#19968;&#36143;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;&#26368;&#31361;&#20986;&#30340;&#26159;&#65292;&#22312;&#25152;&#26377;IsoBench&#38382;&#39064;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;Claude-3 Opus&#22312;&#25552;&#20379;&#22270;&#20687;&#32780;&#19981;&#26159;&#25991;&#26412;&#26102;&#24615;&#33021;&#19979;&#38477;28.7&#20998;&#65307;&#21516;&#26679;&#65292;GPT-4 Turbo&#24615;&#33021;&#19979;&#38477;18.7&#20998;&#65292;Gemini Pro&#19979;&#38477;14.9&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01266v1 Announce Type: new  Abstract: Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#28041;&#21450;&#21040;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#31561;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.20329</link><description>&lt;p&gt;
ReALM: &#21442;&#32771;&#35299;&#26512;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
ReALM: Reference Resolution As Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#28041;&#21450;&#21040;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#31561;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#32771;&#35299;&#26512;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#29702;&#35299;&#21644;&#25104;&#21151;&#22788;&#29702;&#21508;&#31181;&#19978;&#19979;&#25991;&#33267;&#20851;&#37325;&#35201;&#12290; &#36825;&#31181;&#19978;&#19979;&#25991;&#26082;&#21253;&#25324;&#20808;&#21069;&#30340;&#23545;&#35805;&#65292;&#20063;&#21253;&#25324;&#19982;&#38750;&#23545;&#35805;&#23454;&#20307;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#29992;&#25143;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#25110;&#21518;&#21488;&#36816;&#34892;&#30340;&#23454;&#20307;&#12290; &#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#20204;&#22312;&#21442;&#32771;&#35299;&#26512;&#20013;&#30340;&#36816;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#23545;&#35805;&#23454;&#20307;&#65292;&#20173;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290; &#26412;&#25991;&#23637;&#31034;&#20102;LLMs&#22914;&#20309;&#34987;&#29992;&#26469;&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#20854;&#20013;&#28041;&#21450;&#23631;&#24149;&#19978;&#30340;&#36825;&#31181;&#23454;&#20307;&#31561;&#20256;&#32479;&#19978;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#21442;&#32771;&#35299;&#26512;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#31867;&#20284;&#21151;&#33021;&#30340;&#31995;&#32479;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20329v1 Announce Type: cross  Abstract: Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19024</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#38750;&#23545;&#31216;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#31574;&#30053;&#35757;&#32451;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#19968;&#20010;&#24120;&#29992;&#30340;&#31616;&#21270;&#20551;&#35774;&#26159;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#37117;&#34920;&#29616;&#20986;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#22870;&#21169;&#27169;&#22411;&#29420;&#31435;&#30340;&#23545;&#31216;&#24615;&#65306;&#22870;&#21169;&#21487;&#33021;&#19981;&#28385;&#36275;&#19982;&#21160;&#21147;&#23398;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21482;&#20551;&#23450;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#23545;&#31216;&#24615;&#30340;&#24773;&#20917;&#65292;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#23398;&#20064;&#20013;&#21487;&#24212;&#29992;&#23545;&#31216;&#25216;&#26415;&#30340;&#38382;&#39064;&#33539;&#22260;&#12290;&#25105;&#20204;&#21033;&#29992;&#21345;&#22612;&#24681;&#31227;&#21160;&#26694;&#26550;&#26041;&#27861;&#24341;&#20837;&#19968;&#31181;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#36896;&#65292;&#36825;&#31181;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#25351;&#23450;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#21040;&#20102;&#26356;&#20934;&#30830;&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19024v1 Announce Type: cross  Abstract: Recent work in reinforcement learning has leveraged symmetries in the model to improve sample efficiency in training a policy. A commonly used simplifying assumption is that the dynamics and reward both exhibit the same symmetry. However, in many real-world environments, the dynamical model exhibits symmetry independent of the reward model: the reward may not satisfy the same symmetries as the dynamics. In this paper, we investigate scenarios where only the dynamics are assumed to exhibit symmetry, extending the scope of problems in reinforcement learning and learning in control theory where symmetry techniques can be applied. We use Cartan's moving frame method to introduce a technique for learning dynamics which, by construction, exhibit specified symmetries. We demonstrate through numerical experiments that the proposed method learns a more accurate dynamical model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#24037;&#20855;&#22312;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#33021;&#32791;&#24314;&#27169;&#30340;&#22256;&#38590;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;</title><link>https://arxiv.org/abs/2403.17632</link><description>&lt;p&gt;
&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#33021;&#32791;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17632
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#24037;&#20855;&#22312;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#33021;&#32791;&#24314;&#27169;&#30340;&#22256;&#38590;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#25317;&#22581;&#21644;&#29615;&#22659;&#24694;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#26085;&#30410;&#21152;&#21095;&#65292;&#20984;&#26174;&#20102;&#22312;&#22478;&#24066;&#31354;&#38388;&#25512;&#34892;E-Mobility&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;E-&#28369;&#26495;&#36710;&#21644;E-&#33258;&#34892;&#36710;&#31561;&#24494;&#22411;E-Mobility&#24037;&#20855;&#22312;&#36825;&#19968;&#36716;&#21464;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20026;&#22478;&#24066;&#36890;&#21220;&#32773;&#25552;&#20379;&#21487;&#25345;&#32493;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#30340;&#33021;&#32791;&#27169;&#24335;&#26159;&#24433;&#21709;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#23545;&#20110;&#20986;&#34892;&#35268;&#21010;&#20197;&#21450;&#22686;&#24378;&#29992;&#25143;&#22312;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#26102;&#30340;&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#38024;&#23545;&#29305;&#23450;&#31227;&#21160;&#24037;&#20855;&#21644;&#26465;&#20214;&#23450;&#21046;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#24443;&#24213;&#27169;&#22411;&#35780;&#20272;&#21644;&#39564;&#35777;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29233;&#23572;&#20848;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#33021;&#32791;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17632v1 Announce Type: new  Abstract: The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for ene
&lt;/p&gt;</description></item><item><title>LaRE^2 &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#21644;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#21644;&#22686;&#24378;&#65292;&#20174;&#32780;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.17465</link><description>&lt;p&gt;
LaRE^2: &#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#25193;&#25955;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17465
&lt;/p&gt;
&lt;p&gt;
LaRE^2 &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#21644;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#21644;&#22686;&#24378;&#65292;&#20174;&#32780;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17465v1 &#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#20351;&#30495;&#23454;&#22270;&#20687;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#21306;&#20998;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#23613;&#31649;&#36825;&#19968;&#36827;&#23637;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#26041;&#27861;&#65288;LaRE^2&#65289;&#26469;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#65292;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#29305;&#24449;&#12290;LaRE&#22312;&#29305;&#24449;&#25552;&#21462;&#25928;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21306;&#20998;&#30495;&#20551;&#25152;&#38656;&#30340;&#20851;&#38190;&#32447;&#32034;&#12290;&#20026;&#20102;&#21033;&#29992;LaRE&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;LaRE&#24341;&#23548;&#30340;&#26041;&#24335;&#32454;&#21270;&#22270;&#20687;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17465v1 Announce Type: cross  Abstract: The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;MatchSeg&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#21644;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#22686;&#24378;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.15901</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#32771;&#22270;&#20687;&#21305;&#37197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#21106;&#65306;MatchSeg
&lt;/p&gt;
&lt;p&gt;
MatchSeg: Towards Better Segmentation via Reference Image Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15901
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;MatchSeg&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#21644;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#22686;&#24378;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#39640;&#26114;&#19988;&#32791;&#26102;&#12290;Few-shot learning&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#25903;&#25345;&#38598;&#65289;&#26469;&#25351;&#23548;&#39044;&#27979;&#26032;&#30340;&#12289;&#26410;&#26631;&#35760;&#22270;&#20687;&#65288;&#31216;&#20026;&#26597;&#35810;&#38598;&#65289;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#20811;&#26381;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#21463;&#21040;&#36825;&#19968;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MatchSeg&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25112;&#30053;&#24615;&#21442;&#32771;&#22270;&#20687;&#21305;&#37197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#22312;&#23450;&#20041;&#25903;&#25345;&#38598;&#26102;&#36873;&#25321;&#39640;&#24230;&#30456;&#20851;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#21152;&#24378;&#25903;&#25345;&#21644;&#26597;&#35810;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15901v1 Announce Type: new  Abstract: Recently, automated medical image segmentation methods based on deep learning have achieved great success. However, they heavily rely on large annotated datasets, which are costly and time-consuming to acquire. Few-shot learning aims to overcome the need for annotated data by using a small labeled dataset, known as a support set, to guide predicting labels for new, unlabeled images, known as the query set. Inspired by this paradigm, we introduce MatchSeg, a novel framework that enhances medical image segmentation through strategic reference image matching. We leverage contrastive language-image pre-training (CLIP) to select highly relevant samples when defining the support set. Additionally, we design a joint attention module to strengthen the interaction between support and query features, facilitating a more effective knowledge transfer between support and query sets. We validated our method across four public datasets. Experimental re
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#30340;&#30693;&#35782;&#20256;&#36755;&#32473;&#23458;&#25143;&#31471;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#20256;&#39640;&#25928;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#26041;&#26696;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15760</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23558;&#26381;&#21153;&#22120;&#31471;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;&#32473;&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#23458;&#25143;&#31471;&#30340;&#19978;&#20256;&#39640;&#25928;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15760
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#30340;&#30693;&#35782;&#20256;&#36755;&#32473;&#23458;&#25143;&#31471;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#20256;&#39640;&#25928;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#26041;&#26696;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#65288;HtFL&#65289;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#20256;&#39640;&#25928;&#30340;&#30693;&#35782;&#20256;&#36755;&#26041;&#26696;&#65292;&#31216;&#20026;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#24490;&#29615;&#65288;FedKTL&#65289;&#65292;&#20197;&#22788;&#29702;&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#12290;FedKTL&#21487;&#20197;&#36890;&#36807;&#26381;&#21153;&#22120;&#19978;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#30340;&#25512;&#29702;&#20135;&#29983;&#19982;&#23458;&#25143;&#31471;&#20219;&#21153;&#30456;&#20851;&#30340;&#21407;&#22411;&#22270;&#20687;-&#21521;&#37327;&#23545;&#12290;&#20511;&#21161;&#36825;&#20123;&#23545;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#21487;&#20197;&#36890;&#36807;&#38468;&#21152;&#30340;&#30417;&#30563;&#26412;&#22320;&#20219;&#21153;&#23558;&#26469;&#33258;&#29983;&#25104;&#22120;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#20854;&#26412;&#22320;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;CNN&#21644;ViT&#22312;&#20869;&#30340;14&#31181;&#27169;&#22411;&#19979;&#65292;&#23545;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#19978;&#20256;&#39640;&#25928;&#30340;FedKTL&#36229;&#36234;&#20102;&#19971;&#31181;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15760v1 Announce Type: new  Abstract: Heterogeneous Federated Learning (HtFL) enables collaborative learning on multiple clients with different model architectures while preserving privacy. Despite recent research progress, knowledge sharing in HtFL is still difficult due to data and model heterogeneity. To tackle this issue, we leverage the knowledge stored in pre-trained generators and propose a new upload-efficient knowledge transfer scheme called Federated Knowledge-Transfer Loop (FedKTL). Our FedKTL can produce client-task-related prototypical image-vector pairs via the generator's inference on the server. With these pairs, each client can transfer pre-existing knowledge from the generator to its local model through an additional supervised local task. We conduct extensive experiments on four datasets under two types of data heterogeneity with 14 kinds of models including CNNs and ViTs. Results show that our upload-efficient FedKTL surpasses seven state-of-the-art metho
&lt;/p&gt;</description></item><item><title>MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14624</link><description>&lt;p&gt;
MathVerse&#65306;&#24744;&#30340;&#22810;&#27169;&#24335;LLM&#26159;&#21542;&#30495;&#27491;&#30475;&#21040;&#20102;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#22270;&#34920;&#65311;
&lt;/p&gt;
&lt;p&gt;
MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14624
&lt;/p&gt;
&lt;p&gt;
MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#26410;&#20805;&#20998;&#35780;&#20272;&#21644;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#65292;&#23558;&#36807;&#22810;&#30340;&#35270;&#35273;&#20869;&#23481;&#34701;&#20837;&#25991;&#26412;&#38382;&#39064;&#20013;&#65292;&#36825;&#26377;&#21161;&#20110;MLLM&#22312;&#19981;&#30495;&#27491;&#35299;&#37322;&#36755;&#20837;&#22270;&#34920;&#30340;&#24773;&#20917;&#19979;&#25512;&#23548;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MathVerse&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#12290;&#25105;&#20204;&#31934;&#24515;&#25910;&#38598;&#20102;2,612&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#23398;&#31185;&#25968;&#23398;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#22270;&#34920;&#65292;&#26469;&#28304;&#20110;&#20844;&#24320;&#28192;&#36947;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#38382;&#39064;&#30001;&#20154;&#24037;&#27880;&#37322;&#32773;&#36716;&#21270;&#20026;&#20845;&#20010;&#19981;&#21516;&#29256;&#26412;&#65292;&#27599;&#20010;&#29256;&#26412;&#22312;&#22810;&#27169;&#24335;&#20013;&#25552;&#20379;&#19981;&#21516;&#31243;&#24230;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#20849;&#36129;&#29486;&#20102;15K&#20010;&#27979;&#35797;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;MathVerse&#33021;&#22815;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14624v1 Announce Type: cross  Abstract: The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to co
&lt;/p&gt;</description></item><item><title>&#21338;&#24328;&#35770;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#23454;&#26045;&#32593;&#32476;&#27450;&#39575;&#31574;&#30053;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#20026;&#25552;&#21319;&#20027;&#21160;&#21644;&#33258;&#21160;&#21270;&#32593;&#32476;&#38450;&#24481;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.10570</link><description>&lt;p&gt;
&#25112;&#30053;&#32593;&#32476;&#25112;&#20013;&#30340;&#29983;&#29289;&#20849;&#29983;&#28216;&#25103;&#21644;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10570
&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#23454;&#26045;&#32593;&#32476;&#27450;&#39575;&#31574;&#30053;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#20026;&#25552;&#21319;&#20027;&#21160;&#21644;&#33258;&#21160;&#21270;&#32593;&#32476;&#38450;&#24481;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#20020;&#30528;&#32593;&#32476;&#25112;&#25112;&#26415;&#30340;&#24555;&#36895;&#28436;&#21464;&#12289;&#24773;&#25253;&#19981;&#23545;&#31216;&#24615;&#22686;&#21152;&#21644;&#40657;&#23458;&#24037;&#20855;&#30340;&#26085;&#30410;&#26131;&#24471;&#65292;&#25105;&#20204;&#27491;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#32593;&#32476;&#25112;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#32593;&#32476;&#27450;&#39575;&#20316;&#20026;&#25105;&#20204;&#38450;&#24481;&#31574;&#30053;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#23853;&#38706;&#22836;&#35282;&#65292;&#26088;&#22312;&#24212;&#23545;&#26085;&#30410;&#22797;&#26434;&#30340;&#25915;&#20987;&#12290;&#26412;&#31456;&#26088;&#22312;&#24378;&#35843;&#21338;&#24328;&#35770;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#22312;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#23454;&#26045;&#32593;&#32476;&#27450;&#39575;&#31574;&#30053;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#21338;&#24328;&#27169;&#22411;&#65288;GMs&#65289;&#20316;&#20026;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#22810;&#26679;&#30340;&#23545;&#25239;&#24615;&#20132;&#20114;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21253;&#23481;&#23545;&#25239;&#24615;&#30693;&#35782;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#35265;&#35299;&#12290;&#21516;&#26102;&#65292;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21019;&#24314;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#23450;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26500;&#24314;&#22359;&#12290;&#36890;&#36807;&#21033;&#29992;&#21338;&#24328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#19981;&#20165;&#20445;&#25252;&#25105;&#20204;&#30340;&#32593;&#32476;&#20813;&#21463;&#25915;&#20987;&#65292;&#32780;&#19988;&#25552;&#39640;&#20027;&#21160;&#21644;&#33258;&#21160;&#21270;&#32593;&#32476;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10570v1 Announce Type: cross  Abstract: We are currently facing unprecedented cyber warfare with the rapid evolution of tactics, increasing asymmetry of intelligence, and the growing accessibility of hacking tools. In this landscape, cyber deception emerges as a critical component of our defense strategy against increasingly sophisticated attacks. This chapter aims to highlight the pivotal role of game-theoretic models and foundation models (FMs) in analyzing, designing, and implementing cyber deception tactics. Game models (GMs) serve as a foundational framework for modeling diverse adversarial interactions, allowing us to encapsulate both adversarial knowledge and domain-specific insights. Meanwhile, FMs serve as the building blocks for creating tailored machine learning models suited to given applications. By leveraging the synergy between GMs and FMs, we can advance proactive and automated cyber defense mechanisms by not only securing our networks against attacks but als
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;HPO&#26041;&#27861;&#65292;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#23454;&#29616;&#24555;&#36895;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.01888</link><description>&lt;p&gt;
&#38646;&#25104;&#26412;&#22522;&#20934;&#19978;&#24322;&#27493;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#30340;&#24555;&#36895;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01888
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;HPO&#26041;&#27861;&#65292;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#23454;&#29616;&#24555;&#36895;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#20854;&#32467;&#26524;&#24448;&#24448;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#30340;&#31934;&#24515;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;&#32791;&#26102;&#24615;&#20351;&#24471;&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#25302;&#24930;&#20102;&#39640;&#25928;HPO&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;&#26412;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20419;&#36827;&#38646;&#25104;&#26412;&#22522;&#20934;&#19979;&#39640;&#25928;&#30340;&#24182;&#34892;HPO&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#23384;&#20648;&#22312;&#25991;&#20214;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#35745;&#31639;&#31934;&#30830;&#30340;&#36820;&#22238;&#39034;&#24207;&#65292;&#28040;&#38500;&#20102;&#38271;&#26102;&#38388;&#30340;&#31561;&#24453;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;HPO&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01888v1 Announce Type: new  Abstract: While deep learning has celebrated many successes, its results often hinge on the meticulous selection of hyperparameters (HPs). However, the time-consuming nature of deep learning training makes HP optimization (HPO) a costly endeavor, slowing down the development of efficient HPO tools. While zero-cost benchmarks, which provide performance and runtime without actual training, offer a solution for non-parallel setups, they fall short in parallel setups as each worker must communicate its queried runtime to return its evaluation in the exact order. This work addresses this challenge by introducing a user-friendly Python package that facilitates efficient parallel HPO with zero-cost benchmarks. Our approach calculates the exact return order based on the information stored in file system, eliminating the need for long waiting times and enabling much faster HPO evaluations. We first verify the correctness of our approach through extensive t
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#20010;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#23558;&#35774;&#35745;&#32771;&#34385;&#20174;&#24444;&#27492;&#21644;&#22522;&#30784;&#27169;&#22411;&#35299;&#32806;</title><link>https://arxiv.org/abs/2403.01695</link><description>&lt;p&gt;
DyCE&#65306;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21387;&#32553;&#21644;&#25193;&#23637;&#30340;&#21160;&#24577;&#21487;&#37197;&#32622;&#36864;&#20986;
&lt;/p&gt;
&lt;p&gt;
DyCE: Dynamic Configurable Exiting for Deep Learning Compression and Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01695
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#20010;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#23558;&#35774;&#35745;&#32771;&#34385;&#20174;&#24444;&#27492;&#21644;&#22522;&#30784;&#27169;&#22411;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#38656;&#35201;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#26377;&#25928;&#37096;&#32626;&#26102;&#65292;&#20351;&#29992;&#32553;&#25918;&#21644;&#21387;&#32553;&#25216;&#26415;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#25216;&#26415;&#65292;&#22914;&#20462;&#21098;&#21644;&#37327;&#21270;&#65292;&#36890;&#24120;&#26159;&#38745;&#24577;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21160;&#24577;&#21387;&#32553;&#26041;&#27861;&#65288;&#22914;&#25552;&#21069;&#36864;&#20986;&#65289;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#26679;&#26412;&#30340;&#22256;&#38590;&#31243;&#24230;&#24182;&#26681;&#25454;&#38656;&#35201;&#20998;&#37197;&#35745;&#31639;&#26469;&#38477;&#20302;&#22797;&#26434;&#24615;&#12290;&#21160;&#24577;&#26041;&#27861;&#65292;&#23613;&#31649;&#20855;&#26377;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#19982;&#38745;&#24577;&#26041;&#27861;&#20849;&#23384;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#29616;&#19978;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#21160;&#24577;&#37096;&#20998;&#30340;&#20219;&#20309;&#21464;&#21270;&#37117;&#20250;&#24433;&#21709;&#21518;&#32493;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#21160;&#24577;&#21387;&#32553;&#35774;&#35745;&#37117;&#26159;&#21333;&#29255;&#30340;&#65292;&#19982;&#22522;&#30784;&#27169;&#22411;&#32039;&#23494;&#38598;&#25104;&#65292;&#20174;&#32780;&#20351;&#20854;&#38590;&#20197;&#36866;&#24212;&#26032;&#39062;&#22522;&#30784;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#31181;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#20174;&#32780;&#20351;&#35774;&#35745;&#32771;&#34385;&#30456;&#20114;&#35299;&#32806;&#20197;&#21450;&#19982;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01695v1 Announce Type: cross  Abstract: Modern deep learning (DL) models necessitate the employment of scaling and compression techniques for effective deployment in resource-constrained environments. Most existing techniques, such as pruning and quantization are generally static. On the other hand, dynamic compression methods, such as early exits, reduce complexity by recognizing the difficulty of input samples and allocating computation as needed. Dynamic methods, despite their superior flexibility and potential for co-existing with static methods, pose significant challenges in terms of implementation due to any changes in dynamic parts will influence subsequent processes. Moreover, most current dynamic compression designs are monolithic and tightly integrated with base models, thereby complicating the adaptation to novel base models. This paper introduces DyCE, an dynamic configurable early-exit framework that decouples design considerations from each other and from the 
&lt;/p&gt;</description></item><item><title>PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.00929</link><description>&lt;p&gt;
&#21033;&#29992;&#34892;&#20026;&#21407;&#35821;&#25645;&#24314;&#20219;&#21153;&#30340;&#26694;&#26550;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00929
&lt;/p&gt;
&lt;p&gt;
PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#35753;&#26426;&#22120;&#20154;&#23398;&#20250;&#22797;&#26434;&#30340;&#25805;&#20316;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#21463;&#21040;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#22797;&#21512;&#35823;&#24046;&#20250;&#22312;&#20219;&#21153;&#26102;&#27573;&#20869;&#32047;&#31215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRIME&#65288;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#25968;&#25454;&#25928;&#29575;&#27169;&#20223;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;PRIME&#36890;&#36807;&#23558;&#20219;&#21153;&#28436;&#31034;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#26469;&#25645;&#24314;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#23398;&#20064;&#19968;&#20010;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#26469;&#23545;&#21407;&#35821;&#24207;&#21015;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PRIME&#22312;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#29575;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#39640;&#20986;10-34&#65285;&#65292;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#39640;&#20986;20-48&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00929v1 Announce Type: cross  Abstract: Imitation learning has shown great potential for enabling robots to acquire complex manipulation behaviors. However, these algorithms suffer from high sample complexity in long-horizon tasks, where compounding errors accumulate over the task horizons. We present PRIME (PRimitive-based IMitation with data Efficiency), a behavior primitive-based framework designed for improving the data efficiency of imitation learning. PRIME scaffolds robot tasks by decomposing task demonstrations into primitive sequences, followed by learning a high-level control policy to sequence primitives through imitation learning. Our experiments demonstrate that PRIME achieves a significant performance improvement in multi-stage manipulation tasks, with 10-34% higher success rates in simulation over state-of-the-art baselines and 20-48% on physical hardware.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;SAR&#22270;&#20687;&#20013;&#33337;&#21482;&#26816;&#27979;&#30340;Wilcoxon&#38750;&#21442;&#25968;CFAR&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#24050;&#30693;&#26434;&#27874;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#32500;&#25345;&#30446;&#26631;&#26816;&#27979;&#30340;&#24658;&#23450;&#34394;&#35686;&#29575;</title><link>https://arxiv.org/abs/2402.18579</link><description>&lt;p&gt;
SAR&#22270;&#20687;&#20013;&#29992;&#20110;&#33337;&#21482;&#26816;&#27979;&#30340;Wilcoxon&#38750;&#21442;&#25968;&#21270;CFAR&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Wilcoxon Nonparametric CFAR Scheme for Ship Detection in SAR Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;SAR&#22270;&#20687;&#20013;&#33337;&#21482;&#26816;&#27979;&#30340;Wilcoxon&#38750;&#21442;&#25968;CFAR&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#24050;&#30693;&#26434;&#27874;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#32500;&#25345;&#30446;&#26631;&#26816;&#27979;&#30340;&#24658;&#23450;&#34394;&#35686;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#25968;&#34394;&#35686;&#29575;&#65288;CFAR&#65289;&#26816;&#27979;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#30446;&#21069;SAR&#22270;&#20687;&#20013;&#26816;&#27979;&#33337;&#21482;&#30446;&#26631;&#65292;&#36825;&#20123;&#31639;&#27861;&#22522;&#20110;&#21508;&#31181;&#32479;&#35745;&#20998;&#24067;&#65292;&#22914;&#39640;&#26031;&#20998;&#24067;&#12289;Gamma&#20998;&#24067;&#12289;Weibull&#20998;&#24067;&#12289;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#12289;G0&#20998;&#24067;&#12289;alpha&#31283;&#23450;&#20998;&#24067;&#31561;&#12290;&#28982;&#32780;&#65292;SAR&#22270;&#20687;&#20013;&#30340;&#26434;&#25955;&#32972;&#26223;&#22797;&#26434;&#22810;&#21464;&#12290;&#24403;&#23454;&#38469;&#26434;&#25955;&#32972;&#26223;&#20559;&#31163;&#20551;&#23450;&#30340;&#32479;&#35745;&#20998;&#24067;&#26102;&#65292;&#21442;&#25968;&#21270;CFAR&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#23558;&#19979;&#38477;&#12290;&#38500;&#20102;&#21442;&#25968;&#21270;CFAR&#26041;&#26696;&#65292;&#36824;&#26377;&#21478;&#19968;&#31867;&#38750;&#21442;&#25968;&#21270;CFAR&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#24050;&#30693;&#26434;&#27874;&#20998;&#24067;&#30340;&#20551;&#35774;&#24773;&#20917;&#19979;&#20445;&#25345;&#30446;&#26631;&#26816;&#27979;&#30340;&#24658;&#23450;&#34394;&#35686;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;SAR&#22270;&#20687;&#20013;&#33337;&#21482;&#26816;&#27979;&#30340;Wilcoxon&#38750;&#21442;&#25968;&#21270;CFAR&#26041;&#26696;&#65292;&#24182;&#25512;&#23548;&#20102;Wilcoxon&#38750;&#21442;&#25968;&#26816;&#27979;&#22120;&#30340;&#34394;&#35686;&#29575;&#30340;&#23553;&#38381;&#24418;&#24335;&#20197;&#30830;&#23450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18579v1 Announce Type: cross  Abstract: The parametric constant false alarm rate (CFAR) detection algorithms which are based on various statistical distributions, such as Gaussian, Gamma, Weibull, log-normal, G0 distribution, alpha-stable distribution, etc, are most widely used to detect the ship targets in SAR image at present. However, the clutter background in SAR images is complicated and variable. When the actual clutter background deviates from the assumed statistical distribution, the performance of the parametric CFAR detector will deteriorate. In addition to the parametric CFAR schemes, there is another class of nonparametric CFAR detectors which can maintain a constant false alarm rate for the target detection without the assumption of a known clutter distribution. In this work, the Wilcoxon nonparametric CFAR scheme for ship detection in SAR image is proposed and analyzed, and a closed form of the false alarm rate for the Wilcoxon nonparametric detector to determi
&lt;/p&gt;</description></item><item><title>WeakSAM&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#23398;&#20064;&#30340;&#20840;&#29699;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;PGT&#29983;&#25104;&#21644;RoI&#20002;&#24323;&#27491;&#21017;&#21270;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14812</link><description>&lt;p&gt;
WeakSAM: &#20219;&#24847;&#20998;&#21106;&#36935;&#19978;&#24369;&#30417;&#30563;&#23454;&#20363;&#32423;&#21035;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14812
&lt;/p&gt;
&lt;p&gt;
WeakSAM&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#23398;&#20064;&#30340;&#20840;&#29699;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;PGT&#29983;&#25104;&#21644;RoI&#20002;&#24323;&#27491;&#21017;&#21270;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30340;&#35270;&#35273;&#35782;&#21035;&#20351;&#29992;&#19981;&#31934;&#30830;&#30340;&#30417;&#30563;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#23427;&#26174;&#33879;&#38477;&#20302;&#20102;&#20154;&#24037;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#19988;&#20256;&#32479;&#19978;&#20381;&#36182;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;WeakSAM&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;&#22312;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#39044;&#20808;&#23398;&#20064;&#30340;&#20840;&#29699;&#30693;&#35782;&#65292;&#21363;Segment Anything Model (SAM)&#65292;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#29289;&#20307;&#26816;&#27979;&#65288;WSOD&#65289;&#21644;&#20998;&#21106;&#12290;WeakSAM&#36890;&#36807;&#33258;&#36866;&#24212;PGT&#29983;&#25104;&#21644;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;RoI&#65289;&#20002;&#24323;&#27491;&#21017;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;WSOD&#37325;&#26032;&#35757;&#32451;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#21363;&#20266;&#26631;&#20934;&#22320;&#38754;&#30495;&#30456;&#65288;PGT&#65289;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#20855;&#26377;&#22024;&#26434;PGT&#23454;&#20363;&#12290;&#23427;&#36824;&#35299;&#20915;&#20102;SAM&#22312;&#33258;&#21160;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#26102;&#38656;&#35201;&#25552;&#31034;&#21644;&#31867;&#21035;&#26080;&#24863;&#30693;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;WeakSAM&#22312;WSOD&#21644;WSIS&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14812v1 Announce Type: cross  Abstract: Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling. This paper introduces WeakSAM and solves the weakly-supervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM's problems of requiring prompts and category unawareness for automatic object detection and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with larg
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#12289;&#29702;&#35299;&#35268;&#21017;&#21644;&#27861;&#21017;&#12289;&#25552;&#20379;&#35821;&#22659;&#31561;&#26041;&#24335;&#25552;&#39640;&#33258;&#21160;&#39542;&#39542;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13602</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#25512;&#29702;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13602
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#12289;&#29702;&#35299;&#35268;&#21017;&#21644;&#27861;&#21017;&#12289;&#25552;&#20379;&#35821;&#22659;&#31561;&#26041;&#24335;&#25552;&#39640;&#33258;&#21160;&#39542;&#39542;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#20687;&#12289;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#20197;&#21450;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23558;&#36825;&#31181;&#39640;&#32423;&#25512;&#29702;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30456;&#32467;&#21512;&#20197;&#29992;&#20110;&#21160;&#24577;&#24773;&#20917;&#19979;&#30340;&#20915;&#31574;&#30340;&#27867;&#21270;&#33021;&#21147;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#28151;&#21512;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#24212;&#29992;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#20551;&#35774;LLMs&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20351;&#23427;&#20204;&#20998;&#26512;&#26816;&#27979;&#21040;&#30340;&#29289;&#20307;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#29702;&#35299;&#39550;&#39542;&#35268;&#23450;&#21644;&#29289;&#29702;&#27861;&#21017;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#35821;&#22659;&#26469;&#25913;&#21892;&#33258;&#21160;&#39550;&#39542;&#12290;&#36825;&#35299;&#20915;&#20102;&#22797;&#26434;&#24773;&#26223;&#65292;&#22914;&#20302;&#33021;&#35265;&#24230;&#65288;&#30001;&#20110;&#22825;&#27668;&#26465;&#20214;&#65289;&#19979;&#30340;&#20915;&#31574;&#65292;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#19981;&#36275;&#20197;&#32988;&#20219;&#12290;&#25105;&#20204;&#36890;&#36807;&#20934;&#30830;&#24615;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36825;&#31181;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13602v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by co
&lt;/p&gt;</description></item><item><title>G-SciEdBERT&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#24503;&#35821;&#31185;&#23398;&#25945;&#32946;BERT&#65292;&#29992;&#20110;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#20219;&#21153;&#30340;&#20070;&#38754;&#22238;&#31572;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#24503;&#35821;&#31185;&#23398;&#22238;&#31572;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#35780;&#20998;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;10%&#30340;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.06584</link><description>&lt;p&gt;
G-SciEdBERT: &#29992;&#20110;&#24503;&#35821;&#31185;&#23398;&#35780;&#20272;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06584
&lt;/p&gt;
&lt;p&gt;
G-SciEdBERT&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#24503;&#35821;&#31185;&#23398;&#25945;&#32946;BERT&#65292;&#29992;&#20110;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#20219;&#21153;&#30340;&#20070;&#38754;&#22238;&#31572;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#24503;&#35821;&#31185;&#23398;&#22238;&#31572;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#35780;&#20998;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;10%&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#20026;&#21508;&#31181;&#35821;&#35328;&#65288;&#20363;&#22914;&#24503;&#35821;&#20013;&#30340;&#24503;&#35821;BERT [G-BERT]&#65289;&#30340;&#33258;&#21160;&#35780;&#20998;&#31995;&#32479;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#33258;&#21160;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#38382;&#39064;&#30340;&#20070;&#38754;&#22238;&#31572;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26631;&#20934;&#30340;G-BERT&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#31185;&#23398;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#33021;&#19982;&#23398;&#29983;&#30340;&#20889;&#20316;&#39118;&#26684;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#24503;&#35821;&#31185;&#23398;&#25945;&#32946;BERT&#65288;G-SciEdBERT&#65289;&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#20219;&#21153;&#30340;&#20070;&#38754;&#22238;&#31572;&#12290;&#25105;&#20204;&#20351;&#29992;G-BERT&#65292;&#22312;5M&#20010;&#26631;&#35760;&#30340;PISA 2015&#22269;&#38469;&#23398;&#29983;&#35780;&#20272;&#30340;50K&#20010;&#24503;&#35821;&#31185;&#23398;&#22238;&#31572;&#35821;&#26009;&#24211;&#19978;&#23545;G-SciEdBERT&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;59&#20010;&#35780;&#20272;&#39033;&#30446;&#19978;&#23545;G-SciEdBERT&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#26816;&#26597;&#20102;&#35780;&#20998;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24615;&#33021;&#19982;G-BERT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;G-SciEdBERT&#22312;&#35780;&#20998;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#34920;&#26126;&#20854;&#35780;&#20998;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of natural language processing has paved the way for automated scoring systems in various languages, such as German (e.g., German BERT [G-BERT]). Automatically scoring written responses to science questions in German is a complex task and challenging for standard G-BERT as they lack contextual knowledge in the science domain and may be unaligned with student writing styles. This paper developed a contextualized German Science Education BERT (G-SciEdBERT), an innovative large language model tailored for scoring German-written responses to science tasks. Using G-BERT, we pre-trained G-SciEdBERT on a corpus of 50K German written science responses with 5M tokens to the Programme for International Student Assessment (PISA) 2015. We fine-tuned G-SciEdBERT on 59 assessment items and examined the scoring accuracy. We then compared its performance with G-BERT. Our findings reveal a substantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a 10% increase of quad
&lt;/p&gt;</description></item><item><title>CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2402.05374</link><description>&lt;p&gt;
CIC&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CIC: A framework for Culturally-aware Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05374
&lt;/p&gt;
&lt;p&gt;
CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VLPs&#65289;&#22914;BLIP&#20174;&#22270;&#20687;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#25152;&#25551;&#32472;&#30340;&#25991;&#21270;&#20803;&#32032;&#65288;&#20363;&#22914;&#20122;&#27954;&#25991;&#21270;&#32676;&#20307;&#30340;&#20256;&#32479;&#26381;&#35013;&#65289;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#24615;&#23383;&#24149;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;\textbf{&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#65288;CIC&#65289;}&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#20195;&#34920;&#19981;&#21516;&#25991;&#21270;&#30340;&#22270;&#20687;&#20013;&#29983;&#25104;&#23383;&#24149;&#24182;&#25551;&#36848;&#25991;&#21270;&#20803;&#32032;&#12290;&#21463;&#21040;&#23558;&#35270;&#35273;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#32452;&#21512;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#65288;1&#65289;&#26681;&#25454;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#38382;&#39064;&#20174;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#25552;&#21462;&#25991;&#21270;&#35270;&#35273;&#20803;&#32032;&#65292;&#65288;3&#65289;&#20351;&#29992;&#24102;&#26377;&#25552;&#31034;&#30340;LLMs&#29983;&#25104;&#25991;&#21270;&#24863;&#30693;&#23383;&#24149;&#12290;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#22823;&#23398;&#30340;45&#21517;&#21442;&#19982;&#32773;&#19978;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;UniMem&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#35760;&#24518;&#22686;&#24378;&#30340;&#35282;&#24230;&#37325;&#26032;&#21046;&#23450;&#20102;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;UniMix&#26469;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03009</link><description>&lt;p&gt;
UniMem&#65306;&#36808;&#21521;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
UniMem: Towards a Unified View of Long-Context Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;UniMem&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#35760;&#24518;&#22686;&#24378;&#30340;&#35282;&#24230;&#37325;&#26032;&#21046;&#23450;&#20102;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;UniMix&#26469;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#26159;&#38480;&#21046;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#33021;&#21147;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#34429;&#28982;&#23384;&#22312;&#21508;&#31181;&#33268;&#21147;&#20110;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#26159;&#23396;&#31435;&#22320;&#24320;&#21457;&#30340;&#65292;&#32570;&#20047;&#23545;&#23427;&#20204;&#30340;&#20248;&#28857;&#30340;&#31995;&#32479;&#20998;&#26512;&#21644;&#25972;&#21512;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UniMem&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20174;LLM&#30340;&#35760;&#24518;&#22686;&#24378;&#30340;&#35282;&#24230;&#37325;&#26032;&#21046;&#23450;&#20102;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#12290; UniMem&#30340;&#29305;&#28857;&#26159;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#20869;&#23384;&#31649;&#29702;&#65292;&#20869;&#23384;&#20889;&#20837;&#65292;&#20869;&#23384;&#35835;&#21462;&#21644;&#20869;&#23384;&#27880;&#20837;&#65292;&#20026;&#20102;&#29702;&#35299;&#21508;&#31181;&#38271;&#19978;&#19979;&#25991;&#26041;&#27861;&#25552;&#20379;&#20102;&#31995;&#32479;&#29702;&#35770;&#12290;&#25105;&#20204;&#22522;&#20110;UniMem&#37325;&#26032;&#21046;&#23450;&#20102;16&#31181;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;Transformer-XL&#65292;&#35760;&#24518;&#21270;Transformer&#65292;RMT&#21644;Longformer&#20013;&#30340;&#22235;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#23427;&#20204;&#30340;&#35774;&#35745;&#21407;&#21017;&#21644;&#20248;&#21183;&#12290;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UniMix&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-context processing is a critical ability that constrains the applicability of large language models. Although there exist various methods devoted to enhancing the long-context processing ability of large language models (LLMs), they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a unified framework that reformulates existing long-context methods from the view of memory augmentation of LLMs. UniMem is characterized by four key dimensions: Memory Management, Memory Writing, Memory Reading, and Memory Injection, providing a systematic theory for understanding various long-context methods. We reformulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an inn
&lt;/p&gt;</description></item><item><title>Bergeron&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#40774;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#21442;&#25968;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2312.00029</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#20934;&#26694;&#26550;&#25269;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00029
&lt;/p&gt;
&lt;p&gt;
Bergeron&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#40774;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#21442;&#25968;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#36827;&#23637;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#20195;&#23545;&#40784;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#23436;&#20840;&#38450;&#27490;&#22312;&#27169;&#22411;&#34987;&#33988;&#24847;&#25915;&#20987;&#26102;&#20135;&#29983;&#26377;&#23475;&#24212;&#23545;&#12290;&#20026;&#20102;&#24110;&#21161;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Bergeron&#65306;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;LLMs&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;&#65292;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#21442;&#25968;&#24494;&#35843;&#12290;Bergeron&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#65307;&#27425;&#35201;LLM&#27169;&#25311;&#21463;&#20445;&#25252;&#30340;&#20027;&#35201;LLM&#30340;&#33391;&#30693;&#12290;&#35813;&#26694;&#26550;&#22312;&#30417;&#35270;&#36755;&#20986;&#20197;&#26816;&#27979;&#20219;&#20309;&#26377;&#23475;&#20869;&#23481;&#30340;&#21516;&#26102;&#65292;&#26356;&#22909;&#22320;&#20445;&#25252;&#20027;&#35201;&#27169;&#22411;&#20813;&#21463;&#20837;&#20405;&#25915;&#20987;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#20351;&#29992;Bergeron&#26469;&#34917;&#20805;&#29616;&#26377;&#23545;&#40784;&#35757;&#32451;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00029v2 Announce Type: replace-cross  Abstract: Research into AI alignment has grown considerably since the recent introduction of increasingly capable Large Language Models (LLMs). Unfortunately, modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked. These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts. To help mitigate this issue, we introduce Bergeron: a framework designed to improve the robustness of LLMs against attacks without any additional parameter fine-tuning. Bergeron is organized into two tiers; with a secondary LLM emulating the conscience of a protected, primary LLM. This framework better safeguards the primary model against incoming attacks while monitoring its output for any harmful content. Empirical analysis shows that, by using Bergeron to complement models with existing alignment traini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.09101</link><description>&lt;p&gt;
&#26397;&#21521;&#22810;&#27493;&#25512;&#29702;&#30340;&#31572;&#26696;&#26657;&#20934;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified View of Answer Calibration for Multi-Step Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#25193;&#23637;&#20102;&#25913;&#36827;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#24120;&#23558;&#22810;&#27493;&#25512;&#29702;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#36335;&#24452;&#29983;&#25104;&#20197;&#29983;&#25104;&#25512;&#29702;&#36335;&#24452;&#65307;&#21644;&#31572;&#26696;&#26657;&#20934;&#21518;&#22788;&#29702;&#25512;&#29702;&#36335;&#24452;&#20197;&#33719;&#24471;&#26368;&#32456;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#32570;&#20047;&#23545;&#19981;&#21516;&#31572;&#26696;&#26657;&#20934;&#26041;&#27861;&#30340;&#31995;&#32479;&#20998;&#26512;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#22810;&#36335;&#24452;&#19978;&#30340;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21487;&#33021;&#21551;&#31034;&#20248;&#21270;&#22810;&#27493;&#25512;&#29702;&#31995;&#32479;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09101v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: path generation to generate the reasoning path(s); and answer calibration post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for opti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2305.13168</link><description>&lt;p&gt;
LLMs&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#65306;&#26368;&#26032;&#21151;&#33021;&#19982;&#26410;&#26469;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26500;&#24314;&#21644;&#25512;&#29702;&#20013;&#30340;&#25968;&#37327;&#21270;&#21644;&#36136;&#21270;&#35780;&#20272;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#37325;&#28857;&#20851;&#27880;&#28085;&#30422;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#38382;&#31572;&#22235;&#20010;&#20856;&#22411;&#20219;&#21153;&#65292;&#20174;&#32780;&#20840;&#38754;&#25506;&#32034;&#20102;LLMs&#22312;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#32463;&#39564;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;GPT-4&#20026;&#20195;&#34920;&#30340;LLMs&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#32780;&#19981;&#26159;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#34429;&#28982;GPT-4&#22312;&#19982;KG&#26500;&#24314;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#36824;&#25193;&#23637;&#21040;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#34394;&#25311;&#30693;&#35782;&#25552;&#21462;&#30340;&#26500;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#30340;&#26041;&#24335;&#65292;&#23558;&#30693;&#35782;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13444</link><description>&lt;p&gt;
&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65306;&#22522;&#20110;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption. (arXiv:2401.13444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#30340;&#26041;&#24335;&#65292;&#23558;&#30693;&#35782;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26356;&#26032;&#23427;&#20204;&#30340;&#30693;&#35782;&#38754;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#24403;&#38754;&#23545;&#19981;&#29087;&#24713;&#30340;&#26597;&#35810;&#26102;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#20102;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;LLMs&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23558;LLMs&#35270;&#20026;&#20027;&#35201;&#30340;&#20915;&#31574;&#32773;&#65292;&#23545;&#20854;&#33021;&#21147;&#25552;&#20986;&#20102;&#36739;&#39640;&#30340;&#35201;&#27714;&#12290;&#23545;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#19988;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#30340;LLMs&#26469;&#35828;&#65292;&#36825;&#26159;&#19981;&#22826;&#21512;&#36866;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#20026;&#26680;&#24515;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65288;CGPE&#65289;&#65292;&#23427;&#23558;&#30693;&#35782;&#24211;&#19982;LLMs&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#23545;&#27169;&#22411;&#30340;&#33021;&#21147;&#35201;&#27714;&#36739;&#20302;&#12290;&#21463;&#20154;&#31867;&#25163;&#21160;&#26816;&#32034;&#30693;&#35782;&#30340;&#26041;&#27861;&#21551;&#21457;&#65292;CGPE&#21033;&#29992;&#38382;&#39064;&#20013;&#30340;&#20449;&#24687;&#20316;&#20026;&#32447;&#32034;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#30693;&#35782;&#24211;&#20013;&#25152;&#38656;&#30340;&#30693;&#35782;&#36335;&#24452;&#12290;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CGPE&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#29992;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#19988;&#24615;&#33021;&#36739;&#24046;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, large language models (LLMs) have showcased remarkable capabilities. However, updating their knowledge poses challenges, potentially leading to inaccuracies when confronted with unfamiliar queries. While integrating knowledge graphs with LLMs has been explored, existing approaches treat LLMs as primary decision-makers, imposing high demands on their capabilities. This is particularly unsuitable for LLMs with lower computational costs and relatively poorer performance. In this paper, we introduce a Clue-Guided Path Exploration framework (CGPE) that efficiently merges a knowledge base with an LLM, placing less stringent requirements on the model's capabilities. Inspired by the method humans use to manually retrieve knowledge, CGPE employs information from the question as clues to systematically explore the required knowledge path within the knowledge base. Experiments on open-source datasets reveal that CGPE outperforms previous methods and is highly applicable to LLMs w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;SCRL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#28789;&#24039;&#30340;&#25163;&#25429;&#25417;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#12290;&#35813;&#31639;&#27861;&#22312;&#22522;&#32447;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08809</link><description>&lt;p&gt;
DexCatch: &#23398;&#20064;&#29992;&#28789;&#24039;&#30340;&#25163;&#25429;&#25417;&#20219;&#24847;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands. (arXiv:2310.08809v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;SCRL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#28789;&#24039;&#30340;&#25163;&#25429;&#25417;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#12290;&#35813;&#31639;&#27861;&#22312;&#22522;&#32447;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#23454;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#28789;&#24039;&#25805;&#32437;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#25343;&#21462;&#21644;&#25918;&#32622;&#20219;&#21153;&#30340;&#25104;&#21151;&#29575;&#19978;&#12290;&#19982;&#25343;&#21462;&#21644;&#25918;&#32622;&#30456;&#27604;&#65292;&#25243;&#25509;&#34892;&#20026;&#26377;&#28508;&#21147;&#22312;&#26080;&#38656;&#23558;&#29289;&#20307;&#36816;&#36865;&#21040;&#30446;&#30340;&#22320;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#25343;&#21462;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#21160;&#24577;&#30340;&#28789;&#24039;&#25805;&#32437;&#30001;&#20110;&#22823;&#37327;&#30340;&#21160;&#24577;&#25509;&#35302;&#32780;&#38754;&#20020;&#30528;&#31283;&#23450;&#25511;&#21046;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#65288;SCRL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#28789;&#24039;&#30340;&#25163;&#25429;&#25417;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#12290;&#35813;&#31639;&#27861;&#22312;&#22522;&#32447;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#22312;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#36801;&#31227;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#25163;&#20013;&#30340;&#29289;&#20307;&#38754;&#21521;&#20391;&#38754;&#38750;&#24120;&#19981;&#31283;&#23450;&#65292;&#30001;&#20110;&#32570;&#20047;&#26469;&#33258;&#25163;&#25484;&#30340;&#25903;&#25745;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#28982;&#21487;&#20197;&#22312;&#26368;&#20855;&#25361;&#25112;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#34892;&#20026;&#30340;&#35270;&#39057;&#28436;&#31034;&#21644;&#21512;&#20316;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving human-like dexterous manipulation remains a crucial area of research in robotics. Current research focuses on improving the success rate of pick-and-place tasks. Compared with pick-and-place, throw-catching behavior has the potential to increase picking speed without transporting objects to their destination. However, dynamic dexterous manipulation poses a major challenge for stable control due to a large number of dynamic contacts. In this paper, we propose a Stability-Constrained Reinforcement Learning (SCRL) algorithm to learn to catch diverse objects with dexterous hands. The SCRL algorithm outperforms baselines by a large margin, and the learned policies show strong zero-shot transfer performance on unseen objects. Remarkably, even though the object in a hand facing sideward is extremely unstable due to the lack of support from the palm, our method can still achieve a high level of success in the most challenging task. Video demonstrations of learned behaviors and the co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03708</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#35270;&#21516;&#20161;&#65306;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#19982;&#26222;&#36890;&#26631;&#35760;&#32773;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#21487;&#33021;&#19981;&#36866;&#24212;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26041;&#27861;&#36873;&#25321;&#36890;&#36807;&#25910;&#38598;&#22810;&#32500;&#24230;&#21453;&#39304;&#24182;&#20026;&#27599;&#20010;&#32500;&#24230;&#21019;&#24314;&#19981;&#21516;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#26377;&#30410;&#24615;&#65292;&#26080;&#23475;&#24615;&#65292;&#35802;&#23454;&#24615;&#65289;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#22870;&#21169;&#26435;&#37325;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#23558;LM&#35843;&#25972;&#21040;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#24494;&#35843;&#22312;MORLHF&#20013;&#19981;&#31283;&#23450;&#19988;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#22240;&#20026;&#21508;&#31181;&#24120;&#24120;&#30683;&#30462;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#25193;&#23637;&#21040;&#22810;&#20010;&#23545;&#40784;&#30446;&#26631;&#12290;&#22522;&#26412;&#19978;&#65292;MODPO&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;LM&#26469;&#20195;&#34920;&#19981;&#21516;&#30340;&#38598;&#20307;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#36827;&#34892;&#32452;&#21512;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;LM&#26681;&#25454;MOD&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings. With a simple cross-entropy loss, the LMs optimized against the MOD
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15238</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#23398;&#20064;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Using Generated Privileged Information by Text-to-Image Diffusion Models. (arXiv:2309.15238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#26159;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#39069;&#22806;&#30340;&#25968;&#25454;&#34920;&#31034;&#20013;&#33719;&#30410;&#65292;&#36825;&#34987;&#31216;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#19981;&#30475;&#21040;&#39069;&#22806;&#34920;&#31034;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#21487;&#33719;&#24471;&#29305;&#26435;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20154;&#24037;&#29305;&#26435;&#20449;&#24687;&#12290;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#21407;&#22987;&#25991;&#26412;&#26679;&#26412;&#36827;&#19968;&#27493;&#29992;&#20110;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#26469;&#35757;&#32451;&#22810;&#27169;&#24577;&#25945;&#24072;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#22810;&#27169;&#24577;&#25945;&#24072;&#30340;&#30693;&#35782;&#34987;&#33976;&#39311;&#21040;&#22522;&#20110;&#25991;&#26412;&#30340;&#65288;&#21333;&#27169;&#24577;&#65289;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#31216;&#20026;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65288;LUGPI&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Using Privileged Information is a particular type of knowledge distillation where the teacher model benefits from an additional data representation during training, called privileged information, improving the student model, which does not see the extra representation. However, privileged information is rarely available in practice. To this end, we propose a text classification framework that harnesses text-to-image diffusion models to generate artificial privileged information. The generated images and the original text samples are further used to train multimodal teacher models based on state-of-the-art transformer-based architectures. Finally, the knowledge from multimodal teachers is distilled into a text-based (unimodal) student. Hence, by employing a generative model to produce synthetic data as privileged information, we guide the training of the student model. Our framework, called Learning Using Generated Privileged Information (LUGPI), yields noticeable performance g
&lt;/p&gt;</description></item><item><title>Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;&#65292;&#22312;&#20943;&#23567;&#27169;&#22411;&#38544;&#34255;&#22823;&#23567;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.16475</link><description>&lt;p&gt;
Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Transformer Compression via Subspace Projection. (arXiv:2308.16475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16475
&lt;/p&gt;
&lt;p&gt;
Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;&#65292;&#22312;&#20943;&#23567;&#27169;&#22411;&#38544;&#34255;&#22823;&#23567;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TCSP&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#30340;&#38544;&#34255;&#22823;&#23567;&#26469;&#21387;&#32553;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#25972;&#20010;&#36716;&#25442;&#27169;&#22411;&#25237;&#24433;&#21040;&#19968;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#30697;&#38453;&#19982;&#20943;&#23567;&#32500;&#24230;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#20010;&#23376;&#31354;&#38388;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#19981;&#21516;&#23618;&#27425;&#30340;&#37319;&#26679;&#25968;&#25454;&#23454;&#20363;&#30340;&#29305;&#24449;&#30697;&#38453;&#20998;&#35299;&#20026;&#19968;&#20010;&#25237;&#24433;&#30697;&#38453;&#12290;&#20026;&#20102;&#35780;&#20272;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#24212;&#29992;TCSP&#26469;&#21387;&#32553;T5&#21644;BERT&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TCSP&#22312;&#20445;&#35777;&#26368;&#22810;1.6%&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;44%&#30340;&#21387;&#32553;&#27604;&#65292;&#36229;&#36807;&#25110;&#32773;&#36798;&#21040;&#20102;&#20808;&#21069;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;TCSP&#36824;&#19982;&#20854;&#20182;&#30446;&#26631;&#36807;&#28388;&#22120;&#21644;&#27880;&#24847;&#21147;&#22836;&#22823;&#23567;&#21387;&#32553;&#30340;&#26041;&#27861;&#30456;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TCSP, a novel method for compressing a transformer model by focusing on reducing the hidden size of the model. By projecting the whole transform model into a subspace, we enable matrix operations between the weight matrices in the model and features in a reduced-dimensional space, leading to significant reductions in model parameters and computing resources. To establish this subspace, we decompose the feature matrix, derived from different layers of sampled data instances, into a projection matrix. For evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a compression ratio of 44\% with at most 1.6\% degradation in accuracy, surpassing or matching prior compression methods. Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;CSO-MA&#65292;&#36890;&#36807;&#22810;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#24212;&#29992;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10875</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#21450;&#20854;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#29983;&#29289;&#32479;&#35745;&#23398;&#12289;&#29983;&#24577;&#23398;&#21644;&#21046;&#36896;&#19994;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Metaheuristic Algorithms in Artificial Intelligence with Applications to Bioinformatics, Biostatistics, Ecology and, the Manufacturing Industries. (arXiv:2308.10875v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10875
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;CSO-MA&#65292;&#36890;&#36807;&#22810;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#24212;&#29992;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#31185;&#39046;&#22495;&#20013;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#25361;&#25112;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#31216;&#20026;&#20855;&#26377;&#31361;&#21464;&#20195;&#29702;&#30340;&#31454;&#20105;&#24615;&#32676;&#20307;&#20248;&#21270;&#22120;(CSO-MA)&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30456;&#23545;&#20110;&#31454;&#20105;&#23545;&#25163;&#22312;&#32479;&#35745;&#31185;&#23398;&#20013;&#21508;&#31181;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#28789;&#27963;&#24615;&#21644;&#36229;&#36234;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#39640;&#25928;&#19988;&#21487;&#20197;&#25972;&#21512;&#21508;&#31181;&#25104;&#26412;&#32467;&#26500;&#25110;&#22810;&#20010;&#29992;&#25143;&#25351;&#23450;&#30340;&#38750;&#32447;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#24212;&#29992;&#21253;&#25324;(i)&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#36890;&#36807;&#21333;&#32454;&#32990;&#24191;&#20041;&#36235;&#21183;&#27169;&#22411;&#25214;&#21040;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#20197;&#30740;&#31350;&#20266;&#26102;&#24577;&#65292;(ii) &#20272;&#35745;&#25945;&#32946;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;Rasch&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;(iii) &#22312;&#39532;&#23572;&#21487;&#22827;&#26356;&#26032;&#27169;&#22411;&#20013;&#20026;Cox&#22238;&#24402;&#25214;&#21040;M-&#20272;&#35745;&#65292;(iv) &#30697;&#38453;&#34917;&#20840;&#20197;&#22635;&#34917;&#20004;&#20010;&#36830;&#36830;&#19981;&#36890;&#22270;&#20013;&#30340;&#32570;&#22833;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nature-inspired metaheuristic algorithms are important components of artificial intelligence, and are increasingly used across disciplines to tackle various types of challenging optimization problems. We apply a newly proposed nature-inspired metaheuristic algorithm called competitive swarm optimizer with mutated agents (CSO-MA) and demonstrate its flexibility and out-performance relative to its competitors in a variety of optimization problems in the statistical sciences. In particular, we show the algorithm is efficient and can incorporate various cost structures or multiple user-specified nonlinear constraints. Our applications include (i) finding maximum likelihood estimates of parameters in a single cell generalized trend model to study pseudotime in bioinformatics, (ii) estimating parameters in a commonly used Rasch model in education research, (iii) finding M-estimates for a Cox regression in a Markov renewal model and (iv) matrix completion to impute missing values in a two com
&lt;/p&gt;</description></item><item><title>RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07922</link><description>&lt;p&gt;
RAVEN&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07922
&lt;/p&gt;
&lt;p&gt;
RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;ATLAS&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#39044;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#38271;&#24230;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RAVEN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#19978;&#19979;&#25991;&#31034;&#20363;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#27169;&#22411;&#20462;&#25913;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RAVEN&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#26126;&#26174;&#20248;&#20110;ATLAS&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LoraHub&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.13269</link><description>&lt;p&gt;
LoraHub: &#36890;&#36807;&#21160;&#24577;LoRA&#32452;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#20219;&#21153;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition. (arXiv:2307.13269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LoraHub&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24120;&#24120;&#34987;&#29992;&#20110;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24494;&#35843;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;LoraHub&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#30446;&#30340;&#24615;&#32452;&#35013;&#22312;&#19981;&#21516;&#32473;&#23450;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#30340;&#25112;&#30053;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#20165;&#20973;&#20511;&#26469;&#33258;&#26032;&#20219;&#21153;&#30340;&#20960;&#20010;&#31034;&#20363;&#65292;LoraHub&#21487;&#20197;&#28789;&#27963;&#22320;&#32452;&#21512;&#22810;&#20010;LoRA&#27169;&#22359;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#32452;&#21512;&#26082;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20063;&#19981;&#38656;&#35201;&#26799;&#24230;&#12290;&#25105;&#20204;&#20174;Big-Bench Hard&#65288;BBH&#65289;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#20986;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#22312;&#27599;&#20010;&#25512;&#29702;&#36755;&#20837;&#26049;&#36793;&#19981;&#38656;&#35201;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#22521;&#32946;&#19968;&#20010;LoRA&#31038;&#21306;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#20854;&#20013;&#20998;&#20139;&#20182;&#20204;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a strategic framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a novel task, LoraHub enables the fluid combination of multiple LoRA modules, eradicating the need for human expertise. Notably, the composition requires neither additional model parameters nor gradients. Our empirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest that LoraHub can effectively mimic the performance of in-context learning in few-shot scenarios, excluding the necessity of in-context examples alongside each inference input. A significant contribution of our research is the fostering of a community for LoRA, where users can share their trained LoRA module
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;MuZero&#31639;&#27861;&#65292;&#21457;&#29616;&#23427;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#24403;&#21069;&#31574;&#30053;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.00840</link><description>&lt;p&gt;
MuZero&#23398;&#21040;&#20102;&#20160;&#20040;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
What model does MuZero learn?. (arXiv:2306.00840v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;MuZero&#31639;&#27861;&#65292;&#21457;&#29616;&#23427;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#24403;&#21069;&#31574;&#30053;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#26395;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#26377;&#21487;&#33021;&#20174;&#22797;&#26434;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#32039;&#20945;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#35268;&#21010;&#33021;&#21147;&#30340;&#25552;&#21319;&#24403;&#21069;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MuZero&#36825;&#20010;&#33879;&#21517;&#30340;&#22522;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#22312;&#23454;&#29616;&#20540;&#31561;&#20215;&#27169;&#22411;&#30340;&#23398;&#20064;&#30446;&#26631;&#19978;&#30340;&#25104;&#23601;&#20197;&#21450;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#23545;&#31574;&#30053;&#25913;&#36827;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#35832;&#22810;&#20854;&#20182;&#35266;&#28857;&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;MuZero&#23398;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#35268;&#21010;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#24403;&#21069;&#31574;&#30053;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning has drawn considerable interest in recent years, given its promise to improve sample efficiency. Moreover, when using deep-learned models, it is potentially possible to learn compact models from complex sensor data. However, the effectiveness of these learned models, particularly their capacity to plan, i.e., to improve the current policy, remains unclear. In this work, we study MuZero, a well-known deep model-based reinforcement learning algorithm, and explore how far it achieves its learning objective of a value-equivalent model and how useful the learned models are for policy improvement. Amongst various other insights, we conclude that the model learned by MuZero cannot effectively generalize to evaluate unseen policies, which limits the extent to which we can additionally improve the current policy by planning with the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22870;&#21169;&#21152;&#26435;&#21644;&#25439;&#22833;&#21152;&#26435;&#26799;&#24230;&#21512;&#24182;&#65292;&#20197;&#26356;&#22909;&#22320;&#25552;&#39640;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12778</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#21644;&#22870;&#21169;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Loss and Reward Weighing for increased learning in Distributed Reinforcement Learning. (arXiv:2304.12778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22870;&#21169;&#21152;&#26435;&#21644;&#25439;&#22833;&#21152;&#26435;&#26799;&#24230;&#21512;&#24182;&#65292;&#20197;&#26356;&#22909;&#22320;&#25552;&#39640;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#20013;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#21363;&#22870;&#21169;&#21152;&#26435;&#65288;R-Weighted&#65289;&#21644;&#25439;&#22833;&#21152;&#26435;&#65288;L-Weighted&#65289;&#26799;&#24230;&#21512;&#24182;&#12290; R / L &#21152;&#26435;&#26041;&#27861;&#26367;&#25442;&#20102;&#35757;&#32451;&#22810;&#20010;&#20195;&#29702;&#30340;&#26631;&#20934;&#23454;&#36341;&#65292;&#20363;&#22914;&#23545;&#26799;&#24230;&#27714;&#21644;&#25110;&#24179;&#22343;&#12290;&#27599;&#20010;&#20195;&#29702;&#22312;&#19981;&#21516;&#21021;&#22987;&#21270;&#29256;&#26412;&#30340;&#30456;&#21516;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#36825;&#20250;&#20174;&#19981;&#21516;&#30340;actor&#33719;&#24471;&#19981;&#21516;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces two learning schemes for distributed agents in Reinforcement Learning (RL) environments, namely Reward-Weighted (R-Weighted) and Loss-Weighted (L-Weighted) gradient merger. The R/L weighted methods replace standard practices for training multiple agents, such as summing or averaging the gradients. The core of our methods is to scale the gradient of each actor based on how high the reward (for R-Weighted) or the loss (for L-Weighted) is compared to the other actors. During training, each agent operates in differently initialized versions of the same environment, which gives different gradients from different actors. In essence, the R-Weights and L-Weights of each agent inform the other agents of its potential, which again reports which environment should be prioritized for learning. This approach of distributed learning is possible because environments that yield higher rewards, or low losses, have more critical information than environments that yield lower reward
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Product Autoencoder&#26469;&#36890;&#36807;&#21487;&#31649;&#29702;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#23454;&#29616;&#30456;&#23545;&#36739;&#22823;&#30340;&#20195;&#30721;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.16424</link><description>&lt;p&gt;
ProductAE&#65306;&#38754;&#21521;&#22823;&#32500;&#25968;&#30340;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#32416;&#38169;&#30721;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
ProductAE: Toward Deep Learning Driven Error-Correction Codes of Large Dimensions. (arXiv:2303.16424v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Product Autoencoder&#26469;&#36890;&#36807;&#21487;&#31649;&#29702;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#23454;&#29616;&#30456;&#23545;&#36739;&#22823;&#30340;&#20195;&#30721;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20960;&#21313;&#24180;&#30340;&#29702;&#35770;&#30740;&#31350;&#24050;&#32463;&#21457;&#26126;&#20102;&#20960;&#20010;&#32416;&#38169;&#30721;&#31867;&#21035;&#65292;&#20294;&#36825;&#20123;&#30721;&#30340;&#35774;&#35745;&#21364;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#20381;&#38752;&#20154;&#31867;&#26234;&#24935;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#35774;&#35745;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24037;&#20855;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#21644;&#21152;&#36895;&#65292;&#20174;&#32780;&#23454;&#29616;&#19982;&#32463;&#20856;&#35774;&#35745;&#30456;&#27604;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#22686;&#30410;&#30340;ML&#39537;&#21160;&#30340;&#32416;&#38169;&#30721;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#25361;&#25112;&#26159;&#65292;&#23545;&#20110;&#22823;&#30721;&#32500;&#24230;&#26469;&#35828;&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#23436;&#20840;&#30340;ML&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#26469;&#35828;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#65292;&#22914;&#26524;&#19981;&#26159;&#19981;&#21487;&#33021;&#30340;&#30340;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Product Autoencoder&#65288;ProductAE&#65289;-&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#65288;&#32534;&#30721;&#22120;&#65292;&#35299;&#30721;&#22120;&#65289;&#23545;&#30340;&#31995;&#21015;-&#26088;&#22312;&#36890;&#36807;&#21487;&#31649;&#29702;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#23454;&#29616;&#30456;&#23545;&#36739;&#22823;&#30340;&#20195;&#30721;&#65288;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65289;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#32463;&#20856;&#20056;&#31215;&#30721;&#30340;&#24605;&#24819;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;ProductAE&#26500;&#24314;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
While decades of theoretical research have led to the invention of several classes of error-correction codes, the design of such codes is an extremely challenging task, mostly driven by human ingenuity. Recent studies demonstrate that such designs can be effectively automated and accelerated via tools from machine learning (ML), thus enabling ML-driven classes of error-correction codes with promising performance gains compared to classical designs. A fundamental challenge, however, is that it is prohibitively complex, if not impossible, to design and train fully ML-driven encoder and decoder pairs for large code dimensions. In this paper, we propose Product Autoencoder (ProductAE) -- a computationally-efficient family of deep learning driven (encoder, decoder) pairs -- aimed at enabling the training of relatively large codes (both encoder and decoder) with a manageable training complexity. We build upon ideas from classical product codes and propose constructing large neural codes usin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20294;&#26410;&#26469;&#21313;&#24180;&#38543;&#30528;&#38556;&#30861;&#34987;&#20811;&#26381;&#65292;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2303.07103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Could a Large Language Model be Conscious?. (arXiv:2303.07103v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20294;&#26410;&#26469;&#21313;&#24180;&#38543;&#30528;&#38556;&#30861;&#34987;&#20811;&#26381;&#65292;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26222;&#36941;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24863;&#30693;&#25110;&#24847;&#35782;&#12290;&#25105;&#20204;&#26159;&#21542;&#24212;&#35813;&#35748;&#30495;&#32771;&#34385;&#36825;&#20010;&#24819;&#27861;&#65311;&#26412;&#25991;&#23558;&#20998;&#26512;&#25903;&#25345;&#21644;&#21453;&#23545;&#36825;&#20010;&#24819;&#27861;&#30340;&#26368;&#26377;&#21147;&#30340;&#29702;&#30001;&#12290;&#26681;&#25454;&#24847;&#35782;&#31185;&#23398;&#20013;&#30340;&#20027;&#27969;&#20551;&#35774;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20363;&#22914;&#32570;&#20047;&#24490;&#29615;&#22788;&#29702;&#12289;&#20840;&#23616;&#30340;&#24037;&#20316;&#31354;&#38388;&#21644;&#32479;&#19968;&#30340;&#26234;&#33021;&#26426;&#26500;&#31561;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#38556;&#30861;&#22312;&#26410;&#26469;&#21313;&#24180;&#24038;&#21491;&#37117;&#21487;&#33021;&#34987;&#20811;&#26381;&#12290;&#20316;&#32773;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#34429;&#28982;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#36739;&#23567;&#65292;&#20294;&#25105;&#20204;&#24212;&#35813;&#35748;&#30495;&#32771;&#34385;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has recently been widespread discussion of whether large language models might be sentient or conscious. Should we take this idea seriously? I will break down the strongest reasons for and against. Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models: for example, their lack of recurrent processing, a global workspace, and unified agency. At the same time, it is quite possible that these obstacles will be overcome in the next decade or so. I conclude that while it is somewhat unlikely that current large language models are conscious, we should take seriously the possibility that successors to large language models may be conscious in the not-too-distant future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#26080;&#26799;&#24230;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#30340;&#23545;&#25239;&#34917;&#19969;&#65292;&#25915;&#20987;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.04238</link><description>&lt;p&gt;
&#21306;&#22495;&#38544;&#24418;&#34917;&#19969;&#65306;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#29289;&#20307;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors. (arXiv:2303.04238v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#26080;&#26799;&#24230;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#30340;&#23545;&#25239;&#34917;&#19969;&#65292;&#25915;&#20987;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#36234;&#26469;&#36234;&#24341;&#36215;&#20851;&#27880;&#12290;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#65292;&#21363;&#25152;&#35859;&#30340;&#30333;&#30418;&#25915;&#20987;&#65292;&#22312;&#20854;&#20013;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#36890;&#24120;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#30456;&#23545;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#38656;&#20351;&#29992;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#23398;&#20064;&#22270;&#20687;&#27969;&#24418;&#26469;&#29983;&#25104;&#33258;&#28982;&#30340;&#29289;&#29702;&#23545;&#25239;&#34917;&#19969;&#65292;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#23618;&#38754;&#19978;&#22343;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks on deep-learning models have been receiving increased attention in recent years. Work in this area has mostly focused on gradient-based techniques, so-called white-box attacks, wherein the attacker has access to the targeted model's internal parameters; such an assumption is usually unrealistic in the real world. Some attacks additionally use the entire pixel space to fool a given model, which is neither practical nor physical (i.e., real-world). On the contrary, we propose herein a gradient-free method that uses the learned image manifold of a pretrained generative adversarial network (GAN) to generate naturalistic physical adversarial patches for object detectors. We show that our proposed method works both digitally and physically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#30701;&#26399;&#12289;&#24773;&#33410;&#21644;&#35821;&#20041;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24314;&#27169;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#30701;&#26399;&#35760;&#24518;&#30340;&#31649;&#29702;&#21644;&#23384;&#20648;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#20154;&#31867;&#35760;&#24518;&#31995;&#32479;&#32467;&#26500;&#30340;&#20195;&#29702;&#27604;&#27809;&#26377;&#35813;&#32467;&#26500;&#30340;&#20195;&#29702;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2212.02098</link><description>&lt;p&gt;
&#19968;&#20010;&#20855;&#26377;&#30701;&#26399;&#12289;&#24773;&#33410;&#21644;&#35821;&#20041;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Machine with Short-Term, Episodic, and Semantic Memory Systems. (arXiv:2212.02098v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#30701;&#26399;&#12289;&#24773;&#33410;&#21644;&#35821;&#20041;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24314;&#27169;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#30701;&#26399;&#35760;&#24518;&#30340;&#31649;&#29702;&#21644;&#23384;&#20648;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#20154;&#31867;&#35760;&#24518;&#31995;&#32479;&#32467;&#26500;&#30340;&#20195;&#29702;&#27604;&#27809;&#26377;&#35813;&#32467;&#26500;&#30340;&#20195;&#29702;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#35748;&#30693;&#31185;&#23398;&#29702;&#35770;&#20013;&#26174;&#24615;&#20154;&#31867;&#35760;&#24518;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#30701;&#26399;&#12289;&#24773;&#33410;&#21644;&#35821;&#20041;&#35760;&#24518;&#31995;&#32479;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#27599;&#20010;&#35760;&#24518;&#31995;&#32479;&#37117;&#29992;&#30693;&#35782;&#22270;&#35889;&#24314;&#27169;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#31995;&#32479;&#24182;&#20998;&#26512;&#35813;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#29615;&#22659;&#8220;&#25151;&#38388;&#8221;&#65292;&#22312;&#36825;&#20010;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#24517;&#39035;&#23398;&#20064;&#22914;&#20309;&#32534;&#30721;&#12289;&#23384;&#20648;&#21644;&#26816;&#32034;&#35760;&#24518;&#65292;&#36890;&#36807;&#22238;&#31572;&#38382;&#39064;&#26469;&#26368;&#22823;&#21270;&#22238;&#25253;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#20195;&#29702;&#25104;&#21151;&#23398;&#20064;&#20102;&#30701;&#26399;&#35760;&#24518;&#26159;&#21542;&#24212;&#35813;&#34987;&#36951;&#24536;&#65292;&#36824;&#26159;&#24212;&#35813;&#23384;&#20648;&#22312;&#24773;&#33410;&#25110;&#35821;&#20041;&#35760;&#24518;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20855;&#26377;&#31867;&#20154;&#35760;&#24518;&#31995;&#32479;&#30340;&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;&#27809;&#26377;&#36825;&#31181;&#35760;&#24518;&#32467;&#26500;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the cognitive science theory of the explicit human memory systems, we have modeled an agent with short-term, episodic, and semantic memory systems, each of which is modeled with a knowledge graph. To evaluate this system and analyze the behavior of this agent, we designed and released our own reinforcement learning agent environment, "the Room", where an agent has to learn how to encode, store, and retrieve memories to maximize its return by answering questions. We show that our deep Q-learning based agent successfully learns whether a short-term memory should be forgotten, or rather be stored in the episodic or semantic memory systems. Our experiments indicate that an agent with human-like memory systems can outperform an agent without this memory structure in the environment.
&lt;/p&gt;</description></item></channel></rss>