<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#24037;&#20855;&#22312;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#33021;&#32791;&#24314;&#27169;&#30340;&#22256;&#38590;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;</title><link>https://arxiv.org/abs/2403.17632</link><description>&lt;p&gt;
&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#33021;&#32791;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17632
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#24037;&#20855;&#22312;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#33021;&#32791;&#24314;&#27169;&#30340;&#22256;&#38590;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#25317;&#22581;&#21644;&#29615;&#22659;&#24694;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#26085;&#30410;&#21152;&#21095;&#65292;&#20984;&#26174;&#20102;&#22312;&#22478;&#24066;&#31354;&#38388;&#25512;&#34892;E-Mobility&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;E-&#28369;&#26495;&#36710;&#21644;E-&#33258;&#34892;&#36710;&#31561;&#24494;&#22411;E-Mobility&#24037;&#20855;&#22312;&#36825;&#19968;&#36716;&#21464;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20026;&#22478;&#24066;&#36890;&#21220;&#32773;&#25552;&#20379;&#21487;&#25345;&#32493;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#30340;&#33021;&#32791;&#27169;&#24335;&#26159;&#24433;&#21709;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#23545;&#20110;&#20986;&#34892;&#35268;&#21010;&#20197;&#21450;&#22686;&#24378;&#29992;&#25143;&#22312;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#26102;&#30340;&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#38024;&#23545;&#29305;&#23450;&#31227;&#21160;&#24037;&#20855;&#21644;&#26465;&#20214;&#23450;&#21046;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#24443;&#24213;&#27169;&#22411;&#35780;&#20272;&#21644;&#39564;&#35777;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29233;&#23572;&#20848;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#33021;&#32791;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17632v1 Announce Type: new  Abstract: The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for ene
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#20419;&#36827;&#20195;&#29702;&#38388;&#19978;&#19979;&#25991;&#20999;&#25442;&#12289;&#23454;&#29616;&#24182;&#21457;&#25191;&#34892;&#20197;&#21450;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.16971</link><description>&lt;p&gt;
LLM Agent Operating System
&lt;/p&gt;
&lt;p&gt;
LLM Agent Operating System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16971
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#20419;&#36827;&#20195;&#29702;&#38388;&#19978;&#19979;&#25991;&#20999;&#25442;&#12289;&#23454;&#29616;&#24182;&#21457;&#25191;&#34892;&#20197;&#21450;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16971v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20195;&#29702;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#25928;&#29575;&#21644;&#21151;&#25928;&#12290;&#20854;&#20013;&#21253;&#25324;&#20195;&#29702;&#35831;&#27714;&#22312;LLM&#19978;&#30340;&#27425;&#20248;&#35843;&#24230;&#21644;&#36164;&#28304;&#20998;&#37197;&#12289;&#22312;&#20195;&#29702;&#21644;LLM&#20043;&#38388;&#20132;&#20114;&#26102;&#20445;&#25345;&#19978;&#19979;&#25991;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#23558;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#21644;&#19987;&#19994;&#21270;&#30340;&#24322;&#26500;&#20195;&#29702;&#38598;&#25104;&#22312;&#19968;&#36215;&#30340;&#22797;&#26434;&#24615;&#12290;&#20195;&#29702;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#30340;&#24555;&#36895;&#22686;&#21152;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#36164;&#28304;&#29942;&#39048;&#21644;&#27425;&#20248;&#36164;&#28304;&#21033;&#29992;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;AIOS&#65292;&#19968;&#31181;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#65288;OS&#65289;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;AIOS&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#65292;&#20419;&#36827;&#20195;&#29702;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20999;&#25442;&#65292;&#23454;&#29616;&#20195;&#29702;&#30340;&#24182;&#21457;&#25191;&#34892;&#65292;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16971v1 Announce Type: cross  Abstract: The integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS). Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents
&lt;/p&gt;</description></item><item><title>&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;</title><link>https://arxiv.org/abs/2403.09539</link><description>&lt;p&gt;
API&#20445;&#25252;&#30340;LLMs&#30340;&#26631;&#24535;&#27844;&#38706;&#19987;&#26377;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Logits of API-Protected LLMs Leak Proprietary Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09539
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21830;&#19994;&#21270;&#23548;&#33268;&#20102;&#39640;&#32423;API-only&#25509;&#20837;&#19987;&#26377;&#27169;&#22411;&#30340;&#24120;&#35265;&#23454;&#36341;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#27169;&#22411;&#26550;&#26500;&#26377;&#20445;&#23432;&#30340;&#20551;&#35774;&#65292;&#20063;&#21487;&#20197;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;API&#26597;&#35810;&#20013;&#23398;&#20064;&#20851;&#20110;API&#20445;&#25252;&#30340;LLM&#30340;&#22823;&#37327;&#38750;&#20844;&#24320;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;OpenAI&#30340;gpt-3.5-turbo&#20165;&#33457;&#36153;&#19981;&#21040;1000&#32654;&#20803;&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#38598;&#20013;&#22312;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#19978;&#65306;&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;&#20102;softmax&#29942;&#39048;&#30340;&#24433;&#21709;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#36755;&#20986;&#21040;&#23436;&#25972;&#36755;&#20986;&#31354;&#38388;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#27169;&#22411;&#22270;&#20687;&#25110;&#27169;&#22411;&#31614;&#21517;&#65292;&#20174;&#32780;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#35299;&#38145;&#20102;&#20960;&#31181;&#21151;&#33021;&#65306;&#26377;&#25928;&#21457;&#29616;LLM&#30340;&#38544;&#34255;&#22823;&#23567;&#65292;&#33719;&#21462;&#23436;&#25972;&#35789;&#27719;&#36755;&#20986;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#19981;&#21516;&#27169;&#22411;&#26356;&#26032;&#65292;&#35782;&#21035;&#32473;&#23450;&#21333;&#20010;&#23436;&#25972;LLM&#36755;&#20986;&#30340;&#28304;LLM&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09539v1 Announce Type: cross  Abstract: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and eve
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Curry-DPO&#30340;&#26041;&#27861;&#65292;&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#20013;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#20559;&#22909;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#21333;&#19968;&#23545;DPO&#35774;&#32622;&#26377;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07230</link><description>&lt;p&gt;
Curry-DPO&#65306;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#25490;&#21517;&#20559;&#22909;&#22686;&#24378;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Curry-DPO: Enhancing Alignment using Curriculum Learning &amp; Ranked Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Curry-DPO&#30340;&#26041;&#27861;&#65292;&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#20013;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#20559;&#22909;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#21333;&#19968;&#23545;DPO&#35774;&#32622;&#26377;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;(&#36890;&#24120;&#26159;&#27599;&#20010;&#29992;&#25143;&#25552;&#31034;&#36873;&#25321;&#21644;&#25298;&#32477;&#30340;&#21709;&#24212;&#23545;)&#23558;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#20110;&#32473;&#23450;&#25552;&#31034;&#21487;&#33021;&#20250;&#23384;&#22312;&#22810;&#20010;&#21709;&#24212;&#65292;&#36825;&#20123;&#21709;&#24212;&#30340;&#36136;&#37327;&#30456;&#23545;&#20110;&#24444;&#27492;&#32780;&#35328;&#26377;&#25152;&#19981;&#21516;&#12290;&#26377;&#20102;&#36825;&#20123;&#22810;&#20010;&#21709;&#24212;&#30340;&#36136;&#37327;&#35780;&#32423;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36825;&#20123;&#21709;&#24212;&#20026;&#32473;&#23450;&#25552;&#31034;&#21019;&#24314;&#22810;&#20010;&#20559;&#22909;&#23545;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#31995;&#32479;&#22320;&#21033;&#29992;&#26500;&#24314;&#30340;&#22810;&#20010;&#20559;&#22909;&#23545;&#26469;&#36827;&#34892;DPO&#35757;&#32451;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#26631;&#20934;&#23558;&#36825;&#20123;&#22810;&#20010;&#20559;&#22909;&#25968;&#25454;&#23545;&#20174;&#26131;&#21040;&#38590;(&#27169;&#25311;&#35838;&#31243;&#35757;&#32451;)&#25490;&#24207;&#12290;&#25105;&#20204;&#35814;&#32454;&#27604;&#36739;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#26631;&#20934;&#21333;&#19968;&#23545;DPO&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Curry-DPO&#65292;&#22312;MTbench&#12289;Vicuna&#12289;Wiz&#19978;&#22987;&#32456;&#34920;&#29616;&#20986;&#22686;&#24378;&#30340;&#24615;&#33021;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07230v1 Announce Type: cross  Abstract: Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences. In practice, multiple responses can exist for a given prompt with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt. Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, Wiz
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#20013;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18424</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Emotion Classification in Low and Moderate Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#20013;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20998;&#26512;&#20840;&#29699;&#33539;&#22260;&#20869;&#20154;&#20204;&#24773;&#32490;&#29366;&#24577;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20840;&#29699;&#26377;7100&#22810;&#31181;&#27963;&#36291;&#35821;&#35328;&#65292;&#20026;&#27599;&#31181;&#35821;&#35328;&#26500;&#24314;&#24773;&#24863;&#20998;&#31867;&#26159;&#19968;&#39033;&#21171;&#21160;&#23494;&#38598;&#22411;&#24037;&#20316;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#21644;&#28626;&#21361;&#35821;&#35328;&#65292;&#24314;&#31435;&#24773;&#24863;&#20998;&#31867;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65288;&#20363;&#22914;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#30340;&#33521;&#35821;&#65289;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#24182;&#23558;&#23398;&#20064;&#36801;&#31227;&#21040;&#20302;&#36164;&#28304;&#21644;&#20013;&#31561;&#36164;&#28304;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#27604;&#36739;&#24182;&#23545;&#27604;&#20102;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#25110;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#30340;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#19968;&#31181;&#26041;&#27861;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#27880;&#25237;&#24433;&#21040;&#20302;&#36164;&#28304;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#30452;&#25509;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#23398;&#20064;&#36801;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;6&#31181;&#35821;&#35328;&#19978;&#30340;&#26377;&#25928;&#24615;&#65306;Fa
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18424v1 Announce Type: cross  Abstract: It is important to be able to analyze the emotional state of people around the globe. There are 7100+ active languages spoken around the world and building emotion classification for each language is labor intensive. Particularly for low-resource and endangered languages, building emotion classification can be quite challenging. We present a cross-lingual emotion classifier, where we train an emotion classifier with resource-rich languages (i.e. \textit{English} in our work) and transfer the learning to low and moderate resource languages. We compare and contrast two approaches of transfer learning from a high-resource language to a low or moderate-resource language. One approach projects the annotation from a high-resource language to low and moderate-resource language in parallel corpora and the other one uses direct transfer from high-resource language to the other languages. We show the efficacy of our approaches on 6 languages: Fa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hal-Eval&#65292;&#19968;&#20010;&#36890;&#29992;&#21644;&#32454;&#31890;&#24230;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24187;&#35273;&#20998;&#31867;&#27861;&#65292;&#19987;&#27880;&#20110;&#20107;&#20214;&#24187;&#35273;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#36807;&#28388;&#32454;&#31890;&#24230;&#24187;&#35273;&#25968;&#25454;&#26469;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#21508;&#31181;&#24187;&#35273;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15721</link><description>&lt;p&gt;
Hal-Eval: &#19968;&#31181;&#38754;&#21521;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#21644;&#32454;&#31890;&#24230;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hal-Eval&#65292;&#19968;&#20010;&#36890;&#29992;&#21644;&#32454;&#31890;&#24230;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24187;&#35273;&#20998;&#31867;&#27861;&#65292;&#19987;&#27880;&#20110;&#20107;&#20214;&#24187;&#35273;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#36807;&#28388;&#32454;&#31890;&#24230;&#24187;&#35273;&#25968;&#25454;&#26469;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#21508;&#31181;&#24187;&#35273;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#22270;&#29255;&#21644;&#20854;&#25551;&#36848;&#20043;&#38388;&#23384;&#22312;&#24187;&#35273;&#19981;&#19968;&#33268;&#12290;&#20197;&#24448;&#23545;LVLMs&#36827;&#34892;&#30340;&#24187;&#35273;&#35780;&#20272;&#30740;&#31350;&#21457;&#29616;&#20102;&#20851;&#20110;&#23545;&#35937;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#30340;&#24187;&#35273;&#65292;&#20294;&#24573;&#30053;&#20102;&#22260;&#32469;&#34394;&#26500;&#23454;&#20307;&#21019;&#24314;&#25972;&#20010;&#21465;&#20107;&#30340;&#22797;&#26434;&#24187;&#35273;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#24187;&#35273;&#20998;&#31867;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#31867;&#21035;&#65306;&#20107;&#20214;&#24187;&#35273;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20808;&#36827;&#30340;LLMs&#29983;&#25104;&#21644;&#36807;&#28388;&#30001;&#21508;&#31181;&#31867;&#22411;&#30340;&#24187;&#35273;&#32452;&#25104;&#30340;&#32454;&#31890;&#24230;&#24187;&#35273;&#25968;&#25454;&#65292;&#29305;&#21035;&#20851;&#27880;&#20107;&#20214;&#24187;&#35273;&#65292;&#20026;&#22312;&#25105;&#20204;&#30340;&#36890;&#29992;&#35780;&#20272;&#26694;&#26550;&#20869;&#38598;&#25104;&#36776;&#21035;&#21644;&#29983;&#25104;&#35780;&#20272;&#26041;&#27861;&#22880;&#23450;&#22522;&#30784;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#21487;&#20197;&#29420;&#29305;&#22320;&#35780;&#20272;LVLMs&#22788;&#29702;&#24191;&#27867;&#24187;&#35273;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#21487;&#38752;&#21644;&#20840;&#38754;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15721v1 Announce Type: new  Abstract: Large Vision Language Models exhibit remarkable capabilities but struggle with hallucinations inconsistencies between images and their descriptions. Previous hallucination evaluation studies on LVLMs have identified hallucinations in terms of objects, attributes, and relations but overlooked complex hallucinations that create an entire narrative around a fictional entity. In this paper, we introduce a refined taxonomy of hallucinations, featuring a new category: Event Hallucination. We then utilize advanced LLMs to generate and filter fine grained hallucinatory data consisting of various types of hallucinations, with a particular focus on event hallucinations, laying the groundwork for integrating discriminative and generative evaluation methods within our universal evaluation framework. The proposed benchmark distinctively assesses LVLMs ability to tackle a broad spectrum of hallucinations, making it a reliable and comprehensive tool fo
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#23454;&#26045;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#25277;&#26679;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#26469;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12789</link><description>&lt;p&gt;
&#26080;&#38656;&#20844;&#24179;&#35757;&#32451;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#65306;&#19968;&#31181;&#21463;&#24433;&#21709;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12789
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#23454;&#26045;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#25277;&#26679;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#26469;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#24212;&#35813;&#30830;&#20445;&#26469;&#33258;&#19981;&#21516;&#32676;&#20307;&#30340;&#20154;&#20204;&#21463;&#30410;&#65292;&#32780;&#32676;&#20307;&#20449;&#24687;&#24448;&#24448;&#26159;&#25935;&#24863;&#30340;&#65292;&#19981;&#36866;&#21512;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19968;&#20010;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#20294;&#25490;&#38500;&#25935;&#24863;&#23646;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#32780;&#19981;&#23454;&#29616;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#21487;&#33021;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#22312;&#20855;&#26377;&#36866;&#24403;&#20998;&#24067;&#20559;&#31227;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20256;&#32479;&#35757;&#32451;&#21487;&#20197;&#21516;&#26102;&#20943;&#23569;&#20844;&#24179;&#24046;&#36317;&#30340;&#19978;&#38480;&#21644;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#65292;&#34920;&#26126;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#21487;&#20197;&#21516;&#27493;&#25552;&#39640;&#65292;&#21482;&#38656;&#31616;&#21333;&#22320;&#36827;&#34892;&#20256;&#32479;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25277;&#26679;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#35775;&#38382;&#26032;&#25968;&#25454;&#30340;&#25935;&#24863;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12789v1 Announce Type: cross  Abstract: A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27719;&#38598;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25514;&#26045;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294; GPT-4 &#23637;&#31034;&#20102;&#33021;&#21147;&#30340;&#39134;&#36291;&#12290;</title><link>https://arxiv.org/abs/2402.07510</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#31192;&#23494;&#21246;&#32467;
&lt;/p&gt;
&lt;p&gt;
Secret Collusion Among Generative AI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27719;&#38598;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25514;&#26045;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294; GPT-4 &#23637;&#31034;&#20102;&#33021;&#21147;&#30340;&#39134;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33021;&#21147;&#19978;&#30340;&#22686;&#24378;&#20026;&#36890;&#20449;&#30340;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#22242;&#38431;&#35299;&#20915;&#32852;&#21512;&#20219;&#21153;&#30340;&#24212;&#29992;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#26410;&#32463;&#25480;&#26435;&#20998;&#20139;&#20449;&#24687;&#25110;&#20854;&#20182;&#19981;&#24517;&#35201;&#30340;&#20195;&#29702;&#21327;&#35843;&#24418;&#24335;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#25361;&#25112;&#12290;&#29616;&#20195;&#38544;&#20889;&#26415;&#25216;&#26415;&#21487;&#33021;&#20351;&#36825;&#31181;&#21160;&#24577;&#38590;&#20197;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#27762;&#21462;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30456;&#20851;&#27010;&#24565;&#65292;&#20840;&#38754;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38544;&#20889;&#26415;&#30340;&#21160;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26159;&#19968;&#20010;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#34429;&#28982;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#65292;&#20294; GPT-4 &#26174;&#31034;&#20986;&#33021;&#21147;&#30340;&#39134;&#36291;&#65292;&#36825;&#34920;&#26126;&#26377;&#24517;&#35201;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need fo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#31867;&#20154;&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#31526;&#21512;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#25903;&#25345;&#20262;&#29702;&#31561;&#22810;&#26041;&#38754;&#30340;&#20215;&#20540;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2312.14106</link><description>&lt;p&gt;
&#23398;&#20064;&#31867;&#20154;&#34920;&#31034;&#20197;&#23454;&#29616;&#23398;&#20064;&#31867;&#20154;&#20215;&#20540;&#35266;
&lt;/p&gt;
&lt;p&gt;
Learning Human-like Representations to Enable Learning Human Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14106
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31867;&#20154;&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#31526;&#21512;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#25903;&#25345;&#20262;&#29702;&#31561;&#22810;&#26041;&#38754;&#30340;&#20215;&#20540;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#26500;&#24314;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#20197;&#36991;&#20813;&#36896;&#25104;&#20260;&#23475;&#25110;&#36829;&#21453;&#31038;&#20250;&#23545;&#21487;&#25509;&#21463;&#34892;&#20026;&#30340;&#26631;&#20934;&#65311;&#25105;&#20204;&#35748;&#20026;&#65292;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#34920;&#24449;&#23545;&#40784;&#26377;&#21161;&#20110;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#12290;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23398;&#20064;&#31867;&#20154;&#31867;&#23545;&#19990;&#30028;&#30340;&#34920;&#31034;&#20855;&#26377;&#35768;&#22810;&#24050;&#30693;&#22909;&#22788;&#65292;&#21253;&#25324;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12289;&#22686;&#24378;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#21644;&#25552;&#39640;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#34920;&#31034;&#23545;&#40784;&#20063;&#21487;&#20197;&#25903;&#25345;&#20215;&#20540;&#23545;&#40784;&#65292;&#20351;ML&#31995;&#32479;&#36981;&#24490;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#20851;&#27880;&#20262;&#29702;&#23398;&#20316;&#20026;&#20215;&#20540;&#23545;&#40784;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#24182;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#32622;&#20013;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#35757;&#32451;ML&#20195;&#29702;&#65292;&#20854;&#20013;&#22870;&#21169;&#21453;&#26144;&#25152;&#36873;&#34892;&#21160;&#30340;&#36947;&#24503;&#21487;&#25509;&#21463;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21512;&#25104;&#23454;&#39564;&#26469;&#35777;&#26126;&#20195;&#29702;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#34920;&#31034;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14106v2 Announce Type: replace  Abstract: How can we build AI systems that are aligned with human values to avoid causing harm or violating societal standards for acceptable behavior? We argue that representational alignment between humans and AI agents facilitates value alignment. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We propose that this kind of representational alignment between machine learning (ML) models and humans can also support value alignment, allowing ML systems to conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train ML agents using a variety of methods in a multi-armed bandit setting, where rewards reflect the moral acceptability of the chosen action. We use a synthetic experiment to demonstrate that agents' representational alignment with the environment bounds
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20174;&#25968;&#25454;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#21407;&#21017;&#65292;&#24182;&#21457;&#29616;&#20102;&#23616;&#37096;&#32467;&#26500;&#25509;&#36817;&#24615;&#12289;&#20840;&#23616;&#32467;&#26500;&#25509;&#36817;&#24615;&#21644;&#29305;&#24449;&#25509;&#36817;&#24615;&#19977;&#20010;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#21457;&#29616;&#20102;&#20840;&#23616;&#32467;&#26500;&#25509;&#36817;&#24615;&#21482;&#22312;&#23616;&#37096;&#32467;&#26500;&#25509;&#36817;&#24615;&#19981;&#36275;&#26102;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#29305;&#24449;&#21644;&#32467;&#26500;&#25509;&#36817;&#24615;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#38142;&#25509;&#39044;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#65292;&#21551;&#21457;&#20102;GNN4LP&#30340;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2310.00793</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38142;&#25509;&#39044;&#27979;: &#19968;&#20010;&#25968;&#25454;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Revisiting Link Prediction: A Data Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20174;&#25968;&#25454;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#21407;&#21017;&#65292;&#24182;&#21457;&#29616;&#20102;&#23616;&#37096;&#32467;&#26500;&#25509;&#36817;&#24615;&#12289;&#20840;&#23616;&#32467;&#26500;&#25509;&#36817;&#24615;&#21644;&#29305;&#24449;&#25509;&#36817;&#24615;&#19977;&#20010;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#21457;&#29616;&#20102;&#20840;&#23616;&#32467;&#26500;&#25509;&#36817;&#24615;&#21482;&#22312;&#23616;&#37096;&#32467;&#26500;&#25509;&#36817;&#24615;&#19981;&#36275;&#26102;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#29305;&#24449;&#21644;&#32467;&#26500;&#25509;&#36817;&#24615;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#38142;&#25509;&#39044;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#65292;&#21551;&#21457;&#20102;GNN4LP&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#19968;&#39033;&#22522;&#20110;&#22270;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#20363;&#22914;&#26379;&#21451;&#25512;&#33616;&#12289;&#34507;&#30333;&#36136;&#20998;&#26512;&#21644;&#33647;&#29289;&#20114;&#20316;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22810;&#20010;&#39046;&#22495;&#65292;&#23427;&#20204;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#38142;&#25509;&#24418;&#25104;&#26426;&#21046;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#35777;&#25454;&#24378;&#35843;&#20102;&#19968;&#20010;&#26222;&#36941;&#36866;&#29992;&#20110;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#31639;&#27861;&#30340;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35270;&#35282;&#25506;&#32034;&#38142;&#25509;&#39044;&#27979;&#30340;&#21407;&#21017;&#65292;&#36328;&#36234;&#19981;&#21516;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#20010;&#23545;&#38142;&#25509;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#26412;&#22240;&#32032;:&#23616;&#37096;&#32467;&#26500;&#25509;&#36817;&#24615;&#12289;&#20840;&#23616;&#32467;&#26500;&#25509;&#36817;&#24615;&#21644;&#29305;&#24449;&#25509;&#36817;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20854;&#20013; (i)&#21482;&#26377;&#22312;&#23616;&#37096;&#32467;&#26500;&#25509;&#36817;&#24615;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20840;&#23616;&#32467;&#26500;&#25509;&#36817;&#24615;&#25165;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290; (ii)&#29305;&#24449;&#21644;&#32467;&#26500;&#25509;&#36817;&#24615;&#20043;&#38388;&#23384;&#22312;&#19981;&#20860;&#23481;&#24615;&#12290;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#23548;&#33268;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN4LP) &#25345;&#32493;&#22320;
&lt;/p&gt;
&lt;p&gt;
Link prediction, a fundamental task on graphs, has proven indispensable in various applications, e.g., friend recommendation, protein analysis, and drug interaction prediction. However, since datasets span a multitude of domains, they could have distinct underlying mechanisms of link formation. Evidence in existing literature underscores the absence of a universally best algorithm suitable for all datasets. In this paper, we endeavor to explore principles of link prediction across diverse datasets from a data-centric perspective. We recognize three fundamental factors critical to link prediction: local structural proximity, global structural proximity, and feature proximity. We then unearth relationships among those factors where (i) global structural proximity only shows effectiveness when local structural proximity is deficient. (ii) The incompatibility can be found between feature and structural proximity. Such incompatibility leads to GNNs for Link Prediction (GNN4LP) consistently 
&lt;/p&gt;</description></item><item><title>&#36793;&#32536;&#21464;&#25442;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26550;&#26500;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.10119</link><description>&lt;p&gt;
&#36208;&#21521;&#22522;&#20110;&#21407;&#21017;&#30340;&#22270;&#24418;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Principled Graph Transformers. (arXiv:2401.10119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10119
&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#21464;&#25442;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26550;&#26500;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;k&#32500;Weisfeiler-Leman&#65288;k-WL&#65289;&#23618;&#27425;&#32467;&#26500;&#30340;&#22270;&#24418;&#23398;&#20064;&#26550;&#26500;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#24456;&#22909;&#29702;&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26550;&#26500;&#22312;&#30495;&#23454;&#20219;&#21153;&#20013;&#24448;&#24448;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24433;&#21709;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#22914;&#22270;&#24418;&#21464;&#25442;&#22120;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#23558;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;k-WL&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#22240;&#20026;&#36825;&#20123;&#26550;&#26500;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#26469;&#23454;&#29616;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#36793;&#32536;&#21464;&#25442;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#33410;&#28857;&#23545;&#32780;&#19981;&#26159;&#33410;&#28857;&#19978;&#36827;&#34892;&#25805;&#20316;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36793;&#32536;&#21464;&#25442;&#22120;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29702;&#35770;&#23545;&#40784;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#30001;&#35821;&#20041;&#36890;&#20449;&#22686;&#24378;&#30340;&#26080;&#32447;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20379;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#20108;&#36827;&#21046;&#20301;&#25552;&#21462;&#21644;&#20256;&#36755;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#25552;&#20379;&#26368;&#20248;AIGC&#26381;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.17705</link><description>&lt;p&gt;
&#19968;&#31181;&#30001;&#35821;&#20041;&#36890;&#20449;&#22686;&#24378;&#30340;&#26080;&#32447;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20379;&#24212;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication. (arXiv:2310.17705v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17705
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#30001;&#35821;&#20041;&#36890;&#20449;&#22686;&#24378;&#30340;&#26080;&#32447;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20379;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#20108;&#36827;&#21046;&#20301;&#25552;&#21462;&#21644;&#20256;&#36755;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#25552;&#20379;&#26368;&#20248;AIGC&#26381;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#36890;&#36807;&#21019;&#24314;&#22810;&#26679;&#21270;&#19988;&#39640;&#36136;&#37327;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#26469;&#28385;&#36275;&#24191;&#22823;&#29992;&#25143;&#32676;&#20307;&#30340;&#38656;&#27714;&#12290;&#38543;&#30528;&#31227;&#21160;&#35774;&#22791;&#30340;&#26222;&#21450;&#21644;&#31227;&#21160;&#27969;&#37327;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#36890;&#36807;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#25552;&#20379;&#23545;&#39640;&#36136;&#37327;AIGC&#26381;&#21153;&#30340;&#26080;&#22788;&#19981;&#22312;&#30340;&#35775;&#38382;&#24050;&#25104;&#20026;AIGC&#20135;&#21697;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#31283;&#23450;&#30340;&#20449;&#36947;&#12289;&#26377;&#38480;&#30340;&#24102;&#23485;&#36164;&#28304;&#21644;&#20998;&#24067;&#19981;&#22343;&#21248;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#26080;&#32447;&#32593;&#32476;&#20013;&#25552;&#20379;&#26368;&#20248;&#30340;AIGC&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#35821;&#20041;&#36890;&#20449;&#65288;SemCom&#65289;&#22686;&#24378;&#30340;AIGC&#65288;SemAIGC&#65289;&#29983;&#25104;&#21644;&#20256;&#36755;&#26694;&#26550;&#65292;&#20854;&#20013;&#21482;&#38656;&#25552;&#21462;&#21644;&#20256;&#36755;&#20869;&#23481;&#30340;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#20108;&#36827;&#21046;&#20301;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SemAIGC&#22312;&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#38598;&#25104;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20869;&#23481;&#29983;&#25104;&#21644;&#28789;&#27963;&#35843;&#25972;&#35745;&#31639;&#24037;&#20316;&#36127;&#36733;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI applications are recently catering to a vast user base by creating diverse and high-quality AI-generated content (AIGC). With the proliferation of mobile devices and rapid growth of mobile traffic, providing ubiquitous access to high-quality AIGC services via wireless communication networks is becoming the future direction for AIGC products. However, it is challenging to provide optimal AIGC services in wireless networks with unstable channels, limited bandwidth resources, and unevenly distributed computational resources. To tackle these challenges, we propose a semantic communication (SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where only semantic information of the content rather than all the binary bits should be extracted and transmitted by using SemCom. Specifically, SemAIGC integrates diffusion-based models within the semantic encoder and decoder for efficient content generation and flexible adjustment of the computing workload of both tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#32418;&#38431;&#34892;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#39304;&#24490;&#29615;&#21644;&#32972;&#26223;&#23398;&#20064;&#26469;&#35780;&#20272;&#21644;&#26292;&#38706;&#29983;&#25104;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#29305;&#21035;&#26159;&#23545;&#19981;&#23433;&#20840;&#21644;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#32972;&#26223;&#25915;&#20987;&#31574;&#30053;&#21487;&#20197;&#23398;&#20064;&#20986;&#26377;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#25552;&#31034;&#12290;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#65292;&#35813;&#31574;&#30053;&#22312;&#26292;&#38706;&#28431;&#27934;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#65292;&#29978;&#33267;&#22312;&#27169;&#22411;&#22686;&#21152;&#23433;&#20840;&#21151;&#33021;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#33021;&#22815;&#21457;&#29616;&#28431;&#27934;&#12290;&#35813;&#26694;&#26550;&#20063;&#36866;&#29992;&#20110;&#32418;&#38431;&#34892;&#21160;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.04265</link><description>&lt;p&gt;
FLIRT: &#21453;&#39304;&#24490;&#29615;&#32972;&#26223;&#19979;&#30340;&#32418;&#38431;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
FLIRT: Feedback Loop In-context Red Teaming. (arXiv:2308.04265v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#32418;&#38431;&#34892;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#39304;&#24490;&#29615;&#21644;&#32972;&#26223;&#23398;&#20064;&#26469;&#35780;&#20272;&#21644;&#26292;&#38706;&#29983;&#25104;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#29305;&#21035;&#26159;&#23545;&#19981;&#23433;&#20840;&#21644;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#32972;&#26223;&#25915;&#20987;&#31574;&#30053;&#21487;&#20197;&#23398;&#20064;&#20986;&#26377;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#25552;&#31034;&#12290;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#65292;&#35813;&#31574;&#30053;&#22312;&#26292;&#38706;&#28431;&#27934;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#65292;&#29978;&#33267;&#22312;&#27169;&#22411;&#22686;&#21152;&#23433;&#20840;&#21151;&#33021;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#33021;&#22815;&#21457;&#29616;&#28431;&#27934;&#12290;&#35813;&#26694;&#26550;&#20063;&#36866;&#29992;&#20110;&#32418;&#38431;&#34892;&#21160;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35686;&#21578;&#65306;&#26412;&#35770;&#25991;&#20869;&#23481;&#21487;&#33021;&#19981;&#21512;&#36866;&#25110;&#20882;&#29359;&#20154;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21487;&#20379;&#20844;&#20247;&#20351;&#29992;&#65292;&#27979;&#35797;&#21644;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#30340;&#28431;&#27934;&#24050;&#25104;&#20026;&#19968;&#39033;&#20248;&#20808;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#32418;&#38431;&#34892;&#21160;&#26694;&#26550;&#65292;&#23545;&#32473;&#23450;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#26292;&#38706;&#20854;&#23545;&#19981;&#23433;&#20840;&#21644;&#19981;&#36866;&#24403;&#20869;&#23481;&#29983;&#25104;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#21453;&#39304;&#24490;&#29615;&#20013;&#30340;&#32972;&#26223;&#23398;&#20064;&#26469;&#36827;&#34892;&#32418;&#38431;&#34892;&#21160;&#65292;&#24182;&#28608;&#21457;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#32972;&#26223;&#25915;&#20987;&#31574;&#30053;&#65292;&#29992;&#20110;&#33258;&#21160;&#23398;&#20064;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#26377;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#22312;&#26292;&#38706;&#31283;&#23450;&#25193;&#25955;(SD)&#27169;&#22411;&#30340;&#28431;&#27934;&#26102;&#26174;&#33879;&#26356;&#26377;&#25928;&#65292;&#21363;&#20351;&#21518;&#32773;&#37319;&#29992;&#20102;&#23433;&#20840;&#21151;&#33021;&#30340;&#22686;&#24378;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23545;&#20110;&#32418;&#38431;&#34892;&#21160;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#20063;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Warning: this paper contains content that may be inappropriate or offensive.  As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. Here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. We propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. Our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in Stable Diffusion (SD) model, even when the latter is enhanced with safety features. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text mo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;3D&#24314;&#27169;&#65292;&#21046;&#20316;&#20986;&#19982;&#26085;&#24120;&#26381;&#35013;&#32441;&#29702;&#30456;&#20284;&#30340;&#23545;&#25239;&#24615;&#20266;&#35013;&#32441;&#29702;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#35270;&#35282;&#19979;&#36991;&#24320;&#20154;&#29289;&#26816;&#27979;&#65292;&#23454;&#29616;&#33258;&#28982;&#22806;&#35266;&#30340;&#26381;&#35013;&#32441;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.01778</link><description>&lt;p&gt;
&#36890;&#36807;3D&#24314;&#27169;&#65292;&#23454;&#29616;&#33258;&#28982;&#22806;&#35266;&#30340;&#26381;&#35013;&#32441;&#29702;&#20197;&#36867;&#36991;&#20154;&#29289;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling. (arXiv:2307.01778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01778
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;3D&#24314;&#27169;&#65292;&#21046;&#20316;&#20986;&#19982;&#26085;&#24120;&#26381;&#35013;&#32441;&#29702;&#30456;&#20284;&#30340;&#23545;&#25239;&#24615;&#20266;&#35013;&#32441;&#29702;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#35270;&#35282;&#19979;&#36991;&#24320;&#20154;&#29289;&#26816;&#27979;&#65292;&#23454;&#29616;&#33258;&#28982;&#22806;&#35266;&#30340;&#26381;&#35013;&#32441;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21046;&#20316;&#23545;&#25239;&#24615;&#26381;&#35013;&#26469;&#36867;&#36991;&#20154;&#29289;&#26816;&#27979;&#22120;&#65292;&#20294;&#35201;&#20040;&#21482;&#23545;&#38480;&#23450;&#30340;&#35270;&#35282;&#26377;&#25928;&#65292;&#35201;&#20040;&#23545;&#20154;&#31867;&#38750;&#24120;&#26126;&#26174;&#12290;&#25105;&#20204;&#26088;&#22312;&#22522;&#20110;3D&#24314;&#27169;&#26469;&#21046;&#20316;&#23545;&#25239;&#24615;&#30340;&#26381;&#35013;&#32441;&#29702;&#65292;&#36825;&#20010;&#24819;&#27861;&#24050;&#32463;&#34987;&#29992;&#20110;&#21046;&#20316;&#21018;&#24615;&#30340;&#23545;&#25239;&#24615;&#29289;&#20307;&#65292;&#22914;3D&#25171;&#21360;&#30340;&#20044;&#40863;&#12290;&#19982;&#21018;&#24615;&#29289;&#20307;&#19981;&#21516;&#65292;&#20154;&#31867;&#21644;&#26381;&#35013;&#26159;&#38750;&#21018;&#24615;&#30340;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#23454;&#38469;&#21046;&#20316;&#20013;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#21046;&#20316;&#20986;&#30475;&#36215;&#26469;&#33258;&#28982;&#30340;&#23545;&#25239;&#24615;&#26381;&#35013;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#35270;&#35282;&#19979;&#36991;&#24320;&#20154;&#29289;&#26816;&#27979;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31867;&#20284;&#20110;&#26085;&#24120;&#26381;&#35013;&#32441;&#29702;&#20043;&#19968;&#30340;&#23545;&#25239;&#24615;&#20266;&#35013;&#32441;&#29702;&#65288;AdvCaT&#65289;&#65292;&#21363;&#20266;&#35013;&#32441;&#29702;&#12290;&#25105;&#20204;&#21033;&#29992;Voronoi&#22270;&#21644;Gumbel-softmax&#25216;&#24039;&#26469;&#21442;&#25968;&#21270;&#20266;&#35013;&#32441;&#29702;&#65292;&#24182;&#36890;&#36807;3D&#24314;&#27169;&#26469;&#20248;&#21270;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22686;&#24378;&#31649;&#36947;&#65292;&#23558;&#25299;&#25169;&#21512;&#29702;&#30340;&#25237;&#24433;&#65288;TopoProj&#65289;&#21644;Thin Plate Spline&#65288;TPS&#65289;&#32467;&#21512;&#22312;3D&#32593;&#26684;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have proposed to craft adversarial clothes for evading person detectors, while they are either only effective at limited viewing angles or very conspicuous to humans. We aim to craft adversarial texture for clothes based on 3D modeling, an idea that has been used to craft rigid adversarial objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes are non-rigid, leading to difficulties in physical realization. In order to craft natural-looking adversarial clothes that can evade person detectors at multiple viewing angles, we propose adversarial camouflage textures (AdvCaT) that resemble one kind of the typical textures of daily clothes, camouflage textures. We leverage the Voronoi diagram and Gumbel-softmax trick to parameterize the camouflage textures and optimize the parameters via 3D modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes combining topologically plausible projection (TopoProj) and Thin Plate Spline (TPS) to narr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.07909</link><description>&lt;p&gt;
&#29983;&#25104;AI&#20013;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Text-to-image Diffusion Model in Generative AI: A Survey. (arXiv:2303.07909v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#27969;&#34892;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#24037;&#20316;&#65292;&#26412;&#35843;&#26597;&#20174;&#31616;&#21333;&#20171;&#32461;&#22522;&#26412;&#25193;&#25955;&#27169;&#22411;&#22914;&#20309;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#24320;&#22987;&#65292;&#25509;&#30528;&#26159;&#26465;&#20214;&#25110;&#24341;&#23548;&#22914;&#20309;&#25913;&#36827;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#24635;&#32467;&#20102;&#25991;&#26412;&#24341;&#23548;&#21019;&#24847;&#29983;&#25104;&#21644;&#22270;&#20687;&#32534;&#36753;&#30340;&#24212;&#29992;&#12290;&#38500;&#20102;&#36804;&#20170;&#20026;&#27490;&#25152;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey reviews text-to-image diffusion models in the context that diffusion models have emerged to be popular for a wide range of generative tasks. As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23618;&#32423;&#32465;&#23450;&#21644;&#32852;&#24819;&#26816;&#32034;&#21464;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#22768;&#26126;&#24615;&#35760;&#24518;&#30340;&#22312;&#32447;&#39044;&#27979;&#22788;&#29702;&#31995;&#32479;&#21487;&#33021;&#20250;&#24863;&#30693;&#21040;&#33258;&#24049;&#20855;&#26377;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2301.07016</link><description>&lt;p&gt;
&#24847;&#35782;&#26159;&#23398;&#20064;&#30340;&#36807;&#31243;&#65306;&#36890;&#36807;&#32465;&#23450;&#23398;&#20064;&#30340;&#39044;&#27979;&#22788;&#29702;&#31995;&#32479;&#21487;&#33021;&#20250;&#23558;&#33258;&#24049;&#24863;&#30693;&#20026;&#26377;&#24847;&#35782;&#30340;
&lt;/p&gt;
&lt;p&gt;
Consciousness is learning: predictive processing systems that learn by binding may perceive themselves as conscious. (arXiv:2301.07016v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07016
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23618;&#32423;&#32465;&#23450;&#21644;&#32852;&#24819;&#26816;&#32034;&#21464;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#22768;&#26126;&#24615;&#35760;&#24518;&#30340;&#22312;&#32447;&#39044;&#27979;&#22788;&#29702;&#31995;&#32479;&#21487;&#33021;&#20250;&#24863;&#30693;&#21040;&#33258;&#24049;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#29305;&#23450;&#22797;&#26434;&#39046;&#22495;&#23454;&#29616;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#39640;&#25928;&#22320;&#27867;&#21270;&#20173;&#28982;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#22312;&#20154;&#31867;&#36523;&#19978;&#65292;&#36825;&#31181;&#23398;&#20064;&#36890;&#36807;&#22768;&#26126;&#24615;&#23384;&#20648;&#36807;&#31243;&#36827;&#34892;&#65292;&#24182;&#19988;&#19982;&#24847;&#35782;&#23494;&#20999;&#30456;&#20851;&#12290;&#39044;&#27979;&#22788;&#29702;&#34987;&#25512;&#24191;&#20026;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#29702;&#26694;&#26550;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#30382;&#36136;&#22914;&#20309;&#23454;&#29616;&#28145;&#24230;&#29983;&#25104;&#24863;&#30693;&#27169;&#22411;&#65292;&#29992;&#20110;&#24863;&#23448;&#25968;&#25454;&#21644;&#34892;&#20026;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#22788;&#29702;&#23545;&#20110;&#24555;&#36895;&#32452;&#25104;&#24335;&#23398;&#20064;&#25110;&#24847;&#35782;&#20043;&#35868;&#25552;&#20379;&#20102;&#24456;&#23569;&#30340;&#30452;&#25509;&#35265;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#65292;&#36890;&#36807;&#36890;&#36807;&#32465;&#23450;&#39044;&#27979;&#20013;&#30340;&#23618;&#27425;&#27169;&#22411;&#26469;&#23454;&#29616;&#22312;&#32447;&#23398;&#20064;&#65292;&#39044;&#27979;&#22788;&#29702;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#20174;&#21333;&#20010;&#31034;&#20363;&#20013;&#20026;&#24863;&#30693;&#21644;&#34892;&#21160;&#24418;&#25104;&#24037;&#20316;&#35760;&#24518;&#65292;&#22312;&#26032;&#24773;&#20917;&#19979;&#28789;&#27963;&#27867;&#21270;&#65292;&#36825;&#21487;&#36890;&#36807;&#32852;&#24819;&#26816;&#32034;&#21464;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#30340;&#22768;&#26126;&#24615;&#35760;&#24518;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#22312;&#32447;&#23618;&#32423;&#39044;&#27979;&#32465;&#23450;&#8221;&#65292;&#20063;&#21487;&#33021;&#26159;&#31995;&#32479;&#24863;&#30693;&#33258;&#24049;&#20855;&#26377;&#24847;&#35782;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#20851;&#20110;&#24863;&#30693;&#30340;&#12289;&#36816;&#21160;&#30340;&#12289;&#35748;&#30693;&#30340;&#21644;&#24773;&#24863;&#30340;&#24847;&#35782;&#30340;&#32479;&#19968;&#35299;&#37322;&#65292;&#24182;&#20855;&#26377;&#36827;&#21270;&#21644;&#21457;&#32946;&#29983;&#29289;&#23398;&#30340;&#28145;&#21051;&#26681;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have achieved superhuman performance in specific complex domains. Yet learning online from few examples and efficiently generalizing across domains remains elusive. In humans such learning proceeds via declarative memory formation and is closely associated with consciousness. Predictive processing has been advanced as a principled Bayesian inference framework for understanding the cortex as implementing deep generative perceptual models for both sensory data and action control. However, predictive processing offers little direct insight into fast compositional learning or the mystery of consciousness. Here we propose that through implementing online learning by hierarchical binding of unpredicted inferences, a predictive processing system may flexibly generalize in novel situations by forming working memories for perceptions and actions from single examples, which can become short- and long-term declarative memories retrievable by associative recall. We argu
&lt;/p&gt;</description></item></channel></rss>