<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#37325;&#26032;&#24605;&#32771;Kullback-Leibler&#25955;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kullback-Leibler&#25955;&#24230;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#65292;&#22312;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#19982;&#20043;&#21069;&#26029;&#35328;&#30340;&#36870;Kullback-Leibler&#65288;RKL&#65289;&#25955;&#24230;&#23547;&#25214;&#27169;&#24335;&#24182;&#22240;&#27492;&#20248;&#20110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#27491;&#21521;Kullback-Leibler&#65288;FKL&#65289;&#25955;&#24230;&#30456;&#21453;&#65292;&#23454;&#38469;&#19978;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#37117;&#27809;&#26377;&#20307;&#29616;&#20986;&#23547;&#25214;&#27169;&#24335;&#25110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#29305;&#24615;&#12290;&#30456;&#21453;&#65292;&#21457;&#29616;RKL&#21644;FKL&#20855;&#26377;&#30456;&#21516;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#22312;&#36275;&#22815;&#25968;&#37327;&#30340;&#26102;&#20195;&#20043;&#21518;&#37117;&#20250;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#32422;&#26463;&#65292;LLMs&#24456;&#23569;&#34987;&#35757;&#32451;&#22914;&#27492;&#22810;&#30340;&#26102;&#20195;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;RKL&#22312;&#20998;&#24067;&#30340;&#23614;&#37096;&#65292;&#32780;FKL&#22312;&#24320;&#22987;&#26102;&#20195;&#20391;&#37325;&#20110;&#20998;&#24067;&#30340;&#22836;&#37096;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;Kullback-Leiber&#65288;AKL&#65289;&#25955;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#26435;&#37325;&#26469;&#32452;&#21512;F
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02657v1 Announce Type: cross  Abstract: Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine F
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#37327;&#21270;&#22122;&#22768;&#26657;&#27491;&#26041;&#26696;&#65288;QNCD&#65289;&#65292;&#26088;&#22312;&#20943;&#23567;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#22122;&#22768;&#65292;&#35299;&#20915;&#20102;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#37319;&#26679;&#21152;&#36895;&#30340;&#24433;&#21709;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19140</link><description>&lt;p&gt;
QNCD&#65306;&#25193;&#25955;&#27169;&#22411;&#30340;&#37327;&#21270;&#22122;&#22768;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
QNCD: Quantization Noise Correction for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19140
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#37327;&#21270;&#22122;&#22768;&#26657;&#27491;&#26041;&#26696;&#65288;QNCD&#65289;&#65292;&#26088;&#22312;&#20943;&#23567;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#22122;&#22768;&#65292;&#35299;&#20915;&#20102;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#37319;&#26679;&#21152;&#36895;&#30340;&#24433;&#21709;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#24314;&#31435;&#20102;&#36136;&#37327;&#21644;&#21019;&#36896;&#21147;&#30340;&#26032;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36845;&#20195;&#21435;&#22122;&#36807;&#31243;&#20013;&#38656;&#35201;&#30340;&#23494;&#38598;&#35745;&#31639;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#21152;&#36895;&#37319;&#26679;&#65292;&#23613;&#31649;&#20197;&#20302;&#27604;&#29305;&#35774;&#32622;&#26497;&#22823;&#38477;&#20302;&#20102;&#26679;&#26412;&#36136;&#37327;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#37327;&#21270;&#22122;&#22768;&#26657;&#27491;&#26041;&#26696;&#65288;QNCD&#65289;&#65292;&#26088;&#22312;&#20943;&#23567;&#25972;&#20010;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#37327;&#21270;&#22122;&#22768;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#37327;&#21270;&#25361;&#25112;&#65306;&#20869;&#37096;&#21644;&#22806;&#37096;&#37327;&#21270;&#22122;&#22768;&#12290;&#20869;&#37096;&#37327;&#21270;&#22122;&#22768;&#20027;&#35201;&#30001;&#20110;&#23884;&#20837;&#22312;resblock&#27169;&#22359;&#20013;&#32780;&#21152;&#21095;&#65292;&#25193;&#23637;&#20102;&#28608;&#27963;&#37327;&#21270;&#33539;&#22260;&#65292;&#22312;&#27599;&#20010;&#21333;&#29420;&#30340;&#21435;&#22122;&#27493;&#39588;&#20013;&#22686;&#21152;&#20102;&#24178;&#25200;&#12290;&#27492;&#22806;&#65292;&#22806;&#37096;&#37327;&#21270;&#22122;&#22768;&#28304;&#33258;&#25972;&#20010;&#21435;&#22122;&#36807;&#31243;&#20013;&#30340;&#32047;&#31215;&#37327;&#21270;&#20559;&#24046;&#65292;&#25913;&#21464;&#20102;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19140v1 Announce Type: cross  Abstract: Diffusion models have revolutionized image synthesis, setting new benchmarks in quality and creativity. However, their widespread adoption is hindered by the intensive computation required during the iterative denoising process. Post-training quantization (PTQ) presents a solution to accelerate sampling, aibeit at the expense of sample quality, extremely in low-bit settings. Addressing this, our study introduces a unified Quantization Noise Correction Scheme (QNCD), aimed at minishing quantization noise throughout the sampling process. We identify two primary quantization challenges: intra and inter quantization noise. Intra quantization noise, mainly exacerbated by embeddings in the resblock module, extends activation quantization ranges, increasing disturbances in each single denosing step. Besides, inter quantization noise stems from cumulative quantization deviations across the entire denoising process, altering data distributions 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#12290;&#36890;&#36807;&#32508;&#36848;&#25991;&#29486;&#21644;&#19987;&#23478;&#21672;&#35810;&#65292;&#26500;&#24314;&#20102;&#25968;&#25454;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#29983;&#25104;&#20102;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;MLLM&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#20027;&#27969;&#25945;&#32946;&#20013;&#30340;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#20559;&#37325;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.06264</link><description>&lt;p&gt;
LLaVA-Docent&#65306;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#30340;&#25945;&#23398;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#12290;&#36890;&#36807;&#32508;&#36848;&#25991;&#29486;&#21644;&#19987;&#23478;&#21672;&#35810;&#65292;&#26500;&#24314;&#20102;&#25968;&#25454;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#29983;&#25104;&#20102;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;MLLM&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#20027;&#27969;&#25945;&#32946;&#20013;&#30340;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#20559;&#37325;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#37492;&#36175;&#23545;&#20110;&#22521;&#20859;&#23398;&#20064;&#32773;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#24773;&#24863;&#26234;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#24120;&#38754;&#20020;&#33402;&#26415;&#36164;&#28304;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24369;&#21183;&#23398;&#29983;&#65292;&#24182;&#19988;&#22312;&#20027;&#27969;&#25945;&#32946;&#20013;&#36807;&#24230;&#24378;&#35843;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#31185;&#30446;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#25216;&#26415;&#36827;&#27493;&#20026;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#26469;&#21033;&#29992;&#36825;&#20123;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#21672;&#35810;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#25968;&#25454;&#26694;&#26550;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;GPT-4&#21033;&#29992;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;MLLM&#65288;&#21363;LLaVA-Docent&#65289;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#20845;&#21517;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Art appreciation is vital in nurturing critical thinking and emotional intelligence among learners. However, traditional art appreciation education has often been hindered by limited access to art resources, especially for disadvantaged students, and an imbalanced emphasis on STEM subjects in mainstream education. In response to these challenges, recent technological advancements have paved the way for innovative solutions. This study explores the application of multi-modal large language models (MLLMs) in art appreciation education, focusing on developing LLaVA-Docent, a model that leverages these advancements. Our approach involved a comprehensive literature review and consultations with experts in the field, leading to developing a robust data framework. Utilizing this framework, we generated a virtual dialogue dataset that was leveraged by GPT-4. This dataset was instrumental in training the MLLM, named LLaVA-Docent. Six researchers conducted quantitative and qualitative evaluation
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20934;&#30830;&#21448;&#26131;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#23454;&#38469;&#19978;&#36229;&#36807;&#20102;&#21442;&#32771;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05680</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interpretable classifiers for tabular data via discretization and feature selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20934;&#30830;&#21448;&#26131;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#23454;&#38469;&#19978;&#36229;&#36807;&#20102;&#21442;&#32771;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#20998;&#31867;&#22120;&#26159;&#31616;&#30701;&#30340;DNF&#20844;&#24335;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#25968;&#25454;&#31163;&#25955;&#21270;&#20026;&#24067;&#23572;&#24418;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#32467;&#21512;&#38750;&#24120;&#24555;&#36895;&#30340;&#31639;&#27861;&#26469;&#20135;&#29983;&#26368;&#20339;&#30340;&#24067;&#23572;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;14&#20010;&#23454;&#39564;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#65292;&#24471;&#21040;&#30340;&#32467;&#26524;&#30340;&#20934;&#30830;&#24230;&#20027;&#35201;&#19982;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#20197;&#21450;&#25991;&#29486;&#20013;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#29616;&#26377;&#32467;&#26524;&#30456;&#20284;&#12290;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#38469;&#19978;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#21442;&#32771;&#32467;&#26524;&#65292;&#23613;&#31649;&#25105;&#20204;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#30340;&#21363;&#26102;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20010;&#20851;&#20110;&#20174;&#29616;&#23454;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#20998;&#31867;&#22120;&#19982;&#26469;&#33258;&#25968;&#25454;&#32972;&#26223;&#20998;&#24067;&#30340;&#26368;&#20339;&#20998;&#31867;&#22120;&#30456;&#23545;&#24212;&#30340;&#27010;&#29575;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method for computing immediately human interpretable yet accurate classifiers from tabular data. The classifiers obtained are short DNF-formulas, computed via first discretizing the original data to Boolean form and then using feature selection coupled with a very fast algorithm for producing the best possible Boolean classifier for the setting. We demonstrate the approach via 14 experiments, obtaining results with accuracies mainly similar to ones obtained via random forests, XGBoost, and existing results for the same datasets in the literature. In several cases, our approach in fact outperforms the reference results in relation to accuracy, even though the main objective of our study is the immediate interpretability of our classifiers. We also prove a new result on the probability that the classifier we obtain from real-life data corresponds to the ideally best classifier with respect to the background distribution the data comes from.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;TSIS&#31639;&#27861;&#20316;&#20026;t-SMILES&#30340;&#34917;&#20805;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TSIS&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#27861;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02164</link><description>&lt;p&gt;
TSIS: t-SMILES&#30340;&#34917;&#20805;&#31639;&#27861;&#29992;&#20110;&#22522;&#20110;&#29255;&#27573;&#30340;&#20998;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
TSIS: A Supplementary Algorithm to t-SMILES for Fragment-based Molecular Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;TSIS&#31639;&#27861;&#20316;&#20026;t-SMILES&#30340;&#34917;&#20805;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TSIS&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#27861;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#31526;&#20018;&#22522;&#26412;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#22914;SMILES&#65292;&#22312;&#32447;&#24615;&#34920;&#31034;&#20998;&#23376;&#20449;&#24687;&#26041;&#38754;&#26159;&#20107;&#23454;&#19978;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#20351;&#29992;&#37197;&#23545;&#31526;&#21495;&#21644;&#35299;&#26512;&#31639;&#27861;&#23548;&#33268;&#20102;&#38271;&#30340;&#35821;&#27861;&#20381;&#36182;&#20851;&#31995;&#65292;&#20351;&#24471;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20063;&#38590;&#20197;&#20934;&#30830;&#29702;&#35299;&#35821;&#27861;&#21644;&#35821;&#20041;&#12290;&#23613;&#31649;DeepSMILES&#21644;SELFIES&#24050;&#32463;&#35299;&#20915;&#20102;&#26576;&#20123;&#38480;&#21046;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#22788;&#29702;&#39640;&#32423;&#35821;&#27861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20351;&#24471;&#19968;&#20123;&#23383;&#31526;&#20018;&#38590;&#20197;&#38405;&#35835;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#34917;&#20805;&#31639;&#27861;TSIS&#65288;TSID&#31616;&#21270;&#65289;&#65292;&#29992;&#20110;t-SMILES&#23478;&#26063;&#12290;TSIS&#19982;&#21478;&#19968;&#20010;&#22522;&#20110;&#29255;&#27573;&#30340;&#32447;&#24615;&#35299;&#20915;&#26041;&#26696;SAFE&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SAFE&#22312;&#22788;&#29702;&#35821;&#27861;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26102;&#23384;&#22312;&#25361;&#25112;&#12290;TSIS&#32487;&#32493;&#20351;&#29992;t-SMILES&#20013;&#23450;&#20041;&#30340;&#26641;&#20316;&#20026;&#20854;&#22522;&#30784;&#25968;&#25454;&#32467;&#26500;&#65292;&#36825;&#20351;&#20854;&#19982;SAFE&#27169;&#22411;&#26377;&#25152;&#19981;&#21516;&#12290;TSIS&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;SAFE&#27169;&#22411;&#65292;&#34920;&#26126;t-SMILES&#30340;&#26641;&#32467;&#26500;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
String-based molecular representations, such as SMILES, are a de facto standard for linearly representing molecular information. However, the must be paired symbols and the parsing algorithm result in long grammatical dependencies, making it difficult for even state-of-the-art deep learning models to accurately comprehend the syntax and semantics. Although DeepSMILES and SELFIES have addressed certain limitations, they still struggle with advanced grammar, which makes some strings difficult to read. This study introduces a supplementary algorithm, TSIS (TSID Simplified), to t-SMILES family. Comparative experiments between TSIS and another fragment-based linear solution, SAFE, indicate that SAFE presents challenges in managing long-term dependencies in grammar. TSIS continues to use the tree defined in t-SMILES as its foundational data structure, which sets it apart from the SAFE model. The performance of TSIS models surpasses that of SAFE models, indicating that the tree structure of t
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#23458;&#39532;&#25289;&#26494;&#22312;&#36719;&#20214;&#34892;&#19994;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#22312;&#25945;&#32946;&#39046;&#22495;&#24102;&#26469;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.17434</link><description>&lt;p&gt;
&#22312;&#40657;&#23458;&#39532;&#25289;&#26494;&#20013;&#38598;&#25104;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;: &#26426;&#36935;&#65292;&#25361;&#25112;&#21644;&#25945;&#32946;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Integrating Generative AI in Hackathons: Opportunities, Challenges, and Educational Implications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17434
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#23458;&#39532;&#25289;&#26494;&#22312;&#36719;&#20214;&#34892;&#19994;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#22312;&#25945;&#32946;&#39046;&#22495;&#24102;&#26469;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#23458;&#39532;&#25289;&#26494;&#21644;&#36719;&#20214;&#31454;&#36187;&#22312;&#36719;&#20214;&#34892;&#19994;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#23427;&#20204;&#23545;&#32452;&#32455;&#21644;&#23398;&#29983;&#30340;&#21019;&#26032;&#21644;&#25216;&#33021;&#21457;&#23637;&#36215;&#21040;&#37325;&#35201;&#25512;&#21160;&#20316;&#29992;&#12290;&#36825;&#20123;&#24179;&#21488;&#20351;&#20844;&#21496;&#33021;&#22815;&#36805;&#36895;&#21407;&#22411;&#21270;&#24819;&#27861;&#65292;&#32780;&#23398;&#29983;&#21017;&#33719;&#24471;&#20016;&#23500;&#30340;&#23398;&#20064;&#32463;&#39564;&#65292;&#22686;&#24378;&#20182;&#20204;&#30340;&#23454;&#36341;&#25216;&#33021;&#12290;&#22810;&#24180;&#26469;&#65292;&#40657;&#23458;&#39532;&#25289;&#26494;&#24050;&#32463;&#20174;&#31616;&#21333;&#30340;&#31454;&#20105;&#27963;&#21160;&#36716;&#21464;&#20026;&#37325;&#35201;&#30340;&#25945;&#32946;&#24037;&#20855;&#65292;&#23558;&#29702;&#35770;&#30693;&#35782;&#19982;&#23454;&#38469;&#38382;&#39064;&#35299;&#20915;&#30456;&#32467;&#21512;&#12290;&#23558;&#40657;&#23458;&#39532;&#25289;&#26494;&#32435;&#20837;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#36719;&#20214;&#24037;&#31243;&#35838;&#31243;&#30340;&#25972;&#21512;&#26088;&#22312;&#22312;&#21512;&#20316;&#30340;&#29615;&#22659;&#20013;&#23545;&#40784;&#25945;&#32946;&#33021;&#21147;&#65292;&#36890;&#36807;&#20135;&#23398;&#21512;&#20316;&#20419;&#36827;&#21516;&#34892;&#20043;&#38388;&#30340;&#36830;&#25509;&#21644;&#20016;&#23500;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#39640;&#32423;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#34701;&#21512;&#36827;&#40657;&#23458;&#39532;&#25289;&#26494;&#27491;&#22312;&#25913;&#21464;&#23427;&#20204;&#30340;&#32467;&#26500;&#21644;&#32467;&#26524;&#12290;&#36825;&#31181;&#28436;&#21464;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#22914;&#22686;&#24378;&#30340;&#23398;&#20064;&#20307;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Hackathons and software competitions, increasingly pivotal in the software industry, serve as vital catalysts for innovation and skill development for both organizations and students. These platforms enable companies to prototype ideas swiftly, while students gain enriched learning experiences, enhancing their practical skills. Over the years, hackathons have transitioned from mere competitive events to significant educational tools, fusing theoretical knowledge with real-world problem-solving. The integration of hackathons into computer science and software engineering curricula aims to align educational proficiencies within a collaborative context, promoting peer connectivity and enriched learning via industry-academia collaborations. However, the infusion of advanced technologies, notably artificial intelligence (AI), and machine learning, into hackathons is revolutionizing their structure and outcomes. This evolution brings forth both opportunities, like enhanced learning experienc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20116;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;&#31574;&#30053;&#26469;&#24314;&#31435;&#31185;&#23398;&#30740;&#31350;&#20013;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;&#12290;&#20840;&#29699;&#20849;&#35782;&#12289;&#19987;&#19994;&#22521;&#35757;&#21644;&#21512;&#29702;&#30340;&#25191;&#34892;&#23545;&#20110;&#20419;&#36827;AI&#30340;&#30410;&#22788;&#21644;&#32500;&#25252;&#30740;&#31350;&#35802;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.15284</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#24314;&#31435;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Building ethical guidelines for generative AI in scientific research. (arXiv:2401.15284v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20116;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;&#31574;&#30053;&#26469;&#24314;&#31435;&#31185;&#23398;&#30740;&#31350;&#20013;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;&#12290;&#20840;&#29699;&#20849;&#35782;&#12289;&#19987;&#19994;&#22521;&#35757;&#21644;&#21512;&#29702;&#30340;&#25191;&#34892;&#23545;&#20110;&#20419;&#36827;AI&#30340;&#30410;&#22788;&#21644;&#32500;&#25252;&#30740;&#31350;&#35802;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#27491;&#22312;&#36805;&#36895;&#25913;&#21464;&#23398;&#26415;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#31185;&#23398;&#20013;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;&#30340;&#35752;&#35770;&#20173;&#28982;&#38646;&#25955;&#65292;&#24378;&#35843;&#20102;&#21327;&#21830;&#19968;&#33268;&#24615;&#26631;&#20934;&#30340;&#32039;&#36843;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20116;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;&#31574;&#30053;&#30340;&#24320;&#21457;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65306;&#20102;&#35299;&#27169;&#22411;&#22312;&#30495;&#23454;&#24615;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65307;&#23562;&#37325;&#38544;&#31169;&#12289;&#26426;&#23494;&#21644;&#29256;&#26435;&#65307;&#22312;&#34701;&#20837;&#27169;&#22411;&#36755;&#20986;&#26102;&#36991;&#20813;&#25220;&#34989;&#21644;&#36829;&#21453;&#25919;&#31574;&#65307;&#30830;&#20445;&#24212;&#29992;&#24102;&#26469;&#24635;&#20307;&#21033;&#30410;&#65307;&#20197;&#21450;&#36879;&#26126;&#12289;&#21487;&#22797;&#21046;&#22320;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#12290;&#36890;&#36807;&#21015;&#20030;&#24120;&#35265;&#22330;&#26223;&#26469;&#23637;&#31034;&#28508;&#22312;&#30340;&#20262;&#29702;&#36829;&#35268;&#34892;&#20026;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20840;&#29699;&#20849;&#35782;&#20197;&#21450;&#19987;&#19994;&#22521;&#35757;&#21644;&#21512;&#29702;&#30340;&#25191;&#34892;&#26159;&#20419;&#36827;AI&#30340;&#30410;&#22788;&#24182;&#32500;&#25252;&#30740;&#31350;&#35802;&#20449;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative artificial intelligence tools like large language models are rapidly transforming academic research and real world applications. However, discussions on ethical guidelines for generative AI in science remain fragmented, underscoring the urgent need for consensus based standards. This paper offers an initial framework by developing analyses and mitigation strategies across five key themes: understanding model limitations regarding truthfulness and bias; respecting privacy, confidentiality, and copyright; avoiding plagiarism and policy violations when incorporating model output; ensuring applications provide overall benefit; and using AI transparently and reproducibly. Common scenarios are outlined to demonstrate potential ethical violations. We argue that global consensus coupled with professional training and reasonable enforcement are critical to promoting the benefits of AI while safeguarding research integrity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#30340;&#26159;&#35774;&#35745;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#26469;&#24179;&#34913;&#34987;&#25298;&#32477;&#30340;&#39044;&#27979;&#27604;&#20363;&#21644;&#25152;&#36873;&#39044;&#27979;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12708</link><description>&lt;p&gt;
&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Network Benchmarks for Selective Classification. (arXiv:2401.12708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#30340;&#26159;&#35774;&#35745;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#26469;&#24179;&#34913;&#34987;&#25298;&#32477;&#30340;&#39044;&#27979;&#27604;&#20363;&#21644;&#25152;&#36873;&#39044;&#27979;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20855;&#26377;&#31038;&#20250;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#30340;&#37096;&#32626;&#22686;&#21152;&#65292;&#23545;&#21487;&#38752;&#21644;&#21487;&#20449;&#39044;&#27979;&#30340;&#38656;&#27714;&#20063;&#26085;&#30410;&#22686;&#38271;&#12290;&#23454;&#29616;&#36825;&#20123;&#35201;&#27714;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20801;&#35768;&#27169;&#22411;&#22312;&#23384;&#22312;&#39640;&#38169;&#35823;&#39118;&#38505;&#26102;&#25918;&#24323;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#38656;&#35201;&#20026;&#27169;&#22411;&#28155;&#21152;&#36873;&#25321;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#36873;&#25321;&#27169;&#22411;&#23558;&#25552;&#20379;&#39044;&#27979;&#30340;&#20363;&#23376;&#12290;&#36873;&#25321;&#24615;&#20998;&#31867;&#26694;&#26550;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#24179;&#34913;&#34987;&#25298;&#32477;&#39044;&#27979;&#27604;&#20363;&#65288;&#21363;&#27169;&#22411;&#19981;&#36827;&#34892;&#39044;&#27979;&#30340;&#20363;&#23376;&#27604;&#20363;&#65289;&#19982;&#22312;&#25152;&#36873;&#39044;&#27979;&#19978;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#20043;&#38388;&#30340;&#26426;&#21046;&#12290;&#23384;&#22312;&#22810;&#20010;&#36873;&#25321;&#24615;&#20998;&#31867;&#26694;&#26550;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#20173;&#23616;&#38480;&#20110;&#37096;&#20998;&#26041;&#27861;&#21644;&#35774;&#32622;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#32473;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#24456;&#23569;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing deployment of machine learning models in many socially-sensitive tasks, there is a growing demand for reliable and trustworthy predictions. One way to accomplish these requirements is to allow a model to abstain from making a prediction when there is a high risk of making an error. This requires adding a selection mechanism to the model, which selects those examples for which the model will provide a prediction. The selective classification framework aims to design a mechanism that balances the fraction of rejected predictions (i.e., the proportion of examples for which the model does not make a prediction) versus the improvement in predictive performance on the selected predictions. Multiple selective classification frameworks exist, most of which rely on deep neural network architectures. However, the empirical evaluation of the existing approaches is still limited to partial comparisons among methods and settings, providing practitioners with little insight into 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.11973</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;: &#38754;&#21521;&#35270;&#39057;&#34920;&#31034;&#30340;&#20813;&#36951;&#24536;&#20248;&#32988;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65288;LTH&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#20551;&#35774;&#24378;&#35843;&#22312;&#36739;&#22823;&#30340;&#23494;&#38598;&#32593;&#32476;&#20013;&#23384;&#22312;&#39640;&#25928;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#30340;&#31232;&#30095;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#31168;&#30340;&#20248;&#32988;&#23376;&#32593;&#32476;&#65288;WSN&#65289;&#22312;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#26469;&#33258;&#23494;&#38598;&#32593;&#32476;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#26435;&#37325;&#65292;&#22312;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#65288;TIL&#65289;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#36719;&#23376;&#32593;&#32476;&#65288;SoftNet&#65289;&#30340;WSN&#21464;&#20307;&#65292;&#20197;&#38450;&#27490;&#25968;&#25454;&#26679;&#26412;&#31232;&#32570;&#26102;&#30340;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#20102;WSN&#26435;&#37325;&#30340;&#31232;&#30095;&#37325;&#29992;&#65292;&#29992;&#20110;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65288;VIL&#65289;&#12290;&#32771;&#34385;&#20102;&#22312;WSN&#20013;&#20351;&#29992;&#20613;&#31435;&#21494;&#23376;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FSO&#65289;&#65292;&#23427;&#33021;&#22815;&#23545;&#35270;&#39057;&#36827;&#34892;&#32039;&#20945;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#24102;&#23485;&#19979;&#35782;&#21035;&#21487;&#37325;&#29992;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;FSO&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#21253;&#25324;VIL&#12289;TIL&#21644;FSCIL&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#29616;&#35937;&#65292;&#21457;&#29616;&#22266;&#23450;&#19981;&#21464;&#30340;&#23376;&#31354;&#38388;&#23548;&#33268;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#12290;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#24179;&#28369;&#21521;&#37327;&#30340;&#23384;&#22312;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16800</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31561;&#32423;&#23849;&#22604;&#23548;&#33268;&#24179;&#28369;&#36807;&#24230;&#21644;&#20851;&#32852;&#36807;&#39640;
&lt;/p&gt;
&lt;p&gt;
Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks. (arXiv:2308.16800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#29616;&#35937;&#65292;&#21457;&#29616;&#22266;&#23450;&#19981;&#21464;&#30340;&#23376;&#31354;&#38388;&#23548;&#33268;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#12290;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#24179;&#28369;&#21521;&#37327;&#30340;&#23384;&#22312;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#30340;&#26032;&#29702;&#35770;&#35265;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22266;&#23450;&#19981;&#21464;&#23376;&#31354;&#38388;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23427;&#34920;&#29616;&#20986;&#19968;&#31181;&#30456;&#23545;&#30340;&#34892;&#20026;&#65292;&#19981;&#21463;&#29305;&#24449;&#36716;&#25442;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38416;&#26126;&#20102;&#19982;&#25910;&#25947;&#21040;&#24120;&#25968;&#29366;&#24577;&#21644;&#33410;&#28857;&#29366;&#24577;&#30340;&#36807;&#20998;&#20998;&#31163;&#30456;&#20851;&#30340;&#26368;&#26032;&#35266;&#23519;&#32467;&#26524;&#65292;&#22240;&#20026;&#23376;&#31354;&#38388;&#30340;&#25918;&#22823;&#21482;&#21462;&#20915;&#20110;&#32858;&#21512;&#20989;&#25968;&#30340;&#39057;&#35889;&#12290;&#22312;&#32447;&#24615;&#22330;&#26223;&#20013;&#65292;&#36825;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#30001;&#20302;&#32500;&#23376;&#31354;&#38388;&#20027;&#23548;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#29305;&#24449;&#36716;&#25442;&#26080;&#20851;&#30340;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#12290;&#24403;&#24179;&#28369;&#21521;&#37327;&#36328;&#36234;&#36825;&#20010;&#23376;&#31354;&#38388;&#26102;&#65292;&#36825;&#20250;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#22312;&#25105;&#20204;&#30340;&#29702;&#35770;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#30410;&#29305;&#24615;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#39640;&#20851;&#32852;&#21644;&#31561;&#32423;&#23849;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our study reveals new theoretical insights into over-smoothing and feature over-correlation in deep graph neural networks. We show the prevalence of invariant subspaces, demonstrating a fixed relative behavior that is unaffected by feature transformations. Our work clarifies recent observations related to convergence to a constant state and a potential over-separation of node states, as the amplification of subspaces only depends on the spectrum of the aggregation function. In linear scenarios, this leads to node representations being dominated by a low-dimensional subspace with an asymptotic convergence rate independent of the feature transformations. This causes a rank collapse of the node representations, resulting in over-smoothing when smooth vectors span this subspace, and over-correlation even when over-smoothing is avoided. Guided by our theory, we propose a sum of Kronecker products as a beneficial property that can provably prevent over-smoothing, over-correlation, and rank c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#19979;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#12289;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16534</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints. (arXiv:2308.16534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#19979;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#12289;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#21644;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26465;&#20214;&#21644;&#38750;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26465;&#20214;&#29983;&#25104;&#22522;&#20110;&#29305;&#23450;&#35757;&#32451;&#30340;&#26465;&#20214;&#27169;&#22411;&#25110;&#20998;&#31867;&#22120;&#25351;&#23548;&#65292;&#36825;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#22122;&#22768;&#20381;&#36182;&#30340;&#20998;&#31867;&#22120;&#65292;&#21363;&#20351;&#23545;&#20110;&#26410;&#25439;&#22351;&#25968;&#25454;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#32473;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#26465;&#20214;&#35780;&#20998;&#29983;&#25104;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25805;&#32437;&#23398;&#20064;&#24471;&#21040;&#30340;&#35780;&#20998;&#65292;&#20197;&#20415;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#20174;&#38750;&#24402;&#19968;&#21270;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#28789;&#27963;&#32780;&#25968;&#20540;&#31283;&#23450;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#29992;&#20110;&#32534;&#30721;&#36719;&#36923;&#36753;&#32422;&#26463;&#12290;&#23558;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#20294;&#26159;&#36817;&#20284;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25913;&#36827;&#36817;&#20284;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based and diffusion models have emerged as effective approaches for both conditional and unconditional generation. Still conditional generation is based on either a specific training of a conditional model or classifier guidance, which requires training a noise-dependent classifier, even when the classifier for uncorrupted data is given. We propose an approach to sample from unconditional score-based generative models enforcing arbitrary logical constraints, without any additional training. Firstly, we show how to manipulate the learned score in order to sample from an un-normalized distribution conditional on a user-defined constraint. Then, we define a flexible and numerically stable neuro-symbolic framework for encoding soft logical constraints. Combining these two ingredients we obtain a general, but approximate, conditional sampling algorithm. We further developed effective heuristics aimed at improving the approximation. Finally, we show the effectiveness of our approach fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;GPT-4&#22312;&#31185;&#23398;&#21644;&#25968;&#23398;&#38382;&#39064;&#19978;&#20351;&#29992;Wolfram Alpha&#21644;Code Interpreter&#25554;&#20214;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#25554;&#20214;&#26174;&#33879;&#25552;&#21319;&#20102;GPT&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#25509;&#21475;&#25925;&#38556;&#20173;&#28982;&#26159;&#20854;&#21487;&#38752;&#24615;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.05713</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#25968;&#23398;&#21644;&#31185;&#23398;&#38382;&#39064;&#19978;&#20351;&#29992;Wolfram Alpha&#21644;Code Interpreter&#25554;&#20214;&#27979;&#35797;GPT-4
&lt;/p&gt;
&lt;p&gt;
Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems. (arXiv:2308.05713v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;GPT-4&#22312;&#31185;&#23398;&#21644;&#25968;&#23398;&#38382;&#39064;&#19978;&#20351;&#29992;Wolfram Alpha&#21644;Code Interpreter&#25554;&#20214;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#25554;&#20214;&#26174;&#33879;&#25552;&#21319;&#20102;GPT&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#25509;&#21475;&#25925;&#38556;&#20173;&#28982;&#26159;&#20854;&#21487;&#38752;&#24615;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#25551;&#36848;&#20102;&#22312;2023&#24180;6&#26376;&#33267;8&#26376;&#26399;&#38388;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#31185;&#23398;&#21644;&#25968;&#23398;&#39046;&#22495;&#36827;&#34892;&#30340;105&#20010;&#21407;&#21019;&#38382;&#39064;&#30340;&#27979;&#35797;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;Wolfram Alpha&#21644;Code Interpreter&#25554;&#20214;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#36825;&#20123;&#25554;&#20214;&#26174;&#33879;&#22686;&#24378;&#20102;GPT&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#32463;&#24120;&#20986;&#29616;&#8220;&#25509;&#21475;&#8221;&#25925;&#38556;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;GPT&#32463;&#24120;&#22312;&#38382;&#39064;&#30340;&#34920;&#36848;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#26080;&#27861;&#20174;&#25554;&#20214;&#20013;&#24471;&#21040;&#26377;&#29992;&#30340;&#31572;&#26696;&#12290;&#35299;&#20915;&#36825;&#20123;&#25509;&#21475;&#25925;&#38556;&#20284;&#20046;&#26159;&#20351;GPT&#25104;&#20026;&#21487;&#38752;&#30340;&#22823;&#23398;&#32423;&#35745;&#31639;&#38382;&#39064;&#24037;&#20855;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report describes a test of the large language model GPT-4 with the Wolfram Alpha and the Code Interpreter plug-ins on 105 original problems in science and math, at the high school and college levels, carried out in June-August 2023. Our tests suggest that the plug-ins significantly enhance GPT's ability to solve these problems. Having said that, there are still often "interface" failures; that is, GPT often has trouble formulating problems in a way that elicits useful answers from the plug-ins. Fixing these interface failures seems like a central challenge in making GPT a reliable tool for college-level calculation problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AI&#20215;&#20540;&#38142;&#30340;&#27010;&#24565;&#65292;&#20197;&#28385;&#36275;AI&#20262;&#29702;&#30740;&#31350;&#21644;&#24178;&#39044;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;AI&#20215;&#20540;&#38142;&#28041;&#21450;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#26041;&#21521;&#65292;&#26088;&#22312;&#25512;&#21160;&#26356;&#20855;&#20262;&#29702;&#30340;AI&#21457;&#23637;&#21644;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.16787</link><description>&lt;p&gt;
AI&#20215;&#20540;&#38142;&#30340;&#20262;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Ethics of AI Value Chains. (arXiv:2307.16787v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AI&#20215;&#20540;&#38142;&#30340;&#27010;&#24565;&#65292;&#20197;&#28385;&#36275;AI&#20262;&#29702;&#30740;&#31350;&#21644;&#24178;&#39044;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;AI&#20215;&#20540;&#38142;&#28041;&#21450;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#26041;&#21521;&#65292;&#26088;&#22312;&#25512;&#21160;&#26356;&#20855;&#20262;&#29702;&#30340;AI&#21457;&#23637;&#21644;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23545;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#12289;&#20174;&#19994;&#20154;&#21592;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#65292;&#20182;&#20204;&#38656;&#35201;&#26356;&#22810;&#32508;&#21512;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#21644;&#24178;&#39044;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#27963;&#21160;&#35268;&#27169;&#19979;&#30340;AI&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AI&#20215;&#20540;&#38142;&#20316;&#20026;&#19968;&#20010;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#30340;&#32508;&#21512;&#27010;&#24565;&#12290;&#20026;&#20102;&#26356;&#28165;&#26224;&#22320;&#29702;&#35770;&#21270;AI&#20215;&#20540;&#38142;&#65292;&#24182;&#22312;&#27010;&#24565;&#19978;&#23558;&#20854;&#19982;&#20379;&#24212;&#38142;&#21306;&#20998;&#24320;&#26469;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#25112;&#30053;&#31649;&#29702;&#12289;&#26381;&#21153;&#31185;&#23398;&#12289;&#32463;&#27982;&#22320;&#29702;&#23398;&#12289;&#34892;&#19994;&#12289;&#25919;&#24220;&#21644;&#24212;&#29992;&#30740;&#31350;&#25991;&#29486;&#20013;&#20851;&#20110;&#20215;&#20540;&#38142;&#21644;AI&#20215;&#20540;&#38142;&#30340;&#29702;&#35770;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#28085;&#30422;AI&#20215;&#20540;&#38142;&#28041;&#21450;&#30340;&#20262;&#29702;&#38382;&#39064;&#30340;67&#20010;&#26469;&#28304;&#36827;&#34892;&#32508;&#21512;&#35780;&#20272;&#12290;&#26681;&#25454;&#25105;&#20204;&#32508;&#21512;&#35780;&#20272;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#26041;&#21521;&#65292;&#30740;&#31350;&#20154;&#21592;&#12289;&#20174;&#19994;&#20154;&#21592;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#21487;&#20197;&#37319;&#21462;&#36825;&#20123;&#26041;&#21521;&#26469;&#25512;&#21160;&#22312;AI&#20215;&#20540;&#38142;&#19978;&#23454;&#29616;&#26356;&#20855;&#20262;&#29702;&#30340;&#21457;&#23637;&#21644;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21644;&#24314;&#35758;&#26377;&#21161;&#20110;&#25512;&#36827;&#30740;&#31350;&#35758;&#31243;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers, practitioners, and policymakers with an interest in AI ethics need more integrative approaches for studying and intervening in AI systems across many contexts and scales of activity. This paper presents AI value chains as an integrative concept that satisfies that need. To more clearly theorize AI value chains and conceptually distinguish them from supply chains, we review theories of value chains and AI value chains from the strategic management, service science, economic geography, industry, government, and applied research literature. We then conduct an integrative review of a sample of 67 sources that cover the ethical concerns implicated in AI value chains. Building upon the findings of our integrative review, we recommend four future directions that researchers, practitioners, and policymakers can take to advance more ethical practices of AI development and use across AI value chains. Our review and recommendations contribute to the advancement of research agendas, i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#21475;&#20449;&#24687;&#19981;&#23436;&#20840;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26367;&#20195;&#25935;&#24863;&#23646;&#24615;&#30340;&#23646;&#24615;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#23545;&#25512;&#26029;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#26368;&#20302;&#30340;&#20154;&#21475;&#20449;&#24687;&#26679;&#26412;&#36827;&#34892;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.13081</link><description>&lt;p&gt;
&#20154;&#21475;&#31232;&#32570;&#21046;&#24230;&#19979;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fairness Under Demographic Scarce Regime. (arXiv:2307.13081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13081
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#21475;&#20449;&#24687;&#19981;&#23436;&#20840;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26367;&#20195;&#25935;&#24863;&#23646;&#24615;&#30340;&#23646;&#24615;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#23545;&#25512;&#26029;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#26368;&#20302;&#30340;&#20154;&#21475;&#20449;&#24687;&#26679;&#26412;&#36827;&#34892;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#20551;&#35774;&#27169;&#22411;&#21487;&#20197;&#23436;&#20840;&#35775;&#38382;&#20154;&#21475;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#26399;&#38388;&#26410;&#20445;&#30041;&#35760;&#24405;&#25110;&#20986;&#20110;&#38544;&#31169;&#21407;&#22240;&#65292;&#23384;&#22312;&#20154;&#21475;&#20449;&#24687;&#37096;&#20998;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;&#36825;&#31181;&#24773;&#20917;&#34987;&#31216;&#20026;&#20154;&#21475;&#31232;&#32570;&#21046;&#24230;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35757;&#32451;&#19968;&#20010;&#23646;&#24615;&#20998;&#31867;&#22120;&#26469;&#26367;&#20195;&#32570;&#22833;&#30340;&#25935;&#24863;&#23646;&#24615;&#65288;&#20195;&#29702;&#65289;&#20173;&#28982;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#30495;&#23454;&#25935;&#24863;&#23646;&#24615;&#30456;&#27604;&#65292;&#20351;&#29992;&#20195;&#29702;&#25935;&#24863;&#23646;&#24615;&#20250;&#21152;&#21095;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26500;&#24314;&#23646;&#24615;&#20998;&#31867;&#22120;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23646;&#24615;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#23545;&#20855;&#26377;&#25512;&#26029;&#20986;&#30340;&#26368;&#20302;&#19981;&#30830;&#23450;&#24615;&#30340;&#20154;&#21475;&#20449;&#24687;&#30340;&#26679;&#26412;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#26679;&#26412;&#19978;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#32422;&#26463;&#20250;&#25439;&#23475;&#31639;&#27861;&#30340;&#24635;&#20307;&#20934;&#30830;&#24615;&#65292;&#20294;&#21487;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as demographic scarce regime. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (proxy) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#21644; Sobolev &#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#32771;&#34385;&#20013;&#24515;&#38544;&#31169;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#30452;&#26041;&#22270;&#20272;&#35745;&#22120;&#22312; L2 &#39118;&#38505;&#19979;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#20998;&#24067;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#27491;&#24120;&#24046;&#20998;&#38544;&#31169;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65307;&#21516;&#26102;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#26045;&#21152;&#38544;&#31169;&#20250;&#38477;&#20302;&#23545;&#20110; Sobolev &#23494;&#24230;&#30340;&#27491;&#21017;&#26497;&#23567;&#39118;&#38505;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#21457;&#29616;&#22312;&#32431;&#25237;&#24433;&#20272;&#35745;&#35774;&#23450;&#19979;&#65292;&#25152;&#35859;&#30340;&#25237;&#24433;&#20272;&#35745;&#22120;&#23545;&#20110;&#30456;&#21516;&#31867;&#23494;&#24230;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.14535</link><description>&lt;p&gt;
&#20851;&#20110;&#20013;&#24515;&#38544;&#31169;&#22312;&#23494;&#24230;&#20272;&#35745;&#20013;&#30340;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
About the Cost of Central Privacy in Density Estimation. (arXiv:2306.14535v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#21644; Sobolev &#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#32771;&#34385;&#20013;&#24515;&#38544;&#31169;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#30452;&#26041;&#22270;&#20272;&#35745;&#22120;&#22312; L2 &#39118;&#38505;&#19979;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#20998;&#24067;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#27491;&#24120;&#24046;&#20998;&#38544;&#31169;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65307;&#21516;&#26102;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#26045;&#21152;&#38544;&#31169;&#20250;&#38477;&#20302;&#23545;&#20110; Sobolev &#23494;&#24230;&#30340;&#27491;&#21017;&#26497;&#23567;&#39118;&#38505;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#21457;&#29616;&#22312;&#32431;&#25237;&#24433;&#20272;&#35745;&#35774;&#23450;&#19979;&#65292;&#25152;&#35859;&#30340;&#25237;&#24433;&#20272;&#35745;&#22120;&#23545;&#20110;&#30456;&#21516;&#31867;&#23494;&#24230;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#21033;&#26222;&#24076;&#33576;&#21644; Sobolev &#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#65292;&#22312;&#20013;&#24515;&#38544;&#31169;&#26465;&#20214;&#19979;&#36827;&#34892;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#38544;&#31169;&#39044;&#31639;&#19981;&#26159;&#24120;&#25968;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;&#20013;&#24515;&#24046;&#20998;&#38544;&#31169;&#23450;&#20041;&#65292;&#20197;&#21450;&#36739;&#26032;&#30340;&#20013;&#24515;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#27010;&#24565;&#12290;&#25105;&#20204;&#35777;&#23454;&#20102; Barber &amp; Duchi (2014) &#30340;&#32467;&#26524;&#65292;&#21363;&#30452;&#26041;&#22270;&#20272;&#35745;&#22120;&#22312;&#23545;&#20110; L2 &#39118;&#38505;&#19979;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#20998;&#24067;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#27491;&#24120;&#24046;&#20998;&#38544;&#31169;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65292;&#25105;&#20204;&#23558;&#20854;&#25193;&#23637;&#21040;&#20854;&#20182;&#33539;&#25968;&#21644;&#38544;&#31169;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#26356;&#39640;&#31243;&#24230;&#30340;&#20809;&#28369;&#24615;&#65292;&#24471;&#20986;&#20004;&#20010;&#32467;&#35770;&#65306;&#39318;&#20808;&#65292;&#19982;&#24120;&#25968;&#38544;&#31169;&#39044;&#31639;&#38656;&#35201;&#30340;&#24773;&#20917;&#30456;&#21453;&#65288;Wasserman &amp;amp; Zhou, 2010&#65289;&#65292;&#22312; Sobolev &#23494;&#24230;&#19978;&#26045;&#21152;&#38544;&#31169;&#20250;&#38477;&#20302;&#27491;&#21017;&#26497;&#23567;&#39118;&#38505;&#20272;&#35745;&#12290;&#20854;&#27425;&#65292;&#22312;&#36825;&#31181;&#26032;&#30340;&#32431;&#25237;&#24433;&#20272;&#35745;&#35774;&#23450;&#19979;&#65292;&#25152;&#35859;&#30340;&#25237;&#24433;&#20272;&#35745;&#22120;&#23545;&#20110;&#30456;&#21516;&#31867;&#23494;&#24230;&#26159;&#20960;&#20046;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study non-parametric density estimation for densities in Lipschitz and Sobolev spaces, and under central privacy. In particular, we investigate regimes where the privacy budget is not supposed to be constant. We consider the classical definition of central differential privacy, but also the more recent notion of central concentrated differential privacy. We recover the result of Barber \&amp; Duchi (2014) stating that histogram estimators are optimal against Lipschitz distributions for the L2 risk, and under regular differential privacy, and we extend it to other norms and notions of privacy. Then, we investigate higher degrees of smoothness, drawing two conclusions: First, and contrary to what happens with constant privacy budget (Wasserman \&amp; Zhou, 2010), there are regimes where imposing privacy degrades the regular minimax risk of estimation on Sobolev densities. Second, so-called projection estimators are near-optimal against the same classes of densities in this new setup with pure
&lt;/p&gt;</description></item><item><title>PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2305.19472</link><description>&lt;p&gt;
PlaSma: &#20026; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#21046;&#23450;&#22686;&#24378;&#36807;&#31243;&#30693;&#35782;&#27169;&#22411;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19472
&lt;/p&gt;
&lt;p&gt;
PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35268;&#21010;&#26159;&#26426;&#22120;&#30340;&#19968;&#39033;&#37325;&#35201;&#32780;&#21448;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23427;&#23558;&#19968;&#20010;&#39640;&#32423;&#30446;&#26631;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26102;&#38388;&#39034;&#24207;&#30340;&#27493;&#39588;&#12290;&#23427;&#38656;&#35201;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#25512;&#29702;&#20986;&#24120;&#24120;&#26159;&#21453;&#20107;&#23454;&#30340;&#22797;&#26434;&#24773;&#22659;&#65292;&#20363;&#22914; "&#27809;&#26377;&#30005;&#35805;&#26102;&#23433;&#25490;&#21307;&#29983;&#30340;&#32422;&#20250;"&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#21463;&#21040;&#26114;&#36149;&#30340; API &#35843;&#29992;&#21644;&#21487;&#22797;&#29616;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; PlaSma&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#26041;&#27861;&#65292;&#20351;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36807;&#31243;&#30693;&#35782;&#21644; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31526;&#21495;&#36807;&#31243;&#30693;&#35782;&#33976;&#39311;&#26469;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#65292;&#20197;&#21450;&#19968;&#31181;&#25512;&#29702;&#31639;&#27861;&#26469;&#20419;&#36827;&#26356;&#32467;&#26500;&#21270;&#21644;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21453;&#20107;&#23454;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00008</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00008
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#39072;&#35206;&#20154;&#24037;&#26234;&#33021;&#30340;&#22810;&#20010;&#39046;&#22495;&#12290;&#20854;&#20013;&#26368;&#26174;&#33879;&#30340;&#24212;&#29992;&#20043;&#19968;&#26159;&#21019;&#20316;&#65292;&#20363;&#22914;&#35799;&#27468;&#25110;&#25925;&#20107;&#65306;&#29983;&#25104;&#30340;&#36755;&#20986;&#36890;&#24120;&#20855;&#26377;&#24778;&#20154;&#30340;&#36136;&#37327;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;LLMs&#30495;&#30340;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21019;&#36896;&#24615;&#30340;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21019;&#36896;&#24615;&#29702;&#35770;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLMs&#30340;&#21457;&#23637;&#65292;&#25506;&#35752;&#20102;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19982;LLMs&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#26041;&#38754;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#26131;&#8221;&#21644;&#8220;&#38590;&#8221;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arise: can LLMs really be considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. Then, we identify a set of "easy" and "hard" problems in machine creativity, discussing them in relation to LLMs. Finally, we analyze the societal impact of these technologies with a particular focus on the creative industries.
&lt;/p&gt;</description></item></channel></rss>