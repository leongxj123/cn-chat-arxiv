<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;Multi-Grain Stereotype&#65288;MGS&#65289;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24314;&#31435;&#38472;&#35268;&#26816;&#27979;&#30340;&#22522;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;MGS&#25968;&#25454;&#35757;&#32451;&#30340;&#33521;&#25991;&#25991;&#26412;&#30340;&#38472;&#35268;&#20998;&#31867;&#22120;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.01768</link><description>&lt;p&gt;
&#29992;&#20110;&#22686;&#24378;&#22522;&#20110;&#25991;&#26412;&#30340;&#38472;&#35268;&#26816;&#27979;&#21644;&#22522;&#20110;&#25506;&#27979;&#30340;&#20559;&#35265;&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Auditing Large Language Models for Enhanced Text-Based Stereotype Detection and Probing-Based Bias Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01768
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;Multi-Grain Stereotype&#65288;MGS&#65289;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24314;&#31435;&#38472;&#35268;&#26816;&#27979;&#30340;&#22522;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;MGS&#25968;&#25454;&#35757;&#32451;&#30340;&#33521;&#25991;&#25991;&#26412;&#30340;&#38472;&#35268;&#20998;&#31867;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#38754;&#21521;&#20154;&#31867;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#20250;&#22797;&#21046;&#29978;&#33267;&#21152;&#21095;&#33258;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38472;&#35268;&#36755;&#20986;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Multi-Grain Stereotype&#65288;MGS&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;51,867&#20010;&#23454;&#20363;&#65292;&#28085;&#30422;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#32844;&#19994;&#12289;&#23447;&#25945;&#21644;&#38472;&#35268;&#25991;&#26412;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#20808;&#21069;&#20844;&#24320;&#30340;&#38472;&#35268;&#26816;&#27979;&#25968;&#25454;&#38598;&#25910;&#38598;&#32780;&#26469;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26088;&#22312;&#20026;&#38472;&#35268;&#26816;&#27979;&#24314;&#31435;&#22522;&#32447;&#30340;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24494;&#35843;&#20102;&#22810;&#31181;&#26550;&#26500;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;MGS&#35757;&#32451;&#30340;&#33521;&#25991;&#25991;&#26412;&#30340;&#38472;&#35268;&#20998;&#31867;&#22120;&#27169;&#22411;&#12290;&#20026;&#20102;&#20102;&#35299;&#25105;&#20204;&#30340;&#38472;&#35268;&#26816;&#27979;&#22120;&#26159;&#21542;&#25429;&#25417;&#21040;&#19982;&#20154;&#31867;&#24120;&#35782;&#19968;&#33268;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#21508;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01768v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) have significantly increased their presence in human-facing Artificial Intelligence (AI) applications. However, LLMs could reproduce and even exacerbate stereotypical outputs from training data. This work introduces the Multi-Grain Stereotype (MGS) dataset, encompassing 51,867 instances across gender, race, profession, religion, and stereotypical text, collected by fusing multiple previously publicly available stereotype detection datasets. We explore different machine learning approaches aimed at establishing baselines for stereotype detection, and fine-tune several language models of various architectures and model sizes, presenting in this work a series of stereotypes classifier models for English text trained on MGS. To understand whether our stereotype detectors capture relevant features (aligning with human common sense) we utilise a variety of explanainable AI tools, including 
&lt;/p&gt;</description></item><item><title>SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18870</link><description>&lt;p&gt;
SugarcaneNet2024: LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024: An Optimized Weighted Average Ensemble Approach of LASSO Regularized Pre-trained Models for Sugarcane Disease Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18870
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29976;&#34071;&#20316;&#20026;&#19990;&#30028;&#31958;&#19994;&#30340;&#20851;&#38190;&#20316;&#29289;&#65292;&#23481;&#26131;&#21463;&#22810;&#31181;&#30149;&#23475;&#20405;&#23475;&#65292;&#36825;&#20123;&#30149;&#23475;&#23545;&#20854;&#20135;&#37327;&#21644;&#36136;&#37327;&#37117;&#26377;&#37325;&#22823;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#26377;&#25928;&#31649;&#29702;&#21644;&#23454;&#26045;&#39044;&#38450;&#25514;&#26045;&#65292;&#24517;&#39035;&#21450;&#26102;&#20934;&#30830;&#22320;&#26816;&#27979;&#30149;&#23475;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SugarcaneNet2024&#30340;&#29420;&#29305;&#27169;&#22411;&#65292;&#36890;&#36807;&#21494;&#29255;&#22270;&#20687;&#22788;&#29702;&#65292;&#33021;&#22815;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#33258;&#21160;&#24555;&#36895;&#26816;&#27979;&#29976;&#34071;&#30149;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#27719;&#24635;&#20102;&#19971;&#20010;&#23450;&#21046;&#30340;&#12289;&#32463;&#36807;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#65292;&#29305;&#21035;&#26159;InceptionV3&#12289;InceptionResNetV2&#12289;DenseNet201&#12289;DenseNet169&#12289;Xception&#21644;ResNet152V2&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#24213;&#37096;&#28155;&#21152;&#20102;&#19977;&#23618;&#26356;&#23494;&#38598;&#23618;&#65292;&#20855;&#26377;0.0001&#30340;LASSO&#27491;&#21017;&#21270;&#65292;&#19977;&#20010;30%&#30340;dropout&#23618;&#21644;&#19977;&#20010;&#21551;&#29992;renorm&#30340;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18870v1 Announce Type: cross  Abstract: Sugarcane, a key crop for the world's sugar industry, is prone to several diseases that have a substantial negative influence on both its yield and quality. To effectively manage and implement preventative initiatives, diseases must be detected promptly and accurately. In this study, we present a unique model called sugarcaneNet2024 that outperforms previous methods for automatically and quickly detecting sugarcane disease through leaf image processing. Our proposed model consolidates an optimized weighted average ensemble of seven customized and LASSO-regularized pre-trained models, particularly InceptionV3, InceptionResNetV2, DenseNet201, DenseNet169, Xception, and ResNet152V2. Initially, we added three more dense layers with 0.0001 LASSO regularization, three 30% dropout layers, and three batch normalizations with renorm enabled at the bottom of these pre-trained models to improve the performance. The accuracy of sugarcane leaf dise
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;</title><link>https://arxiv.org/abs/2403.11116</link><description>&lt;p&gt;
&#21338;&#22763;&#35770;&#25991;&#65306;&#19968;&#20010;&#25552;&#31034;&#30340;&#35270;&#35273;&#24187;&#35273;&#35780;&#20272;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PhD: A Prompted Visual Hallucination Evaluation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#25512;&#21160;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#21457;&#23637;&#12290;&#22312;LLMs&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#24187;&#35273;&#25361;&#25112;&#20063;&#20986;&#29616;&#22312;LVLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;LVLM&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;&#19978;&#65292;&#24573;&#30053;&#20102;LVLM&#24187;&#35273;&#30340;&#22810;&#26679;&#21270;&#31867;&#22411;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22266;&#26377;&#35270;&#35273;&#35821;&#35328;&#24187;&#35273;&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#65292;&#23545;&#23548;&#33268;&#24187;&#35273;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;IVL-Hallu&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#65288;a&#65289;&#23545;&#35937;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#35937;&#30340;&#35823;&#35782;&#21035;&#32780;&#20135;&#29983;&#65292;&#65288;b&#65289;&#23646;&#24615;&#24187;&#35273;&#65292;&#30001;&#20110;&#23646;&#24615;&#30340;&#35823;&#35782;&#21035;&#32780;&#24341;&#36215;&#65292;&#65288;c&#65289;&#22810;&#27169;&#24577;&#20914;&#31361;&#24187;&#35273;&#65292;&#28304;&#33258;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#20197;&#21450;&#65288;d&#65289;&#21453;&#24120;&#35782;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#31435;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11116v1 Announce Type: cross  Abstract: The rapid growth of Large Language Models (LLMs) has driven the development of Large Vision-Language Models (LVLMs). The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs. However, most existing efforts mainly focus on object hallucination in LVLM, ignoring diverse types of LVLM hallucinations. In this study, we delve into the Intrinsic Vision-Language Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of IVL-Hallu on their causes and reflections. Specifically, we propose several novel IVL-Hallu tasks and categorize them into four types: (a) object hallucination, which arises from the misidentification of objects, (b) attribute hallucination, which is caused by the misidentification of attributes, (c) multi-modal conflicting hallucination, which derives from the contradictions between textual and visual information, and (d) counter-common-sense hallucination, which owes to the contradictions betwee
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;LIGHTCODE&#36731;&#37327;&#32423;&#31070;&#32463;&#32534;&#30721;&#26041;&#26696;&#65292;&#22312;&#20855;&#22791;&#35299;&#37322;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#22312;&#20302;&#20449;&#22122;&#27604;&#21306;&#22495;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10751</link><description>&lt;p&gt;
LIGHTCODE&#65306;&#20855;&#26377;&#21453;&#39304;&#36890;&#36947;&#30340;&#20809;&#35299;&#26512;&#21644;&#31070;&#32463;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LIGHTCODE: Light Analytical and Neural Codes for Channels with Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;LIGHTCODE&#36731;&#37327;&#32423;&#31070;&#32463;&#32534;&#30721;&#26041;&#26696;&#65292;&#22312;&#20855;&#22791;&#35299;&#37322;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#22312;&#20302;&#20449;&#22122;&#27604;&#21306;&#22495;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36947;&#21453;&#39304;&#20013;&#21487;&#38752;&#19988;&#39640;&#25928;&#30340;&#32534;&#30721;&#26041;&#26696;&#35774;&#35745;&#19968;&#30452;&#26159;&#36890;&#20449;&#29702;&#35770;&#20013;&#19968;&#39033;&#38271;&#26399;&#25361;&#25112;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#31070;&#32463;&#32534;&#30721;&#24448;&#24448;&#38754;&#20020;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#26377;&#38480;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#35299;&#37322;&#24615;&#24378;&#19988;&#26356;&#36866;&#29992;&#20110;&#36890;&#20449;&#31995;&#32479;&#30340;&#20302;&#22797;&#26434;&#24230;&#32534;&#30721;&#26041;&#26696;&#12290;&#25105;&#20204;&#20808;&#36827;&#20102;&#35299;&#26512;&#32534;&#30721;&#21644;&#31070;&#32463;&#32534;&#30721;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;POWERBLAST&#65292;&#19968;&#31181;&#21463;Schalkwijk-Kailath&#65288;SK&#65289;&#21644;Gallager-Nakiboglu&#65288;GN&#65289;&#26041;&#26696;&#21551;&#21457;&#30340;&#35299;&#26512;&#32534;&#30721;&#26041;&#26696;&#65292;&#22312;&#39640;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#21306;&#22495;&#23454;&#29616;&#20102;&#26126;&#26174;&#30340;&#21487;&#38752;&#24615;&#25913;&#36827;&#65292;&#32988;&#36807;&#31070;&#32463;&#32534;&#30721;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#22686;&#24378;&#20302;SNR&#21306;&#22495;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LIGHTCODE&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#32534;&#30721;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10751v1 Announce Type: cross  Abstract: The design of reliable and efficient codes for channels with feedback remains a longstanding challenge in communication theory. While significant improvements have been achieved by leveraging deep learning techniques, neural codes often suffer from high computational costs, a lack of interpretability, and limited practicality in resource-constrained settings. We focus on designing low-complexity coding schemes that are interpretable and more suitable for communication systems. We advance both analytical and neural codes. First, we demonstrate that POWERBLAST, an analytical coding scheme inspired by Schalkwijk-Kailath (SK) and Gallager-Nakiboglu (GN) schemes, achieves notable reliability improvements over both SK and GN schemes, outperforming neural codes in high signal-to-noise ratio (SNR) regions. Next, to enhance reliability in low-SNR regions, we propose LIGHTCODE, a lightweight neural code that achieves state-of-the-art reliability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23450;&#20041;&#20102;&#35268;&#26684;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#21363;&#31995;&#32479;&#36807;&#24230;&#20851;&#27880;&#25351;&#23450;&#25351;&#26631;&#32780;&#25439;&#23475;&#20102;&#39640;&#32423;&#35201;&#27714;&#21644;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08425</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#35268;&#26684;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Specification Overfitting in Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#35268;&#26684;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#21363;&#31995;&#32479;&#36807;&#24230;&#20851;&#27880;&#25351;&#23450;&#25351;&#26631;&#32780;&#25439;&#23475;&#20102;&#39640;&#32423;&#35201;&#27714;&#21644;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#32463;&#24120;&#34987;&#25209;&#35780;&#23384;&#22312;&#22266;&#26377;&#30340;&#20559;&#35265;&#65292;&#20197;&#21450;&#32570;&#20047;&#25511;&#21046;&#12289;&#38382;&#36131;&#21644;&#36879;&#26126;&#24230;&#65292;&#30417;&#31649;&#26426;&#26500;&#22240;&#27492;&#38590;&#20197;&#25511;&#21046;&#36825;&#31181;&#25216;&#26415;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#39640;&#32423;&#35201;&#27714;&#65292;&#22914;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#38656;&#35201;&#34987;&#24418;&#24335;&#21270;&#20026;&#20855;&#20307;&#30340;&#35268;&#26684;&#24230;&#37327;&#65292;&#32780;&#36825;&#20123;&#24230;&#37327;&#26159;&#25429;&#25417;&#22522;&#26412;&#35201;&#27714;&#30340;&#29420;&#31435;&#26041;&#38754;&#30340;&#19981;&#23436;&#32654;&#20195;&#29702;&#12290;&#37492;&#20110;&#19981;&#21516;&#25351;&#26631;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#30340;&#26435;&#34913;&#21450;&#20854;&#23545;&#36807;&#24230;&#20248;&#21270;&#30340;&#33030;&#24369;&#24615;&#65292;&#23558;&#35268;&#26684;&#24230;&#37327;&#25972;&#21512;&#21040;&#31995;&#32479;&#24320;&#21457;&#36807;&#31243;&#20013;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#35268;&#26684;&#36807;&#24230;&#25311;&#21512;&#65292;&#21363;&#31995;&#32479;&#36807;&#24230;&#20391;&#37325;&#20110;&#25351;&#23450;&#30340;&#24230;&#37327;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#39640;&#32423;&#35201;&#27714;&#21644;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#25991;&#29486;&#35843;&#30740;&#65292;&#23545;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#25552;&#20986;&#12289;&#27979;&#37327;&#21644;&#20248;&#21270;&#35268;&#26684;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08425v1 Announce Type: new  Abstract: Machine learning (ML) and artificial intelligence (AI) approaches are often criticized for their inherent bias and for their lack of control, accountability, and transparency. Consequently, regulatory bodies struggle with containing this technology's potential negative side effects. High-level requirements such as fairness and robustness need to be formalized into concrete specification metrics, imperfect proxies that capture isolated aspects of the underlying requirements. Given possible trade-offs between different metrics and their vulnerability to over-optimization, integrating specification metrics in system development processes is not trivial. This paper defines specification overfitting, a scenario where systems focus excessively on specified metrics to the detriment of high-level requirements and task performance. We present an extensive literature survey to categorize how researchers propose, measure, and optimize specification
&lt;/p&gt;</description></item><item><title>NeuPAN &#26159;&#19968;&#31181;&#23454;&#26102;&#12289;&#39640;&#24230;&#20934;&#30830;&#12289;&#26080;&#22320;&#22270;&#12289;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#19988;&#23545;&#29615;&#22659;&#19981;&#21464;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#22823;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#21407;&#22987;&#28857;&#30452;&#25509;&#26144;&#23556;&#21040;&#23398;&#20064;&#21040;&#30340;&#22810;&#24103;&#36317;&#31163;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#31471;&#21040;&#31471;&#27169;&#22411;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2403.06828</link><description>&lt;p&gt;
NeuPAN:&#30452;&#25509;&#28857;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06828
&lt;/p&gt;
&lt;p&gt;
NeuPAN &#26159;&#19968;&#31181;&#23454;&#26102;&#12289;&#39640;&#24230;&#20934;&#30830;&#12289;&#26080;&#22320;&#22270;&#12289;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#19988;&#23545;&#29615;&#22659;&#19981;&#21464;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#22823;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#21407;&#22987;&#28857;&#30452;&#25509;&#26144;&#23556;&#21040;&#23398;&#20064;&#21040;&#30340;&#22810;&#24103;&#36317;&#31163;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#31471;&#21040;&#31471;&#27169;&#22411;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#23545;&#38750;&#20840;&#21521;&#26426;&#22120;&#20154;&#36827;&#34892;&#23548;&#33322;&#38656;&#35201;&#26497;&#20854;&#31934;&#30830;&#30340;&#24863;&#30693;&#21644;&#36816;&#21160;&#20197;&#36991;&#20813;&#30896;&#25758;&#12290;&#26412;&#25991;&#25552;&#20986;NeuPAN&#65306;&#19968;&#31181;&#23454;&#26102;&#12289;&#39640;&#24230;&#20934;&#30830;&#12289;&#26080;&#22320;&#22270;&#12289;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#65292;&#19988;&#23545;&#29615;&#22659;&#19981;&#21464;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#35299;&#20915;&#26041;&#26696;&#12290;NeuPAN&#37319;&#29992;&#32039;&#32806;&#21512;&#30340;&#24863;&#30693;-&#36816;&#21160;&#26694;&#26550;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26377;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#65306;1&#65289;&#23427;&#30452;&#25509;&#23558;&#21407;&#22987;&#28857;&#26144;&#23556;&#21040;&#23398;&#20064;&#21040;&#30340;&#22810;&#24103;&#36317;&#31163;&#31354;&#38388;&#65292;&#36991;&#20813;&#20102;&#20174;&#24863;&#30693;&#21040;&#25511;&#21046;&#30340;&#35823;&#24046;&#20256;&#25773;&#65307;2&#65289;&#20174;&#31471;&#21040;&#31471;&#22522;&#20110;&#27169;&#22411;&#23398;&#20064;&#30340;&#35282;&#24230;&#36827;&#34892;&#35299;&#37322;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#25910;&#25947;&#12290;NeuPAN&#30340;&#20851;&#38190;&#22312;&#20110;&#21033;&#29992;&#25554;&#25300;&#24335;&#65288;PnP&#65289;&#20132;&#26367;&#26368;&#23567;&#21270;&#20256;&#24863;&#22120;&#65288;PAN&#65289;&#32593;&#32476;&#35299;&#39640;&#32500;&#31471;&#21040;&#31471;&#25968;&#23398;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#28857;&#32423;&#32422;&#26463;&#65292;&#20351;NeuPAN&#33021;&#22815;&#30452;&#25509;&#29983;&#25104;&#23454;&#26102;&#12289;&#31471;&#21040;&#31471;&#12289;&#29289;&#29702;&#21487;&#35299;&#37322;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06828v1 Announce Type: cross  Abstract: Navigating a nonholonomic robot in a cluttered environment requires extremely accurate perception and locomotion for collision avoidance. This paper presents NeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, and environment-invariant robot navigation solution. Leveraging a tightly-coupled perception-locomotion framework, NeuPAN has two key innovations compared to existing approaches: 1) it directly maps raw points to a learned multi-frame distance space, avoiding error propagation from perception to control; 2) it is interpretable from an end-to-end model-based learning perspective, enabling provable convergence. The crux of NeuPAN is to solve a high-dimensional end-to-end mathematical model with various point-level constraints using the plug-and-play (PnP) proximal alternating-minimization network (PAN) with neurons in the loop. This allows NeuPAN to generate real-time, end-to-end, physically-interpretable motions direct
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#26032;&#25351;&#26631;&#23545;&#25239;&#36229;&#20307;&#31215;&#26469;&#20840;&#38754;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#31181;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05100</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#25239;&#30028;&#38480;&#65306;&#36890;&#36807;&#23545;&#25239;&#36229;&#20307;&#31215;&#37327;&#21270;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05100
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#26032;&#25351;&#26631;&#23545;&#25239;&#36229;&#20307;&#31215;&#26469;&#20840;&#38754;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#31181;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#26085;&#30410;&#20005;&#37325;&#30340;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#23545;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#20381;&#36182;&#20110;&#23545;&#25239;&#20934;&#30830;&#24615;&#65292;&#35813;&#25351;&#26631;&#34913;&#37327;&#27169;&#22411;&#22312;&#29305;&#23450;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#21333;&#19968;&#25351;&#26631;&#24182;&#19981;&#33021;&#23436;&#20840;&#27010;&#25324;&#27169;&#22411;&#23545;&#19981;&#21516;&#31243;&#24230;&#25200;&#21160;&#30340;&#25972;&#20307;&#38887;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#23545;&#25239;&#36229;&#20307;&#31215;&#65292;&#20174;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#32508;&#21512;&#35780;&#20272;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#25351;&#26631;&#20801;&#35768;&#28145;&#20837;&#27604;&#36739;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#25215;&#35748;&#20102;&#36739;&#24369;&#30340;&#38450;&#24481;&#31574;&#30053;&#25152;&#24102;&#26469;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#22343;&#21248;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05100v1 Announce Type: cross  Abstract: The escalating threat of adversarial attacks on deep learning models, particularly in security-critical fields, has underscored the need for robust deep learning systems. Conventional robustness evaluations have relied on adversarial accuracy, which measures a model's performance under a specific perturbation intensity. However, this singular metric does not fully encapsulate the overall resilience of a model against varying degrees of perturbation. To address this gap, we propose a new metric termed adversarial hypervolume, assessing the robustness of deep learning models comprehensively over a range of perturbation intensities from a multi-objective optimization standpoint. This metric allows for an in-depth comparison of defense mechanisms and recognizes the trivial improvements in robustness afforded by less potent defensive strategies. Additionally, we adopt a novel training algorithm that enhances adversarial robustness uniformly
&lt;/p&gt;</description></item><item><title>ARNN&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#35745;&#31639;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;LSTM gate&#30340;&#20248;&#21183;&#65292;&#24182;&#36991;&#20813;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.03276</link><description>&lt;p&gt;
ARNN: &#29992;&#20110;&#35782;&#21035;&#30315;&#30187;&#21457;&#20316;&#30340;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03276
&lt;/p&gt;
&lt;p&gt;
ARNN&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#35745;&#31639;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;LSTM gate&#30340;&#20248;&#21183;&#65292;&#24182;&#36991;&#20813;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ARNN&#65289;&#65292;&#20854;&#27839;&#30528;&#24207;&#21015;&#24490;&#29615;&#24212;&#29992;&#27880;&#24847;&#21147;&#23618;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#19978;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;&#21333;&#36890;&#36947;&#20449;&#21495;&#65292;&#24182;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#27880;&#24847;&#21147;&#23618;&#26159;&#19968;&#31181;&#35745;&#31639;&#21333;&#20803;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35745;&#31639;&#19968;&#32452;&#24191;&#27867;&#25968;&#37327;&#30340;&#29366;&#24577;&#21521;&#37327;&#21644;&#36755;&#20837;&#20449;&#21495;&#30340;&#36882;&#24402;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21463;&#21040;&#20102;&#27880;&#24847;&#21147;&#23618;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#30340;&#21551;&#21457;&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#39118;&#26684;&#38376;&#65292;&#20294;&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#23558;&#36825;&#31181;&#20856;&#22411;&#21333;&#20803;&#25193;&#23637;&#21040;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#24182;&#34892;&#21270;&#12290;&#23427;&#32487;&#25215;&#20102;&#27880;&#24847;&#21147;&#23618;&#21644;LSTM&#38376;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#24322;&#36136;&#23454;&#39564;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03276v1 Announce Type: cross  Abstract: We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length. The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation. In this cell, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals. Our architecture is inspired in part by the attention layer and long short-term memory (LSTM) cells, and it uses long-short style gates, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks. We evaluated the model effectiveness through extensive experiments with heterogeneou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20195;&#29702;&#36827;&#34892;&#29305;&#24449;&#23545;&#40784;&#65292;&#20197;&#35299;&#20915;&#39044;&#20808;&#35745;&#31639;&#29305;&#24449;&#26080;&#27861;&#21306;&#20998;&#26631;&#35760;&#26679;&#26412;&#31867;&#21035;&#21644;&#36991;&#20813;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#26679;&#26412;&#26102;&#29306;&#29298;&#23453;&#36149;&#39044;&#35757;&#32451;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01101</link><description>&lt;p&gt;
&#29305;&#24449;&#23545;&#40784;&#65306;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#32972;&#26223;&#19979;&#36890;&#36807;&#20195;&#29702;&#24605;&#32771;&#39640;&#25928;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#36827;&#34892;&#29305;&#24449;&#23545;&#40784;&#65292;&#20197;&#35299;&#20915;&#39044;&#20808;&#35745;&#31639;&#29305;&#24449;&#26080;&#27861;&#21306;&#20998;&#26631;&#35760;&#26679;&#26412;&#31867;&#21035;&#21644;&#36991;&#20813;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#26679;&#26412;&#26102;&#29306;&#29298;&#23453;&#36149;&#39044;&#35757;&#32451;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26377;&#26395;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32452;&#21512;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#23427;&#39044;&#20808;&#35745;&#31639;&#29305;&#24449;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20250;&#22312;&#20027;&#21160;&#23398;&#20064;&#24615;&#33021;&#19978;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#65292;&#29978;&#33267;&#21487;&#33021;&#36229;&#36807;&#35745;&#31639;&#25104;&#26412;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01101v1 Announce Type: cross  Abstract: Fine-tuning the pre-trained model with active learning holds promise for reducing annotation costs. However, this combination introduces significant computational costs, particularly with the growing scale of pre-trained models. Recent research has proposed proxy-based active learning, which pre-computes features to reduce computational costs. Yet, this approach often incurs a significant loss in active learning performance, which may even outweigh the computational cost savings. In this paper, we argue the performance drop stems not only from pre-computed features' inability to distinguish between categories of labeled samples, resulting in the selection of redundant samples but also from the tendency to compromise valuable pre-trained information when fine-tuning with samples selected through the proxy model. To address this issue, we propose a novel method called aligned selection via proxy to update pre-computed features while sele
&lt;/p&gt;</description></item><item><title>RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.17747</link><description>&lt;p&gt;
&#24403;&#20320;&#30340;AI&#27450;&#39575;&#20320;&#65306;&#22312;&#22870;&#21169;&#23398;&#20064;&#20013;&#20154;&#31867;&#35780;&#20272;&#32773;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17747
&lt;/p&gt;
&lt;p&gt;
RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#36807;&#21435;&#20998;&#26512;&#20551;&#35774;&#20154;&#31867;&#23436;&#20840;&#35266;&#23519;&#21040;&#29615;&#22659;&#12290;&#24403;&#20154;&#31867;&#21453;&#39304;&#20165;&#22522;&#20110;&#37096;&#20998;&#35266;&#23519;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#23545;&#20004;&#31181;&#22833;&#36133;&#24773;&#20917;&#36827;&#34892;&#20102;&#27491;&#24335;&#23450;&#20041;&#65306;&#27450;&#39575;&#21644;&#36807;&#24230;&#36777;&#25252;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#24314;&#27169;&#20026;&#23545;&#36712;&#36857;&#20449;&#24565;&#30340;Boltzmann-&#29702;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLHF&#20445;&#35777;&#20250;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#20854;&#24615;&#33021;&#12289;&#20026;&#20102;&#30041;&#19979;&#21360;&#35937;&#32780;&#36807;&#24230;&#36777;&#25252;&#25110;&#32773;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#30340;&#26465;&#20214;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25968;&#23398;&#22320;&#21051;&#30011;&#20102;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#22914;&#20309;&#36716;&#21270;&#20026;&#65288;&#32570;&#20047;&#65289;&#23398;&#21040;&#30340;&#22238;&#25253;&#20989;&#25968;&#20013;&#30340;&#27169;&#31946;&#24615;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#20351;&#24471;&#22312;&#29702;&#35770;&#19978;&#21487;&#33021;&#24674;&#22797;&#22238;&#25253;&#20989;&#25968;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19981;&#21487;&#20943;&#23569;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17747v1 Announce Type: cross  Abstract: Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35266;&#23519;&#20102;Transformer&#20869;&#37096;&#30005;&#36335;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20943;&#27861;&#22312;Transformer&#19978;&#36896;&#25104;&#20102;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#20056;&#27861;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65292;&#22810;&#39033;&#24335;&#21472;&#21152;&#20102;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#24182;&#19981;&#28165;&#26224;&#65292;Grokking&#29978;&#33267;&#21487;&#20197;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#36731;&#26494;&#21457;&#29983;&#12290;</title><link>https://arxiv.org/abs/2402.16726</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35299;&#37322;&#29702;&#35299;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Interpreting Grokked Transformers in Complex Modular Arithmetic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35266;&#23519;&#20102;Transformer&#20869;&#37096;&#30005;&#36335;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20943;&#27861;&#22312;Transformer&#19978;&#36896;&#25104;&#20102;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#20056;&#27861;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65292;&#22810;&#39033;&#24335;&#21472;&#21152;&#20102;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#24182;&#19981;&#28165;&#26224;&#65292;Grokking&#29978;&#33267;&#21487;&#20197;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#36731;&#26494;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#19968;&#30452;&#26159;&#35299;&#24320;&#24310;&#36831;&#27867;&#21270;&#20043;&#35868;&#30340;&#31215;&#26497;&#25506;&#32034;&#12290;&#22312;&#24050;&#35299;&#23494;&#27169;&#22411;&#20013;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#31639;&#27861;&#26159;&#29702;&#35299;&#20854;&#26426;&#21046;&#30340;&#26263;&#31034;&#24615;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38500;&#20102;&#26368;&#31616;&#21333;&#21644;&#24191;&#20026;&#30740;&#31350;&#30340;&#27169;&#22359;&#21270;&#21152;&#27861;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#35266;&#23519;&#20102;&#36890;&#36807;Grokking&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#23398;&#21040;&#30340;&#20869;&#37096;&#30005;&#36335;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#23427;&#20204;&#21160;&#21147;&#23398;&#19978;&#30340;&#37325;&#22823;&#24046;&#24322;&#65306;&#20943;&#27861;&#23545;Transformer&#20135;&#29983;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65307;&#20056;&#27861;&#22312;&#20613;&#31435;&#21494;&#22495;&#30340;&#25152;&#26377;&#39057;&#29575;&#19978;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65307;&#22810;&#39033;&#24335;&#36890;&#24120;&#23548;&#33268;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#30340;&#21472;&#21152;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#28165;&#26224;&#30340;&#27169;&#24335;&#24182;&#19981;&#26174;&#29616;&#65307;&#21363;&#20351;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#65292;Grokking&#20063;&#24456;&#23481;&#26131;&#21457;&#29983;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#27169;&#22359;&#21270;&#31639;&#26415;&#30340;&#26032;&#39062;&#36827;&#23637;&#24230;&#37327;&#65307;&#20613;&#31435;&#21494;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16726v2 Announce Type: replace-cross  Abstract: Grokking has been actively explored to reveal the mystery of delayed generalization. Identifying interpretable algorithms inside the grokked models is a suggestive hint to understanding its mechanism. In this work, beyond the simplest and well-studied modular addition, we observe the internal circuits learned through grokking in complex modular arithmetic via interpretable reverse engineering, which highlights the significant difference in their dynamics: subtraction poses a strong asymmetry on Transformer; multiplication requires cosine-biased components at all the frequencies in a Fourier domain; polynomials often result in the superposition of the patterns from elementary arithmetic, but clear patterns do not emerge in challenging cases; grokking can easily occur even in higher-degree formulas with basic symmetric and alternating expressions. We also introduce the novel progress measure for modular arithmetic; Fourier Freque
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#29992;&#20110;&#22312;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2402.14259</link><description>&lt;p&gt;
&#21333;&#35789;&#24207;&#21015;&#29109;&#65306;&#36208;&#21521;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#24212;&#29992;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#29992;&#20110;&#22312;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22312;&#30830;&#20445;&#23433;&#20840;&#20851;&#38190;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#21487;&#38752;&#24615;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#30001;&#24418;&#24335;&#30340;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#23578;&#26410;&#24314;&#31435;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#26080;&#20851;&#30340;&#35789;&#27719;&#21644;&#35821;&#24207;&#21547;&#26377;&#26377;&#38480;&#30340;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#36825;&#26159;&#30001;&#20110;&#29983;&#25104;&#19981;&#24179;&#31561;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#35821;&#20041;&#30456;&#20851;&#24615;&#22312;&#21333;&#35789;&#21644;&#24207;&#21015;&#32423;&#21035;&#19978;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#27604;&#20363;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26102;&#26356;&#21152;&#24378;&#35843;&#20851;&#38190;&#35789;&#21644;&#26356;&#30456;&#20851;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;5&#20010;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#65292;&#21033;&#29992;7&#31181;&#8220;&#29616;&#25104;&#30340;&#8221;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;WSE&#19982;6&#31181;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;WSE&#22312;&#24615;&#33021;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14259v1 Announce Type: cross  Abstract: Uncertainty estimation plays a pivotal role in ensuring the reliability of safety-critical human-AI interaction systems, particularly in the medical domain. However, a general method for quantifying the uncertainty of free-form answers has yet to be established in open-ended medical question-answering (QA) tasks, where irrelevant words and sequences with limited semantic information can be the primary source of uncertainty due to the presence of generative inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which calibrates the uncertainty proportion at both the word and sequence levels according to the semantic relevance, with greater emphasis placed on keywords and more relevant sequences when performing uncertainty quantification. We compare WSE with 6 baseline methods on 5 free-form medical QA datasets, utilizing 7 "off-the-shelf" large language models (LLMs), and show that WSE exhibits superior performance on ac
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05271</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#24341;&#21457;&#20102;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#26435;&#37325;&#19982;&#32463;&#39564;NTK&#20043;&#38388;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05271
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#65292;&#22312;&#19968;&#33324;&#32467;&#26500;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#20010;&#35828;&#27861;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#37327;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#30456;&#20851;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#24341;&#20837;&#30340;NFA&#26159;&#30001;&#38548;&#31163;&#36825;&#31181;&#23545;&#40784;&#30340;&#20013;&#24515;&#21270;NFA&#39537;&#21160;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of sim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#19982;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#39044;&#27979;&#22810;&#27169;&#24577;&#33829;&#38144;&#27963;&#21160;&#25928;&#26524;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26089;&#26399;&#26816;&#27979;&#21487;&#33021;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#22810;&#27169;&#24577;&#27963;&#21160;&#24182;&#35780;&#20272;&#21644;&#22686;&#24378;&#33829;&#38144;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03607</link><description>&lt;p&gt;
&#25552;&#39640;&#22810;&#27169;&#24577;&#33829;&#38144;&#30340;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#65306;&#30693;&#35782;&#22522;&#30784;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#19982;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#39044;&#27979;&#22810;&#27169;&#24577;&#33829;&#38144;&#27963;&#21160;&#25928;&#26524;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26089;&#26399;&#26816;&#27979;&#21487;&#33021;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#22810;&#27169;&#24577;&#27963;&#21160;&#24182;&#35780;&#20272;&#21644;&#22686;&#24378;&#33829;&#38144;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#35774;&#22791;&#30340;&#26222;&#21450;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#32447;&#20307;&#39564;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#35270;&#35273;&#27169;&#22411;&#65288;LVM&#65289;&#20173;&#28982;&#21463;&#21040;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#20851;&#31995;&#30340;&#25972;&#20307;&#24847;&#20041;&#30340;&#38480;&#21046;&#12290;&#32570;&#20047;&#26126;&#30830;&#30340;&#24120;&#35782;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#20316;&#20026;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65289;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20165;&#36890;&#36807;&#25429;&#25417;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#39640;&#32423;&#27169;&#24335;&#26469;&#23398;&#20064;&#38544;&#24335;&#34920;&#31034;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#36328;&#27169;&#24577;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#26174;&#24335;&#30340;&#24120;&#35782;&#30693;&#35782;&#20197;&#30693;&#35782;&#22270;&#35889;&#30340;&#24418;&#24335;&#19982;&#22823;&#22411;&#30340;VLM&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21363;&#39044;&#27979;&#22810;&#27169;&#24577;&#33829;&#38144;&#27963;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;&#34429;&#28982;&#33829;&#38144;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#35828;&#26381;&#21147;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#26089;&#26399;&#21457;&#29616;&#21487;&#33021;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#22810;&#27169;&#24577;&#27963;&#21160;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#35780;&#20272;&#21644;&#22686;&#24378;&#33829;&#38144;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of smart devices with the ability to capture moments in multiple modalities has enabled users to experience multimodal information online. However, large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships. Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual Language Models (VLMs) only learn implicit representations by capturing high-level patterns in vast corpora, missing essential contextual cross-modal cues. In this work, we design a framework to couple explicit commonsense knowledge in the form of knowledge graphs with large VLMs to improve the performance of a downstream task, predicting the effectiveness of multi-modal marketing campaigns. While the marketing application provides a compelling metric for assessing our methods, our approach enables the early detection of likely persuasive multi-modal campaigns and the assessment and augmentation of marketing theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;TSIS&#31639;&#27861;&#20316;&#20026;t-SMILES&#30340;&#34917;&#20805;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TSIS&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#27861;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02164</link><description>&lt;p&gt;
TSIS: t-SMILES&#30340;&#34917;&#20805;&#31639;&#27861;&#29992;&#20110;&#22522;&#20110;&#29255;&#27573;&#30340;&#20998;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
TSIS: A Supplementary Algorithm to t-SMILES for Fragment-based Molecular Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;TSIS&#31639;&#27861;&#20316;&#20026;t-SMILES&#30340;&#34917;&#20805;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TSIS&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#27861;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#31526;&#20018;&#22522;&#26412;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#22914;SMILES&#65292;&#22312;&#32447;&#24615;&#34920;&#31034;&#20998;&#23376;&#20449;&#24687;&#26041;&#38754;&#26159;&#20107;&#23454;&#19978;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#20351;&#29992;&#37197;&#23545;&#31526;&#21495;&#21644;&#35299;&#26512;&#31639;&#27861;&#23548;&#33268;&#20102;&#38271;&#30340;&#35821;&#27861;&#20381;&#36182;&#20851;&#31995;&#65292;&#20351;&#24471;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20063;&#38590;&#20197;&#20934;&#30830;&#29702;&#35299;&#35821;&#27861;&#21644;&#35821;&#20041;&#12290;&#23613;&#31649;DeepSMILES&#21644;SELFIES&#24050;&#32463;&#35299;&#20915;&#20102;&#26576;&#20123;&#38480;&#21046;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#22788;&#29702;&#39640;&#32423;&#35821;&#27861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20351;&#24471;&#19968;&#20123;&#23383;&#31526;&#20018;&#38590;&#20197;&#38405;&#35835;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#34917;&#20805;&#31639;&#27861;TSIS&#65288;TSID&#31616;&#21270;&#65289;&#65292;&#29992;&#20110;t-SMILES&#23478;&#26063;&#12290;TSIS&#19982;&#21478;&#19968;&#20010;&#22522;&#20110;&#29255;&#27573;&#30340;&#32447;&#24615;&#35299;&#20915;&#26041;&#26696;SAFE&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SAFE&#22312;&#22788;&#29702;&#35821;&#27861;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26102;&#23384;&#22312;&#25361;&#25112;&#12290;TSIS&#32487;&#32493;&#20351;&#29992;t-SMILES&#20013;&#23450;&#20041;&#30340;&#26641;&#20316;&#20026;&#20854;&#22522;&#30784;&#25968;&#25454;&#32467;&#26500;&#65292;&#36825;&#20351;&#20854;&#19982;SAFE&#27169;&#22411;&#26377;&#25152;&#19981;&#21516;&#12290;TSIS&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;SAFE&#27169;&#22411;&#65292;&#34920;&#26126;t-SMILES&#30340;&#26641;&#32467;&#26500;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
String-based molecular representations, such as SMILES, are a de facto standard for linearly representing molecular information. However, the must be paired symbols and the parsing algorithm result in long grammatical dependencies, making it difficult for even state-of-the-art deep learning models to accurately comprehend the syntax and semantics. Although DeepSMILES and SELFIES have addressed certain limitations, they still struggle with advanced grammar, which makes some strings difficult to read. This study introduces a supplementary algorithm, TSIS (TSID Simplified), to t-SMILES family. Comparative experiments between TSIS and another fragment-based linear solution, SAFE, indicate that SAFE presents challenges in managing long-term dependencies in grammar. TSIS continues to use the tree defined in t-SMILES as its foundational data structure, which sets it apart from the SAFE model. The performance of TSIS models surpasses that of SAFE models, indicating that the tree structure of t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00068</link><description>&lt;p&gt;
GPT4Battery: &#19968;&#31181;&#22522;&#20110;LLM&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#29366;&#24577;&#65288;SOH&#65289;&#26159;&#35780;&#20272;&#30005;&#27744;&#36864;&#21270;&#27700;&#24179;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#20294;&#38656;&#35201;&#20272;&#35745;&#12290;&#20934;&#30830;&#30340;SOH&#20272;&#35745;&#25552;&#21319;&#20102;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#26816;&#27979;&#12289;&#25511;&#21046;&#21644;&#21453;&#39304;&#33021;&#21147;&#65292;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33021;&#28304;&#31649;&#29702;&#65292;&#24182;&#25351;&#23548;&#26032;&#19968;&#20195;&#30005;&#27744;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;SOH&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20026;&#29983;&#25104;&#23551;&#21629;&#38271;&#26399;&#35757;&#32451;&#25968;&#25454;&#32780;&#36827;&#34892;&#30340;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#30340;&#36864;&#21270;&#23454;&#39564;&#22312;&#24314;&#31435;&#19968;&#20010;&#33021;&#22788;&#29702;&#22810;&#26679;&#21270;&#38146;&#31163;&#23376;&#30005;&#27744;&#65288;&#20363;&#22914;&#65292;&#36328;&#21270;&#23398;&#12289;&#36328;&#21046;&#36896;&#21830;&#21644;&#36328;&#23481;&#37327;&#65289;&#30340;&#22823;&#22411;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#30005;&#27744;&#30340;&#21487;&#35843;&#25972;SOH&#20272;&#35745;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#20026;&#20102;&#36866;&#24212;&#23454;&#38469;&#24773;&#26223;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#25353;&#39034;&#24207;&#20197;&#21450;&#20998;&#24067;&#21464;&#21270;&#30340;&#26041;&#24335;&#21040;&#36798;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
State of health (SOH) is a crucial indicator for assessing the degradation level of batteries that cannot be measured directly but requires estimation. Accurate SOH estimation enhances detection, control, and feedback for Li-ion batteries, allowing for safe and efficient energy management and guiding the development of new-generation batteries. Despite the significant progress in data-driven SOH estimation, the time and resource-consuming degradation experiments for generating lifelong training data pose a challenge in establishing one large model capable of handling diverse types of Li-ion batteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity. Hence, this paper utilizes the strong generalization capability of large language model (LLM) to proposes a novel framework for adaptable SOH estimation across diverse batteries. To match the real scenario where unlabeled data sequentially arrives in use with distribution shifts, the proposed model is modified by a test-time t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65292;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#20123;&#32467;&#26524;&#20063;&#26174;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2401.08273</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;&#23556;&#20987;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Null-Shot Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65292;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#20123;&#32467;&#26524;&#20063;&#26174;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#12290;&#38646;&#23556;&#20987;&#25552;&#31034;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#36890;&#36807;&#25351;&#31034;LLMs&#21033;&#29992;&#20174;&#8220;&#31034;&#20363;&#8221;&#37096;&#20998;&#20013;&#33719;&#21462;&#30340;&#20449;&#24687;&#65288;&#35813;&#20449;&#24687;&#22312;&#25152;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#20013;&#19981;&#23384;&#22312;&#65289;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#34429;&#28982;&#20943;&#23569;&#38169;&#35823;&#20449;&#24687;&#23545;&#20110;LLMs&#30340;&#26085;&#24120;&#21644;&#37325;&#35201;&#29992;&#36884;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#30446;&#21069;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;LLMs&#20173;&#28982;&#20855;&#26377;&#38169;&#35823;&#20449;&#24687;&#65292;&#23454;&#38469;&#19978;&#21487;&#20197;&#21033;&#29992;&#38169;&#35823;&#20449;&#24687;&#26469;&#25552;&#39640;&#19982;&#26631;&#20934;&#38646;&#23556;&#20987;&#25552;&#31034;&#30456;&#27604;&#30340;&#20219;&#21153;&#34920;&#29616;&#12290;&#23545;&#20843;&#20010;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#20843;&#20010;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65289;&#20013;&#65292;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#35266;&#23519;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#22686;&#21152;&#30456;&#23545;&#24615;&#33021;&#22312;LLMs&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20063;&#21487;&#33021;&#34920;&#31034;&#27599;&#20010;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08273v2 Announce Type: replace-cross Abstract: This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across the LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show 
&lt;/p&gt;</description></item><item><title>SupplyGraph&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.15299</link><description>&lt;p&gt;
SupplyGraph: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks. (arXiv:2401.15299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15299
&lt;/p&gt;
&lt;p&gt;
SupplyGraph&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#22914;&#36816;&#36755;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;GNNs&#24212;&#29992;&#20110;&#20379;&#24212;&#38142;&#32593;&#32476;&#26041;&#38754;&#65292;&#30446;&#21069;&#23578;&#32570;&#20047;&#30740;&#31350;&#12290;&#20379;&#24212;&#38142;&#32593;&#32476;&#22312;&#32467;&#26500;&#19978;&#31867;&#20284;&#20110;&#22270;&#24418;&#65292;&#20351;&#20854;&#25104;&#20026;&#24212;&#29992;GNN&#26041;&#27861;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#36825;&#20026;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#24320;&#36767;&#20102;&#26080;&#38480;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#27492;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#20351;&#29992;GNN&#26469;&#30740;&#31350;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#30340;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#29992;&#20110;&#29983;&#20135;&#30446;&#30340;&#30340;&#20379;&#24212;&#38142;&#35268;&#21010;&#30340;&#26102;&#38388;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained traction across different domains such as transportation, bio-informatics, language processing, and computer vision. However, there is a noticeable absence of research on applying GNNs to supply chain networks. Supply chain networks are inherently graph-like in structure, making them prime candidates for applying GNN methodologies. This opens up a world of possibilities for optimizing, predicting, and solving even the most complex supply chain problems. A major setback in this approach lies in the absence of real-world benchmark datasets to facilitate the research and resolution of supply chain problems using GNNs. To address the issue, we present a real-world benchmark dataset for temporal tasks, obtained from one of the leading FMCG companies in Bangladesh, focusing on supply chain planning for production purposes. The dataset includes temporal data as node features to enable sales predictions, production planning, and the identification of fa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#20445;&#25252;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#25216;&#26415;&#21019;&#26032;&#19982;&#20262;&#29702;&#21069;&#30651;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.18252</link><description>&lt;p&gt;
&#36328;&#36234;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#29983;&#21629;&#21608;&#26399;&#30340;&#38544;&#31169;&#21644;&#29256;&#26435;&#25361;&#25112;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Navigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI. (arXiv:2311.18252v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18252
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#20445;&#25252;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#25216;&#26415;&#21019;&#26032;&#19982;&#20262;&#29702;&#21069;&#30651;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#23637;&#31034;&#20986;&#22312;&#29983;&#25104;&#30495;&#23454;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#25968;&#25454;&#27169;&#24335;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#23637;&#20063;&#24102;&#26469;&#20102;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#20405;&#29359;&#30340;&#26356;&#39640;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#27169;&#22411;&#35757;&#32451;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#24046;&#20998;&#38544;&#31169;&#12289;&#26426;&#22120;&#36951;&#24536;&#21644;&#25968;&#25454;&#20013;&#27602;&#21482;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#22797;&#26434;&#38382;&#39064;&#30340;&#29255;&#38754;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#29983;&#21629;&#21608;&#26399;&#20869;&#38544;&#31169;&#21644;&#29256;&#26435;&#20445;&#25252;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#12290;&#25105;&#20204;&#20027;&#24352;&#37319;&#29992;&#23558;&#25216;&#26415;&#21019;&#26032;&#19982;&#20262;&#29702;&#21069;&#30651;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#21644;&#21046;&#23450;&#22312;&#29983;&#21629;&#21608;&#26399;&#35270;&#35282;&#19979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#26356;&#24191;&#27867;&#30340;&#35752;&#35770;&#65292;&#24182;&#28608;&#21169;&#23545;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#23436;&#25972;&#24615;&#30340;&#21327;&#21516;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.
&lt;/p&gt;</description></item><item><title>Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17807</link><description>&lt;p&gt;
Clover: &#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Clover: Closed-Loop Verifiable Code Generation. (arXiv:2310.17807v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17807
&lt;/p&gt;
&lt;p&gt;
Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#26159;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#20010;&#36235;&#21183;&#21487;&#33021;&#20250;&#23548;&#33268;&#35768;&#22810;&#19981;&#33391;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#24895;&#26223;&#65306;Clover&#33539;&#24335;&#65292;&#21363;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#65292;&#23427;&#23558;&#27491;&#30830;&#24615;&#26816;&#26597;&#31616;&#21270;&#20026;&#26356;&#21487;&#35775;&#38382;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#22312;Clover&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26816;&#26597;&#22120;&#65292;&#23427;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#12290;&#35813;&#26816;&#26597;&#22120;&#20351;&#29992;&#20102;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#38598;&#25104;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#28857;&#65292;&#21363;Clover&#22312;&#19968;&#33268;&#24615;&#26816;&#26597;&#26041;&#38754;&#24212;&#35813;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#30001;&#25163;&#24037;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65288;CloverBench&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35843;&#26597;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#27880;&#37322;&#30340;Dafny&#31243;&#24207;&#65292;&#38590;&#24230;&#27700;&#24179;&#19982;&#25945;&#31185;&#20070;&#30456;&#24403;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results sho
&lt;/p&gt;</description></item><item><title>Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.17086</link><description>&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#19968;&#39033;&#19982;&#32447;&#24615;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17086
&lt;/p&gt;
&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformers&#21487;&#33021;&#36890;&#36807;&#20869;&#37096;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21363;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#20197;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20026;&#37325;&#28857;&#65292;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#19968;&#20010;&#38750;&#24120;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;&#20174;&#23454;&#35777;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#32493;&#30340;Transformer&#23618;&#30340;&#39044;&#27979;&#19982;&#29275;&#39039;&#27861;&#30340;&#19981;&#21516;&#36845;&#20195;&#38750;&#24120;&#25509;&#36817;&#65292;&#27599;&#20010;&#20013;&#38388;&#23618;&#22823;&#33268;&#35745;&#31639;&#20102;3&#27425;&#36845;&#20195;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#25165;&#33021;&#21305;&#37197;&#39069;&#22806;&#30340;Transformer&#23618;&#65307;&#36825;&#34920;&#26126;Transformers&#20855;&#26377;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#32467;&#21512;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#23545;&#35805;&#20013;&#36880;&#27493;&#29702;&#35299;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07259</link><description>&lt;p&gt;
&#25581;&#31034;&#38544;&#34255;&#30340;&#32852;&#31995;&#65306;&#29992;&#20110;&#35270;&#39057;&#23545;&#35805;&#30340;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog. (arXiv:2310.07259v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#32467;&#21512;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#23545;&#35805;&#20013;&#36880;&#27493;&#29702;&#35299;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#35270;&#35273;&#38382;&#31572;&#30456;&#27604;&#65292;&#35270;&#39057;&#23545;&#35805;&#38656;&#35201;&#23545;&#23545;&#35805;&#21382;&#21490;&#21644;&#35270;&#39057;&#20869;&#23481;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#65292;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#36880;&#27493;&#29702;&#35299;&#22797;&#26434;&#30340;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#20197;&#36335;&#24452;&#36319;&#36394;&#21644;&#32858;&#21512;&#26426;&#21046;&#20026;&#26680;&#24515;&#65292;&#33021;&#22815;&#20174;&#23545;&#35805;&#21382;&#21490;&#20013;&#33719;&#21462;&#37325;&#35201;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#20197;&#35299;&#37322;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21033;&#29992;&#36845;&#20195;&#25512;&#29702;&#32593;&#32476;&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#21644;&#24378;&#35843;&#20851;&#38190;&#35270;&#35273;&#26631;&#35760;&#65292;&#22686;&#24378;&#23545;&#35270;&#35273;&#29702;&#35299;&#30340;&#28145;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;GPT-&#27169;&#22411;&#23558;&#36825;&#20123;&#20016;&#23500;&#30340;&#20449;&#24687;&#32508;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to conventional visual question answering, video-grounded dialog necessitates a profound understanding of both dialog history and video content for accurate response generation. Despite commendable strides made by existing methodologies, they often grapple with the challenges of incrementally understanding intricate dialog histories and assimilating video information. In response to this gap, we present an iterative tracking and reasoning strategy that amalgamates a textual encoder, a visual encoder, and a generator. At its core, our textual encoder is fortified with a path tracking and aggregation mechanism, adept at gleaning nuances from dialog history that are pivotal to deciphering the posed questions. Concurrently, our visual encoder harnesses an iterative reasoning network, meticulously crafted to distill and emphasize critical visual markers from videos, enhancing the depth of visual comprehension. Culminating this enriched information, we employ the pre-trained GPT-
&lt;/p&gt;</description></item><item><title>MaGNet&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13459</link><description>&lt;p&gt;
&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25972;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Model-Agnostic Graph Neural Network for Integrating Local and Global Information. (arXiv:2309.13459v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13459
&lt;/p&gt;
&lt;p&gt;
MaGNet&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#20197;&#22270;&#20026;&#37325;&#28857;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;GNN&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#30001;&#20110;&#40657;&#30418;&#29305;&#24615;&#65292;&#32467;&#26524;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65307;&#26080;&#27861;&#23398;&#20064;&#19981;&#21516;&#39034;&#24207;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MaGNet&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#20174;&#39640;&#38454;&#37051;&#23621;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;MaGNet&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#22270;&#25299;&#25169;&#19979;&#22797;&#26434;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#20272;&#35745;&#27169;&#22411;&#21644;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#37325;&#35201;&#33410;&#28857;&#29305;&#24449;&#30340;&#35299;&#37322;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;Rademacher&#22797;&#26434;&#24230;&#24314;&#31435;&#20102;MaGNet&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved promising performance in a variety of graph-focused tasks. Despite their success, existing GNNs suffer from two significant limitations: a lack of interpretability in results due to their black-box nature, and an inability to learn representations of varying orders. To tackle these issues, we propose a novel Model-agnostic Graph Neural Network (MaGNet) framework, which is able to sequentially integrate information of various orders, extract knowledge from high-order neighbors, and provide meaningful and interpretable results by identifying influential compact graph structures. In particular, MaGNet consists of two components: an estimation model for the latent representation of complex relationships under graph topology, and an interpretation model that identifies influential nodes, edges, and important node features. Theoretically, we establish the generalization error bound for MaGNet via empirical Rademacher complexity, and showcase its pow
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15051</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#22312;&#25512;&#21160;&#33647;&#29289;&#30740;&#21457;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#24739;&#32773;&#25307;&#21215;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;TrialGPT&#65292;&#37319;&#29992;LLMs&#39044;&#27979;&#22522;&#20110;&#26631;&#20934;&#30340;&#21512;&#26684;&#24615;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#24182;&#26681;&#25454;&#24739;&#32773;&#30149;&#21382;&#20013;&#30340;&#33258;&#30001;&#25991;&#26412;&#26469;&#23545;&#20505;&#36873;&#20020;&#24202;&#35797;&#39564;&#36827;&#34892;&#25490;&#21517;&#21644;&#25490;&#38500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;184&#21517;&#24739;&#32773;&#21644;18,238&#20010;&#27880;&#37322;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#38431;&#21015;&#19978;&#35780;&#20272;&#20102;TrialGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;TrialGPT&#22312;&#26631;&#20934;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#31532;&#20108;&#65292;TrialGPT&#30340;&#32508;&#21512;&#35797;&#39564;&#32423;&#21035;&#35780;&#20998;&#19982;&#19987;&#23478;&#26631;&#27880;&#30340;&#21512;&#26684;&#24615;&#39640;&#24230;&#30456;&#20851;&#12290;&#31532;&#19977;&#65292;&#36825;&#20123;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scor
&lt;/p&gt;</description></item><item><title>&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.09218</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36951;&#24536;&#29616;&#35937;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#36229;&#36234;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09218
&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#25351;&#30340;&#26159;&#20808;&#21069;&#33719;&#21462;&#30340;&#20449;&#24687;&#25110;&#30693;&#35782;&#30340;&#20007;&#22833;&#25110;&#24694;&#21270;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20851;&#20110;&#36951;&#24536;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#36951;&#24536;&#26159;&#19968;&#31181;&#26222;&#36941;&#29616;&#35937;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#20013;&#35266;&#23519;&#21040;&#12290;&#36951;&#24536;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20363;&#22914;&#30001;&#20110;&#29983;&#25104;&#22120;&#28418;&#31227;&#32780;&#22312;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20197;&#21450;&#30001;&#20110;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#32780;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26469;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#28041;&#21450;&#21040;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21516;&#26102;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#35843;&#26597;&#37117;&#40664;&#35748;&#35748;&#20026;&#36951;&#24536;&#24635;&#26159;&#26377;&#23475;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#35748;&#20026;&#36951;&#24536;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#20363;&#22914;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#36951;&#24536;&#29616;&#35937;&#65292;
&lt;/p&gt;
&lt;p&gt;
Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20225;&#19994;&#27969;&#31243;&#36164;&#28304;&#20998;&#37197;&#65292;&#20855;&#26377;&#20248;&#20110;&#24120;&#35265;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09970</link><description>&lt;p&gt;
&#23398;&#20064;&#31574;&#30053;&#22312;&#20225;&#19994;&#27969;&#31243;&#36164;&#28304;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning policies for resource allocation in business processes. (arXiv:2304.09970v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20225;&#19994;&#27969;&#31243;&#36164;&#28304;&#20998;&#37197;&#65292;&#20855;&#26377;&#20248;&#20110;&#24120;&#35265;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#20998;&#37197;&#26159;&#23558;&#36164;&#28304;&#20998;&#37197;&#21040;&#24517;&#39035;&#22312;&#36816;&#34892;&#26102;&#21051;&#25191;&#34892;&#30340;&#19994;&#21153;&#27969;&#31243;&#27963;&#21160;&#20013;&#12290;&#34429;&#28982;&#36164;&#28304;&#20998;&#37197;&#22312;&#21046;&#36896;&#31561;&#20854;&#20182;&#39046;&#22495;&#20013;&#24050;&#32463;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#22312;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20013;&#21364;&#21482;&#23384;&#22312;&#23569;&#37327;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#20225;&#19994;&#27969;&#31243;&#30340;&#24212;&#29992;&#25110;&#26159;&#21482;&#38024;&#23545;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#30340;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20225;&#19994;&#27969;&#31243;&#36164;&#28304;&#20998;&#37197;&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#12290;&#22312;&#20195;&#34920;&#20856;&#22411;&#19994;&#21153;&#27969;&#31243;&#32467;&#26500;&#30340;&#19968;&#32452;&#24773;&#26223;&#20197;&#21450;&#22312;&#20195;&#34920;&#29616;&#23454;&#19994;&#21153;&#27969;&#31243;&#30340;&#23436;&#25972;&#32593;&#32476;&#19978;&#65292;&#23558;&#20004;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#20013;&#20248;&#20110;&#25110;&#19982;&#24120;&#35265;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource allocation is the assignment of resources to activities that must be executed in a business process at a particular moment at run-time. While resource allocation is well-studied in other fields, such as manufacturing, there exist only a few methods in business process management. Existing methods are not suited for application in large business processes or focus on optimizing resource allocation for a single case rather than for all cases combined. To fill this gap, this paper proposes two learning-based methods for resource allocation in business processes: a deep reinforcement learning-based approach and a score-based value function approximation approach. The two methods are compared against existing heuristics in a set of scenarios that represent typical business process structures and on a complete network that represents a realistic business process. The results show that our learning-based methods outperform or are competitive with common heuristics in most scenarios a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24179;&#31283;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22788;&#29702;&#24179;&#28369;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31574;&#30053;&#22312;&#20108;&#27425;Lipschitz&#36830;&#32493;&#30340;&#24773;&#20917;&#19979;&#30340;&#36951;&#25022;&#20026; $\tilde O(T^{3/5})$&#12290;</title><link>http://arxiv.org/abs/2301.12366</link><description>&lt;p&gt;
&#24179;&#28369;&#30340;&#38750;&#24179;&#31283;&#36830;&#32493;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Smooth Non-Stationary Bandits. (arXiv:2301.12366v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24179;&#31283;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22788;&#29702;&#24179;&#28369;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31574;&#30053;&#22312;&#20108;&#27425;Lipschitz&#36830;&#32493;&#30340;&#24773;&#20917;&#19979;&#30340;&#36951;&#25022;&#20026; $\tilde O(T^{3/5})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22312;&#32447;&#20915;&#31574;&#24212;&#29992;&#20013;&#65292;&#29615;&#22659;&#37117;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;&#33021;&#22815;&#22788;&#29702;&#21464;&#21270;&#30340;&#36172;&#21338;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26159;&#20026;&#20102;&#20445;&#25252;&#38750;&#24179;&#28369;&#21464;&#21270;&#32780;&#35774;&#35745;&#30340;&#65292;&#20165;&#21463;&#21040;&#24635;&#21464;&#24046;&#25110;&#26102;&#38388;&#19978;&#30340;Lipschitz&#24615;&#30340;&#38480;&#21046;&#65292;&#20854;&#20013;&#23427;&#20204;&#20445;&#35777;$\tilde \Theta(T^{2/3})$&#30340;&#36951;&#25022;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#29615;&#22659;&#32463;&#24120;&#20197;&#24179;&#31283;&#30340;&#26041;&#24335;&#25913;&#21464;&#65292;&#22240;&#27492;&#36825;&#31181;&#31639;&#27861;&#21487;&#33021;&#20250;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#20135;&#29983;&#27604;&#24517;&#35201;&#26356;&#39640;&#30340;&#36951;&#25022;&#65292;&#24182;&#19988;&#19981;&#21033;&#29992;&#21464;&#21270;&#29575;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#38750;&#24179;&#31283;&#30340;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20551;&#35774;&#33218;&#30340;&#24179;&#22343;&#22238;&#25253;&#26159;&#19968;&#20010;$\beta$-H\''older&#20989;&#25968;&#65292;&#21363;&#23427;&#26159;$(\beta-1)$&#27425;Lipschitz&#36830;&#32493;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31574;&#30053;&#65292;&#23545;&#20110;$\beta=2$&#65292;&#23427;&#30340;&#36951;&#25022;&#20026;$\tilde O(T^{3/5})$&#65292;&#20174;&#32780;&#39318;&#27425;&#22312;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#20043;&#38388;&#36827;&#34892;&#20102;&#21306;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20219;&#24847;$\Omg(T^{(\beta+1)/(2\beta+1)})$&#30340;&#19979;&#30028;&#26469;&#34917;&#20805;&#36825;&#20010;&#32467;&#26524;&#65292;&#35828;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications of online decision making, the environment is non-stationary and it is therefore crucial to use bandit algorithms that handle changes. Most existing approaches are designed to protect against non-smooth changes, constrained only by total variation or Lipschitzness over time, where they guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practice environments are often changing {\bf smoothly}, so such algorithms may incur higher-than-necessary regret in these settings and do not leverage information on the rate of change. We study a non-stationary two-armed bandits problem where we assume that an arm's mean reward is a $\beta$-H\"older function over (normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously differentiable. We show the first separation between the smooth and non-smooth regimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$. We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower bound for any int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#20013;&#22269;&#19971;&#20010;&#20379;&#24212;&#21830;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#22312;&#27169;&#22411;&#29256;&#26435;&#20445;&#25252;&#21644;&#25968;&#23383;&#35777;&#25454;&#21462;&#35777;&#31561;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2208.10489</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#65306;&#21021;&#22987;&#25968;&#25454;&#38598;&#19982;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
System Fingerprint Recognition for Deepfake Audio: An Initial Dataset and Investigation. (arXiv:2208.10489v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#20013;&#22269;&#19971;&#20010;&#20379;&#24212;&#21830;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#22312;&#27169;&#22411;&#29256;&#26435;&#20445;&#25252;&#21644;&#25968;&#23383;&#35777;&#25454;&#21462;&#35777;&#31561;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#37325;&#22823;&#23041;&#32961;&#65292;&#20363;&#22914;&#24694;&#24847;&#20869;&#23481;&#25805;&#32437;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#20986;&#29616;&#20102;&#65292;&#26088;&#22312;&#26816;&#27979;&#25152;&#35859;&#30340;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#23545;&#30495;&#23454;&#38899;&#39057;&#21644;&#20266;&#36896;&#38899;&#39057;&#36827;&#34892;&#20108;&#20803;&#26816;&#27979;&#12290;&#22312;&#27169;&#22411;&#29256;&#26435;&#20445;&#25252;&#21644;&#25968;&#23383;&#35777;&#25454;&#21462;&#35777;&#31561;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#30693;&#36947;&#29983;&#25104;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#24037;&#20855;&#25110;&#27169;&#22411;&#26469;&#35299;&#37322;&#20915;&#31574;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#35782;&#21035;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#31995;&#32479;&#25351;&#32441;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#65288;SFR&#65289;&#30340;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#25105;&#20204;&#20174;&#20351;&#29992;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19971;&#20010;&#20013;&#22269;&#20379;&#24212;&#21830;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#20013;&#25910;&#38598;&#20102;&#35813;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#28165;&#26224;&#21644;&#21387;&#32553;&#38598;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22806;&#37096;&#21442;&#32771;&#38899;&#39057;&#65292;&#20197;&#20415;&#36827;&#34892;&#35780;&#20272;&#21644;&#23545;&#27604;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of deep speech synthesis models has posed significant threats to society such as malicious content manipulation. Therefore, many studies have emerged to detect the so-called deepfake audio. However, existing works focus on the binary detection of real audio and fake audio. In real-world scenarios such as model copyright protection and digital evidence forensics, it is needed to know what tool or model generated the deepfake audio to explain the decision. This motivates us to ask: Can we recognize the system fingerprints of deepfake audio? In this paper, we present the first deepfake audio dataset for system fingerprint recognition (SFR) and conduct an initial investigation. We collected the dataset from the speech synthesis systems of seven Chinese vendors that use the latest state-of-the-art deep learning technologies, including both clean and compressed sets. In addition, to facilitate the further development of system fingerprint recognition methods, we provide ex
&lt;/p&gt;</description></item></channel></rss>